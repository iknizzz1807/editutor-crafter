{
  "types": {
    "Job": "fields: job_id str, job_type str, args List[Any], kwargs Dict[str, Any], queue str, priority int, max_retries int, timeout_seconds int, created_at datetime, metadata Dict[str, Any], status JobStatus, attempts int, errors List[Dict], started_at Optional[datetime], completed_at Optional[datetime], result Optional[Any]",
    "JobStatus": "enum values: PENDING, ACTIVE, COMPLETED, FAILED, RETRY_SCHEDULED, DEAD_LETTER",
    "RedisClient": "fields: url str, connection_kwargs Dict[str, Any], _client Optional[redis.Redis]",
    "QueueConfig": "fields: name str, priority int, max_length Optional[int]",
    "WorkerConfig": "fields: queues List[str], concurrency int, heartbeat_interval int, job_timeout int, worker_id Optional[str], hostname str, pid int",
    "RedisConfig": "fields: url str, max_connections int, socket_timeout int, retry_on_timeout bool",
    "SystemConfig": "fields: redis RedisConfig, queues List[QueueConfig], workers List[WorkerConfig], max_payload_size int, job_history_size int, retry_base_delay int, retry_max_attempts int",
    "QueueManager": "fields: config SystemConfig, redis RedisClient, queue_configs Dict[str, QueueConfig]",
    "ValidationError": "Exception raised when job validation fails",
    "QueueFullError": "Exception raised when queue is at maximum capacity",
    "Worker": "fields: config WorkerConfig, worker_id str, redis RedisClient, handlers Dict[str, Callable], heartbeat Optional[WorkerHeartbeat], executor Optional[JobExecutor], state str, current_job Optional[Job], _shutdown_requested bool, _paused bool",
    "WorkerHeartbeat": "fields: worker_id str, redis_client Any, interval int, _stop_event threading.Event, _thread Optional[threading.Thread], _last_successful Optional[str]",
    "RetryManager": "fields: redis RedisClient, config Dict[str, Any], logger logging.Logger, backoff_calculator BackoffCalculator, dead_letter_queue DeadLetterQueue",
    "BackoffCalculator": "fields: config BackoffConfig",
    "BackoffConfig": "fields: base_delay_seconds float, max_delay_seconds float, min_delay_seconds float, jitter_factor float, max_attempts int",
    "DeadLetterQueue": "fields: redis_client RedisClient, key_prefix str",
    "Schedule": "fields: schedule_id str, job_type str, args List[Any], kwargs Dict[str, Any], queue str, cron_expression Optional[str], run_at Optional[datetime], timezone str, enabled bool, unique_key Optional[str], unique_window_seconds int, created_at datetime, last_enqueued_at Optional[datetime], next_run_at Optional[datetime], metadata Dict[str, Any]",
    "ScheduledJob": "fields: schedule_id str, job_id str, scheduled_for datetime, enqueued_at Optional[datetime], status ScheduleStatus",
    "ScheduleStatus": "enum values: PENDING, ENQUEUED, SKIPPED, ERROR",
    "Scheduler": "fields: redis RedisClient, queue_manager QueueManager, polling_interval_seconds int, batch_size int, logger logging.Logger, _running bool",
    "MetricPoint": "fields: timestamp datetime, name str, value float, labels Dict[str, str]",
    "JobSummary": "fields: job_id str, job_type str, queue str, status JobStatus, enqueued_at datetime, started_at Optional[datetime], completed_at Optional[datetime], duration_seconds Optional[float], attempts int, error_count int",
    "WorkerStatus": "fields: worker_id str, hostname str, pid int, state str, current_job Optional[JobSummary], queues List[str], last_heartbeat datetime, processed_count int, failed_count int, uptime_seconds float",
    "QueueMetrics": "fields: queue_name str, pending_count int, active_count int, completed_count int, failed_count int, retry_scheduled_count int, dead_letter_count int, enqueue_rate float, process_rate float, error_rate float, avg_process_time float, oldest_job_age Optional[float]",
    "SystemAlert": "fields: alert_id str, severity str, source str, message str, condition str, triggered_at datetime, acknowledged_at Optional[datetime], resolved_at Optional[datetime], metadata Dict[str, Any]",
    "DashboardConfig": "fields: refresh_interval int, time_range str, queue_filters List[str], worker_filters List[str], job_type_filters List[str], metric_aggregation str",
    "MetricsAggregator": "fields: redis RedisClient, interval_seconds int, _running bool, _thread Optional[threading.Thread]",
    "AlertRule": "fields: rule_id str, name str, severity str, condition str, window_seconds int, cooldown_seconds int, enabled bool, labels Dict[str, Any]",
    "AlertManager": "fields: redis RedisClient, rules Dict[str, AlertRule], rule_key_prefix str",
    "Janitor": "fields: redis RedisClient, scan_interval int, running bool",
    "CircuitBreaker": "fields: name str, failure_threshold int, recovery_timeout float, expected_exceptions tuple, state CircuitState, failure_count int, last_failure_time Optional[float], last_state_change float",
    "CircuitState": "enum values: CLOSED, OPEN, HALF_OPEN",
    "RedisTestContext": "fields: port int, db int, _client Optional[redis.Redis], _lock threading.Lock",
    "SystemMetrics": "fields: timestamp datetime, cpu_percent float, memory_rss int, redis_memory_used int, redis_connected_clients int, queue_depths Dict[str, int], worker_count int, schedule_count int",
    "DiagnosticWorker": "fields: redis RedisClient, interval int, running bool, thread Optional[threading.Thread]",
    "TokenBucketRateLimiter": "fields: redis redis.Redis, lua_script str",
    "DependencyManager": "fields: redis Any, queue_manager QueueManager, pubsub Any"
  },
  "methods": {
    "Job.to_dict() returns Dict[str, Any]": "Convert job to dictionary for serialization",
    "Job.from_dict(data Dict[str, Any]) returns Job": "Reconstruct job from dictionary",
    "Job.serialize() returns str": "Serialize job to JSON string",
    "Job.deserialize(data str) returns Job": "Deserialize job from JSON string",
    "Job.record_error(error Exception) returns None": "Record an error that occurred during job execution",
    "Job.should_retry() returns bool": "Determine if job should be retried",
    "RedisClient.get_instance(url Optional[str], **kwargs) returns RedisClient": "Get singleton RedisClient instance",
    "RedisClient.get_client() returns redis.Redis": "Get the underlying Redis client instance",
    "RedisClient.execute(command str, *args, **kwargs) returns Any": "Execute a Redis command with error handling",
    "RedisClient.pipeline() returns redis.Pipeline": "Return a Redis pipeline for atomic operations",
    "SystemConfig.from_env() returns SystemConfig": "Load configuration from environment variables with defaults",
    "QueueManager.enqueue_job(job Job) returns str": "Validates, serializes, and atomically adds a job to the appropriate Redis queue",
    "QueueManager.bulk_enqueue(jobs List[Job]) returns List[str]": "Enqueues multiple jobs in a single atomic transaction",
    "QueueManager.get_queue_length(queue_name str) returns int": "Returns the current number of pending jobs in the specified queue",
    "QueueManager.peek_queue(queue_name str, count int = 10) returns List[Job]": "Retrieves up to count jobs from the front of the queue without removing them",
    "QueueManager.list_queues(include_internal bool = False) returns List[str]": "Returns names of all configured queues",
    "QueueManager.delete_job(job_id str, queue_name str) returns bool": "Removes a specific job from a queue if it exists",
    "QueueManager.get_job_by_id(job_id str) returns Optional[Job]": "Retrieves a job from Redis by its ID",
    "QueueManager._validate_job(job Job) returns None": "Internal method to validate job before enqueue",
    "Worker.start() returns None": "Starts the worker main loop",
    "Worker.stop(graceful=True) returns None": "Stops the worker, optionally gracefully",
    "Worker.pause() returns None": "Temporarily stops fetching new jobs",
    "Worker.resume() returns None": "Resumes fetching new jobs",
    "Worker.get_status() returns Dict[str, Any]": "Returns current worker status",
    "Worker.register_handler(job_type, handler_func) returns None": "Registers a function to handle a specific job type",
    "WorkerHeartbeat.start() returns None": "Start the heartbeat thread",
    "WorkerHeartbeat.stop() returns None": "Stop the heartbeat thread",
    "WorkerHeartbeat.is_healthy() returns bool": "Check if heartbeat is updating successfully",
    "RetryManager.handle_job_failure(job: Job, error: Exception) returns None": "Handle job failure and schedule retry or move to dead letter",
    "RetryManager.schedule_retry(job: Job, delay_seconds: float) returns str": "Schedule job for retry after delay",
    "RetryManager.move_to_dead_letter(job: Job) returns bool": "Move job to dead letter queue",
    "RetryManager.get_due_retries(queue_name: str, max_count: int = 100) returns List[Job]": "Get jobs whose retry time has arrived",
    "BackoffCalculator.calculate_delay(attempt: int) returns float": "Calculate retry delay for given attempt",
    "BackoffCalculator.calculate_next_retry_time(attempt: int) returns float": "Calculate absolute timestamp for next retry",
    "schedule_delayed_job(job Job, run_at datetime) returns str": "Schedule a job for one-time delayed execution",
    "schedule_recurring_job(job Job, cron_expression str, timezone str = 'UTC') returns str": "Schedule a job for recurring execution based on cron expression",
    "unschedule_job(schedule_id str) returns bool": "Remove a schedule by ID",
    "pause_schedule(schedule_id str) returns bool": "Temporarily disable a schedule",
    "resume_schedule(schedule_id str) returns bool": "Re-enable a paused schedule",
    "get_schedule(schedule_id str) returns Optional[Schedule]": "Retrieve schedule definition by ID",
    "list_schedules(enabled_only bool = False) returns List[Schedule]": "List all schedules",
    "update_schedule(schedule_id str, updates Dict[str, Any]) returns bool": "Update specific fields of a schedule",
    "enqueue_due_jobs(max_jobs int = 100) returns int": "Main polling method: finds due jobs and enqueues them",
    "_calculate_next_run(schedule Schedule, reference_time Optional[datetime] = None) returns datetime": "Calculate next run time for a schedule",
    "_enqueue_scheduled_job(schedule Schedule, scheduled_for datetime) returns Tuple[bool, Optional[str]]": "Enqueue a job from a schedule",
    "_poll_due_schedules() returns int": "Poll for and process due schedules",
    "_process_schedule(schedule Schedule) returns bool": "Process a single due schedule",
    "start() returns None": "Start the scheduler polling loop",
    "stop() returns None": "Stop the scheduler polling loop",
    "calculate_next_runs_for_all_schedules() returns int": "Recalculate next_run_at for all schedules",
    "get_system_metrics() returns Dict[str, List[MetricPoint]]": "Get system-wide metrics",
    "get_queue_metrics() returns List[QueueMetrics]": "Get current metrics for specified queues",
    "get_worker_status() returns List[WorkerStatus]": "Get status of all workers",
    "get_job_history() returns Dict[str, Any]": "Search jobs with filtering and pagination",
    "get_dead_letter_jobs() returns List[Job]": "Get jobs in dead letter queue",
    "retry_dead_letter_job() returns Dict[str, Any]": "Retry a specific dead letter job",
    "delete_dead_letter_job() returns Dict[str, Any]": "Permanently remove a job from dead letter queue",
    "get_alerts() returns List[SystemAlert]": "Get current active alerts",
    "acknowledge_alert() returns Dict[str, Any]": "Mark an alert as acknowledged",
    "create_alert_rule() returns Dict[str, Any]": "Create a new alerting rule",
    "get_metrics_stream() returns StreamingResponse": "Real-time stream of metric updates",
    "MetricsAggregator.start() returns None": "Start the aggregation thread",
    "MetricsAggregator.stop() returns None": "Stop the aggregation thread",
    "MetricsAggregator._run_aggregation() returns None": "Execute a single aggregation cycle",
    "AlertRule.evaluate(metrics_store) returns bool": "Evaluate rule against current metrics",
    "AlertRule.should_alert(last_alert_time) returns bool": "Check if alert should fire considering cooldown",
    "AlertManager.load_rules() returns None": "Load all alert rules from Redis",
    "AlertManager.evaluate_all_rules() returns None": "Evaluate all enabled alert rules",
    "AlertManager.fire_alert(rule, condition_value) returns str": "Fire an alert and store it",
    "Scheduler.enqueue_due_jobs(max_jobs int = 100) returns int": "Main polling method: finds due jobs and enqueues them",
    "Scheduler._calculate_next_run(schedule Schedule, reference_time Optional[datetime] = None) returns datetime": "Calculate next run time for a schedule",
    "Scheduler._enqueue_scheduled_job(schedule Schedule, scheduled_for datetime) returns Tuple[bool, Optional[str]]": "Enqueue a job from a schedule",
    "Scheduler._poll_due_schedules() returns int": "Poll for and process due schedules",
    "Scheduler._process_schedule(schedule Schedule) returns bool": "Process a single due schedule",
    "Scheduler.start() returns None": "Start the scheduler polling loop",
    "Scheduler.stop() returns None": "Stop the scheduler polling loop",
    "Scheduler.calculate_next_runs_for_all_schedules() returns int": "Recalculate next_run_at for all schedules",
    "Janitor._get_stale_processing_queues() returns List[str]": "Find all queue:processing lists where the associated worker is dead",
    "Janitor._requeue_stale_jobs(processing_queue_key: str) returns int": "Move all jobs from a stale processing queue back to its main queue",
    "Janitor.run_once() returns int": "Execute a single cleanup cycle. Returns number of jobs requeued",
    "Janitor.start() returns None": "Start the janitor's main loop",
    "Janitor.stop() returns None": "Stop the janitor",
    "Janitor._cleanup_expired_keys() returns int": "Remove expired keys from Redis to prevent memory bloat",
    "CircuitBreaker.call(func: Callable, *args, **kwargs) returns Any": "Execute function with circuit breaker protection",
    "CircuitBreaker._record_failure() returns None": "Record a failure and update circuit state",
    "BackoffCalculator.calculate_delay(attempt int) returns float": "Calculate retry delay for given attempt",
    "get_queue_contents(queue_name: str, include_processing: bool = True, include_retry: bool = False, limit: int = 50) returns Dict[str, Any]": "Inspect all jobs in a queue, including processing and retry queues",
    "get_job_trace(job_id: str) returns Dict[str, Any]": "Reconstruct the complete lifecycle of a job",
    "run_janitor() returns Dict[str, Any]": "Manually trigger the janitor process to clean up stale jobs",
    "scan_redis_keys(pattern: str = \"*\", limit: int = 100, with_ttl: bool = False) returns Dict[str, Any]": "Scan Redis keys matching pattern",
    "collect_metrics() returns SystemMetrics": "Collect comprehensive system metrics",
    "check_anomalies(metrics: SystemMetrics) returns List[Dict[str, Any]]": "Check for system anomalies based on metrics",
    "run_diagnostics() returns None": "Run one diagnostic cycle",
    "inspect_queue(queue_name: str, limit: int = 10) returns None": "Inspect jobs in a queue",
    "find_stuck_jobs(age_hours: int = 1) returns None": "Find jobs that have been active for too long",
    "cleanup_processing_queues() returns None": "Clean up jobs stuck in processing queues",
    "TokenBucketRateLimiter.check_limit(queue_name str, requests_per_second float, burst_capacity int) returns": "Check rate limit, returns (allowed, wait_time)",
    "DependencyManager.register_job_with_dependencies(job Job, dependencies List[str]) returns": "Register job with dependencies",
    "DependencyManager._handle_job_completion(job_id str, status str) returns": "Handle job completion and notify dependents",
    "DependencyManager.start_listening() returns": "Start listening for job completion events",
    "DependencyManager.cancel_job_chain(job_id str) returns": "Cancel job and all dependents"
  },
  "constants": {
    "PENDING": "Job status: waiting in queue",
    "ACTIVE": "Job status: currently being processed",
    "COMPLETED": "Job status: finished successfully",
    "FAILED": "Job status: finished with error",
    "RETRY_SCHEDULED": "Job status: failed and scheduled for retry",
    "DEAD_LETTER": "Job status: permanently failed after max retries",
    "ENQUEUED": "Schedule status: successfully enqueued to worker queue",
    "SKIPPED": "Schedule status: skipped due to uniqueness constraint",
    "ERROR": "Schedule status: error during enqueue attempt",
    "CRITICAL": "Alert severity level",
    "WARNING": "Alert severity level",
    "INFO": "Alert severity level",
    "CLOSED": "Circuit breaker state: normal operation",
    "OPEN": "Circuit breaker state: failing fast, not attempting operations",
    "HALF_OPEN": "Circuit breaker state: testing if service has recovered"
  },
  "terms": {
    "broker": "Central message store (Redis in our design) that connects producers and consumers",
    "dead letter queue": "Storage for jobs that have exhausted all retry attempts",
    "exponential backoff": "Retry algorithm that doubles wait time between attempts",
    "job": "Unit of work to be processed asynchronously",
    "worker": "Process that fetches and executes jobs from queues",
    "queue": "Named channel for jobs with configurable priority",
    "scheduler": "Component that enqueues jobs for future or recurring execution",
    "FIFO": "First-In-First-Out ordering",
    "SIGTERM": "Signal to request graceful shutdown",
    "idempotent": "Property where repeated execution produces same result",
    "cron": "Time-based job schedule expression",
    "ULID": "Universally Unique Lexicographically Sortable Identifier",
    "YAGNI": "You Ain't Gonna Need It - principle of not implementing features until actually needed",
    "priority weighted polling": "Worker polling strategy that checks higher-priority queues more frequently",
    "atomic pipeline": "Redis feature that executes multiple commands as a single atomic operation",
    "TTL": "Time-To-Live for Redis keys to prevent memory bloat",
    "heartbeat": "Regular status update to indicate worker liveness",
    "graceful shutdown": "Finishing current job before process exit",
    "concurrency": "Number of jobs a worker can process simultaneously",
    "processing queue": "Temporary queue holding jobs currently being executed",
    "job handler": "Function that executes the business logic of a job",
    "jitter": "Random variation added to retry delays to prevent synchronization",
    "timezone": "Geographical region's standard time, used for schedule evaluation",
    "polling interval": "Time between scheduler checks for due jobs",
    "uniqueness window": "Time period during which duplicate job enqueueing is prevented",
    "daylight saving time": "Seasonal time adjustment where clocks are set forward/back",
    "catch-up logic": "Mechanism to process jobs missed during scheduler downtime",
    "Server-Sent Events (SSE)": "HTTP-based technology for server-to-client real-time updates",
    "metric aggregation": "Process of combining raw events into statistical summaries",
    "alert cooldown": "Minimum time between consecutive alerts for same rule",
    "job history archival": "Process of moving old job data from Redis to external storage",
    "real-time dashboard": "Web interface that updates without page refresh",
    "alert fatigue": "Problem where too many alerts cause operators to ignore them",
    "sensitive data redaction": "Process of masking confidential information in logs and displays",
    "adaptive sampling": "Adjusting metric collection frequency based on system load",
    "two-tiered storage": "Architecture with fast cache for recent data and slow storage for history",
    "janitor process": "A maintenance process that cleans up stale jobs from processing queues",
    "thundering herd": "Problem where many retried jobs simultaneously overwhelm the system",
    "retry filter": "Logic to determine if a specific error type should bypass retries",
    "circuit breaker": "Design pattern that fails fast when a service is unavailable, preventing cascading failures",
    "split-brain": "Scenario in distributed systems where network partition causes components to operate independently",
    "zombie job": "Job stuck in processing queue with no active worker processing it",
    "idempotency key": "Unique identifier to prevent duplicate processing",
    "integration tests": "Tests that verify interactions between multiple components with real dependencies",
    "end-to-end tests": "Tests that simulate real user scenarios across the entire system",
    "chaos testing": "Testing system behavior under intentionally injected failures",
    "property-based testing": "Testing by generating random inputs that must satisfy certain properties",
    "flaky test": "Test that passes and fails intermittently without code changes",
    "test fixture": "Reusable setup/teardown code for tests",
    "mock clock": "Simulated time control for testing time-dependent logic",
    "race condition": "Bug where outcome depends on timing of concurrent operations",
    "statistical assertion": "Verification that holds true over many random trials, not just one execution",
    "correlation id": "Unique identifier passed through the entire job lifecycle for tracing",
    "token bucket": "Rate limiting algorithm that allows bursts up to capacity",
    "DAG": "Directed Acyclic Graph - dependency model without cycles",
    "Lua script": "Redis server-side script for atomic operations",
    "protocol versioning": "Technique for maintaining backward compatibility in APIs"
  }
}