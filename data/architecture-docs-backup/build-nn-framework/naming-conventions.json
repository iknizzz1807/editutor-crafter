{
  "types": {
    "Tensor": "data: np.ndarray, requires_grad: bool, grad: Optional[Tensor], grad_fn: Optional[Operation]",
    "Operation": "inputs: Tuple[Tensor, ...], abstract base for forward() and backward()",
    "Add": "extends Operation, implements element-wise addition",
    "Multiply": "extends Operation, implements element-wise multiplication",
    "MatMul": "extends Operation, implements matrix multiplication",
    "Module": "_parameters: Dict[str, Tensor], _modules: Dict[str, Module], training: bool",
    "Linear": "extends Module, weight: Tensor, bias: Optional[Tensor], in_features: int, out_features: int",
    "ReLU": "extends Module, inplace: bool",
    "Sigmoid": "extends Module, no parameters",
    "Tanh": "extends Module, no parameters",
    "Sequential": "extends Module, contains ordered list of modules",
    "Optimizer": "param_groups: List[Dict], state: Dict[int, Dict]",
    "SGD": "extends Optimizer, momentum buffers in state",
    "Adam": "extends Optimizer, first_moments: Dict, second_moments: Dict, step_count: int",
    "LRScheduler": "optimizer: Optimizer, last_epoch: int",
    "DataLoader": "dataset: Tuple, batch_size: int, shuffle: bool",
    "Loss": "reduction: str",
    "TrainingCoordinator": "model: Module, optimizer: Optimizer, loss_function: Loss, monitor: TrainingMonitor",
    "TrainingMonitor": "check_frequency: int, step_count: int, loss_history: List[float], gradient_norms: List[float]",
    "ComputationGraphTracker": "_active_graphs: Set, _cleanup_callbacks: List",
    "GraphCleanupContext": "tensors: List[Tensor]",
    "NeuralFrameworkError": "message: str, details: Dict, stack_info: List",
    "ShapeError": "shapes: List[Tuple], operation: str",
    "BroadcastingError": "shape1: Tuple, shape2: Tuple, step_analysis: List[str]",
    "GradientError": "base error for gradient computation failures",
    "NumericalInstabilityError": "tensor_info: Dict with nan_count, inf_count",
    "MemoryMonitor": "warning_threshold: float, critical_threshold: float, baseline_memory: int",
    "GradientMonitor": "check_frequency: int, step_count: int, gradient_history: List",
    "GradientTester": "tolerance: float, test_results: List",
    "TrainingResult": "final_loss: float, final_accuracy: float, loss_history: List[float], convergence_epoch: Optional[int], training_time: float",
    "EndToEndTester": "tolerance: Dict[str, float], results: Dict",
    "MilestoneCheckpoints": "static methods for milestone verification",
    "ToyDatasets": "static methods for synthetic dataset generation",
    "GPUTensor": "data: array, device: str, requires_grad: bool, grad_fn: Operation",
    "FusionOptimizer": "fusion_patterns: List",
    "Conv2d": "in_channels: int, out_channels: int, kernel_size: int, weight: Tensor, bias: Tensor",
    "MaxPool2d": "kernel_size: int, stride: int",
    "ModelSerializer": "static methods for save/load",
    "FrameworkProfiler": "operation_times: Dict, memory_usage: List, gpu_utilization: List"
  },
  "methods": {
    "__init__(data, requires_grad, grad_fn)": "initialize tensor with data and gradient tracking",
    "backward(gradient=None)": "initiate backpropagation from this tensor",
    "_backward(gradient)": "internal recursive gradient computation",
    "__add__(other)": "tensor addition with gradient tracking",
    "__mul__(other)": "element-wise multiplication with gradient tracking",
    "matmul(other)": "matrix multiplication with gradient tracking",
    "forward(*inputs)": "compute forward pass, return numpy array",
    "backward(grad_output)": "compute gradients w.r.t inputs",
    "numerical_gradient(f, inputs, h)": "compute numerical gradients using finite differences",
    "check_gradients(f, inputs, tolerance)": "compare autodiff vs numerical gradients",
    "broadcast_shapes(shape1, shape2)": "compute broadcasted shape or raise error",
    "unbroadcast_gradient(grad, original_shape)": "reduce gradient to original parameter shape",
    "parameters()": "recursively collect all trainable tensors",
    "register_parameter(name, param)": "register tensor as trainable parameter",
    "register_module(name, module)": "register submodule for recursive operations",
    "topological_sort(start_tensor)": "compute backward pass order",
    "clear_computation_graph(tensor)": "break reference cycles",
    "forward(*inputs) -> Tensor": "abstract method defining forward computation",
    "parameters() -> Iterator[Tensor]": "recursively collect all trainable parameters",
    "named_parameters(prefix) -> Iterator[Tuple[str, Tensor]]": "collect parameters with hierarchical names",
    "train(mode=True) -> Module": "set training mode",
    "eval() -> Module": "set evaluation mode",
    "zero_grad()": "clear gradients from previous step",
    "reset_parameters()": "reinitialize weights using default strategy",
    "xavier_uniform_(tensor, gain)": "initialize with Xavier uniform distribution",
    "kaiming_uniform_(tensor, a, mode)": "initialize with He uniform distribution",
    "zeros_(tensor)": "initialize tensor with zeros",
    "validate_parameter(param)": "validate object is proper parameter tensor",
    "step() -> None": "apply optimization step",
    "zero_grad() -> None": "clear gradients from previous step",
    "state_dict() -> Dict": "export optimizer state for checkpointing",
    "load_state_dict(state_dict) -> None": "restore optimizer state from checkpoint",
    "get_lr() -> List[float]": "compute learning rates for scheduler",
    "train_epoch(model, data_loader, optimizer, loss_fn) -> float": "train for one epoch, return average loss",
    "validate(model, data_loader, loss_fn) -> float": "evaluate model on validation set",
    "save_checkpoint(path, epoch, model, optimizer, loss) -> None": "save training state to disk",
    "load_checkpoint(path) -> Dict": "load training checkpoint",
    "forward(predictions, targets) -> Tensor": "compute loss with gradient tracking",
    "training_step(inputs, targets) -> Dict[str, float]": "execute complete training step with coordination",
    "validation_step(inputs, targets) -> Dict[str, float]": "execute validation step without gradients",
    "coordinate_forward_pass(model, inputs) -> Tuple[Tensor, List[Tensor]]": "coordinate forward pass with graph tracking",
    "coordinate_backward_pass(loss, parameters) -> Dict[str, Tensor]": "coordinate backward pass with validation",
    "log_step(loss, gradients) -> None": "log training metrics and check stability",
    "cleanup_graphs() -> int": "force cleanup computation graphs",
    "check_gradient_flow(model, loss) -> Dict[str, float]": "check gradient flow through parameters",
    "validate_gradient_computation(model, inputs, targets, loss_fn, tolerance) -> bool": "validate gradients using numerical differentiation",
    "backward(gradient=None) -> None": "initiate backpropagation",
    "validate_tensor_finite(tensor, operation)": "check tensor contains only finite values",
    "create_shape_error_message(shapes, operation, expected_pattern)": "create detailed shape mismatch error",
    "analyze_broadcasting_failure(shape1, shape2)": "analyze why broadcasting failed",
    "validate_broadcasting_safety(tensors, operation)": "validate tensor list can be broadcasted",
    "validate_gradient_flow(model, loss)": "check gradient flow through parameters",
    "cleanup_graphs()": "force cleanup of unreachable computation graphs",
    "check_memory_usage()": "check current memory usage and return status",
    "cleanup_training_step(model, optimizer)": "cleanup after training step",
    "log_step(loss, gradients)": "log training step and check for problems",
    "validate_training_step(model, loss, optimizer)": "validate training step completed successfully",
    "relative_error(a, b, eps)": "compute relative error between arrays",
    "check_gradients(f, inputs, autodiff_grads, tolerance, h)": "compare autodiff vs numerical gradients",
    "test_operation(operation_name, test_function)": "test gradients for specific operation",
    "linear_regression(n_samples, noise, seed)": "generate y = 3x + 2 + noise dataset",
    "xor_problem()": "generate XOR truth table dataset",
    "spiral_dataset(n_points, noise, seed)": "generate two-class spiral pattern",
    "test_linear_regression_convergence(framework_classes)": "verify linear regression solves to analytical solution",
    "test_xor_classification(framework_classes)": "verify XOR problem achieves perfect accuracy",
    "test_overfitting_capability(framework_classes)": "verify framework can overfit small datasets",
    "run_statistical_validation(test_function, n_runs)": "run tests with multiple random seeds",
    "verify_milestone_1(tensor_class, operation_classes)": "automated milestone 1 verification",
    "verify_milestone_2(tensor_class, operation_classes)": "automated milestone 2 verification",
    "verify_milestone_3(module_classes)": "automated milestone 3 verification",
    "verify_milestone_4(optimizer_classes, loss_classes)": "automated milestone 4 verification",
    "check_gradient_flow(model, loss)": "check gradient flow through parameters",
    "to(device) -> GPUTensor": "move tensor between CPU and GPU",
    "optimize_graph(root_tensor) -> Tensor": "apply fusion optimizations to computation graph",
    "im2col(input, kernel_size, stride, padding) -> Tensor": "transform convolution to matrix multiplication",
    "save_model(model, optimizer, path, metadata) -> None": "save complete training state",
    "load_model(path, model_class) -> Tuple": "load model and optimizer from disk",
    "model_to_onnx(model, example_input, output_path) -> None": "export model to ONNX format",
    "profile_operation(name, func, args) -> Any": "time operation execution and collect metrics",
    "step()": "apply optimization step",
    "to(device)": "move tensor between CPU and GPU"
  },
  "constants": {
    "requires_grad": "boolean flag for gradient tracking",
    "grad_fn": "operation that created tensor for autodiff",
    "tolerance": "1e-5 default for gradient checking",
    "training": "boolean flag for training vs evaluation mode",
    "inplace": "boolean flag for in-place operations",
    "learning_rate": "step size multiplier for parameter updates",
    "momentum": "coefficient for velocity accumulation in SGD",
    "beta1": "exponential decay rate for first moment estimates in Adam",
    "beta2": "exponential decay rate for second moment estimates in Adam",
    "epsilon": "small constant for numerical stability",
    "weight_decay": "L2 regularization coefficient",
    "check_frequency": "profiling sample rate",
    "warning_threshold": "0.8 memory usage fraction for warnings",
    "critical_threshold": "0.9 memory usage fraction for critical alerts",
    "h": "1e-5 step size for numerical differentiation",
    "success_rate_threshold": "0.8 minimum for test passage",
    "GPU_AVAILABLE": "boolean indicating CuPy availability",
    "fusion_patterns": "list of operation fusion strategies"
  },
  "terms": {
    "automatic differentiation": "mechanical computation of exact derivatives",
    "reverse-mode autodiff": "backpropagation through computation graph",
    "computation graph": "DAG recording operations for gradient computation",
    "chain rule": "calculus rule for differentiating composed functions",
    "broadcasting": "automatic shape expansion for tensor operations",
    "gradient accumulation": "summing gradients from multiple paths through graph",
    "define-by-run": "dynamic graph construction during forward pass",
    "eager execution": "immediate evaluation of operations",
    "topological sort": "ordering nodes for correct backward pass traversal",
    "educational clarity": "prioritizing code readability and understanding over performance",
    "scope creep": "uncontrolled addition of features beyond core learning objectives",
    "four-layer architecture": "tensor operations, autodiff engine, neural modules, optimizers hierarchy",
    "directed acyclic graph": "graph structure with no cycles for computation flow",
    "module system": "composable building blocks for neural networks",
    "parameter registration": "automatic tracking of trainable tensors",
    "recursive parameter collection": "depth-first traversal to gather all parameters",
    "weight initialization": "setting initial parameter values for stable training",
    "Xavier initialization": "weight initialization maintaining activation variance",
    "He initialization": "weight initialization for ReLU networks",
    "sequential container": "module that chains other modules in linear order",
    "mode switching": "changing behavior between training and evaluation",
    "hierarchical naming": "dot notation names reflecting module structure",
    "parameter sharing": "using same tensor in multiple network locations",
    "optimizer": "algorithm that updates parameters using gradients",
    "stochastic gradient descent": "basic optimization using negative gradient direction",
    "momentum": "velocity accumulation smoothing gradient updates",
    "Adam": "adaptive moment estimation with per-parameter learning rates",
    "bias correction": "compensation for moment estimate initialization bias",
    "learning rate scheduling": "dynamic adjustment of learning rate during training",
    "mini-batch training": "processing fixed-size batches of samples",
    "loss function": "differentiable measure of prediction quality",
    "training loop": "orchestration of forward pass, loss, backward pass, optimization",
    "parameter updates": "modification of model weights using optimizer",
    "cross-entropy loss": "standard loss for classification tasks",
    "mean squared error": "standard loss for regression tasks",
    "forward pass data flow": "how data flows through modules while building computation graph",
    "backward pass coordination": "gradient computation flow from loss back through network",
    "complete training step sequence": "end-to-end sequence of forward, loss, backward, and parameter updates",
    "memory management": "preventing leaks and managing computation graph lifecycle",
    "numerical stability": "avoiding NaN/inf values during computation",
    "shape mismatch": "tensor shapes incompatible for operations",
    "gradient explosion": "gradient values become extremely large",
    "gradient vanishing": "gradient magnitudes become extremely small",
    "numerical instability": "NaN or infinity values in computation",
    "computation graph memory leak": "circular references prevent garbage collection",
    "gradient checkpointing": "trading computation for memory by recomputing activations",
    "topological ordering": "correct sequence for backward pass gradient computation",
    "memory-mapped files": "using disk storage for very large tensors",
    "resource monitoring": "tracking memory and computation resource usage",
    "automatic cleanup": "system manages memory without manual intervention",
    "gradient validation": "comparing autodiff results with numerical differentiation",
    "step-by-step analysis": "detailed breakdown of where operations fail",
    "error handling": "systematic approach to detecting and managing failures",
    "gradient correctness testing": "comparing autodiff results with numerical differentiation",
    "numerical differentiation": "finite difference approximation of derivatives",
    "central difference": "f'(x) â‰ˆ [f(x+h) - f(x-h)] / (2h)",
    "relative error": "scale-invariant comparison of gradient accuracy",
    "milestone verification": "integration testing after each major development stage",
    "end-to-end training tests": "complete neural network training on toy problems",
    "toy datasets": "synthetic problems with known expected behaviors",
    "convergence pattern analysis": "verifying expected training behavior over time",
    "statistical validation": "testing with multiple random seeds for consistency",
    "gradient flow integration": "verifying gradients propagate correctly through entire network",
    "overfitting capability test": "confirming framework can learn by overfitting small datasets",
    "GPU acceleration": "massively parallel computation using graphics processors",
    "operation fusion": "combining multiple operations into optimized kernels",
    "im2col transformation": "reshaping convolution input for matrix multiplication",
    "mixed precision training": "using 16-bit floats for memory efficiency",
    "computation graph optimization": "improving performance through graph transformations",
    "ONNX export": "converting models to Open Neural Network Exchange format",
    "model serialization": "saving and loading trained model state",
    "ecosystem integration": "connecting with existing ML tools and workflows"
  }
}