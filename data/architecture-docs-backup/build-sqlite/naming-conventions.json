{
  "types": {
    "Pager": "fields: file_descriptor int, file_length uint32_t, num_pages uint32_t, pages void*[]",
    "Database": "fields: pager Pager*",
    "Token": "fields: type TokenType, value char*, position uint32_t",
    "AST": "fields: type ASTType, children ASTNode**",
    "Cursor": "fields: btree BTree*, current_page uint32_t, cell_index uint32_t, end_of_table bool, stack_depth uint8_t, stack uint32_t[10]",
    "TransactionState": "fields: state TransactionStatus, journal Journal*, dirty_pages uint32_t*, dirty_count uint32_t",
    "ASTNode": "fields: type ASTType, children ASTNode**, value union {char* str_val; int64_t int_val; double float_val; bool bool_val; Operator op_val; DataType data_type;}",
    "ColumnValue": "fields: type DataType, value union {int64_t int_val; double float_val; char* text_val; struct {void* data; size_t length;} blob_val;}",
    "Row": "fields: rowid int64_t, columns ColumnValue*, column_count uint32_t",
    "PageHeader": "fields: page_type uint8_t, freeblock_start uint16_t, cell_count uint16_t, cell_content_start uint16_t, fragmented_free_bytes uint8_t, rightmost_child uint32_t",
    "Page": "fields: header PageHeader, data uint8_t[]",
    "Lexer": "fields: source const char*, position uint32_t, length uint32_t, error_msg char[256]",
    "Parser": "fields: lexer Lexer*, current_token Token*, error_msg char[256]",
    "BTree": "fields: root_page uint32_t, pager Pager*, schema Schema*",
    "ResultSet": "fields: rows Row**, row_count uint32_t, column_names char**, column_count uint32_t, current_pos uint32_t",
    "ExecutionPlan": "fields: root_operator Operator*, estimated_cost double, estimated_rows uint32_t",
    "Operator": "fields: type OperatorType, children Operator**, next function pointer, reset function pointer, context void*",
    "TableScanOperator": "fields: base Operator, cursor Cursor*, table_name char*, schema Schema*, current_row Row, row_valid bool",
    "IndexScanOperator": "fields: base Operator, cursor Cursor*, index_name char*, table_name char*, key_range KeyRange*",
    "FilterOperator": "fields: base Operator, predicate ASTNode*, child Operator*, eval_ctx ExpressionContext*, current_row Row*",
    "ProjectionOperator": "fields: base Operator, column_indices int*, column_count uint32_t, child Operator*",
    "InsertOperator": "fields: base Operator, table_name char*, columns char**, values Expression**, value_count uint32_t",
    "UpdateOperator": "fields: base Operator, table_name char*, assignments Assignment*, assignment_count uint32_t, where_clause Expression*",
    "DeleteOperator": "fields: base Operator, table_name char*, where_clause Expression*",
    "ExpressionContext": "fields: current_row Row*, table_schema Schema*, user_context void*",
    "Journal": "fields: file_descriptor int, filename char*, in_transaction bool, page_size uint32_t",
    "WAL": "fields: file_descriptor int, filename char*, header WALHeader*, frame_index uint32_t, read_lock uint32_t, write_lock bool",
    "WALHeader": "fields: magic uint32_t, version uint32_t, page_size uint32_t, checkpoint_seq uint32_t, salt1 uint32_t, salt2 uint32_t, checksum uint32_t",
    "WALFrame": "fields: page_number uint32_t, db_size uint32_t, data uint8_t[PAGE_SIZE], checksum uint32_t",
    "JournalHeader": "fields: magic uint32_t, page_size uint32_t, reserved uint32_t[62]",
    "ErrorContext": "fields: code ErrorCode, message char[256], component const char*, line int, file const char*",
    "ValidationContext": "fields: errors ValidationError*, error_count uint32_t, pages_checked uint32_t",
    "ValidationError": "fields: page_num uint32_t, message char[256]",
    "PreparedStatement": "fields: sql const char*, ast AST*, plan ExecutionPlan*, bindings ParameterBinding*, param_count int",
    "PlanCache": "fields: entries PlanCacheEntry**, capacity size_t, size size_t, clock_hand uint64_t",
    "VFS": "fields: xOpen function pointer, xRead function pointer, xWrite function pointer, xSync function pointer, xClose function pointer, xFileSize function pointer, xLock function pointer, xUnlock function pointer, name const char*, xRandomness function pointer, app_data void*",
    "AggregationOperator": "fields: base Operator, child Operator*, groups AggregationGroup*, specs AggregationSpec*, num_specs int, group_by_present bool, group_by_columns int*, num_group_columns int",
    "AggregationSpec": "fields: func AggregationFunc, argument Expression*, output_type DataType, accumulator void*",
    "SubqueryOperator": "fields: base Operator, outer_child Operator*, type SubqueryType, subquery_ast AST*, subquery_plan ExecutionPlan*, cached_result MaterializedResult*, correlation_column int",
    "FullTextSearchOperator": "fields: base Operator, search_query const char*, index FullTextIndex*, current_iterator PostingListIterator*, rankings ResultRanking*, score_threshold double",
    "ForeignKeyConstraint": "fields: name const char*, table_name const char*, column_names const char*[16], ref_table_name const char*, ref_column_names const char*[16], on_delete ForeignKeyAction, on_update ForeignKeyAction, num_columns int",
    "NestedLoopJoinState": "fields: left_row Row, right_row Row, left_row_valid bool, right_row_valid bool, left_exhausted bool, need_new_left bool",
    "MemoryPool": "fields: blocks void**, block_size size_t, used size_t, block_count size_t"
  },
  "methods": {
    "pager_open(filename) returns Pager*": "opens database file, initializes pager",
    "pager_get_page(pager, page_num) returns void*": "gets page from cache or loads from file",
    "pager_flush_page(pager, page_num)": "writes dirty page to disk",
    "pager_close(pager)": "closes database file and frees resources",
    "database_open(filename) returns Database*": "opens database, initializes pager and schema",
    "database_close(db)": "closes database and frees resources",
    "database_execute(db, sql) returns int": "Primary public API: parses SQL, creates execution plan, executes it",
    "parse_statement(sql) returns AST*": "parses SQL text into abstract syntax tree",
    "btree_open_cursor(btree, key) returns Cursor*": "opens cursor positioned at key",
    "journal_begin_transaction(journal)": "starts recording page modifications",
    "varint_encode(value, out) returns int": "encodes 64-bit unsigned integer as varint",
    "varint_decode(in, out) returns int": "decodes varint bytes to 64-bit integer",
    "store_big_endian_u32(buf, value)": "stores 32-bit integer in big-endian order",
    "load_big_endian_u32(buf) returns uint32_t": "loads 32-bit integer from big-endian order",
    "next_token(input, pos) returns Token*": "reads next token from SQL input",
    "deserialize_leaf_cell(cell_start, schema, row) returns bool": "converts leaf cell bytes to Row structure",
    "lexer_next_token(lexer) returns Token*": "reads next token from SQL input",
    "get_parser_error() returns const char*": "returns most recent parser error message",
    "get_parser_error_position() returns uint32_t": "returns character offset where error occurred",
    "lexer_create(source) returns Lexer*": "creates lexer for SQL source",
    "lexer_destroy(lexer)": "destroys lexer and frees resources",
    "lexer_get_error(lexer) returns const char*": "returns lexer error message if any",
    "btree_create_table(db, table_name, schema) returns bool": "creates new B-tree for table",
    "cursor_next(cursor) returns bool": "advances cursor to next key-value pair",
    "cursor_get_row(cursor, row) returns bool": "retrieves current row at cursor",
    "btree_insert(btree, key, row) returns bool": "inserts key-value pair into B-tree",
    "btree_delete(btree, key) returns bool": "removes key-value pair from B-tree",
    "btree_update(btree, key, row) returns bool": "updates value for existing key",
    "btree_find(btree, key, row) returns bool": "finds key and retrieves value",
    "page_serialize(page, buffer)": "serializes Page structure to buffer",
    "page_deserialize(page, buffer) returns bool": "deserializes buffer to Page structure",
    "store_big_endian_u16(buf, value)": "stores 16-bit integer in big-endian order",
    "load_big_endian_u16(buf) returns uint16_t": "loads 16-bit integer from big-endian order",
    "execute_select(db, ast) returns ResultSet*": "Executes a SELECT AST, returning a collection of rows",
    "execute_insert(db, ast) returns int64_t": "Executes INSERT, returns number of rows inserted",
    "execute_update(db, ast) returns int64_t": "Executes UPDATE, returns number of rows modified",
    "execute_delete(db, ast) returns int64_t": "Executes DELETE, returns number of rows removed",
    "execute_create_table(db, ast) returns bool": "Executes CREATE TABLE, initializes B-tree and schema catalog",
    "execute_create_index(db, ast) returns bool": "Executes CREATE INDEX, builds secondary index B-tree",
    "explain_query(db, ast) returns char*": "Returns textual description of execution plan for a query",
    "create_expression_context(schema) returns ExpressionContext*": "Creates context for evaluating expressions against rows",
    "destroy_expression_context(context)": "Frees expression evaluation context",
    "evaluate_expression(context, expr, result) returns TriStateBool": "Evaluates expression tree to a column value",
    "evaluate_predicate(context, where_clause) returns TriStateBool": "Evaluates WHERE clause to true/false/null",
    "create_table_scan_operator(db, table_name) returns Operator*": "Creates operator that scans entire table",
    "create_filter_operator(child, predicate, schema) returns Operator*": "Creates operator that filters rows based on predicate",
    "create_select_plan(db, select_ast) returns ExecutionPlan*": "Creates execution plan for SELECT statement",
    "transaction_begin(db) returns bool": "starts a new transaction",
    "transaction_commit(db) returns bool": "commits the current transaction",
    "transaction_rollback(db) returns bool": "rolls back the current transaction",
    "wal_read_page(pager, page_num) returns bool": "(WAL mode only) checks WAL for most recent page version",
    "wal_checkpoint(db, force) returns bool": "(WAL mode only) moves committed pages from WAL to main DB",
    "journal_open(database_filename) returns Journal*": "creates a journal object",
    "journal_begin_transaction(journal) returns bool": "starts a journal transaction",
    "journal_page_before_write(journal, page_num, page_data) returns bool": "saves original page to journal",
    "journal_commit(journal) returns bool": "finalizes and deletes journal",
    "journal_rollback(journal, db_fd) returns bool": "restores pages from journal to DB",
    "journal_close(journal)": "closes and frees journal",
    "set_error(code, component, file, line, fmt, ...) returns ErrorCode": "Sets error in thread-local context with formatted message",
    "db_malloc(size) returns void*": "Allocates memory with error handling",
    "db_free(ptr)": "Frees memory and sets pointer to NULL",
    "db_open_file(filename, flags) returns int": "Opens file with comprehensive error checking",
    "db_write_full(fd, buf, count) returns ssize_t": "Writes all data with retry on interrupt",
    "recover_from_crash(db_filename) returns bool": "Performs crash recovery from journal",
    "compute_page_checksum(page_data, page_size) returns uint32_t": "Computes checksum for corruption detection",
    "lexer_create(source) returns": "creates lexer for SQL source",
    "lexer_next_token(lexer) returns": "reads next token from SQL input",
    "lexer_get_error(lexer) returns": "returns lexer error message if any",
    "database_open(filename) returns": "opens database, initializes pager and schema",
    "database_execute(db, sql) returns": "Primary public API: parses SQL, creates execution plan, executes it",
    "parse_statement(sql) returns": "parses SQL text into abstract syntax tree",
    "get_parser_error() returns": "returns most recent parser error message",
    "get_parser_error_position() returns": "returns character offset where error occurred",
    "pager_open(filename) returns": "opens database file, initializes pager",
    "pager_get_page(pager, page_num) returns": "gets page from cache or loads from file",
    "btree_open_cursor(btree, key) returns": "opens cursor positioned at key",
    "cursor_next(cursor) returns": "advances cursor to next key-value pair",
    "cursor_get_row(cursor, row) returns": "retrieves current row at cursor",
    "btree_create_table(db, table_name, schema) returns": "creates new B-tree for table",
    "btree_insert(btree, key, row) returns": "inserts key-value pair into B-tree",
    "btree_delete(btree, key) returns": "removes key-value pair from B-tree",
    "btree_update(btree, key, row) returns": "updates value for existing key",
    "btree_find(btree, key, row) returns": "finds key and retrieves value",
    "page_deserialize(page, buffer) returns": "deserializes buffer to Page structure",
    "deserialize_leaf_cell(cell_start, schema, row) returns": "converts leaf cell bytes to Row structure",
    "varint_encode(value, out) returns": "encodes 64-bit unsigned integer as varint",
    "varint_decode(in, out) returns": "decodes varint bytes to 64-bit integer",
    "load_big_endian_u16(buf) returns": "loads 16-bit integer from big-endian order",
    "load_big_endian_u32(buf) returns": "loads 32-bit integer from big-endian order",
    "execute_select(db, ast) returns": "Executes a SELECT AST, returning a collection of rows",
    "execute_insert(db, ast) returns": "Executes INSERT, returns number of rows inserted",
    "execute_update(db, ast) returns": "Executes UPDATE, returns number of rows modified",
    "execute_delete(db, ast) returns": "Executes DELETE, returns number of rows removed",
    "execute_create_table(db, ast) returns": "Executes CREATE TABLE, initializes B-tree and schema catalog",
    "execute_create_index(db, ast) returns": "Executes CREATE INDEX, builds secondary index B-tree",
    "explain_query(db, ast) returns": "Returns textual description of execution plan for a query",
    "create_expression_context(schema) returns": "Creates context for evaluating expressions against rows",
    "evaluate_expression(context, expr, result) returns": "Evaluates expression tree to a column value",
    "evaluate_predicate(context, where_clause) returns": "Evaluates WHERE clause to true/false/null",
    "create_table_scan_operator(db, table_name) returns": "Creates operator that scans entire table",
    "create_filter_operator(child, predicate, schema) returns": "Creates operator that filters rows based on predicate",
    "create_select_plan(db, select_ast) returns": "Creates execution plan for SELECT statement",
    "transaction_begin(db) returns": "starts a new transaction",
    "transaction_commit(db) returns": "commits the current transaction",
    "transaction_rollback(db) returns": "rolls back the current transaction",
    "journal_open(database_filename) returns": "creates a journal object",
    "journal_begin_transaction(journal) returns": "starts a journal transaction",
    "journal_page_before_write(journal, page_num, page_data) returns": "saves original page to journal",
    "journal_commit(journal) returns": "finalizes and deletes journal",
    "journal_rollback(journal, db_fd) returns": "restores pages from journal to DB",
    "wal_read_page(pager, page_num) returns": "(WAL mode only) checks WAL for most recent page version",
    "wal_checkpoint(db, force) returns": "(WAL mode only) moves committed pages from WAL to main DB",
    "set_error(code, component, file, line, fmt, ...) returns": "Sets error in thread-local context with formatted message",
    "db_malloc(size) returns": "Allocates memory with error handling",
    "db_open_file(filename, flags) returns": "Opens file with comprehensive error checking",
    "db_write_full(fd, buf, count) returns": "Writes all data with retry on interrupt",
    "recover_from_crash(db_filename) returns": "Performs crash recovery from journal",
    "compute_page_checksum(page_data, page_size) returns": "Computes checksum for corruption detection",
    "btree_validate(btree, page_num, min_key, max_key, ctx) returns": "validates B-tree invariants recursively",
    "trace_init(level, filename)": "initializes debug tracing system",
    "trace_printf(level, func, file, line, fmt, ...)": "formats and outputs trace message",
    "trace_page_dump(label, page_data, size)": "hex dumps page content to trace output",
    "trace_close()": "closes trace file",
    "db_prepare(db, sql) returns": "creates a prepared statement from SQL",
    "db_execute_prepared(stmt, ...) returns": "executes a prepared statement with parameters",
    "db_finalize(stmt)": "releases a prepared statement",
    "db_register_udf(db, udf) returns": "registers a user-defined function",
    "validate_foreign_keys(db, ctx) returns": "validates all foreign key relationships",
    "vfs_get_default() returns": "gets platform-specific default VFS",
    "vfs_create_memory() returns": "creates in-memory VFS for testing",
    "vfs_create_wrapper(base_vfs) returns": "creates VFS wrapper for encryption/compression",
    "pool_alloc(pool, size) returns": "allocates memory from pool",
    "pool_reset(pool)": "resets memory pool for reuse",
    "create_nested_loop_join_operator(left_child, right_child, join_condition, join_type, left_schema, right_schema) returns": "creates nested loop join operator"
  },
  "constants": {
    "PAGE_SIZE": "4096",
    "TABLE_MAX_PAGES": "100",
    "INPUT_BUFFER_SIZE": "1024",
    "MAX_IDENTIFIER_LEN": "255",
    "PAGE_TYPE_INTERNAL": "0x05",
    "PAGE_TYPE_LEAF": "0x0D",
    "PAGE_TYPE_FREELIST": "0x0A",
    "TOKEN_SELECT": "SELECT keyword token",
    "TOKEN_FROM": "FROM keyword token",
    "TOKEN_WHERE": "WHERE keyword token",
    "TOKEN_IDENTIFIER": "identifier token",
    "TOKEN_STRING": "string literal token",
    "TOKEN_NUMBER": "numeric literal token",
    "TOKEN_EQUAL": "= operator token",
    "TOKEN_AND": "AND logical operator",
    "TOKEN_OR": "OR logical operator",
    "TOKEN_EOF": "end of input token",
    "AST_SELECT": "SELECT statement AST node type",
    "AST_COLUMN_REF": "column reference AST node type",
    "AST_BINARY_OP": "binary operator AST node type",
    "OPERATOR_TABLE_SCAN": "Operator type for full table scan",
    "OPERATOR_INDEX_SCAN": "Operator type for index scan",
    "OPERATOR_FILTER": "Operator type for WHERE filtering",
    "OPERATOR_PROJECTION": "Operator type for column selection",
    "OPERATOR_INSERT": "Operator type for INSERT execution",
    "OPERATOR_UPDATE": "Operator type for UPDATE execution",
    "OPERATOR_DELETE": "Operator type for DELETE execution",
    "OPERATOR_ROW_READY": "Operator.next() return value when row available",
    "OPERATOR_EOF": "Operator.next() return value when no more rows",
    "OPERATOR_ERROR": "Operator.next() return value on error",
    "BOOL_FALSE": "Three-valued logic false (0)",
    "BOOL_TRUE": "Three-valued logic true (1)",
    "BOOL_NULL": "Three-valued logic unknown/null (2)",
    "TRANSACTION_IDLE": "no active transaction",
    "TRANSACTION_ACTIVE": "BEGIN issued, writes accumulating",
    "TRANSACTION_COMMITTING": "COMMIT in progress",
    "TRANSACTION_ROLLING_BACK": "ROLLBACK in progress",
    "JOURNAL_MAGIC": "0x4A524E4C ('JRNL')",
    "TOKEN_GREATER": "> operator token",
    "AST_INSERT": "INSERT statement AST node type",
    "AST_LITERAL_STRING": "string literal AST node type",
    "AST_LITERAL_NUMBER": "numeric literal AST node type",
    "ERR_OK": "Success (0)",
    "ERR_SYNTAX": "SQL syntax error",
    "ERR_SEMANTIC": "SQL semantic error",
    "ERR_IO": "I/O error",
    "ERR_NOMEM": "Out of memory",
    "ERR_CORRUPT": "Data corruption detected",
    "ERR_CONSTRAINT": "Constraint violation",
    "ERR_INTERNAL": "Internal implementation error",
    "ERR_BUSY": "Database or resource busy",
    "ERR_LOCKED": "Lock conflict",
    "DEBUG_LEVEL": "environment variable controlling trace verbosity (0-4)",
    "VFS_OPEN_READONLY": "0x0001",
    "VFS_OPEN_READWRITE": "0x0002",
    "VFS_OPEN_CREATE": "0x0004",
    "VFS_OPEN_EXCLUSIVE": "0x0008",
    "VFS_OPEN_DELETEONCLOSE": "0x0010",
    "VFS_OPEN_TEMPORARY": "0x0020",
    "VFS_LOCK_SHARED": "1",
    "VFS_LOCK_RESERVED": "2",
    "VFS_LOCK_PENDING": "3",
    "VFS_LOCK_EXCLUSIVE": "4",
    "TOKEN_JOIN": "JOIN keyword token",
    "TOKEN_ON": "ON keyword token",
    "TOKEN_INNER": "INNER keyword token",
    "TOKEN_OUTER": "OUTER keyword token",
    "AST_SUBQUERY": "subquery AST node type",
    "AST_PARAMETER": "parameter placeholder AST node type",
    "PAGE_TYPE_FULLTEXT": "full-text index page type",
    "INNER_JOIN": "inner join type",
    "LEFT_OUTER_JOIN": "left outer join type",
    "RIGHT_OUTER_JOIN": "right outer join type",
    "FULL_OUTER_JOIN": "full outer join type",
    "OPERATOR_NESTED_LOOP_JOIN": "nested loop join operator type",
    "OPERATOR_HASH_JOIN": "hash join operator type",
    "RESTRICT": "foreign key restrict action",
    "CASCADE": "foreign key cascade action",
    "SET_NULL": "foreign key set null action",
    "SET_DEFAULT": "foreign key set default action",
    "NO_ACTION": "foreign key no action"
  },
  "terms": {
    "REPL": "Read-Eval-Print Loop, interactive shell",
    "B-tree": "balanced tree structure for efficient lookups",
    "ACID": "Atomicity, Consistency, Isolation, Durability",
    "WAL": "Write-Ahead Logging",
    "fsync": "force OS to write buffered data to disk",
    "DML": "Data Manipulation Language (SELECT, INSERT, UPDATE, DELETE)",
    "DDL": "Data Definition Language (CREATE TABLE, CREATE INDEX)",
    "AST": "Abstract Syntax Tree representing parsed SQL structure",
    "MVCC": "Multi-Version Concurrency Control",
    "varint": "variable-length integer encoding for compact storage",
    "cursor": "iterator-like object for traversing B-tree nodes",
    "pager": "component managing page cache and file I/O",
    "cell": "key-value pair stored in B-tree page",
    "Token": "smallest meaningful unit of SQL from lexer",
    "Row": "in-memory representation of a table record",
    "Cursor": "iterator for traversing B-tree",
    "Page": "fixed-size (4096-byte) unit of disk I/O",
    "PageHeader": "metadata at start of each page describing its type and layout",
    "LeafCell": "key-value pair in a B-tree leaf page storing a row",
    "InternalCell": "separator key and child pointer in a B-tree internal page",
    "serialization": "converting in-memory structures to bytes for disk",
    "deserialization": "converting bytes from disk to in-memory structures",
    "endianness": "byte order of multi-byte integers",
    "lexer": "component that breaks SQL text into tokens",
    "tokenization": "process of converting SQL text into tokens",
    "recursive descent parsing": "top-down parsing technique using mutually recursive functions",
    "operator precedence": "rules defining order of operator evaluation",
    "escaped quotes": "SQL convention of doubling quotes within string literals",
    "overflow page": "additional page for storing large rows that don't fit in leaf cell",
    "free block list": "linked list of free space within a page",
    "separator key": "key in internal page that divides child subtrees",
    "binary search": "O(log n) search algorithm within sorted arrays",
    "Query Execution Engine": "Component responsible for executing SQL operations",
    "volcano model": "Iterator model where operators implement next() method",
    "operator": "Processing unit in query execution pipeline",
    "execution plan": "Tree of operators representing query execution strategy",
    "cost-based optimization": "Choosing execution plan based on estimated cost",
    "three-valued logic": "Boolean logic with TRUE, FALSE, and NULL/UNKNOWN values for expression evaluation",
    "index-only scan": "Query execution using only index without table access",
    "predicate pushdown": "Applying WHERE filters as early as possible in pipeline",
    "pipelined execution": "Row-by-row processing without materializing intermediates",
    "rollback journal": "classic journaling method saving original pages before modification",
    "Write-Ahead Logging (WAL)": "journaling method writing changes to log before main DB",
    "atomicity": "property that a transaction is all-or-nothing",
    "durability": "property that committed changes survive crashes",
    "checkpointing": "process of moving changes from WAL to main DB",
    "hot journal": "journal file with commit record left after crash",
    "torn page": "partially written page due to power failure during write operation",
    "write amplification": "Writing data multiple times for durability",
    "write-ahead rule": "Never write changes to main database before logging recovery information",
    "short-circuit evaluation": "Stopping expression evaluation once outcome determined",
    "atomicity protocol": "Two-phase commit process ensuring all-or-nothing semantics",
    "dirty pages": "Pages in cache that differ from disk version",
    "page cache": "In-memory buffer of recently accessed disk pages",
    "concurrent readers": "Multiple queries reading simultaneously during write transaction",
    "forward recovery": "Replaying changes from journal after crash",
    "backward recovery": "Restoring original pages from journal after crash",
    "hot journal recovery": "Automatic recovery from journal file after crash",
    "fail-fast": "Detect errors early to minimize damage",
    "defensive hierarchy": "Layered approach to corruption handling",
    "atomicity guarantee": "All-or-nothing property of transactions",
    "resource cleanup pattern": "Structured approach to freeing resources on error paths",
    "golden-master approach": "Using a reference implementation (SQLite3) to verify correctness",
    "property-based testing": "Testing that certain properties hold for all inputs",
    "fuzz testing": "Feeding random inputs to find crashes",
    "boundary testing": "Testing at the edges of input domains",
    "error injection": "Simulating failures to test error handling",
    "integration point": "Interface between two components that needs testing",
    "end-to-end test": "Test that exercises the entire system from user input to output",
    "test harness": "Framework for running tests and reporting results",
    "mock": "Simulated component used in testing",
    "coverage analysis": "Measuring which code paths are exercised by tests",
    "regression detection": "Catching when new changes break existing functionality",
    "temporary database": "Database file created for a single test and deleted afterwards",
    "concurrent scenario": "Test involving multiple threads or processes",
    "crash simulation": "Artificially terminating a process to test recovery",
    "performance regression": "When new code makes operations slower",
    "memory leak detection": "Finding memory that is allocated but never freed",
    "forensic investigation": "methodical process of examining symptoms to determine root cause",
    "golden-master testing": "comparing output against reference implementation (SQLite3) to verify correctness",
    "consistency checking": "verifying data structure invariants to detect corruption early",
    "JOIN": "combine rows from two or more tables based on related column",
    "prepared statement": "precompiled SQL statement that can be executed multiple times with different parameters",
    "Virtual File System (VFS)": "abstraction layer for file operations, enabling portability and testing",
    "aggregation function": "function that operates on a set of rows to return a single value (COUNT, SUM, AVG, etc.)",
    "GROUP BY": "SQL clause for grouping rows that share common values",
    "subquery": "query nested inside another query",
    "correlated subquery": "subquery that references columns from outer query",
    "full-text search": "search technique for finding documents containing specific words or phrases",
    "inverted index": "data structure mapping content to its locations, used for full-text search",
    "User-Defined Function (UDF)": "function defined by user to extend SQL functionality",
    "foreign key constraint": "constraint ensuring referential integrity between tables",
    "referential integrity": "property ensuring relationships between tables remain consistent",
    "cascade": "foreign key action propagating changes from parent to child rows",
    "materialization": "storing intermediate query results in memory or on disk",
    "decorrelation": "query optimization transforming correlated subquery to join",
    "hash join": "join algorithm using hash tables for efficient equality joins",
    "nested loop join": "join algorithm using nested iteration over both tables"
  }
}