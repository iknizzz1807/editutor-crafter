{
  "types": {
    "TestStatus": "Enum: PENDING, RUNNING, PASSED, FAILED, ERRORED, SKIPPED",
    "TestCase": "fields: nodeid str, func Callable, file_path str, line_no int, fixtures list[str], markers set[str], parameters Dict[str, Any]",
    "TestResult": "fields: test_case TestCase, status TestStatus, message Optional[str], exception Optional[Exception], traceback Optional[str], duration float, attempts List[TestResult]",
    "TestSuite": "fields: name str, tests list[TestCase]",
    "Fixture": "fields: name str, func Callable, scope FixtureScope, dependencies list[str]",
    "Configuration": "fields: file_patterns list[str], start_dir Path, test_name_filters list[str], verbose bool, quiet bool, output_format str, parallel bool, max_workers Optional[int], show_durations bool, junit_xml_path Optional[Path], markers list[str], retries int, pdb bool, coverage bool, update_snapshots bool",
    "FixtureScope": "Enum: FUNCTION, CLASS, MODULE, SESSION",
    "FixtureRequest": "fields: fixture_name str, scope FixtureScope, test_case Optional[TestCase], cache_key tuple",
    "AssertionFailure": "fields: message str, expected Any, actual Any, diff Optional[str], hint Optional[str], assertion_type str",
    "ComparisonContext": "fields: tolerance float, ignore_case bool, ignore_whitespace bool, unordered bool",
    "DiffResult": "fields: summary str, unified_diff Optional[str], context_lines int",
    "ComparisonResult": "Enum: EQUAL, DIFFERENT, TYPE_MISMATCH, APPROXIMATELY_EQUAL",
    "AssertionType": "Enum: EQUALITY, TRUTHINESS, EXCEPTION, COMPARISON, MEMBERSHIP, TYPE_CHECK, COLLECTION, CUSTOM",
    "TypeComparatorRegistry": "fields: _comparators Dict[type, Callable], _default_comparator Callable",
    "FixtureRegistry": "fields: _fixtures Dict[str, Fixture], _discovered bool",
    "FixtureLifecycleManager": "fields: registry FixtureRegistry, _cache Dict[Tuple, Any], _generators Dict[Tuple, Generator], _active_requests set",
    "ModuleStats": "fields: name str, total int, passed int, failed int, errored int, skipped int, time float",
    "TestRunStatistics": "fields: total int, passed int, failed int, errored int, skipped int, total_time float, start_time Optional[datetime], end_time Optional[datetime], by_module Dict[str, ModuleStats], slowest_tests List[Tuple[str, float]]",
    "FormattedError": "fields: type_name str, message str, traceback Optional[str], context Optional[dict]",
    "RunContext": "fields: test_case TestCase, start_time float, end_time Optional[float], error Optional[Exception], assertion_failure Optional[AssertionError], fixtures_loaded List[str]",
    "DebugLogger": "fields: component str, thread_id int",
    "PluginRegistry": "fields: _plugins dict, _event_handlers dict",
    "TestFilter": "fields: patterns list[str], markers list[str]",
    "SnapshotStore": "fields: snapshot_dir Path",
    "CoverageMerger": "fields: coverage_data dict",
    "RetryingRunner": "fields: base_runner SimpleRunner, max_retries int, delay float"
  },
  "methods": {
    "SimpleRunner.run_test(test_case) returns TestResult": "Execute a single test and return its result",
    "SimpleRunner.run_suite(suite) returns list[TestResult]": "Run all tests in a suite and return their results",
    "discover_tests(start_path, pattern) returns List[TestCase]": "Discover all test functions in modules under start_path",
    "resolve_patterns_to_paths(patterns, base_dir) returns List[Path]": "Convert file patterns to concrete file paths",
    "module_path_to_name(file_path, root_dir) returns str": "Convert a filesystem path to a Python importable module name",
    "parse_cli_args(args) returns Configuration": "Parse command-line arguments into a Configuration object",
    "_import_module_from_file(file_path) returns module": "Import a module from a filesystem path",
    "_find_tests_in_module(module) returns Iterator[TestCase]": "Yield TestCase objects for test functions found in a module",
    "assert_equal(actual, expected, msg, **kwargs)": "Assert that actual equals expected with optional tolerance",
    "assert_true(condition, msg)": "Assert that condition is truthy",
    "assert_raises(expected_exception, *args, **kwargs)": "Context manager to assert that code raises an exception",
    "assert_that(actual, matcher, msg)": "Assert that actual value satisfies the matcher",
    "__matches__(actual)": "Check if actual value matches the condition",
    "__describe__()": "Describe what the matcher expects",
    "__describe_mismatch__(actual)": "Describe why actual value didn't match",
    "FixtureRegistry.register(fixture) returns None": "Register a fixture definition",
    "FixtureRegistry.get(name) returns Fixture": "Get a fixture definition by name",
    "FixtureRegistry.scan_module(module) returns None": "Scan a module for fixture decorators",
    "fixture(func, scope, name) returns Callable": "Decorator to mark a function as a fixture",
    "FixtureLifecycleManager.get_fixture_value(fixture_name, test_case) returns Any": "Get the value of a fixture for a given test context",
    "FixtureLifecycleManager.teardown_scope(scope, scope_id) returns None": "Teardown all fixtures of a given scope and scope_id",
    "should_use_color() returns bool": "Determine if we should output color codes",
    "colorize(text, color_code) returns str": "Wrap text in color codes if colors are enabled",
    "StatisticsCollector.add_result(result) returns None": "Add a test result to statistics",
    "StatisticsCollector.finalize() returns None": "Finalize statistics after all results added",
    "StatisticsCollector.get_summary_string() returns str": "Generate a human-readable summary string",
    "ConsoleFormatter.format(stats, results) returns str": "Format complete test run results for console output",
    "JUnitFormatter.format(stats, results) returns str": "Format test results as JUnit XML string",
    "_escape_xml(text) returns str": "Escape text for safe inclusion in XML",
    "_create_cdata(text) returns str": "Wrap text in CDATA section if it contains special characters",
    "capture_exception(exc) returns FormattedError": "Capture an exception into a FormattedError",
    "_worker_function(test_chunk) returns Tuple[List[TestResult], List[dict]]": "Worker process function - runs tests and returns results",
    "debug_log(component, message, data)": "Log debug message if debugging enabled",
    "get_logger(component) returns DebugLogger": "Get a logger instance for a component",
    "inspect_fixture_state(lifecycle_manager) returns Dict[str, Any]": "Return current state of fixture system for debugging",
    "diagnose_test_failure(test_result, runner) returns Dict[str, Any]": "Analyze a test failure and provide diagnostic information",
    "get_execution_context(test_case) returns Dict[str, Any]": "Return the execution context for a test (for debugging)",
    "mark(*marker_names) returns decorator": "Decorator to add markers to a test function or class",
    "get_markers(obj) returns Set[str]": "Retrieve markers from a test function or class",
    "run_test(test_case) returns TestResult": "Execute a single test with retries (RetryingRunner version)",
    "schedule_tests(suite) returns List[List[TestCase]]": "Partition tests into batches that can run in parallel without fixture conflicts",
    "assert_match_snapshot(actual) returns None": "Assert that actual matches stored snapshot",
    "given(*strategies) returns decorator": "Decorator for property-based test parameter generation",
    "post_mortem_debug(exc) returns None": "Enter debugger after exception"
  },
  "constants": {
    "RESET": "ANSI reset code",
    "BOLD": "ANSI bold code",
    "RED": "ANSI red color",
    "GREEN": "ANSI green color",
    "YELLOW": "ANSI yellow color",
    "BLUE": "ANSI blue color",
    "MAGENTA": "ANSI magenta color",
    "CYAN": "ANSI cyan color",
    "WHITE": "ANSI white color",
    "GRAY": "ANSI gray color",
    "PASS_COLOR": "Color for passed tests",
    "FAIL_COLOR": "Color for failed tests",
    "ERROR_COLOR": "Color for errored tests",
    "SKIP_COLOR": "Color for skipped tests",
    "RUNNING_COLOR": "Color for running tests",
    "PASS_ICON": "Icon for passed tests",
    "FAIL_ICON": "Icon for failed tests",
    "ERROR_ICON": "Icon for errored tests",
    "SKIP_ICON": "Icon for skipped tests",
    "DEBUG_ENABLED": "Whether debug logging is enabled via environment variable",
    "DEBUG_LEVEL": "Debug verbosity level (1-3)",
    "DEBUG_COMPONENTS": "Comma-separated list of components to debug",
    "TestStatus.FLAKY": "Enum value for flaky tests (proposed addition)"
  },
  "terms": {
    "Test Conductor": "Mental model: framework as an orchestrator of tests",
    "Discovery": "Process of automatically finding test functions in codebase",
    "Isolation": "Ensuring tests do not interfere with each other's state",
    "Fixture": "Reusable resource or setup/teardown logic for tests",
    "Assertion Engine": "Component that evaluates conditions and produces helpful failure messages",
    "xUnit": "Family of testing frameworks following a class-based pattern",
    "Convention-over-configuration": "Design paradigm using sensible defaults instead of explicit configuration",
    "JUnit XML": "Standard XML format for test results used by CI/CD systems",
    "Pipeline Architecture": "Linear sequence of components where output of one becomes input to the next",
    "God Object": "Anti-pattern where a single class handles too many responsibilities",
    "DNA of the test framework": "Mental model: data structures as fundamental instructions",
    "Traffic light system": "Mental model: TestStatus as indicators of test state",
    "Recipe card": "Mental model: TestCase as instructions for a test",
    "Medical chart": "Mental model: TestResult as record of test execution",
    "Stage crew": "Mental model: fixtures as behind-the-scenes support",
    "Playlist of tests": "Mental model: TestSuite as organized collection",
    "Control panel": "Mental model: Configuration as settings interface",
    "Work order": "Mental model: FixtureRequest as instruction for fixture creation",
    "Matchers API": "Extensible API allowing user-defined assertion predicates with messages",
    "Comparison Inspector": "Mental model: examiner who compares artifacts with detailed reports",
    "Rulebook Builder": "Mental model: creating custom verification rules for domain-specific checks",
    "Evidence Examiner": "Mental model: detailed comparison with side-by-side highlighting",
    "stage crew": "Mental model: fixtures as behind-the-scenes support for tests",
    "Resource Pool Manager": "Mental model: fixture system as manager of scoped resources",
    "The Restaurant Order": "Mental model: test parameters as order items fulfilled by fixture kitchen",
    "scope boundary": "The point when all tests of a certain scope complete, triggering teardown",
    "circular dependency": "When fixture A depends on B and B depends on A (directly or indirectly)",
    "scope leak": "When a test holds reference to a fixture value after its teardown",
    "control panel": "Mental model for CLI as interface for configuring test runs",
    "performance review": "Mental model for Reporter as system that analyzes and reports test outcomes",
    "convention-over-configuration": "Design paradigm using sensible defaults instead of explicit configuration",
    "Strategy pattern": "Design pattern defining a family of algorithms, encapsulating each one, and making them interchangeable",
    "StatisticsCollector": "Component that accumulates and computes statistics from test results",
    "ConsoleFormatter": "Component that formats test results for human-readable console output",
    "JUnitFormatter": "Component that formats test results as JUnit XML",
    "OutputWriter": "Component that handles writing formatted output to stdout or files",
    "progress indicator": "Visual feedback during test execution (dots or letters)",
    "mutually exclusive": "Command-line flags that cannot be used together",
    "XML escaping": "Process of replacing special XML characters with entity references",
    "CDATA section": "XML construct for including text that should not be parsed as XML",
    "exit code": "Process return code indicating success (0) or failure (non-zero)",
    "pattern resolution": "Process of converting file/directory/glob patterns to concrete file paths",
    "resilient conductor": "Mental model: framework as conductor that handles musician errors gracefully",
    "fail-fast vs. fail-safe": "Design tradeoff: stop early on critical errors vs. continue on non-critical errors",
    "error isolation": "Principle: errors in one test should not affect execution of other tests",
    "actionable error messages": "Error messages that suggest fixes or next steps",
    "hierarchical error reporting": "Showing root cause first then supporting details",
    "Self-validation": "Using the framework to test itself",
    "Property-based testing": "Generating random inputs to test invariants",
    "Golden master testing": "Comparing output against known-good references",
    "Cross-validation": "Verifying with multiple independent methods",
    "Normalization": "Replacing variable parts of output with placeholders",
    "Integration tests": "Tests that verify multiple components working together",
    "forensic analysis": "Debugging approach treating symptoms as clues to trace back to root cause",
    "troubleshooting playbook": "Structured guide for diagnosing and fixing common issues",
    "binary search debugging": "Method of isolating bugs by repeatedly testing halfway points in execution flow",
    "minimal reproduction case": "Smallest possible test case that demonstrates a bug",
    "component isolation testing": "Testing components independently by mocking their dependencies",
    "Plugin System": "Modular extension mechanism allowing third-party enhancements",
    "Custom Markers": "Metadata annotations for test categorization and filtering",
    "Parameterized Tests": "Single test definition executed with multiple input sets",
    "Test Doubles": "Mocks, stubs, and fakes replacing real dependencies",
    "Fixture-Aware Parallel Execution": "Parallel scheduling that accounts for shared fixture instances",
    "Snapshot Testing": "Automated comparison of output against stored references",
    "Property-Based Testing": "Testing properties with randomly generated inputs",
    "Code Coverage Integration": "Measuring which code is executed during tests",
    "Test Retries": "Automatic re-execution of failing tests",
    "Interactive Debugger Integration": "Dropping into debugger on test failure"
  }
}