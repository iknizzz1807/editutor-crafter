{
  "types": {
    "NodeId": "str identifier",
    "Term": "int election term",
    "LogIndex": "int log position",
    "LogEntry": "fields: term int, index int, data bytes, timestamp float",
    "NodeState": "enum: FOLLOWER, CANDIDATE, LEADER",
    "RequestVoteRequest": "fields: term int, candidate_id str, last_log_index int, last_log_term int",
    "RequestVoteResponse": "fields: term int, vote_granted bool",
    "AppendEntriesRequest": "fields: term int, leader_id str, prev_log_index int, prev_log_term int, entries List[LogEntry], leader_commit int",
    "AppendEntriesResponse": "fields: term int, success bool, last_log_index int, conflict_index int, conflict_term int",
    "InstallSnapshotRequest": "fields: term int, leader_id str, last_included_index int, last_included_term int, offset int, data bytes, done bool",
    "InstallSnapshotResponse": "fields: term int",
    "PersistentState": "fields: current_term Term, voted_for Optional[NodeId], log List[LogEntry]",
    "VolatileState": "fields: node_state NodeState, commit_index LogIndex, last_applied LogIndex, leader_id Optional[NodeId], election_deadline float",
    "SnapshotMetadata": "fields: last_included_index int, last_included_term int, creation_timestamp float, checksum str, size_bytes int",
    "ClusterConfiguration": "fields: nodes Set[str], version int, timestamp float",
    "JointConfiguration": "fields: old_config ClusterConfiguration, new_config ClusterConfiguration, initiated_at float",
    "ConfigurationState": "enum: STABLE, JOINT, TRANSITIONING",
    "HealthMetrics": "fields: node_id str, last_response_time float, avg_latency_ms float, success_rate float, last_log_index int, responses_received int, last_heartbeat float",
    "TestMetrics": "fields: start_time float, end_time float, operations_attempted int, operations_succeeded int, operations_failed int, election_count int, leader_changes int, average_latency_ms float, p99_latency_ms float, max_latency_ms float, network_messages_sent int, network_bytes_sent int",
    "VirtualNetwork": "fields: partitions Set[frozenset], delays Dict[tuple, float], drop_rates Dict[tuple, float], message_count int, bytes_sent int",
    "ClusterSimulator": "fields: cluster_size int, nodes Dict[str, Mock], virtual_network VirtualNetwork, metrics TestMetrics, current_leader Optional[str], operation_latencies List[float]",
    "PropertyTest": "fields: cluster_size int, simulator ClusterSimulator",
    "ChaosTestSuite": "fields: cluster_size int, simulator ClusterSimulator",
    "PerformanceTestSuite": "fields: results Dict[str, TestMetrics]",
    "LogLevel": "enum: TRACE, DEBUG, INFO, WARN, ERROR",
    "LogContext": "fields: node_id str, term int, component str, timestamp float",
    "ConsensusLogger": "fields: node_id str, logger Logger",
    "StateInspector": "fields: cluster_nodes Dict",
    "PerformanceMetrics": "fields: operation_latencies List[float], rpc_latencies Dict, election_count int, leader_changes int, throughput_ops_per_sec float, memory_usage_mb float",
    "PerformanceProfiler": "fields: node_id str, metrics PerformanceMetrics, operation_start_times Dict",
    "ConsistencyChecker": "fields: cluster_state Dict, violations List",
    "BatchedRequest": "fields: request_id str, data bytes, timestamp float, completion_future asyncio.Future, priority int",
    "BatchConfiguration": "fields: max_batch_size int, max_batch_delay_ms float, enable_adaptive_sizing bool, adaptive_scaling_factor float, priority_flush_threshold int",
    "AdaptiveBatchManager": "fields: config BatchConfiguration, flush_callback Callable, pending_requests deque, batch_timer Optional, lock threading.Lock",
    "ReadIndexState": "enum: PENDING, CONFIRMED, EXPIRED",
    "ReadIndexRequest": "fields: read_index LogIndex, request_time float, confirmation_future asyncio.Future, required_confirmations int, received_confirmations Set",
    "ReadIndexManager": "fields: node_id NodeId, consensus_engine, pending_read_indexes Dict, current_lease_expiry Optional, lease_duration_ms float",
    "TransactionState": "enum: PREPARING, PREPARED, COMMITTING, COMMITTED, ABORTING, ABORTED",
    "ShardOperation": "fields: shard_id int, operation_type str, key str, value Optional, expected_version Optional",
    "CrossShardTransaction": "fields: transaction_id str, operations List, participating_shards Set, coordinator_shard int, state TransactionState",
    "MultiRaftCoordinator": "fields: num_shards int, shard_leaders Dict, raft_groups Dict, active_transactions Dict"
  },
  "methods": {
    "start() -> None": "initialize consensus node",
    "handle_request_vote(request) -> response": "process vote request with split-brain prevention",
    "handle_append_entries(request) -> response": "process log replication from leader",
    "send_request_vote(target, request) -> response": "send vote request to peer",
    "send_append_entries(target, request) -> response": "send log entries to follower",
    "update_term(term, voted_for) -> None": "update persistent term and vote state",
    "to_dict() -> Dict": "serialize entry for persistence",
    "from_dict(data) -> LogEntry": "deserialize entry from storage",
    "save_to_file(filepath) -> None": "persist state to disk with fsync",
    "load_from_file(filepath) -> PersistentState": "load state from disk",
    "start_election() -> None": "transition to CANDIDATE and begin election",
    "become_leader() -> None": "transition to LEADER after winning election",
    "create_snapshot() -> bytes": "serialize state machine state for persistence",
    "restore_from_snapshot(snapshot_data) -> None": "restore state machine from snapshot",
    "save_snapshot(data, metadata) -> None": "atomically persist snapshot with metadata",
    "load_snapshot() -> Tuple": "load existing snapshot with validation",
    "install_snapshot_from_leader(request) -> response": "handle chunked snapshot installation from leader",
    "send_install_snapshot(follower_id, client) -> bool": "send snapshot chunks to follower",
    "start_membership_change(new_nodes) -> bool": "initiate membership change to new configuration",
    "complete_membership_change() -> bool": "finish transition to new configuration",
    "rollback_membership_change() -> bool": "revert to original configuration",
    "check_dual_majority(responding_nodes) -> bool": "verify dual majority during joint consensus",
    "is_node_ready_for_promotion(node_id, leader_log_index) -> bool": "check if new node ready for voting",
    "record_rpc_response(node_id, success, latency_ms, log_index) -> None": "update health metrics for node",
    "get_disruptive_nodes() -> List[str]": "identify nodes harming cluster performance",
    "send_rpc(target, message_type, data) -> response": "send RPC to target node",
    "process_client_request(data) -> result": "handle client operation",
    "handle_leader_failure() -> None": "coordinate leader failure response",
    "handle_network_partition(nodes) -> None": "manage network partition",
    "detect_partition() -> bool": "check for network partition",
    "record_rpc_response(node_id, success, latency, index) -> None": "update node health metrics",
    "record_rpc_response(node_id, success, latency_ms, log_index)": "update health metrics for node",
    "validate_cluster_consistency() -> List[str]": "check consensus invariants across cluster",
    "success_rate() -> float": "calculate operation success rate",
    "throughput_ops_per_sec() -> float": "calculate sustained throughput",
    "create_partition(group_a, group_b) -> None": "create network partition between two groups",
    "heal_partition() -> None": "remove all network partitions",
    "set_delay(src, dst, delay_ms) -> None": "set network delay between two nodes",
    "set_drop_rate(src, dst, drop_rate) -> None": "set packet drop rate between two nodes",
    "can_deliver(src, dst) -> bool": "check if message can be delivered given current partitions",
    "should_drop(src, dst) -> bool": "determine if message should be dropped",
    "get_delay(src, dst) -> float": "get network delay between nodes",
    "send_message(src, dst, message) -> bool": "simulate sending message with network conditions",
    "create_cluster() -> None": "initialize cluster with specified number of nodes",
    "start_cluster() -> None": "start all nodes and wait for leader election",
    "kill_node(node_id) -> None": "simulate node crash/failure",
    "restart_node(node_id) -> None": "restart a previously killed node",
    "submit_request(data) -> bool": "submit client request to current leader",
    "verify_consistency() -> List[str]": "verify all nodes have consistent state",
    "finalize_metrics() -> TestMetrics": "calculate final test metrics",
    "test_election_safety(num_iterations) -> None": "test that at most one leader exists per term",
    "verify_election_safety() -> List[str]": "verify election safety property",
    "test_log_matching_property(num_iterations) -> None": "test that log matching property holds under all conditions",
    "verify_log_matching() -> List[str]": "verify log matching property across all nodes",
    "test_leader_isolation() -> TestMetrics": "test leader isolation and recovery",
    "_generate_load(ops_per_second, duration_seconds) -> None": "generate steady background load",
    "test_sustained_throughput(cluster_size) -> TestMetrics": "test maximum sustained throughput",
    "_run_sustained_load(simulator, ops_per_second, duration_seconds) -> None": "run sustained load for specified duration",
    "verify_election_safety_invariant(cluster_state)": "verify election safety: at most one leader per term",
    "verify_log_consistency_invariant(cluster_state)": "verify log matching property across all nodes",
    "test_chaos_leader_isolation()": "test leader isolation scenario with background load",
    "test_performance_sustained_throughput()": "measure maximum sustained throughput under realistic conditions",
    "get_logger(node_id) -> ConsensusLogger": "get or create logger for specific node",
    "log_election_event(event_type, term, **kwargs) -> None": "log election events with consistent terminology",
    "log_replication_event(event_type, term, **kwargs) -> None": "log replication events with consistent terminology",
    "log_rpc_event(rpc_type, target, success, latency_ms, **kwargs) -> None": "log RPC operations with timing",
    "dump_cluster_state() -> Dict": "create comprehensive cluster state dump",
    "analyze_election_state() -> Dict": "analyze current election state and issues",
    "check_replication_health() -> Dict": "analyze log replication health",
    "create_state_snapshot(node) -> Dict": "create detailed single node state snapshot",
    "add_operation_latency(latency_ms) -> None": "record client operation latency",
    "add_rpc_latency(rpc_type, latency_ms) -> None": "record RPC latency by type",
    "get_latency_percentiles() -> Dict": "calculate latency distribution",
    "calculate_throughput(window_seconds) -> float": "calculate ops per second",
    "start_operation(operation_id) -> None": "start timing consensus operation",
    "end_operation(operation_id) -> float": "end timing and record latency",
    "profile_rpc(rpc_type, target_node, rpc_func, *args, **kwargs)": "profile RPC call with timing",
    "generate_report() -> Dict": "generate performance report",
    "check_election_safety() -> List[str]": "verify at most one leader per term",
    "check_log_matching() -> List[str]": "verify log matching property",
    "check_commit_safety() -> List[str]": "verify committed entries never change",
    "check_term_monotonicity() -> List[str]": "verify terms never decrease",
    "run_all_checks() -> List[str]": "run comprehensive consistency validation",
    "add_request(request_id, data, priority) -> asyncio.Future": "add request to batch with completion tracking",
    "request_consistent_read() -> LogIndex": "request read index for consistent follower reads",
    "handle_read_index_request(follower_id) -> LogIndex": "handle ReadIndex request from follower",
    "update_lease(heartbeat_responses) -> None": "update leader lease based on heartbeat responses",
    "has_valid_lease() -> bool": "check if current leader lease is valid",
    "get_shard_for_key(key) -> int": "determine which shard handles the given key",
    "execute_single_shard_operation(key, operation, value) -> Any": "execute operation on single shard",
    "execute_cross_shard_transaction(operations) -> bool": "execute multi-shard transaction using two-phase commit",
    "add_shard(shard_id, raft_node) -> None": "add new shard to multi-raft cluster",
    "remove_shard(shard_id) -> None": "remove shard from multi-raft cluster",
    "validate_code_terminology(file_content) -> List[str]": "check code for consistent terminology usage",
    "validate_documentation_terminology(doc_content) -> List[str]": "check documentation for consistent terminology usage"
  },
  "constants": {
    "ELECTION_TIMEOUT_MIN": "150ms minimum election timeout",
    "ELECTION_TIMEOUT_MAX": "300ms maximum election timeout",
    "HEARTBEAT_INTERVAL": "50ms leader heartbeat interval",
    "MAX_CLUSTER_SIZE": "9 nodes maximum",
    "SNAPSHOT_THRESHOLD": "10000 entries trigger snapshot",
    "CHUNK_SIZE": "64KB for chunked transfer",
    "JOINT_CONSENSUS_TIMEOUT": "60 seconds maximum time in joint consensus",
    "CATCHUP_TIMEOUT": "30 seconds for new node preparation"
  },
  "terms": {
    "consensus": "agreement among distributed nodes",
    "quorum": "majority of nodes for decisions",
    "split vote": "election with no majority winner",
    "log matching property": "identical log prefixes guarantee",
    "joint consensus": "two-phase membership change protocol",
    "partition tolerance": "operation during network splits",
    "state machine replication": "identical state across nodes",
    "linearizability": "atomic operation appearance",
    "persistent state": "crash-surviving data",
    "volatile state": "reconstructible data",
    "snapshot": "point-in-time state capture",
    "log compaction": "removing old entries using snapshots",
    "chunked transfer": "sending large data in small pieces",
    "state machine freeze": "pausing modifications during snapshot",
    "log truncation": "removing log entries covered by snapshots",
    "disruptive server": "node harming cluster performance",
    "dual majority": "majorities required from both configurations",
    "configuration change": "process of adding or removing nodes from cluster membership",
    "catch-up process": "bringing new nodes to current state",
    "split-brain prevention": "single leader guarantee",
    "heartbeat": "leadership assertion message",
    "election timeout": "period before starting election",
    "failure detection": "identifying node/network failures",
    "cascading failure": "single failure triggering multiple failures",
    "adaptive timeout": "dynamic timeout adjustment",
    "thundering herd": "simultaneous election problem",
    "property-based testing": "testing invariants rather than scenarios",
    "chaos testing": "simulating failures for robustness",
    "performance testing": "measuring throughput, latency, and behavior under load",
    "milestone checkpoints": "verification criteria and expected behavior after each milestone implementation",
    "generative testing": "framework automatically creates thousands of different scenarios",
    "shrinking": "automatically reducing failing test case to minimal example",
    "coordination overhead": "cost of achieving distributed agreement",
    "emergent properties": "behaviors appearing only when nodes interact",
    "correlated failures": "multiple failures caused by same systemic issue",
    "split brain": "multiple nodes believing they are leader",
    "deterministic replay": "reproducing exact distributed scenarios",
    "consistency checking": "validating distributed invariants",
    "performance profiling": "measuring timing and resource usage",
    "structured logging": "consistent format across components",
    "term confusion": "inconsistent views of current term",
    "log inconsistency": "different entries at same index",
    "snapshot corruption": "invalid or incomplete snapshot data",
    "batching": "collecting requests to reduce overhead",
    "pipelining": "sending multiple requests without waiting for responses",
    "ReadIndex": "protocol for consistent follower reads",
    "lease-based reads": "local leader reads during guaranteed leadership periods",
    "multi-Raft": "multiple consensus groups for scaling",
    "cross-shard coordination": "managing operations spanning multiple Raft groups",
    "adaptive batching": "dynamic batch size adjustment based on load",
    "session-based deduplication": "preventing duplicate operations using client sessions",
    "split-brain": "multiple nodes believing they are leader",
    "term": "logical clock value for leadership epochs"
  }
}