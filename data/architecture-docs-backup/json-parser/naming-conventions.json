{
  "types": {
    "TokenType": "enum with STRING, NUMBER, BOOLEAN, NULL, LEFT_BRACE, RIGHT_BRACE, LEFT_BRACKET, RIGHT_BRACKET, COLON, COMMA, EOF",
    "Position": "fields: line int, column int, offset int",
    "Token": "fields: type TokenType, value Any, position Position, raw str",
    "JSONParseError": "base exception with message and position",
    "TokenizationError": "lexical analysis error",
    "ParseError": "syntax analysis error",
    "JSONTokenizer": "lexical analyzer class",
    "JSONParser": "recursive descent parser class",
    "ParseContext": "enum with DOCUMENT_ROOT, OBJECT_START, OBJECT_KEY, OBJECT_VALUE, OBJECT_SEPARATOR, ARRAY_START, ARRAY_VALUE",
    "ParserState": "fields: tokens, current_position, context_stack, max_nesting_depth, nesting_depth",
    "SchemaNode": "represents JSON schema definition for validation",
    "ValidationError": "schema validation failure information"
  },
  "methods": {
    "tokenize() -> list[Token]": "convert text to token sequence",
    "next_token() -> Token": "get next token from input",
    "parse() -> Any": "parse tokens to native data structures",
    "parse_value() -> Any": "parse any JSON value",
    "parse_object() -> dict": "parse JSON object",
    "parse_array() -> list": "parse JSON array",
    "current_token() -> Token": "get current token without advancing",
    "peek_token(offset=0) -> Token": "look ahead at token with offset",
    "consume_token() -> Token": "consume current token and advance",
    "expect_token(TokenType) -> Token": "consumes and validates expected token type",
    "push_context(ParseContext)": "enter new parsing context",
    "pop_context() -> ParseContext": "exits current parsing context",
    "current_context() -> ParseContext": "get current context without changing state",
    "validate_token_for_context(Token) -> bool": "check if token valid for context",
    "is_at_end() -> bool": "check if all tokens consumed",
    "current_char() -> str": "get current character without advancing",
    "peek_char(offset=1) -> str": "look ahead at character",
    "advance() -> str": "move to next character and update position",
    "format_message() -> str": "formats comprehensive error message with position and context",
    "advance(char) -> None": "updates position tracking for single character",
    "current_position() -> Position": "returns current position as immutable object",
    "process_unicode_escape(text, pos, tracker) -> tuple[str, int]": "processes Unicode escape sequences including surrogate pairs",
    "validate_escape_sequence(char, pos) -> str": "validates and converts basic escape sequences",
    "push_context(context, description)": "enters new parsing context with description",
    "current_context_description() -> str": "generates human-readable context description",
    "validate_number_format(str, pos)": "validates number string against JSON specification",
    "detect_trailing_comma(context, token)": "detects and reports trailing comma violations",
    "feed_characters(chunk)": "processes incremental input for streaming",
    "array_start()": "streaming event for array beginning",
    "array_element(value)": "streaming event for array element",
    "object_start()": "streaming event for object beginning",
    "object_key_value(key, value)": "streaming event for key-value pair"
  },
  "constants": {
    "EOF": "end of file token type",
    "VALUE_TOKENS": "set of tokens representing JSON values",
    "STRUCTURAL_TOKENS": "set of tokens for JSON structure",
    "SEPARATOR_TOKENS": "set of delimiter tokens",
    "VALID_TRANSITIONS": "mapping of contexts to valid token types"
  },
  "terms": {
    "tokenization": "lexical analysis phase converting text to tokens",
    "recursive descent": "top-down parsing technique following grammar rules",
    "grammar rule": "formal specification of language syntax",
    "token stream": "sequence of tokens produced by tokenizer",
    "two-phase parsing": "separation of lexical and syntactic analysis",
    "context-free grammar": "grammar type that can handle nested structures",
    "predictive parsing": "parsing without backtracking using lookahead",
    "assembly line processing": "mental model for sequential processing phases",
    "fail-fast error strategy": "stop immediately on first error with detailed diagnostics",
    "single responsibility principle": "each component has one well-defined purpose",
    "character-by-character scanning": "parsing approach that processes one character per iteration",
    "escape sequence": "backslash-prefixed character combinations representing special characters",
    "Unicode escape": "\\uXXXX format for representing Unicode characters",
    "surrogate pair": "two consecutive Unicode escapes for characters outside Basic Multilingual Plane",
    "state machine": "finite state automaton for parsing different token types",
    "position tracking": "maintaining line, column, and offset information during parsing",
    "grammar navigator": "mental model for following grammar rules through token sequences",
    "specification compliance": "adherence to official JSON specification rules",
    "trailing comma": "comma after last element in object or array",
    "context stack": "stack tracking current parsing state for error reporting",
    "token consumption pattern": "systematic method for parser to request and process tokens",
    "error information flow": "how parsing errors capture and preserve context",
    "demand-driven": "parser controls when tokens are requested from tokenizer",
    "token window": "current token plus lookahead capability",
    "error propagation": "how errors bubble up through recursive parsing calls",
    "context enrichment": "adding parsing state information to error messages",
    "milestone verification": "checkpoint testing after each development phase",
    "quality gates": "progressive testing checkpoints ensuring solid foundations",
    "edge case validation": "testing boundary conditions and unusual scenarios",
    "streaming parser": "parser that processes input incrementally without loading complete document",
    "schema validation": "validating JSON documents against JSON Schema specifications",
    "JSONPath": "query language for extracting data from JSON documents",
    "selective parsing": "parsing only document sections that match query criteria",
    "error recovery": "continuing parsing after encountering syntax errors",
    "schema evolution": "supporting multiple versions of JSON schemas simultaneously",
    "transformation pipeline": "modifying JSON documents during parsing",
    "performance optimization": "improving parser speed and memory usage",
    "zero-copy string handling": "avoiding string allocation by using references to original input"
  }
}