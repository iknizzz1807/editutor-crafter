{
  "types": {
    "FeatureVector": "1D numpy array of float64 features",
    "FeatureMatrix": "2D numpy array shape (n_samples, n_features)",
    "ClassLabel": "Union[int, str] for class identifiers",
    "DistanceArray": "1D numpy array of float64 distances",
    "NeighborIndices": "1D numpy array of int32 indices",
    "TrainingData": "features FeatureMatrix, labels List[ClassLabel], n_samples int, n_features int",
    "PredictionResult": "predicted_class ClassLabel, neighbor_indices NeighborIndices, neighbor_distances DistanceArray, confidence float, voting_details Dict",
    "DistanceMetric": "Enum with EUCLIDEAN, MANHATTAN, COSINE",
    "KNNClassifier": "k int, distance_metric DistanceMetric, weighted_voting bool",
    "VotingStrategy": "Enum with MAJORITY, WEIGHTED",
    "CrossValidationResult": "Dict with mean/std metrics and fold results",
    "GridSearchResult": "optimal_k int, optimal_metrics Dict, complete_results Dict",
    "ConfusionMatrix": "Dict with matrix and labels",
    "ClassificationMetrics": "Dict with precision, recall, F1-score per class",
    "FoldIndices": "Tuple of train_indices and val_indices arrays",
    "ReproducibleRandom": "Seed management class",
    "StratifiedSplitter": "Class for stratified data splitting",
    "IndexType": "Enum with LINEAR, KDTREE, BALLTREE, LSH, AUTO",
    "SpatialIndex": "Abstract interface for spatial indexing structures",
    "CustomDistanceMetric": "Abstract base for user-defined distance functions",
    "AggregationStrategy": "Interface for prediction aggregation methods"
  },
  "methods": {
    "get_sample(index: int) -> Tuple[FeatureVector, ClassLabel]": "retrieve single training example by index",
    "fit(X: FeatureMatrix, y: List[ClassLabel]) -> None": "store training data for lazy learning",
    "predict(X: FeatureMatrix) -> List[ClassLabel]": "predict class labels for query points",
    "predict_with_confidence(X: FeatureMatrix) -> List[PredictionResult]": "predict with neighbor and confidence information",
    "load_iris_dataset() -> Tuple[np.ndarray, np.ndarray]": "load classic Iris classification dataset",
    "split_and_scale_data(X, y, test_size, scale_features, random_state) -> Tuple[4x np.ndarray]": "split data and optionally apply feature scaling",
    "calculate_distance(point1, point2, metric) -> float": "compute distance between two feature vectors",
    "calculate_distances_to_point(query_point, training_matrix, metric) -> DistanceArray": "vectorized distances from query to all training samples",
    "validate_compatible_vectors(v1, v2) -> None": "ensure vectors have matching dimensions",
    "euclidean_distance(point1, point2) -> float": "L2 norm distance calculation",
    "manhattan_distance(point1, point2) -> float": "L1 norm distance calculation",
    "cosine_distance(point1, point2) -> float": "angular distance calculation",
    "get_sample(index) -> Tuple[FeatureVector, ClassLabel]": "retrieve single training example by index",
    "find_k_neighbors(query_point: FeatureVector, k: int, distance_metric: DistanceMetric) -> Tuple[NeighborIndices, DistanceArray]": "Find K nearest neighbors to query point using specified distance metric",
    "find_neighbors_within_radius(query_point: FeatureVector, radius: float, distance_metric: DistanceMetric) -> Tuple[NeighborIndices, DistanceArray]": "Find all neighbors within specified distance radius",
    "find_neighbors_batch(query_points: FeatureMatrix, k: int, distance_metric: DistanceMetric) -> List[Tuple[NeighborIndices, DistanceArray]]": "Find K neighbors for multiple query points efficiently",
    "validate_k_parameter(k: int, dataset_size: int) -> None": "Validate K value against dataset constraints and raise appropriate errors",
    "handle_distance_ties(distances: DistanceArray, indices: NeighborIndices, k: int) -> Tuple[NeighborIndices, DistanceArray]": "Resolve ties when multiple neighbors have identical distances",
    "set_voting_strategy(weighted: bool) -> None": "configure voting strategy",
    "set_k_parameter(k: int) -> None": "update number of neighbors",
    "majority_vote(neighbor_labels, k) -> Tuple[str, float]": "simple majority voting implementation",
    "weighted_vote(neighbor_labels, neighbor_distances, k, epsilon) -> Tuple[str, float]": "distance-weighted voting implementation",
    "resolve_ties(tied_classes, neighbor_labels, neighbor_distances) -> str": "tie-breaking using nearest neighbor",
    "_predict_single_point(query_point) -> PredictionResult": "core prediction logic for single point",
    "_find_neighbors(query_point) -> Tuple[NeighborIndices, DistanceArray]": "find K nearest neighbors",
    "_vote(neighbor_labels, neighbor_distances) -> Tuple[ClassLabel, float]": "apply voting strategy",
    "k_fold_cross_validate(X, y, k_folds, k_neighbors, distance_metric, random_seed) -> CrossValidationResult": "performs K-fold cross-validation",
    "grid_search_k(X, y, k_values, cv_folds, distance_metric, random_seed) -> GridSearchResult": "finds optimal K through exhaustive search",
    "calculate_confusion_matrix(y_true, y_pred, class_labels) -> ConfusionMatrix": "computes classification confusion matrix",
    "calculate_classification_metrics(y_true, y_pred, average) -> ClassificationMetrics": "calculates precision, recall, F1",
    "split_stratified_folds(X, y, n_folds, random_seed) -> List[FoldIndices]": "creates stratified train/validation splits",
    "validate_evaluation_parameters(X, y, k_values, cv_folds) -> None": "validates parameters for evaluation pipeline",
    "validate_k_parameter(k, dataset_size) -> None": "validate K value against dataset constraints",
    "validate_training_data(features, labels) -> None": "comprehensive training data validation",
    "validate_query_points(queries, n_features) -> None": "validate query point compatibility",
    "safe_sqrt(values) -> ndarray": "safely compute square root handling negatives",
    "safe_divide(num, den, default) -> ndarray": "safely divide arrays handling division by zero",
    "check_numerical_stability(array, name) -> None": "check array for NaN/inf values",
    "handle_dataset_edge_cases(data, k) -> Tuple": "detect and handle dataset edge cases",
    "handle_prediction_edge_cases(labels, distances) -> Tuple": "handle voting edge cases",
    "detect_data_quality_issues(features, labels) -> List": "detect potential data quality problems",
    "find_k_neighbors(query_point, k, distance_metric) -> Tuple[NeighborIndices, DistanceArray]": "Find K nearest neighbors to query point",
    "fit(X, y) -> None": "build spatial index or train custom metric",
    "predict(X) -> List[ClassLabel]": "predict class labels for query points",
    "predict_with_confidence(X) -> List[PredictionResult]": "predict with neighbor and confidence information",
    "validate_numerical_stability(X, y) -> Dict": "comprehensive numerical validation of KNN components",
    "diagnose_distance_calculation(sample_points) -> Dict": "diagnose distance calculation issues with test points",
    "analyze_classification_bias(X_test, y_test) -> Dict": "analyze classification for systematic biases",
    "profile_performance(X_test, iterations) -> Dict": "profile performance of each KNN component",
    "test_distance_properties(distance_func, test_points) -> bool": "test mathematical properties of distance function",
    "benchmark_distance_scaling(distance_func, max_dimension) -> Dict": "benchmark how distance calculation scales with dimensionality",
    "k_sensitivity_analysis(X_train, y_train, X_test, y_test) -> Dict": "analyze how performance varies with K parameter",
    "analyze_prediction_confidence(X_test, y_test) -> Dict": "analyze relationship between confidence scores and accuracy",
    "profile_scaling_behavior(classifier, dataset_sizes) -> Dict": "profile how performance scales with dataset size",
    "identify_bottlenecks(classifier, X_test) -> Dict": "identify which component is the performance bottleneck",
    "safe_distance_calculation(point1, point2, metric, epsilon) -> float": "distance calculation with comprehensive error checking",
    "diagnose_voting_issues(neighbor_labels, neighbor_distances, prediction, confidence) -> Dict": "diagnose potential issues in voting logic",
    "query(query_point, k) -> Tuple[NeighborIndices, DistanceArray]": "find K nearest neighbors using spatial index",
    "calculate(point1, point2) -> float": "compute custom distance between feature vectors",
    "batch_calculate(query_point, training_matrix) -> DistanceArray": "vectorized distance computation",
    "register_metric(metric) -> None": "add custom metric to registry",
    "create_index(index_type, distance_metric) -> SpatialIndex": "factory for spatial index creation",
    "get_memory_usage() -> int": "return index memory usage in bytes",
    "supports_metric(metric) -> bool": "check metric compatibility with index"
  },
  "constants": {
    "EUCLIDEAN": "euclidean distance metric",
    "MANHATTAN": "manhattan distance metric",
    "COSINE": "cosine similarity metric",
    "MAJORITY": "majority voting strategy",
    "WEIGHTED": "weighted voting strategy",
    "EPSILON": "1e-10 for numerical stability",
    "DISTANCE_TOLERANCE": "1e-8 for distance comparisons",
    "LINEAR": "linear search index type",
    "KDTREE": "KD-tree spatial index",
    "AUTO": "automatic index selection"
  },
  "terms": {
    "lazy learning": "algorithms that defer computation until prediction time",
    "instance-based learning": "learning approach using similarity to stored instances",
    "smoothness assumption": "assumption that similar inputs produce similar outputs",
    "curse of dimensionality": "distance becomes less meaningful in high dimensions",
    "majority voting": "classification by most frequent class among neighbors",
    "weighted voting": "voting where closer neighbors have more influence",
    "bias-variance tradeoff": "small K high variance low bias, large K low variance high bias",
    "vectorized operations": "NumPy operations on entire arrays without Python loops",
    "L2 norm": "Euclidean distance calculation",
    "L1 norm": "Manhattan distance calculation",
    "broadcasting": "NumPy automatic array shape matching for element-wise operations",
    "democratic decision making": "aggregating multiple opinions through voting processes",
    "tie-breaking": "resolving situations where multiple classes receive equal votes",
    "confidence scoring": "quantifying prediction certainty based on neighbor agreement",
    "cross-validation": "statistical method for reliable performance estimation using multiple train/test splits",
    "stratified sampling": "sampling that preserves class distribution across data splits",
    "hyperparameter optimization": "systematic search for optimal model configuration parameters",
    "grid search": "exhaustive evaluation of all hyperparameter combinations",
    "statistical significance": "mathematical confidence that performance differences are not due to random chance",
    "multiple testing correction": "statistical adjustment for increased false positive risk when testing many hypotheses",
    "nested cross-validation": "two-level CV where inner loop selects hyperparameters and outer loop estimates performance",
    "macro averaging": "computing metrics by averaging across classes with equal weight regardless of class frequency",
    "weighted averaging": "computing metrics by averaging across classes weighted by class support",
    "confidence interval": "statistical range indicating uncertainty in performance estimates",
    "information leakage": "contamination where training information influences validation",
    "numerical stability": "robustness of calculations against floating-point errors",
    "edge case handling": "managing boundary conditions and unusual scenarios",
    "graceful degradation": "maintaining partial functionality when full operation impossible",
    "input validation": "verifying data meets requirements before processing",
    "floating-point precision": "limitations of computer arithmetic affecting calculations",
    "spatial indexing": "data structures that accelerate neighbor finding by organizing feature space",
    "locality-sensitive hashing": "approximate nearest neighbors using similarity-preserving hash functions",
    "metric learning": "optimizing distance functions based on training data",
    "ensemble methods": "combining multiple models for improved predictions",
    "online learning": "adapting models to new data without complete retraining",
    "concept drift": "changes in underlying data distribution over time",
    "streaming KNN": "handling continuous data streams with memory constraints",
    "progressive enhancement": "adding advanced features while maintaining backward compatibility"
  }
}