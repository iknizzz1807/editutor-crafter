{
  "types": {
    "LispValue": "value: Any, type: LispValueType",
    "LispValueType": "enum: NUMBER, SYMBOL, LIST, FUNCTION, BUILTIN",
    "LispError": "message: str, source_location: Optional[SourceLocation]",
    "TokenizerError": "inherits from LispError",
    "ParseError": "inherits from LispError",
    "EvaluationError": "message: str, source_location: Optional[int]",
    "NameError": "inherits from EvaluationError",
    "TypeError": "inherits from EvaluationError",
    "ArityError": "expected: int, actual: int, function_name: str",
    "Token": "type: str, value: str, position: int",
    "Environment": "bindings: Dict[str, LispValue], parent: Optional[Environment]",
    "LispFunction": "parameters: List[str], body: LispValue, closure_env: Environment, name: Optional[str]",
    "BuiltinFunction": "implementation: Callable, name: str, arity: Optional[int]",
    "TokenType": "enum: LEFT_PAREN, RIGHT_PAREN, NUMBER, SYMBOL, STRING, QUOTE, COMMENT, WHITESPACE, EOF",
    "Scanner": "text: str, position: int, tokens: List[Token]",
    "Parser": "max_nesting_depth: int",
    "ScopeTracker": "scope_stack: list, lookup_history: list",
    "TailCall": "fields: function LispValue, arguments List[LispValue], environment Environment",
    "EvaluationResult": "value: Optional[Any], error: Optional[LispError], expression_text: str",
    "InterpreterSession": "global_env: Environment, expression_count: int",
    "PipelineCoordinator": "tokenizer, parser, evaluator components",
    "StateManager": "global_environment: Environment, evaluation_count: int",
    "SourceLocation": "line: int, column: int, position: int, length: int",
    "MilestoneValidator": "testing helper class",
    "TraceEvent": "event_type: str, expression: Any, environment_id: int, depth: int, result: Optional[Any], error: Optional[str], timestamp: float",
    "EvaluationTracer": "events: List[TraceEvent], current_depth: int, output_file, max_depth, enabled: bool",
    "EnvironmentInspector": "environment: Environment, inspection_cache: Dict[int, Dict]",
    "ASTVisualizer": "max_depth: int, node_counter: int",
    "REPLDebugger": "evaluator: Evaluator, tracer: EvaluationTracer, commands: Dict[str, Callable]",
    "MacroExpander": "fields: macro_environment Environment, expansion_depth int, max_expansion_depth int",
    "OpCode": "enum: PUSH_NUMBER, PUSH_SYMBOL, LOAD_GLOBAL, STORE_GLOBAL, LOAD_LOCAL, STORE_LOCAL, CALL, RETURN, JUMP, JUMP_IF_FALSE, ADD, SUBTRACT, MULTIPLY, DIVIDE",
    "Instruction": "fields: opcode OpCode, operand Any",
    "VirtualMachine": "fields: stack list, call_stack list, globals dict, instruction_pointer int, instructions list",
    "ProfileData": "fields: function_name str, call_count int, total_time float, self_time float, memory_allocated int",
    "Profiler": "fields: enabled bool, profile_data Dict[str, ProfileData], call_stack List[tuple], total_allocations int"
  },
  "methods": {
    "make_number(value)": "creates numeric LispValue",
    "make_symbol(name)": "creates symbol LispValue",
    "make_list(elements)": "creates list LispValue",
    "is_number(value)": "type predicate for numbers",
    "is_symbol(value)": "type predicate for symbols",
    "is_list(value)": "type predicate for lists",
    "is_truthy()": "Lisp truthiness evaluation",
    "tokenize(text)": "converts text to token stream",
    "parse(tokens)": "builds AST from tokens",
    "evaluate(ast, env)": "main evaluation dispatch function",
    "create_global_environment()": "creates environment with built-ins",
    "is_truthy(value)": "determines Lisp truthiness",
    "read_expr(tokens, pos)": "reads single expression from token stream",
    "read_list(tokens, pos)": "reads parenthesized list from tokens",
    "is_special_form(symbol_name)": "checks if symbol names special form",
    "apply_function(func, args, env)": "applies function to arguments",
    "lookup(name)": "searches environment chain for variable",
    "define(name, value)": "creates binding in current environment",
    "extend()": "creates child environment",
    "make_function(parameters, body, closure_env, name)": "creates user-defined function",
    "make_builtin(implementation, name, arity)": "creates built-in function",
    "scan_all()": "main tokenization loop",
    "current_char()": "return current character or None",
    "advance()": "move to next position",
    "skip_whitespace()": "skip whitespace at current position",
    "scan_string()": "scan string literal with escapes",
    "scan_number()": "scan numeric literal",
    "scan_symbol()": "scan symbol/identifier",
    "scan_comment()": "skip comment to end of line",
    "make_single_char_token(type)": "create single-character token",
    "is_whitespace(char)": "check if character is whitespace",
    "is_digit(char)": "check if character is digit",
    "is_symbol_char(char)": "check if valid symbol character",
    "read_expr(tokens, position, depth) returns": "(LispValue, int) - parse single expression",
    "read_list(tokens, position, depth) returns": "(LispValue, int) - parse parenthesized list",
    "read_quote_expr(tokens, position, depth) returns": "(LispValue, int) - handle quote transformation",
    "parse(tokens) returns": "LispValue - builds AST from tokens",
    "is_at_end(tokens, position) returns": "bool - check if at token stream end",
    "current_token(tokens, position) returns": "Token - get current token safely",
    "make_symbol(name) returns": "LispValue - creates symbol LispValue",
    "make_number(value) returns": "LispValue - creates numeric LispValue",
    "make_list(elements) returns": "LispValue - creates list LispValue",
    "handle_if(args, env)": "processes if special form",
    "handle_define(args, env)": "processes define special form",
    "handle_lambda(args, env)": "processes lambda special form",
    "builtin_add(args)": "addition operator implementation",
    "builtin_subtract(args)": "subtraction operator implementation",
    "builtin_multiply(args)": "multiplication operator implementation",
    "builtin_divide(args)": "division operator implementation",
    "builtin_less_than(args)": "less than comparison",
    "lookup(name) returns": "LispValue - searches environment chain for variable",
    "extend(new_bindings) returns": "Environment - creates child environment",
    "depth() returns": "int - calculates environment chain depth",
    "all_names() returns": "set[str] - gets all accessible variable names",
    "create_global_environment() returns": "Environment - creates environment with built-ins",
    "handle_lambda(args, env) returns": "LispValue - processes lambda special form",
    "handle_define(args, env) returns": "LispValue - processes define special form",
    "apply_function(func, args, current_env) returns": "LispValue - applies function to arguments",
    "make_function(parameters, body, closure_env, name) returns": "LispValue - creates user-defined function",
    "make_builtin(implementation, name, arity) returns": "LispValue - creates built-in function",
    "is_function(value) returns": "bool - check if value is callable",
    "is_user_function(value) returns": "bool - checks if user-defined function",
    "is_builtin_function(value) returns": "bool - checks if built-in function",
    "extend() returns": "Environment - creates child environment",
    "define(name, value) returns": "None - creates binding in current environment",
    "evaluate(ast, env) returns": "LispValue - main evaluation dispatch function",
    "builtin_car(args) returns": "LispValue - extract first element from list",
    "builtin_cdr(args) returns": "LispValue - extract rest of list after first",
    "builtin_cons(args) returns": "LispValue - create new list with element prepended",
    "builtin_list(args) returns": "LispValue - create proper list from arguments",
    "builtin_null_p(args) returns": "LispValue - test if argument is empty list",
    "is_list(value) returns": "bool - type predicate for lists",
    "apply_function(func, args, env) returns": "LispValue - applies function to arguments",
    "extend_with_recursion(bindings, name, func) returns": "Environment - create child with recursion support",
    "create_tail_call(func, args, env) returns": "TailCall - create tail call continuation",
    "execute_trampoline(initial_call) returns": "LispValue - execute tail calls iteratively",
    "evaluate_text(text)": "evaluate multiple expressions from text",
    "with_context(context)": "add context to error message",
    "process_expression(text, environment)": "complete pipeline processing",
    "enrich_tokenizer_error(error, text)": "add source context",
    "enrich_parser_error(error, tokens)": "add syntactic context",
    "enrich_evaluation_error(error, ast)": "add semantic context",
    "prepare_evaluation_context(ast)": "prepare environment for evaluation",
    "finalize_evaluation_context(result, env)": "cleanup after evaluation",
    "handle_define_operation(name, value)": "process global definition",
    "format_error(error, detail_level) returns": "str - formatted error message with context",
    "extract_source_context(location, context_lines) returns": "str - source code context around error",
    "format_suggestions(error) returns": "str - helpful suggestions for fixing error",
    "scan_all() returns": "List[Token] - main tokenization loop",
    "recover_from_error(error) returns": "None - attempt error recovery",
    "synchronize_after_error(tokens, position) returns": "int - next valid parse position",
    "check_resource_limits() returns": "None - verify resource usage within limits",
    "enrich_evaluation_error(error, ast, env) returns": "EvaluationError - error with added context",
    "skip_to_synchronization_point(tokens, pos, sync_tokens) returns": "int - position after synchronization",
    "suggest_similar_names(target, available_names, max_suggestions) returns": "List[str] - similar name suggestions",
    "assertTokenSequence(text, expected_tokens)": "verify tokenization produces expected sequence",
    "assertParseResult(text, expected_ast)": "verify parsing produces expected AST",
    "assertEvaluatesTo(program, expected_result)": "verify evaluation produces expected result",
    "assertTokenizerError(text, pattern)": "verify tokenizer error detection",
    "assertParseError(text, pattern)": "verify parser error detection",
    "assertEvaluationError(program, error_type)": "verify evaluation error detection",
    "debug_pipeline_stages(source_text)": "debugging helper showing pipeline stages",
    "validate_milestone_progress(milestone_number)": "automated milestone validation",
    "trace_evaluation(expression, environment)": "context manager for tracing single evaluation",
    "log_lookup(variable_name, environment, result)": "log variable lookup operation",
    "format_trace_output()": "format collected trace events for display",
    "inspect_chain()": "generate information about entire environment chain",
    "get_all_accessible_variables()": "return all accessible variables with sources",
    "analyze_closure_environments(function_value)": "analyze environment capture in closures",
    "diagnose_lookup_failure(variable_name)": "provide diagnosis when variable lookup fails",
    "visualize_tree(ast, format)": "generate visual representation of AST structure",
    "analyze_structure(ast)": "analyze AST structure and identify issues",
    "compare_structures(ast1, ast2)": "compare two AST structures and highlight differences",
    "handle_debug_command(command_line)": "process debug command and return result",
    "cmd_trace(expression_text)": "enable tracing for expression evaluation",
    "cmd_environment(args)": "display current environment information",
    "expand_macros(ast) returns": "AST - recursively expand all macros in the AST",
    "expand_macro_call(macro_func, args) returns": "AST - expand single macro call by applying macro to arguments",
    "define_macro(name, parameters, body) returns": "None - define new macro in macro environment",
    "execute(instructions) returns": "None - execute sequence of bytecode instructions",
    "execute_instruction(instruction) returns": "None - execute single bytecode instruction",
    "push(value) returns": "None - push value onto operand stack",
    "pop() returns": "Any - pop value from operand stack",
    "enable() returns": "None - enable profiling data collection",
    "disable() returns": "str - disable profiling and generate report",
    "enter_function(function_name) returns": "None - record function entry for timing",
    "exit_function(function_name) returns": "None - record function exit and update timing",
    "generate_report() returns": "str - generate human-readable profiling report",
    "is_number(value) returns": "bool - type predicate for numbers",
    "is_symbol(value) returns": "bool - type predicate for symbols",
    "is_truthy(value) returns": "bool - determines Lisp truthiness",
    "tokenize(text) returns": "List[Token] - converts text to token stream",
    "read_expr(tokens, pos) returns": "tuple[LispValue, int] - reads single expression from token stream",
    "read_list(tokens, pos) returns": "tuple[LispValue, int] - reads parenthesized list from tokens",
    "is_special_form(symbol_name) returns": "bool - checks if symbol names special form",
    "current_char() returns": "Optional[str] - return current character or None",
    "advance() returns": "None - move to next position",
    "skip_whitespace() returns": "None - skip whitespace at current position",
    "scan_string() returns": "Token - scan string literal with escapes",
    "scan_number() returns": "Token - scan numeric literal",
    "scan_symbol() returns": "Token - scan symbol/identifier",
    "scan_comment() returns": "Token - skip comment to end of line",
    "handle_if(args, env) returns": "LispValue - processes if special form",
    "builtin_add(args) returns": "LispValue - addition operator implementation",
    "builtin_subtract(args) returns": "LispValue - subtraction operator implementation"
  },
  "constants": {
    "LISP_TRUE": "truthy value in Lisp",
    "LISP_FALSE": "falsy value in Lisp",
    "EMPTY_LIST": "empty list representation",
    "ESCAPE_SEQUENCES": "mapping of escape chars to actual characters",
    "TokenType.LEFT_PAREN": "( token",
    "TokenType.RIGHT_PAREN": ") token",
    "TokenType.NUMBER": "numeric literal token",
    "TokenType.SYMBOL": "identifier/operator token",
    "TokenType.STRING": "string literal token",
    "TokenType.QUOTE": "' token",
    "TokenType.EOF": "end of file token",
    "QUOTE_SYMBOL": "string 'quote' for transformation",
    "BUILTIN_FUNCTIONS": "registry of built-in functions",
    "SPECIAL_FORMS": "registry mapping special form names to handlers",
    "LispValueType.FUNCTION": "user-defined function type",
    "LispValueType.BUILTIN": "built-in function type"
  },
  "terms": {
    "S-expression": "symbolic expression - Lisp's uniform syntax",
    "AST": "abstract syntax tree built from tokens",
    "environment": "mapping from variable names to values",
    "homoiconicity": "property where code and data have same representation",
    "tree-walking interpreter": "interpreter that directly evaluates AST nodes",
    "REPL": "Read-Eval-Print Loop interactive interface",
    "lexical scoping": "variable resolution based on definition location",
    "closure": "function that captures its lexical environment",
    "special forms": "language constructs that control evaluation like if, define, lambda",
    "pipeline": "three-stage architecture of tokenizer, parser, evaluator",
    "separation of concerns": "architectural principle isolating component responsibilities",
    "recursive descent": "parsing strategy using recursive function calls",
    "discriminated union": "data structure with type tag and value payload",
    "tokenization": "converting text into meaningful tokens",
    "boundary detection": "identifying where tokens start and end",
    "state machine": "algorithm that changes behavior based on current state",
    "lookahead": "examining next character before making decision",
    "escape sequence": "backslash followed by special character in strings",
    "single-pass scanning": "processing each character exactly once",
    "delimiter": "character that separates tokens like whitespace or parentheses",
    "position threading": "functional approach to tracking token stream position",
    "quote syntax": "shorthand 'expr for (quote expr)",
    "panic mode recovery": "error recovery by skipping to synchronization points",
    "nesting depth": "level of parenthesis nesting in expressions",
    "function calls": "regular function application with eager argument evaluation",
    "environment threading": "passing environment context through evaluation calls",
    "self-evaluating expressions": "literals that evaluate to themselves",
    "symbol lookup": "finding variable bindings in environment chain",
    "eager evaluation": "evaluating all arguments before function application",
    "short-circuit evaluation": "stopping evaluation when result is determined",
    "arity checking": "verifying function called with correct argument count",
    "dispatch mechanism": "routing evaluation based on expression type",
    "recursive evaluation": "evaluating subexpressions before combining results",
    "environment chain": "linked sequence of nested environments for scope lookup",
    "closure environment": "captured environment stored in function object",
    "variable shadowing": "inner scope binding hiding outer scope binding with same name",
    "environment capture": "storing reference to defining environment in closure",
    "scope extension": "creating child environment for function calls or let bindings",
    "parameter binding": "mapping argument values to parameter names",
    "recursive function binding": "allowing function to reference itself by name",
    "first-class functions": "functions as values that can be stored and passed",
    "customizable machines": "mental model of functions as configurable templates",
    "structural sharing": "multiple data structures sharing common parts",
    "tail position": "function call whose return value becomes caller's return value",
    "trampoline": "optimization technique converting tail recursion to iteration",
    "continuation": "representation of computation remaining to be done",
    "proper list": "list terminated with empty list marker",
    "improper list": "list not terminated with empty list",
    "cons cell": "fundamental list building block with head and tail",
    "chain links": "mental model of lists as connected chain elements",
    "error propagation": "how errors flow upward through pipeline components",
    "state management": "persistence of environment bindings across evaluations",
    "error enrichment": "adding contextual information as errors propagate upward",
    "error recovery": "continuing processing after recoverable errors",
    "global environment": "persistent environment containing built-ins and user definitions",
    "pipeline coordinator": "component that manages flow between tokenizer, parser, evaluator",
    "state persistence": "global environment maintains bindings across evaluations",
    "evaluation context": "environment and metadata for processing expressions",
    "error context": "additional information added to errors for user comprehension",
    "staged error enrichment": "adding context at each pipeline stage",
    "resource limits": "constraints on memory, stack depth, and computation time",
    "synchronization points": "token positions where parsing can safely resume after errors",
    "unit testing by component": "testing tokenizer, parser, and evaluator in isolation",
    "integration testing strategy": "end-to-end testing of complete Lisp programs",
    "milestone validation checkpoints": "concrete verification criteria for each development stage",
    "property-based verification": "testing essential properties that must hold for all valid inputs",
    "boundary detection accuracy": "correct identification of token start and end positions",
    "token classification correctness": "accurate categorization of tokens by syntactic role",
    "structural correctness": "proper building of nested data structures from tokens",
    "syntactic transformation accuracy": "correct handling of quote syntax transformation",
    "pipeline validation": "ensuring data flows correctly through three-stage pipeline",
    "component interactions": "how tokenizer, parser, and evaluator work together",
    "data flow consistency": "maintaining information integrity across pipeline stages",
    "end-to-end correctness": "complete expressions evaluate to expected results",
    "quality pyramid": "testing structure with unit tests as foundation, integration tests in middle, end-to-end tests at top",
    "evaluation tracing": "step-by-step visibility into expression evaluation process",
    "AST visualization": "transforming internal tree structure into readable format",
    "REPL debugging": "interactive debugging commands within Read-Eval-Print Loop",
    "meta-execution": "how interpreter processes and transforms input programs",
    "boundary detection errors": "tokenizer splits input text at incorrect positions",
    "special form mishandling": "treating special forms like regular functions",
    "environment chain corruption": "broken parent references in environment chain",
    "closure environment capture": "functions capturing reference to defining environment",
    "parameter binding mismatch": "incorrect mapping of arguments to function parameters",
    "macro system": "ability to manipulate code as data before evaluation",
    "bytecode compilation": "translating AST into linear sequence of simple instructions",
    "virtual machine": "specialized engine for executing bytecode instructions",
    "generational garbage collection": "segregating objects by age for efficient collection",
    "constant folding": "evaluating compile-time constant expressions during compilation",
    "dead code elimination": "removing unreachable code paths and unused bindings",
    "language server protocol": "standardized way to integrate language intelligence with editors",
    "execution profiling": "measuring performance characteristics of program execution",
    "hygienic macros": "macro system that avoids variable capture problems",
    "stack-based VM": "virtual machine using operand stack for expression evaluation",
    "copy collection": "garbage collection using from-space and to-space copying",
    "tail call optimization": "optimization technique converting tail recursion to iteration"
  }
}