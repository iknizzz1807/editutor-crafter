title: LoRA Adapter Architecture {
  near: top-center
  style.font-size: 20
  style.bold: true
  style.font-color: "#e6edf3"
}

classes: {
  frozen: {
    style.fill: "#2d3748"
    style.stroke: "#718096"
    style.font-color: "#a0aec0"
    style.stroke-dash: 5
  }
  
  trainable: {
    style.fill: "#3fb950"
    style.stroke: "#68d391"
    style.font-color: "#ffffff"
    style.bold: true
  }
  
  computation: {
    style.fill: "#1a365d"
    style.stroke: "#4299e1"
    style.font-color: "#e6edf3"
  }
  
  layer_container: {
    style.fill: "#1a1a2e"
    style.stroke: "#3fb950"
    style.font-color: "#e6edf3"
  }
}

transformer_layer: Transformer Layer {
  class: layer_container
  
  attention: Multi-Head Attention {
    class: layer_container
    
    frozen_weights: "W_q, W_k, W_v\n(Frozen Base Weights)" {
      class: frozen
      shape: rectangle
    }
    
    lora_adapter: LoRA Adapter {
      class: layer_container
      
      lora_a: "Matrix A\n(rank × input_dim)\nTrainable" {
        class: trainable
        shape: rectangle
      }
      
      lora_b: "Matrix B\n(output_dim × rank)\nTrainable" {
        class: trainable
        shape: rectangle
      }
      
      lora_a -> lora_b: "Low-rank\nfactorization"
    }
    
    addition: "+" {
      class: computation
      shape: circle
    }
    
    frozen_weights -> addition: "h = W₀x"
    lora_adapter -> addition: "Δh = BAx"
    addition -> output_attention: "h + αΔh"
  }
  
  ffn: Feed Forward Network {
    class: layer_container
    
    frozen_ffn: "W_up, W_down\n(Frozen Weights)" {
      class: frozen
      shape: rectangle
    }
    
    lora_ffn: LoRA FFN Adapter {
      class: layer_container
      
      ffn_a: "Matrix A_ffn\n(rank × hidden)" {
        class: trainable
        shape: rectangle
      }
      
      ffn_b: "Matrix B_ffn\n(hidden × rank)" {
        class: trainable
        shape: rectangle
      }
      
      ffn_a -> ffn_b: "rank << hidden"
    }
    
    ffn_add: "+" {
      class: computation
      shape: circle
    }
    
    frozen_ffn -> ffn_add: "Base computation"
    lora_ffn -> ffn_add: "LoRA adaptation"
  }
}

input: "Input x" {
  class: computation
  shape: hexagon
}

output_attention: "Attention Output" {
  class: computation
  shape: hexagon
}

final_output: "Layer Output" {
  class: computation
  shape: hexagon
}

scaling_note: |md
  **Scaling Factor α**
  Controls adaptation strength
  α = 0: Base model only
  α > 0: Includes adaptations
| {
  shape: page
  style.fill: "#2d3440"
  style.stroke: "#fbbf24"
  style.font-color: "#fef3c7"
  near: bottom-right
}

memory_note: |md
  **Memory Efficiency**
  • Base weights: Frozen (shared)
  • LoRA params: ~0.1% of original
  • Rank typically 8-64
| {
  shape: page
  style.fill: "#2d3440"
  style.stroke: "#8b5cf6"
  style.font-color: "#e9d5ff"
  near: bottom-left
}

input -> transformer_layer: "Forward pass"
transformer_layer.attention -> transformer_layer.ffn: "Sequential processing"
transformer_layer.ffn -> final_output: "Adapted features"