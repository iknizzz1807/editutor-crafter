title: Memory Optimization Stack

classes: {
  base_layer: {
    style.fill: "#0f3460"
    style.stroke: "#3fb950"
    style.font-color: "#e6edf3"
    style.bold: true
  }
  adapter_layer: {
    style.fill: "#1a1a2e"
    style.stroke: "#3fb950"
    style.font-color: "#e6edf3"
  }
  training_layer: {
    style.fill: "#16213e"
    style.stroke: "#3fb950"
    style.font-color: "#e6edf3"
  }
  memory_label: {
    style.fill: "#d63031"
    style.font-color: "#ffffff"
    style.stroke: "#e17055"
    style.bold: true
  }
}

base_layer: 4-bit Quantized Base Model {
  class: base_layer
  
  nf4_weights: NF4 Weights {
    style.fill: "#2c3e50"
    style.stroke: "#3498db"
    style.font-color: "#ecf0f1"
  }
  
  double_quant: Double Quantization {
    style.fill: "#2c3e50"
    style.stroke: "#3498db"
    style.font-color: "#ecf0f1"
  }
  
  blockwise_quant: Blockwise Quantization {
    style.fill: "#2c3e50"
    style.stroke: "#3498db"
    style.font-color: "#ecf0f1"
  }
}

adapter_layer: LoRA Adapters {
  class: adapter_layer
  
  lora_a: LoRA Matrix A {
    style.fill: "#8e44ad"
    style.stroke: "#9b59b6"
    style.font-color: "#ffffff"
  }
  
  lora_b: LoRA Matrix B {
    style.fill: "#8e44ad"
    style.stroke: "#9b59b6"
    style.font-color: "#ffffff"
  }
  
  rank_decomp: Low-Rank Decomposition {
    style.fill: "#8e44ad"
    style.stroke: "#9b59b6"
    style.font-color: "#ffffff"
  }
}

training_layer: Mixed-Precision Training {
  class: training_layer
  
  grad_accum: Gradient Accumulation {
    style.fill: "#27ae60"
    style.stroke: "#2ecc71"
    style.font-color: "#ffffff"
  }
  
  fp16_compute: FP16 Computation {
    style.fill: "#27ae60"
    style.stroke: "#2ecc71"
    style.font-color: "#ffffff"
  }
  
  optimizer_states: Optimizer States {
    style.fill: "#27ae60"
    style.stroke: "#2ecc71"
    style.font-color: "#ffffff"
  }
}

base_memory: 4GB VRAM {
  class: memory_label
  near: right
}

adapter_memory: +2GB VRAM {
  class: memory_label
  near: right
}

training_memory: +8GB VRAM {
  class: memory_label
  near: right
}

total_memory: Total: 14GB VRAM {
  style.fill: "#f39c12"
  style.font-color: "#2c3e50"
  style.stroke: "#e67e22"
  style.bold: true
  near: bottom-right
}

base_layer -> adapter_layer: Load adapters on quantized base
adapter_layer -> training_layer: Forward pass with mixed precision
training_layer -> adapter_layer: Backprop through adapters only
adapter_layer -> base_layer: Update LoRA weights

base_layer -- base_memory
adapter_layer -- adapter_memory
training_layer -- training_memory