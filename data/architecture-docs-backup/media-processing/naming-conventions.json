{
  "types": {
    "ProcessingJob": "fields: job_id str, input_file_path str, output_specifications List, priority JobPriority, status JobStatus, created_at datetime, started_at datetime, completed_at datetime, error_message str, retry_count int, webhook_url str, progress_percentage float, estimated_duration int",
    "AppConfig": "fields: redis RedisConfig, storage StorageConfig, processing ProcessingConfig, debug bool",
    "RedisConfig": "fields: host str, port int, db int, password str",
    "StorageConfig": "base_path str, temp_path str, max_file_size int",
    "ProcessingConfig": "fields: max_workers int, job_timeout int, retry_attempts int, webhook_timeout int",
    "OutputSpecification": "fields: output_path str, format str, width int, height int, quality int",
    "BaseModel": "base class with serialization support",
    "MediaMetadata": "fields: file_path str, file_size int, mime_type str, format_name str, created_at datetime, checksum str, width int, height int",
    "ImageMetadata": "color_mode str, bit_depth int, compression str, quality int, has_transparency bool, orientation int, dpi tuple, icc_profile bytes, exif_data dict",
    "VideoMetadata": "duration float, frame_rate float, video_codec str, audio_codec str, bitrate int, keyframe_interval int, pixel_format str, aspect_ratio str, audio_channels int, audio_sample_rate int, subtitle_tracks list",
    "ImageProcessingConfig": "target_width int, target_height int, resize_mode str, interpolation str, crop_position str, background_color str, sharpen_amount float, optimize_for_web bool, max_colors int",
    "VideoTranscodingConfig": "video_codec str, audio_codec str, target_bitrate int, crf_value int, frame_rate float, keyframe_interval int, preset str, profile str, level str, two_pass_encoding bool",
    "AdaptiveBitrateConfig": "variant_name str, width int, height int, bitrate int, max_bitrate int, buffer_size int, audio_bitrate int, segment_duration int",
    "FFmpegError": "message str, exit_code int, stderr str",
    "FFmpegProgressParser": "duration_pattern regex, progress_pattern regex, total_duration float",
    "FFmpegWrapper": "ffmpeg_path str, logger Logger",
    "RedisBackend": "Redis connection and queue operations management",
    "WorkerProcess": "Individual worker with resource monitoring",
    "ResourceMonitor": "real-time resource monitoring",
    "QueueManager": "fields: manages job submission and worker coordination",
    "WorkerCoordinator": "fields: Worker pool management and scaling",
    "JobProgress": "job_id str, percentage float, stage str, stage_progress float, details dict",
    "ProgressUpdate": "job_id str, percentage float, stage str, timestamp datetime",
    "WebhookSecurity": "static methods for signature generation and verification",
    "ProgressTracker": "storage ProgressStorage, calculator ProgressCalculator, notifier WebhookNotifier",
    "ProgressStorage": "redis_client Redis, pg_conn_string str",
    "ProgressCalculator": "stage_weights Dict, stage_order List",
    "ProcessingStage": "enum with stage names and weight values",
    "JobMessage": "complete job message format for queue communication",
    "ProgressMessage": "progress update message format",
    "BaseWorker": "base class for all media processing workers",
    "MetadataStore": "fields: job status persistence layer",
    "ErrorClassification": "fields: category ErrorCategory, retry_eligible bool, max_retries int, backoff_factor float, description str",
    "RetryContext": "fields: job_id str, attempt_number int, last_error str, classification ErrorClassification, scheduled_at float, priority int",
    "JobStatus": "enum: PENDING, PROCESSING, COMPLETED, FAILED",
    "JobPriority": "enum: LOW, NORMAL, HIGH, URGENT",
    "TestMediaGenerator": "utility class for synthetic media creation",
    "MockWebhookServer": "HTTP server for webhook testing",
    "RedisTestManager": "Redis connection and cleanup utilities",
    "MetricPoint": "name str, value float, timestamp float, tags Dict",
    "StructuredFormatter": "JSON log formatting with context",
    "MetricsCollector": "thread-safe metrics collection",
    "SystemDiagnostics": "system health checking utilities",
    "PerformanceProfiler": "CPU and memory profiling",
    "JobInspector": "job debugging utilities",
    "NodeInfo": "node_id str, hostname str, capabilities List[NodeCapability], max_concurrent_jobs int, current_load float, memory_gb int, gpu_count int, network_region str, last_heartbeat float",
    "ClusterRegistry": "redis_url str, heartbeat_interval int, redis aioredis.Redis, local_node_id str",
    "LoadBalancer": "registry ClusterRegistry, node_assignments Dict, performance_history Dict, last_rebalance float, rebalance_interval int",
    "JobRequirements": "job_type JobType, estimated_duration int, memory_mb int, cpu_cores int, requires_gpu bool, input_size_mb int, priority int, network_region str",
    "AutoScaler": "registry ClusterRegistry, load_balancer LoadBalancer, config Dict, scaling_decisions List",
    "ContentAnalyzer": "models Dict, model_config Dict",
    "NodeCapability": "enum with IMAGE_PROCESSING, VIDEO_TRANSCODING, GPU_ACCELERATION, HIGH_MEMORY",
    "JobType": "enum with IMAGE_RESIZE, VIDEO_TRANSCODE, THUMBNAIL_GENERATE, BATCH_PROCESS"
  },
  "methods": {
    "submit_job(input_file, output_specs, priority, webhook_url) -> ProcessingJob": "create and queue new processing job",
    "update_job_progress(job_id, progress_percentage, stage, details)": "update job progress and send notifications",
    "mark_job_completed(job_id, output_files)": "finalize completed job and cleanup",
    "mark_job_failed(job_id, error_message, retry_eligible)": "handle job failure with retry logic",
    "init_config(config_path) -> AppConfig": "initialize global configuration",
    "setup_logging(debug)": "configure structured logging system",
    "job_logging_context(job_id, correlation_id)": "context manager for job-aware logging",
    "generate_job_id() -> str": "generate unique job identifier",
    "to_dict() -> Dict": "convert model to dictionary with datetime serialization",
    "from_dict(cls, data) -> BaseModel": "create model instance from dictionary with type conversion",
    "to_json() -> str": "serialize message to JSON string",
    "from_json(cls, json_str) -> BaseModel": "deserialize message from JSON",
    "mark_started()": "mark job as started and update status",
    "update_progress(percentage, stage)": "update job progress with validation",
    "mark_completed(output_files)": "mark job as successfully completed",
    "mark_failed(error_msg, retryable)": "mark job as failed with error details",
    "is_terminal() -> bool": "check if status represents finished job",
    "can_transition_to(new_status) -> bool": "validate state transition rules",
    "is_image_format() -> bool": "check if format is for still images",
    "is_video_format() -> bool": "check if format is for video content",
    "process_image(input_path, output_specs) -> List[str]": "process single image to multiple output formats",
    "resize_image(image, target_width, target_height, interpolation, resize_mode) -> Image": "resize with specified algorithm and aspect ratio handling",
    "convert_format(image, target_format, quality, optimize) -> bytes": "convert image format with quality settings",
    "generate_thumbnail(image, size, crop_mode) -> Image": "create square thumbnail with smart cropping",
    "extract_and_handle_metadata(image, metadata_mode) -> Dict": "extract and filter metadata based on privacy settings",
    "detect_format(file_path) -> Optional[SupportedFormat]": "detect image format by file signature",
    "load_image(file_path) -> Tuple[Image, Dict]": "load image and extract metadata",
    "check_image_memory_requirements(width, height, channels) -> int": "calculate memory needed for image processing",
    "can_process_image(image_path) -> Tuple[bool, str]": "verify image can be processed within memory limits",
    "execute_command(command, progress_callback, timeout)": "execute FFmpeg with progress monitoring",
    "parse_duration(line) -> Optional[float]": "extract total duration from FFmpeg output",
    "parse_progress(line) -> Optional[float]": "extract current progress percentage",
    "transcode_video(input_path, output_path, config, progress_callback) -> Dict": "main video transcoding operation",
    "_build_transcode_command(input_path, output_path, config, input_metadata) -> List[str]": "construct FFmpeg command from configuration",
    "generate_abr_variants(input_path, output_dir, variant_configs, progress_callback) -> Dict": "create multiple quality variants for adaptive streaming",
    "extract_thumbnail(input_path, output_path, timestamp, resolution) -> bool": "extract single frame as thumbnail image",
    "_terminate_process(process)": "gracefully shutdown FFmpeg process",
    "health_check() -> bool": "verify Redis connection and operations",
    "submit_job_atomic() -> bool": "atomically submit job with deduplication",
    "pop_highest_priority_job() -> ProcessingJob": "blocking pop with worker assignment",
    "get_queue_statistics() -> Dict": "queue depth and processing statistics",
    "scale_worker_pool(target_count)": "dynamically adjust worker pool size",
    "monitor_worker_health()": "check heartbeats and restart failed workers",
    "initialize_job_progress(job_id, total_stages, webhook_url) -> bool": "initialize progress tracking for new job",
    "update_stage_progress(job_id, stage, stage_percentage, details) -> bool": "update progress within current stage",
    "advance_to_next_stage(job_id, next_stage, completion_details) -> bool": "transition job to next processing stage",
    "mark_job_completed(job_id, output_files, processing_duration) -> bool": "mark job as 100% complete and send notifications",
    "mark_job_failed(job_id, error_message, retry_eligible) -> bool": "mark job as failed with retry eligibility",
    "update_progress(job_id, progress) -> bool": "atomically update progress in Redis with sequence validation",
    "get_progress(job_id) -> Optional[JobProgress]": "retrieve current progress from Redis or PostgreSQL",
    "generate_signature(payload, secret) -> str": "generate HMAC-SHA256 signature for webhook",
    "verify_signature(payload, signature, secret, max_age) -> bool": "verify webhook signature and prevent replay attacks",
    "calculate_overall_progress(current_stage, stage_progress) -> float": "calculate overall job percentage from stage progress",
    "initialize_stage_weights(stages, custom_weights) -> bool": "configure stage weights for progress calculation",
    "get_next_job(worker_id, timeout) -> Optional[JobMessage]": "blocking pop highest priority job",
    "publish_progress(progress_message) -> bool": "publish progress update to subscribers",
    "process_job(job) -> bool": "process specific job type",
    "classify_error(exception, context) -> ErrorClassification": "classify an error for retry handling",
    "schedule_retry(job, error, context) -> bool": "schedule a job for retry based on error classification",
    "calculate_retry_delay(attempt, base_delay, backoff_factor, max_delay, jitter_range) -> float": "calculate retry delay with exponential backoff and jitter",
    "process_retry_queue(worker_id) -> Optional[ProcessingJob]": "check retry queue for jobs ready to process",
    "check_circuit_breaker(service) -> bool": "check if circuit breaker allows requests to service",
    "record_service_result(service, success)": "record result of service call for circuit breaker logic",
    "wait_for_job_completion(job_id, timeout) -> Optional[str]": "wait for job to reach terminal state",
    "create_test_image(width, height, format, color) -> bytes": "generate synthetic test image",
    "create_image_with_metadata(width, height, exif_data) -> bytes": "create test image with EXIF data",
    "start_server()": "start mock webhook server",
    "redis_connection()": "context manager for Redis testing",
    "simulate_worker_processing(job_message) -> bool": "simulate worker job processing",
    "test_resize_preserve_aspect_ratio()": "test image resize with aspect ratio",
    "test_format_conversion_quality_settings()": "test format conversion quality",
    "test_exif_orientation_handling()": "test EXIF orientation rotation",
    "test_complete_image_processing_workflow()": "integration test for image processing",
    "test_video_transcoding_with_progress()": "integration test for video transcoding",
    "test_webhook_delivery_retry_logic()": "test webhook retry mechanisms",
    "setup_logging(debug, log_file)": "configure structured logging system",
    "counter(name, value, tags)": "increment counter metric",
    "gauge(name, value, tags)": "set gauge metric value",
    "histogram(name, value, tags)": "add value to histogram",
    "timed_operation(name, tags)": "context manager for timing operations",
    "check_redis_health()": "Redis connectivity and performance check",
    "check_worker_processes()": "worker process status verification",
    "check_disk_space(paths)": "disk usage analysis",
    "get_queue_statistics()": "comprehensive queue metrics",
    "profile_function(func, args, kwargs)": "profile function with CPU and memory analysis",
    "get_memory_usage(func, args, kwargs)": "detailed memory usage measurement",
    "analyze_bottlenecks(profile_name)": "identify performance bottlenecks",
    "start_monitoring(job_id)": "begin resource monitoring",
    "stop_monitoring()": "end monitoring and generate report",
    "inspect_job(job_id)": "comprehensive job state analysis",
    "diagnose_stuck_job(job_id)": "diagnose job processing issues",
    "analyze_queue_health()": "queue performance analysis",
    "register_node(node_info) -> bool": "register node in cluster with heartbeat",
    "get_active_nodes() -> List[NodeInfo]": "retrieve all healthy nodes from registry",
    "find_best_nodes(job_requirements, count) -> List[NodeInfo]": "find optimal nodes for job requirements",
    "assign_job(job) -> Optional[str]": "assign job to most suitable node",
    "record_job_completion(node_id, job_id, job_type, duration)": "update performance history after job completion",
    "evaluate_scaling_needs() -> Dict[str, int]": "analyze cluster and return scaling recommendations",
    "execute_scaling_action(node_type, count_change) -> bool": "execute infrastructure scaling through provider APIs",
    "analyze_image_content(image_path) -> Dict": "AI-powered image analysis for processing optimization",
    "optimize_video_encoding(video_path, target_formats) -> Dict": "analyze video content for optimal encoding parameters",
    "submit_job(input_file, output_specs, priority, webhook_url)": "create and queue new processing job"
  },
  "constants": {
    "JobStatus.PENDING": "job queued awaiting processing",
    "JobStatus.PROCESSING": "job currently being processed",
    "JobStatus.COMPLETED": "job finished successfully",
    "JobStatus.FAILED": "job failed permanently",
    "JobPriority.LOW": "priority level 1",
    "JobPriority.NORMAL": "priority level 5",
    "JobPriority.HIGH": "priority level 10",
    "JobPriority.URGENT": "priority level 20",
    "MediaFormat.JPEG": "JPEG image format",
    "MediaFormat.PNG": "PNG image format",
    "MediaFormat.WEBP": "WebP image format",
    "MediaFormat.MP4": "MP4 video container format",
    "MediaFormat.WEBM": "WebM video container format",
    "ProcessingStage.VALIDATION": "validation stage with 5% weight",
    "ProcessingStage.PREPROCESSING": "preprocessing stage with 10% weight",
    "ProcessingStage.RESIZE_OPERATIONS": "resize operations stage with 40% weight",
    "ProcessingStage.FORMAT_CONVERSION": "format conversion stage with 35% weight",
    "ProcessingStage.OPTIMIZATION": "optimization stage with 8% weight",
    "ProcessingStage.FINALIZATION": "finalization stage with 2% weight",
    "MessageType.PROCESS_MEDIA": "job processing message",
    "MessageType.PROGRESS_UPDATE": "progress notification message",
    "ErrorCategory.TRANSIENT": "temporary failures that typically resolve themselves",
    "ErrorCategory.RESOURCE": "system resource constraints requiring managed backoff",
    "ErrorCategory.PERMANENT": "fundamental problems that will not resolve through retry",
    "ErrorCategory.DEPENDENCY": "external system failures",
    "NodeCapability.IMAGE_PROCESSING": "image processing capability",
    "NodeCapability.VIDEO_TRANSCODING": "video transcoding capability",
    "NodeCapability.GPU_ACCELERATION": "GPU acceleration capability",
    "NodeCapability.HIGH_MEMORY": "high memory capability",
    "JobType.IMAGE_RESIZE": "image resizing job type",
    "JobType.VIDEO_TRANSCODE": "video transcoding job type",
    "JobType.THUMBNAIL_GENERATE": "thumbnail generation job type",
    "JobType.BATCH_PROCESS": "batch processing job type"
  },
  "terms": {
    "processing job": "unit of work containing input file and output specifications",
    "job queue": "message queue system for distributing processing tasks",
    "worker process": "background process that executes media processing operations",
    "progress tracking": "system for monitoring and reporting job completion status",
    "webhook notification": "HTTP callback sent on job status changes",
    "resource-aware scheduling": "job distribution based on node capabilities and current resource utilization",
    "stage-based progress": "progress reporting based on processing phases",
    "exponential backoff": "retry strategy with increasing delays",
    "media metadata": "technical information extracted from media files",
    "output specification": "configuration defining desired processing output",
    "job lifecycle": "progression of job through pending, processing, completed/failed states",
    "serialization": "converting data structures to/from JSON for storage and transmission",
    "state machine": "formal model governing valid job status transitions",
    "resource constraints": "limits on memory, CPU, and processing time for jobs",
    "interpolation algorithm": "mathematical method for calculating new pixel values during resize",
    "EXIF orientation": "metadata tag indicating camera rotation when photo was taken",
    "color space conversion": "transformation between different color representation systems",
    "smart cropping": "automated cropping that preserves visually important image regions",
    "metadata stripping": "removal of embedded information for privacy protection",
    "lossy compression": "compression that reduces file size by discarding some image data",
    "aspect ratio preservation": "maintaining original width to height ratio during resize",
    "adaptive bitrate streaming": "delivery technique with multiple quality variants",
    "quality ladder": "set of video renditions at different resolutions and bitrates",
    "keyframe alignment": "synchronized keyframe placement across ABR variants for smooth switching",
    "CRF encoding": "constant rate factor approach maintaining consistent perceptual quality",
    "GOP structure": "group of pictures organization with keyframes and predicted frames",
    "container format": "file wrapper that holds video, audio and metadata streams",
    "codec compatibility matrix": "validation table ensuring codec/container combinations work on target platforms",
    "hardware acceleration": "GPU-based video encoding for improved performance",
    "progressive download": "streaming technique allowing playback before complete file download",
    "segment alignment": "synchronized segment boundaries across ABR quality variants",
    "dead letter queue": "queue for jobs that failed permanently",
    "worker coordination": "distributed process management",
    "heartbeat monitoring": "health check system for worker processes",
    "priority queue": "queue that processes higher priority jobs first",
    "atomic operation": "indivisible database operation that prevents race conditions",
    "deduplication": "preventing duplicate job processing",
    "graceful shutdown": "orderly termination allowing job completion",
    "sequence number": "monotonically increasing identifier for preventing out-of-order updates",
    "notification threshold": "progress percentage that triggers webhook delivery",
    "HMAC signature": "cryptographic hash for webhook authentication and integrity",
    "monotonic progression": "progress values that only increase, never decrease",
    "stage transition": "moving from one processing phase to the next",
    "webhook delivery storm": "excessive notification traffic that overwhelms recipients",
    "progress reversal": "backward movement of progress percentage due to race conditions",
    "hybrid storage": "combination of Redis for real-time access and PostgreSQL for durability",
    "idempotency key": "unique identifier for detecting and preventing duplicate operations",
    "message serialization": "converting data structures to JSON for storage",
    "circuit breaker": "pattern to prevent cascade failures when external dependencies fail",
    "retry storm": "overwhelming retry attempts that prevent system recovery",
    "resource cleanup": "systematic cleanup of temporary files and resources after job completion or failure",
    "worker recovery": "process of detecting and restarting failed worker processes",
    "error classification": "categorizing errors to determine appropriate retry strategy",
    "unit testing": "testing individual components in isolation",
    "integration testing": "end-to-end testing with real dependencies",
    "test fixtures": "reusable test data and setup",
    "synthetic media": "programmatically generated test files",
    "mock server": "fake HTTP server for testing",
    "milestone checkpoint": "validation point for development progress",
    "webhook delivery": "HTTP notification on job status changes",
    "processing workflow": "complete job lifecycle from submission to completion",
    "test media coverage": "comprehensive format and edge case testing",
    "containerized testing": "isolated test environment using Docker",
    "structured logging": "consistent JSON log format with correlation",
    "metrics collection": "performance and resource measurement",
    "performance profiling": "CPU and memory analysis",
    "resource monitoring": "real-time system resource tracking",
    "job correlation": "tracing requests through processing pipeline",
    "diagnostic tools": "command-line debugging utilities",
    "health checks": "system component status verification",
    "bottleneck analysis": "performance constraint identification",
    "memory profiling": "memory usage pattern analysis",
    "horizontal scaling": "distributing processing across multiple nodes for increased capacity",
    "cluster coordination": "managing distributed nodes through service discovery and health monitoring",
    "adaptive load balancing": "dynamic job assignment using weighted scoring algorithms",
    "auto-scaling policies": "rules for automatically adding or removing nodes based on demand",
    "content-aware optimization": "using AI analysis to optimize processing parameters for specific media content",
    "smart cropping algorithms": "AI-powered cropping that preserves visually important image regions",
    "multi-CDN strategies": "using multiple content delivery networks for optimal global performance",
    "storage lifecycle management": "automatically transitioning files between storage tiers based on access patterns",
    "hybrid processing architectures": "combining on-premise processing with cloud-based specialized services",
    "circuit breaker pattern": "preventing cascade failures when external dependencies become unavailable",
    "graceful degradation": "maintaining core functionality when advanced features or dependencies fail"
  }
}