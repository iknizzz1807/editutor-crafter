{
  "types": {
    "Value": "fields: data float, grad float, _prev Set[Value], _op str, _backward Callable[[], None]",
    "Union": "type annotation for multiple types",
    "Set": "collection of unique Values",
    "List": "ordered collection",
    "Callable": "function type annotation",
    "Neuron": "fields: w List[Value], b Value, nonlin bool",
    "Layer": "fields: neurons List[Neuron]",
    "MLP": "fields: layers List[Layer]",
    "TrainingConfig": "fields: learning_rate float, max_iterations int, loss_threshold float, patience int",
    "TrainingMetrics": "fields: losses List[float], iterations int, best_loss float, patience_counter int",
    "StabilityLevel": "enum: STABLE, WARNING, UNSTABLE, CRITICAL",
    "NumericalValidator": "fields: nan_threshold float, inf_threshold float, gradient_clip_threshold float",
    "RecoveryManager": "fields: checkpoints List[dict], max_checkpoints int",
    "StabilityMonitor": "fields: loss_history List[float], gradient_history List[List[float]], stability_events List[dict]",
    "TensorValue": "fields: data ndarray, grad ndarray, _prev Set[TensorValue], _op str, _backward Callable",
    "Optimizer": "fields: learning_rate float, state Dict",
    "Adam": "fields: learning_rate float, beta1 float, beta2 float, eps float, t int",
    "ConvolutionalLayer": "fields: in_channels int, out_channels int, kernel_size int, stride int, padding int, weight TensorValue, bias TensorValue",
    "BatchNorm": "fields: num_features int, eps float, momentum float, training bool",
    "MultiHeadAttention": "fields: embed_dim int, num_heads int, head_dim int",
    "LearningRateScheduler": "fields: optimizer Optimizer, schedule_type str, initial_lr float"
  },
  "methods": {
    "__init__(data, _children, _op)": "initialize Value with data and graph tracking",
    "__add__(other)": "addition with autodiff support",
    "__mul__(other)": "multiplication with autodiff support",
    "tanh()": "hyperbolic tangent activation",
    "backward()": "compute gradients via reverse-mode automatic differentiation",
    "_build_topo(visited, topo)": "helper to build topological ordering",
    "__repr__()": "string representation for debugging",
    "__pow__(other)": "power operation with autodiff support",
    "__init__(nin, nonlin)": "initialize neuron with weights and bias",
    "__call__(x)": "forward pass computation for neural network components",
    "parameters()": "collect all trainable parameters from component",
    "mse_loss(predictions, targets)": "compute mean squared error loss",
    "gradient_descent_step(parameters, learning_rate)": "apply gradient descent parameter updates",
    "train_network(mlp, training_data, config)": "main training loop with convergence monitoring",
    "zero_gradients(parameters)": "reset all parameter gradients to zero",
    "should_stop(config)": "determine if training should terminate",
    "update(loss)": "update metrics and check convergence",
    "validate_scalar(value, context)": "validate numerical stability of scalar value",
    "clip_gradient(gradient)": "apply gradient clipping to prevent explosion",
    "analyze_gradient_flow(gradients)": "analyze gradient distribution for instabilities",
    "create_checkpoint(parameters, iteration)": "save current parameter state",
    "restore_checkpoint(parameters, checkpoint_id)": "restore parameters from checkpoint",
    "update_metrics(loss, gradients, iteration)": "update stability monitoring metrics",
    "should_intervene()": "determine if stability intervention needed",
    "train_network_with_stability(mlp, data, config)": "enhanced training loop with error handling",
    "numerical_gradient(f, x, eps)": "compute gradient using finite differences",
    "gradient_check(f, parameters, tolerance)": "validate analytical vs numerical gradients",
    "numerical_gradient(f, x, eps) returns float": "compute derivative using finite differences",
    "gradient_check(loss_fn, parameters, tolerance) returns Dict": "compare analytical vs numerical gradients",
    "validate_scalar(value, context) returns StabilityLevel": "check numerical stability of scalar",
    "clip_gradient(gradient) returns float": "apply gradient clipping to prevent explosion",
    "analyze_gradient_flow(parameters) returns Dict": "analyze gradient statistics for instabilities",
    "debug_training_step(mlp, inputs, targets, learning_rate) returns Dict": "execute training step with debugging info",
    "diagnose_training_failure(loss_history, gradient_history) returns str": "analyze metrics to diagnose failures",
    "__matmul__(other)": "matrix multiplication with gradient computation",
    "sum(axis, keepdims)": "tensor summation along specified axes",
    "reshape(shape)": "tensor reshaping with gradient flow preservation",
    "step(parameters)": "perform one optimization step with parameter updates",
    "zero_grad(parameters)": "reset gradients to zero for all parameters",
    "broadcast_backward(grad, original_shape)": "reverse broadcasting for gradient computation",
    "train(mode)": "set training mode for batch normalization",
    "eval()": "set evaluation mode for batch normalization"
  },
  "constants": {
    "grad": "accumulated gradient",
    "data": "scalar float value",
    "_prev": "set of parent Value objects",
    "_op": "operation string for computational graph",
    "_backward": "gradient computation function",
    "w": "weight vector",
    "b": "bias term",
    "nonlin": "activation function flag",
    "neurons": "list of neurons in layer",
    "layers": "list of layers in MLP",
    "learning_rate": "step size multiplier for parameter updates",
    "max_iterations": "maximum number of training steps",
    "loss_threshold": "convergence criterion for loss value",
    "patience": "iterations without improvement before stopping",
    "nan_threshold": "minimum value before considering underflow",
    "inf_threshold": "maximum value before considering overflow",
    "gradient_clip_threshold": "maximum allowed gradient magnitude",
    "max_checkpoints": "number of recovery checkpoints to maintain",
    "beta1": "momentum decay rate for first moment estimation",
    "beta2": "variance decay rate for second moment estimation",
    "eps": "numerical stability constant for division",
    "momentum": "exponential moving average rate for running statistics",
    "num_heads": "number of parallel attention heads",
    "embed_dim": "embedding dimension for attention mechanism",
    "kernel_size": "spatial size of convolutional filters"
  },
  "terms": {
    "automatic differentiation": "computing gradients automatically using computational graphs",
    "computational graph": "directed acyclic graph representing mathematical operations",
    "forward pass": "computing predictions while building computational graph",
    "backward pass": "computing gradients via reverse-mode automatic differentiation",
    "topological sort": "ordering nodes so dependencies come before dependents",
    "gradient accumulation": "summing gradients when values participate in multiple operations",
    "reverse-mode AD": "automatic differentiation that computes all gradients in one backward pass",
    "chain rule": "calculus rule for computing derivatives of composite functions",
    "gradient descent": "optimization algorithm that updates parameters using gradients",
    "loss function": "scalar measure of prediction error for optimization",
    "convergence": "training termination when loss stops improving",
    "parameter update": "applying gradient descent to modify network weights and biases",
    "numerical stability": "property of computations remaining valid under floating-point arithmetic",
    "gradient explosion": "exponential growth of gradients during backpropagation",
    "gradient vanishing": "exponential decay of gradients during backpropagation",
    "NaN propagation": "spread of Not-a-Number values through computational graph",
    "training instability": "breakdown of learning process due to numerical or optimization issues",
    "gradient clipping": "limiting gradient magnitude to prevent explosion",
    "checkpoint recovery": "restoring system state from previously saved point",
    "stability monitoring": "tracking metrics to detect developing training problems",
    "finite difference": "numerical approximation of derivatives using small perturbations",
    "gradient checking": "validating analytical gradients against numerical approximations",
    "relative error": "normalized difference between analytical and numerical gradients",
    "parameter collection": "gathering all trainable weights and biases from network",
    "tensor operations": "multi-dimensional array computations with automatic differentiation",
    "broadcasting": "automatic dimension expansion for tensor operations with different shapes",
    "vectorized operations": "parallel computation across tensor elements for efficiency",
    "momentum optimization": "velocity-based parameter updates that accumulate gradient history",
    "adaptive learning rates": "per-parameter learning rate scaling based on gradient statistics",
    "attention mechanism": "dynamic weighting system for flexible information routing",
    "convolutional layer": "spatial filter application with parameter sharing across locations",
    "batch normalization": "activation distribution control using batch statistics",
    "residual connection": "skip connection enabling direct gradient flow across layers",
    "weight decay": "regularization technique that shrinks parameters independent of gradients",
    "gradient checkpointing": "memory optimization trading computation for storage in deep networks"
  }
}