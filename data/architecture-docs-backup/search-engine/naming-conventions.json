{
  "types": {
    "DocumentId": "fields: u32",
    "TermId": "fields: u32",
    "DocumentFrequency": "fields: u32",
    "TermFrequency": "fields: u32",
    "Position": "fields: u32",
    "Score": "fields: f64",
    "Document": "fields: id DocumentId, fields HashMap<String, Field>, metadata HashMap<String, String>",
    "Field": "fields: name String, field_type FieldType, content String, boost f64",
    "FieldType": "enum: Text, Keyword, Numeric, Date",
    "InvertedIndex": "fields: term_dictionary HashMap<String, TermId>, posting_lists HashMap<TermId, PostingList>, document_store HashMap<DocumentId, Document>, document_lengths HashMap<DocumentId, u32>, document_count u32, term_count u32",
    "PostingList": "fields: term_id TermId, document_frequency DocumentFrequency, postings Vec<Posting>, total_term_frequency u32",
    "ScoringAlgorithm": "enum: TfIdf, Bm25, Custom",
    "TfIdf": "TF-IDF scoring algorithm implementation",
    "Bm25": "BM25 scoring algorithm implementation",
    "FuzzyMatcher": "fields: ngram_index NgramIndex, cache FuzzyCache, max_edit_distance u32",
    "Query": "fields: query_type QueryType, terms Vec<String>, original_text String, field_filters Vec<FieldFilter>, scoring_params ScoringParameters",
    "QueryParser": "component for parsing query strings",
    "QueryExecutor": "query processing pipeline coordinator",
    "SearchEngine": "main coordinator with index, processors, and storage",
    "TextProcessor": "text processing pipeline component",
    "Posting": "fields: document_id DocumentId, term_frequency TermFrequency, positions Vec<Position>, field_frequencies HashMap<String, TermFrequency>",
    "QueryType": "enum: Term, Phrase, Boolean, FieldFilter, Range, Fuzzy",
    "BooleanOperator": "enum: And, Or, Not",
    "FieldFilter": "fields: field_name String, filter_type FilterType, value FilterValue, required bool",
    "FilterType": "enum: Exact, Prefix, Range, Regex",
    "FilterValue": "enum for different value types",
    "ScoringParameters": "fields: algorithm ScoringAlgorithm, field_boosts HashMap<String, f64>, k1 f64, b f64, fuzzy_tolerance u32",
    "ScoreNormalizer": "score normalization utilities",
    "FieldBoostConfig": "field boost configuration with validation",
    "TfIdfScorer": "TF-IDF relevance scoring implementation",
    "Bm25Scorer": "BM25 relevance scoring with configurable parameters",
    "NgramIndex": "fields: bigram_index HashMap<String, HashSet<TermId>>, trigram_index HashMap<String, HashSet<TermId>>, term_frequencies HashMap<TermId, DocumentFrequency>, term_strings HashMap<TermId, String>",
    "FuzzyCache": "fields: entries HashMap<String, CacheEntry>, max_size usize, ttl Duration",
    "EditDistance": "fields: matrix Vec<Vec<u32>>, max_distance u32",
    "Token": "fields: token_type TokenType, position usize, length usize",
    "TokenType": "enum: Term, QuotedPhrase, FieldSpecifier, BooleanAnd, BooleanOr, BooleanNot, LeftParen, RightParen, FuzzyOperator, BoostOperator, RangeOperator, Colon, EndOfInput",
    "Lexer": "fields: input String, position usize, current_char Option<char>",
    "DocumentProcessor": "document indexing pipeline coordinator",
    "SnapshotManager": "thread-safe snapshot manager for consistent reads",
    "LockManager": "hierarchical lock management system",
    "ReadSnapshot": "consistent read view with reference counting",
    "WriteOperation": "exclusive write operation coordinator",
    "BackupManager": "backup creation and restoration coordinator",
    "CorruptionDetector": "index corruption detection and validation",
    "ResourceMonitor": "system resource tracking and alerting",
    "QueryErrorHandler": "query parsing error recovery",
    "QueryParseError": "structured query parsing error information",
    "CorruptionReport": "detailed corruption detection results",
    "RepairResult": "outcome of automatic corruption repair",
    "ServiceLevelUpdate": "service level change notification",
    "MemoryPressureLevel": "enum representing memory pressure severity",
    "MemoryRecoveryResult": "outcome of memory pressure handling",
    "QueryRecoveryResult": "result of query error recovery attempt",
    "ErrorMessage": "user-friendly formatted error message",
    "UserContext": "user experience and query history",
    "BackupMetadata": "fields: backup_id String, timestamp u64, index_version u64, component_checksums HashMap<String, String>, backup_type BackupType, compression_used bool",
    "BackupType": "enum: Full, Incremental, Component",
    "ResourceAlert": "enum: MemoryWarning, DiskWarning, MemoryCritical, DiskCritical",
    "TestDataGenerator": "generates realistic test data for validation",
    "PerformanceTestHarness": "coordinates performance and load testing",
    "TestConfig": "configuration for performance test parameters",
    "OperationMix": "percentages of different operation types",
    "PerformanceResults": "comprehensive performance test outcomes",
    "MilestoneVerifier": "validates milestone completion criteria",
    "SearchLogger": "structured logging system",
    "LogLevel": "enum: Debug, Info, Warn, Error",
    "LogEntry": "structured log entry",
    "OperationTimer": "automatic timing utility",
    "IndexValidator": "index integrity checker",
    "ValidationError": "index validation error report",
    "ValidationWarning": "index validation warning",
    "PerformanceProfiler": "performance measurement system",
    "ShardCoordinator": "fields: shard_registry HashMap<String, ShardInfo>, global_stats_cache Arc<RwLock<GlobalStats>>, result_merger ResultMerger, health_monitor HealthMonitor",
    "ReplicaManager": "fields: replica_config ReplicaConfig, primary_shard Option<ShardInfo>, backup_shards Vec<ShardInfo>, replication_log ReplicationLog",
    "FeatureExtractor": "fields: base_scorers Vec<Box<dyn ScoringAlgorithm>>, embeddings EmbeddingModel, user_profiler UserProfiler, feature_cache LruCache<String, FeatureVector>",
    "RankingModel": "fields: model XGBoostModel, feature_extractor FeatureExtractor, training_data_buffer Vec<TrainingSample>, model_version u64",
    "FacetedSearchEngine": "fields: base_engine SearchEngine, facet_indexes HashMap<String, FacetIndex>, aggregator ResultAggregator",
    "GeoSearchEngine": "fields: text_engine SearchEngine, spatial_index RTreeIndex, distance_calculator DistanceCalculator"
  },
  "methods": {
    "process(text: &str) -> Vec<String>": "convert raw text to normalized terms",
    "add_document(document: Document) -> DocumentId": "add document to search index",
    "search(query_str: &str, limit: usize) -> Vec<(DocumentId, Score)>": "search for matching documents",
    "get_document(doc_id: DocumentId) -> Option<&Document>": "retrieve document by ID",
    "save_to_disk(path: &str) -> Result<(), Error>": "persist index to disk",
    "load_from_disk(path: &str) -> Result<Self, Error>": "load index from disk",
    "tokenize(text: &str) -> Vec<String>": "split text into tokens",
    "normalize(token: &str) -> String": "normalize token case and punctuation",
    "is_stop_word(term: &str) -> bool": "check if term is stop word",
    "calculate_score(document_id: DocumentId, query_terms: &[TermId], index: &InvertedIndex) -> Score": "calculate relevance score for document",
    "build_idf_cache(&mut self, index: &InvertedIndex)": "precompute IDF values for all terms in vocabulary",
    "update_average_length(&mut self, new_length: u32, total_docs: u32)": "update average document length incrementally when adding documents",
    "normalize_range(&self, score: Score) -> Score": "normalize score to 0.0-1.0 range",
    "normalize_sigmoid(&self, score: Score, steepness: f64) -> Score": "apply sigmoid normalization to reduce extreme values",
    "get_boost(&self, field_name: &str) -> f64": "get boost factor for field name with validation",
    "find_fuzzy_matches(query: &str, limit: usize) -> Vec<(TermId, Score)>": "find fuzzy matches for query term",
    "autocomplete(prefix: &str, limit: usize) -> Vec<(String, Score)>": "generate autocomplete suggestions",
    "generate_candidates(query: &str) -> Vec<TermId>": "generate candidate terms using n-gram overlap",
    "distance(s1: &str, s2: &str) -> Option<u32>": "calculate edit distance between strings",
    "extract_ngrams(term: &str) -> (Vec<String>, Vec<String>)": "extract bigrams and trigrams from term",
    "add_term(term_id: TermId, term: &str, frequency: DocumentFrequency)": "add term to n-gram indexes",
    "parse(query_str: &str) -> Result<Query, String>": "parse query string into structured Query object",
    "tokenize() -> Result<Vec<Token>, String>": "convert input string into token stream",
    "parse_expression() -> Result<QueryType, String>": "parse OR expressions with lowest precedence",
    "parse_term_sequence() -> Result<QueryType, String>": "parse AND expressions and implicit conjunction",
    "parse_factor() -> Result<QueryType, String>": "parse NOT expressions and parenthesized groups",
    "parse_primary() -> Result<QueryType, String>": "parse atomic query components",
    "parse_field_filter(field_name: String) -> Result<QueryType, String>": "parse field filter specifications",
    "parse_range_value(field_name: String) -> Result<QueryType, String>": "parse range specifications",
    "scan_next_token() -> Result<Option<Token>, String>": "scan and classify next token from input",
    "advance()": "move to next character in input stream",
    "current_token() -> &Token": "return current token for parsing",
    "advance_token()": "move to next token in stream",
    "expect_token(expected: TokenType) -> Result<(), String>": "verify and consume expected token type",
    "acquire_read_snapshot() -> ReadSnapshot": "acquire consistent read view",
    "begin_write_operation() -> WriteOperation": "begin exclusive write operation",
    "acquire_read_lock() -> ReadLockGuard": "acquire shared read access",
    "acquire_write_lock() -> WriteLockGuard": "acquire exclusive write access",
    "detect_corruption(index: &InvertedIndex) -> CorruptionReport": "comprehensive corruption detection",
    "attempt_auto_repair(corruption_report: &CorruptionReport) -> RepairResult": "automatic repair of detected corruption using recovery strategies",
    "manage_resources() -> ServiceLevelUpdate": "monitor resources and apply degradation strategies when limits approached",
    "handle_memory_pressure(pressure_level: MemoryPressureLevel) -> MemoryRecoveryResult": "handle memory pressure by freeing non-essential caches",
    "handle_query_error(original_query: &str, parse_error: &QueryParseError) -> QueryRecoveryResult": "parse malformed queries using multiple recovery strategies",
    "generate_error_message(error: &QueryError, user_context: &UserContext) -> ErrorMessage": "generate user-friendly error messages with specific suggestions",
    "create_full_backup(index: &InvertedIndex) -> Result<String, std::io::Error>": "create complete backup snapshot with checksums",
    "restore_from_backup(backup_id: &str) -> Result<InvertedIndex, Box<dyn std::error::Error>>": "restore index from backup with validation",
    "start_monitoring()": "begin background resource monitoring",
    "register_alert_callback(callback: F)": "register callback for resource alerts",
    "update_memory_usage(bytes: u64)": "update current memory usage tracking",
    "generate_test_corpus(&self) -> Vec<Document>": "generate realistic test documents with controlled vocabulary",
    "generate_query_workload(&self) -> Vec<String>": "generate representative search queries for testing",
    "create_golden_results(&self, queries: &[String], index: &InvertedIndex) -> HashMap<String, Vec<DocumentId>>": "create expected results for regression testing",
    "run_load_test(&self) -> PerformanceResults": "execute concurrent load test according to configuration",
    "run_stress_test(&self, max_duration: Duration) -> PerformanceResults": "gradually increase load until performance degrades",
    "verify_milestone_1(&mut self) -> Result<(), String>": "verify inverted index construction and persistence",
    "verify_milestone_2(&mut self) -> Result<(), String>": "verify TF-IDF and BM25 ranking accuracy",
    "verify_milestone_3(&mut self) -> Result<(), String>": "verify fuzzy matching and autocomplete functionality",
    "verify_milestone_4(&mut self) -> Result<(), String>": "verify query parsing with boolean operators and filters",
    "log(&self, level: LogLevel, operation: &str, details: Value)": "write structured log entry",
    "start_operation(&self, operation: &str) -> OperationTimer": "begin timed operation",
    "validate_posting_list_ordering(&mut self, index: &InvertedIndex)": "check posting list sort order",
    "validate_term_frequencies(&mut self, index: &InvertedIndex)": "verify term frequency accuracy",
    "validate_cross_references(&mut self, index: &InvertedIndex)": "check reference integrity",
    "is_valid(&self) -> bool": "return validation status",
    "start_timing(&mut self, operation: &str)": "begin performance measurement",
    "end_timing(&mut self, operation: &str)": "complete performance measurement",
    "generate_report(&self) -> String": "create performance analysis report",
    "route_query(query: &Query) -> Vec<ShardTarget>": "analyze query and route to appropriate shards with load balancing",
    "merge_results(shard_results: Vec<ShardResult>) -> Vec<(DocumentId, Score)>": "merge distributed results maintaining global rank order",
    "promote_backup(failed_primary: &str) -> Result<(), ReplicaError>": "promote backup replica to primary during failover",
    "extract_features(query: &Query, document: &Document, user_context: &UserContext) -> FeatureVector": "extract machine learning features for ranking",
    "score_document(query: &Query, document: &Document, user_context: &UserContext) -> Score": "score document using learned ranking model",
    "update_model(feedback: &RelevanceFeedback) -> Result<(), MLError>": "update ranking model with new relevance signals",
    "search_with_facets(query: &Query, facet_config: &FacetConfig) -> FacetedSearchResult": "search with faceted result aggregation",
    "search_geo(query: &Query, location: &GeoPoint, max_distance: f64) -> Vec<GeoSearchResult>": "search with geographic constraints and distance scoring",
    "add_geo_document(document: Document, location: GeoPoint) -> Result<DocumentId, GeoError>": "index document with geographic information"
  },
  "constants": {
    "STOP_WORDS": "common words filtered from indexing",
    "DEFAULT_EDIT_DISTANCE": "maximum allowed typos for fuzzy matching",
    "DEFAULT_BM25_K1": "BM25 term frequency saturation parameter",
    "DEFAULT_BM25_B": "BM25 document length normalization parameter",
    "MAX_CANDIDATES": "maximum candidates for edit distance computation",
    "MIN_NGRAM_OVERLAP": "minimum n-gram overlap threshold",
    "DEFAULT_BACKUP_RETENTION": "maximum number of backup snapshots to retain",
    "MEMORY_WARNING_THRESHOLD": "memory usage percentage that triggers warnings",
    "MEMORY_CRITICAL_THRESHOLD": "memory usage percentage that triggers critical alerts",
    "CORRUPTION_CHECK_INTERVAL": "frequency of background corruption detection",
    "MAX_QUERY_COMPLEXITY": "maximum allowed query complexity before simplification",
    "RESOURCE_CHECK_INTERVAL": "frequency of resource monitoring checks",
    "DEFAULT_SHARD_COUNT": "default number of shards for distributed deployment",
    "REPLICATION_FACTOR": "number of replicas per shard",
    "GLOBAL_STATS_SYNC_INTERVAL": "frequency of global statistics synchronization",
    "FEATURE_CACHE_SIZE": "maximum size of ML feature cache",
    "MODEL_UPDATE_THRESHOLD": "training samples needed to trigger model retraining",
    "MAX_FACET_VALUES": "maximum facet values returned per field",
    "GEO_SEARCH_RADIUS": "default search radius for geographic queries"
  },
  "terms": {
    "inverted index": "data structure mapping terms to documents containing them",
    "posting list": "list of documents containing a specific term",
    "term frequency": "how often a term appears in a document",
    "document frequency": "how many documents contain a term",
    "tokenization": "splitting text into individual terms",
    "normalization": "converting terms to standard form",
    "stemming": "reducing words to root forms",
    "stop words": "common words filtered from indexing",
    "fuzzy matching": "finding approximate matches with typos",
    "edit distance": "measure of similarity between strings",
    "boolean query": "query with AND, OR, NOT operators",
    "phrase query": "query matching exact word sequences",
    "relevance scoring": "calculating document importance for queries",
    "TF-IDF": "term frequency inverse document frequency ranking",
    "BM25": "best matching ranking algorithm with saturation",
    "query parsing": "converting query strings to structured representation",
    "inverse document frequency": "measure weighing rare terms higher than common ones",
    "field boosting": "assigning different weights to document fields",
    "saturation": "diminishing returns for excessive term repetition",
    "length normalization": "adjusting scores based on document length",
    "score aggregation": "combining multiple signals into final relevance scores",
    "n-gram indexing": "indexing character subsequences for fast candidate generation",
    "autocomplete": "real-time query suggestions as users type",
    "candidate generation": "pre-filtering potential matches before expensive similarity calculation",
    "Levenshtein distance": "edit distance counting insertions, deletions, substitutions",
    "Damerau-Levenshtein": "edit distance including transposition operations",
    "trie": "prefix tree structure for efficient prefix matching",
    "transposition": "swapping adjacent characters",
    "field filter": "restricting search to specific document fields",
    "range query": "filtering by numeric or date ranges",
    "recursive descent parsing": "parsing technique using recursive function calls",
    "operator precedence": "rules determining evaluation order of operators",
    "abstract syntax tree": "hierarchical representation of parsed query",
    "lexical analysis": "breaking input into classified tokens",
    "token": "classified sequence of characters from input",
    "implicit conjunction": "treating adjacent terms as AND operations",
    "positional information": "exact word positions within documents for phrase matching",
    "proximity scoring": "relevance boost for terms appearing close together",
    "document indexing flow": "pipeline transforming documents into searchable index structures",
    "query processing flow": "pipeline executing queries against index to produce ranked results",
    "snapshot isolation": "consistent read view during recovery operations",
    "readers-writer lock": "concurrency control allowing multiple readers or single writer",
    "lock ordering": "disciplined sequence of lock acquisition to prevent deadlocks",
    "two-phase commit": "transaction pattern with preparation and commitment phases",
    "copy-on-write": "update strategy creating new versions rather than modifying in place",
    "memory barrier": "synchronization primitive ensuring memory operation ordering",
    "version timestamp": "monotonic identifier for index update generations",
    "thread-local cache": "per-thread data cache eliminating synchronization overhead",
    "lock contention": "performance degradation from concurrent access",
    "deadlock": "circular wait condition where threads block each other permanently",
    "index corruption": "damage to inverted index data structures threatening integrity",
    "corruption detection": "validation process identifying damaged index components",
    "backup restoration": "recovery process rebuilding index from backup snapshots",
    "graceful degradation": "reducing functionality while maintaining core service",
    "resource exhaustion": "system resources approaching or exceeding configured limits",
    "memory pressure": "high memory usage requiring optimization",
    "query error recovery": "automatic correction of malformed query syntax",
    "partial query execution": "processing valid query components while handling invalid parts",
    "service level degradation": "systematic reduction of features based on resource availability",
    "automatic repair": "system-initiated correction of detected corruption",
    "error message generation": "creation of user-friendly explanations for query problems",
    "component unit tests": "tests isolating individual algorithms and data structures",
    "end-to-end integration tests": "tests validating complete workflows",
    "milestone verification": "concrete checkpoints measuring progress",
    "performance testing": "validating latency and throughput requirements",
    "test coverage": "percentage of code exercised by tests",
    "regression testing": "verifying changes don't break existing functionality",
    "load testing": "testing system behavior under concurrent usage",
    "stress testing": "testing system behavior under resource limits",
    "benchmark testing": "measuring performance characteristics systematically",
    "posting list corruption": "damage to term-to-document mappings",
    "tokenization failure": "incorrect splitting of text into terms",
    "normalization inconsistency": "different term representations between indexing and querying",
    "stemming errors": "incorrect word reduction to root forms",
    "score archaeology": "detailed analysis of ranking score components",
    "performance profiling": "systematic measurement of execution characteristics",
    "structured logging": "formatted log entries with consistent fields",
    "index validation": "systematic verification of index integrity",
    "distributed architecture": "scaling search across multiple nodes with sharding and replication",
    "shard distribution": "partitioning index data across nodes for horizontal scaling",
    "term-based sharding": "distributing vocabulary across nodes",
    "document-based sharding": "distributing document collections across nodes",
    "replica management": "coordinating primary and backup copies of index shards",
    "result merging": "combining search results from distributed shards",
    "machine learning integration": "using learned models for search ranking and personalization",
    "learning-to-rank": "supervised ML approach to search relevance ranking",
    "feature engineering": "extracting ML features from queries, documents, and user behavior",
    "personalization": "customizing search results based on user preferences and behavior",
    "faceted search": "interactive filtering through document attribute aggregation",
    "geographic search": "spatial search with location-based relevance",
    "semantic search": "conceptual similarity search using vector embeddings",
    "temporal queries": "time-aware search with date constraints and recency scoring",
    "real-time analytics": "live monitoring and insights into search system performance"
  }
}