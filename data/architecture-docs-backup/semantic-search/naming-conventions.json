{
  "types": {
    "Document": "fields: doc_id str, title str, content str, url Optional[str], metadata Optional[Dict], created_at Optional[str]",
    "DocumentEmbedding": "fields: document Document, embedding np.ndarray, model_name str, embedding_dim int",
    "TextProcessor": "fields: url_pattern re.Pattern, email_pattern re.Pattern, whitespace_pattern re.Pattern",
    "DocumentEncoder": "fields: model SentenceTransformer, model_name str, embedding_dim int, text_processor TextProcessor",
    "QueryRequest": "fields: query_text str, max_results int, filters Optional[Dict], personalization_context Optional[Dict], include_facets bool",
    "SearchResult": "fields: document Document, relevance_score float, snippet str, highlighted_terms List[str], ranking_signals Dict[str, float]",
    "QueryResponse": "fields: query str, results List[SearchResult], total_found int, processing_time_ms float, facets Optional[Dict]",
    "ProcessedQuery": "fields: original_query str, normalized_query str, primary_embedding np.ndarray, expanded_terms List[Tuple[str, float]], entities List[Tuple[str, str]], intent str, negative_terms List[str], multi_vector_components Optional[List[Tuple[str, np.ndarray, float]]], processing_metadata Dict[str, Any]",
    "QueryProcessor": "query understanding and processing",
    "TextNormalizer": "fields: preserve_entities bool, whitespace_pattern re.Pattern, entity_pattern re.Pattern, technical_pattern re.Pattern, stop_words Set[str], preserve_case_terms Set[str]",
    "SynonymExpander": "fields: synonyms Dict[str, List[str]]",
    "EmbeddingCache": "fields: max_size int, ttl_seconds int, exact_cache OrderedDict, normalized_cache OrderedDict, component_cache Dict, stats Dict",
    "RankingSignals": "fields: semantic_score float, bm25_score float, personalization_score float, freshness_score float, click_score Optional[float]",
    "RankingCandidate": "fields: document Document, signals RankingSignals, combined_score float, stage str",
    "PersonalizationContext": "fields: user_id Optional[str], role Optional[str], industry Optional[str], experience_level Optional[str], recent_clicks List[str], topic_preferences Dict[str, float]",
    "AutocompleteRequest": "fields: query str, max_suggestions int, personalization_context Optional[Dict]",
    "AutocompleteResponse": "fields: suggestions List[Dict], processing_time_ms float",
    "ContextInfo": "fields: correlation_id str, user_id Optional[str], request_timestamp datetime, processing_budget_ms int, quality_vs_speed str, component_trace List[str]",
    "ProcessingResult": "fields: success bool, data Any, error_message Optional[str], error_code Optional[str], processing_time_ms float, context ContextInfo",
    "IndexingMessage": "fields: document Document, stage str, result ProcessingResult, checkpoint_id Optional[str], retry_count int, context ContextInfo",
    "SearchMessage": "fields: query_request QueryRequest, processed_query Optional[ProcessedQuery], candidates List[RankingCandidate], final_results List[SearchResult], stage str, context ContextInfo",
    "CircuitBreakerConfig": "fields: failure_threshold int, success_threshold int, timeout_seconds int, half_open_max_calls int",
    "RetryConfig": "fields: max_attempts int, base_delay_seconds float, max_delay_seconds float, exponential_base float, jitter bool",
    "QueryValidationResult": "fields: is_valid bool, sanitized_query str, warnings List[str], suggestions List[str], metadata Dict[str, Any]",
    "ErrorSeverity": "enum: LOW, MEDIUM, HIGH, CRITICAL",
    "ComponentType": "enum: EMBEDDING_MODEL, VECTOR_INDEX, QUERY_PROCESSOR, RANKING_ENGINE, SEARCH_API",
    "CircuitBreakerState": "enum: CLOSED, OPEN, HALF_OPEN",
    "EvaluationMetrics": "fields: precision_at_k Dict[int, float], recall_at_k Dict[int, float], map_score float, ndcg_at_k Dict[int, float], mrr_score float, err_score float",
    "LoadTestResult": "fields: total_requests int, successful_requests int, failed_requests int, avg_response_time float, p50_response_time float, p95_response_time float, p99_response_time float, requests_per_second float, error_rate float",
    "EmbeddingIndex": "vector index for similarity search",
    "RankingEngine": "multi-stage result ranking",
    "SearchAPI": "REST API for search operations",
    "PerformanceMetrics": "fields: operation_name str, start_time float, end_time float, duration_ms float, memory_before_mb float, memory_after_mb float, metadata Dict",
    "VectorDiagnostics": "fields: vector_count int, dimension int, norm_mean float, norm_std float, norm_min float, norm_max float, is_normalized bool, zero_vectors int, identical_vectors int, dimension_stats Dict",
    "RelevanceDebugInfo": "fields: document_id str, title str, semantic_score float, bm25_score float, personalization_score float, freshness_score float, combined_score float, rank_position int, human_relevance Optional[int], debug_notes List[str]",
    "SearchProfiler": "production-ready profiler for semantic search operations",
    "VectorDebugger": "production-ready vector debugging utilities",
    "RelevanceDebugger": "production-ready relevance debugging framework",
    "MediaContent": "content_id str, media_type str, content_path Optional[str], content_data Optional[bytes], text_content Optional[str], extracted_text Optional[str], metadata Optional[Dict]",
    "ShardInfo": "shard_id str, host str, port int, document_count int, last_heartbeat float, is_healthy bool, load_score float",
    "QueryRoute": "target_shards List[str], confidence_scores Dict[str, float], routing_strategy str, estimated_cost float",
    "ConceptFilter": "concept vector representation for semantic filtering",
    "ConversationSession": "session state tracking for conversational search",
    "FederationCoordinator": "query distribution, result aggregation, source selection",
    "SourceAdapter": "translates between federation protocol and source-specific APIs",
    "ResultNormalizer": "harmonizes relevance scores across heterogeneous sources",
    "MultiModalEncoder": "multi-modal document encoder creating unified embeddings",
    "SemanticFilterProcessor": "semantic filtering using concept vectors",
    "ConversationManager": "conversational search session management",
    "DistributedCoordinator": "coordination system for distributed vector search",
    "QueryRouter": "intelligent query routing for distributed search",
    "RealTimeUpdatePipeline": "real-time document update pipeline"
  },
  "methods": {
    "get_searchable_text() -> str": "combine title and content for embedding",
    "clean_text(text: str) -> str": "clean text for embedding generation",
    "encode_document(document: Document) -> DocumentEmbedding": "convert document to vector embedding",
    "encode_texts(texts: List[str]) -> np.ndarray": "batch encode multiple texts efficiently",
    "encode_query(query_text: str) -> np.ndarray": "convert query to embedding vector",
    "cosine_similarity(vec1, vec2) -> float": "compute cosine similarity between vectors",
    "normalize_vector(vec: np.ndarray) -> np.ndarray": "L2 normalize vector to unit length",
    "normalize_query(query: str) -> str": "normalize query text for caching",
    "expand_term(term: str, max_expansions: int) -> List[Tuple[str, float]]": "get synonyms with confidence scores",
    "get(query: str, normalized_query: str) -> Optional[np.ndarray]": "retrieve cached embedding",
    "put(query: str, embedding: np.ndarray, normalized_query: str)": "cache embedding for query",
    "process_query(query_text: str, context: Dict) -> ProcessedQuery": "main query processing pipeline",
    "expand_query_terms(terms: List[str], entities: List[str]) -> List[Tuple[str, float]]": "apply expansion strategies",
    "handle_multi_vector_query(processed_query: ProcessedQuery) -> List[Tuple[str, np.ndarray, float]]": "decompose complex queries",
    "rank_documents(processed_query, personalization_context, max_results) -> List[SearchResult]": "Execute complete multi-stage ranking pipeline",
    "score_candidate(document, processed_query, semantic_score, bm25_score, personalization_context) -> RankingSignals": "compute all ranking signals for query-document pair",
    "combine_signals(signals, query_type) -> float": "combine individual signals into final hybrid score",
    "rerank_candidates(candidates, query_text) -> List[RankingCandidate]": "apply cross-encoder reranking to top candidates",
    "score_document(document, context) -> float": "Compute personalization or freshness score",
    "record_interaction(query, results, click_position, dwell_time)": "record user interaction for learning",
    "get_click_adjustments(query, candidates) -> Dict[str, float]": "Get learned score adjustments from click history",
    "highlight_query_terms(text: str, query_terms: List[str]) -> str": "Mark matched terms in result snippets",
    "format_search_results(results: List, query: str) -> List[SearchResult]": "format results for API response",
    "get_autocomplete_suggestions(query: str, max_suggestions: int) -> List[str]": "Generate typeahead suggestions",
    "compute_facets(results: List[SearchResult], facet_fields: List[str]) -> Dict": "Calculate facet counts from results",
    "record_search_analytics(query: str, results: List, response_time: float)": "Asynchronously log search metrics",
    "create_context(user_id, processing_budget_ms, quality_vs_speed) -> ContextInfo": "create processing context with unique correlation ID",
    "wrap_result(func)": "Decorator to wrap function results in ProcessingResult objects",
    "can_execute() -> bool": "Check if execution is allowed based on current circuit state",
    "record_success()": "Record successful operation, potentially closing the circuit",
    "record_failure()": "Record failed operation, potentially opening the circuit",
    "execute_with_fallback(operation, fallback, context) -> ProcessingResult": "Execute operation with circuit breaker protection and optional fallback",
    "get_health_status() -> Dict[str, Any]": "Get component health and circuit breaker status",
    "validate_and_sanitize(query_text, max_length) -> QueryValidationResult": "main entry point for query validation and sanitization",
    "handle_empty_query(context) -> ProcessingResult": "Handle empty queries by providing trending or contextual suggestions",
    "handle_malformed_query(query_text, validation_result, context) -> ProcessingResult": "Handle queries that can't be processed normally",
    "handle_excessive_length(query_text, max_length, context) -> ProcessingResult": "Handle queries exceeding maximum length limits",
    "evaluate_search_results(query, result_doc_ids, k_values) -> EvaluationMetrics": "evaluate search quality against ground truth",
    "calculate_precision_at_k(binary_relevance, k) -> float": "compute precision at k metric",
    "calculate_dcg(relevance_scores, k) -> float": "compute discounted cumulative gain",
    "calculate_ndcg(relevance_scores, k) -> float": "compute normalized DCG",
    "execute_search_request(session, query) -> Tuple[float, bool]": "execute single search request with timing",
    "run_concurrent_load_test(concurrent_users, duration_seconds, query_pattern) -> LoadTestResult": "run concurrent load testing",
    "simulate_realistic_user_behavior(session) -> List[Tuple[float, bool]]": "simulate realistic search sessions",
    "generate_traffic_spike(baseline_qps, spike_multiplier, spike_duration) -> LoadTestResult": "test traffic spike handling",
    "get_document_count() -> int": "return number of indexed documents",
    "profile_operation(operation_name, **metadata)": "context manager for profiling search operations",
    "profile_function(operation_name)": "decorator for profiling function calls",
    "get_summary() -> Dict[str, Any]": "generate performance summary statistics",
    "diagnose_vectors(vectors, tolerance) -> VectorDiagnostics": "comprehensive vector health check",
    "validate_index_health(index, sample_queries) -> Dict[str, Any]": "validate FAISS index health with sample searches",
    "compare_embeddings(embeddings1, embeddings2, labels) -> Dict[str, Any]": "compare two sets of embeddings for debugging model changes",
    "debug_search_quality(query, results, ground_truth) -> Dict[str, Any]": "comprehensive search quality analysis",
    "normalize_vector(vec) -> np.ndarray": "L2 normalize vector to unit length",
    "encode_query(query_text) -> np.ndarray": "convert query to embedding vector",
    "detect_media_type(file_path: str) -> str": "detect media type from file extension and content",
    "extract_features(image_path: str) -> np.ndarray": "extract CLIP image embeddings",
    "transcribe_audio(audio_path: str) -> str": "transcribe audio to text using Whisper",
    "encode_multimodal_content(content: MediaContent) -> Dict[str, np.ndarray]": "generate embeddings for multi-modal content",
    "create_concept_filter(concept_description: str, example_docs: Optional[List[str]]) -> np.ndarray": "create concept vector from description and examples",
    "apply_semantic_filter(document_embeddings: np.ndarray, concept_filter: np.ndarray, similarity_threshold: float) -> np.ndarray": "filter documents by semantic similarity to concept",
    "contextualize_query(session_id: str, raw_query: str) -> str": "transform raw query using conversational context",
    "route_query(query_embedding: np.ndarray, max_shards: Optional[int]) -> QueryRoute": "determine which shards to query for optimal results",
    "process_document_update(document: Document, operation: str) -> bool": "process single document update with immediate search visibility",
    "consolidate_indices() -> bool": "move stabilized documents from fast_index to main_index",
    "handle_embedding_model_update(new_model_name: str) -> bool": "coordinate system-wide embedding model update",
    "get_autocomplete_suggestions(query, max_suggestions) -> List[str]": "generate typeahead suggestions",
    "compute_facets(results, facet_fields) -> Dict": "calculate facet counts from results",
    "highlight_query_terms(text, query_terms) -> str": "mark matched terms in result snippets",
    "record_search_analytics(query, results, response_time)": "asynchronously log search metrics",
    "handle_multi_vector_query(processed_query) -> List": "decompose complex queries"
  },
  "constants": {
    "DEFAULT_MODEL": "all-MiniLM-L6-v2",
    "EMBEDDING_DIM": "384",
    "MIN_KEYWORD_LENGTH": "3",
    "DEFAULT_MAX_RESULTS": "20",
    "AUTOCOMPLETE_TIMEOUT_MS": "100",
    "SEARCH_TIMEOUT_MS": "500",
    "MAX_QUERY_LENGTH": "500",
    "FACET_COMPUTATION_LIMIT": "1000",
    "IMAGE_EXTENSIONS": "supported image file extensions",
    "AUDIO_EXTENSIONS": "supported audio file extensions",
    "VIDEO_EXTENSIONS": "supported video file extensions",
    "DEFAULT_CONSOLIDATION_THRESHOLD": "1000 documents before consolidation",
    "MAX_CONTEXT_TURNS": "10",
    "HEALTH_CHECK_INTERVAL": "30 seconds between health checks"
  },
  "terms": {
    "lexical search": "keyword-based search using inverted indexes",
    "semantic search": "meaning-based search using vector embeddings",
    "vector embedding": "dense numerical representation of text",
    "cosine similarity": "measure of vector similarity based on angle",
    "vocabulary mismatch": "when users and documents use different terms for same concepts",
    "inverted index": "data structure mapping terms to documents containing them",
    "BM25": "ranking function for lexical search based on term frequency",
    "approximate nearest neighbor": "efficient algorithm for finding similar vectors",
    "HNSW": "Hierarchical Navigable Small World graph for vector search",
    "cross-encoder reranking": "precise but expensive ranking using transformer models",
    "hybrid search": "combining lexical and semantic search approaches",
    "query expansion": "adding synonyms and related terms to improve recall",
    "entity recognition": "identifying proper nouns and technical terms in queries",
    "intent classification": "understanding what type of search the user wants",
    "multi-vector query": "complex query with multiple semantic aspects",
    "semantic drift": "when expansion strays from original query meaning",
    "vector arithmetic": "mathematical operations on embedding vectors",
    "cache invalidation": "removing stale cached data when underlying data changes",
    "multi-stage ranking": "ranking pipeline with fast retrieval then precise reranking",
    "position bias": "tendency to click higher-ranked results regardless of relevance",
    "click-through learning": "using user interaction data to improve ranking",
    "personalization signals": "user context factors for customized ranking",
    "freshness decay": "time-based relevance score reduction",
    "faceted navigation": "Category filtering with filter counts per category",
    "autocomplete": "Typeahead suggestions with sub-100ms latency",
    "query term highlighting": "Marking matched words in result snippets",
    "search analytics": "Query tracking and result quality metrics",
    "zero-result queries": "Searches returning no matches requiring analysis",
    "response time SLA": "Service level agreement for API response latency",
    "rate limiting": "Request throttling to prevent abuse",
    "correlation ID": "unique identifier linking user reports to internal logs",
    "graceful degradation": "maintaining basic functionality when advanced features fail",
    "circuit breaker": "pattern preventing cascading failures by disabling failing components",
    "exponential backoff": "retry strategy with increasing delays between attempts",
    "index corruption": "data integrity issues in vector index requiring recovery",
    "partial results": "incomplete search results returned when some components fail",
    "fallback strategy": "alternative processing approach when primary method fails",
    "timeout budget": "allocated time limit for different processing stages",
    "edge case": "unusual input or conditions that test system boundaries",
    "query sanitization": "cleaning and normalizing user input for safe processing",
    "unicode normalization": "standardizing character representations for consistent processing",
    "intelligent truncation": "shortening queries while preserving important terms",
    "component unavailability": "temporary or permanent failure of system components",
    "fault tolerance": "system's ability to continue operating despite component failures",
    "relevance metrics": "quantitative measures of search result quality",
    "ground truth": "expert judgments of query-document relevance",
    "precision at k": "fraction of top k results that are relevant",
    "recall at k": "fraction of relevant documents found in top k",
    "NDCG": "normalized discounted cumulative gain ranking metric",
    "MAP": "mean average precision across all queries",
    "MRR": "mean reciprocal rank of first relevant result",
    "load testing": "performance testing under realistic traffic",
    "throughput": "requests per second the system can handle",
    "latency percentiles": "response time distribution measurements",
    "SLA": "service level agreement for performance targets",
    "regression testing": "detecting performance degradation from changes",
    "memory fragmentation": "inefficient memory allocation causing out-of-memory despite sufficient total memory",
    "multi-modal search": "search capability across text, image, audio, and video content",
    "cross-modal alignment": "mapping different media types to comparable embedding spaces",
    "semantic filtering": "filtering by abstract concepts rather than explicit metadata",
    "conversational search": "multi-turn search dialogue with context maintenance",
    "distributed indexing": "partitioning vector indices across multiple nodes",
    "search federation": "coordinating search across multiple independent systems",
    "real-time updates": "immediate document visibility without batch processing delays",
    "query routing": "selecting optimal subset of shards for each search query",
    "result harmonization": "normalizing relevance scores across heterogeneous sources",
    "embedding model update": "transitioning to new embedding models without service interruption"
  }
}