{
  "types": {
    "TokenType": "enum with SELECT_KEYWORD, FROM_KEYWORD, WHERE_KEYWORD, etc",
    "Token": "fields: type TokenType, value str, line int, column int",
    "ParseError": "fields: message str, line int, column int, severity ErrorSeverity",
    "TokenizerError": "extends ParseError, fields: invalid_sequence str",
    "SyntaxError": "extends ParseError, fields: parser_state str",
    "UnexpectedTokenError": "extends SyntaxError, fields: expected str, actual_token Token, suggestions List[str]",
    "Tokenizer": "fields: sql_text str, position int, line int, column int, tokens List[Token]",
    "SQLParser": "main parser API class",
    "SelectParser": "extends BaseParser with SELECT-specific parsing methods",
    "DMLParser": "fields: tokens List[Token], position int, current_token Token",
    "SelectStatement": "fields: columns Union[StarExpression, List[AliasedExpression]], table_reference TableReference, where_clause Optional[Expression]",
    "InsertStatement": "fields: table_reference TableReference, column_list Optional[List[Identifier]], value_rows List[List[Expression]]",
    "UpdateStatement": "fields: table_reference TableReference, set_assignments List[AssignmentExpression], where_clause Optional[Expression]",
    "DeleteStatement": "fields: table_reference TableReference, where_clause Optional[Expression]",
    "BinaryOperation": "fields: left ASTNode, operator TokenType, right ASTNode",
    "SourceLocation": "fields: start_line int, start_column int, end_line int, end_column int",
    "ASTNode": "base AST node interface",
    "ASTVisitor": "visitor pattern interface",
    "Identifier": "fields: name str",
    "StringLiteral": "fields: value str",
    "IntegerLiteral": "fields: value int",
    "FloatLiteral": "fields: value float",
    "NullLiteral": "no fields",
    "UnaryOperation": "fields: operator TokenType, operand ASTNode",
    "FunctionCall": "function call AST node",
    "BaseParser": "fields: tokens List[Token], position int, current_token Token",
    "StarExpression": "represents SELECT * wildcard",
    "AliasedExpression": "fields: expression ASTNode, alias Optional[str]",
    "TableReference": "fields: table Union[Identifier, QualifiedIdentifier], alias Optional[str]",
    "QualifiedIdentifier": "fields: qualifier str, name str",
    "ExpressionParser": "fields: tokens List[Token], position int, current_token Token",
    "OperatorInfo": "fields: precedence Precedence, is_right_associative bool",
    "Precedence": "IntEnum with OR=1, AND=2, NOT=3, IS_NULL=4, COMPARISON=5, ADDITION=6, MULTIPLICATION=7, UNARY=8, PRIMARY=9",
    "AssignmentExpression": "fields: target_column Identifier, assigned_value Expression",
    "StructuralValidationError": "extends ParseError, fields: source_location SourceLocation, violation_type str, related_elements List[ASTNode]",
    "ErrorSeverity": "enum ERROR, WARNING, INFO",
    "RecoveryMode": "enum NORMAL, PANIC, SUPPRESSED",
    "SynchronizationPoint": "fields: token_types Set[TokenType], recovery_action Callable, confidence_level float",
    "ErrorRecoveryManager": "fields: recovery_mode RecoveryMode, error_count int, max_errors int",
    "TokenExpectation": "fields: token_type TokenType, value str, line int, column int",
    "ASTExpectation": "fields: node_type str, properties Dict[str, Any], children List[ASTExpectation]",
    "TokenizerTestHelper": "helper class for tokenizer testing",
    "ParserTestHelper": "helper class for parser testing",
    "FixtureLoader": "load test cases from external files",
    "MilestoneValidator": "validation logic for milestone acceptance criteria",
    "TokenizerDebugger": "fields: tokenizer Tokenizer, debug_level DebugLevel, diagnostics List[TokenizerDiagnostic], logger logging.Logger",
    "ParserDebugger": "fields: parser SQLParser, trace_entries List[ParseTraceEntry], ast_snapshots Dict[int, ASTNode]",
    "TokenizerDiagnostic": "fields: position int, character Optional[str], token_start int, token_value str, token_type TokenType, issues List[str]",
    "ParseTraceEntry": "fields: function_name str, entry_position int, entry_token Optional[Token], exit_position Optional[int], exit_token Optional[Token], result_node Optional[ASTNode], error Optional[ParseError], duration_ms float",
    "DebugLevel": "enum with BASIC, DETAILED, VERBOSE",
    "ExtensionRegistry": "fields: _extensions Dict[str, ExtensionInfo], _keyword_extensions Dict[str, TokenType], _operator_extensions Dict[TokenType, OperatorInfo], _statement_parsers Dict[TokenType, Type[BaseParser]], _validation_rules List[ValidationRule]",
    "ExtensionInfo": "fields: name str, extension_type ExtensionType, priority int, dependencies List[str], registration_callback Callable",
    "ExtensionType": "enum KEYWORD, OPERATOR, STATEMENT, EXPRESSION, VALIDATOR",
    "JsonLiteral": "fields: json_text str, parsed_value Optional[Any]",
    "JsonExtractExpression": "fields: json_expression Expression, path_expression Expression, extract_as_text bool",
    "MergeStatement": "fields: target_table TableReference, source_table TableReference, merge_condition Expression, merge_actions List[MergeAction]",
    "MergeAction": "fields: when_condition Optional[Expression]",
    "MergeInsertAction": "fields: column_list Optional[List[Identifier]], value_list List[Expression]",
    "MergeUpdateAction": "fields: set_assignments List[AssignmentExpression]",
    "WhenClause": "fields: when_condition Expression, then_result Expression",
    "CaseExpression": "fields: case_expression Optional[Expression], when_clauses List[WhenClause], else_expression Optional[Expression]",
    "MergeParser": "extends BaseParser",
    "CaseExpressionParser": "fields: expression_parser ExpressionParser",
    "ValidationRule": "interface for AST validation",
    "ParserExtension": "interface for parser extensions",
    "SQL_KEYWORDS": "mapping from uppercase keyword strings to TokenType values",
    "OPERATOR_PRECEDENCE": "Dict mapping TokenType to OperatorInfo",
    "Expression": "base expression AST node"
  },
  "methods": {
    "tokenize()": "main entry point returning token list",
    "_current_char()": "returns current character or None",
    "_peek_char(offset)": "look ahead without advancing",
    "_advance()": "move to next character",
    "_skip_whitespace()": "consume whitespace characters",
    "_read_string_literal(quote_char)": "parse quoted string",
    "tokenize() -> List[Token]": "main tokenization entry point",
    "parse(sql_text) -> ASTNode": "main parsing entry point",
    "tokenize_only(sql_text) -> List[Token]": "tokenization without parsing for debugging",
    "node_type() -> str": "returns node type identifier",
    "children() -> List[ASTNode]": "returns direct child nodes",
    "accept(visitor: ASTVisitor) -> Any": "visitor pattern implementation",
    "source_location() -> SourceLocation": "get position information for error",
    "is_keyword() -> bool": "checks if token is keyword",
    "is_operator() -> bool": "checks if token is operator",
    "is_literal() -> bool": "checks if token is literal",
    "qualified_name() -> str": "returns full qualified identifier name",
    "_current_char() -> Optional[str]": "returns current character or None",
    "_peek_char(offset: int) -> Optional[str]": "look ahead without advancing",
    "_advance() -> None": "move to next character",
    "_skip_whitespace() -> None": "consume whitespace characters",
    "_read_string_literal(quote_char: str) -> Token": "parse quoted string",
    "_parse_identifier_or_keyword() -> Token": "parse identifier or keyword",
    "_parse_number() -> Token": "parse numeric literal",
    "_parse_operator() -> Token": "parse operator token",
    "_create_token(type, value, line, column) -> Token": "create and add token",
    "lookup_keyword(text: str) -> TokenType": "case-insensitive keyword lookup",
    "parse_select_statement() -> SelectStatement": "main SELECT parsing entry point",
    "parse_column_list() -> Union[StarExpression, List[AliasedExpression]]": "parse column specifications or star wildcard",
    "parse_column_spec() -> AliasedExpression": "parse individual column with optional alias",
    "parse_table_reference() -> TableReference": "parse FROM clause table with optional alias",
    "parse_identifier() -> str": "parse and return identifier token value",
    "peek_token(offset: int) -> Optional[Token]": "look ahead at future tokens without consuming",
    "consume_token() -> Token": "advance position and return consumed token",
    "expect_token(expected_type: TokenType) -> Token": "consume expected token or report error",
    "is_at_end() -> bool": "check if parser reached end of tokens",
    "create_source_location(start_token: Token, end_token: Optional[Token]) -> SourceLocation": "create position info from tokens",
    "parse_expression(min_precedence int) -> ASTNode": "precedence climbing parser",
    "parse_primary_expression() -> ASTNode": "parse literals, identifiers, unary ops, parentheses",
    "parse_identifier_expression() -> ASTNode": "parse simple or qualified identifiers",
    "parse_null_test_expression(operand ASTNode) -> ASTNode": "parse IS NULL / IS NOT NULL",
    "parse_parenthesized_expression() -> ASTNode": "parse expressions within parentheses",
    "get_operator_precedence(token_type TokenType) -> Optional[int]": "lookup operator precedence level",
    "is_right_associative(token_type TokenType) -> bool": "check operator associativity",
    "right_binding_power() -> int": "calculate binding power for precedence climbing",
    "parse_insert_statement() -> InsertStatement": "Parse INSERT INTO table [(columns)] VALUES (values) statement",
    "parse_update_statement() -> UpdateStatement": "Parse UPDATE table SET assignments [WHERE condition] statement",
    "parse_delete_statement() -> DeleteStatement": "Parse DELETE FROM table [WHERE condition] statement",
    "parse_assignment_list() -> List[AssignmentExpression]": "Parse comma-separated list of column = value assignments",
    "parse_assignment_expression() -> AssignmentExpression": "Parse single column = value assignment",
    "parse_value_row_list() -> List[List[Expression]]": "Parse comma-separated list of (value, value, ...) rows",
    "parse_value_row() -> List[Expression]": "Parse single (value, value, ...) row",
    "validate_column_value_consistency(column_count: int, value_rows: List[List[Expression]]) -> None": "Validate that each value row has correct number of expressions",
    "report_error(error: ParseError)": "record error and potentially enter recovery mode",
    "synchronize_after_error()": "skip tokens until synchronization point found",
    "enter_panic_mode(error: ParseError)": "enter error recovery mode",
    "attempt_synchronization(current_token: Token) -> bool": "try to synchronize at current position",
    "should_suppress_error(error: ParseError) -> bool": "determine if error is likely cascade effect",
    "with_suggestion(suggestion: str) -> ParseError": "add suggestion to error",
    "with_context(context: str) -> ParseError": "add parsing context to error",
    "create_error_with_context(message: str, context: str) -> SyntaxError": "create error with full context information",
    "matches(actual_token: Token) -> bool": "check if actual token matches expectation",
    "matches(actual_node: ASTNode) -> bool": "recursively validate AST node structure",
    "assert_tokens_match(expected: List[TokenExpectation], actual: List[Token])": "assert actual tokens match expected list",
    "tokenize_and_validate(sql_text: str, expected_tokens: List[TokenExpectation])": "tokenize SQL and validate against expected",
    "parse_and_validate(sql_text: str, expected_ast: ASTExpectation) -> ASTNode": "parse SQL and validate against expected AST",
    "assert_parse_error(sql_text: str, expected_error_type: type, expected_message_content: Optional[str])": "assert parsing raises expected error",
    "load_sql_cases(filename: str) -> List[str]": "load SQL test cases from file",
    "load_expected_ast(filename: str) -> Dict[str, ASTExpectation]": "load expected AST structures from JSON",
    "validate_milestone_1() -> bool": "validate tokenizer milestone acceptance criteria",
    "validate_milestone_2() -> bool": "validate SELECT parser milestone acceptance criteria",
    "validate_milestone_3() -> bool": "validate WHERE clause expression parser milestone",
    "validate_milestone_4() -> bool": "validate INSERT/UPDATE/DELETE parser milestone",
    "debug_tokenization(sql_text: str) -> List[Token]": "perform tokenization with comprehensive debugging output",
    "analyze_keyword_recognition(test_cases: Dict[str, TokenType]) -> Dict[str, bool]": "test keyword recognition with various case combinations",
    "inspect_token_boundaries(sql_text: str) -> List[TokenizerDiagnostic]": "analyze token boundary detection for potential issues",
    "validate_position_tracking(multiline_sql: str) -> bool": "verify line and column tracking accuracy",
    "debug_parse_statement(sql_text: str) -> Optional[ASTNode]": "parse SQL statement with comprehensive debugging trace",
    "analyze_precedence_parsing(expression_sql: str) -> Dict[str, Any]": "deep analysis of expression parsing with precedence validation",
    "inspect_ast_structure(root_node: ASTNode, max_depth: int) -> str": "generate detailed AST structure representation",
    "validate_error_recovery(malformed_sql: str) -> Dict[str, Any]": "test error recovery behavior with malformed SQL",
    "trace_function(function_name: str)": "context manager for tracing parsing function execution",
    "tokenize_only(sql_text: str) -> List[Token]": "tokenization without parsing for debugging",
    "register_extension(extension_info: ExtensionInfo)": "Register a new parser extension with dependency checking",
    "get_keyword_token_type(keyword: str) -> Optional[TokenType]": "Look up extended keyword token types beyond core SQL keywords",
    "get_statement_parser(token_type: TokenType) -> Optional[Type[BaseParser]]": "Retrieve parser class for extended statement types",
    "get_validation_rules() -> List[ValidationRule]": "Return all registered validation rules in execution order",
    "register_json_extension(registry: ExtensionRegistry)": "Register JSON parsing extensions with the parser",
    "parse_merge_statement() -> MergeStatement": "main MERGE parsing entry point",
    "parse_when_clause() -> MergeAction": "parse individual WHEN clauses",
    "parse_merge_insert_action(condition: Optional[Expression]) -> MergeInsertAction": "parse INSERT action",
    "parse_merge_update_action(condition: Optional[Expression]) -> MergeUpdateAction": "parse UPDATE action",
    "parse_case_expression() -> CaseExpression": "parse complete CASE expression",
    "parse_when_clause(is_simple_case: bool) -> WhenClause": "parse WHEN condition/value THEN result clause",
    "integrate_with_expression_parser()": "register CASE expression parsing with main expression parser",
    "is_simple_case() -> bool": "check if simple CASE or searched CASE"
  },
  "constants": {
    "SQL_KEYWORDS": "mapping from uppercase keyword strings to TokenType values",
    "TokenType": "SELECT_KEYWORD, FROM_KEYWORD, WHERE_KEYWORD, IDENTIFIER, ASTERISK, COMMA, DOT, AS_KEYWORD",
    "OPERATOR_PRECEDENCE": "Dict mapping TokenType to OperatorInfo",
    "Precedence.LOWEST": "0",
    "Precedence.OR": "precedence level 1",
    "Precedence.AND": "precedence level 2",
    "Precedence.COMPARISON": "precedence level 5",
    "TokenType.INSERT_KEYWORD": "INSERT keyword token",
    "TokenType.UPDATE_KEYWORD": "UPDATE keyword token",
    "TokenType.DELETE_KEYWORD": "DELETE keyword token",
    "TokenType.VALUES_KEYWORD": "VALUES keyword token",
    "TokenType.SET_KEYWORD": "SET keyword token",
    "MAX_ERRORS": "10",
    "CASCADE_SUPPRESSION_DISTANCE": "3",
    "ErrorSeverity.ERROR": "error level severity",
    "ErrorSeverity.WARNING": "warning level severity",
    "RecoveryMode.NORMAL": "normal parsing mode",
    "RecoveryMode.PANIC": "error recovery mode",
    "TokenType.SELECT_KEYWORD": "SELECT keyword token",
    "TokenType.FROM_KEYWORD": "FROM keyword token",
    "TokenType.WHERE_KEYWORD": "WHERE keyword token",
    "TokenType.IDENTIFIER": "identifier token type",
    "TokenType.ASTERISK": "asterisk token for SELECT *",
    "TokenType.COMMA": "comma punctuation token",
    "TokenType.DOT": "dot punctuation for qualified names",
    "TokenType.AS_KEYWORD": "AS keyword token",
    "TokenType.STRING_LITERAL": "string literal token type",
    "TokenType.INTEGER_LITERAL": "integer literal token type",
    "TokenType.FLOAT_LITERAL": "float literal token type",
    "TokenType.EQUALS": "equals operator token",
    "TokenType.LESS_THAN": "less than operator token",
    "TokenType.GREATER_THAN": "greater than operator token",
    "TokenType.PLUS": "plus operator token",
    "JSON_KEYWORD": "JSON keyword token",
    "JSON_EXTRACT_OPERATOR": "JSON -> operator token",
    "JSON_EXTRACT_TEXT_OPERATOR": "JSON ->> operator token",
    "MERGE_KEYWORD": "MERGE keyword token",
    "INTO_KEYWORD": "INTO keyword token",
    "USING_KEYWORD": "USING keyword token",
    "ON_KEYWORD": "ON keyword token",
    "WHEN_KEYWORD": "WHEN keyword token",
    "MATCHED_KEYWORD": "MATCHED keyword token",
    "THEN_KEYWORD": "THEN keyword token",
    "CASE_KEYWORD": "CASE keyword token",
    "END_KEYWORD": "END keyword token",
    "ELSE_KEYWORD": "ELSE keyword token",
    "WITH_KEYWORD": "WITH keyword token",
    "WINDOW_KEYWORD": "WINDOW keyword token",
    "OVER_KEYWORD": "OVER keyword token"
  },
  "terms": {
    "tokenization": "breaking text into tokens",
    "lexical analysis": "identifying token types",
    "Abstract Syntax Tree": "hierarchical representation of parsed SQL structure",
    "recursive descent": "top-down parsing where each grammar rule becomes a function",
    "precedence climbing": "expression parsing algorithm using recursive precedence thresholds",
    "lookahead": "examining future tokens without consuming them",
    "functional goals": "what the parser must accomplish",
    "non-functional goals": "quality and performance requirements",
    "explicit non-goals": "features intentionally excluded from scope",
    "maximal munch": "longest possible token matching strategy",
    "three-valued logic": "SQL boolean logic supporting TRUE, FALSE, NULL values",
    "visitor pattern": "extensible tree traversal technique",
    "character-by-character scanning": "examining input one character at a time",
    "method dispatch": "calling specialized parsing functions based on character type",
    "case-insensitive": "treating upper and lower case as equivalent",
    "escape sequence": "special character representations like \\n, \\'",
    "doubled quote convention": "using '' to represent literal quote in string",
    "look-ahead": "examining future tokens without consuming them",
    "position tracking": "maintaining line and column information for error reporting",
    "grammar rule": "formal definition of valid syntax patterns",
    "token consumption": "advancing parser position after recognizing expected tokens",
    "qualified identifier": "table.column or schema.table notation",
    "implicit alias": "alias without AS keyword",
    "explicit alias": "alias with AS keyword",
    "star wildcard": "SELECT * representing all columns",
    "binding power": "precedence value used to determine operator parsing order",
    "associativity": "rule determining how equal-precedence operators group",
    "primary expression": "atomic expression that cannot be decomposed by operators",
    "precedence override": "using parentheses to change natural operator evaluation order",
    "unary operation": "single-operand operation like NOT or unary minus",
    "binary operation": "two-operand operation like AND, OR, comparison operators",
    "data modification language": "DML statements that change database state",
    "assignment expression": "column = value expression in SET clauses",
    "value row": "parenthesized list of expressions in VALUES clause",
    "structural validation": "checking logical consistency of parsed statements",
    "statement-specific parser": "parser class specialized for one statement type",
    "column-value correspondence": "matching column count with value count in INSERT",
    "safety-conscious parsing": "flagging potentially dangerous operations in AST",
    "panic-mode recovery": "error recovery strategy that skips tokens until synchronization point",
    "synchronization points": "reliable positions where parsing can resume after errors",
    "cascade errors": "spurious errors caused by earlier parsing failures",
    "error suppression": "preventing false positive error reports during recovery",
    "error recovery": "continuing parsing after encountering syntax errors",
    "tokenization errors": "lexical analysis failures during character-to-token conversion",
    "syntax errors": "grammar rule violations during parsing",
    "precedence errors": "operator precedence violations in expressions",
    "component unit testing": "testing individual parser components in isolation",
    "end-to-end parser testing": "testing complete SQL statement parsing with complex queries",
    "milestone validation checkpoints": "specific tests and expected outputs to verify each milestone",
    "edge case coverage": "testing boundary conditions and unusual syntax",
    "error propagation testing": "verifying errors detected at any stage propagate correctly",
    "performance characteristic testing": "validating parser maintains reasonable performance",
    "test fixture": "external test data files and expected results",
    "acceptance criteria": "specific requirements that must be met for milestone completion",
    "grammar extension patterns": "systematic approaches to adding new language features",
    "plugin architecture": "modular system for registering parser extensions",
    "grammar composition": "reusing existing parsing rules in new combinations",
    "semantic validation": "post-parsing validation of logical consistency",
    "extension registry": "central system for managing parser extensions",
    "keyword extension": "adding new SQL keywords to parser vocabulary",
    "operator extension": "adding new operators with appropriate precedence",
    "statement parser extension": "adding entirely new SQL statement types",
    "expression parser extension": "extending expression parsing capabilities",
    "AST composition": "building complex tree structures from simpler nodes",
    "scope chain management": "tracking variable visibility across nested contexts",
    "context-sensitive parsing": "parsing behavior that depends on current parsing state",
    "left-associative grouping": "operators group left-to-right with equal precedence",
    "table-driven keyword recognition": "using lookup tables instead of hardcoded conditions",
    "dispatch table pattern": "routing different inputs to appropriate handlers",
    "symbol table stack": "hierarchical symbol resolution for nested scopes",
    "validation phases": "multiple passes for different types of error checking",
    "feature modularity": "organizing functionality into independent components"
  }
}