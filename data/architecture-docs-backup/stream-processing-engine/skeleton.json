{
  "title": "Project Chronos: Design Document for a Real-Time Stream Processing Engine",
  "overview": "This document outlines the design and implementation of Chronos, a real-time stream processing engine that ingests unbounded data streams, applies transformations and time-based aggregations (windowing), and delivers results with exactly-once semantics. The key architectural challenge is maintaining high throughput, low latency, and fault-tolerant correctness in a distributed environment where data arrives continuously and out-of-order.",
  "sections": [
    {
      "id": "context-problem",
      "title": "Context and Problem Statement",
      "summary": "Establishes the need for stream processing, contrasts it with batch processing, and explains the core difficulties of handling infinite, unordered data.",
      "subsections": [
        {
          "id": "context-analogy",
          "title": "Mental Model: The Never-Ending Conveyor Belt",
          "summary": "Introduces stream processing using the analogy of sorting and counting items on a fast-moving, continuous conveyor belt where items can arrive in the wrong order."
        },
        {
          "id": "context-problem-details",
          "title": "The Technical Challenge",
          "summary": "Defines the core problems: infinite data, out-of-order events, stateful computation, and the need for fault tolerance without stopping the world."
        },
        {
          "id": "context-existing-approaches",
          "title": "Landscape of Existing Approaches",
          "summary": "Compares the Lambda and Kappa architectures, and introduces established systems like Apache Flink and Spark Streaming via a comparison table."
        }
      ]
    },
    {
      "id": "goals-nongoals",
      "title": "Goals and Non-Goals",
      "summary": "Clearly scopes the project by defining the must-have capabilities and explicit exclusions.",
      "subsections": [
        {
          "id": "goals-list",
          "title": "Goals",
          "summary": "Lists functional and non-functional requirements: core stream operators, event-time processing, state management, exactly-once semantics, and horizontal scalability."
        },
        {
          "id": "nongoals-list",
          "title": "Non-Goals",
          "summary": "Explicitly states what is out of scope: built-in complex event processing (CEP), dynamic job graph modification, a SQL API, and cluster resource management."
        }
      ]
    },
    {
      "id": "high-level-architecture",
      "title": "High-Level Architecture",
      "summary": "Presents the top-level component view of the Chronos engine, its execution model, and the recommended file structure for the codebase.",
      "subsections": [
        {
          "id": "architecture-component-overview",
          "title": "Component Overview and Data Flow",
          "summary": "Describes the roles of the Job Client, JobManager, TaskManagers, Sources, Sinks, and State Backend in processing a data stream job."
        },
        {
          "id": "architecture-file-structure",
          "title": "Recommended File/Module Structure",
          "summary": "Provides a scaffold Java package structure to organize the codebase, separating API, runtime, state, and coordination logic."
        }
      ]
    },
    {
      "id": "data-model",
      "title": "Data Model",
      "summary": "Defines the core data types, messages, and structures that flow through the system, from user records to internal coordination barriers.",
      "subsections": [
        {
          "id": "data-model-core-types",
          "title": "Core Types and Stream Elements",
          "summary": "Describes the Record, StreamRecord (with timestamp), Watermark, and CheckpointBarrier as foundational types, presented in a table."
        },
        {
          "id": "data-model-internal-structures",
          "title": "Internal Structures and Metadata",
          "summary": "Defines types for operator state (ValueState, ListState), window metadata (TimeWindow, WindowState), and job execution metadata (JobGraph, ExecutionVertex)."
        }
      ]
    },
    {
      "id": "component-stream-abstraction",
      "title": "Component Design: Stream Abstraction and API (Milestone 1)",
      "summary": "Designs the user-facing DataStream API and the runtime DAG of operators that executes transformations like map and filter.",
      "subsections": [
        {
          "id": "stream-mental-model",
          "title": "Mental Model: The Assembly Line",
          "summary": "Explains the DataStream API and operator chain as an assembly line where each workstation (operator) transforms the item before passing it on."
        },
        {
          "id": "stream-api-interface",
          "title": "DataStream API Interface",
          "summary": "Details the methods of the DataStream class (map, filter, keyBy) and their semantics, presented in a method table."
        },
        {
          "id": "stream-internal-dag",
          "title": "Internal DAG and Operator Chain",
          "summary": "Describes how API calls are compiled into a JobGraph of StreamTask vertices, and the mechanism for operator chaining to reduce overhead."
        },
        {
          "id": "stream-adr-partitioning",
          "title": "ADR: Keyed vs. Non-Keyed Stream Partitioning",
          "summary": "Architecture Decision Record evaluating options for distributing data across parallel operator instances, selecting key-based hash partitioning."
        },
        {
          "id": "stream-pitfalls",
          "title": "Common Pitfalls",
          "summary": "Lists mistakes like materializing infinite streams, improper operator fusion, and thread-unsafe state in map functions."
        },
        {
          "id": "stream-implementation-guidance",
          "title": "Implementation Guidance",
          "summary": "Provides starter code for a simple record iterator, skeleton code for the MapFunction and KeyedStream classes with TODOs, and Milestone 1 checkpoint tests."
        }
      ]
    },
    {
      "id": "component-windowing",
      "title": "Component Design: Windowing (Milestone 2)",
      "summary": "Designs the windowing subsystem that groups infinite streams into finite chunks of time for aggregation.",
      "subsections": [
        {
          "id": "windowing-mental-model",
          "title": "Mental Model: The Time-Keeper's Calendar",
          "summary": "Explains tumbling, sliding, and session windows using the analogy of marking appointments (events) on a calendar with different rules for page turns (window boundaries)."
        },
        {
          "id": "windowing-assigner-trigger",
          "title": "Window Assigners and Triggers",
          "summary": "Describes the WindowAssigner and Trigger interfaces, and the algorithm for assigning elements to windows and deciding when to fire a window computation."
        },
        {
          "id": "windowing-adr-lateness",
          "title": "ADR: Handling Late Data - Side Output vs. Allowed Lateness",
          "summary": "Evaluates strategies for dealing with events that arrive after their window has been evaluated, deciding to implement both with allowed lateness as the primary mechanism."
        },
        {
          "id": "windowing-pitfalls",
          "title": "Common Pitfalls",
          "summary": "Highlights issues like confusing event vs. processing time, memory leaks from unclosed sessions, and incorrect trigger logic for sliding windows."
        },
        {
          "id": "windowing-implementation-guidance",
          "title": "Implementation Guidance",
          "summary": "Provides starter code for a window metadata class, skeleton code for TumblingWindowAssigner and CountTrigger with TODOs, and Milestone 2 checkpoint tests."
        }
      ]
    },
    {
      "id": "component-event-time-watermarks",
      "title": "Component Design: Event Time and Watermarks (Milestone 3)",
      "summary": "Designs the mechanism to handle event-time semantics and progress tracking using watermarks to deal with out-of-order data.",
      "subsections": [
        {
          "id": "watermark-mental-model",
          "title": "Mental Model: The Punctuality Clock",
          "summary": "Explains event time, processing time, and watermarks using the analogy of a clock that ticks forward based on observed event timestamps, signaling when it's (reasonably) safe to finalize a time period."
        },
        {
          "id": "watermark-generation-propagation",
          "title": "Watermark Generation and Propagation",
          "summary": "Details periodic and punctuated watermark generators, the algorithm for propagating the minimum watermark across parallel streams, and idle source detection."
        },
        {
          "id": "watermark-adr-strategy",
          "title": "ADR: Watermark Generation Strategy - Bounded Out-of-Orderness",
          "summary": "Evaluates watermark strategies (perfect, heuristic, bounded) and selects the bounded-out-of-orderness model for its balance of simplicity and practicality."
        },
        {
          "id": "watermark-pitfalls",
          "title": "Common Pitfalls",
          "summary": "Warns about watermark stalls from idle partitions, using processing time for watermarks, and not configuring sufficient allowed lateness."
        },
        {
          "id": "watermark-implementation-guidance",
          "title": "Implementation Guidance",
          "summary": "Provides starter code for a watermark comparator, skeleton code for BoundedOutOfOrdernessGenerator and WatermarkPropagator with TODOs, and Milestone 3 checkpoint tests."
        }
      ]
    },
    {
      "id": "component-stateful-processing",
      "title": "Component Design: Stateful Processing and Checkpointing (Milestone 4)",
      "summary": "Designs the state management layer and the distributed snapshot (checkpoint) mechanism for fault tolerance.",
      "subsections": [
        {
          "id": "state-mental-model",
          "title": "Mental Model: The Operator's Notebook",
          "summary": "Explains operator state as a personal notebook where each keyed operator instance keeps its own notes (state), and checkpointing as periodically photocopying all notebooks for safekeeping."
        },
        {
          "id": "state-backends-checkpointing",
          "title": "State Backends and Checkpointing Algorithm",
          "summary": "Describes the StateBackend interface, the Chandy-Lamport algorithm using checkpoint barriers, and the steps for asynchronous snapshot creation and recovery."
        },
        {
          "id": "state-adr-storage",
          "title": "ADR: State Storage - In-Heap vs. RocksDB Backend",
          "summary": "Evaluates state storage options, deciding to implement both a fast in-heap backend for small state and a RocksDB backend for large, spillable state."
        },
        {
          "id": "state-pitfalls",
          "title": "Common Pitfalls",
          "summary": "Highlights memory leaks from uncleared state, blocking during checkpointing, and race conditions in concurrent state access."
        },
        {
          "id": "state-implementation-guidance",
          "title": "Implementation Guidance",
          "summary": "Provides starter code for a simple in-memory state backend, skeleton code for CheckpointCoordinator and ValueStateDescriptor with TODOs, and Milestone 4 checkpoint tests."
        }
      ]
    },
    {
      "id": "component-exactly-once",
      "title": "Component Design: Exactly-Once Semantics (Milestone 5)",
      "summary": "Designs the end-to-end exactly-once guarantee, extending checkpointing with transactional sink coordination.",
      "subsections": [
        {
          "id": "exactly-once-mental-model",
          "title": "Mental Model: The Atomic Bank Transfer",
          "summary": "Explains the two-phase commit protocol using the analogy of a coordinated bank transfer that either fully completes or fully rolls back, ensuring no money is duplicated or lost."
        },
        {
          "id": "exactly-once-2pc-sinks",
          "title": "Two-Phase Commit Sinks",
          "summary": "Details the algorithm for a transactional sink: pre-committing to external storage during checkpoint, and finalizing only upon checkpoint completion notification."
        },
        {
          "id": "exactly-once-adr-guarantee-scope",
          "title": "ADR: End-to-End Guarantee Scope - Idempotent vs. Transactional Sinks",
          "summary": "Evaluates methods for sink-side exactly-once, deciding to implement a two-phase commit sink pattern for maximal guarantee, while acknowledging idempotent writes as a valid alternative."
        },
        {
          "id": "exactly-once-pitfalls",
          "title": "Common Pitfalls",
          "summary": "Warns about checkpoint alignment causing backpressure, transaction timeouts, and using non-idempotent sinks without the proper protocol."
        },
        {
          "id": "exactly-once-implementation-guidance",
          "title": "Implementation Guidance",
          "summary": "Provides starter code for a simple transaction log, skeleton code for TwoPhaseCommitSinkFunction and TransactionCoordinator with TODOs, and Milestone 5 end-to-end verification tests."
        }
      ]
    },
    {
      "id": "interactions-data-flow",
      "title": "Interactions and Data Flow",
      "summary": "Traces the journey of a record and a checkpoint through the system using sequence diagrams and descriptions of key control flows.",
      "subsections": [
        {
          "id": "interactions-record-flow",
          "title": "Record Processing Flow",
          "summary": "Sequentially describes the path of a data record from source through operators, window assignment, state updates, and finally to the sink."
        },
        {
          "id": "interactions-checkpoint-flow",
          "title": "Checkpoint Coordination Flow",
          "summary": "Describes the sequence of injecting barriers, aligning input streams, taking snapshots, and finalizing the checkpoint across the distributed job."
        }
      ]
    },
    {
      "id": "error-handling-edge-cases",
      "title": "Error Handling and Edge Cases",
      "summary": "Catalogues potential failure modes and the system's strategies for detection, mitigation, and recovery.",
      "subsections": [
        {
          "id": "error-categories",
          "title": "Failure Categories and Strategies",
          "summary": "Groups failures into TaskManager crashes, network partitions, and external system failures, detailing recovery via checkpoint restart, watermark advancement, and transaction abort/retry."
        },
        {
          "id": "edge-case-catalog",
          "title": "Edge Case Catalog",
          "summary": "Lists and addresses scenarios like poison pills (malformed records), sudden source termination, and massive late data avalanches."
        }
      ]
    },
    {
      "id": "testing-strategy",
      "title": "Testing Strategy",
      "summary": "Outlines a multi-layered approach to verifying correctness, from unit tests to integration and property-based tests.",
      "subsections": [
        {
          "id": "testing-levels",
          "title": "Testing Levels and Techniques",
          "summary": "Describes unit testing for operators, integration testing for job DAGs, and property-based testing for invariants like exactly-once."
        },
        {
          "id": "testing-milestone-checkpoints",
          "title": "Milestone Implementation Checkpoints",
          "summary": "For each of the five milestones, provides a concrete test scenario, expected output, and command to run to verify progress."
        }
      ]
    },
    {
      "id": "debugging-guide",
      "title": "Debugging Guide",
      "summary": "A practical manual for diagnosing and fixing common issues encountered during development, structured as symptom-cause-fix tables.",
      "subsections": [
        {
          "id": "debugging-symptom-table",
          "title": "Common Bug Symptom Table",
          "summary": "Table listing symptoms (e.g., 'Watermark not advancing', 'Duplicate output after crash'), their likely causes, and steps to fix them."
        },
        {
          "id": "debugging-techniques",
          "title": "Diagnostic Techniques and Tools",
          "summary": "Recommends approaches like adding strategic logging for watermarks/barriers, visualizing the operator graph, and using a debug state backend."
        }
      ]
    },
    {
      "id": "future-extensions",
      "title": "Future Extensions",
      "summary": "Suggests potential enhancements to the system that the current architecture is designed to accommodate.",
      "subsections": [
        {
          "id": "extensions-list",
          "title": "Possible Extensions",
          "summary": "Lists ideas like dynamic scaling (rescaling), a Table/SQL API, complex event processing (CEP) library, and support for batch data as a bounded stream."
        }
      ]
    },
    {
      "id": "glossary",
      "title": "Glossary",
      "summary": "Defines key technical terms, acronyms, and domain-specific vocabulary used throughout the document.",
      "subsections": [
        {
          "id": "glossary-table",
          "title": "Terminology Reference",
          "summary": "Alphabetical table of terms from 'Allowed Lateness' to 'Watermark', with clear definitions and references to the section where they are first introduced."
        }
      ]
    }
  ],
  "diagrams": [
    {
      "id": "arch-component",
      "title": "Chronos System Component Architecture",
      "description": "Shows the high-level components: Job Client, JobManager, TaskManagers (with slots), Sources, Sinks, and State Backend. Illustrates deployment relationships and data/control flow between them.",
      "type": "component",
      "relevant_sections": [
        "high-level-architecture"
      ]
    },
    {
      "id": "data-model-class",
      "title": "Core Data Model Class Relationships",
      "description": "A class diagram showing relationships between core types: Record, StreamRecord, Watermark, CheckpointBarrier, Window, and operator State interfaces (ValueState, ListState).",
      "type": "class",
      "relevant_sections": [
        "data-model"
      ]
    },
    {
      "id": "dag-execution",
      "title": "Job Graph to Execution Graph",
      "description": "Shows the transformation from a user-defined operator chain (Source -> Map -> KeyBy -> Window -> Sink) into a parallelized JobGraph and then into the physical ExecutionGraph deployed across TaskManager slots.",
      "type": "flowchart",
      "relevant_sections": [
        "component-stream-abstraction"
      ]
    },
    {
      "id": "window-assignment-flow",
      "title": "Window Assignment and Triggering Flowchart",
      "description": "A flowchart detailing the steps for an incoming element: timestamp extraction, window assignment by WindowAssigner, addition to WindowState, evaluation of Trigger, and eventual window function invocation and emission.",
      "type": "flowchart",
      "relevant_sections": [
        "component-windowing"
      ]
    },
    {
      "id": "watermark-propagation-seq",
      "title": "Watermark Propagation Across Parallel Streams",
      "description": "A sequence diagram showing how watermarks are generated at a source, propagated downstream, and how an operator (like a window) waits for the minimum watermark from all its input partitions before advancing its own clock.",
      "type": "sequence",
      "relevant_sections": [
        "component-event-time-watermarks"
      ]
    },
    {
      "id": "checkpoint-sequence",
      "title": "Distributed Checkpoint Sequence Diagram",
      "description": "Sequence diagram illustrating the Chandy-Lamport algorithm: JobManager injects checkpoint barrier, TaskManager aligns inputs, takes asynchronous snapshot of state, and acknowledges completion back to JobManager.",
      "type": "sequence",
      "relevant_sections": [
        "component-stateful-processing",
        "interactions-data-flow"
      ]
    },
    {
      "id": "two-phase-commit-state",
      "title": "Two-Phase Commit Sink State Machine",
      "description": "A state machine diagram for a transactional sink operator, showing states: INITIALIZING, PRE_COMMITTING, COMMITTING, ABORTING, and transitions based on events like 'checkpoint started', 'checkpoint completed', 'failure detected'.",
      "type": "state-machine",
      "relevant_sections": [
        "component-exactly-once"
      ]
    }
  ]
}