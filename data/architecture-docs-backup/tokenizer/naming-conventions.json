{
  "types": {
    "Token": "token_type: TokenType, lexeme: str, position: Position, literal: Any",
    "TokenType": "enumeration of token categories",
    "Position": "line: int, column: int",
    "Scanner": "source: str, tokens: List[Token], start: int, current: int, position: Position, keywords: Dict[str, TokenType]",
    "LexicalError": "message: str, position: Position, error_type: ErrorType, severity: ErrorSeverity, context: str, suggestion: str",
    "StringProcessor": "class for string and escape handling",
    "CommentProcessor": "class for comment detection",
    "ErrorType": "enumeration of error categories",
    "ErrorSeverity": "enumeration of severity levels",
    "ErrorReporter": "source_lines: List[str], error_count: int, warning_count: int",
    "ErrorRecovery": "scanner: Scanner, recovery_mode: bool, suppression_context: str",
    "LexerDebugger": "enabled: bool, trace_chars: bool, trace_tokens: bool, trace_states: bool, char_count: int, token_count: int",
    "ErrorPatternAnalyzer": "error_patterns: dict, fix_history: dict",
    "TokenStream": "current_token Token, advance Token, peek Token, is_at_end bool",
    "BufferedTokenStream": "scanner Scanner, buffer_size int, token_buffer deque, current_position int",
    "BenchmarkResult": "name str, execution_time float, memory_usage float, tokens_processed int, tokens_per_second float",
    "LexerBenchmark": "results List[BenchmarkResult], test_cases Dict[str, str]",
    "TemplateScanner": "scanner Scanner, template_depth int, expression_brace_depth int",
    "CheckpointableTokenStream": "checkpoints dict, current_checkpoint str",
    "LRTokenStream": "pushback_buffer deque, parser_state dict",
    "AsyncTokenStream": "async_queue queue, cancellation_token token",
    "ReplayableTokenStream": "token_cache list, replay_position int",
    "ErrorCoordinator": "lexical_errors list, syntactic_errors list, recovery_mode bool"
  },
  "methods": {
    "scan_tokens() -> List[Token]": "main method processing entire source into tokens",
    "scan_token() -> None": "processes single token at current position",
    "advance() -> str": "consume and return current character",
    "peek() -> str": "examine current character without consuming",
    "peek_next() -> str": "examines next character without consuming",
    "match(expected: str) -> bool": "conditionally consumes character if matches expected",
    "add_token(token_type: TokenType, literal: Any = None)": "creates token from current lexeme",
    "create_token(token_type, lexeme, position, literal) -> Token": "factory function for creating validated tokens",
    "create_eof_token(position) -> Position": "creates EOF token at specified position",
    "advance_position(position, character) -> Position": "calculates new position after consuming character",
    "is_keyword(lexeme) -> bool": "checks if lexeme is reserved keyword",
    "get_keyword_type(lexeme) -> TokenType": "returns token type for keyword lexeme",
    "create_eof_token(position) -> Token": "creates EOF token at specified position",
    "scan_number() -> None": "scans numeric literals with maximal munch",
    "scan_identifier() -> None": "scans identifiers and keywords",
    "scan_operator(c: str) -> None": "scans single and multi-character operators",
    "is_keyword(lexeme: str) -> bool": "checks if lexeme is reserved keyword",
    "get_keyword_type(lexeme: str) -> TokenType": "returns token type for keyword lexeme",
    "is_digit(c: str) -> bool": "checks if character is digit",
    "is_alpha(c: str) -> bool": "checks if character is alphabetic",
    "is_alphanumeric(c: str) -> bool": "checks if character is alphanumeric or underscore",
    "scan_string_literal() -> None": "scan string from opening to closing quote",
    "process_escape_sequence() -> str": "translate escape sequence to character",
    "skip_single_line_comment() -> None": "skip // comment until newline",
    "skip_multi_line_comment() -> None": "skip /* comment until */",
    "is_at_string_start() -> bool": "check if current char starts string",
    "is_at_comment_start() -> bool": "check if current position starts comment",
    "is_valid_escape(char: str) -> bool": "check if char forms valid escape",
    "translate_escape(char: str) -> str": "translate escape char to literal",
    "create_error(error_type, message, position) -> LexicalError": "factory method for creating error objects",
    "format_error(error, source_lines) -> str": "formats error with source context",
    "recover_from_error(error_type) -> bool": "implements recovery strategy",
    "should_suppress_error(error_type) -> bool": "determines error suppression",
    "report_error(error_type, message, suggestion) -> None": "reports error with context",
    "handle_invalid_character(char) -> None": "handles unrecognized characters",
    "handle_unterminated_string(start_position) -> None": "handles incomplete strings",
    "validate_token_boundary(token_type) -> bool": "validates token boundaries",
    "add_token(token_type, literal=None)": "creates token from current lexeme",
    "log_char_advance(char, position)": "debug logging for character consumption",
    "log_token_creation(token)": "debug logging for token creation",
    "record_error(input_text, expected_tokens, actual_tokens, error_type)": "analyze and record error patterns",
    "create_debug_scanner(source_text, debug_options) -> Scanner": "factory function for creating scanner with debugging enabled",
    "current_token() -> Token": "return current token without advancing",
    "advance() -> Token": "consume and return current token",
    "peek(offset: int = 1) -> Token": "look ahead at token without consuming",
    "is_at_end() -> bool": "check if at EOF token",
    "add_test_case(name: str, source_code: str) -> None": "register benchmark test case",
    "benchmark_scanner(scanner_factory: Callable, test_case: str) -> BenchmarkResult": "execute performance benchmark",
    "run_comparison_benchmark(implementations: Dict[str, Callable]) -> None": "compare multiple implementations",
    "profile_memory_usage(scanner: Scanner, source_code: str) -> Dict": "analyze memory usage patterns",
    "scan_template_literal() -> None": "scan template literal tokens",
    "scan_template_expression() -> List[Token]": "scan embedded expression",
    "is_template_literal_start() -> bool": "check if at template start",
    "create_checkpoint() -> str": "save current stream position",
    "restore_checkpoint(checkpoint: str) -> None": "restore saved stream position",
    "push_back(token: Token) -> None": "return token to stream"
  },
  "constants": {
    "EOF_CHAR": "null character indicating end of input",
    "KEYWORDS": "dictionary mapping reserved words to token types",
    "DIGITS": "set of digit characters",
    "LETTERS": "set of letter characters",
    "IDENTIFIER_CHARS": "set of valid identifier characters",
    "WHITESPACE": "set of whitespace characters excluding newline",
    "ESCAPE_SEQUENCES": "dictionary mapping escape chars to literals",
    "SINGLE_LINE_START": "// comment start sequence",
    "MULTI_LINE_START": "/* comment start sequence",
    "MULTI_LINE_END": "*/ comment end sequence",
    "FATAL": "stops scanning immediately",
    "ERROR": "continues with recovery",
    "WARNING": "continues normally",
    "INFO": "informational only",
    "TEMPLATE_START": "template literal opening",
    "TEMPLATE_MIDDLE": "template literal with expression",
    "TEMPLATE_END": "template literal closing",
    "TEMPLATE_EXPRESSION": "embedded expression token",
    "REGEX": "regular expression literal",
    "REGEX_FLAGS": "regex modifier flags",
    "RAW_STRING": "raw string literal",
    "BINARY_LITERAL": "binary number literal",
    "HEX_LITERAL": "hexadecimal number literal",
    "HEREDOC": "heredoc string literal",
    "HEREDOC_DELIMITER": "heredoc boundary marker",
    "ERROR_TOKEN": "lexical error token"
  },
  "terms": {
    "lexeme": "actual text sequence forming a token",
    "token": "classified lexical unit with type and position",
    "maximal munch": "consume longest possible character sequence",
    "lookahead": "examining upcoming characters without consuming",
    "token stream": "ordered sequence of tokens from lexer",
    "position tracking": "maintaining line and column during scanning",
    "scope creep": "uncontrolled expansion of project requirements",
    "finite state machine": "computational model with discrete states and transitions",
    "boundary detection": "determining where tokens end",
    "keyword lookup": "checking if identifier is reserved word",
    "character classification": "determining character category for scanning decisions",
    "escape sequence": "backslash-prefixed character combinations in strings",
    "string literal": "quoted text treated as data rather than code",
    "comment": "text ignored by lexer for documentation purposes",
    "unterminated": "string or comment missing closing delimiter",
    "context-sensitive lexing": "token interpretation depends on surrounding context",
    "lexical error": "invalid token pattern in source code",
    "error recovery": "continuing scan after detecting errors",
    "cascading errors": "false errors caused by previous error",
    "synchronization point": "safe location to resume scanning",
    "error suppression": "avoiding false positives during recovery",
    "token boundary": "valid separation between adjacent tokens",
    "property-based testing": "generating random inputs to verify invariants",
    "roundtrip property": "ability to reconstruct original input from tokens",
    "shrinking": "reducing failing test cases to minimal examples",
    "invariant": "property that must always hold true",
    "milestone validation": "verifying acceptance criteria at each development stage",
    "character consumption": "advancing through source text",
    "off-by-one error": "position or index calculation mistake by single unit",
    "template literal": "string with embedded expressions using backticks",
    "token streaming": "generating tokens on demand rather than all at once",
    "object pooling": "reusing allocated objects to reduce garbage collection",
    "string interning": "sharing identical string instances to save memory",
    "bulk processing": "handling multiple characters in single operation",
    "lookahead buffering": "maintaining upcoming characters for multi-character decisions",
    "state machine optimization": "table-driven scanning instead of branching logic",
    "checkpointing": "saving scanner state for backtracking support",
    "pushback buffer": "mechanism for returning tokens to stream",
    "synchronization points": "safe locations for error recovery",
    "error coordination": "cooperation between lexer and parser error handling",
    "token replay": "re-providing previously generated tokens",
    "asynchronous parsing": "non-blocking parser integration",
    "benchmark suite": "comprehensive performance testing framework",
    "profile-driven optimization": "using measurement data to guide performance improvements",
    "extension infrastructure": "architectural support for adding new features",
    "parser integration patterns": "standardized interfaces between lexer and parser",
    "lexical analysis": "first phase of compilation transforming characters into tokens"
  }
}