{
  "types": {
    "VectorId": "fields: u64",
    "Dimensions": "usize",
    "Vector": "fields: Vec<f32>",
    "Score": "fields: f32",
    "LayerIndex": "usize",
    "Degree": "usize node connection count",
    "CollectionConfig": "dimensions, metric, hnsw_m, hnsw_ef_construction, hnsw_ef_search",
    "DistanceMetric": "enum: Cosine, Euclidean, DotProduct",
    "VectorDbError": "fields: Storage(StorageError), Hnsw(HNSWError), Query(QueryError), Flow(FlowError), Api(ApiError), Recovery(RecoveryError)",
    "StorageError": "fields: FileCorruption, MemoryMappingFailed, PartialWrite, IdMappingCorruption, DiskSpaceExhausted, ConcurrentWriteConflict",
    "VectorStorage": "trait for vector CRUD operations",
    "HNSWIndex": "fields: layers, nodes, config",
    "SearchQuery": "fields: enum for different query types",
    "VectorDatabase": "fields: top-level orchestrator managing collections and system resources",
    "Collection": "fields: isolated vector namespace with shared configuration",
    "VectorMetadata": "fields: labels, properties, timestamp, external_id",
    "MetadataFilter": "fields: predicates for hybrid search",
    "SearchResponse": "fields: search result container",
    "ApiError": "fields: API-specific error type",
    "HNSWNode": "vector_id: VectorId, layer_connections: Vec<Vec<VectorId>>, max_layer: LayerIndex",
    "HNSWLayer": "entry_point: Option<VectorId>, node_count: usize",
    "VectorFileHeader": "magic: [u8; 8], version: u32, dimensions: u32, vector_count: u64, reserved: [u8; 40]",
    "MappedSegment": "mmap: MmapMut, file: File, capacity_vectors: usize, dimensions: usize, record_size: usize",
    "VectorStorageEngine": "fields: segments, id_mapping, dimensions",
    "SimdCapability": "enum: Scalar, Sse41, Avx2, Neon",
    "KahanSum": "sum, compensation fields",
    "SearchResult": "fields: vector_id, score, metadata",
    "BruteForceSearchEngine": "storage, simd_capability fields",
    "HNSWConfig": "m: usize, m_l: usize, ml: f64, ef_construction: usize, ef_search: usize",
    "ConnectionPool": "fields: manages client connections and request routing",
    "RequestBatcher": "fields: automatic request batching with timeout",
    "SearchRequest": "fields: vector, k, ef_search, filter, exact",
    "CreateCollectionRequest": "fields: name, dimensions, metric, hnsw_config",
    "HNSWError": "fields: HNSW-specific errors",
    "FlowCoordinator": "storage, index, metrics, maintenance_tx fields",
    "FlowMessage": "enum for inter-component messages",
    "MaintenanceOperation": "enum for maintenance tasks",
    "SearchStrategy": "enum: BruteForce, HNSW, Hybrid",
    "FlowError": "fields: error type for flow operations",
    "DistanceMetricsEngine": "SIMD optimization engine",
    "QueryError": "fields: query processing errors",
    "RecoveryError": "fields: recovery procedure error types",
    "HealthChecker": "fields: checks HashMap, check_interval Duration, last_check Option<Instant>, results HashMap",
    "HealthStatus": "fields: healthy bool, message String, details HashMap, timestamp Instant",
    "CircuitBreaker": "fields: failure_threshold usize, recovery_timeout Duration, half_open_max_calls usize, failure_count usize, last_failure_time Option<Instant>, state CircuitState, half_open_calls usize",
    "CircuitState": "fields: Closed, Open, HalfOpen",
    "ProgressiveRepair": "fields: storage_engine VectorStorageEngine, hnsw_index HNSWIndex, current_phase RepairPhase, repair_log Vec<String>",
    "RepairPhase": "fields: DataAccessibility, IndexFunctionality, FullServiceRestoration, SystemOptimization",
    "SystemDiagnostics": "fields: storage_engine VectorStorageEngine, hnsw_index HNSWIndex",
    "DiagnosticReport": "fields: timestamp Instant, overall_health bool, storage_issues Vec<StorageError>, index_issues Vec<HNSWError>, performance_issues Vec<String>, recommendations Vec<RepairRecommendation>",
    "RepairRecommendation": "fields: priority Priority, description String, estimated_downtime Duration, risk_level String, automated bool",
    "Priority": "fields: Critical, High, Medium, Low",
    "CheckpointReport": "milestone, tests_passed, tests_failed, performance_metrics, issues",
    "BenchmarkDataset": "name, vectors, queries, ground_truth",
    "AccuracyReport": "system_name, dataset_name, recall_at_1, recall_at_10, recall_at_100, mean_average_precision",
    "PerformanceSnapshot": "timestamp, search_latency_p95, insert_throughput, memory_usage_mb, recall_at_10",
    "RegressionThresholds": "latency_increase_threshold, throughput_decrease_threshold, memory_increase_threshold, accuracy_decrease_threshold",
    "RegressionAlert": "metric, severity, current_value, baseline_value, threshold_violation, recommendation",
    "AlertSeverity": "enum: Warning, Critical",
    "MilestoneChecker": "storage, test_vectors",
    "VectorDatabaseBenchmark": "datasets, systems",
    "VectorSearchSystem": "trait for search system implementations",
    "PerformanceRegression": "fields: metric_history Vec, thresholds RegressionThresholds",
    "ClusterNode": "fields: node_id String, address String, port u16, node_type NodeType, last_heartbeat Instant, health_status HealthStatus",
    "NodeType": "enum: Coordinator, Data, Metadata",
    "ClusterState": "fields: nodes Arc<RwLock<HashMap<String, ClusterNode>>>, routing_table Arc<RwLock<RoutingTable>>, heartbeat_interval Duration, failure_timeout Duration",
    "RoutingTable": "fields: partitions HashMap<u32, Vec<String>>, hash_ring Vec<(u64, String)>",
    "CompositeDocument": "fields: document_id String, vectors HashMap<String, Vector>, metadata VectorMetadata, timestamp DateTime",
    "MultiModalQuery": "fields: query_vectors HashMap<String, Vector>, modality_weights HashMap<String, f32>, k usize, filters Option<MetadataFilter>",
    "MultiModalSearchEngine": "fields: modality_indexes HashMap<String, Box<dyn SearchEngine>>, fusion_strategy ScoreFusionStrategy",
    "ScoreFusionStrategy": "enum: WeightedSum, LearnedFusion, RankFusion",
    "VectorDatabaseMetrics": "fields: search_latency_histogram Histogram, search_throughput_counter Counter, recall_accuracy_gauge Gauge, hnsw_connectivity_gauge Gauge, index_size_gauge Gauge, memory_fragmentation_gauge Gauge, cpu_utilization_gauge Gauge, memory_usage_gauge Gauge, disk_io_counter Counter",
    "MonitoringSystem": "fields: metrics VectorDatabaseMetrics, health_checker HealthChecker, alert_manager AlertManager, metric_collection_interval Duration",
    "AlertManager": "fields: alert_rules Vec<AlertRule>, notification_channels Vec<NotificationChannel>",
    "AlertRule": "fields: name String, condition AlertCondition, threshold f64, duration Duration, severity AlertSeverity",
    "VectorBackupSystem": "fields: storage_engine Arc<VectorStorageEngine>, backup_storage Box<dyn BackupStorage>, compression_level CompressionLevel",
    "BackupManifest": "fields: backup_id String, timestamp DateTime, collection_name String, backup_type BackupType, file_list Vec<BackupFile>, total_size u64, compression_ratio f32",
    "BackupStorage": "trait for backup storage operations",
    "ClusterError": "fields: cluster operation errors",
    "MonitoringError": "fields: monitoring system errors",
    "BackupError": "fields: backup operation errors",
    "RestoreResult": "fields: recovery operation results",
    "BackupType": "enum: Full, Incremental",
    "BackupFile": "fields: backup file metadata",
    "CompressionLevel": "enum: compression level options",
    "AlertCondition": "enum: alert trigger conditions",
    "NotificationChannel": "enum: alert notification methods",
    "Alert": "fields: alert instance data"
  },
  "methods": {
    "compute_distance(a, b, metric)": "compute distance between vectors using specified metric",
    "cosine_distance(a, b)": "compute cosine distance between vectors",
    "euclidean_distance(a, b)": "compute Euclidean distance between vectors",
    "dot_product_similarity(a, b)": "compute dot product similarity between vectors",
    "insert(vector, metadata) -> VectorId": "store single vector with metadata",
    "batch_insert(vectors) -> Vec<VectorId>": "store multiple vectors efficiently",
    "get(id) -> Option<(Vector, VectorMetadata)>": "retrieve vector by ID",
    "delete(id) -> bool": "remove vector from storage",
    "scan_all() -> Iterator": "iterate over all stored vectors",
    "search(query, k) -> SearchResponse": "find k nearest neighbors",
    "brute_force_search(query, k)": "exact nearest neighbors baseline",
    "hnsw_search(query, k)": "approximate nearest neighbors via HNSW",
    "calculate_recall(true_neighbors, hnsw_neighbors)": "measure search accuracy",
    "new()": "constructor with defaults",
    "with_labels(Vec<String>)": "create metadata with labels",
    "add_property(String, String) -> Option<String>": "insert key-value pair",
    "add_connection(LayerIndex, VectorId)": "add neighbor to layer",
    "get_connections(LayerIndex) -> &[VectorId]": "get neighbors for layer",
    "select_layer_for_node() -> LayerIndex": "probabilistic layer assignment using geometric distribution",
    "write_to_file(&mut File) -> io::Result<()>": "serialize header to file",
    "read_from_file(&mut File) -> io::Result<Self>": "deserialize header from file",
    "calculate_vector_record_size(Dimensions) -> usize": "compute aligned record size",
    "cosine_similarity(a, b)": "compute cosine similarity between vectors",
    "batch_compute_distances(query, candidates, metric)": "compute multiple distances efficiently",
    "detect_simd_capability()": "runtime CPU feature detection",
    "dispatch_dot_product(a, b)": "SIMD-optimized dot product dispatch",
    "is_normalized(vector, epsilon)": "check if vector has unit magnitude",
    "compute_magnitude(vector)": "calculate vector L2 norm",
    "apply_metadata_filter(metadata, filter)": "evaluate filter predicates against metadata",
    "search_top_k(query_vector, k, filter)": "k-nearest neighbor search with optional filtering",
    "search_threshold(query_vector, threshold, filter)": "threshold-based search returning all within cutoff",
    "search_batch_top_k(query_vectors, k, filter)": "batch query support for multiple searches",
    "insert(vector_id, storage) -> Result<(), HNSWError>": "add new vector to HNSW graph with proper connections",
    "search(query_vector, k, ef_search, storage) -> Result<Vec<(VectorId, f32)>, HNSWError>": "find k nearest neighbors using multi-layer traversal",
    "add_connection(layer, neighbor_id)": "add bidirectional edge between nodes",
    "get_connections(layer) -> &[VectorId]": "retrieve neighbors at specified layer",
    "search_layer(entry_points, query_vector, num_closest, layer, storage) -> Result<Vec<(VectorId, f32)>, HNSWError>": "greedy search within single layer",
    "select_neighbors_simple(candidates, m) -> Vec<VectorId>": "choose best m neighbors from candidate set",
    "connect_nodes(node_id, neighbor_id, layer)": "establish bidirectional connection",
    "create_collection(request) -> Result<CollectionResponse, ApiError>": "create new vector collection with configuration",
    "search_vectors(collection, request) -> Result<SearchResponse, ApiError>": "similarity search with optional filtering",
    "insert_vectors(collection, vectors) -> Result<Vec<VectorId>, ApiError>": "batch vector insertion with metadata",
    "batch_inserts(stream, processor) -> Result<Vec<VectorId>, Error>": "accumulate and batch streaming inserts",
    "acquire_connection() -> Option<ConnectionGuard>": "get connection slot with backpressure",
    "to_http_status() -> StatusCode": "convert internal error to HTTP status",
    "to_grpc_code() -> GrpcCode": "convert internal error to gRPC status",
    "stream_search(requests) -> Stream<SearchResponse>": "bidirectional streaming search",
    "is_overloaded() -> bool": "check if server is over capacity",
    "execute_insert_flow(vectors)": "coordinate vector insertion with two-phase protocol",
    "execute_search_flow(query)": "coordinate search with strategy selection",
    "validate_insert_batch(vectors)": "validate batch insertion parameters",
    "select_search_strategy(query, size)": "choose optimal search algorithm",
    "execute_storage_compaction()": "background storage optimization",
    "handle_insert_failure(ids, error)": "error recovery for failed insertions",
    "check() -> Result<HealthStatus, VectorDbError>": "health check execution",
    "run_checks() -> HashMap<String, HealthStatus>": "execute all registered health checks",
    "overall_health() -> bool": "determine if system is healthy overall",
    "call<T, E, F>(operation: F) -> Result<T, VectorDbError>": "execute operation with circuit breaker protection",
    "can_execute() -> bool": "check if circuit breaker allows requests",
    "on_success()": "record successful operation",
    "on_failure()": "record failed operation",
    "get_state() -> CircuitState": "get current circuit breaker state",
    "execute_full_repair() -> Result<(RepairPhase, Vec<String>), VectorDbError>": "run complete progressive repair",
    "repair_data_accessibility() -> Result<Vec<String>, RecoveryError>": "repair storage accessibility issues",
    "repair_index_functionality() -> Result<Vec<String>, RecoveryError>": "rebuild HNSW index from recovered data",
    "run_full_diagnostic() -> Result<DiagnosticReport, VectorDbError>": "comprehensive system health analysis",
    "validate_hnsw_graph() -> Result<GraphValidationReport, VectorDbError>": "validate graph structure correctness",
    "normalized_vector(dimensions)": "property test generator for unit vectors",
    "sparse_vector(dimensions, sparsity)": "property test generator for sparse vectors",
    "validate_storage_milestone()": "verify storage engine milestone completion",
    "validate_metrics_milestone()": "verify distance metrics milestone completion",
    "validate_search_milestone()": "verify brute force search milestone completion",
    "validate_hnsw_milestone()": "verify HNSW index milestone completion",
    "validate_api_milestone()": "verify API server milestone completion",
    "record_test_pass()": "increment passed test counter",
    "record_test_failure(issue)": "record test failure with description",
    "record_performance(metric, value)": "record performance measurement",
    "is_passing()": "check if milestone meets acceptance criteria",
    "add_dataset(dataset)": "add benchmark dataset to test suite",
    "add_system(system)": "add vector search implementation to comparison",
    "run_benchmarks(criterion)": "execute comprehensive benchmark suite",
    "benchmark_search_latency(criterion)": "measure search operation latency",
    "benchmark_accuracy()": "calculate recall metrics against ground truth",
    "record_performance(snapshot)": "add performance measurement to history",
    "detect_regressions()": "identify performance degradation patterns",
    "validate_storage_health() -> Result<(), Vec<StorageError>>": "check storage system integrity",
    "analyze_performance_characteristics() -> Option<Vec<String>>": "identify performance bottlenecks",
    "register_node(node) -> Result<(), ClusterError>": "add node to cluster with routing updates",
    "handle_node_failure(node_id) -> Result<(), ClusterError>": "manage node failure with automatic failover",
    "route_vector_id(vector_id) -> Option<&[String]>": "find nodes responsible for vector ID",
    "search_multi_modal(query) -> Result<Vec<SearchResult>, QueryError>": "execute weighted multi-modal search",
    "start_monitoring() -> Result<(), MonitoringError>": "initialize comprehensive monitoring system",
    "record_search_operation(latency, result_count, recall_score)": "record search performance metrics",
    "evaluate_alerts(metrics) -> Vec<Alert>": "evaluate alert rules against metrics",
    "create_incremental_backup(collection_name, last_backup_timestamp) -> Result<BackupManifest, BackupError>": "create incremental backup with compression",
    "restore_from_backup(backup_manifest, target_timestamp) -> Result<RestoreResult, BackupError>": "restore from backup with index rebuilding",
    "upload_backup(manifest, data) -> Result<(), BackupError>": "upload backup to storage backend",
    "download_backup(backup_id) -> Result<Vec<u8>, BackupError>": "download backup from storage",
    "list_backups(collection_name) -> Result<Vec<BackupManifest>, BackupError>": "list available backups",
    "delete_backup(backup_id) -> Result<(), BackupError>": "remove backup from storage",
    "validate_error_message(message)": "validates error message terminology",
    "extract_technical_terms(text)": "extracts technical terms from documentation",
    "standard_abbreviations()": "returns standard abbreviations and expansions"
  },
  "constants": {
    "ef_construction": "candidate pool size during index building",
    "ef_search": "candidate pool size during search queries",
    "M": "HNSW maximum connections per node",
    "VECTOR_FILE_MAGIC": "b\"VECTOR01\" format identifier",
    "VECTOR_ALIGNMENT": "32 bytes for AVX compatibility",
    "m": "maximum connections per node at layers 1+",
    "m_l": "maximum connections at base layer (typically 2*m)",
    "ml": "layer probability multiplier (1/ln(2.0))",
    "SIMD_CAPABILITY": "detected hardware SIMD features",
    "EPSILON": "floating-point comparison tolerance",
    "BATCH_SIZE_LIMIT": "maximum vectors per batch operation",
    "BATCH_TIMEOUT": "maximum time to wait for batch completion",
    "MAX_CONNECTIONS": "concurrent connection limit",
    "REQUEST_TIMEOUT": "maximum time for request processing",
    "BRUTE_FORCE_THRESHOLD": "collection size threshold for search strategy",
    "MAX_RESULTS": "maximum k value for searches",
    "FAILURE_THRESHOLD": "circuit breaker failure count limit",
    "RECOVERY_TIMEOUT": "time before circuit breaker tries half-open",
    "HALF_OPEN_MAX_CALLS": "maximum calls allowed in half-open state",
    "CHECK_INTERVAL": "frequency of health check execution",
    "REPAIR_CHECKPOINT_INTERVAL": "frequency of repair state snapshots",
    "HEARTBEAT_INTERVAL": "Duration for cluster heartbeat checks",
    "FAILURE_TIMEOUT": "Duration before marking node failed",
    "REPLICATION_FACTOR": "Number of replicas per partition",
    "BACKUP_RETENTION": "Duration to keep backup files",
    "ALERT_EVALUATION_INTERVAL": "Frequency of alert rule evaluation",
    "MAX_BACKUP_SIZE": "Maximum size for single backup file",
    "DEFAULT_M": "16",
    "DEFAULT_M_L": "32",
    "ML": "1.442695040888963",
    "DEFAULT_EF_CONSTRUCTION": "200",
    "DEFAULT_EF_SEARCH": "50"
  },
  "terms": {
    "vector embeddings": "numerical representations of semantic content",
    "curse of dimensionality": "distance relationships become uniform in high-dimensional spaces",
    "brute force search": "exhaustive comparison against all stored vectors",
    "approximate nearest neighbor": "algorithms trading accuracy for speed",
    "HNSW": "Hierarchical Navigable Small World graph algorithm",
    "cosine similarity": "angular distance between vectors",
    "Euclidean distance": "geometric straight-line distance",
    "dot product": "projection magnitude similarity",
    "LSH": "Locality Sensitive Hashing probabilistic approach",
    "recall-latency trade-off": "balance between search accuracy and response time",
    "hybrid search": "combining vector similarity with structured queries",
    "collection": "independent namespace for vectors with shared configuration",
    "metadata filtering": "structured predicates applied during search",
    "sub-linear complexity": "algorithmic performance that scales better than O(n)",
    "p99 latency": "99th percentile response time",
    "recall@k": "fraction of true top-k neighbors found",
    "SIMD": "Single Instruction Multiple Data vectorization",
    "memory-mapped files": "disk files accessed as virtual memory",
    "write-ahead logging": "durability mechanism recording operations before execution",
    "incremental updates": "adding data without full index rebuild",
    "bidirectional edge": "graph connection maintained in both directions",
    "cache line alignment": "memory layout optimized for CPU cache access",
    "delta compression": "encoding values as differences from reference",
    "probabilistic skip list": "layered structure with random level assignment",
    "copy-on-write semantics": "creating new versions rather than modifying in-place",
    "Kahan summation": "numerical technique for reducing floating-point accumulation errors",
    "batch processing": "computing multiple distances simultaneously",
    "vector normalization": "scaling vectors to unit length",
    "floating-point precision": "accuracy limitations in decimal representation",
    "top-k selection": "maintaining k best candidates during streaming evaluation",
    "work-stealing": "parallel processing pattern for dynamic load balancing",
    "cache locality": "memory access patterns optimized for CPU cache performance",
    "greedy search": "algorithm that always moves toward locally optimal choices",
    "layer assignment": "probabilistic process determining maximum layer for each node",
    "entry point": "global starting node for all searches",
    "candidate pool": "priority queue of promising search candidates",
    "degree constraint": "maximum number of connections allowed per node",
    "query processor": "stateful server coordinating database operations",
    "connection pooling": "managed client connection lifecycle",
    "request batching": "automatic grouping of operations for efficiency",
    "backpressure": "flow control preventing resource exhaustion",
    "streaming operations": "incremental request/response processing",
    "graceful degradation": "maintaining service during overload conditions",
    "lock-free metrics": "performance tracking without synchronization overhead",
    "zero-copy deserialization": "parsing without intermediate allocations",
    "bidirectional streaming": "concurrent request and response flows",
    "two-phase protocol": "storage-first with index retry coordination",
    "filter-aware search": "applying predicates during traversal rather than post-processing",
    "incremental maintenance": "small continuous improvements vs batch rebuilds",
    "best-first search": "priority queue guided graph traversal",
    "progressive repair": "prioritized system recovery",
    "adaptive scheduling": "maintenance timing based on system load",
    "message passing": "inter-component communication pattern",
    "lock-free data structures": "concurrent access without blocking",
    "resource exhaustion": "system capacity limits reached",
    "circuit breaker": "failure protection mechanism",
    "health monitoring": "continuous system status tracking",
    "automatic recovery": "self-healing without human intervention",
    "manual intervention": "human-guided recovery procedures",
    "system failure modes": "categories of failures and their impact",
    "error detection strategies": "proactive and reactive failure identification",
    "recovery mechanisms": "procedures to restore normal operation",
    "cascading failure": "failure in one component triggering failures in others",
    "corruption diagnosis": "analysis tools for data integrity issues",
    "selective data recovery": "targeted recovery of specific data subsets",
    "system state checkpointing": "complete system snapshots for rollback capability",
    "property-based testing": "mathematical invariant verification",
    "triangle inequality": "fundamental distance metric property",
    "regression detection": "automated identification of performance degradation",
    "milestone checkpoints": "verification steps confirming implementation progress",
    "ground truth": "exact results used as accuracy baseline",
    "statistical significance": "confidence that measured differences are not due to random variation",
    "warmup period": "initial operations to reach steady-state performance before measurement",
    "symptom-diagnosis approach": "systematic mapping of observable issues to root causes",
    "high-dimensional space debugging": "diagnostic techniques for vector similarity problems",
    "HNSW graph structure debugging": "analysis of navigable small world connectivity",
    "performance regression detection": "automated identification of performance degradation",
    "state consistency debugging": "validation of synchronization across components",
    "SIMD correctness validation": "ensuring vectorized implementations match scalar results",
    "memory-mapped file debugging": "issues with virtual memory and file system integration",
    "distance relationship validation": "verifying mathematical correctness of similarity metrics",
    "search path tracing": "detailed visibility into HNSW query execution",
    "cross-component state validation": "checking synchronization between storage and indexes",
    "distributed deployment": "scaling across multiple machines",
    "hash partitioning": "data distribution using hash functions",
    "consistent hashing": "partition assignment with minimal rebalancing",
    "multi-modal embeddings": "vectors representing multiple content types",
    "composite vector documents": "documents with multiple embedding vectors",
    "weighted multi-modal search": "similarity search across multiple modalities",
    "learned distance metrics": "neural network-based similarity measures",
    "filter-aware HNSW traversal": "applying filters during graph search",
    "incremental vector backup": "space-efficient backup of vector changes",
    "index reconstruction strategy": "rebuilding indexes from vector data",
    "cross-region replication": "maintaining replicas across geographic locations",
    "adaptive parameter selection": "automatic tuning based on workload",
    "continuous performance optimization": "ongoing automatic system tuning",
    "predictive alerting": "ML-based anomaly detection",
    "cross-component correlation": "analyzing metric relationships",
    "capacity planning": "predicting future resource requirements",
    "automated index maintenance": "background index optimization",
    "work stealing": "parallel processing pattern for dynamic load balancing"
  }
}