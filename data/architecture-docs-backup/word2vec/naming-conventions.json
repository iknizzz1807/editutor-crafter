{
  "types": {
    "Vocabulary": "fields: word_to_index Dict[str, int], index_to_word Dict[int, str], word_frequencies Dict[str, int], subsampling_probs Dict[str, float], total_words int",
    "TrainingPairGenerator": "fields: vocabulary Vocabulary, window_size int, subsample bool",
    "SkipGramModel": "fields: vocab_size int, embedding_dim int, input_embeddings ndarray, output_embeddings ndarray",
    "NegativeSampler": "fields: vocabulary Vocabulary, power float, sampler AliasSampler",
    "Word2VecSGD": "fields: initial_lr float, final_lr float, total_steps int, current_step int, gradient_clip float",
    "WordSimilaritySearcher": "fields: embeddings ndarray, vocabulary Vocabulary, normalized_embeddings ndarray",
    "WordAnalogyEvaluator": "fields: embeddings ndarray, vocabulary Vocabulary",
    "EmbeddingVisualizer": "fields: embeddings ndarray, vocabulary Vocabulary",
    "EmbeddingStorage": "fields: static utility class for save/load operations",
    "TrainingCoordinator": "fields: corpus_path str, vocabulary Vocabulary, model SkipGramModel, negative_sampler NegativeSampler, optimizer Word2VecSGD, pair_generator TrainingPairGenerator",
    "MemoryManager": "fields: allocated_buffers Dict, memory_stats Dict",
    "CheckpointManager": "fields: checkpoint_dir str",
    "TestDataGenerator": "utility class for generating controlled test data",
    "NumericalValidators": "validation utilities for floating-point computations",
    "MemoryProfiler": "memory usage monitoring for integration tests",
    "PreprocessingDiagnostics": "debugging tools for preprocessing components",
    "ModelDiagnostics": "debugging tools for neural network components",
    "HuffmanNode": "fields: frequency int, word_index Optional[int], left_child Optional[int], right_child Optional[int]",
    "HuffmanTreeBuilder": "builds binary tree for hierarchical softmax",
    "SubwordVocabularyBuilder": "fields: min_n int, max_n int, boundary_tokens tuple",
    "EmbeddingLayerInitializer": "fields: embeddings ndarray, vocab Dict[str, int]",
    "HybridEmbeddingModel": "neural network combining Word2Vec and transformer representations",
    "AliasSampler": "efficient O(1) sampling algorithm implementation"
  },
  "methods": {
    "build_from_corpus(corpus_path, min_frequency)": "constructs vocabulary from text corpus",
    "word_to_index(word)": "converts word string to integer index",
    "should_subsample(word)": "determines if frequent word should be subsampled",
    "generate_pairs(tokenized_text, window_size)": "yields context-target index pairs",
    "forward(target_indices, context_indices)": "computes prediction scores",
    "get_target_embeddings(target_indices)": "extracts embeddings for target words",
    "compute_similarity_scores(target_embeddings)": "computes dot products with output embeddings",
    "get_word_embedding(word_index)": "retrieves single word embedding vector",
    "get_similar_words(word_index, top_k)": "finds most similar words",
    "sample_negative(target_word_idx, num_samples) returns List[int]": "generates negative samples avoiding target word",
    "sample_batch(target_indices, num_samples) returns ndarray": "generates negative samples for batch",
    "get_learning_rate() returns float": "computes current learning rate with decay",
    "update_embeddings(embeddings, indices, gradients) returns None": "applies SGD updates to specified embedding vectors",
    "train_batch(target_indices, context_indices) returns Dict": "trains on batch with negative sampling",
    "binary_cross_entropy_loss(scores, labels) returns Tuple": "computes stable BCE loss and gradients",
    "stable_sigmoid(x) returns ndarray": "numerically stable sigmoid computation",
    "clip_gradients(gradients) returns ndarray": "applies element-wise gradient clipping",
    "get_similar_words(word_index, top_k) returns List": "finds most similar words using cosine similarity",
    "solve_analogy_3cosadd(word_a, word_b, word_c, top_k) returns List": "solves analogies using additive composition",
    "solve_analogy_3cosmul(word_a, word_b, word_c, top_k) returns List": "solves analogies using multiplicative composition",
    "create_tsne_visualization(selected_embeddings, selected_words, perplexity, n_iter) returns Tuple": "creates 2D t-SNE visualization",
    "safe_cosine_similarity(u, v, epsilon) returns float": "computes numerically stable cosine similarity",
    "batch_cosine_similarities(query_vec, matrix, normalized) returns ndarray": "efficient batch similarity computation",
    "top_k_indices(scores, k, exclude_indices) returns ndarray": "finds top-k indices with exclusion",
    "precompute_normalized_embeddings() returns None": "caches normalized embeddings",
    "select_visualization_words(num_words, strategy) returns Tuple": "selects word subset for visualization",
    "initialize_pipeline() returns None": "initialize all components in dependency order",
    "train_complete_pipeline() returns Dict": "execute full training across all epochs",
    "_train_single_epoch() returns Dict": "process one pass through corpus",
    "validate_memory_requirements(vocab_size, embedding_dim) returns bool": "check if sufficient memory available",
    "allocate_training_buffers(batch_size, embedding_dim) returns None": "pre-allocate all training buffers",
    "get_buffer(buffer_name) returns ndarray": "retrieve pre-allocated buffer",
    "monitor_memory_usage() returns Dict": "track current memory statistics",
    "create_training_batch_iterator(pair_generator, corpus_path, batch_size) returns Iterator": "yields batches from corpus",
    "coordinate_batch_training(model, negative_sampler, optimizer, target_indices, context_indices) returns Dict": "orchestrate single batch training",
    "execute_similarity_evaluation(model, vocabulary, test_words) returns Dict": "run similarity tests",
    "word_to_index(word) returns int": "converts word string to integer index",
    "generate_pairs(tokenized_text, window_size) returns Iterator": "yields context-target index pairs",
    "forward(target_indices, context_indices) returns ndarray": "computes prediction scores",
    "create_simple_corpus(filename, vocab_size, doc_length)": "generates controlled test corpus",
    "assert_gradients_finite(gradients, context)": "validates gradients are finite and bounded",
    "assert_embeddings_normalized(embeddings, tolerance)": "validates embedding norms",
    "assert_probability_distribution(probs, tolerance)": "validates probability distribution",
    "check_memory_growth(max_growth_mb)": "validates memory growth stays within bounds",
    "assert_gradients_finite(gradients, context, tolerance)": "validates gradients are finite and bounded",
    "analyze_tokenization(text, tokenizer_func)": "analyzes tokenization results for debugging",
    "validate_vocabulary_integrity(vocabulary)": "performs comprehensive vocabulary validation",
    "sample_training_pairs(pair_generator, corpus_path, num_samples)": "samples training pairs for manual inspection",
    "validate_forward_pass(model, target_indices, context_indices)": "validates forward pass computation with checks",
    "check_gradient_flow(model, target_indices, context_indices, labels)": "verifies gradients flow correctly through model",
    "build_tree(word_frequencies, embedding_dim)": "constructs Huffman tree from frequency distribution",
    "extract_ngrams(word) returns List[str]": "extracts character n-grams with boundary markers",
    "build_subword_vocab(corpus_words, min_frequency) returns Dict": "builds subword vocabulary with frequency filtering",
    "initialize_transformer_layer(model, tokenizer, layer_name)": "replaces random initialization with Word2Vec vectors",
    "_find_embedding_match(transformer_token) returns Optional[ndarray]": "finds best Word2Vec match for transformer token",
    "_handle_dimension_mismatch(w2v_vector, target_dim) returns ndarray": "adapts vector dimensions through projection or padding",
    "sample_negative(target_word_idx, num_samples)": "generates negative samples",
    "get_learning_rate()": "computes current learning rate with decay",
    "binary_cross_entropy_loss(scores, labels)": "computes stable BCE loss and gradients",
    "stable_sigmoid(x)": "numerically stable sigmoid computation",
    "solve_analogy_3cosadd(word_a, word_b, word_c, top_k)": "solves analogies using additive method",
    "solve_analogy_3cosmul(word_a, word_b, word_c, top_k)": "solves analogies using multiplicative method",
    "create_tsne_visualization(selected_embeddings, selected_words, perplexity, n_iter)": "creates 2D t-SNE visualization",
    "train_complete_pipeline()": "execute full training across all epochs"
  },
  "constants": {
    "EMBEDDING_DIM": "300 - default dimensionality for word vectors",
    "WINDOW_SIZE": "5 - default context window radius",
    "MIN_FREQUENCY": "5 - minimum word frequency threshold",
    "SUBSAMPLE_THRESHOLD": "1e-3 - threshold for subsampling frequent words"
  },
  "terms": {
    "distributional hypothesis": "principle that words in similar contexts have similar meanings",
    "one-hot encoding": "sparse binary vectors with single active dimension per word",
    "dense embeddings": "low-dimensional continuous vector representations",
    "context window": "sliding window of surrounding words for training pairs",
    "negative sampling": "efficient training by contrasting positive and negative examples",
    "skip-gram": "architecture predicting context words from target words",
    "PMI": "pointwise mutual information measuring word co-occurrence strength",
    "subsampling": "randomly discarding frequent words to balance training",
    "tokenization": "breaking text into individual word units",
    "vocabulary construction": "building word-to-index mappings with frequency filtering",
    "training pairs": "target-context word index pairs used for neural network training",
    "softmax": "normalization function converting scores to probabilities",
    "embedding lookup": "extracting dense vector for word index",
    "cosine similarity": "similarity measure based on vector angle",
    "numerical stability": "preventing overflow/underflow in floating point computation",
    "matrix multiplication": "dot product computation between embedding matrices",
    "binary cross-entropy loss": "loss function for binary classification",
    "unigram distribution": "word frequency distribution for negative sampling",
    "alias method": "efficient O(1) sampling algorithm",
    "learning rate scheduling": "adaptive learning rate adjustment during training",
    "gradient clipping": "preventing gradient explosion by capping magnitudes",
    "stochastic gradient descent": "optimization using individual example gradients",
    "smoothed unigram distribution": "word frequencies raised to fractional power for better negative sampling",
    "sigmoid saturation": "problem when sigmoid inputs become too large leading to vanishing gradients",
    "t-SNE": "dimensionality reduction technique for visualization",
    "3CosAdd": "additive method for solving word analogies",
    "3CosMul": "multiplicative method for analogy solving",
    "perplexity": "t-SNE parameter controlling effective neighborhood size",
    "vector arithmetic": "mathematical operations on embedding vectors to capture relationships",
    "nearest neighbors": "words with highest similarity scores to query word",
    "semantic clusters": "groups of semantically related words in visualization space",
    "embedding storage": "persistent formats for saving trained word vectors",
    "training pipeline": "orchestrated sequence of operations from text to embeddings",
    "component initialization": "setup phase preparing all system components",
    "epoch processing": "complete pass through corpus during training",
    "memory management": "efficient allocation and monitoring of system memory",
    "buffer pools": "pre-allocated memory regions reused across operations",
    "batch generation": "creating fixed-size groups of training examples",
    "checkpoint management": "saving and loading training state",
    "convergence monitoring": "tracking training progress and termination conditions",
    "similarity search": "finding semantically related words using vector proximity",
    "analogy evaluation": "testing semantic relationships through vector arithmetic",
    "dimensionality reduction": "projecting high-dimensional embeddings to lower dimensions",
    "memory validation": "checking sufficient memory before training",
    "progress tracking": "monitoring training throughput and completion",
    "gradient computation": "calculating parameter updates from loss functions",
    "gradient explosion": "unbounded growth of gradients during backpropagation",
    "unknown words": "words not present in training vocabulary",
    "memory pressure": "system stress from insufficient available memory",
    "checkpoint corruption": "data integrity issues in saved training state",
    "local minima": "suboptimal solutions where gradient descent can become trapped",
    "unit testing": "testing individual components in isolation",
    "integration testing": "testing component interactions and data flow",
    "milestone checkpoints": "comprehensive validation after each development phase",
    "memory profiling": "monitoring memory usage patterns during execution",
    "convergence behavior": "how training loss decreases over iterations",
    "semantic quality assessment": "evaluating whether embeddings capture meaningful relationships",
    "stochastic behavior validation": "testing randomized components for correct statistical properties",
    "boundary testing": "testing edge cases at component interfaces",
    "finite difference checking": "validating gradients by numerical approximation",
    "vanishing gradients": "gradients becoming too small to enable learning",
    "memory leaks": "continuous memory growth due to unreleased references",
    "dimension mismatches": "incompatible tensor shapes in matrix operations",
    "hierarchical softmax": "tree-structured probability computation",
    "character n-grams": "fixed-length character sequences used in FastText",
    "subword tokenization": "breaking words into meaningful components below word level",
    "out-of-vocabulary words": "words not present in training vocabulary",
    "morphological complexity": "grammatical information encoded in word forms",
    "Byte Pair Encoding": "learned subword segmentation through frequent pair merging",
    "contextual embeddings": "representations that change based on surrounding context",
    "static embeddings": "fixed vector representations regardless of context",
    "transfer learning": "leveraging pre-trained models for new tasks",
    "vocabulary alignment": "matching between different tokenization strategies",
    "embedding layer initialization": "replacing random weights with semantic vectors",
    "hybrid architecture": "combining multiple representation approaches",
    "fusion mechanism": "learnable combination of multiple embedding sources"
  }
}