{
  "types": {
    "AuditEvent": "fields: Actor ActorInfo, Action string, Resource ResourceInfo, Timestamp time.Time, Outcome string, Context RequestContext, Metadata map[string]string",
    "ActorInfo": "fields: ID string, Type string, DisplayName string",
    "ResourceInfo": "fields: ID string, Type string, DisplayName string",
    "RequestContext": "fields: ClientIP string, UserAgent string, SessionID string, CorrelationID string",
    "HashChainEntry": "fields: Sequence uint64, PreviousHash []byte, EventHash []byte, Timestamp time.Time, Signature []byte",
    "SegmentManifest": "fields: SegmentID string, StartTime time.Time, EndTime time.Time, StartSequence uint64, EndSequence uint64, RootHash []byte, PreviousSegmentID string",
    "WAL": "fields: file *os.File, filePath string, mu sync.Mutex, offset int64",
    "StorageEngine": "fields: catalog *Catalog, activeWAL *wal.WAL, activeSeq uint64, mu sync.RWMutex, config Config",
    "QueryFilters": "fields: ActorIDs []string, ResourceTypes []string, Action string, Outcome string, Metadata map[string]string",
    "QueryResult": "fields: Events []AuditEvent, NextCursor string, TotalCount int",
    "EventValidator": "fields: maxEventSize int, maxClockSkew time.Duration, metadataKeyRegex *regexp.Regexp",
    "Config": "fields: DataDir string, MaxSegmentSize int64, MaxClockSkew time.Duration",
    "TimeRangeIterator": "fields: storage *storage.Engine, startTime time.Time, endTime time.Time, filters QueryFilters, currentSegment *storage.SegmentReader, currentEvent *event.AuditEvent, lastSeq uint64, initialized bool",
    "QueryEngine": "fields: storage *storage.Engine, catalog *Catalog",
    "AuditError": "fields: Code ErrCode, Message string, Details map[string]interface{}, Inner error, IsFatal bool",
    "Verifier": "fields: dataDir string, verbose bool, logger *log.Logger",
    "SegmentTailer": "fields: storage *storage.Engine, publisher streaming.Publisher, pollInterval time.Duration, lastSealedSeq uint64",
    "ZstdCompressor": "fields: encoder *zstd.Encoder, decoder *zstd.Decoder"
  },
  "methods": {
    "EventValidator.Validate(AuditEvent) error": "Validates event against schema rules and applies PII masking",
    "StorageEngine.AppendEvent(AuditEvent) (HashChainEntry, error)": "appends event to log with hash chain",
    "StorageEngine.VerifyChain(startSeq uint64) (bool, error)": "verifies hash chain integrity from sequence",
    "QueryEngine.Query(filters QueryFilters, startTime, endTime time.Time) (QueryResult, error)": "executes filtered time-range query",
    "ExportEngine.ToCSV(events []AuditEvent, writer io.Writer) error": "Exports events to CSV format",
    "SubmitAuditEvent(event AuditEvent) (string, error)": "Primary Ingestor API. Returns acknowledgment token.",
    "AppendEvent(event AuditEvent) (HashChainEntry, error)": "Core write operation for StorageEngine.",
    "ReadEvent(sequence uint64) (AuditEvent, error)": "Retrieves event by sequence number.",
    "VerifyChain(startSeq uint64) (bool, error)": "Verifies hash chain integrity from startSeq.",
    "GetActiveSegmentInfo() SegmentManifest": "Returns metadata about active segment.",
    "Query(filters QueryFilters, startTime time.Time, endTime time.Time) (QueryResult, error)": "Main query method for QueryEngine.",
    "QueryResult.NextPage(cursor string) (QueryResult, error)": "Retrieves next page of results.",
    "Export(filters QueryFilters, format ExportFormat, writer io.Writer) error": "Streams matching events to writer for export.",
    "OpenWAL(path string) (*WAL, error)": "Opens or creates a WAL file.",
    "WAL.Append(record []byte) (int64, error)": "Appends a record to the WAL and fsyncs.",
    "WAL.ReadAt(offset int64) ([]byte, error)": "Reads a record from the given offset.",
    "WAL.Close() error": "Closes the WAL file.",
    "createHashChainEntry(auditEvent event.AuditEvent) (HashChainEntry, error)": "constructs a new HashChainEntry for the given event",
    "computeEntryHash(entry HashChainEntry) ([]byte, error)": "calculates the cryptographic hash of a HashChainEntry",
    "SerializeToJSON(event AuditEvent) ([]byte, error)": "Converts an AuditEvent to a canonical JSON byte slice",
    "MaskPII(fieldValue string) string": "Applies masking rules to potentially sensitive strings",
    "NewAuditEvent(actor ActorInfo, action string, resource ResourceInfo, outcome string) *AuditEvent": "Creates a new AuditEvent with current UTC timestamp",
    "NewEventValidator(maxEventSize int, maxClockSkew time.Duration) *EventValidator": "Creates a new validator with configuration",
    "AppendEvent(AuditEvent) (HashChainEntry, error)": "Core write operation for StorageEngine.",
    "ReadEvent(uint64) (AuditEvent, error)": "Retrieves event by sequence number.",
    "VerifyChain(uint64) (bool, error)": "Verifies hash chain integrity from startSeq.",
    "OpenWAL(string) (*WAL, error)": "Opens or creates a WAL file.",
    "WAL.Append([]byte) (int64, error)": "Appends a record to the WAL and fsyncs.",
    "WAL.ReadAt(int64) ([]byte, error)": "Reads a record from the given offset.",
    "createHashChainEntry(AuditEvent, []byte, uint64) (HashChainEntry, error)": "constructs a new HashChainEntry for the given event",
    "computeEntryHash(HashChainEntry) ([]byte, error)": "calculates the cryptographic hash of a HashChainEntry",
    "rotateActiveSegment() error": "seals current WAL and starts new segment",
    "QueryEngine.Query(filters QueryFilters, startTime time.Time, endTime time.Time, pageSize int) (QueryResult, error)": "Primary query method with pagination",
    "QueryEngine.NextPage(cursor string) (QueryResult, error)": "Retrieves next page using cursor",
    "QueryEngine.Export(filters QueryFilters, format ExportFormat, writer io.Writer) error": "Streams matching events to writer for export",
    "TimeRangeIterator.Next() bool": "Advances iterator to next matching event",
    "TimeRangeIterator.Event() *event.AuditEvent": "Returns current event",
    "TimeRangeIterator.Close() error": "Releases resources",
    "ToCSV(events []AuditEvent, writer io.Writer) error": "Exports events to CSV format",
    "StorageEngine.appendWithRetry([]byte) (int64, error)": "implements exponential backoff for transient errors",
    "StorageEngine.recoverFromCrash() error": "scans WAL and truncates any partial writes after crash",
    "checkDiskSpace(string, int64) (bool, int64, error)": "checks available disk space using syscall.Statfs",
    "NewVerifier(dataDir string, verbose bool) *Verifier": "creates a new verifier instance",
    "Verifier.VerifyChain(startSeq uint64) (bool, error)": "verifies hash chain integrity from startSeq to end of log",
    "DumpSegment(segmentPath string, writer io.Writer) error": "writes human-readable segment contents to writer",
    "computeEntryHashDebug(entry storage.HashChainEntry) ([]byte, string, error)": "verbose version of computeEntryHash for debugging",
    "SegmentTailer.Run(ctx context.Context) error": "starts the tailing process (blocking)",
    "SegmentTailer.GetWatermark() uint64": "returns the last successfully streamed sequence number",
    "ZstdCompressor.Compress(data []byte) ([]byte, error)": "implements the Compressor interface",
    "ZstdCompressor.Decompress(data []byte) ([]byte, error)": "implements the Compressor interface",
    "ZstdCompressor.CompressStream(dst io.Writer, src io.Reader) error": "compresses data from src to dst using seekable format",
    "ZstdCompressor.DecompressStream(dst io.Writer, src io.Reader) error": "decompresses seekable zstd from src to dst",
    "NewSegmentTailer(storage *storage.Engine, publisher streaming.Publisher) *SegmentTailer": "creates a new CDC tailer",
    "NewZstdCompressor() (*ZstdCompressor, error)": "creates a new Zstandard compressor with default settings"
  },
  "constants": {
    "SHA256_HASH_SIZE": "32 bytes",
    "MAX_EVENT_SIZE": "65536 bytes (64KB)",
    "DEFAULT_SEGMENT_SIZE": "1073741824 bytes (1GB)",
    "MAX_PAGE_SIZE": "1000 events",
    "walHeader": "AUDITWALv1",
    "maxRecordSize": "1024 * 1024 (1MB)",
    "ExportFormatCSV": "Constant for CSV export format",
    "ExportFormatJSONL": "Constant for JSON Lines export format",
    "ErrCodeValidation": "VALIDATION_FAILED",
    "ErrCodeDiskFull": "DISK_FULL",
    "ErrCodeCorruption": "DATA_CORRUPTION",
    "ErrCodeSequenceBreak": "SEQUENCE_VIOLATION",
    "ErrCodeClockSkew": "CLOCK_SKEW",
    "ErrCodeTamperDetected": "TAMPER_DETECTED"
  },
  "terms": {
    "Audit Log": "An immutable, chronological record of system activities used for compliance and security investigation.",
    "Hash Chain": "Cryptographic chain where each entry includes hash of previous entry",
    "Write-Ahead Log (WAL)": "Append-only log for durability before main storage",
    "Segment": "Sealed immutable file containing batch of audit events",
    "Time-Series Database (TSDB)": "A database optimized for storing and querying data points indexed by time.",
    "SOC 2": "A compliance framework for service organizations regarding security, availability, processing integrity, confidentiality, and privacy.",
    "GDPR": "General Data Protection Regulation, a EU law on data protection and privacy.",
    "Append-Only": "A property of a data store where data can only be added, never modified or deleted.",
    "Immutable Storage": "Storage where data can only be appended, never modified or deleted",
    "Time-Range Query": "Query filtered by start and end timestamps",
    "Compliance Export": "Formatted output suitable for audit submission",
    "PII Masking": "Redacting personally identifiable information before storage",
    "Schema Validation": "Ensuring events conform to required structure",
    "Ingestor": "Component responsible for accepting and validating audit events.",
    "Storage Engine": "Component responsible for immutable storage and hash chain integrity.",
    "Query Engine": "Component responsible for efficient retrieval and export of audit events",
    "Segment Manager": "Sub-component of Storage Engine that manages the lifecycle of segment files.",
    "Catalog": "In-memory (or persisted) index of all sealed segments and their metadata.",
    "Merkle Tree": "A tree data structure where each leaf node is a hash of a data block, and each non-leaf node is a hash of its children, enabling efficient verification of large datasets.",
    "Canonical JSON": "JSON serialization with deterministic field ordering for consistent hashing",
    "Audit Event": "A structured record capturing who did what, when, and to which resource",
    "fsync": "System call to flush file data from OS cache to durable storage",
    "Cursor-Based Pagination": "Pagination using an opaque token to resume from a specific point, stable under concurrent writes",
    "Segment Pruning": "Skipping entire segment files during query execution based on metadata, to reduce I/O",
    "Streaming Export": "Export method that writes events directly to output as they are read, avoiding loading all results into memory",
    "chain of custody": "forensic protocol documenting handling of evidence",
    "critical severity": "failure requiring immediate intervention",
    "read-only mode": "state where writes are rejected but reads continue",
    "temporary error": "error that might succeed on retry (e.g., resource busy)",
    "atomic rename": "filesystem operation that either fully succeeds or fails without partial state",
    "exponential backoff": "retry strategy with increasing delay between attempts",
    "forensic analysis": "examination of corrupted data to determine root cause",
    "Integrity-First Debugging": "debugging approach that starts by verifying cryptographic hash chain integrity",
    "Differential Analysis": "comparing behavior between known-good and problematic states",
    "Temporal Correlation": "correlating symptoms with system events over time",
    "Layered Verification": "debugging approach that verifies system from cryptographic layer up to application logic",
    "Hex Dump Analysis": "inspecting raw binary files using hexadecimal representation",
    "Concurrency Stress Tester": "tool that runs parallel workloads to expose race conditions",
    "Crash Recovery Simulator": "tool that simulates process crashes to test durability guarantees",
    "Change Data Capture (CDC)": "pattern of tracking data changes in a source system and propagating them to other systems",
    "Zstandard (zstd)": "real-time compression algorithm developed by Facebook, offering high compression ratios and speed",
    "Bloom filter": "probabilistic data structure for testing set membership with configurable false positive rate",
    "SIEM": "Security Information and Event Management system for real-time security monitoring",
    "seekable format": "compression format allowing random access to portions without decompressing entire file",
    "false positive rate": "probability that a Bloom filter incorrectly indicates an element is in the set",
    "replication catalog": "metadata store tracking segment locations across regions",
    "field mapping": "transformation rules converting between data schemas",
    "compression dictionary": "pre-trained data improving compression ratios for specific data patterns",
    "idempotent producer": "message producer that guarantees exactly-once semantics through deduplication",
    "ADR": "Architecture Decision Record - structured document capturing architectural decisions",
    "Atomic Rename": "filesystem operation that either fully succeeds or fails without partial state",
    "Compression Dictionary": "pre-trained data improving compression ratios for specific data patterns",
    "False Positive Rate": "probability that a Bloom filter incorrectly indicates an element is in the set",
    "Field Mapping": "transformation rules converting between data schemas",
    "Forensic Analysis": "examination of corrupted data to determine root cause",
    "Idempotent Producer": "message producer that guarantees exactly-once semantics through deduplication",
    "PII": "Personally Identifiable Information - data that could identify an individual",
    "Replication Catalog": "metadata store tracking segment locations across regions",
    "Seekable Format": "compression format allowing random access without full decompression",
    "UTC Storage": "practice of storing all timestamps in Coordinated Universal Time"
  }
}