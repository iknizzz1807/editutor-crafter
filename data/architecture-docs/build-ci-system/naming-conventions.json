{
  "types": {
    "PipelineConfig": "fields: Source string, Jobs map[string]JobConfig, Environment map[string]string, MatrixAxes map[string][]interface{}",
    "PipelineRun": "fields: ID string, Status string, Trigger string, CommitSHA string, Branch string, CreatedAt time.Time, UpdatedAt time.Time, StartedAt *time.Time, FinishedAt *time.Time, Config *PipelineConfig",
    "JobRun": "fields: ID string, PipelineRunID string, JobName string, Status string, AssignedWorker string, ContainerID string, LogKey string, ArtifactKeys []string, StartedAt *time.Time, FinishedAt *time.Time, Env map[string]string",
    "StepRun": "fields: ID string, JobRunID string, StepName string, Command string, ExitCode int, Output string, StartedAt time.Time, Duration time.Duration",
    "JobConfig": "fields: Name string, RunsOn string, Steps []StepConfig, Needs []string, Environment map[string]string, Matrix map[string]interface{}, Expanded bool",
    "StepConfig": "fields: Name string, Command string, Env map[string]string, If string",
    "Store": "interface for persistent storage of runs",
    "WebhookVerifier": "fields: gitHubSecret []byte, gitLabToken string",
    "Item": "fields: JobRunID string, PipelineRunID string, Priority int, CreatedAt time.Time, index int",
    "PriorityQueue": "fields: (slice of *Item)",
    "HybridQueue": "fields: store Store, heap *PriorityQueue, mu sync.Mutex",
    "LogEvent": "fields: PipelineRunID string, JobRunID string, StepName string, Timestamp time.Time, Line string, Stream string",
    "ClientChannel": "fields: ID string, PipelineRunID string, JobFilter string, StepFilter string, Send chan []byte",
    "LogBroadcaster": "fields: mu sync.RWMutex, clients map[string]map[string]*ClientChannel, clientCounter int",
    "DashboardServer": "fields: Store store.Store, Broadcaster *LogBroadcaster",
    "LogEntry": "fields: Timestamp time.Time, Level string, Component string, Message string, Error string, ErrorType ErrorType, Fields map[string]interface{}, Caller string",
    "Logger": "fields: component string",
    "ErrorType": "type definition with constants",
    "RedisQueue": "fields: client *redis.Client, stream string, group string",
    "CacheConfig": "fields: Key string, Paths []string, When string",
    "CacheStore": "interface for persistent cache storage",
    "ManualEvent": "fields: Repo string, Branch string, CommitSHA string, EventType string, Parameters map[string]string"
  },
  "methods": {
    "ParseConfig(yaml string) (PipelineConfig, error)": "Parses and validates a YAML pipeline configuration file",
    "ExecuteJob(job JobRun, env map[string]string) (JobRun, error)": "Executes a job's steps in an isolated container, injecting environment variables",
    "HandleWebhook(payload []byte, signature string) (PipelineRun, error)": "Validates an incoming Git webhook and triggers a corresponding pipeline run",
    "EnqueueRun(run PipelineRun) (string, error)": "Places a pipeline run into the job queue for execution",
    "DequeueJob(workerID string) (JobRun, error)": "Retrieves the next available job from the queue for a worker to process",
    "StreamLogs(jobID string, writer io.Writer) error": "Streams the real-time log output of a running job to a writer (e.g., HTTP response)",
    "GetRunHistory(limit, offset int) ([]PipelineRun, error)": "Retrieves a paginated list of pipeline runs for the dashboard",
    "CreatePipelineRun(ctx context.Context, run *PipelineRun) error": "Stores a new pipeline run in the database",
    "DequeueJobRun(ctx context.Context, workerID string) (*JobRun, error)": "Atomically assigns the highest-priority eligible job to a worker",
    "ListPipelineRuns(ctx context.Context, limit, offset int) ([]*PipelineRun, error)": "Retrieves a paginated list of pipeline runs for the dashboard",
    "ParseConfig(yamlContent string) (PipelineConfig, error)": "Parses and validates a YAML pipeline configuration file",
    "ResolveEnvVars(config *PipelineConfig, env map[string]string) error": "Substitutes environment variable references in the configuration",
    "ExpandMatrix(job JobConfig) ([]JobConfig, error)": "Expands a job's matrix definition into multiple job configurations",
    "BuildExecutionGraph(config PipelineConfig) (map[string][]string, error)": "Constructs a dependency graph (adjacency list) from job needs",
    "ExecuteJob(ctx context.Context, job JobRun, env map[string]string) (JobRun, error)": "Executes all steps of a job in sequence within a Docker container",
    "StreamLogs(ctx context.Context, jobID string, writer io.Writer) error": "Streams the real-time log output of a currently running job to the provided writer",
    "CollectArtifacts(ctx context.Context, job JobRun, patterns []string) ([]string, error)": "Copies files matching the glob patterns from the completed job container to persistent storage",
    "VerifyGitHubSignature(signatureHeader string, body []byte) error": "Validates a GitHub webhook signature using HMAC-SHA256",
    "VerifyGitLabToken(tokenHeader string) error": "Validates a GitLab webhook token",
    "ExtractEventDetails(r *http.Request, body []byte) (string, string, string, string, error)": "Parses common fields from GitHub and GitLab payloads",
    "EnqueueJobRun(ctx context.Context, job *JobRun) error": "Persists the job and pushes it onto the in-memory heap",
    "MarkJobRunComplete(ctx context.Context, jobID string, status string) error": "Updates job status and potentially unblocks dependent jobs",
    "recoverPendingJobs()": "Rebuilds the heap from persistent storage on startup",
    "NewLogBroadcaster() *LogBroadcaster": "creates a new broadcaster instance",
    "RegisterClient(pipelineRunID, jobFilter, stepFilter string) *ClientChannel": "adds a new client for a specific pipeline run",
    "UnregisterClient(pipelineRunID, clientID string)": "removes a client",
    "Broadcast(event LogEvent)": "sends a log event to all interested clients",
    "SSEHandler(w http.ResponseWriter, r *http.Request, pipelineRunID, jobFilter, stepFilter string)": "sets up Server-Sent Events response",
    "GetRuns(w http.ResponseWriter, r *http.Request)": "handles GET /api/runs",
    "GetRunDetail(w http.ResponseWriter, r *http.Request)": "handles GET /api/runs/:id",
    "StreamLogs(w http.ResponseWriter, r *http.Request)": "handles GET /api/runs/:id/logs",
    "GetBadge(w http.ResponseWriter, r *http.Request)": "handles GET /badge/:repo.svg",
    "GetDAG(w http.ResponseWriter, r *http.Request)": "handles GET /api/runs/:id/dag",
    "ListArtifacts(w http.ResponseWriter, r *http.Request)": "handles GET /api/runs/:id/artifacts",
    "CategorizeError(err error, component string) ErrorType": "determines the ErrorType from an error message and context",
    "RetryWithBackoff(ctx context.Context, maxAttempts int, initialDelay time.Duration, f func() error, shouldRetry func(error) bool) error": "executes f with exponential backoff until success or max attempts",
    "CleanupStuckJobs(ctx context.Context, s store.Store, timeout time.Duration) (int, error)": "finds and marks jobs stuck in RUNNING state",
    "DebugItems() []Item": "returns a copy of all items in the heap for debugging",
    "PrintDebugInfo()": "logs the current state of the queue",
    "NewRedisQueue(addr, password string, db int) (*RedisQueue, error)": "creates a new Redis-backed queue",
    "Restore(ctx context.Context, key string, destPath string) (bool, error)": "restores cache entry, returns hit/miss",
    "Save(ctx context.Context, key string, srcPath string) error": "saves cache entry",
    "createSyntheticPayload(manualEvent *ManualEvent) []byte": "constructs webhook payload from manual event"
  },
  "constants": {
    "STATUS_PENDING": "Initial state of a run/job",
    "STATUS_RUNNING": "Execution is in progress",
    "STATUS_SUCCEEDED": "Execution completed successfully",
    "STATUS_FAILED": "Execution failed",
    "STATUS_CANCELLED": "Execution was cancelled by a user",
    "STATUS_SKIPPED": "Execution was skipped due to conditional logic",
    "EVENT_PUSH": "Webhook event type for a git push",
    "EVENT_PULL_REQUEST": "Webhook event type for a pull request",
    "EVENT_TAG": "Webhook event type for a git tag creation",
    "ErrorTypeUser": "user_error",
    "ErrorTypeInfra": "infrastructure",
    "ErrorTypeSystem": "system_bug",
    "EVENT_BITBUCKET_PUSH": "webhook event type for Bitbucket push",
    "STATUS_WAITING_FOR_APPROVAL": "job is paused waiting for manual approval"
  },
  "terms": {
    "CI/CD": "Continuous Integration and Continuous Delivery/Deployment",
    "Pipeline": "The entire automated process defined to build, test, and deploy code",
    "Stage": "A logical grouping of jobs within a pipeline (e.g., 'test', 'build')",
    "Job": "A unit of work that runs on a single machine/container",
    "Step": "An individual shell command or action within a job",
    "Artifact": "A file or directory produced by a job that is saved for later use",
    "Webhook": "An HTTP callback triggered by an event in a Git repository",
    "DAG": "Directed Acyclic Graph - a structure used to model dependencies between jobs/stages",
    "Matrix Build": "A feature that runs multiple variations of a job by computing the cartesian product of defined axis values",
    "Queue": "A data structure that holds pending jobs for execution in a specific order",
    "Worker": "A process that executes jobs by pulling them from the queue",
    "Priority Scheduling": "A method of ordering jobs in the queue based on importance",
    "Atomic Operation": "An operation that completes entirely or not at all, crucial for maintaining data consistency during concurrent access",
    "Server-Sent Events (SSE)": "unidirectional server-to-client streaming over standard HTTP",
    "CORS": "Cross-Origin Resource Sharing, a mechanism for allowing restricted resources on a web page",
    "backpressure": "resistance or force opposing data flow when consumer is slower than producer",
    "DAG visualization": "graphical representation of Directed Acyclic Graph showing pipeline dependencies",
    "transient failures": "failures that often resolve themselves and warrant retries",
    "persistent failures": "failures that require different handling than retries",
    "correlation IDs": "unique identifiers included in logs to trace requests across components",
    "structured logging": "logging with machine-parsable key-value pairs for aggregation",
    "Distributed work queue": "queue implementation using Redis or similar for horizontal scaling",
    "Plugin ecosystem": "system for extending pipeline steps with reusable actions",
    "Persistent workflow caches": "shared cache storage across pipeline runs to speed up execution",
    "Content-addressable cache": "cache where keys are derived from content hash",
    "S3 backend": "Amazon S3 or compatible object storage service",
    "Consumer group": "Redis Streams feature for distributing messages among multiple consumers",
    "Backpressure": "Resistance or force opposing data flow when a consumer is slower than a producer",
    "Consumer Group": "A Redis Streams feature for distributing messages among multiple consumers in a load-balanced fashion",
    "Content-Addressable Cache": "A cache where keys are derived from a cryptographic hash of the content being stored",
    "Correlation IDs": "Unique identifiers included in log messages and event payloads to trace a single request or pipeline run across multiple system components",
    "Persistent failures": "failures that require different handling than retries",
    "Structured logging": "logging with machine-parsable key-value pairs for aggregation",
    "Transient failures": "failures that often resolve themselves and warrant retries"
  }
}