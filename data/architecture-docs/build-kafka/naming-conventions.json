{
  "types": {
    "Topic": "fields: Name string, Partitions []Partition, Config map[string]string",
    "Partition": "fields: Topic string, ID int, LeaderBrokerID int, ReplicaBrokerIDs []int, Log Log, HighWatermark int64, LogEndOffset int64, ISR []int",
    "Record": "fields: Length int32, Attributes int8, TimestampDelta int64, OffsetDelta int32, Key []byte, Value []byte, Headers []Header",
    "Log": "fields: Segments []LogSegment, CurrentOffset int64, BaseDir string",
    "LogSegment": "fields: BaseOffset int64, DataFile string, IndexFile string, SizeBytes int64",
    "TCPServer": "fields: addr string, listener net.Listener, handler RequestHandler, wg sync.WaitGroup, ctx context.Context, cancel context.CancelFunc",
    "WAL": "fields: file *os.File, filePath string, mu sync.Mutex, offset int64",
    "Server": "fields: config Config, server *network.TCPServer, logDir string, mu sync.RWMutex, topics map[string]*types.Topic, logManager *LogManager, coordinator *Coordinator",
    "Config": "fields: Host string, Port int, DataDir string",
    "ConsumerGroup": "fields: GroupID string, State GroupState, GenerationID int32, LeaderID string, Protocol string, Members map[string]*ConsumerMetadata, TimeoutMs int32, mu sync.RWMutex, rebalanceStartedAt time.Time",
    "ConsumerMetadata": "fields: MemberID string, ClientID string, SubscribedTopics []string, AssignedPartitions map[string][]int32, LastHeartbeat time.Time, JoinWait chan struct{}",
    "OffsetMetadata": "fields: Partition PartitionID, GroupID string, Offset int64, Metadata string, CommitTimestamp int64",
    "Broker": "fields: ID int, Host string, Port int, Rack string",
    "TopicMetadata": "fields: Name string, Partitions []PartitionMetadata, Config map[string]string",
    "PartitionMetadata": "fields: Topic string, PartitionID int, LeaderID int, ReplicaIDs []int, ISR []int",
    "ClusterMetadata": "fields: Brokers map[int]Broker, Topics map[string]TopicMetadata, ControllerID int, ClusterID string",
    "Index": "fields: entries []IndexEntry, mu sync.RWMutex",
    "IndexEntry": "fields: RelativeOffset int32, Position int32",
    "LogConfig": "fields: SegmentMaxBytes int64, IndexInterval int",
    "ProducerConfig": "fields: BootstrapServers []string, ClientID string, Acks AcksLevel, Retries int, RetryBackoffMs int, BatchSize int, LingerMs int, BufferMemory int64, Partitioner Partitioner, CompressionType CompressionType",
    "Producer": "fields: config *ProducerConfig, accumulator *Accumulator, sender *Sender, metadataCache *MetadataCache",
    "Accumulator": "fields: config *ProducerConfig, batches map[TopicPartition]*RecordBatch, mu sync.RWMutex, cond *sync.Cond, closed bool",
    "TopicPartition": "fields: Topic string, Partition int32",
    "HashPartitioner": "fields: hasher hash.Hash32",
    "Sender": "fields: config *ProducerConfig, accumulator *Accumulator, metadataCache *MetadataCache, inFlight map[TopicPartition]*InFlightBatch, mu sync.RWMutex, connPool *ConnectionPool, retryQueue chan *RetryItem, done chan struct{}",
    "RecordBatch": "fields: BaseOffset int64, PartitionLeaderEpoch int32, MagicByte int8, CRC uint32, Attributes int16, LastOffsetDelta int32, FirstTimestamp int64, MaxTimestamp int64, ProducerID int64, ProducerEpoch int16, BaseSequence int32, Records []*Record",
    "Header": "fields: Key string, Value []byte",
    "InFlightBatch": "fields: (placeholder)",
    "RetryItem": "fields: (placeholder)",
    "ConnectionPool": "fields: (placeholder)",
    "MetadataCache": "fields: (placeholder)",
    "OffsetStore": "fields: baseDir string, mu sync.RWMutex, cache map[string]map[string]map[int32]int64",
    "GroupMember": "fields: groupID string, memberID string, generationID int32, state MemberState, assigned map[TopicPartition]bool, coordinator *BrokerConnection, heartbeatInterval time.Duration, lastHeartbeat time.Time, cancelHeartbeat context.CancelFunc, mu sync.RWMutex",
    "RangeAssigner": "fields: (empty struct)",
    "RoundRobinAssigner": "fields: (empty struct)",
    "Assignment": "fields: TopicPartitions map[string][]int32",
    "ReplicaManager": "fields: brokerID int32, config ReplicaConfig, followers map[TopicPartition]*FollowerSyncer, leaders map[TopicPartition]*ISRManager, mu sync.RWMutex, ctx context.Context, cancel context.CancelFunc",
    "ReplicaConfig": "fields: FetchWaitMs int, ReplicaFetchMinBytes int32, ReplicaFetchMaxBytes int32, ReplicaLagTimeMaxMs int",
    "FollowerSyncer": "fields: brokerID int32, topic string, partitionID int32, config ReplicaConfig, log Log, connPool *ConnectionPool, leaderBrokerID int32, leaderEpoch int32, fetchOffset int64, mu sync.RWMutex, ctx context.Context, cancel context.CancelFunc",
    "ISRManager": "fields: topic string, partitionID int32, config ReplicaConfig, partition *Partition, followerState map[int32]*followerStatus, mu sync.RWMutex, ctx context.Context, cancel context.CancelFunc",
    "followerStatus": "fields: lastFetchOffset int64, lastFetchTime time.Time",
    "LeaderElector": "fields: metadataStore *MetadataStore, brokerPool *BrokerPool, ctx context.Context",
    "ProduceRequest": "fields: TransactionalID string, Acks int16, TimeoutMs int32, TopicData []TopicProduceData",
    "TopicProduceData": "fields: Topic string, Partitions []PartitionProduceData",
    "PartitionProduceData": "fields: Partition int32, RecordSet []byte",
    "ProduceResponse": "fields: Responses []TopicProduceResponse, ThrottleTimeMs int32",
    "TopicProduceResponse": "fields: Topic string, PartitionResponses []PartitionProduceResponse",
    "PartitionProduceResponse": "fields: Partition int32, ErrorCode int16, BaseOffset int64, LogAppendTime int64, LogStartOffset int64",
    "FetchRequest": "fields: ReplicaID int32, MaxWaitMs int32, MinBytes int32, MaxBytes int32, IsolationLevel int8, SessionID int32, SessionEpoch int32, Topics []TopicFetch, ForgottenTopics []string",
    "TopicFetch": "fields: Topic string, Partitions []PartitionFetch",
    "PartitionFetch": "fields: Partition int32, FetchOffset int64, LogStartOffset int64, MaxBytes int32",
    "FetchResponse": "fields: ThrottleTimeMs int32, ErrorCode int16, SessionID int32, Responses []TopicFetchResponse",
    "TopicFetchResponse": "fields: Topic string, PartitionResponses []PartitionFetchResponse",
    "PartitionFetchResponse": "fields: Partition int32, ErrorCode int16, HighWatermark int64, LastStableOffset int64, LogStartOffset int64, AbortedTransactions []AbortedTransaction, RecordSet []byte",
    "JoinGroupRequest": "fields: GroupID string, SessionTimeoutMs int32, RebalanceTimeoutMs int32, MemberID string, ProtocolType string, Protocols []GroupProtocol",
    "GroupProtocol": "fields: Name string, Metadata []byte",
    "ProtocolMetadata": "fields: Version int16, Topics []string, UserData []byte",
    "JoinGroupResponse": "fields: ErrorCode int16, GenerationID int32, ProtocolName string, LeaderID string, MemberID string, Members []MemberMetadata",
    "MemberMetadata": "fields: MemberID string, Metadata []byte",
    "MessageHeader": "fields: Size int32, APIKey int16, APIVersion int16, CorrelationID int32, ClientID string",
    "MemoryCoordinator": "fields: mu sync.RWMutex, brokers map[int]*Broker, topics map[string]*TopicMetadata, brokerWatchers []func([]*Broker), topicWatchers map[string][]func(*TopicMetadata), closeChan chan struct{}",
    "AbortedTransaction": "fields: ProducerID int64, FirstOffset int64",
    "TestCluster": "fields: t *testing.T, size int, brokers []*Server, tempDirs []string, mu sync.RWMutex",
    "DebugStateCollector": "fields: broker *types.Server, startTime time.Time, mu sync.RWMutex, metrics map[string]interface{}",
    "BrokerDebugState": "fields: Topics map[string]TopicDebugState, Goroutines int, MemoryMB float64, Uptime string",
    "ctxKey": "fields: (string alias)",
    "TransactionCoordinator": "fields: brokerID int32, logManager *LogManager, pendingTxns map[string]*TransactionMetadata, producerState map[string]*ProducerState, mu sync.RWMutex",
    "TransactionMetadata": "fields: TransactionalID string, ProducerID int64, ProducerEpoch int16, State TransactionState, Partitions []TopicPartition, TimeoutMs int32, LastUpdateTime time.Time",
    "GzipCodec": "fields: (empty struct)",
    "QuotaManager": "fields: rates map[string]int64, buckets map[string]*TokenBucket, mu sync.RWMutex",
    "TokenBucket": "fields: capacity int64, tokens int64, lastRefill time.Time, refillRate int64, mu sync.Mutex",
    "LogCleaner": "fields: log *Log, config CleanerConfig, running bool, mu sync.RWMutex",
    "TierManager": "fields: localCache *LRUCache, remoteClient RemoteStorage, config TierConfig, mu sync.RWMutex"
  },
  "methods": {
    "TCPServer.Start() error": "Begins accepting connections",
    "TCPServer.Stop() error": "Gracefully shuts down the server",
    "WAL.Append(data []byte, sync bool) (int64, error)": "writes data to WAL with optional fsync, returns file offset",
    "WAL.ReadAll() ([][]byte, error)": "reads all entries from WAL",
    "WAL.Close() error": "closes the WAL file",
    "WAL.CurrentOffset() int64": "returns current write offset in bytes",
    "Server.Start() error": "Starts the broker server",
    "Server.CreateTopic(name string, partitionCount int) error": "Creates a new topic",
    "Server.HandleProduce(req *protocol.ProduceRequest) (*protocol.ProduceResponse, error)": "Handles produce request",
    "Server.HandleFetch(req *protocol.FetchRequest) (*protocol.FetchResponse, error)": "Handles fetch request",
    "OpenWAL(path string) (*WAL, error)": "opens or creates a WAL file",
    "NewRecord(key, value []byte, headers map[string][]byte) *Record": "creates a new Record with current timestamp",
    "Log.Append(records []*Record) (int64, error)": "writes a batch of records to the log, returns starting offset",
    "Log.Read(startOffset int64, maxBytes int32) ([]*Record, error)": "fetches records starting from given offset",
    "Index.AddEntry(offsetDelta int32, position int32)": "adds a new index entry",
    "Index.FindEntry(targetDelta int32) (IndexEntry, bool)": "finds index entry with largest relativeOffset <= targetDelta",
    "Producer.Send(ctx context.Context, record *Record) (int64, error)": "Synchronously sends a record, returns partition and offset or error.",
    "Producer.SendAsync(record *Record, callback func(int64, error)) error": "Asynchronously sends a record, invokes callback on completion.",
    "Producer.Flush(ctx context.Context) error": "Blocks until all buffered records are sent and acknowledged.",
    "Producer.Close(ctx context.Context) error": "Gracefully shuts down the producer.",
    "Accumulator.Append(tp TopicPartition, record *Record) *RecordBatch": "Adds a record to the batch for the given TopicPartition, returns batch if ready.",
    "Accumulator.GetReadyBatch() (TopicPartition, *RecordBatch)": "Blocks and returns the next ready batch.",
    "Accumulator.Close()": "Closes the accumulator, signaling sender to stop.",
    "HashPartitioner.Partition(topic string, key []byte, numPartitions int32) (int32, error)": "Selects a partition using hash of key or round-robin for nil key.",
    "Sender.run()": "Main loop that gets batches from accumulator and dispatches them.",
    "Sender.sendBatch(tp TopicPartition, batch *RecordBatch)": "Sends a single batch with retry logic.",
    "Sender.scheduleRetry(tp TopicPartition, batch *RecordBatch, attempt int)": "Schedules a batch for retry after a delay.",
    "Sender.retryLoop()": "Processes items from the retry queue.",
    "RecordBatch.Encode() ([]byte, error)": "Serializes the batch to bytes.",
    "DecodeRecordBatch(data []byte) (*RecordBatch, error)": "Parses bytes into a RecordBatch.",
    "Subscribe(topics []string) error": "Joins consumer group and subscribes to topics",
    "Poll(timeout time.Duration) []*Record": "Fetches records from assigned partitions",
    "CommitSync(offsets map[TopicPartition]int64) error": "Synchronously commits offsets",
    "CommitAsync(offsets map[TopicPartition]int64, callback func(error))": "Asynchronously commits offsets",
    "Close() error": "Leaves group and cleans up",
    "HandleJoinGroup(groupID, memberID, protocolType string, protocols []string, timeoutMs int32) (string, int32, string, string, []*ConsumerMetadata, error)": "Processes join group request",
    "HandleSyncGroup(groupID string, generationID int32, memberID string, assignments map[string]*Assignment) (map[string][]int32, error)": "Distributes partition assignments",
    "HandleHeartbeat(groupID string, generationID int32, memberID string) error": "Validates member liveness",
    "checkRebalanceTimeout()": "Completes timed-out rebalances",
    "joinGroup(topics []string) error": "Performs JoinGroup/SyncGroup protocol",
    "startHeartbeat(ctx context.Context)": "Begins periodic heartbeats",
    "Assign(members []*ConsumerMetadata, topics map[string][]int32) map[string]*Assignment": "Assigns partitions to members (both assigners)",
    "Commit(groupID string, topic string, partition int32, offset int64) error": "Persists offset for group",
    "Fetch(groupID, topic string, partition int32) (int64, bool)": "Retrieves offset for group-topic-partition",
    "persistGroup(groupID string) error": "Writes group offsets to disk",
    "LoadGroup(groupID string) error": "Loads offsets from disk",
    "ReplicaManager.Start() error": "Begins all sync loops for followers and leaders",
    "ReplicaManager.OnNewPartitionAssignment(assignments []PartitionMetadata)": "Updates syncers based on new partition assignments",
    "FollowerSyncer.Run()": "Main sync loop fetching from leader",
    "FollowerSyncer.AppendToLocalLog(records []*Record, leaderHW int64) error": "Appends records to local log with validation",
    "ISRManager.Start()": "Starts periodic ISR check loop",
    "ISRManager.evaluateISR()": "Checks follower lag and updates ISR membership",
    "ISRManager.UpdateFollowerState(followerID int32, fetchOffset int64, fetchedBytes int)": "Updates follower's progress state",
    "LeaderElector.OnBrokerFailure(failedBrokerID int32)": "Triggers leader election for partitions whose leader failed",
    "LeaderElector.selectNewLeader(isr []int32, partitionInfo PartitionMetadata) int32": "Selects a replica from ISR to become leader",
    "ReadMessage(r io.Reader) ([]byte, error)": "Reads length-prefixed message",
    "WriteMessage(w io.Writer, data []byte) error": "Writes length-prefixed message",
    "DecodeHeader(buf []byte) (*MessageHeader, error)": "Parses common message header",
    "EncodeHeader(h *MessageHeader) ([]byte, error)": "Serializes message header",
    "DecodeProduceRequest(apiVersion int16, buf []byte) (*ProduceRequest, error)": "Parses produce request",
    "EncodeProduceRequest(req *ProduceRequest, apiVersion int16) ([]byte, error)": "Serializes produce request",
    "HandleProduceRequest(req *ProduceRequest, broker *Server) (*ProduceResponse, error)": "Processes produce request on broker",
    "RegisterBroker(ctx context.Context, broker *Broker) error": "Registers broker with coordination service",
    "DeregisterBroker(ctx context.Context, brokerID int) error": "Removes broker from coordination service",
    "GetBrokers(ctx context.Context) ([]*Broker, error)": "Returns all registered brokers",
    "CreateTopic(ctx context.Context, topic *TopicMetadata) error": "Creates topic metadata",
    "DeleteTopic(ctx context.Context, topicName string) error": "Deletes topic metadata",
    "GetTopicMetadata(ctx context.Context, topicName string) (*TopicMetadata, error)": "Returns topic metadata",
    "UpdatePartitionLeadership(ctx context.Context, partition *PartitionMetadata) error": "Updates partition leader and ISR",
    "ElectPartitionLeader(ctx context.Context, topic string, partition int, isr []int) (int, error)": "Elects new leader from ISR",
    "WatchBrokers(ctx context.Context, callback func([]*Broker)) (func(), error)": "Registers for broker list changes",
    "WatchTopic(ctx context.Context, topicName string, callback func(*TopicMetadata)) (func(), error)": "Registers for topic metadata changes",
    "NewMemoryCoordinator() *MemoryCoordinator": "Creates in-memory coordination service",
    "LeaderElector.OnBrokerFailure(failedBrokerID int32) error": "Triggers leader election for partitions whose leader failed",
    "validateConsumerGeneration(groupID string, memberID string, generationID int32) error": "Checks if consumer is part of current generation",
    "LogManager.RecoverLogs() error": "Scans all log directories and recovers from partial writes",
    "MemoryCoordinator.checkBrokerHealth()": "Performs dual health checking to detect network partitions",
    "NewTestCluster(t *testing.T, size int) *TestCluster": "Creates an in-process broker cluster for integration tests",
    "TestCluster.Shutdown()": "Gracefully stops all brokers",
    "TestCluster.GetBroker(i int) *Server": "Returns broker at index i",
    "TestCluster.CreateTopic(topic string, partitions, replicationFactor int) error": "Creates a topic on the cluster",
    "TestPartitionOrderingProperty(t *testing.T)": "Property-based test for offset ordering",
    "InitLogger(level string, enableCaller bool)": "Configures global structured logger",
    "WithContext(ctx context.Context) *log.Entry": "Returns log entry with context fields",
    "NewContextWithCorrelation(parent context.Context, corrID string) context.Context": "Creates context with correlation ID",
    "NewContextWithComponent(parent context.Context, component string) context.Context": "Creates context with component name",
    "LogLevelFromString(level string) log.Level": "Safely parses log level string",
    "NewDebugStateCollector(broker *types.Server) *DebugStateCollector": "Creates debug state collector",
    "CollectBrokerState() (map[string]interface{}, error)": "Gathers comprehensive broker state",
    "RecordMetric(name string, value interface{})": "Stores custom metric",
    "GetMemoryStats() map[string]interface{}": "Returns Go memory statistics",
    "GetUptime() string": "Returns formatted uptime",
    "JSONString() (string, error)": "Returns state as pretty-printed JSON",
    "TransactionCoordinator.HandleInitProducerId(transactionalID string, timeoutMs int32) (int64, int16, error)": "Processes request for new producer ID with fencing",
    "TransactionCoordinator.HandleAddPartitions(transactionalID string, partitions []TopicPartition) error": "Adds partitions to ongoing transaction",
    "TransactionCoordinator.HandleEndTransaction(transactionalID string, commit bool) error": "Commits or aborts transaction with two-phase commit",
    "GzipCodec.Compress(src []byte) ([]byte, error)": "Compresses data using gzip",
    "GzipCodec.Decompress(src []byte) ([]byte, error)": "Decompresses gzip data",
    "GzipCodec.Name() string": "Returns codec name 'gzip'",
    "QuotaManager.CheckQuota(clientID string, bytes int) time.Duration": "Returns required delay to stay within quota",
    "LogCleaner.Run()": "Main compaction loop",
    "TierManager.OffloadSegment(segment *LogSegment) error": "Moves segment to remote storage"
  },
  "constants": {
    "GroupState": "enum: Stable, PreparingRebalance, Dead",
    "AcksNone": "Producer acknowledgment level 0 (fire-and-forget)",
    "AcksLeader": "Producer acknowledgment level 1 (leader only)",
    "AcksAll": "Producer acknowledgment level -1 (all ISR)",
    "CompressionNone": "No compression (0x0)",
    "CompressionGZIP": "GZIP compression (0x1)",
    "CompressionSnappy": "Snappy compression (0x2)",
    "ErrNone": "No error (from protocol)",
    "ErrNotLeaderForPartition": "Broker error indicating leadership changed",
    "ErrBufferFull": "Producer error when internal buffer is full",
    "GroupStateStable": "Group is stable with assignments",
    "GroupStatePreparingRebalance": "Group is undergoing rebalance",
    "GroupStateDead": "Group has no members",
    "StateUnjoined": "Consumer not part of any group",
    "StateJoining": "Consumer joining group",
    "StateAwaitingAssignment": "Consumer waiting for partition assignment",
    "StateStable": "Consumer stable with assignments",
    "MaxMessageSize": "100 MB maximum message size",
    "SizeFieldLen": "4 bytes for size prefix",
    "ErrMessageTooLarge": "Error when message exceeds maximum size",
    "ErrInvalidSize": "Error when size field is invalid",
    "session.timeout.ms": "Consumer heartbeat timeout (10000)",
    "replica.lag.time.max.ms": "Max time follower can lag before ISR removal (30000)",
    "request.timeout.ms": "Complete RPC operation timeout (30000)",
    "log.flush.timeout.ms": "Max time for log fsync (default 1000ms)",
    "ErrUnknownMemberId": "Error when consumer member ID is not recognized",
    "correlationIDKey": "ctxKey value 'correlation_id'",
    "componentKey": "ctxKey value 'component'",
    "socket.timeout.ms": "Network read/write deadline (30000)",
    "rebalance.timeout.ms": "Maximum rebalance duration (60000)",
    "fetch.timeout.ms": "Consumer poll wait timeout (500)",
    "heartbeat.timeout.ms": "Consumer liveness detection timeout (10000)",
    "LOG_LEVEL": "Environment variable for log level",
    "TransactionStateEmpty": "Transaction initialized but no data",
    "TransactionStateOngoing": "Partitions added, writes in progress",
    "TransactionStatePrepared": "Two-phase commit prepare phase",
    "TransactionStateCompleted": "Transaction committed/aborted",
    "StorageTierLocal": "Segment stored on local disk",
    "StorageTierRemote": "Segment stored in object storage"
  },
  "terms": {
    "Broker": "A server node in the cluster that stores partition replicas and handles client requests",
    "Producer": "A client that publishes (writes) records to topics",
    "Consumer": "A client that subscribes to and reads records from topics",
    "Consumer Group": "Set of consumers that cooperate to consume a topic",
    "Offset": "A monotonically increasing integer assigned to each record within a partition, representing its position",
    "Partition": "An ordered, immutable sequence of records that is a subset of a topic",
    "Topic": "A named stream of records, divided into one or more partitions",
    "High Watermark": "The offset of the last record that has been successfully replicated to all In-Sync Replicas (ISR)",
    "ISR (In-Sync Replica)": "The set of replicas for a partition that are fully caught up with the leader",
    "Group Coordinator": "Broker that manages consumer group membership and offset storage",
    "Log End Offset (LEO)": "The offset of the next message that will be appended to the log",
    "Data Plane": "Network path for message flow (produce/fetch)",
    "Control Plane": "Network path for coordination (heartbeat, metadata, group management)",
    "Replication Plane": "Network path for data replication between brokers",
    "Segment": "a physical file containing a contiguous range of log records",
    "BaseOffset": "the offset of the first record in a segment",
    "Sparse Index": "an index that maps only some offsets to file positions to save space",
    "Record Batch": "a group of records written together as a single unit on disk and over the network",
    "Internal Topic": "a topic used by the system for its own metadata (e.g., __consumer_offsets)",
    "Log Compaction": "a process that retains only the latest value for each key in a log",
    "Metadata Coordinator": "the component responsible for maintaining and distributing cluster metadata",
    "Controller": "a designated broker that manages partition leadership elections and replica assignments",
    "RecordBatch": "a group of records written together as a single unit on disk and over the network",
    "Partitioner": "Component that selects the target partition for a record",
    "Accumulator": "Producer component that batches records by destination partition",
    "Sender": "Producer component that manages network requests and retries",
    "Acks": "Acknowledgement level controlling durability guarantee",
    "Idempotent Producer": "A producer that uses sequence numbers to prevent duplicates on retry",
    "Rebalance": "Process of redistributing partitions among group members",
    "Generation ID": "Monotonically increasing number identifying group epoch",
    "Member ID": "Unique identifier for a consumer within a group",
    "Partition Assignment": "Mapping of partitions to consumers within a group",
    "Offset Commit": "Persisting consumer progress for resumption",
    "Session Timeout": "Time after which coordinator considers consumer dead",
    "Heartbeat": "Periodic keep-alive signal from consumer to coordinator",
    "Leader Epoch": "Monotonically increasing number identifying a leader's term for fencing",
    "Follower Syncer": "Component that continuously fetches from a partition leader",
    "ISR Manager": "Component that tracks follower progress and manages the in-sync replica set",
    "Unclean Leader Election": "Electing a leader not in the ISR, risking data loss",
    "Wire Protocol": "Binary format for network communication between components",
    "Length-prefixed": "Message format where size precedes data",
    "Nullable string": "String with length prefix where -1 means null",
    "API Key": "Numeric identifier for request type",
    "Correlation ID": "Client-generated ID to match requests and responses",
    "Incremental fetch": "Optimization where only changed partitions are listed",
    "Watch pattern": "Callback-based notification for metadata changes",
    "Coordination service": "Service maintaining cluster metadata and enabling consensus",
    "network partition": "Network split where subsets of brokers cannot communicate",
    "zombie consumer": "Consumer considered dead by coordinator but still processing messages",
    "torn write": "Partial disk write due to crash during write operation",
    "leader epoch": "Monotonically increasing number identifying a leader's term for fencing",
    "generation id": "Monotonically increasing number identifying group epoch",
    "unclean leader election": "Electing a leader not in the ISR, risking data loss",
    "high watermark": "Offset of last record replicated to all ISR members",
    "ISR shrinkage": "Process of followers being removed from in-sync replica set",
    "property-based testing": "Testing technique that verifies properties for randomly generated inputs",
    "test double": "Generic term for mock, stub, fake, or spy used in testing",
    "golden file": "Reference output file for comparison in tests",
    "race condition": "When outcome depends on non-deterministic timing of concurrent operations",
    "integration test": "Test that verifies multiple components work together",
    "structured logging": "Logging with key-value pairs instead of formatted strings",
    "correlation ID": "Unique identifier passed through request chains for tracing",
    "debug endpoint": "HTTP endpoint exposing internal system state for diagnosis",
    "profiling": "Measuring resource usage (CPU, memory) to identify bottlenecks",
    "goroutine leak": "Goroutines that are created but never terminate, causing memory growth",
    "heap profile": "Snapshot of memory allocations at a point in time",
    "CPU profile": "Snapshot of CPU usage over a time period",
    "state dump": "Complete snapshot of system internal state for debugging",
    "metrics collection": "Gathering quantitative measurements of system behavior",
    "timeout-based debugging": "Using timeouts to identify slow operations or deadlocks",
    "controlled experimentation": "Systematic testing of hypotheses about system behavior",
    "rebalance storm": "Frequent rebalancing of consumer groups causing instability",
    "memory leak": "Continuous growth of memory usage without corresponding data growth",
    "duplicate detection": "Identifying and eliminating repeated processing of same data",
    "message loss": "Messages sent by producer never received by consumers",
    "compression ratio": "Ratio of compressed size to original size",
    "transactional ID": "Unique identifier for producer enabling fencing",
    "two-phase commit": "Distributed commit protocol with prepare/commit phases",
    "token bucket": "Algorithm for rate limiting using token refill",
    "log compaction": "Process of removing older records for the same key",
    "tiered storage": "Hierarchical storage with different cost/performance characteristics",
    "zero-copy": "Data transfer without CPU copying between buffers",
    "multiplexing": "Carrying multiple logical streams over single connection",
    "quota enforcement": "Limiting resource usage per client",
    "clean offset": "Offset before which log compaction is guaranteed complete"
  }
}