{
  "types": {
    "ResourceAttributes": "fields: service_name string, service_version string, instance_id string, environment string",
    "LogRecord": "fields: timestamp int64, body string, severity string, trace_id string, span_id string, resource ResourceAttributes, attributes map[string]string",
    "Span": "fields: trace_id string, span_id string, parent_span_id string, name string, start_time int64, end_time int64, resource ResourceAttributes, attributes map[string]string",
    "MetricPoint": "fields: name string, value float64, timestamp int64, exemplar Exemplar, resource ResourceAttributes, attributes map[string]string",
    "Exemplar": "fields: trace_id string, value float64, timestamp int64",
    "AlertRule": "fields: name string, condition string, for_duration string, severity string, labels map[string]string, annotations map[string]string, evaluation_interval string, notification_channels []string, silenced bool, created_at int64, updated_at int64",
    "LogSeverity": "enum type with constants: SeverityTrace, SeverityDebug, SeverityInfo, SeverityWarn, SeverityError, SeverityFatal, SeverityUnspecified",
    "Batch": "fields: signal_type string, items []interface{}, resource ResourceAttributes, received_at int64",
    "batchBuffer": "fields: mu sync.Mutex, items []interface{}, resource *telemetry.ResourceAttributes, signalType string, lastFlush time.Time, maxSize int, flushInterval time.Duration",
    "Pipeline": "fields: logChan chan *telemetry.LogRecord, metricChan chan *telemetry.MetricPoint, traceChan chan *telemetry.Span, batchChan chan *Batch, cancelFunc context.CancelFunc",
    "Normalizer": "fields: none in skeleton",
    "otlpService": "fields: embedded gRPC server stubs",
    "LogStorageEngine": "Component managing log storage",
    "WriteAheadLog": "Interface: Append(record) (offset, error), ReadFrom(offset) ([]*LogRecord, error), Truncate(offset) error",
    "InMemorySegment": "fields: maxSize int, logs map[string]*LogRecord, fieldIndex map[string]map[string][]string, fullTextIndex map[string][]string",
    "ImmutableSegment": "fields: segmentID uuid.UUID, minTime int64, maxTime int64, dataFile string, fieldIndexFile string, fullTextIndexFile string",
    "LogQuery": "fields: QueryString string, StartTime int64, EndTime int64, Filters map[string]string, Limit int, Offset int",
    "MetricStorageEngine": "Component managing metric storage",
    "Series": "fields: id uint64, metricName string, resource ResourceAttributes, labels map[string]string, lastTimestamp int64, lastValue float64",
    "Chunk": "fields: seriesID uint64, minTime int64, maxTime int64, data []byte, exemplars []Exemplar",
    "Block": "fields: blockID uuid.UUID, minTime int64, maxTime int64, resolution Duration, chunkFiles []string",
    "MetricQuery": "fields: metricName string, resourceFilter ResourceAttributes, labelMatchers map[string]string, startTime int64, endTime int64, aggregation string, step int64",
    "TraceStorageEngine": "Component managing trace storage",
    "TraceMetadata": "fields: traceID string, startTime int64, endTime int64, rootServiceName string, spanCount int32, spanIDs []string, storageLocation string",
    "SpanStorageBlock": "fields: blockID uuid.UUID, minTime int64, maxTime int64, dataFile string",
    "TraceQuery": "fields: serviceName string, operationName string, attributes map[string]string, minDuration Duration, maxDuration Duration, startTime int64, endTime int64, limit int",
    "CorrelationRecord": "fields: trace_id string, log_references []LogRef, exemplar_references []ExemplarRef",
    "LogRef": "fields: log_id string, timestamp int64, storage_hint string",
    "ExemplarRef": "fields: metric_name string, series_id uint64, chunk_id string, exemplar_index int, timestamp int64, value float64",
    "QueryRequest": "fields: Signal string, StartTime int64, EndTime int64, Filters []Filter, Limit int32, Cursor string",
    "Filter": "fields: Field string, Operator string, Value any",
    "QueryResponse": "fields: Results []any, NextCursor string, Total int64",
    "CrossSignalQueryRequest": "fields: RootQuery QueryRequest, CorrelateWith []CorrelationSpec",
    "CorrelationSpec": "fields: Signal string, LinkField string",
    "PlanNode": "interface",
    "ScanLogsNode": "implements PlanNode",
    "ScanTracesNode": "implements PlanNode",
    "ScanMetricsNode": "implements PlanNode",
    "FilterNode": "implements PlanNode",
    "CorrelationJoinNode": "implements PlanNode",
    "MergeSortNode": "implements PlanNode",
    "LimitNode": "implements PlanNode",
    "AlertInstance": "fields: id string, rule_name string, labels map[string]string, annotations map[string]string, severity string, start_time int64, end_time int64, current_value string, generator_url string, fingerprint string",
    "NotificationChannel": "interface",
    "RuleStore": "fields: mu sync.RWMutex, rules map[string]*AlertRule, rulesDir string, watcher *fsnotify.Watcher, logger *zap.Logger, updateChan chan map[string]*AlertRule, stopChan chan struct{}",
    "RuleEvaluator": "fields: rule *rules.AlertRule, stateStore state.Store, queryEngine query.Engine, logger *zap.Logger, lastError error",
    "Stream": "interface with Next and Key methods",
    "DLQRecord": "fields: ID string, ReceivedAt time.Time, SignalType string, RawData []byte, Error string, Resource string",
    "FileDLQ": "fields: mu sync.Mutex, file *os.File, encoder *json.Encoder, directory string, maxSize int64",
    "LogInspector": "fields: storage *log.LogStorageEngine"
  },
  "methods": {
    "IngestionPipeline.ReceiveOTLP(request) returns (response)": "Accepts OTLP Export requests via gRPC, applies backpressure",
    "QueryEngine.ExecuteQuery(query) returns (paginated_results)": "Parses and executes a cross-signal query, merging results from storage engines",
    "AlertingEngine.EvaluateRules() returns (fired_alerts)": "Periodically evaluates all defined alert rules against the query engine",
    "ResourceAttributes.IsValid() returns bool": "performs basic validation on required fields",
    "LogRecord.HasTraceContext() returns bool": "returns true if the log record is associated with a trace",
    "Span.Duration() returns int64": "returns the span's duration in nanoseconds",
    "Span.IsRoot() returns bool": "returns true if the span has no parent",
    "MetricPoint.HasExemplar() returns bool": "returns true if the metric point includes an exemplar",
    "Normalizer.NormalizeLog(log *LogRecord) returns error": "validates and standardizes LogRecord fields",
    "Normalizer.NormalizeSpan(span *Span) returns error": "validates and normalizes a Span",
    "Normalizer.NormalizeMetric(metric *MetricPoint) returns error": "validates and normalizes a MetricPoint",
    "Pipeline.Start(ctx context.Context)": "starts all pipeline stage goroutines",
    "Pipeline.Stop()": "stops the pipeline gracefully",
    "Pipeline.ReceiveLog(log *telemetry.LogRecord) returns error": "injects a log into the pipeline, returns ErrPipelineFull if backpressure needed",
    "batchBuffer.add(item interface{}, resource *telemetry.ResourceAttributes) returns bool": "adds an item to the buffer, returns true if buffer should be flushed due to size",
    "batchBuffer.shouldFlushOnTime() returns bool": "checks if buffer should be flushed due to time interval",
    "batchBuffer.take() returns ([]interface{}, *telemetry.ResourceAttributes)": "takes all items and the resource from the buffer, resetting it",
    "LogStorageEngine.WriteBatch(records) returns error": "Writes a batch of log records to storage",
    "Chunk.encodePoint(timestamp, value) returns ([]byte, error)": "Encodes a timestamp-value pair using Gorilla compression",
    "BuildTreeFromSpans(spans) returns (*Span, error)": "Reconstructs a span tree from a list of spans",
    "CrossSignalCorrelationIndex.IndexLogRecord(log, storageHint) returns error": "Indexes a log record's trace correlation",
    "WriteAheadLog.Append(data) returns (int64, error)": "Appends data to the WAL and fsyncs",
    "WriteAheadLog.ReadFrom(startOffset) returns ([][]byte, error)": "Reads records from the WAL starting at offset",
    "QueryEngine.ExecuteQuery(ctx, req) returns (*QueryResponse, error)": "Primary query execution method",
    "QueryEngine.ExecuteCrossSignalQuery(ctx, req) returns (*CrossSignalQueryResponse, error)": "Executes queries correlating across signals",
    "Planner.BuildPlan(queryAST) returns (*PlanNode, error)": "Generates an execution plan from a query AST",
    "RuleStore.GetAllRules() returns map[string]*AlertRule": "Returns a copy of all rules (thread-safe)",
    "RuleStore.GetRule(name string) returns (*AlertRule, bool)": "Returns a specific rule by name",
    "RuleStore.UpdateChan() returns <-chan map[string]*AlertRule": "Returns a channel that receives updated rule sets",
    "RuleStore.Stop()": "Stops the file watcher",
    "RuleEvaluator.Evaluate(ctx context.Context) returns error": "Executes the rule condition and updates alert states",
    "RuleEvaluator.evaluateCondition(result any, rule *rules.AlertRule) returns (bool, error)": "Determines if the query result meets the alert condition",
    "RuleEvaluator.updateAlertState(prevState state.AlertState, conditionMet bool, duration time.Duration, now time.Time) returns (state.AlertState, []state.TransitionAction)": "Implements the state machine transitions",
    "NotificationChannel.Send(alert *AlertInstance, config ChannelConfig) returns error": "Sends notification through this channel",
    "NotificationChannel.ValidateConfig(config ChannelConfig) returns error": "Validates channel configuration",
    "NotificationChannel.Test(config ChannelConfig) returns error": "Sends test message to verify channel works",
    "Pipeline.fanoutStage(ctx context.Context)": "Orchestrates concurrent writing to storage and indexing",
    "MergeStreams(ctx context.Context, streams []Stream) returns (chan interface{}, chan error)": "Merges multiple sorted streams using a min-heap",
    "FileDLQ.Write(record DLQRecord) returns error": "Adds a record to the DLQ",
    "FileDLQ.Close() returns error": "Releases the DLQ file handle",
    "LogStorageEngine.enterReadOnlyMode(reason string)": "Transitions the engine to read-only state",
    "LogStorageEngine.ExitReadOnlyMode() returns error": "Attempts to exit read-only mode after a health check",
    "LogInspector.HandleSegments(w http.ResponseWriter, r *http.Request) returns": "debug endpoint to list log segments",
    "LogInspector.HandleSearchIndex(w http.ResponseWriter, r *http.Request) returns": "debug endpoint to search index directly",
    "LogInspector.HandleWALStatus(w http.ResponseWriter, r *http.Request) returns": "debug endpoint for WAL statistics",
    "StartPprofEndpoint(port int) returns error": "starts pprof HTTP server",
    "MetricsHandler() returns http.Handler": "returns Prometheus metrics handler",
    "UpdateChannelLength(channelName string, length int)": "updates pipeline channel length metric",
    "LoggerWithCorrelation(ctx context.Context) returns *slog.Logger": "returns logger with correlation ID from context"
  },
  "constants": {
    "OTLP_GRPC_PORT": "4317",
    "MAX_BATCH_SIZE": "1000",
    "DEFAULT_RETENTION_DAYS": "7",
    "TraceIDLength": "32",
    "SpanIDLength": "16",
    "SeverityTrace": "\"TRACE\"",
    "SeverityDebug": "\"DEBUG\"",
    "SeverityInfo": "\"INFO\"",
    "SeverityWarn": "\"WARN\"",
    "SeverityError": "\"ERROR\"",
    "SeverityFatal": "\"FATAL\"",
    "SeverityUnspecified": "\"UNSPECIFIED\"",
    "INGEST_CHANNEL_BUFFER": "1000",
    "BATCH_CHANNEL_BUFFER": "100",
    "FLUSH_INTERVAL_MS": "100",
    "SEGMENT_MAX_SIZE": "Configurable size for log segments",
    "CHUNK_DURATION": "Fixed time window for metric chunks (e.g., 2 hours)",
    "correlationIDKey": "context key for correlation ID"
  },
  "terms": {
    "OTLP": "OpenTelemetry Protocol",
    "Exemplar": "A sample measurement (e.g., a trace) linked to a metric data point",
    "Cardinality": "the number of unique time series defined by metric name + attribute combinations",
    "Backpressure": "A feedback mechanism to slow down producers when consumers are overwhelmed",
    "Cross-Signal Index": "A secondary index mapping trace_id to related log and metric data locations",
    "Telemetry Refinery": "Mental model for the observability platform architecture",
    "Polyglot Persistence": "Using multiple specialized storage systems for different data types",
    "Resource Model": "defines the immutable identity of the source producing telemetry",
    "Trace-Log correlation": "linking logs to traces via shared trace_id and span_id",
    "OpenTelemetry semantic conventions": "standardized attribute names for observability data",
    "Pipeline Pattern": "A design pattern where data flows through a series of processing stages",
    "Circuit Breaker": "A design pattern to detect failures and prevent cascading issues by failing fast",
    "Dead Letter Queue": "A storage area for messages that cannot be processed normally",
    "Segment-Based Architecture": "Storage design using immutable, sorted files (segments) for logs",
    "Gorilla Compression": "Time-series compression algorithm using delta-of-delta and XOR encoding",
    "Downsampling": "Aggregating high-resolution data into lower resolution for long-term retention",
    "Write-Ahead Log (WAL)": "A log where changes are recorded before they are applied to the main data structures, ensuring durability",
    "Trace Tree Reconstruction": "The process of building a hierarchical tree of spans from individual span records using parent_span_id pointers",
    "Query Engine": "Component that provides a unified interface to query logs, metrics, and traces",
    "Query Execution Planner": "Subcomponent that transforms a query into an efficient execution plan",
    "Cross-Signal Query": "A query that joins data across different telemetry signals (logs, metrics, traces)",
    "Cursor-Based Pagination": "Pagination using an opaque token to resume query execution",
    "Filter Pushdown": "Optimization where query filters are sent to storage engines for early filtering",
    "Plan Node": "An element in a query execution plan tree (e.g., Scan, Filter, Join)",
    "alert fatigue": "The phenomenon where teams become desensitized due to excessive, duplicate, or irrelevant notifications",
    "alert instance": "A specific occurrence of an alert rule with particular label values",
    "fingerprint": "Hash of label key-value pairs used for alert deduplication",
    "notification channel": "A configured endpoint for sending alert notifications (Slack, email, etc.)",
    "escalation policy": "Rules for escalating unacknowledged alerts to additional channels or higher severity",
    "deduplication": "Process of preventing multiple notifications for the same underlying issue",
    "rolling mean/stddev": "Statistical model using mean and standard deviation over a sliding time window",
    "z-score": "Number of standard deviations a data point is from the mean",
    "state machine": "A computational model with defined states and transition rules between them",
    "pull-based evaluation": "Alert evaluation approach where conditions are checked periodically by querying stored data",
    "for_duration": "The time an alert condition must persist before triggering",
    "repeat interval": "Minimum time between repeat notifications for the same alert",
    "backpressure": "A feedback mechanism to slow down producers when consumers are overwhelmed",
    "filter pushdown": "Optimization where query filters are sent to storage engines for early filtering",
    "cursor-based pagination": "Pagination using an opaque token to resume query execution",
    "repeat_interval": "Minimum time between repeat notifications for the same alert",
    "Dead Letter Queue (DLQ)": "A persistent queue for invalid or unprocessable records",
    "Read-Only Fallback": "A degraded mode where a storage engine serves reads but rejects writes",
    "Graceful Degradation": "The system fails in predictable, safe ways that preserve core functionality",
    "Exponential Backoff": "A retry strategy where wait time increases exponentially after each failure",
    "Testing Pyramid": "A strategy of using unit, integration, and end-to-end tests in a layered approach",
    "Property-Based Testing": "Automated testing that generates random inputs to verify properties of the code",
    "Testcontainers": "A library to provision disposable Docker containers for integration tests",
    "Golden File": "A file containing expected output for a test, used for comparison",
    "Race Condition": "A flaw where the output depends on the sequence or timing of uncontrollable events",
    "Mock": "A simulated object that mimics the behavior of a real object in controlled ways",
    "Integration Test": "A test that verifies the interaction between multiple components",
    "End-to-End Test": "A test that verifies the entire system from start to finish in a production-like environment",
    "Load Test": "A test that evaluates system performance under expected or peak load conditions",
    "pprof": "Go's profiling tool for analyzing runtime performance",
    "Coverage Profile": "Data showing which parts of the code were executed during tests",
    "request-scoped logger": "logger that includes correlation ID from context",
    "diagnostic control room": "mental model for debugging with multiple consoles",
    "trace-inception": "using the platform to debug itself via internal traces",
    "meta-observability": "observability of the observability platform itself"
  }
}