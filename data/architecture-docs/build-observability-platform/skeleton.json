{
  "title": "Build Your Own Observability Platform: Design Document",
  "overview": "This document outlines the design of a unified observability platform that ingests, stores, and correlates logs, metrics, and traces. The key architectural challenge is creating a single system that efficiently handles the distinct data shapes and query patterns of these three 'pillars' while enabling powerful cross-signal correlation to debug complex distributed systems.",
  "sections": [
    {
      "id": "context-problem-statement",
      "title": "1. Context and Problem Statement",
      "summary": "Explains the problem of fragmented system visibility in microservices, the concept of observability vs. monitoring, and the existing landscape of solutions.",
      "subsections": [
        {
          "id": "real-world-analogy",
          "title": "Real-World Analogy: The Air Traffic Control Center",
          "summary": "Compares a microservices architecture to an airspace, where individual data signals are like radar blips, radio chatter, and flight plans. True control requires correlating them all."
        },
        {
          "id": "problem-intuition",
          "title": "The Core Problem: Debugging in the Dark",
          "summary": "Describes the pain of chasing a bug across disconnected logs, dashboards, and trace viewers."
        },
        {
          "id": "existing-approaches",
          "title": "Existing Approaches and Their Trade-offs",
          "summary": "Compares commercial SaaS platforms, DIY point solutions, and OpenTelemetry-based approaches using a structured table."
        }
      ]
    },
    {
      "id": "goals-non-goals",
      "title": "2. Goals and Non-Goals",
      "summary": "Defines the essential capabilities (correlation, high-scale ingestion, unified query) and explicit boundaries (e.g., no ML-based root cause, no custom UI framework) of the project.",
      "subsections": [
        {
          "id": "must-do",
          "title": "Must Achieve (Goals)",
          "summary": "List of core functional and non-functional requirements derived from the acceptance criteria."
        },
        {
          "id": "wont-do",
          "title": "Will Not Do (Non-Goals)",
          "summary": "Explicitly scopes out features to keep the project focused and manageable for learning."
        }
      ]
    },
    {
      "id": "high-level-architecture",
      "title": "3. High-Level Architecture",
      "summary": "Presents the bird's-eye view of the system's components, their responsibilities, and how they connect, with a mental model of a 'telemetry refinery'.",
      "subsections": [
        {
          "id": "component-overview",
          "title": "The Telemetry Refinery: A Mental Model",
          "summary": "Intuitively explains the system as a refinery that ingests raw telemetry, processes and indexes it, and stores it in specialized silos for querying."
        },
        {
          "id": "file-module-structure",
          "title": "Recommended File & Module Structure",
          "summary": "Provides a suggested Go module layout for organizing the codebase, separating concerns like ingestion, storage engines, and query API."
        }
      ]
    },
    {
      "id": "data-model",
      "title": "4. Data Model",
      "summary": "Details the unified schema for logs, metrics, and traces, focusing on shared resource attributes, correlation IDs, and exemplars. Corresponds to Milestone 1.",
      "subsections": [
        {
          "id": "unified-resource-model",
          "title": "Unified Resource Model: Who Sent This?",
          "summary": "Defines the attributes that identify the source (service, pod, environment) of all telemetry data."
        },
        {
          "id": "signal-specific-models",
          "title": "Signal-Specific Data Structures",
          "summary": "Describes the core fields for Spans, Log Records, and Metric Data Points, highlighting their unique characteristics."
        },
        {
          "id": "correlation-mechanisms",
          "title": "Correlation Mechanisms: The Glue",
          "summary": "Explains how trace_id/span_id link logs to traces, and how exemplars link metrics to traces."
        }
      ]
    },
    {
      "id": "component-ingestion-pipeline",
      "title": "5. Component: Ingestion Pipeline",
      "summary": "Design of the high-throughput OTLP receiver, batcher, and normalizer. Includes ADRs on protocol choice and backpressure strategy. Corresponds to Milestone 2.",
      "subsections": [
        {
          "id": "ingestion-mental-model",
          "title": "Mental Model: A High-Capacity Loading Dock",
          "summary": "Compares the pipeline to a dock where trucks (clients) unload crates (batches) onto conveyors, with traffic lights (backpressure) to prevent jams."
        },
        {
          "id": "adr-otlp-protocol",
          "title": "ADR: Choosing OTLP/gRPC as the Primary Protocol",
          "summary": "Decision record evaluating OTLP/gRPC vs. vendor-specific protocols and raw HTTP/JSON."
        },
        {
          "id": "implementation-guidance-ingestion",
          "title": "Implementation Guidance",
          "summary": "Technology table, starter code for gRPC server setup and batch buffer, and skeleton code for the core processor with TODOs."
        }
      ]
    },
    {
      "id": "component-storage-engines",
      "title": "6. Component: Storage Engines",
      "summary": "Design of the three specialized storage backends for logs, metrics, and traces, and the cross-signal index. Corresponds to Milestone 3.",
      "subsections": [
        {
          "id": "storage-mental-model",
          "title": "Mental Model: Specialized Warehouses",
          "summary": "Compares the storage engines to different types of warehouses: a document warehouse (logs), a time-series vault (metrics), and a graph archive (traces), connected by a master index."
        },
        {
          "id": "log-storage-design",
          "title": "Log Storage: Full-Text Search with Inverted Index",
          "summary": "Design for storing log records with field and full-text indexing, using a Write-Ahead Log (WAL)."
        },
        {
          "id": "metric-storage-design",
          "title": "Metric Storage: Time-Series Database",
          "summary": "Design for high-cardinality metric storage, focusing on compression (e.g., Gorilla), downsampling, and retention policies."
        },
        {
          "id": "trace-storage-design",
          "title": "Trace Storage: Span Tree Reconstruction",
          "summary": "Design for storing spans and efficiently reconstructing complete trace trees for querying."
        },
        {
          "id": "cross-signal-index",
          "title": "Cross-Signal Correlation Index",
          "summary": "Design of a secondary index that maps trace_ids to related log record IDs and metric exemplars."
        }
      ]
    },
    {
      "id": "component-query-engine",
      "title": "5. Component: Query Engine",
      "summary": "Design of the unified query API, language, and execution planner that can join data across storage engines. Corresponds to Milestone 4.",
      "subsections": [
        {
          "id": "query-mental-model",
          "title": "Mental Model: The Universal Library Catalog",
          "summary": "Compares the query engine to a librarian who can search card catalogs (indexes) across three specialized libraries (storage engines) to find related information."
        },
        {
          "id": "query-language-api",
          "title": "Query Language and API Design",
          "summary": "Specifies the filter syntax, pagination model, and HTTP/gRPC API endpoints."
        },
        {
          "id": "query-execution-planner",
          "title": "Query Execution Planner",
          "summary": "Describes how a query is parsed, planned into a sequence of storage engine calls, and results are merged and paginated."
        }
      ]
    },
    {
      "id": "component-alerting-engine",
      "title": "6. Component: Alerting & Anomaly Detection Engine",
      "summary": "Design of the rule evaluator, simple anomaly detector, and notification router. Corresponds to Milestone 5.",
      "subsections": [
        {
          "id": "alerting-mental-model",
          "title": "Mental Model: The Vigilant Watchtower",
          "summary": "Compares the alerting engine to watchtowers with lookouts (rules) scanning the horizon (data streams) and sending signals (notifications) when they spot smoke (anomalies)."
        },
        {
          "id": "rule-evaluation",
          "title": "Rule Evaluation Engine",
          "summary": "Design for periodically evaluating threshold and multi-signal rules against the query engine."
        },
        {
          "id": "anomaly-detection-basics",
          "title": "Anomaly Detection: Simple Statistical Baselines",
          "summary": "Design for a simple learning baseline (e.g., rolling average/StdDev) to detect metric deviations."
        },
        {
          "id": "notification-routing",
          "title": "Notification Routing and Deduplication",
          "summary": "Design for routing alerts to channels (e.g., Slack, PagerDuty) and suppressing duplicate alerts."
        }
      ]
    },
    {
      "id": "interactions-data-flow",
      "title": "7. Interactions and Data Flow",
      "summary": "Walks through the end-to-end flow for key user journeys: ingesting telemetry, executing a cross-signal query, and triggering an alert.",
      "subsections": [
        {
          "id": "journey-telemetry-ingestion",
          "title": "Journey 1: Telemetry Ingestion Path",
          "summary": "Sequence of steps from an application emitting a span to it being stored and indexed."
        },
        {
          "id": "journey-cross-signal-query",
          "title": "Journey 2: Cross-Signal Query Execution",
          "summary": "Sequence of steps for a user query like 'show me logs for slow traces from service X'."
        },
        {
          "id": "journey-alert-firing",
          "title": "Journey 3: Alert Evaluation and Firing",
          "summary": "Sequence of steps from a metric exceeding a threshold to a notification being sent."
        }
      ]
    },
    {
      "id": "error-handling-edge-cases",
      "title": "8. Error Handling and Edge Cases",
      "summary": "Describes failure modes (e.g., storage full, corrupt batch), detection strategies, and recovery mechanisms like retry queues and graceful degradation.",
      "subsections": [
        {
          "id": "failure-modes-ingestion",
          "title": "Failure Modes in Ingestion",
          "summary": "Covers backpressure, malformed data, and transient downstream failures."
        },
        {
          "id": "failure-modes-storage",
          "title": "Failure Modes in Storage",
          "summary": "Covers disk full, corruption, and high query load."
        },
        {
          "id": "recovery-strategies",
          "title": "Recovery and Mitigation Strategies",
          "summary": "Strategies like circuit breakers, dead-letter queues for bad data, and read-only fallbacks."
        }
      ]
    },
    {
      "id": "testing-strategy",
      "title": "9. Testing Strategy",
      "summary": "Outlines a testing pyramid approach, property-based tests for the data model, and integration tests for cross-component flows. Includes milestone checkpoints.",
      "subsections": [
        {
          "id": "testing-pyramid",
          "title": "The Testing Pyramid",
          "summary": "Unit, integration, and end-to-end test recommendations for each component."
        },
        {
          "id": "milestone-checkpoints",
          "title": "Milestone Implementation Checkpoints",
          "summary": "For each milestone, describes what to test and the expected output to verify progress (e.g., 'After Milestone 1, you should be able to query logs by trace_id')."
        }
      ]
    },
    {
      "id": "debugging-guide",
      "title": "10. Debugging Guide",
      "summary": "Provides a symptom-cause-fix table for common implementation bugs, and techniques for inspecting pipeline flow and storage state.",
      "subsections": [
        {
          "id": "common-bugs-table",
          "title": "Common Bugs: Symptom \u2192 Cause \u2192 Fix",
          "summary": "Table addressing issues like 'No logs appear in search', 'High memory usage', 'Query times out'."
        },
        {
          "id": "debugging-techniques",
          "title": "Debugging Techniques and Tools",
          "summary": "Recommendations for adding debug logging, using pprof for profiling, and inspecting internal indexes."
        }
      ]
    },
    {
      "id": "future-extensions",
      "title": "11. Future Extensions",
      "summary": "Sketches potential advanced features the architecture could accommodate, such as continuous profiling, user monitoring (RUM), and ML-based root cause analysis.",
      "subsections": [
        {
          "id": "advanced-features",
          "title": "Possible Advanced Features",
          "summary": "Brief description of enhancements like profiling integration, dynamic sampling, and advanced anomaly detection."
        }
      ]
    },
    {
      "id": "glossary",
      "title": "12. Glossary",
      "summary": "Definitions of key terms used in the document, such as OTLP, cardinality, exemplar, and resource attributes.",
      "subsections": [
        {
          "id": "terms-table",
          "title": "Terms and Definitions",
          "summary": "Alphabetical table of terms with definitions and references to their first appearance."
        }
      ]
    }
  ],
  "diagrams": [
    {
      "id": "diagram-system-overview",
      "title": "System Component Overview",
      "description": "Shows the main components (Ingestion Pipeline, Storage Engines, Query Engine, Alerting Engine) and their connections via internal APIs. Include data flow arrows for telemetry in and queries/alerts out.",
      "type": "component",
      "relevant_sections": [
        "high-level-architecture",
        "interactions-data-flow"
      ]
    },
    {
      "id": "diagram-unified-data-model",
      "title": "Unified Data Model Relationships",
      "description": "A class diagram showing the Resource, Span, LogRecord, and MetricPoint entities, with their attributes and relationships (e.g., Span 'has many' LogRecords, MetricPoint 'has optional' Exemplar linking to Trace).",
      "type": "class",
      "relevant_sections": [
        "data-model"
      ]
    },
    {
      "id": "diagram-ingestion-sequence",
      "title": "Ingestion Pipeline Sequence",
      "description": "Sequence diagram showing an app sending OTLP data to the Receiver, which batches it, passes to the Normalizer, and then to the appropriate Storage Engine and Cross-Signal Index. Include backpressure feedback loop.",
      "type": "sequence",
      "relevant_sections": [
        "component-ingestion-pipeline",
        "interactions-data-flow"
      ]
    },
    {
      "id": "diagram-query-execution",
      "title": "Cross-Signal Query Execution",
      "description": "Sequence diagram showing the Query Engine receiving a request, consulting the Cross-Signal Index, fanning out requests to Log, Metric, and Trace storage, and merging/paginating the results.",
      "type": "sequence",
      "relevant_sections": [
        "component-query-engine",
        "interactions-data-flow"
      ]
    },
    {
      "id": "diagram-alert-state-machine",
      "title": "Alert Lifecycle State Machine",
      "description": "State machine showing alert states: Inactive -> Pending (when condition met but not for duration) -> Firing -> Resolved (when condition no longer met) -> Inactive. Include transitions for silencing.",
      "type": "state-machine",
      "relevant_sections": [
        "component-alerting-engine"
      ]
    },
    {
      "id": "diagram-log-storage-internal",
      "title": "Log Storage Internal Architecture",
      "description": "Component diagram of the Log Storage engine, showing incoming batches written to a Write-Ahead Log (WAL), an in-memory segment, background compaction to disk, and the separate Full-Text and Field Indexes.",
      "type": "component",
      "relevant_sections": [
        "component-storage-engines"
      ]
    }
  ]
}