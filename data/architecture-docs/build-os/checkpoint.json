{
  "project_id": "build-os",
  "meta": {
    "id": "build-os",
    "name": "Build Your Own OS",
    "description": "x86 operating system kernel with interrupts, memory management, and preemptive scheduling",
    "difficulty": "expert",
    "estimated_hours": "120-200",
    "essence": "Bootstrap from firmware to protected mode via GDT configuration, hardware interrupt handling through IDT and PIC/APIC, physical and virtual memory management with page tables and heap allocation, and preemptive process scheduling with context switching and user-mode transitions via TSS.\n",
    "why_important": "Building an OS kernel demystifies the abstraction layer between hardware and applications, teaching foundational systems concepts that underlie every modern computing platform \u2014 from interrupt handling to memory isolation to process scheduling.\n",
    "learning_outcomes": [
      "Implement a bootloader transitioning from real mode to 32-bit protected mode",
      "Configure GDT with proper segment descriptors for kernel and user mode",
      "Set up IDT and interrupt service routines for CPU exceptions and hardware IRQs",
      "Build physical memory allocators (bitmap or buddy)",
      "Implement virtual memory with page tables and demand paging",
      "Create process control blocks and preemptive context switching",
      "Set up TSS for ring 3 \u2192 ring 0 transitions",
      "Implement system call interface via software interrupts",
      "Debug with QEMU, serial port logging, and GDB stubs"
    ],
    "skills": [
      "x86 Assembly",
      "Hardware Interrupts (PIC, APIC)",
      "GDT/IDT/TSS Configuration",
      "Page Table Management",
      "Context Switching",
      "Kernel Programming (freestanding)",
      "Low-Level Debugging",
      "Linker Script Design"
    ],
    "tags": [
      "bootloader",
      "build-from-scratch",
      "c",
      "expert",
      "interrupts",
      "kernel",
      "rust",
      "scheduling",
      "systems",
      "zig"
    ],
    "architecture_doc": "architecture-docs/build-os/index.md",
    "languages": {
      "recommended": [
        "C",
        "Rust",
        "Zig"
      ],
      "also_possible": []
    },
    "resources": [
      {
        "type": "book",
        "name": "Operating Systems: Three Easy Pieces",
        "url": "https://pages.cs.wisc.edu/~remzi/OSTEP/"
      },
      {
        "type": "tutorial",
        "name": "Writing an OS in Rust (phil-opp)",
        "url": "https://os.phil-opp.com/"
      },
      {
        "type": "reference",
        "name": "OSDev Wiki",
        "url": "https://wiki.osdev.org/"
      },
      {
        "type": "reference",
        "name": "Intel Software Developer Manuals",
        "url": "https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
      }
    ],
    "prerequisites": [
      {
        "type": "skill",
        "name": "x86 assembly language (AT&T or NASM syntax)"
      },
      {
        "type": "skill",
        "name": "C programming (freestanding, no stdlib)"
      },
      {
        "type": "skill",
        "name": "Computer architecture (registers, stack, memory bus)"
      },
      {
        "type": "skill",
        "name": "Binary and hexadecimal arithmetic"
      }
    ],
    "milestones": [
      {
        "id": "build-os-m1",
        "name": "Bootloader, GDT, and Kernel Entry",
        "description": "Boot from BIOS, configure the GDT for protected mode, load the kernel into memory, and transfer control to kernel C code.\n",
        "acceptance_criteria": [
          "Bootloader code fits within MBR constraints (512 bytes with 0x55AA signature at bytes 510-511) or implements two-stage loading",
          "Bootloader reads kernel binary from disk using BIOS INT 13h and loads it to physical address 0x100000 (1MB mark)",
          "A20 line is successfully enabled using at least one method (BIOS INT 15h/2401, fast A20 via port 0x92, or keyboard controller)",
          "GDT is configured with exactly 5 entries: null descriptor (index 0), kernel code segment (ring 0, base=0, limit=4GB, executable+readable), kernel data segment (ring 0, base=0, limit=4GB, writable), user code segment (ring 3), user data segment (ring 3)",
          "GDTR is loaded with lgdt instruction before setting CR0.PE",
          "Protected mode is entered by setting CR0.PE bit (bit 0 of CR0) to 1",
          "Far jump (jmp 0x08:label) is executed after CR0.PE is set to load CS with kernel code selector and flush the instruction pipeline",
          "All data segment registers (DS, ES, FS, GS, SS) are loaded with kernel data selector (0x10) after protected mode entry",
          "32-bit stack pointer (ESP) is initialized to a valid memory region below 1MB or above kernel load address",
          "Kernel entry point (in assembly) zeroes the BSS section from __bss_start to __bss_end as defined in linker script",
          "C entry point function is called with proper stack alignment and direction flag cleared (cld)",
          "VGA text mode driver writes characters with color attributes to memory-mapped buffer at 0xB8000",
          "Serial port COM1 (0x3F8) is initialized with 115200 baud, 8N1 configuration",
          "kprintf-style function supports basic format specifiers (%c, %s, %d, %x, %p) and outputs to both VGA and serial",
          "Linker script places .text section at 0x100000 with proper section alignment (4KB page-aligned sections)",
          "Kernel boots successfully in QEMU and displays welcome message on both VGA console and serial output",
          "Build system produces valid bootable disk image with bootloader in sector 0 and kernel starting at sector 2"
        ],
        "pitfalls": [
          "GDT misconfiguration is the #1 cause of triple-faults; verify base=0, limit=0xFFFFF, granularity=4KB, and correct access bytes for each segment",
          "Forgetting the far jump after setting CR0.PE leaves the CPU in an inconsistent state with real-mode CS",
          "Not disabling interrupts (cli) before loading GDT and entering protected mode; stale IVT entries cause immediate faults",
          "Linker script must place kernel at the correct virtual/physical address; mismatch causes garbage execution",
          "BSS is not guaranteed to be zero on bare metal; zeroing it is the kernel's responsibility (no CRT0)",
          "A20 line must be enabled for addresses above 1MB; on some hardware it's disabled by default"
        ],
        "concepts": [
          "x86 boot process (BIOS \u2192 MBR \u2192 kernel)",
          "Global Descriptor Table and segmentation",
          "Real mode \u2192 protected mode transition",
          "Freestanding C environment"
        ],
        "skills": [
          "x86 assembly for boot code",
          "GDT configuration",
          "Linker script design",
          "Serial port I/O",
          "VGA text mode"
        ],
        "deliverables": [
          "Bootloader (stage1 + optional stage2) loading kernel from disk",
          "GDT with null, kernel code, kernel data, user code, user data descriptors",
          "Protected mode transition with far jump and segment register reload",
          "BSS zeroing and C entry point",
          "VGA text mode driver with color support",
          "Serial port debug output driver",
          "Linker script placing kernel sections at correct addresses"
        ],
        "estimated_hours": "20-30"
      },
      {
        "id": "build-os-m2",
        "name": "Interrupts, Exceptions, and Keyboard",
        "description": "Set up the IDT, PIC, interrupt handlers for CPU exceptions and hardware IRQs, and a PS/2 keyboard driver.\n",
        "acceptance_criteria": [
          "IDT contains 256 entries; entries 0-31 handle CPU exceptions (divide error, page fault, GPF, etc.) with descriptive error messages",
          "Interrupt handlers save all general-purpose registers on entry and restore them before iret; error code is popped for exceptions that push one",
          "PIC (8259) is remapped so IRQ0-7 map to vectors 32-39 and IRQ8-15 map to vectors 40-47, avoiding conflicts with CPU exception vectors 0-31",
          "EOI (End of Interrupt) is sent to the correct PIC (master or slave) at the end of each IRQ handler",
          "Timer interrupt (IRQ0, PIT channel 0) fires at a configurable frequency (e.g., 100Hz); a global tick counter is incremented on each interrupt",
          "Keyboard interrupt (IRQ1) reads PS/2 scancode from port 0x60 and converts to ASCII using a scancode-to-ASCII table; characters are placed in a keyboard buffer",
          "Double fault (exception 8) handler catches cascading faults and halts with a diagnostic message instead of triple-faulting",
          "Interrupts are enabled (sti) after IDT and PIC setup is complete",
          "IDT contains 256 entries with entries 0-31 configured for CPU exceptions including divide error, page fault, and general protection fault with descriptive error messages",
          "All interrupt handlers save general-purpose registers (pusha) and segment registers on entry and restore them before iret; handlers for exceptions 8, 10-14 account for the error code pushed by the CPU",
          "PIC 8259 is remapped so IRQ0-7 map to vectors 32-39 and IRQ8-15 map to vectors 40-47, avoiding conflicts with CPU exception vectors 0-31",
          "EOI is sent to the correct PIC (master for IRQ0-7, both master and slave for IRQ8-15) at the end of each IRQ handler before iret",
          "Timer interrupt (IRQ0 via PIT channel 0) fires at a configurable frequency (e.g., 100Hz) and increments a global tick counter on each interrupt",
          "Keyboard interrupt (IRQ1) reads PS/2 scancode from port 0x60, converts to ASCII using a scancode-to-ASCII table, and places characters in a circular keyboard buffer",
          "Double fault handler (exception 8) catches cascading faults, prints diagnostic information including error code, and halts the system instead of allowing a triple fault",
          "Interrupts are enabled (sti) only after IDT is loaded and PIC is remapped and configured"
        ],
        "pitfalls": [
          "Forgetting to send EOI causes the PIC to stop delivering interrupts; the system appears to hang",
          "PIC remapping: the default mapping overlaps CPU exceptions (IRQ0=vector 8=double fault); this must be remapped BEFORE enabling interrupts",
          "Not saving/restoring all registers in interrupt handlers causes mysterious register corruption in interrupted code",
          "Exceptions 8 (double fault), 10-14 push an error code; others don't; the handler must account for this or the stack frame is misaligned",
          "PS/2 keyboard sends make AND break codes; ignoring break codes causes repeated character issues"
        ],
        "concepts": [
          "Interrupt Descriptor Table",
          "Programmable Interrupt Controller (8259 PIC)",
          "CPU exception handling",
          "Hardware device drivers"
        ],
        "skills": [
          "IDT configuration",
          "PIC programming (ICW1-ICW4)",
          "Interrupt service routine development",
          "PS/2 keyboard protocol"
        ],
        "deliverables": [
          "IDT with 256 entries and proper gate types",
          "CPU exception handlers (0-31) with error messages",
          "PIC remapping and EOI handling",
          "PIT timer at configurable frequency with tick counter",
          "PS/2 keyboard driver with scancode-to-ASCII conversion",
          "Keyboard input buffer"
        ],
        "estimated_hours": "15-25"
      },
      {
        "id": "build-os-m3",
        "name": "Physical and Virtual Memory Management",
        "description": "Implement physical frame allocator, page tables for virtual memory, and a kernel heap allocator.\n",
        "acceptance_criteria": [
          "Physical memory map is obtained from multiboot info or E820 memory map; regions are classified as usable, reserved, or ACPI",
          "Physical frame allocator (bitmap or free-list) allocates and frees individual 4KB page frames; double-free and allocating reserved frames are prevented",
          "Page directory and page tables are set up for identity-mapping the first N MB (covering kernel + VGA + MMIO) and higher-half mapping the kernel (e.g., kernel at 0xC0000000 virtual)",
          "Paging is enabled by loading CR3 with the page directory physical address and setting CR0.PG bit",
          "TLB is flushed (invlpg or full CR3 reload) after modifying page table entries",
          "Page fault handler (exception 14) reads CR2 for faulting address and prints diagnostic info (address, error code: present/write/user)",
          "Kernel heap allocator (kmalloc/kfree) provides dynamic memory allocation from a dedicated virtual address range; uses the page allocator for backing frames",
          "Identity map is kept for low memory so VGA (0xB8000) and MMIO regions remain accessible at their physical addresses",
          "Physical memory map obtained from multiboot info or E820, with regions classified as usable/reserved/ACPI",
          "Physical frame allocator (bitmap or free-list) allocates and frees 4KB frames with double-free prevention",
          "Page directory and page tables configured for identity-mapping (kernel + VGA + MMIO) and higher-half kernel mapping (0xC0000000+)",
          "Paging enabled by loading CR3 with page directory physical address and setting CR0.PG bit",
          "TLB flushed with invlpg or CR3 reload after modifying page table entries",
          "Page fault handler reads CR2 for faulting address and prints diagnostic (address, error code bits for present/write/user)",
          "Kernel heap allocator (kmalloc/kfree) provides dynamic allocation from dedicated virtual range using page allocator for backing frames",
          "Identity map maintained for low memory so VGA (0xB8000) and MMIO regions remain accessible at physical addresses"
        ],
        "pitfalls": [
          "Enabling paging without identity-mapping the currently executing code causes an immediate page fault on the next instruction",
          "Not flushing the TLB after page table changes causes stale translations; invlpg flushes a single page, CR3 reload flushes all",
          "Page directory/table entries have specific flag bits (present, writable, user-accessible, write-through, cache-disable); wrong flags cause faults or security holes",
          "Physical frame allocator must not allocate frames used by the kernel binary, page tables, or multiboot data",
          "Higher-half kernel mapping requires the linker script to use virtual addresses while the boot code uses physical addresses until paging is on"
        ],
        "concepts": [
          "Physical memory allocation",
          {
            "x86 paging (two-level": "PD + PT on 32-bit)"
          },
          "Higher-half kernel mapping",
          "TLB management"
        ],
        "skills": [
          "Page table implementation",
          "Physical frame allocator",
          "CR3/CR0 register manipulation",
          "Kernel heap design"
        ],
        "deliverables": [
          "Memory map parser (E820 or multiboot)",
          "Physical frame allocator (bitmap-based)",
          "Page directory and page table setup",
          "Identity mapping + higher-half kernel mapping",
          "Paging enablement (CR0.PG + CR3)",
          "Page fault handler reading CR2",
          "Kernel heap allocator (kmalloc/kfree)"
        ],
        "estimated_hours": "30-45"
      },
      {
        "id": "build-os-m4",
        "name": "Processes and Preemptive Scheduling",
        "description": "Implement process control blocks, context switching, preemptive round-robin scheduling, TSS for ring transitions, and a basic system call interface.\n",
        "acceptance_criteria": [
          "Process control block (PCB) stores: PID, register state (EIP, ESP, EBP, general regs, EFLAGS), page directory pointer, process state (ready/running/blocked), and kernel stack pointer",
          "Context switch saves current process registers to its PCB and loads the next process's registers; implemented in assembly for correctness",
          "TSS (Task State Segment) is configured with the kernel stack pointer (SS0: ESP0) so the CPU knows which stack to use when transitioning from ring 3 to ring 0 on interrupt/syscall",
          "Timer interrupt (IRQ0) triggers the scheduler; scheduler selects the next ready process in round-robin order and performs a context switch",
          "At least 3 kernel-mode processes run concurrently, each printing to a different screen region, demonstrating preemptive multitasking",
          "User-mode processes: at least one process runs in ring 3 with its own page directory; accessing kernel memory from user mode triggers a page fault (user-bit not set)",
          "System call interface via INT 0x80: user-mode process triggers a software interrupt; kernel reads syscall number from EAX and arguments from EBX/ECX/EDX; at minimum implement sys_write and sys_exit",
          "TSS ESP0 is updated on every context switch to point to the current process's kernel stack top",
          "Process control block (PCB) stores PID, register state (EIP, ESP, EBP, general-purpose registers, EFLAGS), page directory pointer, process state (ready/running/blocked), and kernel stack pointer",
          "Context switch saves current process registers to its PCB and loads the next process's registers using assembly implementation for correctness",
          "TSS (Task State Segment) is configured with kernel stack pointer (SS0:ESP0) so the CPU knows which stack to use for ring 3 \u2192 ring 0 transitions",
          "Timer interrupt (IRQ0) triggers the scheduler which selects the next ready process in round-robin order and performs context switch",
          "User-mode processes run in ring 3 with their own page directory; accessing kernel memory triggers page fault due to supervisor-only bit",
          "System call interface via INT 0x80: kernel reads syscall number from EAX and arguments from EBX/ECX/EDX, implementing sys_write and sys_exit at minimum"
        ],
        "pitfalls": [
          "Context switch must save ALL registers including EFLAGS; missing a register causes subtle corruption that manifests much later",
          "TSS is required for ring 3 \u2192 ring 0 transitions; without it, the CPU doesn't know what kernel stack to use and triple-faults",
          "Stack corruption during context switch: each process needs its own kernel stack; reusing the same stack corrupts saved state",
          "Preemptive scheduling requires re-enabling interrupts after the context switch or the system freezes",
          "Not disabling interrupts during critical sections of the context switch causes nested interrupts and stack overflow",
          "User-mode processes need their own page directory with kernel pages mapped but marked supervisor-only"
        ],
        "concepts": [
          "Process control blocks",
          "Context switching (register save/restore)",
          "Preemptive scheduling with timer interrupts",
          "Task State Segment for privilege transitions",
          "System call interface"
        ],
        "skills": [
          "Context switch assembly",
          "TSS configuration",
          "Scheduler design",
          "Ring 0/Ring 3 transitions",
          "System call dispatch"
        ],
        "deliverables": [
          "Process control block structure",
          "Context switch routine (assembly)",
          "TSS setup and per-process ESP0 update",
          "Round-robin scheduler triggered by timer interrupt",
          "Kernel-mode multi-process demo",
          "User-mode process with ring 3 segments",
          "System call handler (INT 0x80) with sys_write and sys_exit"
        ],
        "estimated_hours": "35-55"
      }
    ],
    "domain": "systems"
  },
  "blueprint": {
    "title": "Build Your Own OS",
    "overview": "This project constructs an x86 operating system kernel from bare metal firmware through protected mode, hardware interrupt handling, virtual memory management, and preemptive multitasking with user-mode isolation. Starting from the very first byte the BIOS loads off disk, the learner will implement every layer that modern operating systems hide: the GDT that defines segmentation, the IDT that routes interrupts, the page tables that virtualize memory, and the TSS that enables privilege transitions. By the end, multiple user-mode processes will run concurrently under a preemptive round-robin scheduler, issuing system calls through INT 0x80.\n\nThe project is structured as four milestones that mirror the actual dependency chain of an x86 kernel: you cannot handle interrupts without a GDT, cannot manage memory without interrupt handlers for page faults, and cannot schedule processes without both memory isolation and timer interrupts. Each milestone builds precisely on the hardware mechanisms established in the previous one, creating a layered system where every abstraction is one the learner built themselves.\n\nThis is an expert-level, 120-200 hour endeavor in freestanding C (or Rust/Zig) and x86 assembly. The learner will debug triple-faults with QEMU and GDB, read Intel SDM entries for obscure register bits, and develop an intimate understanding of the contract between software and the x86 hardware platform.",
    "design_philosophy": "An operating system is the ultimate negotiation between software abstractions and hardware constraints. Every design decision \u2014 from GDT segment granularity to page table entry flags to TSS stack pointers \u2014 exists because the x86 CPU physically requires specific data structures at specific addresses with specific bit layouts before it will perform a privilege transition, translate a virtual address, or route an interrupt. This project teaches by forcing the learner to satisfy the CPU's requirements byte by byte, revealing that the 'magic' of modern OSes is actually an intricate but deterministic conversation with silicon. The atlas is structured to show not just what to build, but why the hardware demands it \u2014 connecting each software structure back to the CPU microarchitecture that consumes it.",
    "is_build_your_own": true,
    "prerequisites": {
      "assumed_known": [
        "x86 assembly language (AT&T or NASM syntax) \u2014 register operations, stack manipulation, addressing modes",
        "C programming in freestanding environments \u2014 no stdlib, manual memory management, volatile qualifiers",
        "Computer architecture fundamentals \u2014 registers, ALU, stack pointer, memory bus, instruction fetch cycle",
        "Binary and hexadecimal arithmetic \u2014 bit masking, shifting, two's complement",
        "Basic understanding of what an OS does from a user perspective (processes, files, memory)"
      ],
      "must_teach_first": [
        {
          "concept": "x86 privilege rings (ring 0 vs ring 3)",
          "depth": "intermediate",
          "when": "Milestone 1"
        },
        {
          "concept": "Segment descriptors and the flat memory model",
          "depth": "detailed",
          "when": "Milestone 1"
        },
        {
          "concept": "Real mode vs protected mode execution environment",
          "depth": "detailed",
          "when": "Milestone 1"
        },
        {
          "concept": "Interrupt descriptor types (trap gate, interrupt gate, task gate)",
          "depth": "intermediate",
          "when": "Milestone 2"
        },
        {
          "concept": "x86 two-level paging (PDE \u2192 PTE \u2192 physical frame)",
          "depth": "detailed",
          "when": "Milestone 3"
        },
        {
          "concept": "Hardware-enforced stack switching on privilege change",
          "depth": "detailed",
          "when": "Milestone 4"
        },
        {
          "concept": "Linker scripts for bare-metal \u2014 sections, VMA vs LMA",
          "depth": "basic",
          "when": "Milestone 1"
        }
      ]
    },
    "milestones": [
      {
        "id": "build-os-m1",
        "title": "Bootloader, GDT, and Kernel Entry",
        "anchor_id": "anchor-m1-bootloader-gdt",
        "summary": "Boot from BIOS, enable the A20 line, configure a 5-entry GDT for flat segmentation, transition from 16-bit real mode to 32-bit protected mode via CR0.PE and a far jump, load the kernel from disk to the 1MB mark, zero BSS, initialize VGA text mode and serial output, and call the C entry point. This milestone establishes the execution environment that every subsequent milestone depends on.",
        "misconception": "Developers assume protected mode is just 'bigger registers' \u2014 that switching from real mode to protected mode is like flipping a feature flag, and the CPU otherwise behaves the same way. They expect the transition to be: set a bit, done.",
        "reveal": "Protected mode is an entirely different CPU personality. The moment CR0.PE is set, every memory access is mediated through the GDT's segment descriptors. The instruction currently being fetched is still using a real-mode CS value \u2014 the CPU is in a schizophrenic half-state where the pipeline holds real-mode decoded instructions but the memory subsystem expects protected-mode segments. The far jump isn't a 'nice to have'; it's the only way to atomically load CS with a valid protected-mode selector and flush the prefetch queue. Without it, the very next instruction may decode against stale segment state and triple-fault. The GDT isn't configuration \u2014 it's the CPU's runtime data structure that it reads on every single memory access.",
        "cascade": [
          "Segment selectors \u2014 understanding that CS/DS/SS are indices into the GDT (not addresses) unlocks how ring 0 vs ring 3 isolation actually works at the hardware level",
          "Linker scripts (cross-domain: compiler toolchains) \u2014 VMA vs LMA distinction becomes obvious once you realize boot code runs at physical addresses but the kernel is linked at virtual addresses; this same concept appears in embedded firmware, PIC shared libraries, and UEFI applications",
          "CPU pipeline flush \u2014 the far jump requirement reveals that the x86 pipeline can hold inconsistent state, which connects to branch prediction, speculative execution, and even Spectre/Meltdown mitigations",
          "A20 line legacy \u2014 understanding the physical address wrapping of the 8086 connects to how backward compatibility constraints shape modern CPU design, similar to how UTF-8's design was constrained by ASCII compatibility",
          "Freestanding C runtime \u2014 realizing there's no CRT0 to zero BSS connects to how Go/Rust runtimes, JVM class loading, and even JavaScript engine initialization all bootstrap their own execution environments"
        ],
        "yaml_acceptance_criteria": [
          "Bootloader code fits within MBR constraints (512 bytes with 0x55AA signature at bytes 510-511) or implements two-stage loading",
          "Bootloader reads kernel binary from disk using BIOS INT 13h and loads it to physical address 0x100000 (1MB mark)",
          "A20 line is successfully enabled using at least one method (BIOS INT 15h/2401, fast A20 via port 0x92, or keyboard controller)",
          "GDT is configured with exactly 5 entries: null descriptor (index 0), kernel code segment (ring 0, base=0, limit=4GB, executable+readable), kernel data segment (ring 0, base=0, limit=4GB, writable), user code segment (ring 3), user data segment (ring 3)",
          "GDTR is loaded with lgdt instruction before setting CR0.PE",
          "Protected mode is entered by setting CR0.PE bit (bit 0 of CR0) to 1",
          "Far jump (jmp 0x08:label) is executed after CR0.PE is set to load CS with kernel code selector and flush the instruction pipeline",
          "All data segment registers (DS, ES, FS, GS, SS) are loaded with kernel data selector (0x10) after protected mode entry",
          "32-bit stack pointer (ESP) is initialized to a valid memory region below 1MB or above kernel load address",
          "Kernel entry point (in assembly) zeroes the BSS section from __bss_start to __bss_end as defined in linker script",
          "C entry point function is called with proper stack alignment and direction flag cleared (cld)",
          "VGA text mode driver writes characters with color attributes to memory-mapped buffer at 0xB8000",
          "Serial port COM1 (0x3F8) is initialized with 115200 baud, 8N1 configuration",
          "kprintf-style function supports basic format specifiers (%c, %s, %d, %x, %p) and outputs to both VGA and serial",
          "Linker script places .text section at 0x100000 with proper section alignment (4KB page-aligned sections)",
          "Kernel boots successfully in QEMU and displays welcome message on both VGA console and serial output",
          "Build system produces valid bootable disk image with bootloader in sector 0 and kernel starting at sector 2"
        ]
      },
      {
        "id": "build-os-m2",
        "title": "Interrupts, Exceptions, and Keyboard",
        "anchor_id": "anchor-m2-interrupts",
        "summary": "Set up the 256-entry IDT with proper gate descriptors, implement CPU exception handlers for vectors 0-31 (including the critical double fault handler), remap the 8259 PIC so hardware IRQs don't collide with CPU exceptions, implement PIT timer interrupts at 100Hz with a global tick counter, and build a PS/2 keyboard driver that converts scancodes to ASCII and buffers input. This milestone gives the kernel the ability to respond to hardware events \u2014 the foundation for everything from scheduling to I/O.",
        "misconception": "Developers think interrupts are like callbacks or event handlers in application programming \u2014 that they're just functions the OS 'registers' and the hardware 'calls' when something happens. They assume the CPU handles the mechanics of saving state, switching context, and returning cleanly.",
        "reveal": "The CPU does almost nothing automatically on interrupt entry beyond pushing 5 values (SS, ESP, EFLAGS, CS, EIP \u2014 and only SS/ESP if there's a privilege change) and optionally an error code. Everything else is YOUR responsibility: saving general-purpose registers, saving segment registers, acknowledging the interrupt controller (EOI), and restoring everything in exact reverse order. The error code asymmetry is particularly treacherous \u2014 exceptions 8, 10-14 push an error code but others don't, meaning your stack frame layout changes depending on which interrupt fired. If your handler assumes the wrong layout, it pops the error code as EIP and returns to a garbage address. Furthermore, the default PIC mapping puts IRQ0 at vector 8 \u2014 the same as the double fault exception \u2014 meaning a timer tick and a cascading CPU failure are literally indistinguishable until you remap the PIC.",
        "cascade": [
          "Interrupt latency and real-time systems \u2014 understanding the register save/restore overhead explains why RTOS kernels minimize ISR work and defer to bottom-half handlers, and why Linux has PREEMPT_RT patches",
          "EOI protocol and flow control (cross-domain: network protocols) \u2014 the PIC's EOI requirement is identical in concept to TCP ACKs: the sender (PIC) stops sending until the receiver (CPU) acknowledges, which is the same flow-control pattern used in USB, PCIe, and even HTTP/2 stream windowing",
          "Exception error codes and stack frame forensics \u2014 knowing that page faults (14) push an error code with present/write/user bits directly enables implementing demand paging, copy-on-write, and memory-mapped files in Milestone 3",
          "Circular buffer design \u2014 the keyboard buffer is a lock-free single-producer single-consumer ring buffer, the same structure used in io_uring, DPDK packet rings, and audio driver DMA buffers",
          "Double fault as a safety net \u2014 understanding exception 8 as the CPU's last-resort handler before triple-fault connects to watchdog timers, hardware NMI, and the broader concept of fail-safe hierarchies in distributed systems"
        ],
        "yaml_acceptance_criteria": [
          "IDT contains 256 entries; entries 0-31 handle CPU exceptions (divide error, page fault, GPF, etc.) with descriptive error messages",
          "Interrupt handlers save all general-purpose registers on entry and restore them before iret; error code is popped for exceptions that push one",
          "PIC (8259) is remapped so IRQ0-7 map to vectors 32-39 and IRQ8-15 map to vectors 40-47, avoiding conflicts with CPU exception vectors 0-31",
          "EOI (End of Interrupt) is sent to the correct PIC (master or slave) at the end of each IRQ handler",
          "Timer interrupt (IRQ0, PIT channel 0) fires at a configurable frequency (e.g., 100Hz); a global tick counter is incremented on each interrupt",
          "Keyboard interrupt (IRQ1) reads PS/2 scancode from port 0x60 and converts to ASCII using a scancode-to-ASCII table; characters are placed in a keyboard buffer",
          "Double fault (exception 8) handler catches cascading faults and halts with a diagnostic message instead of triple-faulting",
          "Interrupts are enabled (sti) after IDT and PIC setup is complete",
          "IDT contains 256 entries with entries 0-31 configured for CPU exceptions including divide error, page fault, and general protection fault with descriptive error messages",
          "All interrupt handlers save general-purpose registers (pusha) and segment registers on entry and restore them before iret; handlers for exceptions 8, 10-14 account for the error code pushed by the CPU",
          "PIC 8259 is remapped so IRQ0-7 map to vectors 32-39 and IRQ8-15 map to vectors 40-47, avoiding conflicts with CPU exception vectors 0-31",
          "EOI is sent to the correct PIC (master for IRQ0-7, both master and slave for IRQ8-15) at the end of each IRQ handler before iret",
          "Timer interrupt (IRQ0 via PIT channel 0) fires at a configurable frequency (e.g., 100Hz) and increments a global tick counter on each interrupt",
          "Keyboard interrupt (IRQ1) reads PS/2 scancode from port 0x60, converts to ASCII using a scancode-to-ASCII table, and places characters in a circular keyboard buffer",
          "Double fault handler (exception 8) catches cascading faults, prints diagnostic information including error code, and halts the system instead of allowing a triple fault",
          "Interrupts are enabled (sti) only after IDT is loaded and PIC is remapped and configured"
        ]
      },
      {
        "id": "build-os-m3",
        "title": "Physical and Virtual Memory Management",
        "anchor_id": "anchor-m3-memory",
        "summary": "Parse the E820 memory map to discover physical memory regions, implement a bitmap-based physical frame allocator for 4KB pages, construct two-level page tables (page directory + page tables) for both identity-mapping low memory and higher-half kernel mapping at 0xC0000000, enable paging via CR3/CR0.PG, implement a page fault handler that reads CR2 for diagnostic information, and build a kernel heap allocator (kmalloc/kfree) backed by the physical frame allocator. This milestone transforms the flat physical address space into an isolated virtual address space.",
        "misconception": "Developers think virtual memory is primarily about 'giving each process its own address space' \u2014 a software organizational tool. They assume the MMU is like a lookup table the OS consults, and that paging is mainly about isolation and convenience.",
        "reveal": "The MMU is not software consulting a table \u2014 it's dedicated hardware that intercepts every single memory access on every single instruction and translates it through the page table hierarchy before the access reaches the cache/memory bus. This happens on the critical path of every load, store, and instruction fetch. The page table isn't a data structure the OS 'uses'; it's a data structure the CPU hardware walks autonomously, in a format the silicon dictates down to individual bits. The TLB exists because this two-level walk (PDE \u2192 PTE \u2192 physical frame) would add 2 extra memory accesses to every instruction \u2014 a 3x slowdown \u2014 so the CPU caches translations. This means modifying a page table entry without flushing the TLB leaves the CPU using stale translations, and the resulting bugs are non-deterministic because TLB eviction depends on access patterns. The identity-mapping requirement during the paging transition reveals the deepest truth: the instruction enabling paging is fetched using a physical address, but the NEXT instruction is fetched using a virtual address. If the virtual mapping doesn't map that physical address to itself, the CPU faults on the instruction immediately after enabling paging.",
        "cascade": [
          "TLB shootdown and multicore scalability \u2014 understanding per-CPU TLBs explains why mmap/munmap are expensive on multi-core systems and why TLB shootdown IPIs are a major performance bottleneck in Linux, connecting to NUMA-aware allocation in databases like ScyllaDB",
          "Demand paging and copy-on-write (cross-domain: containerization) \u2014 the page fault handler you build here is the exact mechanism that enables fork(), lazy allocation, memory-mapped files, and Docker's overlay filesystem copy-on-write layers",
          "Cache coloring and page frame allocation \u2014 physical frame choice affects L1/L2 cache set mapping, connecting memory allocation to cache performance; jemalloc and mimalloc use this insight for arena design",
          "Higher-half kernel and ASLR \u2014 mapping the kernel at a fixed high address explains KASLR (randomizing that address), which connects to the broader security concept of address space layout randomization used in every modern OS and browser",
          "Hardware page table walkers \u2014 understanding that the CPU walks your data structure autonomously connects to how GPU texture samplers, IOMMU for DMA, and even network card descriptor rings all use hardware-walked in-memory data structures"
        ],
        "yaml_acceptance_criteria": [
          "Physical memory map is obtained from multiboot info or E820 memory map; regions are classified as usable, reserved, or ACPI",
          "Physical frame allocator (bitmap or free-list) allocates and frees individual 4KB page frames; double-free and allocating reserved frames are prevented",
          "Page directory and page tables are set up for identity-mapping the first N MB (covering kernel + VGA + MMIO) and higher-half mapping the kernel (e.g., kernel at 0xC0000000 virtual)",
          "Paging is enabled by loading CR3 with the page directory physical address and setting CR0.PG bit",
          "TLB is flushed (invlpg or full CR3 reload) after modifying page table entries",
          "Page fault handler (exception 14) reads CR2 for faulting address and prints diagnostic info (address, error code: present/write/user)",
          "Kernel heap allocator (kmalloc/kfree) provides dynamic memory allocation from a dedicated virtual address range; uses the page allocator for backing frames",
          "Identity map is kept for low memory so VGA (0xB8000) and MMIO regions remain accessible at their physical addresses",
          "Physical memory map obtained from multiboot info or E820, with regions classified as usable/reserved/ACPI",
          "Physical frame allocator (bitmap or free-list) allocates and frees 4KB frames with double-free prevention",
          "Page directory and page tables configured for identity-mapping (kernel + VGA + MMIO) and higher-half kernel mapping (0xC0000000+)",
          "Paging enabled by loading CR3 with page directory physical address and setting CR0.PG bit",
          "TLB flushed with invlpg or CR3 reload after modifying page table entries",
          "Page fault handler reads CR2 for faulting address and prints diagnostic (address, error code bits for present/write/user)",
          "Kernel heap allocator (kmalloc/kfree) provides dynamic allocation from dedicated virtual range using page allocator for backing frames",
          "Identity map maintained for low memory so VGA (0xB8000) and MMIO regions remain accessible at physical addresses"
        ]
      },
      {
        "id": "build-os-m4",
        "title": "Processes and Preemptive Scheduling",
        "anchor_id": "anchor-m4-processes",
        "summary": "Design and implement the process control block (PCB) storing PID, full register state, page directory pointer, and process state. Build an assembly context switch routine that saves and restores all registers including EFLAGS. Configure the TSS for ring 3 \u2192 ring 0 stack switching. Implement a round-robin scheduler triggered by the PIT timer interrupt. Create user-mode processes running in ring 3 with their own page directories, and implement a system call interface via INT 0x80 with at least sys_write and sys_exit. This milestone brings together every previous mechanism into a working multitasking kernel.",
        "misconception": "Developers think a context switch is like saving and loading a game \u2014 you serialize the 'state' of one process, deserialize another, and continue. They imagine it as a clean, well-defined operation with clear boundaries, like swapping JSON objects.",
        "reveal": "A context switch is a controlled explosion of paradoxes. The switch code itself is running as the OLD process when it starts and as the NEW process when it returns \u2014 the identity of 'who is running' changes mid-function. The stack pointer (ESP) swap is the moment of metamorphosis: before the swap, pushes/pops hit the old process's kernel stack; after the swap, they hit the new process's kernel stack. If you push 5 registers, swap ESP, then pop 5 registers, you pushed onto one process's stack and popped from a completely different process's stack \u2014 and this is correct behavior. The TSS adds another layer: it exists solely because when a user-mode process (ring 3) triggers an interrupt, the CPU needs to switch to a kernel stack BEFORE any interrupt handler code runs. The CPU reads ESP0 from the TSS hardware structure to find this stack \u2014 meaning the TSS must be updated on every context switch, or the next interrupt in the new process will use the OLD process's kernel stack, corrupting both. The scheduler runs inside a timer interrupt handler, meaning it must re-enable interrupts for the new process (or the system freezes) while keeping them disabled during the actual register swap (or nested interrupts corrupt the switch).",
        "cascade": [
          "Green threads and coroutines (cross-domain: Go/Rust/JavaScript) \u2014 understanding hardware context switching reveals that Go goroutines, Rust async tasks, and JavaScript promises all implement the same save/restore pattern in software, trading hardware privilege transitions for faster cooperative switching",
          "Spectre and Meltdown mitigations \u2014 per-process page directories and the ring 0/3 transition via TSS are exactly what KPTI (Kernel Page Table Isolation) modifies; understanding the normal mechanism makes the vulnerability and mitigation obvious",
          "Lock-free scheduling and priority inversion \u2014 the critical section during context switch (interrupts disabled) connects to real-time OS priority inversion problems, the Mars Pathfinder bug, and why Linux uses priority inheritance in futexes",
          "Container isolation \u2014 process page directories providing memory isolation is the same mechanism cgroups and namespaces extend for container runtimes like Docker/containerd; understanding ring 3 isolation explains what containers actually guarantee (and don't)",
          "System call overhead \u2014 understanding that INT 0x80 requires a full privilege transition (ring 3 \u2192 ring 0 via TSS, register save, dispatch, register restore, iret) explains why Linux added vDSO for gettimeofday and why io_uring batches syscalls"
        ],
        "yaml_acceptance_criteria": [
          "Process control block (PCB) stores: PID, register state (EIP, ESP, EBP, general regs, EFLAGS), page directory pointer, process state (ready/running/blocked), and kernel stack pointer",
          "Context switch saves current process registers to its PCB and loads the next process's registers; implemented in assembly for correctness",
          "TSS (Task State Segment) is configured with the kernel stack pointer (SS0: ESP0) so the CPU knows which stack to use when transitioning from ring 3 to ring 0 on interrupt/syscall",
          "Timer interrupt (IRQ0) triggers the scheduler; scheduler selects the next ready process in round-robin order and performs a context switch",
          "At least 3 kernel-mode processes run concurrently, each printing to a different screen region, demonstrating preemptive multitasking",
          "User-mode processes: at least one process runs in ring 3 with its own page directory; accessing kernel memory from user mode triggers a page fault (user-bit not set)",
          "System call interface via INT 0x80: user-mode process triggers a software interrupt; kernel reads syscall number from EAX and arguments from EBX/ECX/EDX; at minimum implement sys_write and sys_exit",
          "TSS ESP0 is updated on every context switch to point to the current process's kernel stack top",
          "Process control block (PCB) stores PID, register state (EIP, ESP, EBP, general-purpose registers, EFLAGS), page directory pointer, process state (ready/running/blocked), and kernel stack pointer",
          "Context switch saves current process registers to its PCB and loads the next process's registers using assembly implementation for correctness",
          "TSS (Task State Segment) is configured with kernel stack pointer (SS0:ESP0) so the CPU knows which stack to use for ring 3 \u2192 ring 0 transitions",
          "Timer interrupt (IRQ0) triggers the scheduler which selects the next ready process in round-robin order and performs context switch",
          "User-mode processes run in ring 3 with their own page directory; accessing kernel memory triggers page fault due to supervisor-only bit",
          "System call interface via INT 0x80: kernel reads syscall number from EAX and arguments from EBX/ECX/EDX, implementing sys_write and sys_exit at minimum"
        ]
      }
    ],
    "diagrams": [
      {
        "id": "diag-satellite-os-map",
        "title": "OS Kernel \u2014 Satellite System Map",
        "description": "The complete project-wide map showing all four milestones and their interdependencies. Shows the boot \u2192 interrupts \u2192 memory \u2192 processes pipeline, with data flow between components: GDT feeds into IDT gate descriptors, IDT page fault handler feeds into paging, PIT timer interrupt feeds into scheduler, page directories feed into per-process isolation. Every component ID is labeled and cross-referenced. This is the 'Home Base' diagram that orients the learner.",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "satellite"
      },
      {
        "id": "diag-m1-boot-sequence-timeline",
        "title": "x86 Boot Sequence \u2014 BIOS to C Entry Point",
        "description": "A detailed timeline/data_walk showing every step from power-on: BIOS POST \u2192 MBR load at 0x7C00 \u2192 stage2 load \u2192 INT 13h disk reads \u2192 A20 enable \u2192 GDT load \u2192 CR0.PE set \u2192 far jump \u2192 segment register reload \u2192 BSS zero \u2192 C entry. Each step annotated with the CPU mode (real/protected), address width (20-bit/32-bit), and what would happen if skipped.",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "street"
      },
      {
        "id": "diag-m1-memory-map-physical",
        "title": "Physical Memory Map at Boot",
        "description": "A structure_layout diagram showing the entire first 16MB of physical address space during boot: 0x0000-0x03FF (IVT), 0x0400-0x04FF (BDA), 0x7C00-0x7DFF (MBR), 0x7E00+ (stage2), 0xA0000-0xBFFFF (VGA), 0xB8000 (text mode buffer), 0x100000+ (kernel load address). Annotated with which regions are safe to use and which are reserved by BIOS/hardware.",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "microscopic"
      },
      {
        "id": "diag-m1-gdt-entry-bitfield",
        "title": "GDT Segment Descriptor \u2014 Byte-Level Layout",
        "description": "A structure_layout diagram showing the 8-byte GDT entry format with every bit field labeled: limit[0:15], base[0:15], base[16:23], access byte (P, DPL, S, E, DC, RW, A), flags (G, D/B, L, AVL), limit[16:19], base[24:31]. Shows the exact values for all 5 entries (null, kernel code 0x08, kernel data 0x10, user code 0x18, user data 0x20) with hex bytes. Highlights the non-contiguous layout of base and limit fields \u2014 a common source of bugs.",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "microscopic"
      },
      {
        "id": "diag-m1-real-to-protected-transition",
        "title": "Real Mode \u2192 Protected Mode \u2014 The Critical Transition",
        "description": "A before_after diagram showing CPU state immediately before and after the far jump. BEFORE: CR0.PE=1 but CS still holds real-mode value, prefetch queue contains real-mode decoded instructions, memory accesses use segment:offset. AFTER: CS=0x08 (kernel code selector), prefetch queue flushed, memory accesses go through GDT. Highlights the 'schizophrenic' half-state between CR0 set and far jump completed.",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "microscopic"
      },
      {
        "id": "diag-m1-segment-selector-resolution",
        "title": "Segment Selector \u2192 GDT \u2192 Linear Address Resolution",
        "description": "A data_walk showing how a segment selector value (e.g., 0x08) is decomposed into index (1), TI (0=GDT), RPL (0=ring 0), then used to index into the GDT to retrieve the base address and limit, producing a linear address. Shows the path for both kernel code (0x08) and user code (0x18 with RPL=3).",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "street"
      },
      {
        "id": "diag-m1-linker-script-sections",
        "title": "Linker Script \u2014 Section Layout and VMA vs LMA",
        "description": "A structure_layout showing the linker script's output sections (.text, .rodata, .data, .bss) mapped to virtual addresses starting at 0x100000, with 4KB page alignment. Shows __bss_start and __bss_end symbols, the VMA (where code thinks it runs) vs LMA (where bootloader loads it), and how the kernel entry point address is communicated to the bootloader.",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "street"
      },
      {
        "id": "diag-m1-vga-serial-output",
        "title": "VGA Text Buffer and Serial Port \u2014 Memory-Mapped vs Port I/O",
        "description": "A before_after/structure_layout showing VGA text buffer at 0xB8000 with the 2-byte per character format (ASCII byte + attribute byte with foreground/background color), contrasted with serial port COM1 at I/O port 0x3F8 using in/out instructions. Shows the 8N1 UART configuration registers (DLAB, divisor latch, LCR, IER).",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "microscopic"
      },
      {
        "id": "diag-m1-a20-line-wrapping",
        "title": "A20 Line \u2014 Address Wrapping Problem",
        "description": "A before_after diagram showing memory addressing with A20 disabled (addresses above 1MB wrap to 0x0) vs enabled (addresses above 1MB access real memory). Shows the historical 8086 wrapping behavior and why the keyboard controller is involved. Illustrates the three enable methods: BIOS INT 15h, fast A20 port 0x92, keyboard controller.",
        "anchor_target": "anchor-m1-bootloader-gdt",
        "level": "street"
      },
      {
        "id": "diag-m2-idt-entry-format",
        "title": "IDT Gate Descriptor \u2014 Byte-Level Layout",
        "description": "A structure_layout showing the 8-byte IDT entry: offset[0:15], segment selector (always 0x08 for kernel code), reserved byte, type/attributes (P, DPL, gate type: 0xE for 32-bit interrupt gate, 0xF for trap gate), offset[16:31]. Shows the split offset field and how the CPU reconstructs the full ISR address. Contrasts interrupt gate (clears IF) vs trap gate (leaves IF unchanged).",
        "anchor_target": "anchor-m2-interrupts",
        "level": "microscopic"
      },
      {
        "id": "diag-m2-interrupt-dispatch-flow",
        "title": "Interrupt Dispatch \u2014 Hardware to Handler and Back",
        "description": "A data_walk tracing a complete interrupt lifecycle: hardware signal \u2192 PIC arbitration \u2192 vector number on data bus \u2192 CPU reads IDT[vector] \u2192 pushes EFLAGS/CS/EIP (and error code for exceptions) \u2192 jumps to handler \u2192 handler pushes registers \u2192 handler body \u2192 EOI to PIC \u2192 pops registers \u2192 iret restores EFLAGS/CS/EIP. Shows exact stack contents at each stage.",
        "anchor_target": "anchor-m2-interrupts",
        "level": "street"
      },
      {
        "id": "diag-m2-interrupt-stack-frame",
        "title": "Interrupt Stack Frame \u2014 With and Without Error Code",
        "description": "A structure_layout showing two side-by-side stack frames: one for exceptions WITHOUT error code (e.g., divide error #0: EIP, CS, EFLAGS on stack) and one for exceptions WITH error code (e.g., page fault #14: error code, EIP, CS, EFLAGS). Shows how a unified handler stub must push a dummy error code for the no-error-code case to normalize the frame. Includes the extra SS/ESP pushed on privilege change.",
        "anchor_target": "anchor-m2-interrupts",
        "level": "microscopic"
      },
      {
        "id": "diag-m2-pic-remapping",
        "title": "8259 PIC Remapping \u2014 Before and After",
        "description": "A before_after diagram showing the default PIC mapping (IRQ0=vector 0, which collides with divide error; IRQ1=vector 1, colliding with debug exception, etc.) vs the remapped configuration (IRQ0-7 \u2192 vectors 32-39, IRQ8-15 \u2192 vectors 40-47). Shows the ICW1-ICW4 initialization sequence for both master and slave PICs, the cascade wiring (slave on IRQ2), and the OCW commands for masking and EOI.",
        "anchor_target": "anchor-m2-interrupts",
        "level": "street"
      },
      {
        "id": "diag-m2-pic-cascade-architecture",
        "title": "8259 PIC Master/Slave Cascade \u2014 Hardware Wiring",
        "description": "A structure_layout showing the dual-PIC architecture: master PIC at ports 0x20-0x21 with IRQ0-7, slave PIC at ports 0xA0-0xA1 with IRQ8-15, slave connected to master's IRQ2 input. Shows the EOI routing: IRQ0-7 need EOI to master only, IRQ8-15 need EOI to both slave AND master. Annotates which IRQ lines connect to which hardware (PIT=IRQ0, keyboard=IRQ1, cascade=IRQ2, COM ports, etc.).",
        "anchor_target": "anchor-m2-interrupts",
        "level": "microscopic"
      },
      {
        "id": "diag-m2-pit-timer-programming",
        "title": "PIT Channel 0 \u2014 Frequency Configuration",
        "description": "A trace_example showing how to program the PIT for 100Hz: write command byte 0x36 to port 0x43 (channel 0, lobyte/hibyte, mode 3 square wave), calculate divisor = 1193180 / 100 = 11932, write low byte to port 0x40, write high byte to port 0x40. Shows the relationship between base frequency (1.193182 MHz), divisor, and output frequency.",
        "anchor_target": "anchor-m2-interrupts",
        "level": "microscopic"
      },
      {
        "id": "diag-m2-keyboard-scancode-pipeline",
        "title": "PS/2 Keyboard \u2014 Scancode to ASCII Pipeline",
        "description": "A data_walk tracing a keypress from hardware to buffer: key down \u2192 PS/2 controller generates scancode set 1 \u2192 IRQ1 fires \u2192 handler reads port 0x60 \u2192 checks bit 7 for make/break \u2192 looks up scancode in ASCII table \u2192 pushes to circular buffer. Shows make code (0x1E for 'A') vs break code (0x9E), shift state tracking, and the ring buffer with read/write pointers.",
        "anchor_target": "anchor-m2-interrupts",
        "level": "street"
      },
      {
        "id": "diag-m2-exception-vector-table",
        "title": "CPU Exception Vectors 0-31 \u2014 Classification Table",
        "description": "A structure_layout showing all 32 CPU exceptions: vector number, name, type (fault/trap/abort), whether an error code is pushed, and brief description. Highlights the critical ones: #0 divide error, #6 invalid opcode, #8 double fault (abort), #13 general protection fault, #14 page fault. Color-codes by severity and groups by error-code/no-error-code.",
        "anchor_target": "anchor-m2-interrupts",
        "level": "street"
      },
      {
        "id": "diag-m2-double-fault-cascade",
        "title": "Exception Cascading \u2014 Fault \u2192 Double Fault \u2192 Triple Fault",
        "description": "A state_evolution diagram showing how a single fault (e.g., page fault while handling a GPF) escalates: original exception \u2192 CPU tries to invoke handler \u2192 second exception during handler dispatch \u2192 double fault (#8) \u2192 if double fault handler itself faults \u2192 triple fault \u2192 CPU reset. Shows why a working double fault handler is the last line of defense.",
        "anchor_target": "anchor-m2-interrupts",
        "level": "street"
      },
      {
        "id": "diag-m3-e820-memory-map",
        "title": "E820 Memory Map \u2014 Physical Memory Discovery",
        "description": "A structure_layout showing a typical E820 memory map from QEMU with 128MB RAM: 0x0-0x9FFFF (usable, 640KB), 0xA0000-0xFFFFF (reserved, VGA/ROM), 0x100000-0x7FFFFFF (usable, ~127MB), reserved ACPI regions. Shows the E820 entry structure (base, length, type) and how the frame allocator must skip reserved regions and the kernel's own memory.",
        "anchor_target": "anchor-m3-memory",
        "level": "street"
      },
      {
        "id": "diag-m3-bitmap-allocator",
        "title": "Bitmap Frame Allocator \u2014 Structure and Operations",
        "description": "A structure_layout showing the bitmap array where each bit represents a 4KB physical frame: bit 0 = frame at 0x0, bit 1 = frame at 0x1000, etc. Shows the initial state with kernel frames, page table frames, and reserved regions marked as allocated. Demonstrates alloc (find first zero bit, set it, return frame address) and free (clear bit with double-free check). Includes the byte offset calculation: frame_number / 8 = byte index, frame_number % 8 = bit index.",
        "anchor_target": "anchor-m3-memory",
        "level": "microscopic"
      },
      {
        "id": "diag-m3-two-level-page-table",
        "title": "x86 Two-Level Paging \u2014 Virtual Address Translation",
        "description": "A data_walk showing a 32-bit virtual address split into PD index (bits 31:22, 10 bits \u2192 1024 entries), PT index (bits 21:12, 10 bits \u2192 1024 entries), and page offset (bits 11:0, 12 bits \u2192 4096 bytes). Traces CR3 \u2192 page directory \u2192 PDE \u2192 page table \u2192 PTE \u2192 physical frame + offset. Shows the 4MB coverage per PDE (1024 PTEs \u00d7 4KB) and total 4GB addressable space (1024 PDEs \u00d7 4MB).",
        "anchor_target": "anchor-m3-memory",
        "level": "street"
      },
      {
        "id": "diag-m3-pde-pte-bitfield",
        "title": "Page Directory/Table Entry \u2014 Bit-Level Layout",
        "description": "A structure_layout showing the 32-bit PDE and PTE formats: physical address (bits 31:12, page-aligned), AVL (bits 11:9), G (bit 8), PAT/0 (bit 7), D dirty (bit 6, PTE only), A accessed (bit 5), PCD (bit 4), PWT (bit 3), U/S user/supervisor (bit 2), R/W read/write (bit 1), P present (bit 0). Shows example entries for kernel pages (P=1, R/W=1, U/S=0) vs user pages (P=1, R/W=1, U/S=1). Highlights that the address field is only 20 bits because pages are 4KB-aligned.",
        "anchor_target": "anchor-m3-memory",
        "level": "microscopic"
      },
      {
        "id": "diag-m3-identity-plus-higher-half",
        "title": "Dual Mapping \u2014 Identity Map + Higher-Half Kernel",
        "description": "A before_after diagram showing virtual address space layout. Shows the identity map (virtual 0x0 \u2192 physical 0x0 for first N MB) coexisting with the higher-half map (virtual 0xC0000000 \u2192 physical 0x100000 for kernel). Explains why both are needed during the transition: code executing at physical address must still work after paging is enabled (identity map), but the kernel's linked addresses are at 0xC0000000+ (higher-half). Shows which PDE entries are populated for each mapping.",
        "anchor_target": "anchor-m3-memory",
        "level": "street"
      },
      {
        "id": "diag-m3-paging-enable-moment",
        "title": "The Paging Enable Moment \u2014 CR3 and CR0.PG",
        "description": "A state_evolution diagram showing the exact sequence: 1) Build page tables in physical memory, 2) Load CR3 with page directory physical address, 3) Set CR0.PG \u2014 AT THIS EXACT INSTRUCTION the MMU activates: the instruction that set CR0.PG was fetched using a physical address, but the NEXT instruction is fetched using a virtual address. Shows why identity mapping the currently executing code is mandatory. Shows the subsequent jump to the higher-half address to 'escape' the identity map.",
        "anchor_target": "anchor-m3-memory",
        "level": "microscopic"
      },
      {
        "id": "diag-m3-tlb-flush-scenarios",
        "title": "TLB \u2014 Caching Translations and Flush Requirements",
        "description": "A before_after diagram showing the TLB as a cache of virtual\u2192physical translations. Shows a scenario where a page table entry is modified (e.g., changing permissions) but the TLB still holds the old translation, causing the CPU to use stale permissions. Demonstrates invlpg (flushes one entry) vs CR3 reload (flushes all entries). Annotates the performance trade-off: invlpg is O(1) but per-page, CR3 reload flushes everything including still-valid entries.",
        "anchor_target": "anchor-m3-memory",
        "level": "street"
      },
      {
        "id": "diag-m3-page-fault-error-code",
        "title": "Page Fault (#14) \u2014 CR2 and Error Code Decoding",
        "description": "A trace_example showing a page fault scenario: user process accesses 0xC0100000 (kernel memory) \u2192 CPU walks page table \u2192 PTE has U/S=0 (supervisor only) \u2192 page fault \u2192 CPU pushes error code (bits: P=1 present, W/R=0 read, U/S=1 user mode) and stores 0xC0100000 in CR2. Shows how to decode the error code bits to determine fault cause. Contrasts with a not-present fault (P=0) for demand paging.",
        "anchor_target": "anchor-m3-memory",
        "level": "microscopic"
      },
      {
        "id": "diag-m3-kernel-heap-architecture",
        "title": "Kernel Heap (kmalloc/kfree) \u2014 Virtual Memory Backing",
        "description": "A structure_layout showing the kernel heap occupying a virtual address range (e.g., 0xD0000000-0xDFFFFFFF), with the heap allocator (simple free-list or boundary-tag) managing sub-page allocations, and each page of heap space backed by a physical frame obtained from the bitmap allocator. Shows the layered relationship: kmalloc \u2192 heap free-list \u2192 page allocator \u2192 bitmap \u2192 physical frames.",
        "anchor_target": "anchor-m3-memory",
        "level": "street"
      },
      {
        "id": "diag-m3-virtual-address-space-layout",
        "title": "Complete Virtual Address Space Layout",
        "description": "A structure_layout showing the full 4GB virtual address space after paging is enabled: 0x0-0xBFFFFFFF (user space, 3GB), 0xC0000000-0xC0FFFFFF (kernel code/data, higher-half), 0xC1000000+ (kernel heap), VGA at identity-mapped 0xB8000, page tables recursively mapped (optional). Shows the clean separation between user and kernel memory regions.",
        "anchor_target": "anchor-m3-memory",
        "level": "street"
      },
      {
        "id": "diag-m4-pcb-structure-layout",
        "title": "Process Control Block \u2014 Byte-Level Structure",
        "description": "A structure_layout showing the PCB struct with exact field offsets: PID (offset 0, 4 bytes), state enum (offset 4, 4 bytes), EIP (offset 8), ESP (offset 12), EBP (offset 16), EAX-EDI (offsets 20-48), EFLAGS (offset 52), CR3/page_directory (offset 56), kernel_stack_top (offset 60), next pointer for ready queue (offset 64). Shows total size and cache line analysis (fits in 1-2 cache lines). Annotates which fields are saved by hardware vs software during context switch.",
        "anchor_target": "anchor-m4-processes",
        "level": "microscopic"
      },
      {
        "id": "diag-m4-context-switch-assembly",
        "title": "Context Switch \u2014 Register Save/Restore Trace",
        "description": "A data_walk tracing the assembly context switch instruction by instruction: push EAX through push EDI \u2192 save ESP to old_pcb.esp \u2192 load ESP from new_pcb.esp \u2192 pop EDI through pop EAX \u2192 ret (pops new EIP). Shows the stack contents at each step, highlighting the moment ESP changes as the 'identity swap'. Color-codes which stack is active (old process blue, new process green).",
        "anchor_target": "anchor-m4-processes",
        "level": "microscopic"
      },
      {
        "id": "diag-m4-context-switch-stacks",
        "title": "Two Kernel Stacks \u2014 The ESP Swap Moment",
        "description": "A before_after diagram showing two kernel stacks side by side. BEFORE: ESP points to process A's stack with saved registers. The single instruction 'mov esp, [new_pcb + ESP_OFFSET]' switches. AFTER: ESP points to process B's stack with its previously saved registers. Shows that push/pop before the swap affect stack A, and push/pop after affect stack B. This is the 'metamorphosis' moment.",
        "anchor_target": "anchor-m4-processes",
        "level": "street"
      },
      {
        "id": "diag-m4-tss-structure",
        "title": "Task State Segment \u2014 Structure and SS0:ESP0",
        "description": "A structure_layout showing the 104-byte TSS with all fields, highlighting SS0 (offset 0x08) and ESP0 (offset 0x04) as the critical fields for ring 3\u21920 transitions. Shows the TSS descriptor in the GDT (entry at index 5, selector 0x28), the ltr instruction to load it, and the requirement to update ESP0 on every context switch. All other TSS fields can be zero for software task switching.",
        "anchor_target": "anchor-m4-processes",
        "level": "microscopic"
      },
      {
        "id": "diag-m4-ring-transition-mechanism",
        "title": "Ring 3 \u2192 Ring 0 \u2014 The Complete Privilege Transition",
        "description": "A data_walk tracing a user-mode interrupt (INT 0x80 syscall): CPU detects CPL=3\u2192DPL=0 transition \u2192 reads SS0:ESP0 from TSS \u2192 switches to kernel stack \u2192 pushes user SS, user ESP, EFLAGS, user CS, user EIP \u2192 jumps to IDT handler \u2192 handler runs in ring 0. On return: iret pops EIP, CS, EFLAGS, ESP, SS \u2192 detects CPL change \u2192 switches back to user stack. Shows both stacks and all pushed/popped values.",
        "anchor_target": "anchor-m4-processes",
        "level": "street"
      },
      {
        "id": "diag-m4-scheduler-flow",
        "title": "Round-Robin Scheduler \u2014 Timer Interrupt to Context Switch",
        "description": "A state_evolution diagram showing the complete scheduling flow: Process A running \u2192 PIT fires IRQ0 \u2192 interrupt pushes registers \u2192 scheduler function called \u2192 scheduler picks next ready process (B) from circular queue \u2192 context switch to B \u2192 B's registers restored \u2192 iret returns to B's code. Shows the ready queue as a circular linked list of PCBs with the current pointer advancing on each tick.",
        "anchor_target": "anchor-m4-processes",
        "level": "street"
      },
      {
        "id": "diag-m4-process-states",
        "title": "Process State Machine \u2014 Ready, Running, Blocked",
        "description": "A state_evolution diagram showing process state transitions: CREATED \u2192 READY (added to ready queue), READY \u2192 RUNNING (selected by scheduler), RUNNING \u2192 READY (timer preemption), RUNNING \u2192 BLOCKED (waiting for I/O or syscall), BLOCKED \u2192 READY (I/O complete or event), RUNNING \u2192 TERMINATED (sys_exit). Annotates each transition with what triggers it and what kernel code performs it.",
        "anchor_target": "anchor-m4-processes",
        "level": "street"
      },
      {
        "id": "diag-m4-user-mode-page-directory",
        "title": "Per-Process Page Directory \u2014 User/Kernel Split",
        "description": "A structure_layout showing two page directories side by side (Process A and Process B). Both share the same kernel page table entries (PDEs 768-1023 for 0xC0000000+) but have different user-space entries (PDEs 0-767). Shows that kernel pages have U/S=0 (supervisor only) while user pages have U/S=1. Demonstrates how switching CR3 changes the user-space mapping while kernel space remains identical.",
        "anchor_target": "anchor-m4-processes",
        "level": "street"
      },
      {
        "id": "diag-m4-syscall-dispatch",
        "title": "System Call Dispatch \u2014 INT 0x80 to Handler Table",
        "description": "A data_walk showing a user process calling sys_write: user code sets EAX=1 (syscall number), EBX=fd, ECX=buffer pointer, EDX=length \u2192 INT 0x80 \u2192 ring transition via TSS \u2192 syscall handler reads EAX \u2192 indexes into syscall_table[EAX] \u2192 calls sys_write_impl(ebx, ecx, edx) \u2192 return value placed in EAX \u2192 iret back to user mode. Shows the syscall table as a function pointer array.",
        "anchor_target": "anchor-m4-processes",
        "level": "street"
      },
      {
        "id": "diag-m4-interrupt-reentrancy",
        "title": "Interrupt Enable/Disable \u2014 Critical Section During Switch",
        "description": "A trace_example showing the dangerous window: timer interrupt fires \u2192 scheduler begins context switch \u2192 if interrupts are enabled, another timer interrupt could fire mid-switch \u2192 nested switch corrupts partially-saved state. Shows the cli/sti placement: cli before saving registers, switch ESP, then sti after the new process is fully loaded. Annotates that EFLAGS.IF in the new process's saved state must have interrupts enabled or the process freezes.",
        "anchor_target": "anchor-m4-processes",
        "level": "microscopic"
      },
      {
        "id": "diag-m4-three-process-demo",
        "title": "Three-Process Demo \u2014 Interleaved Execution Timeline",
        "description": "A trace_example showing a timeline of three kernel-mode processes (A, B, C) each printing to different VGA screen regions. Shows timer ticks interleaving execution: A runs for 10ms \u2192 tick \u2192 switch to B \u2192 B runs 10ms \u2192 tick \u2192 switch to C \u2192 tick \u2192 back to A. Demonstrates preemptive behavior: processes don't yield voluntarily, the timer forces switches. Shows VGA output growing in three separate regions simultaneously.",
        "anchor_target": "anchor-m4-processes",
        "level": "street"
      }
    ]
  },
  "accumulated_md": "# Build Your Own OS\n\nThis project constructs an x86 operating system kernel from bare metal firmware through protected mode, hardware interrupt handling, virtual memory management, and preemptive multitasking with user-mode isolation. Starting from the very first byte the BIOS loads off disk, the learner will implement every layer that modern operating systems hide: the GDT that defines segmentation, the IDT that routes interrupts, the page tables that virtualize memory, and the TSS that enables privilege transitions. By the end, multiple user-mode processes will run concurrently under a preemptive round-robin scheduler, issuing system calls through INT 0x80.\n\nThe project is structured as four milestones that mirror the actual dependency chain of an x86 kernel: you cannot handle interrupts without a GDT, cannot manage memory without interrupt handlers for page faults, and cannot schedule processes without both memory isolation and timer interrupts. Each milestone builds precisely on the hardware mechanisms established in the previous one, creating a layered system where every abstraction is one the learner built themselves.\n\nThis is an expert-level, 120-200 hour endeavor in freestanding C (or Rust/Zig) and x86 assembly. The learner will debug triple-faults with QEMU and GDB, read Intel SDM entries for obscure register bits, and develop an intimate understanding of the contract between software and the x86 hardware platform.\n\n\n\n<!-- MS_ID: build-os-m1 -->\n# Milestone 1: Bootloader, GDT, and Kernel Entry\n\n![OS Kernel \u2014 Satellite System Map](./diagrams/diag-satellite-os-map.svg)\n\nBefore any interrupt fires, before any page table exists, before any process runs \u2014 there is this milestone. Every mechanism you will build in the next three milestones depends on a specific, precise contract being established here: the CPU must be in 32-bit protected mode, segment registers must point at valid GDT entries, and a C function must be reachable at a known physical address. Get this wrong by a single bit and the machine triple-faults silently. Get it right and you have created an execution environment from nothing.\nThis is the hardest kind of systems work: debugging code that runs before any debugger knows the machine exists.\n---\n## The World Before Your Kernel\n\n![x86 Boot Sequence \u2014 BIOS to C Entry Point](./diagrams/diag-m1-boot-sequence-timeline.svg)\n\nWhen you press the power button, the CPU starts executing at physical address `0xFFFFFFF0` \u2014 the very top of 32-bit addressable space. The chip at that address is not your operating system. It's firmware: on x86 systems, this is the BIOS (Basic Input/Output System \u2014 a ROM-resident program that has existed in essentially the same form since the IBM PC of 1981). The BIOS performs a Power-On Self-Test (POST), enumerates hardware, and then \u2014 crucially \u2014 searches for a bootable device.\nIt checks each storage device in order. For each disk, it reads the first 512-byte sector \u2014 the MBR, or Master Boot Record \u2014 and checks whether bytes 510 and 511 equal `0x55` and `0xAA`. If they do, the BIOS declares this sector executable, copies it to physical address `0x7C00`, and jumps to it.\nThe CPU at this point is running in **real mode**: a 16-bit execution environment with a 1 MB address limit, no memory protection, no virtual addressing, and a direct, unmediated view of physical hardware. Every program that ran on an original IBM PC ran in this mode. Your bootloader starts here.\n**The fundamental tension**: Your kernel is a substantial binary \u2014 potentially hundreds of kilobytes \u2014 that must eventually run in a fully-featured 32-bit environment. But you wake up with 512 bytes, a 1 MB address ceiling, no disk driver, no file system, and a CPU that has never heard of segments as protected-mode descriptors. You must bootstrap from this primitive state to a sophisticated one, using only the tools available at each stage to reach the tools needed for the next.\n---\n## Stage 1: The 512-Byte Problem\n{{DIAGRAM:diag-m1-memory-map-physical}}\nYour first constraint is absolute: the MBR is exactly 512 bytes, the last two of which must be `0x55AA`. Subtract 2 bytes for the signature, and you have 510 bytes in which to:\n- Set up a minimal stack\n- Enable the A20 address line\n- Read the kernel from disk\n- Transition to protected mode (or hand off to a stage-2 loader)\n- Jump to the kernel\n510 bytes is not enough to do all of this comfortably if your kernel requires disk reads with error handling, a full GDT setup, and a memory map query. This is why most real bootloaders (GRUB, SYSLINUX, U-Boot) use **two-stage loading**: the 512-byte stage 1 does the absolute minimum \u2014 load a larger stage 2 from a fixed disk location \u2014 and the stage 2 does everything else with far more room.\nFor this project, you have a choice:\n| Approach | Complexity | Flexibility | Used By |\n|---|---|---|---|\n| **Single-stage (stage 1 only)** | Lower | Limited to ~450 bytes | Minimal toy OSes |\n| **Two-stage \u2713** | Moderate | Stage 2 can be kilobytes | GRUB, SYSLINUX, most real OSes |\n| **GRUB/Multiboot** | Low (for your code) | Maximum | Linux, most production kernels |\nThe two-stage approach is the right learning experience. Stage 1 fits in 512 bytes, loads stage 2 from a known disk location (sectors 2\u2013N), and jumps to it. Stage 2 has kilobytes of room to set up the GDT properly, enable the A20 line with fallback methods, read the kernel, and perform the protected-mode transition.\n### Real Mode Addressing: How It Works\nIn real mode, the CPU computes physical addresses as:\n```\nphysical_address = segment_register \u00d7 16 + offset\n```\nSo `CS:IP = 0x07C0:0x0000` gives physical address `0x07C00`. This segmented addressing is why real mode can address only 1 MB: segments are 16-bit, so the maximum segment value is `0xFFFF`, the maximum offset is `0xFFFF`, and `0xFFFF \u00d7 16 + 0xFFFF = 0x10FFEF` \u2014 just over 1 MB. The A20 issue (coming shortly) is precisely about what happens to that extra bit.\nYour stage-1 bootsector will execute at `0x7C00`. Set up your stack immediately:\n```nasm\n[BITS 16]\n[ORG 0x7C00]\nstart:\n    cli                     ; Disable interrupts \u2014 we're touching segment regs\n    xor ax, ax\n    mov ds, ax\n    mov es, ax\n    mov ss, ax\n    mov sp, 0x7C00          ; Stack grows down from 0x7C00 (below our code)\n    sti\n    ; Load stage 2 from disk sectors 2-9 (8 sectors = 4KB)\n    mov bx, 0x7E00          ; Load stage 2 right after MBR\n    mov ah, 0x02            ; BIOS INT 13h function: read sectors\n    mov al, 8               ; Read 8 sectors\n    mov ch, 0               ; Cylinder 0\n    mov cl, 2               ; Start at sector 2 (sector 1 is the MBR)\n    mov dh, 0               ; Head 0\n    int 0x13                ; BIOS disk read\n    jc disk_error           ; CF set on error\n    jmp 0x0000:0x7E00       ; Jump to stage 2\ndisk_error:\n    hlt\ntimes 510 - ($ - $$) db 0  ; Pad to 510 bytes\ndw 0xAA55                   ; Boot signature (little-endian: 0x55, 0xAA)\n```\n> **Note on `dw 0xAA55` vs `db 0x55, 0xAA`**: x86 is little-endian. When you write `dw 0xAA55`, NASM stores the low byte (`0x55`) first, then the high byte (`0xAA`). The BIOS reads bytes 510 and 511 as `0x55` and `0xAA` respectively. Either form works; be aware of what you're writing.\n---\n## The A20 Line: A 40-Year-Old Backwards Compatibility Hack\n\n![A20 Line \u2014 Address Wrapping Problem](./diagrams/diag-m1-a20-line-wrapping.svg)\n\nHere is one of the most bizarre stories in PC history. The original Intel 8086 had a 20-bit address bus \u2014 address lines A0 through A19 \u2014 which allowed it to address exactly 1 MB (`2^20` bytes). Programs took advantage of a quirk: accessing memory above `0xFFFFF` (the top of 1 MB) would wrap around to address `0x00000 + overflow`. This was undefined behavior, but some 8086 programs accidentally relied on it.\nWhen the 80286 arrived with 24 address lines and the ability to address 16 MB, those programs broke \u2014 because now address `0x100000` was an actual, distinct memory location instead of wrapping to `0x00000`. IBM's solution was to gate the 21st address line (A20) through the keyboard controller: leave A20 disabled by default (forcing wrap-around, preserving 8086 behavior), and let software enable it when it needs to access memory above 1 MB.\nThis is why your bootloader must explicitly enable A20. Without it, any access to addresses `0x100000`\u2013`0x10FFEF` silently wraps to `0x00000`\u2013`0x0FFEF`. Your kernel will be loaded to `0x100000`, and that load will appear to overwrite low memory. The system will silently corrupt itself.\n**Three methods to enable A20**, in order of reliability:\n**Method 1: BIOS INT 15h, function 0x2401**\n```nasm\nmov ax, 0x2401\nint 0x15\njc a20_failed   ; Carry flag set on error\n```\nClean, but not all BIOS implementations support it. Try it first.\n**Method 2: Fast A20 via I/O port 0x92**\n```nasm\nin al, 0x92\nor al, 0x02     ; Set bit 1 (A20 enable)\nand al, 0xFE    ; Clear bit 0 (do NOT reset the machine!)\nout 0x92, al\n```\nSupported on most modern systems. Fast and simple, but originally a PS/2 extension.\n**Method 3: Keyboard controller (8042)**\n```nasm\ncall wait_kbd_cmd\nmov al, 0xAD        ; Disable keyboard\nout 0x64, al\ncall wait_kbd_cmd\nmov al, 0xD0        ; Read output port\nout 0x64, al\ncall wait_kbd_data\nin al, 0x60         ; Read current value\npush ax\ncall wait_kbd_cmd\nmov al, 0xD1        ; Write output port\nout 0x64, al\ncall wait_kbd_cmd\npop ax\nor al, 0x02         ; Set A20 bit\nout 0x60, al\ncall wait_kbd_cmd\nmov al, 0xAE        ; Enable keyboard\nout 0x64, al\nwait_kbd_cmd:\n    in al, 0x64\n    test al, 0x02   ; Wait until input buffer empty\n    jnz wait_kbd_cmd\n    ret\nwait_kbd_data:\n    in al, 0x64\n    test al, 0x01   ; Wait until output buffer full\n    jz wait_kbd_data\n    ret\n```\nThe most reliable method but verbose. Required on some older hardware.\n**Verify A20 is enabled**: Write a value to address `0x7DFE` (just below the bootsector) and check whether the same value appears at `0x17FFE` (`0x7DFE + 0x10000` \u2014 one segment above, with wrap-around). If they differ, A20 is active.\n> **Knowledge Cascade \u2014 Backwards Compatibility as CPU Design Constraint**: The A20 story is a microcosm of how x86 design has been shaped by backward compatibility obligations for 40 years. The same philosophy explains why modern x86 CPUs boot in 16-bit real mode (IBM PC compatibility), why the legacy 8259 PIC is still present alongside the APIC, and why the first 640 KB of physical memory is forever \"conventional memory.\" Understanding A20 helps you see that modern x86 CPUs are archaeological sites \u2014 each layer of hardware is partially constrained by decisions made in 1978. Compare this to UTF-8: designed to be backward-compatible with ASCII (0x00\u20130x7F are identical), which is why it became universal even though UCS-2 was technically cleaner.\n---\n## Loading the Kernel from Disk\nYour kernel binary must reach physical address `0x100000` (the 1 MB mark \u2014 chosen specifically to be above the BIOS data area, above the VGA memory at `0xB8000`, and above the initial stack). Use BIOS INT 13h extended read (function `0x42`) if available, which supports 28-bit LBA addressing:\n```nasm\n; Disk Address Packet for INT 13h extended read\ndap:\n    db 0x10         ; Size of DAP = 16 bytes\n    db 0            ; Reserved\n    dw 64           ; Number of sectors to read (64 \u00d7 512 = 32KB)\n    dw 0x0000       ; Target offset\n    dw 0x1000       ; Target segment (0x1000:0x0000 = physical 0x10000)\n                    ; NOTE: Use segment 0x1000 to reach 0x10000 first,\n                    ; then copy to 0x100000 after A20+protected mode\n    dd 2            ; LBA start sector (low 32 bits)\n    dd 0            ; LBA start sector (high 32 bits)\nload_kernel:\n    mov si, dap\n    mov ah, 0x42\n    mov dl, 0x80    ; Drive number (0x80 = first hard disk)\n    int 0x13\n    jc disk_error\n```\n> **Why not load directly to `0x100000`?** In real mode, segment registers are 16-bit, and the physical address formula limits you to a maximum of `0xFFFF0 + 0xFFFF = 0x10FFEF`. You can reach `0x100000` with segment `0x1000` and offset `0x0000` \u2014 but only after A20 is enabled. The BIOS INT 13h approach works fine for loading into the low megabyte. For kernels that must be above 1 MB, enable A20 first, then use the `unreal mode` trick (briefly enter protected mode to set descriptor cache registers, then return to real mode with 32-bit offsets) \u2014 or simply load low and copy high after entering protected mode. The simplest approach: load at `0x10000` (64KB), enter protected mode, then `rep movsd` the kernel to `0x100000`.\n---\n## The GDT: The CPU's Runtime Segmentation Database\nThis is where the real architecture begins. The Global Descriptor Table (GDT) is not configuration you write once and forget. It is a data structure the CPU **reads on every single memory access** to determine whether that access is permitted, what privilege level it requires, and how to translate segment-relative addresses.\n### Segment Descriptors: The 8-Byte Contract\n{{DIAGRAM:diag-m1-gdt-entry-bitfield}}\nEvery entry in the GDT is exactly 8 bytes (64 bits). The layout is, historically, bizarre \u2014 Intel spread the base address and limit fields across non-contiguous bytes for backward compatibility with the 80286's 24-bit descriptors. Understanding the bit layout is mandatory because you will construct these entries by hand.\n```\nByte:   7        6        5        4        3        2        1        0\n       [Base    ][Flags+  ][Access  ][Base   ][Base   ][Limit  ][Limit  ]\n       [31:24   ][Limit   ][Byte    ][23:16  ][15:08  ][15:08  ][07:00  ]\n                [19:16   ]\n```\nLet's decode each field:\n**Base (32 bits, split across bytes 2, 3, 4, 7)**: The physical starting address of this segment. For a flat memory model, this is always `0x00000000`.\n**Limit (20 bits, split across bytes 0\u20131 and the low 4 bits of byte 6)**: The maximum addressable unit within the segment. With granularity bit set (G=1), this is in 4KB pages \u2014 so a limit of `0xFFFFF` means `0xFFFFF \u00d7 4096 + 4095 = 0xFFFFFFFF` (all 4 GB).\n**Access byte (byte 5)**:\n```\nBit 7: Present (P) \u2014 must be 1 for valid descriptor\nBit 6-5: DPL \u2014 Descriptor Privilege Level (0 = ring 0/kernel, 3 = ring 3/user)\nBit 4: Descriptor type (1 = code/data segment, 0 = system descriptor)\nBit 3: Executable (1 = code segment, 0 = data segment)\nBit 2: Direction/Conforming \u2014 for data: grows down if 1; for code: conforming if 1\nBit 1: Readable/Writable (code: readable; data: writable)\nBit 0: Accessed \u2014 CPU sets this on access; leave 0\n```\n**Flags nibble (high 4 bits of byte 6)**:\n```\nBit 7 (G): Granularity \u2014 0 = limit in bytes, 1 = limit in 4KB pages\nBit 6 (D/B): Default operation size \u2014 1 = 32-bit segment\nBit 5 (L): 64-bit code segment (set for long mode; leave 0 for 32-bit)\nBit 4: Available for OS use\n```\n\n> **\ud83d\udd11 Foundation: x86 privilege rings**\n> \n> ## x86 Privilege Rings\n### What It Is\nThe x86 architecture enforces hardware-level isolation between software layers using a system of **privilege rings** \u2014 numbered 0 through 3, where lower numbers mean more privilege. Think of them as concentric security boundaries burned into the CPU itself.\n- **Ring 0 (kernel mode):** Unrestricted access. Code here can execute any instruction, access any memory, reprogram hardware, modify the CPU's own control registers (`CR0`, `CR3`, `CR4`), and install interrupt handlers. Your kernel runs here.\n- **Ring 3 (user mode):** Heavily restricted. Code here cannot execute privileged instructions (like `hlt`, `in`, `out`, or loading segment registers with kernel selectors), cannot directly access hardware, and is confined to its own virtual address space. Applications run here.\n- **Rings 1 and 2:** Defined by the architecture but almost universally unused in modern systems. Linux and Windows use only rings 0 and 3.\n### The Enforcement Mechanism: CPL, DPL, and RPL\nThe CPU enforces privilege through three values embedded in segment selectors and descriptors:\n| Field | Stands For | Where It Lives |\n|-------|-----------|----------------|\n| **CPL** | Current Privilege Level | Low 2 bits of `CS` register |\n| **DPL** | Descriptor Privilege Level | Inside the segment/gate descriptor in the GDT/LDT |\n| **RPL** | Requested Privilege Level | Low 2 bits of any segment selector |\n**The rule:** Access is granted only if `max(CPL, RPL) \u2264 DPL`.\nIn plain terms: to access a segment, the *least privileged* of the current code and the requestor must still be *at least as privileged as* the segment requires. DPL acts as a minimum-privilege gate.\n**Concrete example:** A kernel data segment has `DPL=0`. A user-mode process with `CPL=3` tries to load that segment selector into `DS`. The CPU computes `max(3, RPL) \u2265 1 > 0`, so it raises a **General Protection Fault (#GP)**. Hardware enforced, no software check needed.\n**How ring transitions happen:** You can't just jump to ring 0. Controlled transitions occur through:\n- **System calls** via `int 0x80`, `sysenter`, or `syscall` instructions \u2014 these atomically switch CPL, stack, and instruction pointer through a call gate or MSR-defined entry point.\n- **Interrupts and exceptions** \u2014 the CPU saves user state and switches to a kernel-defined handler at ring 0.\n- **`iret`** \u2014 returns from interrupt/exception, restoring the previous CPL.\n### Key Insight\n> **Rings are enforced by the CPU on every memory access and privileged instruction \u2014 not by software.** Your kernel doesn't check if user code is \"allowed\" to read kernel memory; the hardware simply won't allow it if the page tables and segment DPLs are set correctly. This is why a buggy kernel that accidentally maps kernel pages as user-accessible (`U/S` bit set in the page table) is a catastrophic security vulnerability \u2014 you've defeated the hardware guard yourself.\n\n### The Five Entries You Need\nYour GDT must have exactly these five entries:\n```c\n// Descriptor encoded as two 32-bit halves for clarity\nstruct gdt_entry {\n    uint16_t limit_low;     // Limit bits 0-15\n    uint16_t base_low;      // Base bits 0-15\n    uint8_t  base_mid;      // Base bits 16-23\n    uint8_t  access;        // Access byte\n    uint8_t  flags_limit;   // Flags[7:4] + Limit[19:16]\n    uint8_t  base_high;     // Base bits 24-31\n} __attribute__((packed));\n```\n| Index | Selector | Purpose | Base | Limit | Access | DPL |\n|---|---|---|---|---|---|---|\n| 0 | `0x00` | Null descriptor | 0 | 0 | `0x00` | \u2014 |\n| 1 | `0x08` | Kernel code | 0 | 4GB | `0x9A` | Ring 0 |\n| 2 | `0x10` | Kernel data | 0 | 4GB | `0x92` | Ring 0 |\n| 3 | `0x18` | User code | 0 | 4GB | `0xFA` | Ring 3 |\n| 4 | `0x20` | User data | 0 | 4GB | `0xF2` | Ring 3 |\nDecoding the access bytes:\n- `0x9A` = `1001 1010`: P=1, DPL=00 (ring 0), S=1 (code/data), E=1 (code), C=0, R=1 (readable), A=0\n- `0x92` = `1001 0010`: P=1, DPL=00 (ring 0), S=1, E=0 (data), D=0, W=1 (writable), A=0\n- `0xFA` = `1111 1010`: P=1, DPL=11 (ring 3), S=1, E=1 (code), C=0, R=1, A=0\n- `0xF2` = `1111 0010`: P=1, DPL=11 (ring 3), S=1, E=0 (data), D=0, W=1, A=0\nFor all segments, flags byte = `0xCF` (G=1 for 4KB granularity, D=1 for 32-bit, L=0, AVL=0) combined with limit bits `0xF` gives the upper byte `0xCF`.\nIn concrete C:\n```c\nstatic struct gdt_entry gdt[5];\nstatic struct gdtr {\n    uint16_t limit;\n    uint32_t base;\n} __attribute__((packed)) gdtr;\nstatic void set_gdt_entry(int i, uint32_t base, uint32_t limit,\n                          uint8_t access, uint8_t flags) {\n    gdt[i].base_low    = (base & 0xFFFF);\n    gdt[i].base_mid    = (base >> 16) & 0xFF;\n    gdt[i].base_high   = (base >> 24) & 0xFF;\n    gdt[i].limit_low   = (limit & 0xFFFF);\n    gdt[i].flags_limit = ((limit >> 16) & 0x0F) | (flags & 0xF0);\n    gdt[i].access      = access;\n}\nvoid gdt_init(void) {\n    set_gdt_entry(0, 0, 0,          0x00, 0x00); // Null\n    set_gdt_entry(1, 0, 0xFFFFFFFF, 0x9A, 0xCF); // Kernel code\n    set_gdt_entry(2, 0, 0xFFFFFFFF, 0x92, 0xCF); // Kernel data\n    set_gdt_entry(3, 0, 0xFFFFFFFF, 0xFA, 0xCF); // User code\n    set_gdt_entry(4, 0, 0xFFFFFFFF, 0xF2, 0xCF); // User data\n    gdtr.limit = sizeof(gdt) - 1;\n    gdtr.base  = (uint32_t)gdt;\n    lgdt(&gdtr); // Assembly: lgdt [gdtr]\n}\n```\n\n> **\ud83d\udd11 Foundation: Segment descriptors and the flat memory model**\n> \n> ## Segment Descriptors and the Flat Memory Model\n### What It Is\nIn protected mode, the x86 CPU doesn't let you use memory addresses directly \u2014 every memory access goes through a **segment**. A segment descriptor is an 8-byte structure stored in a table (the **Global Descriptor Table**, or GDT) that defines a segment's properties:\n```\nBits  63-56, 39-32 : Base address (split across the descriptor)\nBits  51-48, 15-0  : Limit (size of segment, in bytes or 4KB pages)\nBits  55-52        : Flags (granularity, 32/64-bit, etc.)\nBits  47-40        : Access byte (present, DPL, type, etc.)\n```\nWhen you load a selector (e.g., `mov ds, ax`), the CPU finds the corresponding descriptor in the GDT, reads the base and limit, and uses them to translate every logical address:\n```\nPhysical address = Segment Base + Offset\n```\nThe CPU also checks that `Offset \u2264 Limit` \u2014 violations raise a #GP or #SS fault.\n### The Flat Model: Making Segmentation Invisible\nModern 32-bit operating systems (Linux, Windows, your hobby kernel) use the **flat memory model**: every segment descriptor is configured with `Base = 0` and `Limit = 0xFFFFFFFF` (4 GB, using 4KB page granularity).\nWith this setup:\n```\nPhysical address = 0 + Offset = Offset\n```\nSegmentation becomes a no-op. The logical address *is* the linear address. You still need the GDT (the CPU requires it in protected mode), and you still get DPL/ring enforcement from the segment descriptors, but the base/limit translation does nothing useful \u2014 it spans all of memory.\nA minimal flat-model GDT has four entries:\n1. **Null descriptor** (index 0, required by the architecture \u2014 always all zeros)\n2. **Kernel code segment** \u2014 Base=0, Limit=4GB, DPL=0, executable/readable\n3. **Kernel data segment** \u2014 Base=0, Limit=4GB, DPL=0, readable/writable\n4. **User code/data segments** \u2014 same geometry, DPL=3\n### Why Paging Instead\nSegmentation has fundamental problems for modern OS design:\n- Segments must be **contiguous** in linear address space \u2014 you can't easily swap pieces to disk.\n- Sharing memory between segments is awkward.\n- The x86-64 (long mode) architecture **deprecated segmentation** \u2014 `CS`, `DS`, `ES`, `SS` bases are forced to 0 by the hardware regardless of descriptors (only `FS` and `GS` retain configurable bases, used for thread-local storage).\n**Paging**, by contrast, operates on fixed 4KB pages, supports non-contiguous physical allocation, enables demand paging, and provides fine-grained per-page permissions. Modern kernels do all meaningful memory isolation through the page tables, using segmentation only to satisfy the CPU's requirements and enforce the ring boundary.\n### Key Insight\n> **The flat model is a deliberate workaround:** the x86 CPU mandates segmentation, so instead of fighting it, every modern OS neutralizes it by making all segments span the entire address space. You set it up once (load the GDT, reload segment registers), confirm that logical = linear, and then forget about it \u2014 paging handles everything real. When you write your GDT setup code, you're not enabling a feature; you're satisfying a hardware prerequisite so you can get to the interesting part.\n\n### Segment Selectors: Not Addresses, Indices\n\n![Segment Selector \u2192 GDT \u2192 Linear Address Resolution](./diagrams/diag-m1-segment-selector-resolution.svg)\n\nWhen you load `0x08` into CS, you are not loading an address. You are loading a **segment selector**: a 16-bit value where bits 15\u20133 are the index into the GDT (index 1 = `0x08 >> 3`), bit 2 is the TI flag (0 = GDT, 1 = LDT), and bits 1\u20130 are the RPL (Requested Privilege Level).\n| Selector | Binary | GDT Index | TI | RPL |\n|---|---|---|---|---|\n| `0x00` | `0000 0000 0000 0000` | 0 (null) | 0 | 0 |\n| `0x08` | `0000 0000 0000 1000` | 1 | 0 | 0 |\n| `0x10` | `0000 0000 0001 0000` | 2 | 0 | 0 |\n| `0x1B` | `0000 0000 0001 1011` | 3 | 0 | **3** |\n| `0x23` | `0000 0000 0010 0011` | 4 | 0 | **3** |\nWhen a user-mode program (ring 3) loads a kernel data descriptor with RPL=0 into DS, the CPU's DPL/RPL/CPL comparison rules prevent the access. This is the hardware mechanism that makes ring isolation real \u2014 not a software convention, but a check the CPU performs on **every memory access**.\n---\n## The Revelation: Protected Mode Is a Different CPU Personality\nHere is what almost everyone gets wrong the first time they read about protected mode:\nThey picture it like enabling a feature flag. You set CR0.PE and the CPU gains bigger registers, segment protection, and 32-bit addressing. The CPU \"upgrades\" in place.\n**This model is completely wrong.**\nWhen you set CR0.PE, you do not change the CPU gradually. You trigger a fundamental mode switch in the CPU's internal state machine. From that moment, **every memory access is mediated through segment descriptors**. The instruction at CS:IP that is currently being fetched and decoded? The CPU is partway through fetching it using the real-mode CS register value. But the memory subsystem is now expecting that CS contains a protected-mode segment selector pointing at a GDT entry.\nThe CPU is in a schizophrenic half-state: the pipeline has decoded instructions using real-mode segment semantics, but the execution unit expects protected-mode descriptors.\n\n![Real Mode \u2192 Protected Mode \u2014 The Critical Transition](./diagrams/diag-m1-real-to-protected-transition.svg)\n\n**The far jump is not optional ceremony.** It is the only instruction that atomically:\n1. Loads CS with a new value (the kernel code selector `0x08`)\n2. Forces the CPU to validate CS against the GDT\n3. Flushes the instruction prefetch queue \u2014 discarding any partially-decoded real-mode instructions\nWithout the far jump, the instruction immediately after `mov cr0, eax` will execute with a CS that the CPU's protected-mode logic cannot validate. The result is a General Protection Fault. Which causes a Double Fault (because the IDT isn't set up yet). Which causes a Triple Fault. Which causes the CPU to reset.\nYou will never see an error message. The machine will simply restart.\nHere is the exact sequence, every line of which is mandatory:\n```nasm\n[BITS 16]\nprotected_mode_enter:\n    cli                         ; MUST disable interrupts \u2014 BIOS IVT is now invalid\n    lgdt [gdt_descriptor]       ; Load GDTR before setting CR0.PE\n    mov eax, cr0\n    or  eax, 0x01               ; Set CR0.PE (bit 0)\n    mov cr0, eax                ; We are now in protected mode\n                                ; But CS still holds real-mode value!\n    ; The far jump atomically loads CS with selector 0x08\n    ; and flushes the prefetch queue\n    jmp 0x08:protected_mode_entry\n[BITS 32]\nprotected_mode_entry:\n    ; CS = 0x08 (kernel code descriptor, ring 0)\n    ; Now load all data segment registers\n    mov ax, 0x10                ; Kernel data selector\n    mov ds, ax\n    mov es, ax\n    mov fs, ax\n    mov gs, ax\n    mov ss, ax\n    mov esp, 0x9FC00            ; Set 32-bit stack (just below EBDA)\n    ; Now we are fully in 32-bit protected mode\n    call kernel_main\n```\n> **Why `cli` before `lgdt`?** The BIOS set up an Interrupt Vector Table (IVT \u2014 the real-mode equivalent of the IDT) at address `0x0000`. If an interrupt fires after you've loaded the GDT but before you've set up the IDT, the CPU will try to dispatch the interrupt through the IVT... in protected mode. The IVT entries are real-mode far pointers, not IDT gate descriptors. The CPU will misinterpret them as protected-mode IDT entries and triple-fault. Disabling interrupts before touching GDT/CR0 buys you the time to set up the IDT properly. You will enable interrupts again (`sti`) only after the IDT is configured in Milestone 2.\n> **Knowledge Cascade \u2014 CPU Pipeline and Speculative Execution**: The far jump requirement reveals something fundamental about how CPUs work: the prefetch queue. Modern CPUs speculatively fetch and decode instructions several cycles ahead of the current execution point. When you change CR0.PE, the CPU's execution unit switches modes, but the prefetch queue may already contain instructions decoded under old assumptions. The far jump forces a pipeline flush. This same principle \u2014 that the CPU's pipeline holds state that is inconsistent with current reality \u2014 is the root mechanism behind Spectre and Meltdown. Both attacks exploit the CPU's willingness to speculatively execute instructions before validating permission bits, then observe the side effects through cache timing. Understanding the CR0.PE far jump teaches you the real shape of the CPU's execution model.\n\n> **\ud83d\udd11 Foundation: Real mode vs protected mode**\n> \n> ## Real Mode vs Protected Mode\n### What It Is\nThese are two fundamentally different CPU operating environments that the x86 processor supports. When your machine powers on, the CPU starts in **real mode** \u2014 a compatibility mode that mimics the original Intel 8086 from 1978. Your kernel's job is to leave real mode and enter **protected mode** (or long mode on 64-bit systems) as early as possible.\n---\n### Real Mode\n**Memory addressing:** Real mode uses *segmented* addressing with 16-bit registers. A physical address is computed as:\n```\nPhysical address = (Segment Register \u00d7 16) + Offset\n```\nExample: `CS = 0xF000`, `IP = 0xFFF0` \u2192 physical address `0xFFFF0` (the reset vector). This gives a maximum addressable space of **1 MB** (20-bit address bus, `0x00000`\u2013`0xFFFFF`).\n**Key characteristics:**\n- No memory protection whatsoever \u2014 any code can read/write anything.\n- No privilege levels \u2014 all code runs with full hardware access.\n- 16-bit registers and default operand sizes.\n- Direct hardware access through BIOS interrupt calls (`int 0x10` for video, `int 0x13` for disk, etc.).\n- No virtual memory, no paging.\nReal mode exists for historical compatibility and to give firmware (BIOS/UEFI) a known, simple environment to run before handing control to a bootloader.\n---\n### Protected Mode\nProtected mode is what makes a real OS possible. To enter it, you must:\n1. Disable interrupts (`cli`)\n2. Load a valid GDT (`lgdt`)\n3. Set bit 0 (`PE`) of control register `CR0`\n4. Perform a far jump to flush the instruction pipeline and load a new `CS` with a valid protected-mode selector\n```nasm\nmov eax, cr0\nor  eax, 0x1        ; set PE bit\nmov cr0, eax\njmp 0x08:protected_entry   ; far jump \u2014 selector 0x08 = kernel code segment\n```\n**What changes at the hardware level:**\n| Feature | Real Mode | Protected Mode |\n|---------|-----------|----------------|\n| Address space | 1 MB | 4 GB (32-bit) |\n| Addressing | Segment \u00d7 16 + Offset | GDT/LDT descriptor lookup |\n| Privilege levels | None | Rings 0\u20133 enforced by hardware |\n| Memory protection | None | Segmentation + (optionally) paging |\n| BIOS interrupts | Available | **Broken** \u2014 BIOS is 16-bit real-mode code |\n| Operand default | 16-bit | 32-bit |\n| Paging | No | Optional (enabled via `CR0.PG`) |\n**Critical gotcha:** Once you enter protected mode, BIOS interrupt calls no longer work. You must have already collected everything you need from the BIOS (memory map via `int 0x15/E820`, video mode info, etc.) *before* the mode switch, or use special techniques (vm86 mode, or just don't use BIOS at all).\n---\n### Key Insight\n> **The CPU mode switch is a one-way door you must prepare carefully before walking through.** Real mode is the staging area where you gather information (memory map, hardware parameters) and set up prerequisites (GDT, initial stack). Protected mode is where your kernel actually lives. The transition itself \u2014 setting `CR0.PE` and the far jump \u2014 takes less than five instructions, but those five instructions require that everything else already be correct. A missing or malformed GDT entry doesn't give you an error message; it causes an immediate fault with no handler, and the machine reboots. There's no debugger, no safety net \u2014 just you, the CPU, and the architecture manual.\n\n---\n## The Linker Script: Teaching the Toolchain About Physical Reality\n\n![Linker Script \u2014 Section Layout and VMA vs LMA](./diagrams/diag-m1-linker-script-sections.svg)\n\nYour C compiler doesn't know you're writing an operating system. It generates code using symbol addresses assigned by the linker. The linker doesn't know your kernel will be loaded at physical address `0x100000`. You must tell it \u2014 with a linker script.\n\n> **\ud83d\udd11 Foundation: Linker scripts for bare-metal**\n> \n> ## Linker Scripts for Bare-Metal Kernel Development\n### What It Is\nA linker script is a configuration file (typically with a `.ld` extension) that tells the linker **how to construct the final binary**: where to place each section of code and data in memory, what address the binary assumes it will run at, and where execution should begin. For normal userspace programs, the toolchain generates a default script that works with the OS's virtual memory layout. For a bare-metal kernel, there is no OS \u2014 *you* define the layout, and the linker script is how you do it.\nA minimal kernel linker script looks like this:\n```ld\nENTRY(_start)\nSECTIONS {\n    . = 0x100000;          /* Load kernel at 1MB physical */\n    .text : {\n        *(.text)           /* All code from all object files */\n    }\n    .rodata : {\n        *(.rodata)         /* Read-only data (string literals, const globals) */\n    }\n    .data : {\n        *(.data)           /* Initialized global/static variables */\n    }\n    .bss : {\n        _bss_start = .;\n        *(.bss)            /* Uninitialized globals (zeroed at startup) */\n        *(COMMON)\n        _bss_end = .;\n    }\n}\n```\n---\n### ENTRY(): The Execution Start Point\n`ENTRY(_start)` tells the linker which symbol is the **entry point** \u2014 the first instruction the CPU will execute. This is embedded in the ELF header. Your bootloader (or a tool like GRUB via Multiboot) reads this and jumps to that address.\nWithout `ENTRY()`, the linker may default to the beginning of `.text`, which might work \u2014 or might land in the middle of a function. For a kernel, you need this to be explicit and precise.\n---\n### VMA vs LMA: The Critical Distinction\nThis is the most important \u2014 and most confusing \u2014 concept in linker scripts:\n- **VMA (Virtual Memory Address):** The address the code *thinks* it's running at. All internal symbol references (function calls, global variable accesses) are resolved to VMA.\n- **LMA (Load Memory Address):** The address where the section is *physically stored* in the binary image \u2014 where it will be loaded into memory.\n**For a simple kernel loaded at a fixed physical address, VMA = LMA**, and you don't need to think about this. Set `. = 0x100000` and everything works.\n**VMA \u2260 LMA becomes critical** when:\n- Your kernel is a higher-half kernel (linked at `0xC0100000` VMA but loaded at `0x100000` LMA) \u2014 the kernel uses high virtual addresses but lives in low physical memory before paging is enabled.\n- You're copying initialized data from ROM (LMA in flash) to RAM (VMA in SRAM) on embedded systems.\nSyntax to specify both:\n```ld\n.data VMA_ADDRESS : AT(LMA_ADDRESS) {\n    *(.data)\n}\n```\n**Concrete higher-half example:**\n```ld\n. = 0xC0100000;   /* VMA: kernel thinks it's here */\n.text : AT(0x100000) {   /* LMA: actually loaded here */\n    *(.text)\n}\n```\nYour early boot code (before enabling paging) must use physical addresses directly. Once you enable paging with an identity map + higher-half map, the VMA addressing works correctly.\n---\n### SECTIONS{}: Controlling the Memory Map\nThe `SECTIONS` block defines every output section, in order. The location counter (`.`) tracks the current address and advances as sections are placed. You can:\n- Set it explicitly: `. = 0x100000;`\n- Align it: `. = ALIGN(4096);` (page-align for paging setup)\n- Export symbols to your kernel's C code: `_kernel_end = .;`\nThose exported symbols are **directly usable in C**:\n```c\nextern uintptr_t _bss_start, _bss_end;\n// Zero the BSS section at boot\nmemset(&_bss_start, 0, &_bss_end - &_bss_start);\n```\nThis is how your kernel knows its own layout at runtime without hardcoding addresses.\n---\n### Why the BSS Section Needs Special Attention\nThe `.bss` section holds uninitialized globals. The linker records its size but doesn't actually fill the binary with zeros (that would waste space in the image). **Your kernel's startup code must zero it manually** before calling any C code that relies on zero-initialized globals. The linker script gives you `_bss_start` and `_bss_end` symbols specifically for this purpose.\n---\n### Key Insight\n> **The linker script is the translation layer between \"I compiled some C files\" and \"a CPU can boot this.\"** It answers questions the linker cannot answer alone: Where in physical memory does this kernel live? What virtual address do symbols resolve to? Where does execution start? When VMA \u2260 LMA, it manages the split between where code is stored and where it runs \u2014 which is the core challenge of higher-half kernels and ROM-to-RAM systems. Get the linker script wrong, and your kernel jumps to the wrong address and triple-faults silently. Get it right, and every symbol, every section boundary, every page-alignment is exactly where your boot code expects it.\n\n**VMA vs LMA \u2014 the key distinction:**\n- **VMA (Virtual Memory Address)**: The address the code *thinks it's at* \u2014 what symbols resolve to, what pointers contain, what the compiler emits in its instructions.\n- **LMA (Load Memory Address)**: The address where the binary is *actually placed* in the file and loaded by the bootloader.\nFor Milestone 1, VMA = LMA = `0x100000` \u2014 the kernel runs at its load address. This will change in Milestone 3 (higher-half mapping), where the kernel will be linked at `0xC0100000` (VMA) but loaded at `0x100000` (LMA). Understanding VMA/LMA now prevents complete confusion later.\n```ld\n/* kernel.ld \u2014 Linker script for 32-bit x86 kernel */\nENTRY(kernel_entry)         /* Symbol called by bootloader far jump */\nSECTIONS {\n    . = 0x100000;           /* Load at 1MB physical */\n    .text ALIGN(4096) : {\n        *(.multiboot)       /* Multiboot header must be in first 8KB */\n        *(.text)\n        *(.text.*)\n    }\n    .rodata ALIGN(4096) : {\n        *(.rodata)\n        *(.rodata.*)\n    }\n    .data ALIGN(4096) : {\n        *(.data)\n        *(.data.*)\n    }\n    .bss ALIGN(4096) : {\n        __bss_start = .;    /* Symbol for BSS zero-ing */\n        *(.bss)\n        *(.bss.*)\n        *(COMMON)           /* Uninitialized global variables */\n        __bss_end = .;      /* Symbol for BSS zero-ing */\n    }\n    __kernel_end = .;       /* Useful for allocator in Milestone 3 */\n}\n```\n**Why 4KB alignment?** Because Milestone 3 will enable paging with 4KB pages. Having each section start on a page boundary makes it possible to give pages different permissions: `.text` pages marked execute-only, `.data` pages marked no-execute. This isn't enforced now, but your layout decisions now either enable or prevent it later.\n**Why must `__bss_start` and `__bss_end` be in the linker script?** The BSS section contains uninitialized global variables. On a normal system, the C runtime (CRT0 \u2014 the `_start` code that libc provides before calling `main()`) zeros BSS before your code runs. You are writing a freestanding kernel \u2014 there is no CRT0. The CPU loaded your binary from disk, and the disk image doesn't store zeros for the BSS section (a disk image only stores the initialized data). Whatever happened to be in RAM at those addresses is still there. Any global variable you declare as `int x;` may contain garbage. You must zero BSS yourself.\n---\n## Kernel Entry Point: The Assembly-to-C Bridge\nYour bootloader jumps to `kernel_entry`, which is an assembly function. This assembly stub handles the tasks that must happen before C can safely run:\n```nasm\n[BITS 32]\n[GLOBAL kernel_entry]\n[EXTERN kernel_main]\n[EXTERN __bss_start]\n[EXTERN __bss_end]\nkernel_entry:\n    ; Zero the BSS section\n    mov edi, __bss_start\n    mov ecx, __bss_end\n    sub ecx, edi            ; ECX = number of bytes to zero\n    xor eax, eax\n    rep stosb               ; Zero ECX bytes starting at EDI\n    ; Set up stack (linker script ensures this doesn't overlap kernel)\n    mov esp, kernel_stack_top\n    ; Clear direction flag (required for string operations, DF=0 by ABI)\n    cld\n    ; Call the C entry point\n    ; ABI note: EBP = 0 marks the end of the call chain for stack unwinding\n    xor ebp, ebp\n    call kernel_main\n    ; kernel_main should never return, but halt if it does\n.hang:\n    cli\n    hlt\n    jmp .hang\nsection .bss\nkernel_stack: resb 16384   ; 16 KB kernel stack\nkernel_stack_top:\n```\n**Why `cld`?** The x86 direction flag (DF) controls whether string instructions (`stosb`, `movsb`, `scasb`) increment or decrement their pointer registers. DF=0 means increment (forward direction); DF=1 means decrement (backward). The C ABI assumes DF=0 on function entry. If DF is set when you call C code, `memcpy`, `memset`, and any other code using `rep movs/stos` will run backwards and corrupt memory. The BIOS may have left DF in an indeterminate state. Clearing it explicitly costs one instruction and prevents a category of subtle bugs.\n**Why zero EBP?** The x86 calling convention uses EBP as the frame pointer. Stack unwinding (for debugging or backtraces) walks the chain of saved EBPs until it hits zero. Setting EBP=0 before calling `kernel_main` marks `kernel_main` as the bottom of the call stack, which makes backtraces work correctly.\n---\n## VGA Text Mode: Writing Directly to the Screen\n{{DIAGRAM:diag-m1-vga-serial-output}}\nThe VGA text mode buffer is memory-mapped at physical address `0xB8000`. No driver, no system call, no abstraction \u2014 you write directly to RAM and pixels appear on screen. This is the closest relationship between software and hardware you will encounter.\nThe buffer is organized as a 80\u00d725 grid. Each character cell is 2 bytes:\n- Byte 0: ASCII character code\n- Byte 1: Color attribute\nThe color attribute byte:\n```\nBits 7:4 \u2014 Background color (0-15)\nBits 3:0 \u2014 Foreground color (0-15)\n```\nColor values: 0=Black, 1=Blue, 2=Green, 3=Cyan, 4=Red, 5=Magenta, 6=Brown, 7=Light Gray, 8=Dark Gray, 9=Light Blue, 10=Light Green, 11=Light Cyan, 12=Light Red, 13=Light Magenta, 14=Yellow, 15=White.\n```c\n#define VGA_BUFFER ((volatile uint16_t *)0xB8000)\n#define VGA_WIDTH  80\n#define VGA_HEIGHT 25\n#define VGA_COLOR(fg, bg) ((bg << 4) | fg)\n#define VGA_ENTRY(ch, color) ((uint16_t)(color << 8) | (uint8_t)(ch))\nstatic int vga_row = 0, vga_col = 0;\nstatic uint8_t vga_color = VGA_COLOR(15, 0); // White on black\nvoid vga_clear(void) {\n    for (int i = 0; i < VGA_WIDTH * VGA_HEIGHT; i++)\n        VGA_BUFFER[i] = VGA_ENTRY(' ', vga_color);\n    vga_row = vga_col = 0;\n}\nvoid vga_putchar(char c) {\n    if (c == '\\n') {\n        vga_col = 0;\n        if (++vga_row == VGA_HEIGHT) vga_scroll();\n        return;\n    }\n    VGA_BUFFER[vga_row * VGA_WIDTH + vga_col] = VGA_ENTRY(c, vga_color);\n    if (++vga_col == VGA_WIDTH) {\n        vga_col = 0;\n        if (++vga_row == VGA_HEIGHT) vga_scroll();\n    }\n}\nstatic void vga_scroll(void) {\n    // Copy rows 1-24 up to rows 0-23\n    for (int i = 0; i < (VGA_HEIGHT - 1) * VGA_WIDTH; i++)\n        VGA_BUFFER[i] = VGA_BUFFER[i + VGA_WIDTH];\n    // Clear last row\n    for (int i = (VGA_HEIGHT - 1) * VGA_WIDTH; i < VGA_HEIGHT * VGA_WIDTH; i++)\n        VGA_BUFFER[i] = VGA_ENTRY(' ', vga_color);\n    vga_row = VGA_HEIGHT - 1;\n}\n```\n**Why `volatile`?** The compiler doesn't know that writing to `0xB8000` has observable side effects (pixels on screen). Without `volatile`, the compiler may optimize away writes it deems \"redundant\" (e.g., writing the same location twice \u2014 it might keep only the last write), or reorder them. With `volatile`, every write is performed in program order, exactly as written. This qualifier is mandatory for any memory-mapped I/O in a freestanding environment.\n> **Hardware Soul**: The VGA buffer at `0xB8000` is not ordinary RAM \u2014 it is memory-mapped I/O (MMIO). When your CPU writes to `0xB8000`, the memory controller routes that write not to a DRAM chip but to the VGA card's own framebuffer memory. The VGA hardware continuously scans this framebuffer to refresh the display. This is why the `volatile` qualifier matters at the hardware level: the \"side effect\" of a VGA write is a hardware register state change in the VGA controller, which the compiler cannot see or reason about. Without `volatile`, the compiler might prove that no C code reads back from `0xB8000` and eliminate the write as dead code.\n---\n## Serial Port: Debug Output That Never Lies\nVGA is great for showing output to a human. Serial port is better for debugging \u2014 it's simple enough to initialize in under 10 instructions, works before VGA is configured, and can pipe output to a file with QEMU (`-serial file:serial.log`).\nCOM1 is at I/O port base `0x3F8`. Initialize it to 115200 baud, 8N1 (8 data bits, No parity, 1 stop bit):\n```c\n#define COM1 0x3F8\nstatic inline void outb(uint16_t port, uint8_t val) {\n    __asm__ volatile (\"outb %0, %1\" : : \"a\"(val), \"Nd\"(port));\n}\nstatic inline uint8_t inb(uint16_t port) {\n    uint8_t val;\n    __asm__ volatile (\"inb %1, %0\" : \"=a\"(val) : \"Nd\"(port));\n    return val;\n}\nvoid serial_init(void) {\n    outb(COM1 + 1, 0x00); // Disable interrupts\n    outb(COM1 + 3, 0x80); // Enable DLAB (Divisor Latch Access Bit)\n    outb(COM1 + 0, 0x01); // Divisor low byte: 1 \u2192 115200 baud\n    outb(COM1 + 1, 0x00); // Divisor high byte\n    outb(COM1 + 3, 0x03); // 8N1: 8 data bits, no parity, 1 stop bit; disable DLAB\n    outb(COM1 + 2, 0xC7); // Enable FIFO, clear, 14-byte threshold\n    outb(COM1 + 4, 0x0B); // Enable IRQ, set RTS/DSR\n}\nvoid serial_putchar(char c) {\n    while (!(inb(COM1 + 5) & 0x20)); // Wait until transmit buffer empty\n    outb(COM1, c);\n}\n```\n**Why port-based I/O instead of memory-mapped?** x86 provides two I/O address spaces: memory (accessed via load/store instructions) and I/O ports (accessed via `in`/`out` instructions). The distinction is a hardware routing decision in the chipset. Legacy peripherals like the UART, PIC, PIT, and keyboard controller use port I/O. Modern devices (PCIe, framebuffers) use MMIO. The `in`/`out` instructions are privileged in protected mode \u2014 only ring 0 can execute them by default (controlled by the IOPL field in EFLAGS and the TSS I/O permission bitmap, which becomes relevant in Milestone 4).\n**Baud rate calculation**: The 8250/16550 UART has an internal clock of 115200 Hz divided by the divisor register. To achieve 115200 baud, set divisor = 115200 / 115200 = 1. For 9600 baud, divisor = 12.\n---\n## kprintf: The Kernel's Lifeline\n```c\n#include <stdarg.h>\nvoid kprintf(const char *fmt, ...) {\n    va_list args;\n    va_start(args, fmt);\n    for (; *fmt; fmt++) {\n        if (*fmt != '%') {\n            vga_putchar(*fmt);\n            serial_putchar(*fmt);\n            continue;\n        }\n        fmt++;\n        switch (*fmt) {\n            case 'c': {\n                char c = (char)va_arg(args, int);\n                vga_putchar(c); serial_putchar(c);\n                break;\n            }\n            case 's': {\n                const char *s = va_arg(args, const char *);\n                for (; *s; s++) { vga_putchar(*s); serial_putchar(*s); }\n                break;\n            }\n            case 'd': {\n                int n = va_arg(args, int);\n                if (n < 0) { vga_putchar('-'); serial_putchar('-'); n = -n; }\n                // Convert to decimal string\n                char buf[12]; int i = 0;\n                if (n == 0) buf[i++] = '0';\n                while (n > 0) { buf[i++] = '0' + (n % 10); n /= 10; }\n                while (i--) { vga_putchar(buf[i]); serial_putchar(buf[i]); }\n                break;\n            }\n            case 'x': case 'p': {\n                uint32_t n = va_arg(args, uint32_t);\n                if (*fmt == 'p') {\n                    vga_putchar('0'); serial_putchar('0');\n                    vga_putchar('x'); serial_putchar('x');\n                }\n                char hex[] = \"0123456789abcdef\";\n                for (int shift = 28; shift >= 0; shift -= 4) {\n                    char c = hex[(n >> shift) & 0xF];\n                    vga_putchar(c); serial_putchar(c);\n                }\n                break;\n            }\n            default:\n                vga_putchar('%'); serial_putchar('%');\n                vga_putchar(*fmt); serial_putchar(*fmt);\n        }\n    }\n    va_end(args);\n}\n```\n> **Why not use `sprintf` + then print?** No stdlib. `sprintf` lives in libc, which you don't have. Every function you call must be something you wrote. This is the freestanding constraint: you are the runtime, the standard library, and the operating system simultaneously. This same experience is what Go's runtime team faced bootstrapping the Go runtime without a C runtime (documented in `cmd/internal/obj`, the Go assembler), and what the JVM faces initializing the first classloader before any Java code can run.\n---\n## Build System: From Source to Bootable Image\nThe final deliverable of this milestone is a bootable disk image. Here is the complete build chain:\n```makefile\n# Toolchain (cross-compiler targeting i686-elf, no host OS assumptions)\nCC      = i686-elf-gcc\nAS      = nasm\nLD      = i686-elf-ld\nCFLAGS  = -m32 -ffreestanding -fno-stack-protector -fno-builtin \\\n          -nostdlib -nostdinc -Wall -Wextra -O2\nASFLAGS = -f elf32\nLDFLAGS = -T kernel.ld -nostdlib\nKERNEL_OBJS = kernel_entry.o kernel_main.o vga.o serial.o kprintf.o\n.PHONY: all clean run\nall: os.img\n# Compile kernel objects\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n%.o: %.asm\n\t$(AS) $(ASFLAGS) $< -o $@\n# Stage 1 bootloader (raw binary)\nstage1.bin: stage1.asm\n\t$(AS) -f bin stage1.asm -o stage1.bin\n# Stage 2 loader (raw binary)\nstage2.bin: stage2.asm\n\t$(AS) -f bin stage2.asm -o stage2.bin\n# Kernel ELF\nkernel.elf: $(KERNEL_OBJS)\n\t$(LD) $(LDFLAGS) -o $@ $^\n# Extract raw kernel binary from ELF\nkernel.bin: kernel.elf\n\tobjcopy -O binary $< $@\n# Assemble disk image:\n# Sector 0: stage1 (512 bytes)\n# Sector 1: padding (512 bytes, stage2 starts at sector 2 per INT 13h)\n# Sector 2-9: stage2 (8 sectors = 4KB)\n# Sector 10+: kernel\nos.img: stage1.bin stage2.bin kernel.bin\n\tdd if=/dev/zero of=os.img bs=512 count=2880      # 1.44MB blank disk\n\tdd if=stage1.bin of=os.img conv=notrunc           # Write MBR\n\tdd if=stage2.bin of=os.img conv=notrunc seek=2    # Stage 2 at sector 2\n\tdd if=kernel.bin of=os.img conv=notrunc seek=10   # Kernel at sector 10\nrun: os.img\n\tqemu-system-i386 -drive file=os.img,format=raw \\\n\t                 -serial stdio \\                   # Serial \u2192 terminal\n\t                 -d int,cpu_reset \\                # Log interrupts and resets\n\t                 -no-reboot                        # Don't reboot on triple-fault\n```\n**Critical compiler flags explained:**\n- `-ffreestanding`: Tells GCC it cannot assume a standard C library or runtime is available. Functions like `memcpy` will not be implicitly linked.\n- `-fno-stack-protector`: Stack canaries require `__stack_chk_fail` from libc. Without libc, linking fails.\n- `-fno-builtin`: Prevents GCC from replacing your `memset` calls with SIMD intrinsics that assume alignment or libc presence.\n- `-nostdlib -nostdinc`: Do not search for or link standard library headers or binaries.\n- `-m32`: Compile for 32-bit x86, not the host 64-bit target.\n**Why a cross-compiler?** You are compiling code for a 32-bit bare-metal environment using a compiler that runs on your 64-bit Linux/macOS host. The host compiler's default target includes the host OS's ABI, calling conventions, and runtime assumptions. `i686-elf-gcc` targets the `i686-elf` triple: 32-bit x86 processor with ELF binary format and no operating system. Build it with `crosstool-ng` or download a prebuilt binary. Using the host `gcc` with `-m32` is tempting but will generate code linked against the host's libc and ABI \u2014 it will break in subtle ways.\n---\n## Debugging with QEMU: Your Triple-Fault Survival Guide\nTriple-faults are silent. The machine resets without any error message. Your survival tools:\n**1. QEMU flags for debugging:**\n```bash\n-d int,cpu_reset    # Log every interrupt and CPU reset to stderr\n-no-reboot          # Halt instead of rebooting on triple-fault\n-S                  # Freeze CPU at startup (wait for GDB)\n-s                  # Listen for GDB on port 1234\n```\n**2. Attach GDB:**\n```bash\ngdb kernel.elf\n(gdb) target remote :1234\n(gdb) set architecture i386\n(gdb) break kernel_main\n(gdb) continue\n```\n**3. QEMU monitor (Ctrl+Alt+2 in windowed mode):**\n```\ninfo registers      # Dump all CPU registers\ninfo tlb            # Show TLB state\nxp /10i 0x100000   # Disassemble 10 instructions at physical 0x100000\n```\n**4. Read QEMU's `-d int` output**: A triple-fault will appear as a rapid succession of exceptions. Look for the first exception \u2014 that's your actual bug. The CPU exception number, the CS:EIP at the time, and the error code tell you exactly what went wrong.\n**Common failure modes:**\n| Symptom | Likely Cause |\n|---|---|\n| CPU reset immediately after MBR loads | Boot signature `0x55AA` missing or wrong endianness |\n| CPU reset after `lgdt` | GDT base address is wrong (physical address miscalculated) |\n| CPU reset after `jmp 0x08:` | GDT descriptor 1 has wrong access byte or granularity |\n| CPU reset in first C instruction | ESP points to unmapped or read-only memory |\n| Wild behavior in C | BSS not zeroed; global variables contain garbage |\n| VGA output garbled | `volatile` missing from VGA buffer pointer |\n| Serial output missing | COM1 initialization sequence out of order |\n---\n## System Awareness: Where You Stand\n\n![x86 Boot Sequence \u2014 BIOS to C Entry Point](./diagrams/diag-m1-boot-sequence-timeline.svg)\n\nAfter this milestone, you have built the foundation layer of the OS kernel map:\n**What exists**: A 32-bit protected-mode execution environment, a valid GDT (4 privilege-level ring 0/3 code and data descriptors), a kernel binary at physical `0x100000`, a zeroed BSS section, an initialized stack, VGA and serial output, and `kprintf`.\n**What does not exist yet**: The IDT (so any exception \u2014 including a NULL pointer dereference \u2014 will triple-fault silently), any memory allocator (your kernel has no `malloc`), page tables (you're running with paging disabled, accessing physical addresses directly), and any notion of a \"process.\"\n**Immediate dependency**: Milestone 2 will install the IDT immediately. Once you have an IDT, a divide-by-zero will print a helpful error message instead of silently resetting. This single change transforms debugging from archaeology to engineering.\n---\n## Knowledge Cascade\n**1. Segment selectors unlock ring isolation.** Understanding that `0x08` is an index into the GDT \u2014 not an address \u2014 is the insight that makes ring 0/ring 3 isolation make sense at the hardware level. When you later configure user-mode segments (selectors `0x1B` and `0x23` with RPL=3), the CPU enforces privilege by comparing the current ring (CPL, in CS bits 0-1) against the descriptor's DPL on every memory access. Ring isolation is not a software fiction \u2014 it is a per-instruction hardware check.\n**2. VMA/LMA and cross-domain firmware.** The VMA vs LMA distinction in your linker script is identical to the concept of Position-Independent Code (PIC) in shared libraries, boot-time relocation in UEFI firmware, and ROM vs RAM execution in embedded systems. In all cases: the code is physically at one address (LMA), believes it's at another (VMA), and needs either a relocation mechanism or identity mapping to reconcile them. When you enable higher-half mapping in Milestone 3, you will directly experience this \u2014 the kernel will be linked at `0xC0100000` but running at `0x100000`, and every absolute address reference will be wrong until paging bridges the gap.\n**3. Freestanding C runtime and language runtime bootstrapping.** Realizing that you must manually zero BSS because there's no CRT0 connects directly to how all language runtimes bootstrap themselves. The Go runtime's `runtime.schedinit()` performs the same job \u2014 initializing `m0` and `g0` before the scheduler can function \u2014 before calling `runtime.main()`. The JVM bootstraps the root classloader before any Java code runs. JavaScript engines initialize the heap and global scope before evaluating script. Every language runtime has its own \"zero BSS and set up the stack\" moment; yours is just more visible because you wrote it in assembly.\n**4. The far jump and speculative execution.** The requirement to flush the prefetch queue after setting CR0.PE is the same physical mechanism exploited by Spectre variant 1: the CPU speculatively executes instructions based on a prediction, then the actual permission check fails \u2014 but the speculative execution has already left a cache side-channel. Understanding that the CPU's pipeline maintains state independent of privilege enforcement is the conceptual key to understanding an entire class of microarchitectural vulnerabilities.\n**5. A20 and backwards compatibility as design constraint.** The 40-year persistence of the A20 workaround reflects how hardware design decisions compound over time. The same pattern appears in: x86's continued presence of 16-bit real mode (IBM PC compatibility), BIOS firmware surviving until UEFI (PC/AT compatibility), IPv4 NAT proliferating instead of IPv6 deploying (infrastructure compatibility), and JavaScript's retained quirks despite standardization. In each case, breaking backwards compatibility has a cost that the industry keeps deciding is too high to pay. A20 is a particularly vivid example because the workaround is mechanical \u2014 a gate on a wire \u2014 rather than software.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-os-m2 -->\n<!-- MS_ID: build-os-m2 -->\n# Milestone 2: Interrupts, Exceptions, and Keyboard\n\n![OS Kernel \u2014 Satellite System Map](./diagrams/diag-satellite-os-map.svg)\n\nAt the end of Milestone 1, your kernel runs \u2014 but it runs blind and deaf. If you divide by zero, the CPU triple-faults and the machine resets without a word. If the timer fires, there is no handler \u2014 the CPU fetches a garbage IDT entry and resets again. You have built a structure that can exist, but cannot yet *react*. This milestone gives your kernel senses.\nThe ability to respond to hardware events is not a luxury feature you add after the kernel works. It is the prerequisite for *everything*. Preemptive scheduling (Milestone 4) requires a timer interrupt. Demand paging (Milestone 3) requires a page fault handler. Keyboard input requires an IRQ handler. The process of converting your static, \"start once and run forever\" kernel into a dynamic system that interacts with hardware begins here, with the Interrupt Descriptor Table.\n---\n## The Misconception You Must Shed First\nHere is what application programmers think interrupts are: **callbacks**. You register a function, and when the hardware event occurs, the CPU calls it \u2014 saving state, switching context, and restoring everything just like any other function call.\nThis model is completely wrong, and believing it will produce subtly broken interrupt handlers that corrupt process state in ways that manifest hours after the interrupt fired.\nHere is what interrupts actually are: **a violent, asynchronous transfer of control that the CPU initiates mid-instruction-stream, with almost no automatic cleanup.**\nWhen an interrupt fires, the CPU does five things automatically (on a privilege-level change):\n1. Pushes **SS** (stack segment)\n2. Pushes **ESP** (stack pointer)\n3. Pushes **EFLAGS** (processor flags)\n4. Pushes **CS** (code segment)\n5. Pushes **EIP** (instruction pointer \u2014 the return address)\nThat's it. Your general-purpose registers \u2014 EAX, EBX, ECX, EDX, ESI, EDI, EBP \u2014 are **untouched**. They contain whatever the interrupted code had in them. If your interrupt handler uses EAX to do its work without saving it first, you have silently destroyed a value that the interrupted code was about to use. The interrupted code will resume and find its data corrupted. You will spend hours debugging the resulting behavior before realizing the problem was in your interrupt handler, not in the code that crashed.\nThe additional brutality: **some exceptions push an error code, and some don't.** Exceptions 8 (double fault), 10 (invalid TSS), 11 (segment not present), 12 (stack fault), 13 (general protection fault), and 14 (page fault) push an additional value onto the stack before the handler receives control. If your handler assumes there's no error code and pops EIP from the wrong position, it jumps to garbage. The stack layout changes based on *which* interrupt fired \u2014 and you won't know which one at compile time.\nThis is the real shape of interrupt handling. Everything in this milestone flows from understanding these constraints precisely.\n---\n## The Interrupt Descriptor Table: 256 Gates to the Kernel\n{{DIAGRAM:diag-m2-idt-entry-format}}\nThe IDT (Interrupt Descriptor Table) is the protected-mode equivalent of the real-mode Interrupt Vector Table. Where the IVT held flat 16-bit far pointers, the IDT holds 8-byte **gate descriptors** \u2014 structured entries that tell the CPU where to find a handler, what privilege level is required to call it, and what kind of gate controls the transition.\nYou register the IDT's location and size with the `lidt` instruction, which takes a 6-byte IDTR structure (same structure as the GDTR you loaded in Milestone 1):\n```c\nstruct idtr {\n    uint16_t limit;  // Size of IDT in bytes minus 1\n    uint32_t base;   // Linear address of IDT\n} __attribute__((packed));\n```\nThe IDT can hold up to 256 entries. You need exactly 256. Vectors 0\u201331 are reserved for CPU exceptions (architected by Intel, cannot be reassigned). Vectors 32\u2013255 are available for software and hardware interrupts.\n### Gate Descriptor Format\nEach 8-byte IDT entry encodes: the handler address (split, like GDT base addresses, across two non-contiguous fields), the segment selector to use when calling the handler, and flags that control gate type and privilege:\n```\nBytes 7-4:\n  Bits 31-16: Handler address bits 31-16 (high word)\n  Bits 15:    Present bit (P) \u2014 must be 1\n  Bits 14-13: DPL \u2014 Descriptor Privilege Level\n  Bit 12:     0 (reserved, always 0)\n  Bits 11-8:  Gate type (1110 = 32-bit interrupt gate, 1111 = 32-bit trap gate)\n  Bits 7-5:   Reserved (must be 0)\n  Bits 4-0:   Reserved (must be 0)\nBytes 3-2: Segment selector (which GDT entry to use \u2014 0x08 for kernel code)\nBytes 1-0: Handler address bits 15-0 (low word)\n```\nIn C:\n```c\nstruct idt_entry {\n    uint16_t offset_low;   // Handler address bits 0-15\n    uint16_t selector;     // GDT selector (0x08 = kernel code segment)\n    uint8_t  reserved;     // Always 0\n    uint8_t  flags;        // P=1, DPL, gate type\n    uint16_t offset_high;  // Handler address bits 16-31\n} __attribute__((packed));\nstatic struct idt_entry idt[256];\nstatic struct idtr idtr;\nvoid idt_set_gate(int vec, uint32_t handler, uint16_t sel, uint8_t flags) {\n    idt[vec].offset_low  = handler & 0xFFFF;\n    idt[vec].selector    = sel;\n    idt[vec].reserved    = 0;\n    idt[vec].flags       = flags;\n    idt[vec].offset_high = (handler >> 16) & 0xFFFF;\n}\n```\n**Gate type: interrupt gate vs trap gate.** The flags byte encodes the gate type in bits 3\u20130 (with bit 4 always 0 for 32-bit descriptors, bit 11 set for 32-bit vs 16-bit):\n- **Interrupt gate** (`0x8E` = `1000 1110`): The CPU automatically clears the IF (interrupt flag) on entry, preventing nested interrupts. Use this for hardware IRQ handlers where you don't want re-entrant interrupts from the same device.\n- **Trap gate** (`0x8F` = `1000 1111`): The IF flag is **not** cleared. Interrupts can nest. Use this for software exceptions like the system call gate (`INT 0x80`) \u2014 you want the kernel to remain interruptible while processing a syscall.\nFor hardware IRQ handlers (timer, keyboard) and CPU exception handlers, use interrupt gates (`0x8E`). For the system call interface (Milestone 4), you'll use a trap gate with DPL=3 so user code can invoke it.\nThe DPL field controls who can *invoke* the gate with a software `int n` instruction. DPL=0 means only ring 0 code can use `int n` to trigger this vector. DPL=3 lets user-mode code trigger it. Hardware interrupts bypass this check entirely \u2014 they fire regardless of DPL.\n---\n## The Stack Frame Problem: Every Handler Must Know Its Layout\n\n![Interrupt Stack Frame \u2014 With and Without Error Code](./diagrams/diag-m2-interrupt-stack-frame.svg)\n\nBefore writing a single interrupt handler, you must understand exactly what the stack looks like when your handler receives control. This is the most error-prone part of interrupt handling, and getting it wrong produces stack corruption that is nearly impossible to debug.\n**Without error code (exceptions 0-7, 9, 15-31, and all IRQs):**\n```\n[ESP+16] EFLAGS\n[ESP+12] CS\n[ESP+8]  EIP       \u2190 return address\n[ESP+4]  (nothing \u2014 ESP was decremented to here)\n[ESP+0]  \u2190 ESP when handler is called (if you pushed nothing yet)\n```\nAfter `pusha` (which pushes EAX, ECX, EDX, EBX, ESP, EBP, ESI, EDI \u2014 32 bytes):\n```\n[ESP+48] EFLAGS\n[ESP+44] CS\n[ESP+40] EIP\n[ESP+36] EDI (from pusha)\n[ESP+32] ESI\n[ESP+28] EBP\n[ESP+24] old ESP (as it was before interrupt \u2014 pushed by pusha)\n[ESP+20] EBX\n[ESP+16] EDX\n[ESP+12] ECX\n[ESP+8]  EAX\n[ESP+0]  \u2190 ESP after pusha\n```\n**With error code (exceptions 8, 10-14):**\nThe CPU pushes the error code *before* the handler receives control, slipping it between EIP and the handler's own stack frame:\n```\n[ESP+16] EFLAGS\n[ESP+12] CS\n[ESP+8]  EIP\n[ESP+4]  Error code  \u2190 pushed by CPU for exceptions 8, 10-14\n[ESP+0]  \u2190 ESP when handler is called\n```\nAfter `pusha`:\n```\n[ESP+52] EFLAGS\n[ESP+48] CS\n[ESP+44] EIP\n[ESP+40] Error code\n[ESP+36] EDI\n...\n[ESP+0]  \u2190 ESP after pusha\n```\nThe error code must be removed from the stack before `iret` \u2014 otherwise `iret` pops it as EFLAGS and restores execution to a garbage address. You must either `pop` it (discard it or save it to a C variable) or `add esp, 4` before the `popa`/`iret` sequence.\n**The correct approach for a unified ISR stub system:**\nRather than writing 256 separate assembly stubs by hand, you write **two stub templates** \u2014 one for exceptions without error codes and one for exceptions with error codes \u2014 and use a macro to generate all 256:\n```nasm\n%macro ISR_NOERR 1\nisr_%1:\n    push dword 0        ; Push fake error code (maintain uniform stack layout)\n    push dword %1       ; Push interrupt number\n    jmp isr_common_stub\n%endmacro\n%macro ISR_ERR 1\nisr_%1:\n                        ; CPU already pushed error code\n    push dword %1       ; Push interrupt number\n    jmp isr_common_stub\n%endmacro\n```\nThe fake error code for ISR_NOERR is the key insight: by pushing `0` for exceptions that don't produce an error code, you make the stack frame **uniform**. Your C handler receives the same structure regardless of which exception fired:\n```c\nstruct interrupt_frame {\n    // Pushed by pusha (in reverse order \u2014 EDI pushed last, so lowest address)\n    uint32_t edi, esi, ebp, esp_dummy, ebx, edx, ecx, eax;\n    // Pushed by our stub\n    uint32_t int_no;\n    uint32_t err_code;  // 0 for exceptions that don't push one\n    // Pushed by CPU\n    uint32_t eip, cs, eflags;\n    uint32_t user_esp, user_ss;  // Only present if privilege level changed\n};\n```\nThe common stub:\n```nasm\nisr_common_stub:\n    pusha               ; Save all general-purpose registers (EDI, ESI, EBP, ESP, EBX, EDX, ECX, EAX)\n    push ds             ; Save segment registers\n    push es\n    push fs\n    push gs\n    mov ax, 0x10        ; Load kernel data segment\n    mov ds, ax\n    mov es, ax\n    mov fs, ax\n    mov gs, ax\n    push esp            ; Pass pointer to saved register frame as argument to C handler\n    call interrupt_dispatch ; C function: void interrupt_dispatch(struct interrupt_frame *)\n    add esp, 4          ; Clean up pushed argument\n    pop gs\n    pop fs\n    pop es\n    pop ds\n    popa\n    add esp, 8          ; Remove int_no and err_code from stack\n    iret                ; Restores EIP, CS, EFLAGS (and ESP, SS if privilege changed)\n```\nGenerate the 256 stubs with macros:\n```nasm\nISR_NOERR 0    ; Divide error\nISR_NOERR 1    ; Debug\nISR_NOERR 2    ; NMI\nISR_NOERR 3    ; Breakpoint\nISR_NOERR 4    ; Overflow\nISR_NOERR 5    ; Bound range exceeded\nISR_NOERR 6    ; Invalid opcode\nISR_NOERR 7    ; Device not available (FPU)\nISR_ERR   8    ; Double fault\nISR_NOERR 9    ; Coprocessor segment overrun (obsolete)\nISR_ERR   10   ; Invalid TSS\nISR_ERR   11   ; Segment not present\nISR_ERR   12   ; Stack fault\nISR_ERR   13   ; General protection fault\nISR_ERR   14   ; Page fault\nISR_NOERR 15   ; Reserved\nISR_NOERR 16   ; x87 floating-point exception\nISR_ERR   17   ; Alignment check\nISR_NOERR 18   ; Machine check\nISR_NOERR 19   ; SIMD floating-point exception\n; ... 20-31: ISR_NOERR\n; ... 32-255: IRQ handlers (ISR_NOERR, CPU doesn't push error codes for IRQs)\n```\nThen in your IDT initialization:\n```c\nvoid idt_init(void) {\n    // CPU exceptions\n    idt_set_gate(0,  (uint32_t)isr_0,  0x08, 0x8E);\n    idt_set_gate(1,  (uint32_t)isr_1,  0x08, 0x8E);\n    // ... all 256 entries\n    idt_set_gate(13, (uint32_t)isr_13, 0x08, 0x8E);\n    idt_set_gate(14, (uint32_t)isr_14, 0x08, 0x8E);\n    // IRQs (after PIC remapping \u2014 see next section)\n    idt_set_gate(32, (uint32_t)isr_32, 0x08, 0x8E); // Timer (IRQ0 \u2192 vector 32)\n    idt_set_gate(33, (uint32_t)isr_33, 0x08, 0x8E); // Keyboard (IRQ1 \u2192 vector 33)\n    // ...\n    idtr.limit = sizeof(idt) - 1;\n    idtr.base  = (uint32_t)idt;\n    __asm__ volatile (\"lidt [%0]\" : : \"r\"(&idtr));\n}\n```\n> **Hardware Soul**: Each IDT gate lookup requires the CPU to read an 8-byte structure from memory. This happens for every interrupt and exception. The IDT is almost always hot in L1 cache because it's accessed frequently \u2014 but the first lookup after a cache eviction costs a full memory read. The `idtr` register itself (a CPU internal register) holds the base and limit, so the CPU doesn't re-read the IDTR structure on every interrupt \u2014 only the table entry pointed to by `idtr.base + (vector \u00d7 8)`. On modern out-of-order CPUs, the interrupt arrival causes a pipeline flush (the interrupt itself is a control-flow redirect), costing 10-20 cycles before your handler begins executing, before any register saves. This overhead is fundamental and irreducible \u2014 it's why RTOS kernels measuring interrupt latency in microseconds treat every saved register as a real cost.\n---\n## CPU Exception Vectors: What Each One Means\n\n![CPU Exception Vectors 0-31 \u2014 Classification Table](./diagrams/diag-m2-exception-vector-table.svg)\n\nIntel defines 32 exception vectors (0\u201331). Understanding what triggers each one matters for debugging \u2014 when your kernel triple-faults during development, the specific exception number tells you exactly what went wrong:\n| Vector | Mnemonic | Cause | Error Code? | Type |\n|--------|----------|-------|-------------|------|\n| 0  | #DE | Division by zero or overflow | No | Fault |\n| 1  | #DB | Debug \u2014 breakpoint or single-step | No | Fault/Trap |\n| 2  | #NMI | Non-maskable interrupt (hardware failure) | No | Interrupt |\n| 3  | #BP | Breakpoint (`int3` instruction) | No | Trap |\n| 4  | #OF | Overflow (`into` instruction) | No | Trap |\n| 5  | #BR | BOUND range exceeded | No | Fault |\n| 6  | #UD | Invalid opcode | No | Fault |\n| 7  | #NM | Device not available (FPU/MMX) | No | Fault |\n| 8  | #DF | **Double fault** | Yes (always 0) | Abort |\n| 9  | \u2014 | Coprocessor overrun (obsolete) | No | Fault |\n| 10 | #TS | Invalid TSS | Yes | Fault |\n| 11 | #NP | Segment not present | Yes | Fault |\n| 12 | #SS | Stack segment fault | Yes | Fault |\n| 13 | #GP | **General protection fault** | Yes | Fault |\n| 14 | #PF | **Page fault** | Yes | Fault |\n| 15 | \u2014 | Reserved | \u2014 | \u2014 |\n| 16 | #MF | x87 floating-point exception | No | Fault |\n| 17 | #AC | Alignment check | Yes | Fault |\n| 18 | #MC | Machine check (hardware error) | No | Abort |\n| 19 | #XM | SIMD floating-point exception | No | Fault |\n| 20-31 | \u2014 | Reserved | \u2014 | \u2014 |\n**Fault vs Trap vs Abort:** These describe whether EIP points to the faulting instruction or the next one when the exception fires, and whether recovery is possible:\n- **Fault**: EIP points to the faulting instruction. The handler can fix the condition and restart the instruction (e.g., page fault \u2014 map the page, then re-execute the faulting memory access).\n- **Trap**: EIP points to the instruction *after* the trap. The instruction executed; the trap fires afterward (e.g., breakpoint \u2014 the `int3` ran, now you handle it).\n- **Abort**: The CPU cannot reliably save or restore state. Recovery is impossible; the system must halt (e.g., machine check, double fault).\n**The three exceptions you'll encounter most during development:**\n**#DE (0) \u2014 Divide by zero.** Null pointer arithmetic, integer division, or integer overflow via `idiv`. Easy to trigger with `int x = 1/0;`. Your handler should print a message and halt (or kill the current process in Milestone 4).\n**#GP (13) \u2014 General protection fault.** The catch-all for protection violations. Caused by: accessing a segment descriptor with wrong DPL, using a null segment selector, executing a privileged instruction from ring 3, or any number of other protection violations. The error code encodes which segment selector (if any) caused the fault. Print the error code and EIP from the interrupt frame to diagnose.\n**#PF (14) \u2014 Page fault.** Fired when an access violates page permissions or accesses an unmapped page. The faulting address is in CR2 (not the frame \u2014 you must read CR2 explicitly). The error code encodes: bit 0 (P) = page present when fault occurred (0 = page not mapped, 1 = permission violation), bit 1 (W) = write access, bit 2 (U) = user-mode access. In Milestone 3, your page fault handler will implement demand paging by inspecting these bits. For now, print them and halt.\nYour dispatch function in C:\n```c\nstatic const char *exception_names[] = {\n    \"Division Error\",          \"Debug\",\n    \"Non-Maskable Interrupt\",  \"Breakpoint\",\n    \"Overflow\",                \"Bound Range Exceeded\",\n    \"Invalid Opcode\",          \"Device Not Available\",\n    \"Double Fault\",            \"Coprocessor Segment Overrun\",\n    \"Invalid TSS\",             \"Segment Not Present\",\n    \"Stack Fault\",             \"General Protection Fault\",\n    \"Page Fault\",              \"Reserved\",\n    \"x87 Floating-Point\",      \"Alignment Check\",\n    \"Machine Check\",           \"SIMD Floating-Point\",\n    /* 20-31 reserved */\n};\nvoid interrupt_dispatch(struct interrupt_frame *frame) {\n    if (frame->int_no < 32) {\n        // CPU exception\n        kprintf(\"\\n\\033[31m[EXCEPTION #%d: %s]\\033[0m\\n\",\n                frame->int_no,\n                frame->int_no < 20 ? exception_names[frame->int_no] : \"Reserved\");\n        kprintf(\"  EIP=0x%x  CS=0x%x  EFLAGS=0x%x\\n\",\n                frame->eip, frame->cs, frame->eflags);\n        kprintf(\"  EAX=0x%x  EBX=0x%x  ECX=0x%x  EDX=0x%x\\n\",\n                frame->eax, frame->ebx, frame->ecx, frame->edx);\n        kprintf(\"  Error code: 0x%x\\n\", frame->err_code);\n        if (frame->int_no == 14) {\n            uint32_t cr2;\n            __asm__ volatile (\"mov %0, cr2\" : \"=r\"(cr2));\n            kprintf(\"  Page fault at address: 0x%x (P=%d W=%d U=%d)\\n\",\n                    cr2,\n                    frame->err_code & 1,\n                    (frame->err_code >> 1) & 1,\n                    (frame->err_code >> 2) & 1);\n        }\n        if (frame->int_no == 8) {\n            kprintf(\"  DOUBLE FAULT \u2014 halting\\n\");\n            for (;;) __asm__ volatile (\"cli; hlt\");\n        }\n        kprintf(\"  System halted.\\n\");\n        for (;;) __asm__ volatile (\"cli; hlt\");\n    } else if (frame->int_no < 48) {\n        // Hardware IRQ (32-47)\n        irq_dispatch(frame->int_no - 32, frame);\n    }\n}\n```\n---\n## The Double Fault: Your Last Line of Defense\n\n![Exception Cascading \u2014 Fault \u2192 Double Fault \u2192 Triple Fault](./diagrams/diag-m2-double-fault-cascade.svg)\n\nThe double fault (#DF, vector 8) deserves special attention because it is the CPU's emergency backstop before the catastrophic **triple fault** that resets the machine.\nHere is the cascade:\n1. An exception fires (e.g., #GP at vector 13).\n2. The CPU tries to call the handler at `idt[13]`.\n3. If *that* fails \u2014 because the IDT entry is invalid, the stack is corrupt, the handler address is unmapped \u2014 the CPU tries to invoke the double fault handler at `idt[8]`.\n4. If *that* fails \u2014 the double fault handler itself triggers an exception \u2014 the CPU has no recourse. It raises a **triple fault**, which is not an exception at all but a CPU reset signal. The machine reboots with no message.\nThe double fault handler protects you from triple faults during development. During the period when your IDT is partially set up, or when you introduce a bug that corrupts the stack, the double fault handler is the difference between \"kernel panics with a message\" and \"machine silently reboots.\"\nThe double fault always pushes error code `0` (it's fixed \u2014 the CPU can't tell you which original fault caused it). Your double fault handler should:\n```c\n// In interrupt_dispatch, for int_no == 8:\nkprintf(\"[DOUBLE FAULT] The kernel encountered an unrecoverable error.\\n\");\nkprintf(\"EIP=0x%x  ESP=0x%x  EFLAGS=0x%x\\n\",\n        frame->eip, frame->esp_dummy, frame->eflags);\nkprintf(\"This indicates stack corruption, invalid IDT entry,\\n\");\nkprintf(\"or an exception during exception handling.\\n\");\nkprintf(\"Error code (always 0): 0x%x\\n\", frame->err_code);\nfor (;;) __asm__ volatile (\"cli; hlt\");\n```\n> **Practical note on the double fault stack**: The double fault handler requires a valid stack to push its frame onto. If the original fault was caused by stack corruption (ESP pointing to unmapped memory), then the CPU may not even be able to deliver the double fault \u2014 triggering a triple fault immediately. The robust solution is a **dedicated double fault stack** referenced through a separate TSS. This is the Task Gate mechanism: `idt[8]` points to a Task Gate descriptor that performs a hardware task switch to a minimal TSS with its own stack. This is the only interrupt that conventionally uses a task gate on x86. For your implementation, a simple interrupt gate is fine for now \u2014 it handles the common debugging case where the fault was a logic error rather than stack pointer corruption.\n---\n## The PIC Crisis: Why Your Timer and Double Fault Collide\n{{DIAGRAM:diag-m2-pic-remapping}}\nThis is the most counterintuitive problem in x86 interrupt setup, and it affects every kernel implementer the first time they encounter it.\nThe Intel 8259 PIC (Programmable Interrupt Controller) is the chip that routes hardware interrupts \u2014 timer, keyboard, serial ports, disk controllers \u2014 to the CPU. Your system has two of them: a master PIC and a slave PIC cascaded together, collectively handling 16 IRQ lines (IRQ0\u2013IRQ15).\n{{DIAGRAM:diag-m2-pic-cascade-architecture}}\n**The original IBM PC mapped IRQs to CPU vectors starting at 8:**\n- Master PIC: IRQ0 (timer) \u2192 vector 8, IRQ1 (keyboard) \u2192 vector 9, ..., IRQ7 \u2192 vector 15\n- Slave PIC: IRQ8 \u2192 vector 70, ..., IRQ15 \u2192 vector 77\nIn real mode, this was fine. Vectors 0\u20137 were CPU exceptions in the real-mode IVT, but the IVT entries were just data \u2014 there was no hardware enforcement. Real-mode software could use both.\n**In protected mode, this mapping is catastrophic.**\nThe CPU defines vectors 0\u201331 as CPU exceptions with precise meanings. Vector 8 is **the double fault**. Vector 13 is **the general protection fault**. Vector 14 is **the page fault**.\nWith the default PIC mapping:\n- IRQ0 (timer tick) \u2192 vector 8 \u2192 **double fault**\n- IRQ1 (key pressed) \u2192 vector 9 \u2192 **coprocessor overrun**\nThe CPU cannot distinguish between \"the timer fired\" and \"the kernel has experienced a cascading fault.\" Your timer ISR would be called in what the CPU believes is a double fault context. If you enable interrupts before remapping the PIC, the first timer tick triggers what looks like a double fault, your double fault handler runs (if it exists), and everything is wrong.\n**The fix: remap both PICs before enabling interrupts.**\nThe 8259 PIC is configured through a sequence of Initialization Command Words (ICW1\u2013ICW4) written to specific I/O ports. You must reinitialize both PICs with new base vectors:\n- Master PIC: IRQ0\u2013IRQ7 \u2192 vectors 32\u201339\n- Slave PIC: IRQ8\u2013IRQ15 \u2192 vectors 40\u201347\n```c\n#define PIC_MASTER_CMD   0x20\n#define PIC_MASTER_DATA  0x21\n#define PIC_SLAVE_CMD    0xA0\n#define PIC_SLAVE_DATA   0xA1\n#define ICW1_INIT  0x10  // Initialization command\n#define ICW1_ICW4  0x01  // ICW4 is required\n#define ICW4_8086  0x01  // 8086/88 mode (as opposed to MCS-80 mode)\nstatic inline void io_wait(void) {\n    // Write to port 0x80 (unused) to introduce ~1-4\u03bcs delay\n    // Required for old ISA hardware that needs time to process commands\n    outb(0x80, 0);\n}\nvoid pic_remap(uint8_t master_offset, uint8_t slave_offset) {\n    // Save current interrupt masks\n    uint8_t master_mask = inb(PIC_MASTER_DATA);\n    uint8_t slave_mask  = inb(PIC_SLAVE_DATA);\n    // ICW1: Start initialization sequence (cascade mode)\n    outb(PIC_MASTER_CMD, ICW1_INIT | ICW1_ICW4);\n    io_wait();\n    outb(PIC_SLAVE_CMD,  ICW1_INIT | ICW1_ICW4);\n    io_wait();\n    // ICW2: Set vector offsets\n    outb(PIC_MASTER_DATA, master_offset); // Master: IRQ0 \u2192 vector 32\n    io_wait();\n    outb(PIC_SLAVE_DATA,  slave_offset);  // Slave: IRQ8 \u2192 vector 40\n    io_wait();\n    // ICW3: Tell master that slave is on IRQ2; tell slave its cascade identity\n    outb(PIC_MASTER_DATA, 0x04); // Master: slave connected at IRQ2 (bit 2 = 0b00000100)\n    io_wait();\n    outb(PIC_SLAVE_DATA,  0x02); // Slave: its cascade identity is IRQ2 (binary 010)\n    io_wait();\n    // ICW4: Set 8086 mode\n    outb(PIC_MASTER_DATA, ICW4_8086);\n    io_wait();\n    outb(PIC_SLAVE_DATA,  ICW4_8086);\n    io_wait();\n    // Restore interrupt masks\n    // (Mask all IRQs except timer (bit 0) and keyboard (bit 1))\n    outb(PIC_MASTER_DATA, master_mask);\n    outb(PIC_SLAVE_DATA,  slave_mask);\n}\n// Call this during kernel initialization, before sti\nvoid pic_init(void) {\n    pic_remap(0x20, 0x28); // Master starts at 32 (0x20), slave at 40 (0x28)\n    // Mask all IRQs initially, unmask only what we handle\n    outb(PIC_MASTER_DATA, 0xFC); // 1111 1100 = mask all except IRQ0 (timer) and IRQ1 (keyboard)\n    outb(PIC_SLAVE_DATA,  0xFF); // 1111 1111 = mask all slave IRQs\n}\n```\n> **Why `io_wait()`?** The old ISA bus (on which the 8259 PIC originated) runs at 8MHz, far slower than modern CPUs. Writing multiple bytes to PIC ports too quickly doesn't give the PIC time to process each command before the next arrives. The `io_wait()` function writes to port 0x80 \u2014 a port that doesn't correspond to any real hardware, used only as a delay mechanism \u2014 to burn a few hundred nanoseconds between writes. On modern systems this is rarely necessary, but it costs nothing and prevents mysterious failures on real hardware.\n---\n## The EOI Protocol: Acknowledgment as Flow Control\nAfter your IRQ handler runs, you must send an **End of Interrupt** (EOI) signal to the PIC. Without it, the PIC stops delivering interrupts \u2014 not just from the IRQ that fired, but all further interrupts at that priority level or lower.\nThe reason: the 8259 PIC uses **interrupt service register (ISR)** tracking. When it delivers an IRQ to the CPU, it sets the corresponding bit in its ISR register, which blocks further interrupts at that priority until the bit is cleared. The EOI command clears it. The PIC is literally waiting for your acknowledgment before it will deliver another interrupt.\nThis is the same flow-control pattern as TCP: the sender (PIC) cannot send more data (interrupts) until the receiver (CPU/kernel) acknowledges (EOI). The same concept appears in USB, PCIe, and HTTP/2 flow control windows \u2014 all prevent a fast producer from overwhelming a slow consumer.\n```c\n#define PIC_EOI 0x20\nvoid pic_send_eoi(uint8_t irq) {\n    if (irq >= 8) {\n        // Slave PIC also needs EOI for IRQ8-15\n        outb(PIC_SLAVE_CMD, PIC_EOI);\n    }\n    // Master PIC always gets EOI\n    outb(PIC_MASTER_CMD, PIC_EOI);\n}\n```\n**The master/slave distinction is critical.** For IRQ0\u2013IRQ7 (master only), send EOI only to the master. For IRQ8\u2013IRQ15 (slave PICs), send EOI to both slave and master \u2014 the slave must release the IRQ, and the master must release IRQ2 (the cascade line that the slave connects through).\nYour IRQ dispatch function:\n```c\nvoid irq_dispatch(uint8_t irq, struct interrupt_frame *frame) {\n    switch (irq) {\n        case 0: timer_handler(frame); break;\n        case 1: keyboard_handler(frame); break;\n        default: break; // Spurious or unhandled IRQ\n    }\n    pic_send_eoi(irq);\n}\n```\n**Spurious IRQs**: Sometimes the PIC reports an interrupt that didn't actually occur \u2014 a \"spurious IRQ.\" This happens when the CPU acknowledges an interrupt that the PIC has since cancelled (due to electrical noise or race conditions). IRQ7 (master spurious) and IRQ15 (slave spurious) are the designated spurious vectors. A robust kernel checks the PIC's In-Service Register (ISR) before processing these:\n```c\n// Read PIC In-Service Register\nuint16_t pic_get_isr(void) {\n    outb(PIC_MASTER_CMD, 0x0B); // OCW3: read ISR\n    outb(PIC_SLAVE_CMD,  0x0B);\n    return (inb(PIC_SLAVE_CMD) << 8) | inb(PIC_MASTER_CMD);\n}\n```\nIf bit 7 of the master ISR is not set when IRQ7 fires, it's spurious \u2014 don't send EOI, don't process. For IRQ15 spurious, send EOI only to the master (the slave didn't actually assert an interrupt).\n---\n## The PIT Timer: Heartbeat of Your Kernel\n{{DIAGRAM:diag-m2-pit-timer-programming}}\nThe PIT (Programmable Interval Timer, Intel 8253/8254) generates periodic interrupts on IRQ0. It is the clock source for your kernel tick \u2014 the regular interval at which the scheduler will run (Milestone 4) and from which all timekeeping derives.\nThe PIT has an internal oscillator running at exactly 1,193,182 Hz (this frequency comes from dividing the original IBM PC's 14.318 MHz crystal by 12, then by 4 \u2014 a chain of historical accidents that became a permanent standard). You program a **divisor**: the PIT divides its input frequency by your divisor and fires an interrupt at the resulting rate.\n```\nTimer frequency = 1,193,182 Hz / divisor\nDivisor = 1,193,182 / desired_frequency\n```\nFor 100Hz (10ms tick interval): divisor = 1193182 / 100 = 11931 (\u2248 11932).\n```c\n#define PIT_CHANNEL0  0x40  // Channel 0 data port\n#define PIT_COMMAND   0x43  // Mode/command register\nvoid pit_init(uint32_t frequency) {\n    uint16_t divisor = 1193182 / frequency;\n    // Command byte: channel 0, access mode=lo/hi byte, mode 3 (square wave), binary\n    // Bits: 7-6=channel(00), 5-4=access(11=lo+hi), 3-1=mode(011=square wave), 0=binary(0)\n    outb(PIT_COMMAND, 0x36);\n    // Write divisor: low byte first, then high byte\n    outb(PIT_CHANNEL0, divisor & 0xFF);\n    outb(PIT_CHANNEL0, (divisor >> 8) & 0xFF);\n}\n```\n**Mode 3 (square wave generator)** is the standard mode for a timer: it fires an interrupt every `divisor` input ticks, then reloads and repeats automatically. You set it once and it runs forever without further programming.\nYour timer handler:\n```c\nstatic volatile uint64_t tick_counter = 0;\nvoid timer_handler(struct interrupt_frame *frame) {\n    tick_counter++;\n    // In Milestone 4, this becomes: schedule(); (preempt current process)\n}\nuint64_t pit_get_ticks(void) {\n    return tick_counter;\n}\nuint64_t pit_get_ms(void) {\n    return tick_counter * 10; // At 100Hz, each tick = 10ms\n}\n```\n**Why `volatile`?** The `tick_counter` is modified in an interrupt handler (asynchronously) and read from non-interrupt code (synchronously). Without `volatile`, the compiler may cache the value in a register and never re-read it from memory, causing the non-interrupt code to see a stale value. `volatile` forces every read to fetch from memory.\n> **Interrupt Latency and Real-Time Systems**: The overhead you're experiencing \u2014 saving 8 general-purpose registers, 4 segment registers, plus the 5 values the CPU pushes automatically \u2014 is 17 values \u00d7 4 bytes = 68 bytes of stack writes on every interrupt. At 100Hz this costs effectively nothing. But consider a system with 10,000 interrupts/second (e.g., a network driver at 10Gbps with small packets): interrupt overhead becomes a measurable fraction of CPU time. This is why RTOS kernels (FreeRTOS, VxWorks, Zephyr) minimize what they save/restore in ISRs and immediately defer work to \"bottom-half\" handlers that run in a task context. Linux's softirq/tasklet system and the NET_RX_SOFTIRQ that processes network packets are direct responses to this overhead. The PREEMPT_RT patchset takes this further \u2014 making interrupt handlers themselves preemptible by converting them to kernel threads, reducing worst-case latency at the cost of some throughput.\n**Initialize everything, then enable interrupts:**\n```c\nvoid kernel_main(void) {\n    vga_clear();\n    serial_init();\n    kprintf(\"Kernel started.\\n\");\n    gdt_init();           // Already done in M1\n    idt_init();           // Set up all 256 IDT entries\n    pic_remap(0x20, 0x28); // Remap PICs BEFORE enabling interrupts\n    pit_init(100);        // 100Hz timer\n    __asm__ volatile (\"sti\"); // NOW it is safe to enable interrupts\n    kprintf(\"Interrupts enabled. Tick: %llu\\n\", pit_get_ticks());\n    for (;;) {\n        __asm__ volatile (\"hlt\"); // Wait for next interrupt\n    }\n}\n```\nThe `hlt` instruction in the main loop is intentional: it halts the CPU until the next interrupt, reducing power consumption and making QEMU simulation cheaper. The kernel's \"idle\" state is resting between interrupts.\n---\n## The PS/2 Keyboard Driver\n\n![PS/2 Keyboard \u2014 Scancode to ASCII Pipeline](./diagrams/diag-m2-keyboard-scancode-pipeline.svg)\n\nWhen you press a key on a PS/2 keyboard, the keyboard controller generates an IRQ1. Reading port `0x60` gives you a **scancode** \u2014 a hardware-specific code number that identifies which key was pressed or released, independent of any software notion of ASCII characters.\n### Scancodes: Make and Break\nPS/2 keyboards generate two scancodes per key event:\n- **Make code** (key press): The scancode for the key, e.g., `0x1E` for 'A'.\n- **Break code** (key release): The make code with bit 7 set, e.g., `0x9E` for releasing 'A'.\nYour driver must distinguish between these. Make code arrives with bit 7 clear. Break code arrives with bit 7 set (value \u2265 `0x80`). If you ignore break codes and treat every scancode as a key press, holding down 'A' generates a stream of make codes from the keyboard's autorepeat \u2014 which is correct \u2014 but releasing 'A' then sends `0x9E`, which your scancode table maps to nothing (or garbage) if you haven't masked bit 7. Even worse, if you track modifier key state (Shift, Ctrl, Alt), you must see the break code to know when the modifier was released.\n### Scancode Set 1 to ASCII\nThe mapping from PS/2 Set 1 scancodes to ASCII is a lookup table:\n```c\nstatic const char scancode_to_ascii[128] = {\n    0,   27, '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '-', '=',\n    '\\b', '\\t', 'q', 'w', 'e', 'r', 't', 'y', 'u', 'i', 'o', 'p', '[', ']',\n    '\\n', 0, 'a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', ';', '\\'', '`',\n    0, '\\\\', 'z', 'x', 'c', 'v', 'b', 'n', 'm', ',', '.', '/', 0,\n    '*', 0, ' ', 0,  // Caps Lock, F1-F12, etc.\n    // ... entries 58-127 are function keys, numpad, etc. (0 = no ASCII representation)\n};\nstatic const char scancode_to_ascii_shift[128] = {\n    0,   27, '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+',\n    '\\b', '\\t', 'Q', 'W', 'E', 'R', 'T', 'Y', 'U', 'I', 'O', 'P', '{', '}',\n    '\\n', 0, 'A', 'S', 'D', 'F', 'G', 'H', 'J', 'K', 'L', ':', '\"', '~',\n    0, '|', 'Z', 'X', 'C', 'V', 'B', 'N', 'M', '<', '>', '?', 0,\n    '*', 0, ' ', 0,\n};\n```\nModifier tracking:\n```c\nstatic int shift_pressed = 0;\nstatic int ctrl_pressed  = 0;\nstatic int alt_pressed   = 0;\n#define SCANCODE_LSHIFT 0x2A\n#define SCANCODE_RSHIFT 0x36\n#define SCANCODE_LCTRL  0x1D\n#define SCANCODE_LALT   0x38\n#define SCANCODE_CAPS   0x3A\n```\n### Circular Keyboard Buffer\nThe keyboard driver must not block the IRQ handler. The handler's job is minimal: read the scancode, convert it to ASCII, and put it in a buffer. Something else \u2014 a `keyboard_read()` function called from kernel or user code \u2014 consumes from that buffer.\nThis is a **circular buffer** (also called a ring buffer): a fixed-size array where `head` tracks the next position to write and `tail` tracks the next position to read. When a pointer reaches the end of the array, it wraps back to 0.\n```c\n#define KEYBOARD_BUFFER_SIZE 256  // Must be power of 2 for efficient wrapping\ntypedef struct {\n    char    buf[KEYBOARD_BUFFER_SIZE];\n    uint8_t head;  // Next write position\n    uint8_t tail;  // Next read position\n} ring_buffer_t;\nstatic ring_buffer_t kbd_buf = {0};\nstatic int ring_full(ring_buffer_t *rb) {\n    return ((rb->head + 1) % KEYBOARD_BUFFER_SIZE) == rb->tail;\n}\nstatic int ring_empty(ring_buffer_t *rb) {\n    return rb->head == rb->tail;\n}\nstatic void ring_push(ring_buffer_t *rb, char c) {\n    if (!ring_full(rb)) {\n        rb->buf[rb->head] = c;\n        rb->head = (rb->head + 1) % KEYBOARD_BUFFER_SIZE;\n    }\n    // If full: silently drop. Real OSes might set a \"keyboard overrun\" flag.\n}\nstatic char ring_pop(ring_buffer_t *rb) {\n    if (ring_empty(rb)) return 0;\n    char c = rb->buf[rb->tail];\n    rb->tail = (rb->tail + 1) % KEYBOARD_BUFFER_SIZE;\n    return c;\n}\n```\n> **Circular buffers across the stack**: The keyboard buffer you just built is the same data structure that powers io_uring's submission and completion queues (the kernel shares ring buffer memory with user space \u2014 no syscall needed per I/O completion), DPDK's packet rings (NIC driver writes, application reads, both lock-free with a single producer and single consumer), Linux's `pipe` (reading and writing processes share a circular 64KB buffer), and audio driver DMA buffers (the sound card DMA-writes audio data into a ring; your driver reads it out). When the producer index (head) and consumer index (tail) are maintained by different \"actors\" \u2014 in your case, the IRQ handler and kernel code \u2014 the structure becomes a **lock-free single-producer single-consumer queue**, one of the few data structures that can be implemented safely without atomics when both producer and consumer run on the same CPU. This is because interrupt handlers can only *interrupt* running code \u2014 they can't run concurrently on a single-core system.\nThe complete keyboard IRQ handler:\n```c\nvoid keyboard_handler(struct interrupt_frame *frame) {\n    (void)frame;\n    uint8_t scancode = inb(0x60); // Read scancode from PS/2 data port\n    if (scancode & 0x80) {\n        // Break code (key release) \u2014 strip the high bit to get the key\n        uint8_t key = scancode & 0x7F;\n        if (key == SCANCODE_LSHIFT || key == SCANCODE_RSHIFT) shift_pressed = 0;\n        if (key == SCANCODE_LCTRL)  ctrl_pressed  = 0;\n        if (key == SCANCODE_LALT)   alt_pressed   = 0;\n        return; // Don't add to buffer on key release\n    }\n    // Make code (key press)\n    if (scancode == SCANCODE_LSHIFT || scancode == SCANCODE_RSHIFT) {\n        shift_pressed = 1;\n        return;\n    }\n    if (scancode == SCANCODE_LCTRL) { ctrl_pressed = 1; return; }\n    if (scancode == SCANCODE_LALT)  { alt_pressed  = 1; return; }\n    // Convert scancode to ASCII\n    char ascii;\n    if (shift_pressed) {\n        ascii = scancode_to_ascii_shift[scancode];\n    } else {\n        ascii = scancode_to_ascii[scancode];\n    }\n    if (ascii != 0) {\n        ring_push(&kbd_buf, ascii);\n    }\n    // EOI is sent by irq_dispatch after this returns\n}\n// Called by kernel or shell code to read a character (blocking)\nchar keyboard_getchar(void) {\n    while (ring_empty(&kbd_buf)) {\n        __asm__ volatile (\"hlt\"); // Wait for an interrupt to fill the buffer\n    }\n    return ring_pop(&kbd_buf);\n}\n```\n> **Hardware Soul**: Port 0x60 is the PS/2 controller data register. Reading it consumes the byte \u2014 the keyboard controller dequeues it and will not deliver the next scancode until the current one is read. This means if you fail to read port 0x60 in your IRQ handler (e.g., the handler doesn't execute because EOI was sent but data wasn't read), the keyboard controller holds the unread byte and the keyboard may stop generating interrupts until it's consumed. Always read port 0x60 in every keyboard IRQ handler, regardless of what you do with the byte. The controller at I/O port base 0x60/0x64 is the Intel 8042 (or a compatible implementation in the chipset), which mediates PS/2 protocol (a serial protocol at 10\u201313kHz) and presents data to the CPU through the familiar port interface.\n---\n## The Complete Initialization Sequence\nThe order of operations matters. Here is the canonical sequence, with the reasoning for each step:\n```c\nvoid kernel_main(void) {\n    // 1. VGA + serial (already working from M1)\n    vga_clear();\n    serial_init();\n    kprintf(\"[OK] Output subsystem initialized\\n\");\n    // 2. GDT (already loaded in bootloader, but reinitialize from C for cleanliness)\n    gdt_init();\n    kprintf(\"[OK] GDT loaded (%d entries)\\n\", 5);\n    // 3. IDT \u2014 set up gate descriptors, but DON'T enable interrupts yet\n    idt_init();\n    kprintf(\"[OK] IDT loaded (%d gates)\\n\", 256);\n    // 4. Remap PIC BEFORE enabling interrupts\n    // If we enabled interrupts before remapping, the first timer tick\n    // would fire at vector 8 (double fault). Disaster.\n    pic_remap(0x20, 0x28);\n    kprintf(\"[OK] PIC remapped: master=0x20, slave=0x28\\n\");\n    // 5. Program PIT to desired frequency\n    pit_init(100); // 100Hz = 10ms ticks\n    kprintf(\"[OK] PIT initialized at 100Hz\\n\");\n    // 6. Enable interrupts (sti)\n    // Everything must be in place before this instruction\n    __asm__ volatile (\"sti\");\n    kprintf(\"[OK] Interrupts enabled\\n\");\n    // 7. Test: wait a second, check tick counter\n    uint64_t start = pit_get_ticks();\n    while (pit_get_ticks() - start < 100) {\n        __asm__ volatile (\"hlt\");\n    }\n    kprintf(\"[OK] 1 second elapsed (100 ticks). Timer is working.\\n\");\n    // 8. Test keyboard\n    kprintf(\"Press any key: \");\n    char c = keyboard_getchar();\n    kprintf(\"Got: '%c' (0x%x)\\n\", c, (uint8_t)c);\n    // Idle loop\n    for (;;) __asm__ volatile (\"hlt\");\n}\n```\nThis sequence is the correct one. Every step before `sti` is preparation. `sti` is the moment you open the door to hardware reality.\n---\n## Debugging Interrupt Problems\nInterrupt bugs are notoriously difficult to diagnose because they are asynchronous. Here are the tools and patterns:\n**QEMU's `-d int` flag**: The most powerful debugging tool for interrupt problems. It logs every interrupt and exception to stderr, including the vector number, EIP, and error code. A triple fault will appear as a rapid sequence of exceptions. The *first* exception in the log is your actual bug.\n```bash\nqemu-system-i386 -d int,cpu_reset -no-reboot -drive file=os.img,format=raw 2>&1 | head -50\n```\n**Common failure modes and diagnoses:**\n| Symptom | Diagnosis |\n|---------|-----------|\n| Machine resets immediately on first timer tick | PIC not remapped; IRQ0 firing at vector 8 (double fault vector) |\n| `sti` causes immediate reset | IDT not loaded (`lidt` not called, or wrong IDTR) |\n| Interrupt fires but handler corrupts register state | `pusha`/`popa` missing or in wrong order |\n| Keyboard interrupt fires but no character appears | Break code not handled; scancode lookup table wrong |\n| Interrupts fire once then stop | EOI not sent; PIC waiting for acknowledgment |\n| Double fault on valid exception | Stack pointer corrupted before exception; IDT entry invalid |\n| Timer fires but tick counter never increments | `volatile` missing from `tick_counter` |\n| Wrong character for keypress | Not handling break codes correctly; shift state corrupted |\n**GDB debugging of interrupt handlers:**\n```bash\n# In one terminal:\nqemu-system-i386 -s -S -no-reboot -drive file=os.img,format=raw\n# In another:\ngdb kernel.elf\n(gdb) target remote :1234\n(gdb) set architecture i386\n(gdb) break isr_common_stub     # Break when any interrupt fires\n(gdb) break keyboard_handler\n(gdb) break timer_handler\n(gdb) continue\n```\nWhen GDB breaks inside your ISR, examine the stack frame:\n```gdb\n(gdb) info registers            # Current register state (post-pusha)\n(gdb) x/20x $esp                # Raw stack contents\n(gdb) p *((struct interrupt_frame *)$esp)  # Typed stack frame view\n```\n---\n## Three-Level View: What Happens When You Press 'A'\nThis is the most concrete trace of the hardware-to-software stack in this milestone.\n**Level 3 \u2014 Hardware:**\n1. You press 'A'. The key mechanism closes a circuit.\n2. The PS/2 keyboard's internal microcontroller detects the keypress, generates scancode `0x1E`, and serializes it onto the PS/2 clock/data lines at ~10kHz.\n3. The PS/2 host controller (Intel 8042 or chipset equivalent) receives the serial byte, buffers it in its output register, and asserts the keyboard interrupt line (IRQ1).\n4. The 8259 master PIC receives the IRQ1 signal. Since IRQ1 is not masked, it asserts the CPU's INTR pin.\n**Level 2 \u2014 CPU and Kernel:**\n5. The CPU, currently executing `hlt` in the idle loop, detects INTR asserted.\n6. The CPU finishes its current instruction (or `hlt` interruption is immediate), then:\n   - Checks IF (interrupt flag) in EFLAGS \u2014 it's set (we called `sti`), so proceed.\n   - Acknowledges the PIC via the INTA bus cycle; PIC responds with vector 33 (remapped IRQ1).\n   - Pushes EFLAGS, CS, EIP onto the kernel stack.\n   - Reads `idt[33].selector` and `idt[33].offset` \u2014 these point to `isr_33` in kernel code.\n   - Loads CS with the kernel code selector, EIP with the handler address.\n7. `isr_33` runs: pushes `0` (no error code) and `33` (interrupt number), jumps to `isr_common_stub`.\n8. `isr_common_stub`: `pusha`, push segment registers, reload kernel data segments, call `interrupt_dispatch`.\n9. `interrupt_dispatch` routes to `irq_dispatch(1, frame)`, which calls `keyboard_handler`.\n10. `keyboard_handler`: reads `0x1E` from port 0x60. Bit 7 clear = make code. `scancode_to_ascii[0x1E]` = `'a'`. Calls `ring_push(&kbd_buf, 'a')`.\n11. Returns through `irq_dispatch`, which calls `pic_send_eoi(1)` \u2014 writes `0x20` to port `0x20` (master PIC command).\n12. `isr_common_stub` restores: pop segment regs, `popa`, `add esp, 8`, `iret`.\n13. `iret` pops EIP, CS, EFLAGS \u2014 restoring the idle loop's `hlt`.\n**Level 1 \u2014 Application:**\n14. `keyboard_getchar()` finds `ring_empty()` is now false, calls `ring_pop()`, returns `'a'`.\n15. The kernel's test code prints the character.\nThirteen distinct steps from finger-press to character output. Every one of them is code you wrote.\n---\n## System Awareness: Where You Now Stand\n\n![OS Kernel \u2014 Satellite System Map](./diagrams/diag-satellite-os-map.svg)\n\nAfter this milestone, your kernel can:\n**React to hardware events.** The timer fires 100 times per second and you know it. A key pressed generates an ASCII character in a buffer. A divide-by-zero produces a diagnostic message instead of a silent reset.\n**Survive development bugs.** The double fault handler catches cascading faults and gives you a message. The exception handlers for #GP and #PF print EIP and the faulting address. Debugging has shifted from archaeology (guess what went wrong) to engineering (read the error message).\n**What's still missing.** You have no memory allocator \u2014 `kmalloc` doesn't exist. You have no process concept \u2014 only the single kernel thread running `kernel_main`. You have no virtual memory \u2014 physical and virtual addresses are the same thing. All of these come in Milestones 3 and 4.\n**The critical connection forward.** The page fault handler you wrote in this milestone \u2014 which currently just prints and halts \u2014 will become the most important function in Milestone 3. When you enable paging, every access to an unmapped page fires exception 14. Your handler will inspect CR2 and the error code, map the missing page, and return via `iret` to re-execute the faulting instruction. The handler you built here is the foundation that makes demand paging possible.\nThe timer interrupt you wrote \u2014 which currently only increments a counter \u2014 becomes the scheduler's trigger in Milestone 4. Every tick will call `schedule()`, which saves the current process's registers to a PCB and loads the next process's registers. The interrupt frame structure you defined in this milestone is the exact memory layout the context switcher will read and write.\n---\n## Knowledge Cascade\n**1. Interrupt latency and real-time operating systems.** Every register you push in `isr_common_stub` is a memory write with a cache-line cost. Eight GP registers + four segment registers = 12 writes. At 100Hz this is invisible. At 100,000 interrupts/second (a 10Gbps NIC receiving minimum-size frames), this overhead becomes the dominant CPU cost. RTOS kernels (Zephyr, FreeRTOS, VxWorks) respond by using **minimal ISR prologs** \u2014 saving only the registers actually used by the handler \u2014 and immediately deferring all real work to task context. Linux's **bottom-half** mechanism (softirqs, tasklets, workqueues) exists for exactly this reason: the hardware IRQ handler does the absolute minimum (acknowledge, enqueue data), and a \"softirq\" runs later in a schedulable context. The `PREEMPT_RT` patchset converts even these softirqs to preemptible kernel threads. Every design decision traces back to the fundamental question you just encoded in assembly: how many registers do you save?\n**2. EOI as flow control \u2014 a universal pattern.** The PIC stops delivering interrupts until it receives EOI. This is not a quirk of the 8259 \u2014 it is a **fundamental flow control pattern** that appears throughout systems design. TCP uses acknowledgment numbers (the receiver says \"I have processed up to byte N; you may send more\"). USB uses handshake packets (ACK, NAK, STALL) to confirm or reject each transfer. PCIe uses credit-based flow control (the receiver grants credits; the sender consumes them per packet; no new packets without credits). HTTP/2 uses stream windows. The 8259 PIC's EOI register is the simplest expression of this principle: a single-bit \"I'm done, send the next one.\" When you understand EOI, you understand the motivating principle behind every flow-control protocol you will ever encounter.\n**3. Exception error codes and stack forensics.** The page fault error code (present/write/user bits) is not merely debugging information. It is the **state machine input** for demand paging. When your Milestone 3 page fault handler receives a fault with P=0 (page not present), W=1 (was a write), U=0 (kernel access), it knows: allocate a new frame, map it into the page table at the faulting address (from CR2), and return via `iret` to re-execute the write. This is demand paging. The same mechanism implements copy-on-write: on a write fault to a shared page (P=1, W=1), allocate a new frame, copy the data, remap, and return. Memory-mapped files: on a page-not-present fault in the mmap region, read the file block into a new frame, map it, return. Every memory management technique in Linux and every modern OS is built on this single mechanism: a page fault handler that reads an error code, takes corrective action, and returns to retry the faulting instruction.\n**4. The double fault and fail-safe hierarchies.** The double fault handler is a specific instance of a general systems pattern: **layered fault containment**. The application has try/catch. The runtime has an unhandled exception handler. The OS has a process-level segfault handler. The CPU has the double fault. The system has watchdog timers and NMIs (Non-Maskable Interrupts \u2014 interrupts that cannot be masked even with `cli`, used for hardware-level fault detection). The PCIe bus has hardware error-correcting codes. DRAM has ECC. At each level, the goal is the same: catch failures that escaped the layer below, log diagnostic information, and either recover or fail cleanly. The double fault handler you built is the last software layer in this hierarchy. Below it is only hardware \u2014 and hardware failures (machine check exception #18, triple fault) produce resets, not error messages.\n**5. Circular buffers as the universal producer-consumer interface.** The keyboard ring buffer you built is architecturally identical to structures used throughout the OS stack. Linux `pipe(2)` is a 64KB circular buffer between two processes. `io_uring`'s submission queue and completion queue are lock-free circular buffers shared between kernel and user space via mmap \u2014 the user submits I/O descriptors by advancing the SQ head, the kernel consumes them; the kernel writes completions by advancing the CQ head, the user consumes them. DPDK's `rte_ring` is a lock-free circular buffer with SIMD-optimized bulk enqueue/dequeue, capable of millions of packets per second. The pattern is: fixed-size array, head (write position), tail (read position), full when `(head+1) % size == tail`, empty when `head == tail`. Your keyboard buffer has a single producer (IRQ handler) and single consumer (keyboard_getchar). This is the easiest case: no atomics needed on a single-core system. Multi-producer or multi-consumer rings require careful use of atomic compare-and-swap to avoid the ABA problem \u2014 this is what `rte_ring` solves with a two-phase enqueue protocol.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-os-m3 -->\n<!-- MS_ID: build-os-m3 -->\n# Milestone 3: Physical and Virtual Memory Management\n\n![OS Kernel \u2014 Satellite System Map](./diagrams/diag-satellite-os-map.svg)\n\nAt the end of Milestone 2, your kernel can hear the hardware \u2014 timer ticks, keypresses, exceptions. But it is running in a world with no boundaries. Every address you use is a physical address. Your kernel, VGA buffer, stack, and any future process data all coexist in one flat, unprotected address space. There is no `malloc`, no isolation, no way to give a future process its own view of memory. This milestone changes all of that. You will teach the CPU to lie \u2014 to present every piece of software with a virtual address space that the hardware translates, transparently and continuously, to physical reality.\nThis is the milestone where your kernel transitions from \"a program that runs on bare metal\" to \"an operating system that controls the machine.\"\n---\n## The Revelation: The MMU Is Not a Software Lookup Table\nBefore we build anything, you need to shed a misconception that almost every developer carries when they first approach virtual memory.\nHere is what most people imagine: the OS maintains a table mapping virtual addresses to physical addresses. When the CPU needs to access memory, it checks this table \u2014 like a dictionary lookup \u2014 translates the address, and proceeds. The table is a data structure the OS \"uses\" on demand.\n**This model is completely wrong, and understanding why changes how you think about everything in this milestone.**\nThe Memory Management Unit (MMU) is not a software component. It is a dedicated circuit inside the CPU that intercepts **every single memory access** \u2014 every load, every store, every instruction fetch \u2014 and performs address translation **on the critical path** of execution. The CPU does not \"consult\" the page table; the MMU hardware walks it autonomously, in a format dictated down to individual bits by the silicon, before the access reaches any cache or the memory bus.\nThe implication is staggering. Your page directory and page tables are not arbitrary data structures you designed \u2014 they are a contract with the hardware, and the CPU walks them in a specific, mandatory sequence thousands of times per second, without asking your kernel for permission.\n**The TLB (Translation Lookaside Buffer) exists because of this.** A two-level page table walk requires two memory reads (one for the Page Directory Entry, one for the Page Table Entry) before the actual memory access. Without caching, every memory instruction would incur three total memory accesses \u2014 a 3\u00d7 slowdown on every instruction the CPU executes. The TLB is a small, fully-associative cache inside the CPU that stores recently-used virtual\u2192physical translations. When a translation is cached (TLB hit), the walk is bypassed entirely. When it's not (TLB miss), the CPU walks the page tables, stores the result in the TLB, and proceeds.\nThis creates the critical constraint that will shape much of your implementation: **modifying a page table entry without flushing the TLB leaves the CPU using a stale translation.** The bug this produces is non-deterministic, because TLB eviction depends on access patterns \u2014 the stale entry might be evicted immediately by other accesses, or it might persist for thousands of instructions, causing a fault in a completely unrelated code path. This is not a latent bug you can reason about statically. It is a Heisenbug that appears and disappears depending on what other code runs before the affected page is accessed.\nAnd there is one more truth, the deepest one, that the identity-mapping requirement will force you to confront directly: **the instruction that enables paging is fetched using a physical address. The next instruction is fetched using a virtual address.** If the virtual\u2192physical mapping for that next instruction address does not map to the same physical memory, the CPU faults on the instruction immediately after `mov cr0, eax`. There is no grace period, no transition period, no fallback. One instruction with physical addressing, the next with virtual \u2014 the boundary is absolute.\nNow you understand why identity mapping exists. Now let's build everything.\n---\n## Phase 1: Discovering Physical Memory with E820\n{{DIAGRAM:diag-m3-e820-memory-map}}\nBefore you can allocate physical memory, you need to know how much you have and where it is. RAM is not a contiguous block from address 0 to the top. Physical address space has holes: regions reserved for BIOS ROMs, MMIO for hardware devices (VGA at `0xB8000`\u2013`0xBFFFF`), ACPI tables, and the Extended BIOS Data Area (EBDA) just below 1MB.\n**The E820 interface** (named for BIOS function INT 15h/EAX=0xE820) is the standardized way to query the BIOS for a physical memory map before leaving real mode. If you used GRUB as your bootloader, the Multiboot specification provides this information in the `multiboot_info` structure passed to your kernel. Either way, the result is a list of memory regions, each classified as one of:\n- **Type 1 (Usable)**: RAM your OS can use freely.\n- **Type 2 (Reserved)**: Firmware data, MMIO regions, ROM \u2014 do not touch.\n- **Type 3 (ACPI Reclaimable)**: ACPI tables; can be freed after the OS reads them.\n- **Type 4 (ACPI NVS)**: ACPI non-volatile storage; never reclaim.\n- **Type 5 (Bad Memory)**: Defective RAM reported by firmware.\nIf you're using GRUB + Multiboot (the recommended approach for this project), the bootloader fills in an `mmap_addr` field and `mmap_length` field in the Multiboot info structure:\n```c\n// Multiboot info structure (abbreviated \u2014 only fields we need)\ntypedef struct {\n    uint32_t flags;          // Bit flags indicating which fields are valid\n    // ... (memory, cmdline, mods fields) ...\n    uint32_t mmap_length;    // Total size of memory map entries in bytes\n    uint32_t mmap_addr;      // Physical address of first mmap entry\n} __attribute__((packed)) multiboot_info_t;\n// Each entry in the memory map\ntypedef struct {\n    uint32_t size;           // Size of this entry minus 4 (not including this field)\n    uint64_t base_addr;      // Starting physical address of region\n    uint64_t length;         // Length of region in bytes\n    uint32_t type;           // 1=usable, 2=reserved, 3=ACPI, 4=NVS, 5=bad\n} __attribute__((packed)) multiboot_mmap_entry_t;\n```\n> **Why `uint64_t` for addresses in a 32-bit kernel?** Even a 32-bit x86 CPU can address up to 64 GB of physical memory using PAE (Physical Address Extension). The BIOS reports 64-bit physical addresses because it must work with all CPU variants. Your simple 32-bit kernel will only use the low 32 bits, but you must read 64-bit values from the structure to correctly advance the pointer to the next entry.\nParse the memory map immediately in `kernel_main`, before any other initialization that touches memory:\n```c\n#define MBOOT_FLAG_MMAP  (1 << 6)  // Bit 6 of flags = mmap fields valid\ntypedef struct {\n    uint64_t base;\n    uint64_t length;\n    uint32_t type;\n} phys_region_t;\n#define MAX_REGIONS 64\nstatic phys_region_t phys_regions[MAX_REGIONS];\nstatic int phys_region_count = 0;\nvoid mmap_parse(multiboot_info_t *mbi) {\n    if (!(mbi->flags & MBOOT_FLAG_MMAP)) {\n        kprintf(\"[PANIC] Bootloader did not provide memory map\\n\");\n        for (;;) __asm__ volatile (\"hlt\");\n    }\n    multiboot_mmap_entry_t *entry =\n        (multiboot_mmap_entry_t *)(uintptr_t)mbi->mmap_addr;\n    uintptr_t end = mbi->mmap_addr + mbi->mmap_length;\n    kprintf(\"Physical memory map:\\n\");\n    while ((uintptr_t)entry < end) {\n        if (phys_region_count < MAX_REGIONS) {\n            phys_regions[phys_region_count].base   = entry->base_addr;\n            phys_regions[phys_region_count].length = entry->length;\n            phys_regions[phys_region_count].type   = entry->type;\n            phys_region_count++;\n        }\n        static const char *type_names[] = {\n            \"\", \"Usable\", \"Reserved\", \"ACPI Reclaim\", \"ACPI NVS\", \"Bad\"\n        };\n        uint32_t t = entry->type;\n        kprintf(\"  0x%08x - 0x%08x  %s\\n\",\n                (uint32_t)entry->base_addr,\n                (uint32_t)(entry->base_addr + entry->length - 1),\n                (t >= 1 && t <= 5) ? type_names[t] : \"Unknown\");\n        // Advance to next entry: size field + 4 bytes for the size field itself\n        entry = (multiboot_mmap_entry_t *)((uintptr_t)entry + entry->size + 4);\n    }\n}\n```\nA typical memory map on a 128MB QEMU machine looks like:\n```\nPhysical memory map:\n  0x00000000 - 0x0009FBFF  Usable        (low conventional memory: 639 KB)\n  0x0009FC00 - 0x0009FFFF  Reserved      (EBDA \u2014 Extended BIOS Data Area)\n  0x000F0000 - 0x000FFFFF  Reserved      (BIOS ROM)\n  0x00100000 - 0x07FDFFFF  Usable        (high memory: ~127 MB, your kernel lives here)\n  0x07FE0000 - 0x07FFFFFF  Reserved      (ACPI tables)\n  0xFFFC0000 - 0xFFFFFFFF  Reserved      (BIOS flash/ROM)\n```\nThis map tells you everything you need to bootstrap the physical frame allocator: which address ranges are safe to use, and which must be permanently forbidden.\n---\n## Phase 2: The Bitmap Frame Allocator\n{{DIAGRAM:diag-m3-bitmap-allocator}}\nYou know which physical memory regions are usable. Now you need a data structure that lets you allocate and free individual 4KB **page frames** \u2014 4KB-aligned chunks of physical memory that the MMU can map into page tables.\n> **Why 4KB?** The x86 page size is architecturally fixed at 4KB for standard (non-huge) pages. This comes from the page directory/table entry format, which stores 20-bit physical frame numbers \u2014 the remaining 12 bits of a 32-bit address are the offset within the page, giving 2\u00b9\u00b2 = 4096 bytes per page. This is not a design choice you make; it is the hardware contract.\n**The data structure: a bitmap.** One bit per 4KB frame. Bit = 0 means the frame is free; bit = 1 means it is used (or reserved). For 128MB of physical memory, you need 128MB / 4KB = 32,768 frames, which requires 32,768 / 8 = 4,096 bytes = 4KB for the bitmap. The bitmap fits in a single page \u2014 an elegant property of the data structure.\n```c\n#define PAGE_SIZE       4096\n#define FRAMES_PER_WORD 32           // 32-bit words\n#define MAX_FRAMES      (1024 * 1024 / 4)  // 4GB / 4KB = 1M frames max\n// The bitmap lives in BSS \u2014 zeroed by our boot stub\nstatic uint32_t frame_bitmap[MAX_FRAMES / FRAMES_PER_WORD];\nstatic uint32_t total_frames = 0;\nstatic uint32_t free_frames  = 0;\n// Mark a frame as used (set bit)\nstatic void frame_set(uint32_t frame) {\n    frame_bitmap[frame / 32] |= (1u << (frame % 32));\n}\n// Mark a frame as free (clear bit)\nstatic void frame_clear(uint32_t frame) {\n    frame_bitmap[frame / 32] &= ~(1u << (frame % 32));\n}\n// Test if a frame is used\nstatic int frame_test(uint32_t frame) {\n    return (frame_bitmap[frame / 32] >> (frame % 32)) & 1;\n}\n```\n**Initialization: reserve everything, then free the usable regions.** This is the safer invariant \u2014 start with all frames marked used, then free only what the memory map says is safe. Any region not explicitly freed remains permanently allocated, which prevents you from ever accidentally handing out MMIO space or BIOS ROM:\n```c\nvoid pmm_init(multiboot_info_t *mbi) {\n    // Calculate total frames from highest usable address\n    // (conservatively use 128MB or whatever the machine has)\n    uint64_t highest = 0;\n    for (int i = 0; i < phys_region_count; i++) {\n        uint64_t top = phys_regions[i].base + phys_regions[i].length;\n        if (top > highest) highest = top;\n    }\n    total_frames = (uint32_t)(highest / PAGE_SIZE);\n    // Start: mark ALL frames as used\n    // memset is fine here \u2014 we zeroed BSS, but frame_bitmap may not be in BSS\n    // if it's in .bss it's already zero... but we want all-used (0xFF not 0x00)\n    // So we set all bits to 1:\n    for (uint32_t i = 0; i < total_frames / 32 + 1; i++)\n        frame_bitmap[i] = 0xFFFFFFFF;\n    // Free frames from usable E820 regions\n    for (int i = 0; i < phys_region_count; i++) {\n        if (phys_regions[i].type != 1) continue;  // Skip non-usable\n        uint64_t base = phys_regions[i].base;\n        uint64_t len  = phys_regions[i].length;\n        // Align to 4KB boundaries (round base up, round length down)\n        uint32_t frame_start = (uint32_t)((base + PAGE_SIZE - 1) / PAGE_SIZE);\n        uint32_t frame_end   = (uint32_t)((base + len) / PAGE_SIZE);\n        for (uint32_t f = frame_start; f < frame_end; f++) {\n            frame_clear(f);\n            free_frames++;\n        }\n    }\n    // Re-mark frames occupied by the kernel binary as used\n    // __kernel_start and __kernel_end come from the linker script\n    extern uintptr_t __kernel_start, __kernel_end;\n    uint32_t kstart_frame = (uint32_t)&__kernel_start / PAGE_SIZE;\n    uint32_t kend_frame   = ((uint32_t)&__kernel_end + PAGE_SIZE - 1) / PAGE_SIZE;\n    for (uint32_t f = kstart_frame; f < kend_frame; f++) {\n        if (!frame_test(f)) { frame_set(f); free_frames--; }\n    }\n    // Re-mark frame 0 as used (BIOS data area, NULL pointer protection)\n    if (!frame_test(0)) { frame_set(0); free_frames--; }\n    kprintf(\"[PMM] %u KB total, %u KB free (%u frames)\\n\",\n            total_frames * 4, free_frames * 4, free_frames);\n}\n```\n**Allocation \u2014 first-fit scan:**\n```c\nuint32_t pmm_alloc_frame(void) {\n    // Scan bitmap for first free frame\n    for (uint32_t i = 0; i < total_frames / 32; i++) {\n        if (frame_bitmap[i] == 0xFFFFFFFF) continue; // All used, skip fast\n        // Found a word with at least one free bit\n        for (uint32_t bit = 0; bit < 32; bit++) {\n            if (!((frame_bitmap[i] >> bit) & 1)) {\n                uint32_t frame = i * 32 + bit;\n                frame_set(frame);\n                free_frames--;\n                return frame * PAGE_SIZE;  // Return physical address\n            }\n        }\n    }\n    // Out of memory\n    kprintf(\"[PMM] OUT OF PHYSICAL MEMORY\\n\");\n    for (;;) __asm__ volatile (\"hlt\");\n    return 0;\n}\nvoid pmm_free_frame(uint32_t phys_addr) {\n    uint32_t frame = phys_addr / PAGE_SIZE;\n    if (frame >= total_frames) {\n        kprintf(\"[PMM] free_frame: invalid address 0x%x\\n\", phys_addr);\n        return;\n    }\n    if (!frame_test(frame)) {\n        // Double-free detected \u2014 always a kernel bug\n        kprintf(\"[PMM] DOUBLE FREE: frame 0x%x (addr 0x%x)\\n\", frame, phys_addr);\n        for (;;) __asm__ volatile (\"hlt\");\n    }\n    frame_clear(frame);\n    free_frames++;\n}\n```\n> **Hardware Soul**: The word-level skip (`if (frame_bitmap[i] == 0xFFFFFFFF) continue`) is not just an optimization \u2014 it's essential for the allocator to remain fast as memory fills up. Scanning 32 bits at once is a single comparison and branch. Without it, a fully-allocated system doing a 32KB kernel stack allocation would scan 1 million bits before declaring OOM. With it, you scan at most 32K words (128KB of bitmap), which fits in L2 cache. This is the same principle behind the `__builtin_ctz` (count trailing zeros) optimization: `bit = __builtin_ctz(~frame_bitmap[i])` finds the first free bit in a word in O(1) using a single hardware instruction (BSF \u2014 Bit Scan Forward on x86). Consider using it.\n**Design decision: Bitmap vs Free List**\n| Approach | Allocation | Deallocation | Memory Overhead | Used By |\n|----------|-----------|--------------|-----------------|---------|\n| **Bitmap \u2713** | O(n) first-fit scan | O(1) | 1 bit/frame = 128KB for 4GB | Simple kernels, early allocators |\n| Free list (stack) | O(1) pop | O(1) push | One pointer/free frame | Linux zone allocator (fast path) |\n| Buddy system | O(log n) | O(log n) merge | Low | Linux buddy allocator, jemalloc |\nThe bitmap wins for clarity and double-free detection. Real OS kernels (Linux) use a buddy allocator for physical frames \u2014 it supports efficient allocation of contiguous multi-page blocks and merges adjacent freed blocks to combat fragmentation. For this milestone, the bitmap gives you everything you need.\n---\n## Phase 3: x86 Two-Level Paging \u2014 The Hardware's Point of View\n{{DIAGRAM:diag-m3-two-level-page-table}}\nNow the architecture that makes virtual memory possible. On 32-bit x86, a virtual address is 32 bits wide. The MMU splits it into three fields:\n```\n31          22 21          12 11           0\n+-------------+--------------+-------------+\n| Dir Index   | Table Index  |   Offset    |\n|  10 bits    |   10 bits    |   12 bits   |\n+-------------+--------------+-------------+\n```\n- **Directory Index (bits 31\u201322)**: Which of 1024 entries in the Page Directory to use.\n- **Table Index (bits 21\u201312)**: Which of 1024 entries in the selected Page Table to use.\n- **Offset (bits 11\u20130)**: Byte offset within the 4KB page (2\u00b9\u00b2 = 4096 bytes, matching page size).\n**The walk proceeds in three steps:**\n1. The CPU reads CR3 to get the physical address of the Page Directory.\n2. Using the Directory Index, it reads the Page Directory Entry (PDE) from the Page Directory. The PDE contains the physical address of a Page Table (if present).\n3. Using the Table Index, it reads the Page Table Entry (PTE) from the Page Table. The PTE contains the physical address of the actual page frame.\n4. The final physical address is `PTE.frame_address | virtual_address.offset`.\n**Why two levels?** A single-level page table mapping all 4GB would require 4GB / 4KB = 1M entries \u00d7 4 bytes = 4MB of page table memory \u2014 **per process**. With 100 processes, that's 400MB just for page tables. Two-level paging solves this with **sparsity**: you only need to allocate Page Table pages for the virtual address regions a process actually uses. An address space that uses only 4MB needs one Page Directory + one Page Table = 8KB, not 4MB.\n{{DIAGRAM:diag-m3-pde-pte-bitfield}}\n**Page Directory Entry (PDE) bit layout:**\n```c\ntypedef struct {\n    uint32_t present    : 1;  // Bit 0: 1 = this entry points to a valid page table\n    uint32_t writable   : 1;  // Bit 1: 1 = page table is writable\n    uint32_t user       : 1;  // Bit 2: 1 = accessible from ring 3 (user mode)\n    uint32_t write_thru : 1;  // Bit 3: write-through caching (usually 0)\n    uint32_t cache_dis  : 1;  // Bit 4: cache disable for this PDE (usually 0)\n    uint32_t accessed   : 1;  // Bit 5: set by CPU when PT is accessed\n    uint32_t _reserved  : 1;  // Bit 6: reserved (must be 0)\n    uint32_t page_size  : 1;  // Bit 7: 0 = 4KB pages, 1 = 4MB huge page\n    uint32_t _ignored   : 4;  // Bits 8-11: available for OS use\n    uint32_t frame      : 20; // Bits 12-31: physical address of page table >> 12\n} pde_t;\n```\n**Page Table Entry (PTE) bit layout:**\n```c\ntypedef struct {\n    uint32_t present    : 1;  // Bit 0: 1 = page is in physical memory\n    uint32_t writable   : 1;  // Bit 1: 0 = read-only, 1 = read/write\n    uint32_t user       : 1;  // Bit 2: 0 = kernel-only (ring 0), 1 = user-accessible\n    uint32_t write_thru : 1;  // Bit 3: write-through cache\n    uint32_t cache_dis  : 1;  // Bit 4: cache disable (set for MMIO regions)\n    uint32_t accessed   : 1;  // Bit 5: CPU sets on any read or write\n    uint32_t dirty      : 1;  // Bit 6: CPU sets on write\n    uint32_t _reserved  : 1;  // Bit 7: reserved (must be 0)\n    uint32_t global     : 1;  // Bit 8: don't flush from TLB on CR3 reload (for kernel pages)\n    uint32_t _avail     : 3;  // Bits 9-11: available for OS use\n    uint32_t frame      : 20; // Bits 12-31: physical address of mapped page >> 12\n} pte_t;\n```\nIn practice, working with bitfields can lead to surprising compiler-generated code. Most kernel developers use `uint32_t` with explicit masks and shifts:\n```c\n#define PTE_PRESENT   (1u << 0)\n#define PTE_WRITABLE  (1u << 1)\n#define PTE_USER      (1u << 2)\n#define PTE_CACHE_DIS (1u << 4)\n#define PTE_FRAME(addr)  ((addr) & 0xFFFFF000u)  // Mask off flag bits\ntypedef uint32_t pde_t;\ntypedef uint32_t pte_t;\n// A page directory: 1024 entries \u00d7 4 bytes = 4096 bytes = exactly one page\ntypedef pde_t page_directory_t[1024] __attribute__((aligned(4096)));\n// A page table: 1024 entries \u00d7 4 bytes = 4096 bytes = exactly one page\ntypedef pte_t page_table_t[1024] __attribute__((aligned(4096)));\n```\n**The `__attribute__((aligned(4096)))` is mandatory.** The CPU reads CR3 and treats the value as a physical address of the page directory. The hardware requires this address to be 4KB-aligned (the low 12 bits of CR3 carry flags, not address bits). If your page directory is not 4KB-aligned, CR3 will point to the wrong location and every memory access will use garbage translations.\n---\n## Phase 4: The Address Space Layout \u2014 Choosing Your Map\n{{DIAGRAM:diag-m3-identity-plus-higher-half}}\nBefore writing a single page table entry, you must decide on your virtual address space layout. This decision shapes everything \u2014 your linker script, your page fault handler, your future process isolation strategy.\nFor this kernel, you will use a **higher-half kernel** layout:\n```\nVirtual Address Space (32-bit: 0x00000000 - 0xFFFFFFFF)\n0x00000000 - 0x003FFFFF   Identity-mapped low memory (first 4MB)\n                           Covers: BIOS data area, VGA buffer (0xB8000),\n                           conventional memory, bootloader stack\n0x00400000 - 0xBFFFFFFF   Available for user processes (future)\n0xC0000000 - 0xC03FFFFF   Higher-half kernel (kernel code + data)\n                           Physical: 0x00100000 - 0x004FFFFF\n                           Linked at: 0xC0100000 (VMA)\n                           Loaded at: 0x00100000 (LMA)\n0xC0400000 - 0xCFFFFFFF   Kernel heap (kmalloc arena)\n                           Physical: allocated on demand by pmm_alloc_frame\n0xD0000000 - 0xFFBFFFFF   Reserved for future kernel use\n0xFFC00000 - 0xFFFFFFFF   Recursive page directory mapping (optional)\n```\n**Why `0xC0000000` (3GB)?** This is the classical 3GB/1GB split used by Linux 2.x kernels: user space gets the lower 3GB, the kernel occupies the upper 1GB of every process's address space. It is a trade-off \u2014 user processes are limited to 3GB virtual address space, but the kernel always has its full mapping present in every address space, eliminating TLB flushes on system calls. Linux x86 64-bit solved this by having an astronomically large 64-bit address space (128TB for user, 128TB for kernel), but on 32-bit the split is a genuine constraint.\n**The VMA/LMA disconnect.** Your linker script must link the kernel at `0xC0100000` (the virtual address it will use after paging is enabled), but the bootloader loaded it at `0x00100000` (physical). The linker emits code with virtual addresses \u2014 function pointers, global variable accesses, `call` instructions \u2014 all resolved to addresses above `0xC0000000`. Before paging is enabled, these addresses are nonsensical. Your pre-paging code (the boot stub that sets up the initial page directory) must work entirely with physical addresses and use position-independent techniques or explicit `- 0xC0000000` adjustments.\nUpdate your linker script:\n```ld\nENTRY(kernel_entry)\nKERNEL_VIRT_BASE = 0xC0000000;\nKERNEL_PHYS_BASE = 0x00100000;\nSECTIONS {\n    . = KERNEL_VIRT_BASE + KERNEL_PHYS_BASE;  /* 0xC0100000 */\n    .text ALIGN(4096) : AT(ADDR(.text) - KERNEL_VIRT_BASE) {\n        *(.multiboot)\n        *(.text)\n        *(.text.*)\n    }\n    .rodata ALIGN(4096) : AT(ADDR(.rodata) - KERNEL_VIRT_BASE) {\n        *(.rodata)\n    }\n    .data ALIGN(4096) : AT(ADDR(.data) - KERNEL_VIRT_BASE) {\n        *(.data)\n    }\n    .bss ALIGN(4096) : AT(ADDR(.bss) - KERNEL_VIRT_BASE) {\n        __bss_start = .;\n        *(.bss)\n        *(COMMON)\n        __bss_end = .;\n    }\n    __kernel_end = .;\n    __kernel_phys_end = __kernel_end - KERNEL_VIRT_BASE;\n}\n```\nThe `AT(...)` directive specifies the LMA (load address) as VMA minus the base offset, so the binary sections are stored at physical addresses while all symbol references use virtual addresses. `objcopy -O binary` strips the ELF headers and produces a flat binary at the physical addresses, which your bootloader loads to `0x100000`.\n---\n## Phase 5: Building the Initial Page Tables\n{{DIAGRAM:diag-m3-paging-enable-moment}}\nThis is the most delicate code you will write. The goal: construct page tables that map both the identity region (low 4MB, physical = virtual) and the higher-half region (kernel at `0xC0100000` \u2192 physical `0x00100000`), then atomically enable paging with CR3 and CR0.PG.\n**The boot page directory and tables must be statically allocated** \u2014 you cannot use `pmm_alloc_frame` before paging is enabled, because the PMM initialization itself requires knowing the memory map, which requires some kernel code to have run. The bootstrap page tables are `static` arrays in the `.data` or `.bss` section, placed at known physical addresses by the linker.\n```c\n// boot_paging.c \u2014 compiled and linked at virtual addresses,\n// but this code runs BEFORE paging is on, so we carefully\n// use physical addresses for the CR3 load.\n// Static page directory and two static page tables:\n// - pt_low:  identity-maps physical 0x00000000 - 0x003FFFFF (first 4MB)  \n// - pt_high: maps virtual 0xC0000000 - 0xC03FFFFF -> physical 0x00000000 - 0x003FFFFF\nstatic page_directory_t boot_pd  __attribute__((aligned(4096)));\nstatic page_table_t     pt_low   __attribute__((aligned(4096)));\nstatic page_table_t     pt_high  __attribute__((aligned(4096)));\n// Macro to convert a virtual address (after linking) to its physical address\n// before paging is enabled\n#define VIRT_TO_PHYS(addr) ((uint32_t)(addr) - 0xC0000000)\nvoid paging_init(void) {\n    // --- Step 1: Fill the low identity-map page table ---\n    // Maps virtual 0x00000000-0x003FFFFF -> physical 0x00000000-0x003FFFFF\n    // Required: this is where execution currently IS. Paging enable would fault\n    // on the very next instruction without this.\n    for (uint32_t i = 0; i < 1024; i++) {\n        pt_low[i] = (i * PAGE_SIZE) | PTE_PRESENT | PTE_WRITABLE;\n    }\n    // --- Step 2: Fill the high kernel page table ---\n    // Maps virtual 0xC0000000-0xC03FFFFF -> physical 0x00000000-0x003FFFFF\n    // Same physical frames as the identity map, different virtual addresses.\n    for (uint32_t i = 0; i < 1024; i++) {\n        pt_high[i] = (i * PAGE_SIZE) | PTE_PRESENT | PTE_WRITABLE;\n    }\n    // --- Step 3: Install page tables in the page directory ---\n    // Index for virtual 0x00000000: (0x00000000 >> 22) = 0\n    boot_pd[0] = VIRT_TO_PHYS((uint32_t)pt_low) | PDE_PRESENT | PDE_WRITABLE;\n    // Index for virtual 0xC0000000: (0xC0000000 >> 22) = 768\n    boot_pd[768] = VIRT_TO_PHYS((uint32_t)pt_high) | PDE_PRESENT | PDE_WRITABLE;\n    // --- Step 4: Load CR3 with the PHYSICAL address of the page directory ---\n    uint32_t pd_phys = VIRT_TO_PHYS((uint32_t)boot_pd);\n    __asm__ volatile (\n        \"mov cr3, %0\"\n        : : \"r\"(pd_phys) : \"memory\"\n    );\n    // --- Step 5: Enable paging by setting CR0.PG (bit 31) ---\n    // After this instruction, the MMU is active.\n    // The NEXT instruction is fetched at a virtual address.\n    // Because of the identity map in boot_pd[0], the physical code\n    // at 0x00100000 is also mapped at virtual 0x00100000 \u2014 so execution\n    // continues seamlessly.\n    uint32_t cr0;\n    __asm__ volatile (\"mov %0, cr0\" : \"=r\"(cr0));\n    cr0 |= (1u << 31);  // Set PG bit\n    __asm__ volatile (\n        \"mov cr0, %0\"\n        : : \"r\"(cr0) : \"memory\"\n    );\n    // --- Step 6: Long jump to higher-half virtual address ---\n    // We are now executing at physical 0x00100000 = virtual 0x00100000\n    // (through the identity map). We want to \"move\" to 0xC0100000.\n    // A far jump achieves this: load EIP with the virtual address.\n    //\n    // In C, we do this by calling a function through a pointer at the high address:\n    void (*kernel_high_entry)(void) = (void (*)(void))0xC0100000 + /* offset to func */;\n    // In practice, this is often done in assembly at the end of the boot stub.\n    // See the assembly snippet below.\n}\n```\n**The moment of transition \u2014 assembly for the high-jump:**\n```nasm\n; After enabling paging in C, control returns here.\n; EIP is currently ~0x00100xxx (physical = virtual via identity map).\n; We need to jump to the higher-half virtual address.\n    ; Calculate the high-half virtual address of 'kernel_main_high'\n    ; by adding the virtual base to the physical address.\n    ; KERNEL_VIRT_BASE = 0xC0000000\n    lea eax, [kernel_main_high]    ; EAX = virtual address from linker (0xC0100xxx)\n    jmp eax                        ; Jump to higher-half virtual address\nkernel_main_high:\n    ; We are now running at 0xC0100xxx.\n    ; The identity map (boot_pd[0]) is still present \u2014 we can remove it now\n    ; by clearing boot_pd[0] and flushing the TLB.\n    ; This makes the NULL page (and low 4MB) unmapped, catching null pointer dereferences.\n    ; Remove identity map\n    extern boot_pd\n    mov dword [boot_pd], 0          ; Clear PDE index 0\n    ; Flush TLB by reloading CR3 (write same value back)\n    mov eax, cr3\n    mov cr3, eax                    ; TLB is now fully flushed\n    ; Set up the kernel stack to the virtual address (linker gave us this)\n    mov esp, kernel_stack_top       ; This is now a virtual address ~0xC01xxxxx\n    ; Call kernel_main with the multiboot info pointer\n    ; Note: EBX was set by GRUB to point to the multiboot_info structure.\n    ; If you preserved it through the paging setup, pass it here.\n    push ebx\n    call kernel_main\n.hang:\n    cli\n    hlt\n    jmp .hang\n```\n> **The identity map removal is a safety mechanism, not ceremony.** After jumping to higher-half virtual addresses, the identity map (virtual 0x00000000 \u2192 physical 0x00000000) is no longer needed. Removing it makes virtual address 0x00000000 unmapped, which means a NULL pointer dereference immediately triggers a page fault \u2014 #PF with CR2=0. Without removing the identity map, a NULL pointer dereference silently reads whatever is at physical address 0, which is BIOS data, and returns garbage. The kernel proceeds as if nothing is wrong and crashes elsewhere, hours or thousands of instructions later. Removing the identity map converts a silent memory corruption into an immediate, diagnosable page fault. Always remove it.\n---\n## Phase 6: The TLB \u2014 Your Most Dangerous Invisible State\n{{DIAGRAM:diag-m3-tlb-flush-scenarios}}\nThe TLB deserves its own section because TLB-related bugs are among the most pernicious in operating system development. They are non-deterministic, they can lie dormant through extensive testing, and they manifest in seemingly unrelated code.\n**What the TLB caches:** Each TLB entry stores one virtual\u2192physical translation, plus the permission bits from that PTE (present, writable, user). When you modify a PTE \u2014 changing permissions, changing the mapped frame, or marking a page not present \u2014 the TLB may still hold the old translation. The CPU will use the stale entry until it is evicted, potentially allowing accesses that should fault, or faulting on accesses that should succeed.\n**When you must flush the TLB:**\n- After mapping a previously unmapped page (new PTE, present=0 \u2192 present=1): No flush needed for the newly mapped page \u2014 there's no stale entry because the page was never mapped. But if you install a new page table (new PDE) while old PDEs point to page tables you've modified, flush anyway to be safe.\n- After unmapping a page (present=1 \u2192 present=0): **Always flush.** Otherwise accesses may succeed using the stale TLB entry even though the page is logically unmapped.\n- After changing a page's permissions (e.g., read-only \u2192 writable or vice versa): **Always flush.** The TLB cached the old permissions.\n- After changing a page's frame (remapping a virtual address to a different physical frame): **Always flush.** The TLB has the wrong physical address.\n**The two flush mechanisms:**\n```c\n// Flush a single page from the TLB: use INVLPG\n// This is efficient \u2014 only evicts the one translation.\nstatic inline void tlb_flush_page(uint32_t virt_addr) {\n    __asm__ volatile (\"invlpg [%0]\" : : \"r\"(virt_addr) : \"memory\");\n}\n// Flush the entire TLB: reload CR3 with the same value\n// Every user-space translation is evicted. Kernel global pages\n// (with PTE_GLOBAL set) survive this on CPUs with PGE support.\nstatic inline void tlb_flush_all(void) {\n    uint32_t cr3;\n    __asm__ volatile (\"mov %0, cr3\" : \"=r\"(cr3));\n    __asm__ volatile (\"mov cr3, %0\" : : \"r\"(cr3) : \"memory\");\n}\n```\n**Prefer `invlpg` over full CR3 reloads.** A full TLB flush evicts all translations, including hot kernel mappings. Subsequent accesses to kernel code and data all miss the TLB initially, causing a burst of page table walks. On a kernel with 256KB of frequently-executed code, a full flush might evict 64 kernel translations \u2014 each costing two extra memory accesses on the next hit. At 1000 context switches per second with a full flush each time, this is measurable overhead. Use `invlpg` when you know which page changed. Use full CR3 reload only when switching address spaces (context switch to a different process).\n> **Knowledge Cascade \u2014 TLB Shootdown and Multicore Scalability**: On a single CPU, TLB management is simple \u2014 you flush your own TLB. On a multicore system, each CPU core has its own private TLB. When the kernel modifies a page table entry on CPU 0, CPU 1's TLB may still hold the stale translation. The kernel must send an **Inter-Processor Interrupt (IPI)** to every other CPU, instructing them to invalidate the affected TLB entries. This is called a **TLB shootdown**, and it is a notorious performance bottleneck. A Linux `munmap(2)` call that frees a large mapping must IPI every other core that might have that mapping in its TLB \u2014 and wait for all of them to acknowledge before completing. On a 64-core NUMA system, this serialization can cost microseconds. Database systems like ScyllaDB and FoundationDB are designed with this in mind: they minimize dynamic memory mapping/unmapping during steady-state operation to avoid TLB shootdown overhead. The HugePage (2MB/1GB pages) optimization partially addresses this \u2014 fewer TLB entries needed means less shootdown cost \u2014 which is why databases and JVMs benefit dramatically from huge pages. Your single-core kernel avoids all of this, but knowing it exists explains why mmap/munmap are \"expensive\" syscalls in production.\n---\n## Phase 7: The Dynamic Virtual Memory Mapper\nOnce the initial page tables are set up and paging is enabled, you need a function that can map any virtual address to any physical frame on demand. This is the engine your heap allocator (and eventually your process loader) will use:\n```c\n// Map a single virtual address to a physical frame\n// Creates the page table if it doesn't exist\nvoid paging_map(page_directory_t *pd, uint32_t virt, uint32_t phys, uint32_t flags) {\n    uint32_t pd_index = virt >> 22;           // Bits 31-22\n    uint32_t pt_index = (virt >> 12) & 0x3FF; // Bits 21-12\n    if (!(pd[pd_index] & PTE_PRESENT)) {\n        // Page table for this PDE doesn't exist \u2014 allocate one\n        uint32_t new_pt_phys = pmm_alloc_frame();\n        // Zero the new page table (critical \u2014 old memory may contain garbage)\n        page_table_t *new_pt = (page_table_t *)(new_pt_phys + 0xC0000000);\n        // ^ Map physical to virtual via known identity: in higher-half kernel,\n        //   kernel physical memory is at virt = phys + 0xC0000000\n        for (int i = 0; i < 1024; i++) (*new_pt)[i] = 0;\n        pd[pd_index] = new_pt_phys | PTE_PRESENT | PTE_WRITABLE | (flags & PTE_USER);\n    }\n    // Get the page table virtual address\n    uint32_t pt_phys = PTE_FRAME(pd[pd_index]);\n    page_table_t *pt = (page_table_t *)(pt_phys + 0xC0000000);\n    if ((*pt)[pt_index] & PTE_PRESENT) {\n        // Already mapped \u2014 this is a kernel bug, not a page fault\n        kprintf(\"[PAGING] WARNING: remapping already-mapped virt 0x%x\\n\", virt);\n    }\n    (*pt)[pt_index] = PTE_FRAME(phys) | PTE_PRESENT | flags;\n    // Flush TLB for this page\n    tlb_flush_page(virt);\n}\n// Unmap a virtual address (mark PTE not-present)\nvoid paging_unmap(page_directory_t *pd, uint32_t virt) {\n    uint32_t pd_index = virt >> 22;\n    uint32_t pt_index = (virt >> 12) & 0x3FF;\n    if (!(pd[pd_index] & PTE_PRESENT)) return; // PDE not present, nothing to unmap\n    uint32_t pt_phys = PTE_FRAME(pd[pd_index]);\n    page_table_t *pt = (page_table_t *)(pt_phys + 0xC0000000);\n    (*pt)[pt_index] = 0; // Clear entire entry, including present bit\n    // CRITICAL: flush TLB for this page BEFORE returning\n    // Without this, accesses to virt may continue using stale translation\n    tlb_flush_page(virt);\n}\n```\n**The `+ 0xC0000000` trick.** After paging is enabled with higher-half mapping, physical address `P` is accessible at virtual address `P + 0xC0000000` \u2014 for physical addresses in the kernel range (0x00000000\u20130x003FFFFF mapped to 0xC0000000\u20130xC03FFFFF). This gives you a simple, fast way to get a usable pointer to any newly allocated physical frame, without needing a separate \"map this frame so I can zero it\" step. This only works for physical addresses in your mapped range \u2014 for larger physical memory, you'd need a more sophisticated approach (like maintaining a permanent \"physical memory window\" mapping in high virtual address space). For this milestone, limit your allocations to the first 256MB and this pattern works cleanly.\n---\n## Phase 8: The Page Fault Handler \u2014 From Diagnostic to Demand Paging\n{{DIAGRAM:diag-m3-page-fault-error-code}}\nIn Milestone 2, your page fault handler (#PF, exception 14) printed a message and halted. Now it has a new role: it is the **entry point for all demand paging**, the mechanism by which the OS lazily maps memory only when it's actually accessed.\n**Reading the fault information:**\n```c\nvoid page_fault_handler(struct interrupt_frame *frame) {\n    // CR2 contains the virtual address that caused the fault\n    uint32_t fault_addr;\n    __asm__ volatile (\"mov %0, cr2\" : \"=r\"(fault_addr));\n    // Decode the error code (pushed by CPU before calling handler)\n    int present  = (frame->err_code >> 0) & 1; // 0=not mapped, 1=protection violation\n    int write    = (frame->err_code >> 1) & 1; // 0=read, 1=write\n    int user     = (frame->err_code >> 2) & 1; // 0=kernel, 1=user mode\n    int reserved = (frame->err_code >> 3) & 1; // 1=reserved bit set in PTE (CPU bug?)\n    int ifetch   = (frame->err_code >> 4) & 1; // 1=instruction fetch (NX bit, if enabled)\n    kprintf(\"\\n[PAGE FAULT] at 0x%08x\\n\", fault_addr);\n    kprintf(\"  Access: %s %s from %s mode\\n\",\n            write ? \"Write\" : \"Read\",\n            ifetch ? \"(instruction fetch)\" : \"\",\n            user ? \"user\" : \"kernel\");\n    kprintf(\"  Cause: %s\\n\",\n            present ? \"Protection violation (page present, wrong permissions)\"\n                    : \"Page not mapped (present=0)\");\n    kprintf(\"  EIP=0x%x  EFLAGS=0x%x\\n\", frame->eip, frame->eflags);\n    // For now: if this is a kernel fault, halt\n    if (!user) {\n        kprintf(\"[KERNEL PAGE FAULT] \u2014 halting\\n\");\n        for (;;) __asm__ volatile (\"cli; hlt\");\n    }\n    // TODO (Milestone 4): If user fault, check if it's a valid demand-paged\n    // address, allocate a frame, map it, and return to retry.\n    // For now, kill the process (or halt).\n    kprintf(\"[USER PAGE FAULT] \u2014 process killed (unimplemented)\\n\");\n    for (;;) __asm__ volatile (\"cli; hlt\");\n}\n```\n**The error code is a state machine.** Consider what each combination means:\n- `present=0, write=0, user=0`: Kernel code accessed an unmapped page. This is always a kernel bug \u2014 a null pointer dereference, use-after-free, or uninitialized pointer.\n- `present=0, write=0, user=1`: User code accessed an unmapped page in a read. With demand paging, this is a valid trigger: allocate the page, map it, return. Without demand paging: segfault.\n- `present=1, write=1, user=1`: User code tried to write a read-only page. With copy-on-write (fork): duplicate the frame, remap. Without: segfault.\n- `present=0, write=0, user=0` at CR2=0x00000000: A null pointer dereference (after you removed the identity map). The most common kernel bug.\n> **Knowledge Cascade \u2014 Demand Paging and Copy-on-Write Across Domains**: The page fault handler you just built is the exact mechanism behind `fork()`, lazy allocation in `malloc`, memory-mapped files, and Docker's overlay filesystem. When Linux calls `fork()`, it doesn't copy the parent's memory \u2014 it marks every page in both processes as read-only and sets a copy-on-write flag. When either process writes to any page, a `present=1, write=1` page fault fires. The handler allocates a new frame, copies the page content, remaps the faulting process to the new frame with write permission, and returns. The write succeeds on retry. This means `fork()` is O(1) in the common case where the child immediately calls `exec()` \u2014 no data is ever copied. Docker's overlay filesystem extends this: each container layer is a set of copy-on-write page mappings; unmodified pages are shared at the physical level, modified pages get their own frames. The mechanism is identical to what you built, applied at the filesystem layer. `mmap(2)` for a 1GB file also uses this: no data is read at `mmap` time. Only accessed pages trigger faults that read disk sectors and map them. Your page fault handler is the architectural ancestor of all of this.\n---\n## Phase 9: The Kernel Heap \u2014 kmalloc and kfree\n{{DIAGRAM:diag-m3-kernel-heap-architecture}}\nThe physical frame allocator gives you 4KB chunks. Your kernel code needs arbitrary-sized allocations \u2014 `uint8_t *buf = kmalloc(37)` for a PS/2 command buffer, `process_t *p = kmalloc(sizeof(process_t))` for a new PCB in Milestone 4. You need a heap.\n**The strategy: a simple slab-like allocator backed by the frame allocator.**\nReserve a range of virtual addresses for the kernel heap \u2014 say `0xC0400000`\u2013`0xCFFFFFFF` (252MB of virtual space). The heap manager maintains a pointer to the \"current brk\" (break pointer \u2014 the end of committed heap memory). On each `kmalloc`, if sufficient space is available in the committed region, serve from it. If not, call `pmm_alloc_frame` to get a new physical frame, use `paging_map` to commit it at the next virtual address, and advance the brk.\nFor this milestone, implement a simple **free-list heap allocator**:\n```c\n#define HEAP_START   0xC0400000\n#define HEAP_END     0xCFFF0000\n#define HEAP_MIN_SIZE 4096       // Grow heap in page increments\n// Each allocation has a header immediately before the returned pointer\ntypedef struct heap_block {\n    uint32_t         size;   // Size of data area (not including header)\n    uint32_t         magic;  // Sanity check against corruption\n    uint8_t          used;   // 1 = allocated, 0 = free\n    struct heap_block *next; // Next block in the free/used list\n    struct heap_block *prev;\n} heap_block_t;\n#define HEAP_MAGIC 0xDEADBEEF\nstatic heap_block_t *heap_head = NULL;\nstatic uint32_t heap_brk = HEAP_START;  // Current top of committed heap\n// Extend the heap by at least 'size' bytes (always extends by full pages)\nstatic heap_block_t *heap_extend(uint32_t size) {\n    uint32_t pages_needed = (size + sizeof(heap_block_t) + PAGE_SIZE - 1) / PAGE_SIZE;\n    uint32_t new_virt = heap_brk;\n    for (uint32_t i = 0; i < pages_needed; i++) {\n        if (heap_brk >= HEAP_END) {\n            kprintf(\"[HEAP] Out of virtual heap space\\n\");\n            return NULL;\n        }\n        uint32_t phys = pmm_alloc_frame();\n        paging_map(current_page_dir, heap_brk, phys, PTE_WRITABLE);\n        heap_brk += PAGE_SIZE;\n    }\n    // Initialize new block header at new_virt\n    heap_block_t *block = (heap_block_t *)new_virt;\n    block->size  = pages_needed * PAGE_SIZE - sizeof(heap_block_t);\n    block->magic = HEAP_MAGIC;\n    block->used  = 0;\n    block->next  = NULL;\n    block->prev  = NULL;\n    // Add to end of block list\n    if (!heap_head) {\n        heap_head = block;\n    } else {\n        heap_block_t *last = heap_head;\n        while (last->next) last = last->next;\n        last->next = block;\n        block->prev = last;\n    }\n    return block;\n}\nvoid *kmalloc(uint32_t size) {\n    if (size == 0) return NULL;\n    // Align size to 8 bytes for natural alignment\n    size = (size + 7) & ~7u;\n    // First-fit search for a free block\n    heap_block_t *block = heap_head;\n    while (block) {\n        if (block->magic != HEAP_MAGIC) {\n            kprintf(\"[HEAP] CORRUPTION: bad magic at 0x%x\\n\", (uint32_t)block);\n            for (;;) __asm__ volatile (\"hlt\");\n        }\n        if (!block->used && block->size >= size) {\n            // Found a suitable free block\n            // Split if remainder is large enough to hold a new block + min allocation\n            if (block->size >= size + sizeof(heap_block_t) + 8) {\n                heap_block_t *remainder =\n                    (heap_block_t *)((uint8_t *)(block + 1) + size);\n                remainder->size  = block->size - size - sizeof(heap_block_t);\n                remainder->magic = HEAP_MAGIC;\n                remainder->used  = 0;\n                remainder->next  = block->next;\n                remainder->prev  = block;\n                if (block->next) block->next->prev = remainder;\n                block->next = remainder;\n                block->size = size;\n            }\n            block->used = 1;\n            return (void *)(block + 1); // Return pointer past header\n        }\n        block = block->next;\n    }\n    // No suitable block found \u2014 extend the heap\n    block = heap_extend(size);\n    if (!block) return NULL;\n    return kmalloc(size); // Retry with new block\n}\nvoid kfree(void *ptr) {\n    if (!ptr) return;\n    heap_block_t *block = (heap_block_t *)ptr - 1; // Header is just before ptr\n    if (block->magic != HEAP_MAGIC) {\n        kprintf(\"[HEAP] kfree: bad magic at 0x%x (invalid or double-free?)\\n\",\n                (uint32_t)block);\n        for (;;) __asm__ volatile (\"hlt\");\n    }\n    if (!block->used) {\n        kprintf(\"[HEAP] kfree: double-free detected at 0x%x\\n\", ptr);\n        for (;;) __asm__ volatile (\"hlt\");\n    }\n    block->used = 0;\n    // Coalesce with next block if also free\n    if (block->next && !block->next->used &&\n        block->next->magic == HEAP_MAGIC) {\n        block->size += sizeof(heap_block_t) + block->next->size;\n        block->next  = block->next->next;\n        if (block->next) block->next->prev = block;\n    }\n    // Coalesce with previous block if also free\n    if (block->prev && !block->prev->used &&\n        block->prev->magic == HEAP_MAGIC) {\n        block->prev->size += sizeof(heap_block_t) + block->size;\n        block->prev->next  = block->next;\n        if (block->next) block->next->prev = block->prev;\n    }\n}\n```\n**Memory layout of an allocated block:**\n```\n| heap_block_t header (20 bytes) | user data (size bytes, aligned to 8) |\n^                                 ^\nblock                             ptr returned to caller\n```\n**The magic value `0xDEADBEEF` catches common corruption.** If a buffer overrun writes into the header of the next block, the magic field is corrupted. On the next `kmalloc` or `kfree` that encounters that block, the magic check fires and you get a diagnostic message pointing to the corrupted block's address. This is a lightweight version of what jemalloc calls \"freelist integrity checking\" \u2014 the production version includes cryptographic checksums and red zones around each allocation.\n> **Hardware Soul**: Each `kmalloc` allocation involves a first-fit linear scan of the free list \u2014 O(n) in the number of live allocations. For a kernel with dozens of active allocations, this is perfectly acceptable. At thousands of allocations, it becomes a bottleneck because the free list is an unpredictable, pointer-chasing linked list \u2014 the worst possible memory access pattern for a cache. Every `block->next` dereference risks a cache miss because the next block is at an arbitrary address. This is why production kernel allocators (jemalloc, mimalloc, the Linux slab allocator) use **size-class segregation**: separate free lists for 8B, 16B, 32B, 64B, ... allocations. Allocating 37 bytes rounds up to the 64-byte class, returning from that class's freelist in O(1). The physical locality of same-size objects also improves cache behavior: objects of similar size tend to be used together, so keeping them in the same page maximizes cache line utilization. Mimalloc additionally uses \"mimalloc segments\" \u2014 4MB regions of physically-contiguous memory \u2014 to ensure that all objects from the same thread live in the same NUMA node, eliminating cross-NUMA memory bus latency. Your heap allocator is the slab allocator's humble ancestor.\n> **Knowledge Cascade \u2014 Cache Coloring and Physical Frame Choice**: Your `pmm_alloc_frame` always returns the first available frame. This is correct for correctness, but suboptimal for performance. The physical frame number determines which cache set in the L1/L2 cache the page maps to \u2014 this is called the **cache set index** and is computed from address bits above the cache line offset. If every allocation returns frames with the same lower bits, all allocations compete for the same cache sets \u2014 causing excessive cache evictions even when total working set fits in cache. **Cache coloring** is the technique of varying the frame number's lower bits across successive allocations so that different objects map to different cache sets. jemalloc implements a version of this through its arena design: each arena's `sbrk`-extended memory has a different alignment, spreading allocations across the 8-way L1 cache. PostgreSQL uses huge pages specifically to reduce the number of TLB entries needed for its buffer pool, since fewer but larger pages occupy fewer TLB entries. These are the same insights your bitmap allocator will need if you push performance \u2014 but for this milestone, first-fit correctness is the goal.\n---\n## Phase 10: Complete Initialization Sequence\n{{DIAGRAM:diag-m3-virtual-address-space-layout}}\nHere is the complete, correct initialization order, with the reason each step must precede the next:\n```c\nvoid kernel_main(multiboot_info_t *mbi) {\n    // NOTE: We are now running at virtual address 0xC0100xxx\n    // The identity map has been removed. NULL dereference = page fault.\n    // 1. Output subsystem (already initialized in boot stub or reinit here)\n    vga_clear();\n    serial_init();\n    kprintf(\"=== Kernel Milestone 3: Memory Management ===\\n\");\n    // 2. GDT and IDT from Milestones 1-2\n    gdt_init();\n    idt_init();\n    // 3. Parse the physical memory map\n    //    Must happen before PMM init, which needs to know memory regions\n    mmap_parse(mbi);\n    // 4. Initialize the physical frame allocator\n    //    Must happen before any paging_map calls\n    pmm_init(mbi);\n    // 5. Paging is already enabled (done in boot stub)\n    //    Install the full page fault handler (which now has PMM available)\n    idt_set_gate(14, (uint32_t)isr_14, 0x08, 0x8E);\n    // 6. Initialize the kernel heap\n    //    Must happen after PMM (heap uses pmm_alloc_frame) and\n    //    after paging (heap uses paging_map)\n    heap_init();  // Sets heap_brk = HEAP_START, heap_head = NULL\n    // 7. PIC and timer from Milestone 2\n    pic_remap(0x20, 0x28);\n    pit_init(100);\n    // 8. Enable interrupts\n    __asm__ volatile (\"sti\");\n    // 9. Test the memory subsystem\n    kprintf(\"\\n--- Memory Subsystem Tests ---\\n\");\n    // Test 1: Basic allocation\n    void *a = kmalloc(64);\n    void *b = kmalloc(128);\n    kprintf(\"kmalloc(64)  = 0x%x\\n\", a);\n    kprintf(\"kmalloc(128) = 0x%x\\n\", b);\n    // Test 2: Write to allocations (touches physical frames \u2014 would page fault if broken)\n    __builtin_memset(a, 0xAA, 64);\n    __builtin_memset(b, 0xBB, 128);\n    kprintf(\"Memory write: OK\\n\");\n    // Test 3: Free and reallocate\n    kfree(a);\n    void *c = kmalloc(32);  // Should reuse a's block (it's larger, will be split)\n    kprintf(\"After kfree+kmalloc(32): c=0x%x (expect near a=0x%x)\\n\", c, a);\n    // Test 4: Physical frame allocator stats\n    kprintf(\"\\nPMM: %u KB free\\n\", pmm_free_frames() * 4);\n    // Test 5: Trigger a page fault deliberately (if you're brave)\n    // volatile uint32_t x = *(volatile uint32_t *)0x00000000;  // NULL deref\n    // This should print a page fault diagnostic and halt \u2014 do NOT uncomment yet\n    kprintf(\"\\n[OK] Memory management initialized.\\n\");\n    for (;;) __asm__ volatile (\"hlt\");\n}\n```\n---\n## Debugging Memory Management: Survival Guide\nMemory bugs in kernel development manifest as either immediate crashes (page faults, triple faults) or silent corruption (wrong data, hung system) depending on where in the initialization sequence the error occurs.\n**QEMU monitor commands for memory debugging:**\n```\n(QEMU) info mem           # Show all virtual memory mappings\n(QEMU) info tlb           # Dump TLB contents\n(QEMU) xp /10x 0x100000  # Examine 10 words at physical 0x100000\n(QEMU) x  /10x 0xC0100000 # Examine 10 words at virtual 0xC0100000 (paging must be on)\n(QEMU) info pg            # Print the page tables (extremely useful)\n```\n**GDB with paging enabled:**\n```bash\n(gdb) target remote :1234\n(gdb) set architecture i386\n# After paging is enabled, GDB uses virtual addresses:\n(gdb) x/10x 0xC0100000    # Examine kernel code\n(gdb) p boot_pd[768]      # Inspect PDE for 0xC0000000 region\n(gdb) p *(page_table_t*)0xC0004000  # Inspect a page table\n```\n**Common failures and diagnoses:**\n| Symptom | Diagnosis |\n|---------|-----------|\n| Triple fault immediately on `mov cr0, eax` (enabling PG) | Identity map missing in page directory; instruction after CR0 write faults |\n| Triple fault on the `jmp` to higher-half address | PDE 768 missing or incorrect physical address in PTE_FRAME |\n| `kmalloc` returns 0 or causes fault | PMM ran out of frames; or `paging_map` failed to commit the heap page |\n| Double-free detected immediately at kernel start | BSS not zeroed; heap `used` field contains garbage; `heap_head` uninitialized |\n| Page fault at CR2=0xDEADBEEF or similar | Freed pointer used after free; magic value overwritten user data |\n| Page fault at CR2 = VGA address (0xB8000) | Identity map removed before VGA access, or VGA not identity-mapped |\n| Everything works in debug build, fails in optimized | `volatile` missing from MMIO pointers; compiler reordered CR3/CR0 writes without `\"memory\"` barrier |\n**The `\"memory\"` clobber in inline assembly** (`__asm__ volatile (\"...\" : : : \"memory\")`) tells the compiler that this instruction may read or write any memory location. It prevents the compiler from reordering memory accesses across the instruction barrier. This is critical for CR3 and CR0 writes \u2014 the compiler must not reorder page table modifications to happen after CR3 is loaded, or the CPU walks a partially-constructed table.\n---\n## System Awareness: Where You Now Stand\n\n![OS Kernel \u2014 Satellite System Map](./diagrams/diag-satellite-os-map.svg)\n\nAfter this milestone, your kernel has fundamentally changed character. It no longer lives in a flat physical address space \u2014 it inhabits an isolated, structured virtual space that the hardware enforces.\n**What you have built:**\n- A physical memory map parser that classifies every byte of physical RAM\n- A bitmap allocator with double-free detection that hands out 4KB frames\n- Two-level page tables that implement both identity mapping and higher-half kernel mapping\n- The paging enable sequence with the precise identity\u2192higher-half handoff\n- TLB management with `invlpg` for targeted flushes\n- A page fault handler that decodes CR2 and the error code\n- A kernel heap allocator with coalescing and corruption detection\n**What connects forward to Milestone 4:**\nThe page directory you created is the first **process address space**. When Milestone 4 introduces processes, each process will have its own page directory. The kernel mapping (PDE 768) will be replicated into every process's page directory \u2014 so the kernel is always accessible for system calls and interrupts \u2014 while the user region (PDEs 0\u2013767) will be unique per process. The page fault handler's `present=0` case will become the demand-pager, and the `present=1, write=1` case will implement copy-on-write for `fork()`.\nThe `kmalloc` you built will immediately serve Milestone 4's first need: allocating `process_t` structures and kernel stacks for new processes.\n**What did not exist before that now does:** Memory isolation. Write to a page mapped read-only and you get a fault. Access an unmapped address and you get a fault. NULL pointer dereferences are caught. The kernel has spatial boundaries that the hardware enforces.\n---\n## Knowledge Cascade\n**1. Higher-Half Kernel and KASLR.** Your kernel maps at `0xC0000000` \u2014 a fixed, known virtual address. This predictability is a security vulnerability: an attacker who knows the kernel is always at `0xC0000000` can use this in exploits (kernel ROP chains, return-to-kernel attacks). **KASLR (Kernel Address Space Layout Randomization)** randomizes this base address at boot time, chosen from a range of possible positions. The mechanism is identical to what you built \u2014 the linker script is parameterized with a base address, and the boot stub sets up page tables using the randomly chosen base. Linux implements KASLR since 3.14; macOS kASLR since 10.8. The same principle applies to user space \u2014 ASLR randomizes the base addresses of the stack, heap, and libraries, defeating hardcoded address attacks. Your understanding of how page tables implement a specific mapping is the prerequisite for understanding how randomizing those mappings improves security.\n**2. Demand Paging and Container Copy-on-Write.** The page fault handler you built \u2014 specifically the path that fires when a not-present page is accessed \u2014 is the mechanism that makes every major memory management optimization possible. `malloc` in modern libc doesn't call `mmap` and touch every allocated byte; it maps virtual pages and lets page faults commit physical frames only as data is written. `fork()` uses the same mechanism via copy-on-write to give a child process a read-only snapshot of the parent's address space in O(1) time. Docker's overlay filesystem \u2014 the mechanism that lets containers share base layers \u2014 is copy-on-write at the filesystem layer using the same principle: shared pages are read-only; the first write to a shared block allocates a new copy. Your page fault handler is architecturally the common ancestor of all of these.\n**3. Hardware Page Table Walkers \u2014 Beyond the CPU.** The CPU's MMU autonomously walking your `page_directory_t` / `page_table_t` data structures is a specific instance of a broader hardware pattern: **hardware-walked in-memory descriptor rings**. GPU texture samplers walk texture descriptor tables to fetch texel data for shader cores \u2014 the format is different but the principle is identical: software prepares a structured table, hardware reads it autonomously. The **IOMMU** (Input/Output Memory Management Unit) provides DMA isolation: it sits between PCIe devices and physical memory, using page tables to restrict what addresses a NIC or GPU can DMA-read/write, preventing a compromised device from reading kernel memory. Network cards use descriptor rings (TX and RX queues) that are hardware-walked by the NIC DMA engine to send and receive packets without CPU involvement. In all cases: software defines a structure in memory; hardware reads and walks it autonomously; the structure format is a hardware contract, not a software choice. Your x86 page table is the most fundamental example.\n**4. jemalloc, mimalloc, and Cache-Aware Allocation.** Your `kmalloc` is a correct but naive free-list allocator. Its O(n) scan and pointer-chasing access pattern are the exact pathologies that jemalloc and mimalloc were designed to eliminate. jemalloc (created for FreeBSD, adopted by Firefox and Meta) uses size-class segregation with per-thread caches to achieve O(1) allocation with minimal lock contention. mimalloc (Microsoft Research) adds \"mimalloc segments\" \u2014 large, physically-contiguous arenas that improve TLB coverage and enable branch-prediction-friendly allocation patterns. The key insight both share: **physical locality matters more than logical simplicity**. Objects used together should live on the same cache lines; objects of similar size should live in the same page; all allocations from a thread should live in the same NUMA node. Your bitmap allocator's na\u00efve first-fit is where the optimization journey starts.\n**5. The TLB and NUMA-Aware Database Design.** TLB misses are invisible \u2014 they don't cause explicit faults, they just add latency to memory accesses. But on large in-memory databases (Redis, ScyllaDB, MemSQL), TLB pressure is measurable and optimizable. ScyllaDB pins each shard to a NUMA node and uses huge pages (2MB) for its memory pool: 2MB pages require 512\u00d7 fewer TLB entries than 4KB pages for the same total memory. With a 1536-entry L2 TLB and 4KB pages, ScyllaDB's 64GB shard would require 16M TLB entries \u2014 an impossible load. With 2MB huge pages, it needs only 32K. The TLB becomes sufficient, and memory accesses stop incurring translation misses. Understanding that the TLB has a finite capacity \u2014 and that every allocated page consumes one entry \u2014 is the insight behind this optimization, and you now understand why from silicon level up.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-os-m4 -->\n<!-- MS_ID: build-os-m4 -->\n# Milestone 4: Processes and Preemptive Scheduling\n\n![OS Kernel \u2014 Satellite System Map](./diagrams/diag-satellite-os-map.svg)\n\nEvery mechanism you built in the previous three milestones was preparation for this moment. The GDT established privilege rings. The IDT gave you hardware interrupt handling. The page tables gave each address space its own identity. Now you will combine all of it into the one abstraction that defines what an operating system actually *is*: the **process**.\nBut before you write a single line of code, you need to confront the deepest conceptual challenge of this milestone \u2014 one that trips up nearly every developer who approaches it with intuitions borrowed from userspace programming.\n---\n## The Revelation: A Context Switch Is Not What You Think It Is\nHere is what most developers imagine when they picture a context switch. They think of it like saving and loading a game: you serialize the \"state\" of one process into some data structure, then deserialize another process's state and resume it. Clean, well-defined, like swapping JSON objects. One process's record is written; another is read; execution continues.\n**This model is wrong in a way that will cause you to write broken code if you rely on it.**\nA context switch is a controlled explosion of paradoxes. The function that performs the switch begins its execution *as the old process* and returns *as a completely different process* \u2014 the identity of \"who is running\" changes inside a single function call. Specifically, the identity changes at the moment you swap the stack pointer (ESP). Before the swap, every push and pop touches the old process's kernel stack. After the swap, they touch the new process's kernel stack. This means you can push five registers onto what is unambiguously Process A's stack, swap ESP, and then pop five registers from what is unambiguously Process B's stack \u2014 and this is not a bug. This is the entire mechanism working correctly.\nThink about what this means concretely. The context switch function has a local variable for `current_process` and `next_process`. It calls `context_switch(next)`. Inside that function, it saves registers to `current->context.esp`, then loads `next->context.esp` into ESP. The CPU is now on next's stack. The function then does `ret` \u2014 which pops the return address from next's stack (a return address that was pushed there the last time *next* was switched out). The function returns \u2014 to *next's* suspended call site, not to the caller that invoked it for the current process.\nThe caller that invoked `context_switch(next)` for the current process will eventually see a return \u2014 but only the next time the scheduler decides to run the current process again. At that point, control will reappear at the instruction after the `context_switch` call, but ESP will be back on the current process's stack, and all of the current process's registers will be restored. From the perspective of the current process, `context_switch` just returned (perhaps thousands of timer ticks later) as if nothing happened.\nThe TSS (Task State Segment) adds another layer of strangeness. When a ring-3 user process is interrupted by a hardware interrupt, the CPU must switch to a kernel stack to run the interrupt handler. But at the moment the interrupt fires, the CPU is executing user code \u2014 it has no idea which kernel stack to use. This is precisely what the TSS is for: a hardware structure that the CPU reads during a privilege transition to find the kernel stack pointer. The field it reads is `SS0:ESP0` \u2014 the kernel stack segment and pointer.\nThe brutal implication: **the TSS must be updated on every context switch.** When you switch from Process A to Process B, you must update the TSS's `ESP0` field to point to Process B's kernel stack top. If you don't, the next interrupt while Process B is running will have the CPU switch to Process A's kernel stack \u2014 corrupting whatever state A had saved there. The system will not crash immediately. It will corrupt data silently and crash unpredictably later. This category of bug is among the hardest to diagnose in operating system development.\nThese three paradoxes \u2014 the identity-change mid-function, the push-on-one-stack-pop-from-another, the TSS ESP0 update requirement \u2014 are what make context switching genuinely difficult to reason about. Once you internalize them, the implementation becomes clear. Let's build it.\n---\n## Phase 1: Designing the Process Control Block\n{{DIAGRAM:diag-m4-pcb-structure-layout}}\nThe Process Control Block (PCB) is the kernel's complete record of a process. Everything the kernel needs to suspend a process and later resume it as if nothing happened lives in the PCB. Every field has a precise reason for existing.\n```c\n// process.h\n#include <stdint.h>\n// Process states \u2014 the nodes of the state machine\ntypedef enum {\n    PROCESS_READY    = 0,  // In the run queue, waiting for CPU time\n    PROCESS_RUNNING  = 1,  // Currently executing on the CPU\n    PROCESS_BLOCKED  = 2,  // Waiting for an event (I/O, sleep, etc.)\n    PROCESS_DEAD     = 3,  // Exited; slot can be reclaimed\n} process_state_t;\n// The saved CPU register context \u2014 exactly what we need to restart a process\n// This layout must match the order of pushes/pops in context_switch.asm\ntypedef struct {\n    uint32_t edi;       // General purpose registers (saved by software)\n    uint32_t esi;\n    uint32_t ebx;\n    uint32_t ebp;       // Frame pointer\n    uint32_t esp;       // Stack pointer \u2014 the linchpin of context switching\n    uint32_t eip;       // Instruction pointer \u2014 where to resume execution\n    uint32_t eflags;    // CPU flags \u2014 including the interrupt enable flag (IF)\n} cpu_context_t;\n// Memory layout of cpu_context_t:\n// Offset  0: edi    (4 bytes)\n// Offset  4: esi    (4 bytes)\n// Offset  8: ebx    (4 bytes)\n// Offset 12: ebp    (4 bytes)\n// Offset 16: esp    (4 bytes)  \u2190 THE PIVOT POINT\n// Offset 20: eip    (4 bytes)\n// Offset 24: eflags (4 bytes)\n// Total: 28 bytes\n#define KERNEL_STACK_SIZE  8192   // 8 KB per process kernel stack\n#define MAX_PROCESSES      64\ntypedef struct process {\n    // Identity\n    uint32_t         pid;           // Process ID (unique, monotonically increasing)\n    char             name[32];      // Human-readable name for debugging\n    // Scheduler state\n    process_state_t  state;         // Current state in the process lifecycle\n    // Saved CPU state (valid only when process is not RUNNING)\n    cpu_context_t    context;       // Register snapshot from last context switch\n    // Memory management\n    uint32_t        *page_directory; // Physical address of this process's page directory\n                                     // Loaded into CR3 on context switch\n    // Kernel stack \u2014 used for interrupt handling and system calls when this process\n    // is the current process. The kernel runs on this stack, not the user stack,\n    // during privilege transitions.\n    uint8_t         *kernel_stack;      // Pointer to bottom of kernel stack allocation\n    uint32_t         kernel_stack_top;  // ESP0 value for TSS \u2014 top of kernel stack\n    // User-mode state (only meaningful for ring-3 processes)\n    uint32_t         user_esp;      // User-mode stack pointer (saved on interrupt entry)\n    // Scheduling metadata\n    uint32_t         ticks_remaining; // Remaining time slice (for future priority scheduler)\n    uint32_t         total_ticks;     // Total CPU ticks consumed (for accounting)\n    // Linked list for run queue\n    struct process  *next;          // Next process in the circular ready queue\n} process_t;\n// The run queue: a circular singly-linked list of READY processes\nextern process_t *current_process;\nextern process_t *process_list_head;\n```\n**Why save EIP separately?** When a process is running normally and gets preempted by a timer interrupt, the CPU automatically pushes EIP (and CS, EFLAGS) onto the kernel stack as part of the interrupt entry sequence. The context switch happens *inside the interrupt handler*, so the EIP that needs to be saved is the EIP of the interrupted instruction \u2014 already on the kernel stack. For the first time a process is ever scheduled, though, we need to *fabricate* an initial context where EIP points to the process's entry function. We do this by initializing the EIP field manually when creating the process. This asymmetry \u2014 EIP is sometimes saved by hardware (real interruption) and sometimes set by software (first scheduling) \u2014 is elegantly handled by how we initialize the kernel stack, which we'll cover shortly.\n**Why `page_directory` stores a physical address?** CR3 must contain a physical address \u2014 the MMU reads it before the TLB has any valid entries for the new address space. If you stored the virtual address of the page directory and wrote it to CR3, you'd be loading a virtual address into a register that expects physical. The result is a page fault on the very first memory access the MMU attempts. Always track physical addresses for hardware registers.\n**Why `kernel_stack_top` as a separate field?** The top of the kernel stack changes as the stack is used (ESP moves down). But the TSS `ESP0` should always be the *initial* top \u2014 the empty, unused value \u2014 so the CPU knows where the fresh kernel stack begins for the next interrupt. If the process is currently blocked in kernel code (e.g., waiting for keyboard input), the actual ESP is deep in the stack, but TSS ESP0 should still be the original top. This is the value we update the TSS with on every switch.\n---\n## Phase 2: Understanding Hardware Stack Switching \u2014 The TSS\n{{DIAGRAM:diag-m4-tss-structure}}\nThe Task State Segment is an x86 hardware structure that has been part of the architecture since the 80286. Intel originally designed it to support full hardware multitasking \u2014 the CPU would automatically save and restore the entire CPU state on a \"task switch\" instruction. Modern operating systems don't use hardware task switching (it's too slow and inflexible), but the TSS is still required for one specific purpose that cannot be avoided: **telling the CPU which kernel stack to use when a ring-3 process is interrupted**.\nHere is precisely what happens when a timer interrupt fires while a user-mode process is running:\n1. The CPU detects the interrupt signal on the INTR pin.\n2. The CPU is currently executing at ring 3 (user mode), with ESP pointing into the user process's stack.\n3. Before any interrupt handler code can run, the CPU must switch to the kernel stack \u2014 because running interrupt handlers on the user's stack would let user code observe or manipulate kernel state.\n4. **The CPU reads the TSS.** Specifically, it reads the `SS0` and `ESP0` fields from the TSS. These give it the kernel stack segment selector and the kernel stack pointer to switch to.\n5. The CPU pushes SS (user stack segment), ESP (user stack pointer), EFLAGS, CS, and EIP onto the *kernel* stack (at the address it just read from the TSS).\n6. Now the kernel stack has the user state saved on it, and the CPU is running at ring 0 with ESP pointing into the kernel stack.\nIf the TSS `ESP0` is wrong \u2014 pointing to the old process's kernel stack \u2014 the CPU pushes the interrupt frame there, corrupting whatever that process had stored. The wrong stack is now used for the interrupt handler, EFLAGS gets corrupted, and the system exhibits mysterious behavior. This is why TSS `ESP0` **must** be updated on every context switch.\nThe TSS structure for 32-bit protected mode:\n```c\n// tss.h\n// The Task State Segment \u2014 hardware-defined structure, fields at fixed offsets\n// Only SS0 and ESP0 are used by modern kernels (software task switching)\n// All other fields are zeroed and ignored\ntypedef struct {\n    uint32_t prev_tss;   // Offset 0:  Link to previous TSS (unused, 0)\n    uint32_t esp0;       // Offset 4:  Kernel stack pointer \u2190 THE KEY FIELD\n    uint32_t ss0;        // Offset 8:  Kernel stack segment (0x10 = kernel data)\n    uint32_t esp1;       // Offset 12: Ring 1 stack (unused, 0)\n    uint32_t ss1;        // Offset 16: Ring 1 stack segment (unused, 0)\n    uint32_t esp2;       // Offset 20: Ring 2 stack (unused, 0)\n    uint32_t ss2;        // Offset 24: Ring 2 stack segment (unused, 0)\n    uint32_t cr3;        // Offset 28: CR3 for hardware task switch (we handle this)\n    uint32_t eip;        // Offset 32: EIP for hardware task switch (unused)\n    uint32_t eflags;     // Offset 36: (unused)\n    uint32_t eax;        // Offset 40: (unused)\n    uint32_t ecx;        // Offset 44: (unused)\n    uint32_t edx;        // Offset 48: (unused)\n    uint32_t ebx;        // Offset 52: (unused)\n    uint32_t esp;        // Offset 56: (unused)\n    uint32_t ebp;        // Offset 60: (unused)\n    uint32_t esi;        // Offset 64: (unused)\n    uint32_t edi;        // Offset 68: (unused)\n    uint16_t es;         // Offset 72: (unused)\n    uint16_t _pad0;\n    uint16_t cs;         // Offset 76: (unused)\n    uint16_t _pad1;\n    uint16_t ss;         // Offset 80: (unused)\n    uint16_t _pad2;\n    uint16_t ds;         // Offset 84: (unused)\n    uint16_t _pad3;\n    uint16_t fs;         // Offset 88: (unused)\n    uint16_t _pad4;\n    uint16_t gs;         // Offset 92: (unused)\n    uint16_t _pad5;\n    uint16_t ldt;        // Offset 96: LDT selector (unused, 0)\n    uint16_t _pad6;\n    uint16_t trap;       // Offset 100: trap on task switch (0)\n    uint16_t iomap_base; // Offset 102: I/O permission bitmap base (0xFFFF = disabled)\n} __attribute__((packed)) tss_t;\nstatic tss_t kernel_tss;\n// Initialize the TSS and register it in the GDT\nvoid tss_init(void) {\n    // Zero everything \u2014 most fields are unused in software task switching\n    __builtin_memset(&kernel_tss, 0, sizeof(tss_t));\n    // The kernel uses selector 0x10 (GDT entry 2) as the data segment\n    kernel_tss.ss0       = 0x10;\n    // esp0 will be set per-process on each context switch \u2014 leave 0 for now\n    kernel_tss.esp0      = 0;\n    // I/O permission bitmap: set base to just past the TSS limit \u2192 no I/O allowed\n    // from ring 3 by default (only ring 0 can use in/out instructions)\n    kernel_tss.iomap_base = sizeof(tss_t);\n    // Register the TSS descriptor in the GDT (GDT entry 5, selector 0x28)\n    // The TSS descriptor is a special \"system\" descriptor (type = 0x89 for 32-bit TSS)\n    // Unlike code/data segments, the base is the actual address of the TSS structure\n    uint32_t base  = (uint32_t)&kernel_tss;\n    uint32_t limit = sizeof(tss_t) - 1;\n    gdt_set_tss_entry(5, base, limit); // You'll add entry 5 to the GDT\n    // Load the TSS selector into the TR (Task Register)\n    // The selector 0x28 with RPL=0: (5 << 3) | 0 = 0x28\n    // The '| 3' is NOT needed here \u2014 the TR is a privileged register\n    __asm__ volatile (\"ltr %0\" : : \"r\"((uint16_t)0x28));\n}\n// Called on every context switch to update ESP0\nvoid tss_set_kernel_stack(uint32_t esp0) {\n    kernel_tss.esp0 = esp0;\n}\n```\n**Adding the TSS descriptor to the GDT:** The GDT currently has 5 entries (null, kernel code, kernel data, user code, user data). You need to add a 6th \u2014 the TSS descriptor. Unlike code and data descriptors that use the S=1 (system=code/data) bit, the TSS descriptor is a **system descriptor** (S=0) with type `0x9` for an available 32-bit TSS:\n```c\nvoid gdt_set_tss_entry(int index, uint32_t base, uint32_t limit) {\n    // type=9 (32-bit TSS, available), S=0 (system descriptor), DPL=0, P=1\n    // access byte = 0x89: Present=1, DPL=0, S=0, Type=1001 (TSS32 available)\n    set_gdt_entry(index, base, limit, 0x89, 0x00);\n    // Note: flags=0x00 because TSS uses byte granularity, not page granularity\n}\n```\n**The GDT now has 6 entries:**\n| Index | Selector | Purpose |\n|-------|----------|---------|\n| 0 | `0x00` | Null descriptor |\n| 1 | `0x08` | Kernel code (ring 0) |\n| 2 | `0x10` | Kernel data (ring 0) |\n| 3 | `0x18` | User code (ring 3, with RPL=3 \u2192 `0x1B`) |\n| 4 | `0x20` | User data (ring 3, with RPL=3 \u2192 `0x23`) |\n| 5 | `0x28` | TSS descriptor |\n> **\ud83d\udd11 Two-level paging and how the TSS interacts with it:** When you switch processes, you load the new process's page directory into CR3. But the TSS itself lives at a kernel virtual address. After the CR3 switch, the TSS must still be accessible \u2014 which means the kernel's virtual address mappings must be present in every process's page directory. This is why your Milestone 3 design replicated the kernel PDEs (entries 768 onward in the page directory) into every process's page directory. The TSS, interrupt handlers, GDT, and IDT all live at kernel virtual addresses that must be reachable from any address space.\n---\n## Phase 3: The Context Switch \u2014 Assembly as the Only Option\n{{DIAGRAM:diag-m4-context-switch-assembly}}\n{{DIAGRAM:diag-m4-context-switch-stacks}}\nThis is the function that cannot be written in C. C compilers assume full control over register usage and stack management. A context switch by definition violates those assumptions: it deliberately changes ESP to point somewhere completely different, and the compiler cannot generate correct code for that without knowing what you're doing. You must write this in assembly.\nThe conceptual sequence of a context switch from Process A to Process B:\n1. **Save A's registers** onto A's kernel stack (or into A's `cpu_context_t`)\n2. **Store A's ESP** into `A->context.esp`\n3. **Load B's ESP** from `B->context.esp` \u2190 **the identity swap happens here**\n4. **Restore B's registers** from B's kernel stack\n5. **Return** \u2014 which pops B's saved EIP and resumes B's execution\n```nasm\n; context_switch.asm\n; Calling convention: void context_switch(cpu_context_t *old_ctx, cpu_context_t *new_ctx)\n; Arguments on stack: [ESP+4] = old_ctx, [ESP+8] = new_ctx\n; (Recall: cdecl calling convention \u2014 caller pushes args right-to-left before call)\n[BITS 32]\n[GLOBAL context_switch_asm]\ncontext_switch_asm:\n    ; \u2500\u2500 SAVE OLD PROCESS CONTEXT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ; At this point: ESP points to old process's kernel stack\n    ; The call instruction already pushed the return EIP onto the stack\n    ; Save callee-saved registers per cdecl ABI:\n    ; EBX, EBP, ESI, EDI must be preserved across function calls.\n    ; EAX, ECX, EDX are caller-saved \u2014 the caller already saved them if needed.\n    push ebx\n    push esi\n    push edi\n    push ebp\n    ; Push EFLAGS \u2014 critical! Includes the Interrupt Flag (IF).\n    ; If IF is set in old process's EFLAGS but we forget to save/restore it,\n    ; the new process might run with interrupts disabled forever.\n    pushfd              ; Push EFLAGS onto stack\n    ; Save old process's ESP into old_ctx->esp\n    ; old_ctx is at [original_ESP + 4 + 20] = [ESP+24] after the 5 pushes above\n    ; Wait \u2014 we need to be careful: after pushfd, ESP has moved.\n    ; Let's track: original ESP = ESP0\n    ;   push ebx  \u2192 ESP = ESP0 - 4\n    ;   push esi  \u2192 ESP = ESP0 - 8\n    ;   push edi  \u2192 ESP = ESP0 - 12\n    ;   push ebp  \u2192 ESP = ESP0 - 16\n    ;   pushfd    \u2192 ESP = ESP0 - 20\n    ; The function argument old_ctx was at [ESP0+4] (4 bytes for return address)\n    ; Now it's at [ESP + 24]: current ESP is ESP0-20, so ESP0+4 = ESP+24\n    mov eax, [esp + 24]     ; EAX = old_ctx (first argument)\n    mov [eax + 16], esp     ; old_ctx->esp = current ESP (offset 16 in cpu_context_t)\n                            ; This captures the entire saved state on the stack\n    ; \u2500\u2500 SWITCH TO NEW PROCESS CONTEXT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ; Load new process's ESP. After this instruction, we are on the new stack.\n    mov eax, [esp + 28]     ; EAX = new_ctx (second argument, now at ESP+28)\n                            ; Why +28? The arguments were pushed BEFORE our saves,\n                            ; so relative to CURRENT esp:\n                            ; [esp+0]  = EFLAGS (just pushed)\n                            ; [esp+4]  = EBP\n                            ; [esp+8]  = EDI\n                            ; [esp+12] = ESI\n                            ; [esp+16] = EBX\n                            ; [esp+20] = return EIP (pushed by 'call')\n                            ; [esp+24] = old_ctx (first arg)\n                            ; [esp+28] = new_ctx (second arg)\n    mov esp, [eax + 16]     ; ESP = new_ctx->esp \u2190 IDENTITY SWAP HAPPENS HERE\n                            ; We are now on the new process's kernel stack\n    ; \u2500\u2500 RESTORE NEW PROCESS CONTEXT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ; The new process's stack has (from bottom): EFLAGS, EBP, EDI, ESI, EBX, EIP\n    ; (exactly what the old process pushed the last time IT was switched out)\n    popfd               ; Restore EFLAGS (including IF \u2014 re-enables interrupts if set)\n    pop ebp\n    pop edi\n    pop esi\n    pop ebx\n    ; The 'ret' instruction pops EIP from the new stack.\n    ; This is the EIP that was pushed when the new process last called context_switch_asm\n    ; (or the fabricated initial EIP set up by process_create).\n    ; Execution resumes at the new process's last suspension point.\n    ret\n```\n**Mapping the stack layout to `cpu_context_t` offsets:**\n```\ncpu_context_t field:    Stack position (when saved):\n  edi     offset 0   \u2192 [esp+8]  after all saves (we use explicit mov, not struct)\n  esi     offset 4   \u2192 [esp+12]\n  ebx     offset 8   \u2192 [esp+16]\n  ebp     offset 12  \u2192 [esp+4]\n  esp     offset 16  \u2192 saved explicitly with mov [eax+16], esp\n  eip     offset 20  \u2192 [esp+20] (return address from 'call')\n  eflags  offset 24  \u2192 [esp+0] (pushed by pushfd)\n```\n> **Wait \u2014 we're saving ESP into a struct field but popping registers from the stack, not from the struct.** This is a deliberate design choice. Instead of copying 7 registers into the struct and then copying them back out, we save the *stack pointer* that points to where all those registers live. When we restore, we just load the saved ESP and then pop \u2014 the registers are already on the stack in the right order from when they were pushed. The struct only needs to store ESP (plus EIP and EFLAGS for the first-time setup case). This is the minimal representation \u2014 the stack *is* the saved context. This is exactly how Linux saves kernel thread contexts in `struct thread_struct`.\n**C wrapper for the scheduler:**\n```c\n// Declared in process.h, implemented in context_switch.asm\nextern void context_switch_asm(cpu_context_t *old_ctx, cpu_context_t *new_ctx);\nvoid context_switch(process_t *old, process_t *new) {\n    // 1. Update the current page directory if processes differ\n    if (old->page_directory != new->page_directory) {\n        uint32_t pd_phys = (uint32_t)new->page_directory; // already physical\n        __asm__ volatile (\"mov cr3, %0\" : : \"r\"(pd_phys) : \"memory\");\n    }\n    // 2. Update TSS ESP0 to point to the new process's kernel stack top\n    //    This MUST happen before the context switch so that any interrupt\n    //    arriving while the new process runs uses the right kernel stack.\n    tss_set_kernel_stack(new->kernel_stack_top);\n    // 3. Update the current_process pointer\n    //    This happens before context_switch_asm so that any code running\n    //    immediately after the switch (in the new process's context) sees\n    //    current_process correctly.\n    current_process = new;\n    // 4. Perform the actual register swap\n    //    After this call returns, we are the NEW process.\n    //    The OLD process will resume here (from ITS perspective) next time\n    //    it is scheduled.\n    context_switch_asm(&old->context, &new->context);\n}\n```\n---\n## Phase 4: Creating a Process \u2014 Fabricating the Initial Context\n{{DIAGRAM:diag-m4-process-states}}\nWhen you create a process for the first time, it has never run, so there are no \"saved registers\" to restore. You must fabricate a kernel stack that *looks exactly like* what the context switch code would have saved for a suspended process. When the scheduler first picks this process and calls `context_switch_asm`, it will pop these fabricated values and `ret` to the fabricated EIP \u2014 launching the process as if it had always existed.\n```c\nprocess_t *process_create(const char *name, void (*entry)(void),\n                           uint32_t *page_dir, int is_user) {\n    // Allocate the PCB from the kernel heap\n    process_t *proc = kmalloc(sizeof(process_t));\n    if (!proc) return NULL;\n    __builtin_memset(proc, 0, sizeof(process_t));\n    // Assign identity\n    static uint32_t next_pid = 1;\n    proc->pid   = next_pid++;\n    proc->state = PROCESS_READY;\n    __builtin_strncpy(proc->name, name, sizeof(proc->name) - 1);\n    // Allocate and set up the kernel stack\n    // This is the stack the process will use when it's in kernel mode\n    // (either because it IS a kernel process, or because it entered via interrupt/syscall)\n    proc->kernel_stack = kmalloc(KERNEL_STACK_SIZE);\n    if (!proc->kernel_stack) { kfree(proc); return NULL; }\n    __builtin_memset(proc->kernel_stack, 0, KERNEL_STACK_SIZE);\n    // kernel_stack_top is the address just past the top of the allocated buffer\n    // (stacks grow downward, so the initial ESP is at the high end)\n    proc->kernel_stack_top = (uint32_t)(proc->kernel_stack) + KERNEL_STACK_SIZE;\n    // Set the page directory\n    proc->page_directory = page_dir;\n    // \u2500\u2500 Fabricate the initial kernel stack frame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    // We need to set up the stack so that when context_switch_asm restores\n    // this process for the first time, it looks like the process was suspended\n    // in the middle of a context_switch_asm call, with EIP pointing to 'entry'.\n    //\n    // The stack (growing downward from kernel_stack_top) must contain:\n    // [top-4]  EFLAGS  (popfd will load this; bit 9 = IF = 1 for interrupts enabled)\n    // [top-8]  EBP     (pop ebp)\n    // [top-12] EDI     (pop edi)\n    // [top-16] ESI     (pop esi)\n    // [top-20] EBX     (pop ebx)\n    // [top-24] entry   (ret will pop this as EIP \u2014 this is where execution starts!)\n    //\n    // After context_switch_asm pops these and executes 'ret', the process\n    // begins running at 'entry' with a clean register state.\n    uint32_t *stack = (uint32_t *)proc->kernel_stack_top;\n    *--stack = 0x00000202;   // EFLAGS: IF=1 (interrupts enabled), reserved bits set\n                              // bit 1 is always 1; bit 9 (IF) must be 1 or the\n                              // process will run with interrupts permanently disabled\n    *--stack = 0;             // EBP = 0 (bottom of call stack, for backtraces)\n    *--stack = 0;             // EDI = 0 (undefined at process start, initialize cleanly)\n    *--stack = 0;             // ESI = 0\n    *--stack = 0;             // EBX = 0\n    *--stack = (uint32_t)entry; // EIP \u2014 where the process will start executing\n    // Save ESP to point to the fabricated stack frame\n    // When context_switch_asm loads this ESP and begins popping,\n    // it will pop the values we just pushed above.\n    proc->context.esp = (uint32_t)stack;\n    // For user-mode processes, set up the user-mode stack separately\n    if (is_user) {\n        // Allocate a user-mode stack page\n        uint32_t user_stack_phys = pmm_alloc_frame();\n        // Map it into the process's address space at a standard user stack address\n        uint32_t user_stack_virt = 0xBFFFF000; // Top of user address space, one page\n        paging_map((page_directory_t *)page_dir, user_stack_virt,\n                   user_stack_phys, PTE_PRESENT | PTE_WRITABLE | PTE_USER);\n        proc->user_esp = user_stack_virt + PAGE_SIZE - 4; // Start at top, push downward\n    }\n    return proc;\n}\n```\n> **Why `EFLAGS = 0x00000202`?** Let's decode this: binary `0000 0000 0000 0000 0000 0010 0000 0010`. Bit 1 is always 1 (reserved, must be set). Bit 9 is the Interrupt Flag (IF) \u2014 setting it means interrupts are enabled when this process starts running. If you forget IF, the process runs with interrupts permanently disabled: the timer never fires, the scheduler never preempts, the system freezes. This is one of the most common bugs when first implementing processes, and it's invisible \u2014 the first process to run just never yields, and nothing else ever executes. Always check IF when your scheduler appears to work for one process but never switches.\n---\n## Phase 5: The Round-Robin Scheduler\n{{DIAGRAM:diag-m4-scheduler-flow}}\nThe scheduler is the policy layer of process management. It answers one question: given that the timer just fired, which process should run next?\nFor this milestone, you implement **round-robin scheduling**: maintain a circular linked list of READY processes and advance to the next one on each timer tick. Every process gets equal CPU time. No priorities, no starvation.\n```c\n// scheduler.c\nprocess_t *current_process   = NULL;\nprocess_t *process_list_head = NULL;  // Head of the circular ready queue\nvoid scheduler_add_process(process_t *proc) {\n    proc->state = PROCESS_READY;\n    if (!process_list_head) {\n        process_list_head = proc;\n        proc->next = proc;  // Circular: points to itself\n    } else {\n        // Insert after current position to maintain order\n        process_t *tail = process_list_head;\n        while (tail->next != process_list_head) tail = tail->next;\n        tail->next = proc;\n        proc->next = process_list_head;\n    }\n}\n// Called from the timer IRQ handler (IRQ0 \u2192 vector 32)\n// IMPORTANT: Interrupts are DISABLED when this runs (we're inside an interrupt gate)\nvoid scheduler_tick(struct interrupt_frame *frame) {\n    if (!current_process) return;\n    // Accounting: track how long this process has run\n    current_process->total_ticks++;\n    // Find the next READY process in the circular list\n    process_t *next = current_process->next;\n    int iterations = 0;\n    while (next->state != PROCESS_READY && next->state != PROCESS_RUNNING) {\n        next = next->next;\n        if (++iterations > MAX_PROCESSES) {\n            // No runnable process found \u2014 this shouldn't happen if we have an idle process\n            kprintf(\"[SCHED] No runnable process!\\n\");\n            return;\n        }\n        if (next == current_process) return; // Only one runnable process; no switch needed\n    }\n    if (next == current_process) return; // Same process \u2014 no context switch needed\n    // Transition states\n    process_t *old = current_process;\n    old->state = PROCESS_READY;\n    next->state = PROCESS_RUNNING;\n    // Perform the context switch\n    // After this call returns (possibly much later), we are back to 'old'\n    context_switch(old, next);\n    // When we reach here, this process has been rescheduled and is running again.\n    // Interrupts may have been re-enabled by popfd in context_switch_asm\n    // (if EFLAGS.IF was set in old's saved context \u2014 and it should be).\n}\n```\n**The critical timing constraint \u2014 when interrupts are enabled.** The timer handler runs with interrupts disabled (we used interrupt gates, which clear IF on entry). The context switch's `popfd` instruction restores the new process's EFLAGS, which has IF=1 set (we initialized it that way). This means after the `popfd` in `context_switch_asm`, interrupts are re-enabled for the new process. This is correct \u2014 the new process should be interruptible. But it also means you cannot do anything interrupt-sensitive between the `popfd` and the `ret` in the context switch code. Since the only remaining instruction is `ret`, this is fine.\n**Interrupts during the switch itself** (between the CR3 load, TSS update, and the register swap) must not happen. Since we entered the scheduler from an interrupt handler (interrupts disabled), we are safe. Never call `sti` inside the context switch path.\n{{DIAGRAM:diag-m4-interrupt-reentrancy}}\n**Connecting the timer interrupt to the scheduler:**\n```c\n// In your IRQ dispatch (from Milestone 2), add this case:\nvoid timer_handler(struct interrupt_frame *frame) {\n    tick_counter++;\n    scheduler_tick(frame);  // This is the only addition needed\n    // pic_send_eoi(0) is called by irq_dispatch after this returns\n}\n```\nThe elegance here: you built the timer interrupt in Milestone 2, never knowing exactly what would go in `timer_handler`. Now you know. The infrastructure was always ready.\n---\n## Phase 6: Three Kernel Processes \u2014 The First Multitasking Demo\n{{DIAGRAM:diag-m4-three-process-demo}}\nBefore touching user mode, verify that kernel-mode preemptive multitasking works. Three kernel processes running concurrently, each writing to a different column of the VGA screen \u2014 a visual demonstration that is unambiguous: if all three columns are filling simultaneously, preemption is working.\n```c\n// demo_processes.c\n// Process A: writes 'A' to the left third of the screen (columns 0-26)\nstatic void process_a(void) {\n    int row = 0;\n    while (1) {\n        if (row >= 25) row = 0;\n        for (int col = 0; col < 26; col++) {\n            // Write directly to VGA buffer at process A's columns\n            volatile uint16_t *vga = (volatile uint16_t *)0xC00B8000;\n            // 0xC00B8000 = 0xB8000 + 0xC0000000 (higher-half mapped VGA)\n            vga[row * 80 + col] = (0x0A << 8) | 'A'; // Green 'A'\n        }\n        // Voluntary yield simulation: just let the timer preempt us\n        // In a real kernel, there would be a sleep() syscall here\n        for (volatile int i = 0; i < 10000; i++);\n        row++;\n    }\n}\nstatic void process_b(void) {\n    int row = 0;\n    while (1) {\n        if (row >= 25) row = 0;\n        for (int col = 27; col < 53; col++) {\n            volatile uint16_t *vga = (volatile uint16_t *)0xC00B8000;\n            vga[row * 80 + col] = (0x0C << 8) | 'B'; // Red 'B'\n        }\n        for (volatile int i = 0; i < 10000; i++);\n        row++;\n    }\n}\nstatic void process_c(void) {\n    int row = 0;\n    while (1) {\n        if (row >= 25) row = 0;\n        for (int col = 54; col < 80; col++) {\n            volatile uint16_t *vga = (volatile uint16_t *)0xC00B8000;\n            vga[row * 80 + col] = (0x0B << 8) | 'C'; // Cyan 'C'\n        }\n        for (volatile int i = 0; i < 10000; i++);\n        row++;\n    }\n}\nvoid demo_kernel_processes(void) {\n    // Use the kernel's own page directory \u2014 all kernel processes share it\n    extern page_directory_t boot_pd;\n    uint32_t *kpd = (uint32_t *)VIRT_TO_PHYS((uint32_t)&boot_pd);\n    process_t *pa = process_create(\"proc_a\", process_a, kpd, 0); // is_user=0\n    process_t *pb = process_create(\"proc_b\", process_b, kpd, 0);\n    process_t *pc = process_create(\"proc_c\", process_c, kpd, 0);\n    scheduler_add_process(pa);\n    scheduler_add_process(pb);\n    scheduler_add_process(pc);\n    // Bootstrap: manually start the first process\n    // current_process must be set before the timer fires\n    current_process = pa;\n    pa->state = PROCESS_RUNNING;\n    // We need to \"jump into\" process A without returning.\n    // Load PA's ESP and simulate a return from context_switch_asm.\n    // This is the initial scheduling bootstrap \u2014 only done once.\n    __asm__ volatile (\n        \"mov esp, %0\\n\"   // Switch to process A's kernel stack (with fabricated frame)\n        \"popfd\\n\"         // Restore EFLAGS (enables interrupts)\n        \"pop ebp\\n\"\n        \"pop edi\\n\"\n        \"pop esi\\n\"\n        \"pop ebx\\n\"\n        \"ret\\n\"           // \"Return\" to process_a \u2014 the fabricated EIP\n        : : \"r\"(pa->context.esp)\n    );\n    // Unreachable \u2014 ret jumps to process_a\n}\n```\n**The bootstrap `__asm__` block** is doing exactly what `context_switch_asm` does in its restore phase \u2014 but without a corresponding save phase. We don't need to save anything because the code that runs this is the kernel's initialization path, which has served its purpose and will never be resumed. After `ret` jumps to `process_a`, the kernel main stack is effectively abandoned (it's still mapped, but no process will use it). Process A's kernel stack becomes the current kernel stack. From this moment, preemptive multitasking is running.\n---\n## Phase 7: User-Mode Processes \u2014 Ring 3 Isolation\n{{DIAGRAM:diag-m4-user-mode-page-directory}}\n{{DIAGRAM:diag-m4-ring-transition-mechanism}}\nThe jump to user mode is the most hardware-intensive transition in your kernel. You cannot simply call a function \u2014 user mode requires specific segment register values, specific EFLAGS settings, and a precise stack layout. The only instruction that can perform this transition is `iret` \u2014 the interrupt return instruction \u2014 which atomically loads CS, EIP, SS, ESP, and EFLAGS from the stack in a way that correctly establishes ring 3.\n> **[[EXPLAIN: x86-hardware-stack-switching-privilege]] x86 hardware-enforced stack switching on privilege change:**\n> When the CPU transitions between privilege levels \u2014 either entering ring 0 from ring 3 (on interrupt/syscall) or returning to ring 3 from ring 0 (on `iret`) \u2014 it automatically switches the active stack. This is a hardware mechanism, not something your code initiates. On a ring 3 \u2192 ring 0 transition (e.g., a timer interrupt fires while user code runs): the CPU reads `SS0:ESP0` from the TSS, atomically switches ESP to the kernel stack, then pushes `SS_user`, `ESP_user`, `EFLAGS`, `CS_user`, and `EIP_user` onto the kernel stack. On a ring 0 \u2192 ring 3 transition (`iret` with a ring-3 CS on the stack): the CPU pops `EIP`, `CS`, `EFLAGS` from the kernel stack, then \u2014 seeing that CS has RPL=3 \u2014 also pops `ESP` and `SS` and switches the stack to the user stack. This is how user code gets its own stack and kernel code gets its own stack, with the hardware enforcing the boundary.\n**Per-process page directory for user mode:**\nA user-mode process must have its own page directory so its memory is isolated from other processes. However, the kernel must still be accessible when interrupts or system calls bring execution into ring 0 \u2014 otherwise the interrupt handler would immediately fault on its first instruction. The solution: copy the kernel PDEs into every process's page directory, but mark user-space pages with `PTE_USER` and kernel pages without it (supervisor-only).\n```c\n// Create a new user-mode page directory with kernel mappings inherited\nuint32_t *create_user_page_directory(void) {\n    // Allocate one page for the new page directory\n    uint32_t pd_phys = pmm_alloc_frame();\n    // Get a virtual address to initialize it\n    page_directory_t *pd = (page_directory_t *)(pd_phys + KERNEL_VIRT_BASE);\n    // Zero all entries \u2014 user space starts empty\n    __builtin_memset(pd, 0, PAGE_SIZE);\n    // Copy kernel PDE entries (indices 768-1023, covering 0xC0000000-0xFFFFFFFF)\n    // These must be identical in all page directories so the kernel is always\n    // reachable, regardless of which process is currently running.\n    extern page_directory_t boot_pd;\n    page_directory_t *kernel_pd = (page_directory_t *)(\n        VIRT_TO_PHYS((uint32_t)&boot_pd) + KERNEL_VIRT_BASE\n    );\n    for (int i = 768; i < 1024; i++) {\n        (*pd)[i] = (*kernel_pd)[i]; // Copy the PDE (including the page table pointer)\n        // Note: We copy the PDE (pointer to page table), not the page table itself.\n        // All processes share the same kernel page tables \u2014 modifications to kernel\n        // mappings are visible to all processes immediately.\n    }\n    // Return the PHYSICAL address \u2014 this is what goes into CR3\n    return (uint32_t *)pd_phys;\n}\n```\n**The `iretd` sequence to enter user mode for the first time:**\n```c\n// Enter user mode for the first time in a newly created user process.\n// Called as the LAST THING in the user process's kernel initialization,\n// from within the process's own kernel stack context (i.e., after the scheduler\n// has switched to this process at least once in kernel mode).\nvoid enter_user_mode(uint32_t user_eip, uint32_t user_esp) {\n    // Disable interrupts before building the iret frame on the kernel stack\n    __asm__ volatile (\"cli\");\n    // Build the stack frame that iret expects for a ring-3 transition:\n    // When iret sees a CS with RPL=3, it additionally pops ESP and SS.\n    // The stack must look like (top to bottom, i.e., pushed in this order):\n    //\n    //   [TOP OF KERNEL STACK - just before iret]\n    //   SS     (user data segment selector with RPL=3: 0x23)\n    //   ESP    (user stack pointer)\n    //   EFLAGS (with IF=1: interrupts enabled in user mode)\n    //   CS     (user code segment selector with RPL=3: 0x1B)\n    //   EIP    (user entry point)\n    //   [iret pops these five values]\n    //\n    // The selectors 0x1B and 0x23 are GDT indices 3 and 4 with RPL=3:\n    //   0x1B = (3 << 3) | 3 = 0b0001_1011\n    //   0x23 = (4 << 3) | 3 = 0b0010_0011\n    __asm__ volatile (\n        // Load all data segment registers with the user data selector\n        // This is required before iret, because after iret CS=0x1B,\n        // and DS/ES/FS/GS must already be valid user-mode selectors.\n        \"mov ax, 0x23\\n\"    // User data selector (ring 3)\n        \"mov ds, ax\\n\"\n        \"mov es, ax\\n\"\n        \"mov fs, ax\\n\"\n        \"mov gs, ax\\n\"\n        // Build iret frame: push in REVERSE order of what iret pops\n        \"push 0x23\\n\"       // SS (user stack segment)\n        \"push %1\\n\"         // ESP (user stack pointer)\n        \"pushfd\\n\"          // EFLAGS \u2014 current value, but we'll OR in IF\n        \"pop eax\\n\"         // Get EFLAGS into EAX\n        \"or eax, 0x200\\n\"   // Set IF (bit 9): user code starts with interrupts enabled\n        \"push eax\\n\"        // Push modified EFLAGS back\n        \"push 0x1B\\n\"       // CS (user code segment, RPL=3)\n        \"push %0\\n\"         // EIP (user entry point)\n        // The iret instruction atomically:\n        // 1. Pops EIP, CS \u2192 CPU is now \"logically\" at ring 3\n        // 2. Sees CS.RPL=3 > current CPL=0, so also pops ESP, SS\n        // 3. Switches the stack to the user stack\n        // 4. CPU is now executing user code with ring-3 privileges\n        \"iretd\\n\"\n        :\n        : \"r\"(user_eip), \"r\"(user_esp)\n        : \"eax\"\n    );\n    // Unreachable \u2014 iretd does not return\n}\n```\n**Verifying isolation \u2014 the supervisor bit test:**\nOnce a user-mode process is running, you can verify that kernel memory is protected by having the user process attempt to read a kernel virtual address. The page tables mark kernel pages without `PTE_USER` (the User/Supervisor bit, bit 2 of the PTE, is 0 for kernel pages). When user code (CPL=3) accesses such a page, the MMU checks the U/S bit: if U=0 and CPL=3, it raises page fault #14 with error code bit 2 (U bit) set to 1 (user-mode access) and bit 0 (P bit) set to 1 (page is present, but forbidden). Your page fault handler should print:\n```\n[PAGE FAULT] at 0xC0100000\n  Access: Read from user mode\n  Cause: Protection violation (page present, wrong permissions)\n  \u2192 User process attempted kernel memory access \u2014 kill process\n```\nThis test is the definitive proof that your ring 3 isolation works.\n---\n## Phase 8: The System Call Interface \u2014 INT 0x80\n{{DIAGRAM:diag-m4-syscall-dispatch}}\nSystem calls are the controlled, audited path through which user-mode processes request kernel services. A user process cannot call `vga_putchar` directly \u2014 that function lives in kernel address space, inaccessible from ring 3. Instead, it issues a software interrupt: `int 0x80`. The CPU treats this like any other interrupt, transitioning to ring 0, loading the kernel stack from the TSS, and calling the IDT handler at vector 128 (`0x80`).\n**Why `int 0x80`?** Historical Unix convention on x86. Linux used `int 0x80` from its first public release through kernel 2.5 (when `sysenter` was added as a faster alternative). The number `0x80` (128) was chosen to be safely away from both CPU exception vectors (0\u201331) and hardware IRQ vectors (32\u201347). For your kernel, `int 0x80` is the clearest mechanism to implement because it uses the exact same IDT machinery you built in Milestone 2.\n**IDT entry for the system call gate:**\nThe system call gate must use DPL=3 (so user code can invoke it via `int 0x80`) and should be a **trap gate** (type `0x8F`) rather than an interrupt gate \u2014 trap gates do not clear the interrupt flag, meaning the kernel remains interruptible during system call processing:\n```c\n// In idt_init(), add:\n// Vector 0x80 = 128: system call gate\n// flags = 0xEF: P=1, DPL=3 (user can invoke), type=1111 (32-bit trap gate)\nidt_set_gate(0x80, (uint32_t)isr_128, 0x08, 0xEF);\n```\n**The calling convention \u2014 registers as arguments:**\n```\nEAX = syscall number\nEBX = argument 1\nECX = argument 2\nEDX = argument 3\nESI = argument 4 (if needed)\nEDI = argument 5 (if needed)\n```\n**Syscall numbers:**\n```c\n#define SYS_EXIT    1\n#define SYS_WRITE   4\n// (following Linux's historical numbering for familiarity)\n```\n**The system call dispatcher:**\n```c\n// syscall.c\ntypedef int32_t (*syscall_handler_t)(uint32_t, uint32_t, uint32_t);\nstatic int32_t sys_exit(uint32_t exit_code, uint32_t _unused1, uint32_t _unused2) {\n    kprintf(\"[SYSCALL] Process %u exited with code %u\\n\",\n            current_process->pid, exit_code);\n    // Mark the current process as dead\n    current_process->state = PROCESS_DEAD;\n    // Remove from the ready queue\n    // Find the previous node in the circular list and bypass current\n    process_t *prev = current_process;\n    while (prev->next != current_process) prev = prev->next;\n    if (prev == current_process) {\n        // Last process \u2014 kernel should enter idle or halt\n        kprintf(\"[KERNEL] All processes exited.\\n\");\n        for (;;) __asm__ volatile (\"cli; hlt\");\n    }\n    prev->next = current_process->next;\n    if (process_list_head == current_process) {\n        process_list_head = current_process->next;\n    }\n    // Switch to the next process immediately\n    // We do this by forcing a schedule \u2014 pick next from the list\n    process_t *next = prev->next;\n    next->state = PROCESS_RUNNING;\n    // Can't call context_switch(current, next) because current is DEAD \u2014\n    // its context doesn't need saving. Use a simplified switch:\n    tss_set_kernel_stack(next->kernel_stack_top);\n    current_process = next;\n    // Load new CR3\n    __asm__ volatile (\"mov cr3, %0\" : : \"r\"(next->page_directory) : \"memory\");\n    // Jump directly into the new process's saved context\n    __asm__ volatile (\n        \"mov esp, %0\\n\"\n        \"popfd\\n\"\n        \"pop ebp\\n\"\n        \"pop edi\\n\"\n        \"pop esi\\n\"\n        \"pop ebx\\n\"\n        \"ret\\n\"\n        : : \"r\"(next->context.esp)\n    );\n    __builtin_unreachable();\n}\n// sys_write: write bytes to a file descriptor\n// For now: fd=1 (stdout) writes to VGA/serial; everything else returns -1\nstatic int32_t sys_write(uint32_t fd, uint32_t buf_virt, uint32_t count) {\n    if (fd != 1) return -1; // Only stdout supported for now\n    // Validate the user pointer: it must be in user address space\n    // A simple check: must be below 0xC0000000 (kernel base)\n    if (buf_virt >= 0xC0000000) {\n        kprintf(\"[SYSCALL] sys_write: invalid user pointer 0x%x\\n\", buf_virt);\n        return -1; // EFAULT\n    }\n    // buf_virt is a virtual address in the user's address space.\n    // Since we share the same CR3 (we're in kernel mode but user's page dir is loaded),\n    // we can access it directly \u2014 the user pages are mapped with PTE_USER, and\n    // kernel code (CPL=0) can always access user pages regardless of the U/S bit.\n    const char *buf = (const char *)buf_virt;\n    uint32_t written = 0;\n    for (uint32_t i = 0; i < count; i++) {\n        char c = buf[i];\n        vga_putchar(c);\n        serial_putchar(c);\n        written++;\n    }\n    return (int32_t)written;\n}\n// Syscall dispatch table\nstatic syscall_handler_t syscall_table[] = {\n    NULL,       // 0: unused\n    sys_exit,   // 1: exit\n    NULL,       // 2: fork (unimplemented)\n    NULL,       // 3: read (unimplemented)\n    sys_write,  // 4: write\n};\n#define SYSCALL_COUNT (sizeof(syscall_table) / sizeof(syscall_table[0]))\n// Called from interrupt_dispatch when int_no == 0x80\nvoid syscall_dispatch(struct interrupt_frame *frame) {\n    uint32_t syscall_no = frame->eax;\n    uint32_t arg1       = frame->ebx;\n    uint32_t arg2       = frame->ecx;\n    uint32_t arg3       = frame->edx;\n    if (syscall_no >= SYSCALL_COUNT || !syscall_table[syscall_no]) {\n        kprintf(\"[SYSCALL] Unknown syscall %u from PID %u\\n\",\n                syscall_no, current_process->pid);\n        frame->eax = (uint32_t)-1; // Return -ENOSYS\n        return;\n    }\n    // Call the handler \u2014 the return value goes into EAX (by the C ABI)\n    // We write it back to frame->eax so iret restores it in the user's EAX\n    int32_t result = syscall_table[syscall_no](arg1, arg2, arg3);\n    frame->eax = (uint32_t)result;\n    // iret in isr_common_stub will restore the updated frame,\n    // including the modified EAX \u2014 so the user process sees the return value.\n}\n```\n**In `interrupt_dispatch`, add the syscall case:**\n```c\nvoid interrupt_dispatch(struct interrupt_frame *frame) {\n    if (frame->int_no < 32) {\n        // CPU exceptions (existing from Milestone 2)\n        // ...\n    } else if (frame->int_no < 48) {\n        // Hardware IRQs (existing from Milestone 2)\n        irq_dispatch(frame->int_no - 32, frame);\n    } else if (frame->int_no == 0x80) {\n        // System call \u2014 no EOI needed (software interrupt, not hardware)\n        syscall_dispatch(frame);\n    }\n}\n```\n**A complete user-mode process:**\n```c\n// This code runs in ring 3. It cannot call kernel functions directly.\n// It must use system calls via int 0x80.\n// User-space \"stdlib\" shim:\nstatic void user_write(const char *str, uint32_t len) {\n    __asm__ volatile (\n        \"int 0x80\"\n        :\n        : \"a\"(4),        // EAX = SYS_WRITE\n          \"b\"(1),        // EBX = fd 1 (stdout)\n          \"c\"(str),      // ECX = buffer pointer\n          \"d\"(len)       // EDX = length\n    );\n}\nstatic void user_exit(uint32_t code) {\n    __asm__ volatile (\n        \"int 0x80\"\n        :\n        : \"a\"(1),    // EAX = SYS_EXIT\n          \"b\"(code)  // EBX = exit code\n    );\n    // If exit returns (shouldn't), spin\n    while(1);\n}\n// The actual user process entry point\nvoid user_process_entry(void) {\n    user_write(\"Hello from ring 3!\\n\", 19);\n    // Try to prove ring 3 is isolated:\n    // The next line would cause a page fault if uncommented:\n    // volatile uint32_t x = *(volatile uint32_t *)0xC0100000; // FORBIDDEN\n    user_write(\"About to exit.\\n\", 15);\n    user_exit(0);\n    // Never reached\n}\n```\n> **Hardware Soul \u2014 The cost of INT 0x80**: Every system call via `int 0x80` incurs:\n> 1. **Interrupt dispatch**: CPU reads IDT entry (likely L1 cache hit after first call).\n> 2. **Privilege transition**: CPU checks CPL vs DPL, reads TSS for ESP0 (~5 memory accesses if TLB-cold). If TLB-warm: ~2 cycles. If cold: 10-30 cycles with memory latency.\n> 3. **Stack switch**: CPU pushes SS, ESP, EFLAGS, CS, EIP onto kernel stack.\n> 4. **ISR execution**: Your `isr_common_stub` runs `pusha` (8 pushes), pushes segment registers (4 pushes), calls `interrupt_dispatch`.\n> 5. **Syscall handler**: Your C handler runs.\n> 6. **Return**: `popa`, restore segments, `iretd` \u2014 same hardware work in reverse.\n>\n> Total on a modern CPU: ~100\u2013300 ns per syscall. Linux `gettimeofday()` was called so frequently (every log line, every network packet) that Linus added the **vDSO** (virtual Dynamic Shared Object): a kernel-provided shared library mapped into user address space that implements `gettimeofday` by reading a memory-mapped counter \u2014 no privilege transition at all. `clock_gettime(CLOCK_MONOTONIC)` via vDSO takes ~15 ns vs ~250 ns via syscall \u2014 a 16\u00d7 difference. `io_uring` takes this further: batch thousands of I/O operations into a shared ring buffer (from Milestone 2 \u2014 same circular buffer!), and the kernel processes them all in one pass. One syscall's worth of overhead for thousands of I/O operations.\n---\n## Phase 9: Complete Initialization Sequence\nBringing everything together in the correct order:\n```c\nvoid kernel_main(multiboot_info_t *mbi) {\n    // Milestone 1 subsystems\n    vga_clear();\n    serial_init();\n    kprintf(\"=== Kernel Milestone 4: Processes and Scheduling ===\\n\");\n    // Milestone 2 subsystems\n    gdt_init();              // GDT: now includes TSS descriptor at entry 5\n    idt_init();              // IDT: now includes vector 0x80 for syscalls\n    pic_remap(0x20, 0x28);\n    pit_init(100);           // 100Hz timer \u2192 scheduler at 10ms intervals\n    // Milestone 3 subsystems (paging already active from boot stub)\n    mmap_parse(mbi);\n    pmm_init(mbi);\n    // (heap was already initialized; we can kmalloc now)\n    // Milestone 4: TSS, processes, scheduler\n    tss_init();              // Register TSS in GDT, load TR register\n    kprintf(\"[OK] TSS initialized (selector 0x28)\\n\");\n    // Create the kernel idle process (runs when nothing else is ready)\n    // This process MUST exist \u2014 if all other processes block, the scheduler\n    // needs something to run. The idle process just halts between ticks.\n    extern void idle_process(void);\n    extern page_directory_t boot_pd;\n    uint32_t *kpd_phys = (uint32_t *)VIRT_TO_PHYS((uint32_t)&boot_pd);\n    process_t *idle = process_create(\"idle\", idle_process, kpd_phys, 0);\n    scheduler_add_process(idle);\n    // Create the three kernel demo processes\n    process_t *pa = process_create(\"proc_a\", process_a, kpd_phys, 0);\n    process_t *pb = process_create(\"proc_b\", process_b, kpd_phys, 0);\n    process_t *pc = process_create(\"proc_c\", process_c, kpd_phys, 0);\n    scheduler_add_process(pa);\n    scheduler_add_process(pb);\n    scheduler_add_process(pc);\n    // Create a user-mode process\n    uint32_t *upd = create_user_page_directory();\n    // Map the user process code into the user page directory\n    // (In a real OS, this loads an ELF binary; here we just map a known function)\n    uint32_t user_code_phys = VIRT_TO_PHYS((uint32_t)user_process_entry);\n    uint32_t user_code_virt = 0x00400000; // Standard user code address\n    paging_map((page_directory_t *)(user_code_phys + KERNEL_VIRT_BASE),\n               user_code_virt, user_code_phys,\n               PTE_PRESENT | PTE_USER); // Readable by user, not writable\n    process_t *upr = process_create(\"user_proc\", (void *)user_code_virt, upd, 1);\n    scheduler_add_process(upr);\n    kprintf(\"[OK] Processes created. Starting scheduler...\\n\");\n    // Enable interrupts AND bootstrap the first process\n    // The timer will immediately begin firing and the scheduler will run\n    __asm__ volatile (\"sti\");\n    // Bootstrap into the idle process (or the first non-idle process)\n    // After this, kernel_main's stack is abandoned\n    current_process = pa; // Start with proc_a\n    pa->state = PROCESS_RUNNING;\n    tss_set_kernel_stack(pa->kernel_stack_top); // TSS must be correct before first interrupt\n    __asm__ volatile (\n        \"mov esp, %0\\n\"\n        \"popfd\\n\"\n        \"pop ebp\\n\"\n        \"pop edi\\n\"\n        \"pop esi\\n\"\n        \"pop ebx\\n\"\n        \"ret\\n\"\n        : : \"r\"(pa->context.esp)\n    );\n    __builtin_unreachable();\n}\nvoid idle_process(void) {\n    while (1) __asm__ volatile (\"hlt\");\n}\n```\n---\n## Debugging This Milestone\n{{DIAGRAM:diag-m4-context-switch-stacks}}\nContext switch bugs are uniquely difficult to debug because they are stateful and timing-dependent. Here are the most common failure modes and how to diagnose each:\n**System freezes immediately after first scheduling:**\n- *Cause*: EFLAGS saved with IF=0. `popfd` in context_switch_asm restores it, leaving interrupts permanently disabled.\n- *Diagnosis*: QEMU `-d int` will show zero interrupts after the freeze. Print `frame->eflags & 0x200` before the first switch \u2014 it must be nonzero.\n- *Fix*: Ensure fabricated EFLAGS has bit 9 set: `0x00000202`.\n**System triple-faults on first timer interrupt in a user process:**\n- *Cause*: TSS `ESP0` not updated, or TSS not loaded (`ltr` not called). CPU uses wrong kernel stack.\n- *Diagnosis*: QEMU `-d int` shows fault at the exact timer tick after context switch.\n- *Fix*: Verify `tss_init()` calls `ltr`; verify `tss_set_kernel_stack` is called in `context_switch`.\n**User process causes page fault immediately on entry:**\n- *Cause*: `enter_user_mode` uses wrong CS/SS selector values, or user code page was mapped with supervisor-only permissions (missing `PTE_USER`).\n- *Diagnosis*: Page fault handler prints CR2 \u2014 if CR2 equals `user_code_virt`, the PTE_USER bit is missing. If CR2 is a garbage address, EIP was wrong.\n- *Fix*: Verify `paging_map` call for user code uses `PTE_PRESENT | PTE_USER`. Verify CS = 0x1B not 0x18.\n**`sys_write` causes page fault on user buffer access:**\n- *Cause*: User passes a kernel virtual address, or the buffer crosses a page boundary where only the first page is mapped.\n- *Diagnosis*: CR2 in the fault handler shows the address that was accessed. Compare to what the user passed in ECX.\n- *Fix*: User pointer validation \u2014 reject addresses >= 0xC0000000. For multi-page buffers, validate each page.\n**Processes corrupt each other's output or crash each other:**\n- *Cause*: Wrong page directory loaded, or kernel PDE sharing broken. Process B's writes affect process A's mapped pages.\n- *Diagnosis*: QEMU `info pg` after a context switch \u2014 verify CR3 matches the expected process's PD physical address.\n- *Fix*: Verify that `create_user_page_directory` correctly copies kernel PDEs without sharing user PTEs.\n**GDB for context switch debugging:**\n```bash\n(gdb) target remote :1234\n(gdb) set architecture i386\n(gdb) break context_switch_asm\n(gdb) commands\n> info registers\n> x/20x $esp\n> continue\n> end\n(gdb) continue\n```\nExamining the stack at the point of the ESP swap:\n```gdb\n(gdb) break *context_switch_asm+32   # Breakpoint at 'mov esp, [eax+16]'\n(gdb) p $esp                          # Before swap: old process's ESP\n(gdb) stepi                           # Execute the swap\n(gdb) p $esp                          # After swap: new process's ESP\n(gdb) x/6x $esp                       # Inspect new stack: should show EFLAGS, EBP, EDI, ESI, EBX, EIP\n```\n---\n## Three-Level View: What Happens Every 10ms\nAt 100Hz, every 10ms, a timer interrupt fires. Here is the complete hardware-to-software trace:\n**Level 3 \u2014 Hardware:**\nThe PIT's channel 0 counter reaches zero, asserts its output line. The 8259 master PIC sees IRQ0 active, checks that it's not masked, asserts the CPU's INTR pin. The CPU finishes its current user-mode instruction, detects INTR, checks that EFLAGS.IF=1, signals acknowledgment. The PIC provides vector 32. The CPU looks up IDT[32], reads the interrupt gate descriptor (handler address + selector `0x08`). CPL (from CS bits 0-1) = 3, IDT gate DPL = 0 \u2192 privilege change required. CPU reads TSS.ESP0 (\u2248 current process's kernel stack top). CPU pushes: old SS (0x23), old ESP (user stack pointer), old EFLAGS, old CS (0x1B), old EIP (interrupted user instruction) \u2014 onto the kernel stack at TSS.ESP0. CPU loads CS = 0x08, EIP = handler address, ESP = TSS.ESP0 - 20 (below the five pushed values).\n**Level 2 \u2014 Kernel:**\n`isr_32` runs: pushes 0 (no error code) and 32 (interrupt number). `isr_common_stub`: `pusha`, push segment regs, reload kernel data segments (0x10), push ESP (pointer to full saved frame), call `interrupt_dispatch`. Dispatch routes to `irq_dispatch(0, frame)` \u2192 `timer_handler(frame)` \u2192 `tick_counter++` \u2192 `scheduler_tick(frame)`. Scheduler finds next READY process (different PID). Sets old state = READY, new state = RUNNING. Calls `context_switch(old, new)`: updates CR3, updates TSS.ESP0, updates `current_process`, calls `context_switch_asm`. `context_switch_asm` pushes EBX, ESI, EDI, EBP, EFLAGS (of the timer handler, i.e., kernel state with IF=0 since we're in an interrupt gate), saves old ESP. Loads new ESP (the new process's kernel stack). Pops EFLAGS (new process's: IF=1 \u2192 interrupts re-enabled), EBP, EDI, ESI, EBX. `ret` pops new process's saved EIP \u2014 either the address after `context_switch_asm` from when new was previously preempted, or `isr_common_stub`'s return path. That path pops segment regs, `popa`, `add esp, 8`, `iretd`. `iretd` pops EIP (user code instruction pointer), CS (0x1B \u2192 ring 3), EFLAGS (user's, with IF=1), ESP (user stack), SS (0x23 \u2192 user stack).\n**Level 1 \u2014 User Process:**\nThe user process resumes at the exact instruction where it was interrupted, with all its registers restored exactly as they were. From the process's perspective, nothing happened. Time has passed (the tick counter advanced), but the process has no observable evidence of having been preempted. This is the fundamental illusion of multitasking: the hardware mechanisms you just built make each process believe it owns the CPU.\n---\n## System Awareness: The Complete Picture\n\n![OS Kernel \u2014 Satellite System Map](./diagrams/diag-satellite-os-map.svg)\n\nYou have now built an operating system. Not a complete one \u2014 there is no filesystem, no networking, no device driver framework \u2014 but all of the mechanisms that define what an OS fundamentally is:\n**What you have:** Hardware interrupt handling that preempts any process at any time. Page tables that isolate each process's address space in hardware. A scheduler that maintains the illusion of concurrent execution across multiple processes. A TSS that enables safe ring 0/ring 3 transitions. A system call interface that is the controlled bridge between user and kernel. A kernel heap that provides dynamic memory to kernel subsystems.\n**The dependency chain, complete:** You cannot have system calls without TSS (ring 3 interrupts need a kernel stack). You cannot have user-mode processes without page directories (ring 3 needs isolated address spaces). You cannot have page directories without the frame allocator (page tables need physical frames). You cannot have the frame allocator without interrupt handling (page faults need a handler). You cannot have interrupt handling without the GDT (IDT gates need valid segment selectors). Every milestone was load-bearing. Nothing was decorative.\n**What you could build next:** A filesystem (read disk sectors, parse a simple format like FAT16 or ext2). A `fork()` syscall (duplicate the current process's page directory using copy-on-write). An `exec()` syscall (load an ELF binary into the current address space). POSIX pipes (two processes sharing a circular buffer \u2014 you built this in Milestone 2). TCP/IP networking (a NIC interrupt handler feeding packets into a ring buffer). You now have the machinery to implement any of these.\n---\n## Knowledge Cascade\n**1. Green Threads, Goroutines, and Async/Await \u2014 Software Context Switching.**\nThe hardware context switch you just implemented \u2014 save registers, swap ESP, restore registers \u2014 is the exact same logical operation performed by Go's goroutine scheduler, Rust's async executor, and JavaScript's event loop. Go's `runtime.mcall()` and `runtime.goexit0()` do in software what your `context_switch_asm` does with hardware assistance. The difference: hardware context switches happen at any instruction (preemptive, timer-driven), while Go's scheduler is cooperative-with-preemption (since Go 1.14, goroutines can be preempted at function call sites via signal-based preemption \u2014 a software approximation of your hardware timer interrupt). JavaScript promises are cooperative and single-threaded: `yield`/`await` are explicit register saves to a heap-allocated coroutine frame. Understanding that `await` is just \"save my registers to the heap, give the scheduler my continuation\" \u2014 and that your PCB is the heap-allocated continuation frame \u2014 makes the entire async ecosystem legible. The Go scheduler's P/M/G model (Goroutine/Machine/Processor) is a software implementation of exactly the process/CPU-thread/scheduler structure you just built in hardware.\n**2. Spectre, Meltdown, and KPTI \u2014 When the Normal Mechanism Becomes the Vulnerability.**\nYour per-process page directories and the ring 0/3 transition via the TSS are the exact mechanisms that Meltdown exploited and KPTI (Kernel Page Table Isolation) was designed to patch. Meltdown worked because the kernel was mapped in every process's page directory (you did this \u2014 \"copy kernel PDEs into every user page directory\"). Even though user mode can't read kernel pages (supervisor bit), speculative execution would *temporarily* access them anyway, leaving cache timing side channels. KPTI's fix: maintain two page directories per process \u2014 one for user mode (minimal kernel mapping, just enough for the syscall entry point) and one for kernel mode (full mapping). On every `iret` to user mode, CR3 switches to the minimal page directory. On every interrupt/syscall, CR3 switches back to the full kernel directory. This costs a TLB flush on every ring transition \u2014 hence the 5\u201330% performance overhead KPTI introduced, which is why server workloads (many syscalls) were hit hardest. You now understand the mechanism well enough that the vulnerability and the mitigation are both obvious.\n**3. The Mars Pathfinder Bug \u2014 Priority Inversion and Real-Time Scheduling.**\nThe critical section during your context switch (interrupts disabled while swapping ESP) is the same primitive that caused the famous Mars Pathfinder mission to reset repeatedly in 1997. Pathfinder's VxWorks kernel had a shared resource (an information bus) accessed by a high-priority meteorological task, a medium-priority communications task, and a low-priority data-collection task. The low-priority task held a mutex when it was preempted by the medium-priority task, which couldn't acquire the mutex, blocking it indefinitely because the medium-priority task was *still running* and preventing the low-priority task from releasing the mutex. The high-priority meteorological task, waiting for the bus, triggered a watchdog timer. The fix was **priority inheritance**: when a low-priority task holds a resource needed by a high-priority task, temporarily elevate the low-priority task's priority until it releases the resource. Linux implements this in futexes (`FUTEX_LOCK_PI`). Your context switch's `cli`/`sti` pairing is a very short critical section \u2014 only a few instructions \u2014 specifically to avoid exactly this class of hazard. Real-time kernels (Zephyr, FreeRTOS) are engineered to keep critical sections bounded to microseconds for this reason.\n**4. Container Isolation \u2014 What Docker Actually Guarantees (and Doesn't).**\nYour user-mode processes running in ring 3 with per-process page directories give you **memory isolation**: one process cannot read or write another's memory (hardware enforced via the supervisor bit, as you demonstrated with the intentional page fault test). Docker containers add a second layer: **namespace isolation** (separate PID, network, mount, and user namespaces via Linux `clone(CLONE_NEWPID|CLONE_NEWNET|...)`), and **resource control** (cgroups limit CPU, memory, and I/O). But the underlying isolation mechanism is exactly what you built: each container process runs in ring 3, the kernel enforces address space isolation through page tables, and system calls are the audited path through which container processes request kernel services. What containers do NOT provide (unlike virtual machines): separate page tables for kernel memory (all containers share the same kernel, including kernel vulnerabilities), and hardware-enforced CPU privilege separation between container and host OS. The Meltdown vulnerability (mentioned above) affected containers because they share a kernel. VMs (with KPTI-like VM/hypervisor page tables) are substantially more isolated. Your PCB-per-process is the atomic unit that container runtimes wrap with namespace and cgroup metadata.\n**5. io_uring and the Anatomy of System Call Overhead.**\nEvery `int 0x80` in your implementation burns a ring transition: ~100\u2013300 ns of privilege switching, TLB pressure from TSS reads, and the `pusha`/`popa` overhead in `isr_common_stub`. For `sys_write` called once per log line, this is negligible. For a web server handling 100,000 requests/second, it is a bottleneck. Linux's response was `io_uring` (2019), which uses two shared ring buffers between kernel and user space \u2014 a Submission Queue and a Completion Queue, the exact circular buffer structure from your Milestone 2 keyboard driver. User code posts I/O descriptors to the SQ (no syscall, just memory writes). The kernel polls the SQ periodically (or when explicitly entered) and posts completions to the CQ. One `io_uring_enter` syscall can process thousands of I/O operations. The syscall overhead is amortized to near-zero per operation. DPDK (the Data Plane Development Kit for high-performance networking) goes further: it bypasses the kernel entirely, having user-space drivers poll NIC hardware directly \u2014 eliminating the ring transition for network I/O entirely. Both approaches are responses to the fundamental cost you just measured: a ring transition is not free, and when done millions of times per second, it matters.\n---\n<!-- END_MS -->\n",
  "current_ms_index": 4,
  "diagrams_to_generate": [
    {
      "id": "diag-m3-e820-memory-map",
      "title": "E820 Memory Map \u2014 Physical Memory Discovery",
      "description": "A structure_layout showing a typical E820 memory map from QEMU with 128MB RAM: 0x0-0x9FFFF (usable, 640KB), 0xA0000-0xFFFFF (reserved, VGA/ROM), 0x100000-0x7FFFFFF (usable, ~127MB), reserved ACPI regions. Shows the E820 entry structure (base, length, type) and how the frame allocator must skip reserved regions and the kernel's own memory.",
      "anchor_target": "anchor-m3-memory",
      "level": "street"
    },
    {
      "id": "diag-m3-bitmap-allocator",
      "title": "Bitmap Frame Allocator \u2014 Structure and Operations",
      "description": "A structure_layout showing the bitmap array where each bit represents a 4KB physical frame: bit 0 = frame at 0x0, bit 1 = frame at 0x1000, etc. Shows the initial state with kernel frames, page table frames, and reserved regions marked as allocated. Demonstrates alloc (find first zero bit, set it, return frame address) and free (clear bit with double-free check). Includes the byte offset calculation: frame_number / 8 = byte index, frame_number % 8 = bit index.",
      "anchor_target": "anchor-m3-memory",
      "level": "microscopic"
    },
    {
      "id": "diag-m3-two-level-page-table",
      "title": "x86 Two-Level Paging \u2014 Virtual Address Translation",
      "description": "A data_walk showing a 32-bit virtual address split into PD index (bits 31:22, 10 bits \u2192 1024 entries), PT index (bits 21:12, 10 bits \u2192 1024 entries), and page offset (bits 11:0, 12 bits \u2192 4096 bytes). Traces CR3 \u2192 page directory \u2192 PDE \u2192 page table \u2192 PTE \u2192 physical frame + offset. Shows the 4MB coverage per PDE (1024 PTEs \u00d7 4KB) and total 4GB addressable space (1024 PDEs \u00d7 4MB).",
      "anchor_target": "anchor-m3-memory",
      "level": "street"
    },
    {
      "id": "diag-m3-pde-pte-bitfield",
      "title": "Page Directory/Table Entry \u2014 Bit-Level Layout",
      "description": "A structure_layout showing the 32-bit PDE and PTE formats: physical address (bits 31:12, page-aligned), AVL (bits 11:9), G (bit 8), PAT/0 (bit 7), D dirty (bit 6, PTE only), A accessed (bit 5), PCD (bit 4), PWT (bit 3), U/S user/supervisor (bit 2), R/W read/write (bit 1), P present (bit 0). Shows example entries for kernel pages (P=1, R/W=1, U/S=0) vs user pages (P=1, R/W=1, U/S=1). Highlights that the address field is only 20 bits because pages are 4KB-aligned.",
      "anchor_target": "anchor-m3-memory",
      "level": "microscopic"
    },
    {
      "id": "diag-m3-identity-plus-higher-half",
      "title": "Dual Mapping \u2014 Identity Map + Higher-Half Kernel",
      "description": "A before_after diagram showing virtual address space layout. Shows the identity map (virtual 0x0 \u2192 physical 0x0 for first N MB) coexisting with the higher-half map (virtual 0xC0000000 \u2192 physical 0x100000 for kernel). Explains why both are needed during the transition: code executing at physical address must still work after paging is enabled (identity map), but the kernel's linked addresses are at 0xC0000000+ (higher-half). Shows which PDE entries are populated for each mapping.",
      "anchor_target": "anchor-m3-memory",
      "level": "street"
    },
    {
      "id": "diag-m3-paging-enable-moment",
      "title": "The Paging Enable Moment \u2014 CR3 and CR0.PG",
      "description": "A state_evolution diagram showing the exact sequence: 1) Build page tables in physical memory, 2) Load CR3 with page directory physical address, 3) Set CR0.PG \u2014 AT THIS EXACT INSTRUCTION the MMU activates: the instruction that set CR0.PG was fetched using a physical address, but the NEXT instruction is fetched using a virtual address. Shows why identity mapping the currently executing code is mandatory. Shows the subsequent jump to the higher-half address to 'escape' the identity map.",
      "anchor_target": "anchor-m3-memory",
      "level": "microscopic"
    },
    {
      "id": "diag-m3-tlb-flush-scenarios",
      "title": "TLB \u2014 Caching Translations and Flush Requirements",
      "description": "A before_after diagram showing the TLB as a cache of virtual\u2192physical translations. Shows a scenario where a page table entry is modified (e.g., changing permissions) but the TLB still holds the old translation, causing the CPU to use stale permissions. Demonstrates invlpg (flushes one entry) vs CR3 reload (flushes all entries). Annotates the performance trade-off: invlpg is O(1) but per-page, CR3 reload flushes everything including still-valid entries.",
      "anchor_target": "anchor-m3-memory",
      "level": "street"
    },
    {
      "id": "diag-m3-page-fault-error-code",
      "title": "Page Fault (#14) \u2014 CR2 and Error Code Decoding",
      "description": "A trace_example showing a page fault scenario: user process accesses 0xC0100000 (kernel memory) \u2192 CPU walks page table \u2192 PTE has U/S=0 (supervisor only) \u2192 page fault \u2192 CPU pushes error code (bits: P=1 present, W/R=0 read, U/S=1 user mode) and stores 0xC0100000 in CR2. Shows how to decode the error code bits to determine fault cause. Contrasts with a not-present fault (P=0) for demand paging.",
      "anchor_target": "anchor-m3-memory",
      "level": "microscopic"
    },
    {
      "id": "diag-m3-kernel-heap-architecture",
      "title": "Kernel Heap (kmalloc/kfree) \u2014 Virtual Memory Backing",
      "description": "A structure_layout showing the kernel heap occupying a virtual address range (e.g., 0xD0000000-0xDFFFFFFF), with the heap allocator (simple free-list or boundary-tag) managing sub-page allocations, and each page of heap space backed by a physical frame obtained from the bitmap allocator. Shows the layered relationship: kmalloc \u2192 heap free-list \u2192 page allocator \u2192 bitmap \u2192 physical frames.",
      "anchor_target": "anchor-m3-memory",
      "level": "street"
    },
    {
      "id": "diag-m3-virtual-address-space-layout",
      "title": "Complete Virtual Address Space Layout",
      "description": "A structure_layout showing the full 4GB virtual address space after paging is enabled: 0x0-0xBFFFFFFF (user space, 3GB), 0xC0000000-0xC0FFFFFF (kernel code/data, higher-half), 0xC1000000+ (kernel heap), VGA at identity-mapped 0xB8000, page tables recursively mapped (optional). Shows the clean separation between user and kernel memory regions.",
      "anchor_target": "anchor-m3-memory",
      "level": "street"
    },
    {
      "id": "diag-m4-pcb-structure-layout",
      "title": "Process Control Block \u2014 Byte-Level Structure",
      "description": "A structure_layout showing the PCB struct with exact field offsets: PID (offset 0, 4 bytes), state enum (offset 4, 4 bytes), EIP (offset 8), ESP (offset 12), EBP (offset 16), EAX-EDI (offsets 20-48), EFLAGS (offset 52), CR3/page_directory (offset 56), kernel_stack_top (offset 60), next pointer for ready queue (offset 64). Shows total size and cache line analysis (fits in 1-2 cache lines). Annotates which fields are saved by hardware vs software during context switch.",
      "anchor_target": "anchor-m4-processes",
      "level": "microscopic"
    },
    {
      "id": "diag-m4-context-switch-assembly",
      "title": "Context Switch \u2014 Register Save/Restore Trace",
      "description": "A data_walk tracing the assembly context switch instruction by instruction: push EAX through push EDI \u2192 save ESP to old_pcb.esp \u2192 load ESP from new_pcb.esp \u2192 pop EDI through pop EAX \u2192 ret (pops new EIP). Shows the stack contents at each step, highlighting the moment ESP changes as the 'identity swap'. Color-codes which stack is active (old process blue, new process green).",
      "anchor_target": "anchor-m4-processes",
      "level": "microscopic"
    },
    {
      "id": "diag-m4-context-switch-stacks",
      "title": "Two Kernel Stacks \u2014 The ESP Swap Moment",
      "description": "A before_after diagram showing two kernel stacks side by side. BEFORE: ESP points to process A's stack with saved registers. The single instruction 'mov esp, [new_pcb + ESP_OFFSET]' switches. AFTER: ESP points to process B's stack with its previously saved registers. Shows that push/pop before the swap affect stack A, and push/pop after affect stack B. This is the 'metamorphosis' moment.",
      "anchor_target": "anchor-m4-processes",
      "level": "street"
    },
    {
      "id": "diag-m4-tss-structure",
      "title": "Task State Segment \u2014 Structure and SS0:ESP0",
      "description": "A structure_layout showing the 104-byte TSS with all fields, highlighting SS0 (offset 0x08) and ESP0 (offset 0x04) as the critical fields for ring 3\u21920 transitions. Shows the TSS descriptor in the GDT (entry at index 5, selector 0x28), the ltr instruction to load it, and the requirement to update ESP0 on every context switch. All other TSS fields can be zero for software task switching.",
      "anchor_target": "anchor-m4-processes",
      "level": "microscopic"
    },
    {
      "id": "diag-m4-ring-transition-mechanism",
      "title": "Ring 3 \u2192 Ring 0 \u2014 The Complete Privilege Transition",
      "description": "A data_walk tracing a user-mode interrupt (INT 0x80 syscall): CPU detects CPL=3\u2192DPL=0 transition \u2192 reads SS0:ESP0 from TSS \u2192 switches to kernel stack \u2192 pushes user SS, user ESP, EFLAGS, user CS, user EIP \u2192 jumps to IDT handler \u2192 handler runs in ring 0. On return: iret pops EIP, CS, EFLAGS, ESP, SS \u2192 detects CPL change \u2192 switches back to user stack. Shows both stacks and all pushed/popped values.",
      "anchor_target": "anchor-m4-processes",
      "level": "street"
    },
    {
      "id": "diag-m4-scheduler-flow",
      "title": "Round-Robin Scheduler \u2014 Timer Interrupt to Context Switch",
      "description": "A state_evolution diagram showing the complete scheduling flow: Process A running \u2192 PIT fires IRQ0 \u2192 interrupt pushes registers \u2192 scheduler function called \u2192 scheduler picks next ready process (B) from circular queue \u2192 context switch to B \u2192 B's registers restored \u2192 iret returns to B's code. Shows the ready queue as a circular linked list of PCBs with the current pointer advancing on each tick.",
      "anchor_target": "anchor-m4-processes",
      "level": "street"
    },
    {
      "id": "diag-m4-process-states",
      "title": "Process State Machine \u2014 Ready, Running, Blocked",
      "description": "A state_evolution diagram showing process state transitions: CREATED \u2192 READY (added to ready queue), READY \u2192 RUNNING (selected by scheduler), RUNNING \u2192 READY (timer preemption), RUNNING \u2192 BLOCKED (waiting for I/O or syscall), BLOCKED \u2192 READY (I/O complete or event), RUNNING \u2192 TERMINATED (sys_exit). Annotates each transition with what triggers it and what kernel code performs it.",
      "anchor_target": "anchor-m4-processes",
      "level": "street"
    },
    {
      "id": "diag-m4-user-mode-page-directory",
      "title": "Per-Process Page Directory \u2014 User/Kernel Split",
      "description": "A structure_layout showing two page directories side by side (Process A and Process B). Both share the same kernel page table entries (PDEs 768-1023 for 0xC0000000+) but have different user-space entries (PDEs 0-767). Shows that kernel pages have U/S=0 (supervisor only) while user pages have U/S=1. Demonstrates how switching CR3 changes the user-space mapping while kernel space remains identical.",
      "anchor_target": "anchor-m4-processes",
      "level": "street"
    },
    {
      "id": "diag-m4-syscall-dispatch",
      "title": "System Call Dispatch \u2014 INT 0x80 to Handler Table",
      "description": "A data_walk showing a user process calling sys_write: user code sets EAX=1 (syscall number), EBX=fd, ECX=buffer pointer, EDX=length \u2192 INT 0x80 \u2192 ring transition via TSS \u2192 syscall handler reads EAX \u2192 indexes into syscall_table[EAX] \u2192 calls sys_write_impl(ebx, ecx, edx) \u2192 return value placed in EAX \u2192 iret back to user mode. Shows the syscall table as a function pointer array.",
      "anchor_target": "anchor-m4-processes",
      "level": "street"
    },
    {
      "id": "diag-m4-interrupt-reentrancy",
      "title": "Interrupt Enable/Disable \u2014 Critical Section During Switch",
      "description": "A trace_example showing the dangerous window: timer interrupt fires \u2192 scheduler begins context switch \u2192 if interrupts are enabled, another timer interrupt could fire mid-switch \u2192 nested switch corrupts partially-saved state. Shows the cli/sti placement: cli before saving registers, switch ESP, then sti after the new process is fully loaded. Annotates that EFLAGS.IF in the new process's saved state must have interrupts enabled or the process freezes.",
      "anchor_target": "anchor-m4-processes",
      "level": "microscopic"
    },
    {
      "id": "diag-m4-three-process-demo",
      "title": "Three-Process Demo \u2014 Interleaved Execution Timeline",
      "description": "A trace_example showing a timeline of three kernel-mode processes (A, B, C) each printing to different VGA screen regions. Shows timer ticks interleaving execution: A runs for 10ms \u2192 tick \u2192 switch to B \u2192 B runs 10ms \u2192 tick \u2192 switch to C \u2192 tick \u2192 back to A. Demonstrates preemptive behavior: processes don't yield voluntarily, the timer forces switches. Shows VGA output growing in three separate regions simultaneously.",
      "anchor_target": "anchor-m4-processes",
      "level": "street"
    }
  ],
  "diagram_attempt": 0,
  "current_diagram_code": null,
  "current_diagram_meta": null,
  "last_error": null,
  "status": "visualizing",
  "phase": "atlas",
  "tdd_blueprint": {},
  "tdd_accumulated_md": "",
  "tdd_current_mod_index": 0,
  "tdd_diagrams_to_generate": [],
  "external_reading": "",
  "running_criteria": [
    {
      "milestone_id": "build-os-m1",
      "criteria": [
        "Bootloader stage 1 fits within 510 bytes (512 bytes minus 2-byte boot signature) and ends with the little-endian boot signature 0x55AA at bytes 510-511 of sector 0",
        "Stage 1 bootloader uses BIOS INT 13h (function 0x02 or 0x42 for LBA) to load the stage 2 loader or kernel binary from disk into a valid memory region",
        "Kernel binary is loaded to physical address 0x100000 (1MB mark) before protected mode transition completes; stage 2 may load to a staging address and copy above 1MB after A20 is enabled",
        "A20 line is explicitly enabled using at least one method (BIOS INT 15h/2401, fast A20 via port 0x92 bit 1, or 8042 keyboard controller); A20 enable is verified before loading above 1MB",
        "GDT contains exactly 5 entries: index 0 (null descriptor, all-zero 8 bytes), index 1 (kernel code, selector 0x08, base=0, limit=4GB, access=0x9A, flags=0xCF), index 2 (kernel data, selector 0x10, base=0, limit=4GB, access=0x92, flags=0xCF), index 3 (user code, selector 0x18/0x1B, base=0, limit=4GB, access=0xFA, DPL=3), index 4 (user data, selector 0x20/0x23, base=0, limit=4GB, access=0xF2, DPL=3)",
        "GDTR is loaded with the lgdt instruction (pointing to a struct containing the 16-bit GDT size limit and 32-bit GDT linear base address) before CR0.PE is set",
        "Interrupts are disabled (cli) before loading the GDT and before setting CR0.PE; they remain disabled until the IDT is configured in Milestone 2",
        "Protected mode is entered by reading CR0, ORing bit 0 (PE) to 1, and writing back to CR0",
        "A far jump (jmp 0x08:label) is executed as the instruction immediately following the CR0.PE write; this atomically loads CS with the kernel code segment selector (0x08) and flushes the CPU instruction prefetch queue",
        "After the far jump, all data segment registers (DS, ES, FS, GS, SS) are loaded with the kernel data selector (0x10) using explicit mov reg, ax instructions",
        "32-bit stack pointer (ESP) is initialized to a valid, writable memory region (e.g., 0x9FC00 or a BSS-allocated stack buffer) before any C function is called",
        "Kernel entry assembly function zeroes the BSS section by iterating from the __bss_start symbol to the __bss_end symbol (both exported by the linker script) using rep stosb or equivalent",
        "Direction flag is cleared (cld) and EBP is set to 0 before calling the C entry point (kernel_main or equivalent)",
        "VGA text mode driver writes 16-bit entries (character byte + color attribute byte) to the volatile uint16_t array at physical address 0xB8000; implements putchar, newline handling, and screen scrolling",
        "VGA driver uses the volatile qualifier on the framebuffer pointer to prevent compiler optimization of writes; color attribute format is correct (bits 7:4 background, bits 3:0 foreground)",
        "Serial port COM1 (base I/O port 0x3F8) is initialized with: DLAB set (port+3 = 0x80), divisor = 1 for 115200 baud (port+0 = 0x01, port+1 = 0x00), 8N1 format (port+3 = 0x03), FIFO enabled (port+2 = 0xC7)",
        "Serial output polls the transmitter holding register empty bit (bit 5 of port+5) before each byte write",
        "kprintf function supports format specifiers %c (character), %s (null-terminated string), %d (signed decimal integer), %x (unsigned hex without prefix), and %p (hex with 0x prefix); output goes to both VGA and serial simultaneously",
        "Linker script defines ENTRY() pointing to the kernel entry assembly symbol; places .text section at 0x100000 with subsequent .rodata, .data, and .bss sections each 4KB-aligned; exports __bss_start, __bss_end, and __kernel_end symbols",
        "Build system uses an i686-elf cross-compiler (not the host gcc) with -ffreestanding -fno-stack-protector -fno-builtin -nostdlib flags; kernel is linked with the custom linker script",
        "Build system produces a raw disk image with stage 1 bootloader at byte offset 0 (sector 0), stage 2 loader starting at sector 2, and kernel binary at a sector offset consistent with the bootloader's INT 13h read parameters",
        "Kernel boots successfully in QEMU with -no-reboot flag (indicating no triple-fault), displays a welcome message on the VGA console, and outputs the same message to the serial port (visible via -serial stdio or -serial file:log.txt)",
        "QEMU invocation includes -d int,cpu_reset to verify no unexpected interrupts or resets occur during boot"
      ]
    },
    {
      "milestone_id": "build-os-m2",
      "criteria": [
        "IDT contains exactly 256 entries (each 8 bytes, total 2048 bytes); entries 0-31 use interrupt gate type (flags=0x8E, selector=0x08); the IDTR is loaded with lidt pointing to the correct base and limit",
        "A unified ISR stub system generates assembly stubs for all 256 vectors using macros; stubs for exceptions 8, 10-14 do not push a fake error code (CPU already pushed one); all other stubs push a dummy error code of 0 to maintain a uniform stack frame layout",
        "The common ISR stub executes pusha then pushes DS, ES, FS, GS before calling the C dispatch function; on exit it pops GS, FS, ES, DS then popa, then removes 8 bytes (int_no + err_code) with add esp 8, then executes iret",
        "The C interrupt_dispatch function receives a pointer to the saved interrupt frame (containing the 8 GP registers from pusha, 4 segment registers, int_no, err_code, and CPU-pushed EIP/CS/EFLAGS) and routes based on int_no",
        "CPU exception handlers 0-31 print: the exception name, vector number, EIP from the frame, CS, EFLAGS, EAX/EBX/ECX/EDX values, and the error code; execution halts after printing",
        "The page fault handler (vector 14) additionally reads CR2 via inline assembly and prints the faulting virtual address and the P/W/U bits of the error code",
        "The double fault handler (vector 8) prints a DOUBLE FAULT message including the error code (always 0) and the EIP field from the interrupt frame, then halts with cli+hlt in an infinite loop",
        "Both 8259 PICs are reinitialized with the four-step ICW1-ICW4 sequence before interrupts are enabled; master PIC is configured with offset 0x20 (IRQ0 maps to vector 32) and slave PIC with offset 0x28 (IRQ8 maps to vector 40)",
        "After PIC remapping, the master PIC data port (0x21) and slave PIC data port (0xA1) have appropriate IRQ masks set; at minimum, all IRQs except IRQ0 (timer) and IRQ1 (keyboard) are masked",
        "EOI (value 0x20) is written to master PIC command port (0x20) after every IRQ0-IRQ7 handler; for IRQ8-IRQ15, EOI is written to both slave command port (0xA0) and master command port (0x20)",
        "PIT channel 0 is programmed in mode 3 (square wave) by writing 0x36 to port 0x43, then writing the divisor low byte then high byte to port 0x40; divisor = 1193182 / frequency, yielding 100Hz with divisor 11931-11932",
        "A volatile uint64_t tick_counter is incremented on every timer IRQ (vector 32) handler invocation; the counter is accessible via a pit_get_ticks() function",
        "The keyboard IRQ handler (vector 33) reads one byte from port 0x60; if bit 7 is set (break/release code), it updates modifier state only; if bit 7 is clear (make code), it looks up the ASCII value using a 128-entry scancode table and pushes non-zero results to the keyboard ring buffer",
        "Left Shift (scancode 0x2A), Right Shift (scancode 0x36), and their release codes (0xAA, 0xB6) are tracked; when shift is active, a second 128-entry shifted scancode table is used for ASCII lookup",
        "The keyboard circular ring buffer has at least 256 bytes capacity; head and tail indices wrap modulo buffer size; ring_push silently drops characters when full; ring_pop returns 0 when empty",
        "sti is executed only after all of the following are complete: IDT loaded (lidt called), PIC remapped (both ICW sequences complete), and PIT initialized; the order in kernel_main enforces this invariant",
        "After enabling interrupts, a busy-wait loop confirms the tick_counter advances by the expected amount (e.g., 100 ticks in approximately 1 second) demonstrating the timer is operational"
      ]
    },
    {
      "milestone_id": "build-os-m3",
      "criteria": [
        "Physical memory map is parsed from the Multiboot information structure (mbi->flags bit 6 checked for validity) before any memory allocation occurs; each E820 region is classified as usable (type 1), reserved (type 2), ACPI reclaimable (type 3), ACPI NVS (type 4), or bad memory (type 5) and printed to the debug console",
        "Bitmap-based physical frame allocator initializes with ALL frames marked used, then selectively frees only regions classified as type-1 usable in the E820 map; frames occupied by the kernel binary (from __kernel_start to __kernel_end), frame 0 (NULL page protection), and any reserved MMIO regions are permanently marked used after freeing",
        "pmm_alloc_frame() scans the bitmap word-by-word (32 bits at a time), returning the physical address of the first free 4KB-aligned frame and marking it used; pmm_free_frame() detects and halts on double-free by checking frame_test() before clearing the bit",
        "Static boot page directory (4KB-aligned) installs at minimum two PDEs: PDE index 0 mapping virtual 0x00000000-0x003FFFFF to physical 0x00000000-0x003FFFFF (identity map), and PDE index 768 (0xC0000000 >> 22) mapping virtual 0xC0000000-0xC03FFFFF to physical 0x00000000-0x003FFFFF (higher-half kernel)",
        "Paging is enabled in the correct sequence: CR3 is loaded with the PHYSICAL address of the page directory (verified by VIRT_TO_PHYS conversion), then CR0 bit 31 (PG) is set; inline assembly includes a 'memory' clobber to prevent compiler reordering of page table writes across the CR0 write",
        "After enabling paging and jumping to the higher-half virtual address (0xC0100000+), the identity map PDE (boot_pd[0]) is cleared and the TLB is flushed by writing CR3 back to itself; subsequent NULL pointer dereferences trigger page fault #14 rather than silently accessing physical address 0",
        "The linker script links the kernel at virtual base 0xC0100000 (VMA) with AT() directives specifying physical LMA = VMA - 0xC0000000; the linker exports __kernel_start, __kernel_end, __bss_start, __bss_end symbols usable from C for BSS zeroing and PMM initialization",
        "paging_map(pd, virt, phys, flags) correctly computes PDE index as virt >> 22 and PTE index as (virt >> 12) & 0x3FF; allocates a new zeroed page table via pmm_alloc_frame() if the PDE is not present; installs the PTE with the correct physical frame address (PTE_FRAME mask applied) and specified flags; calls tlb_flush_page(virt) via the INVLPG instruction before returning",
        "paging_unmap(pd, virt) clears the PTE to 0 and calls tlb_flush_page(virt) before returning; failing to flush is caught in testing by verifying that a remapped page reflects new content rather than stale cache data",
        "Page fault handler (exception 14, vector 14) reads CR2 via inline assembly ('mov %0, cr2') to obtain the faulting virtual address; decodes error code bits: bit 0 (P=present), bit 1 (W=write), bit 2 (U=user mode), bit 4 (I=instruction fetch); prints fault address, access type, and EIP from the interrupt frame to both VGA and serial; halts with 'cli; hlt' on kernel-mode faults",
        "Kernel heap (kmalloc/kfree) reserves virtual range 0xC0400000-0xCFFF0000 for the heap arena; extends the heap by calling pmm_alloc_frame() + paging_map() in PAGE_SIZE increments when no suitable free block exists; each allocation header contains a magic value (e.g., 0xDEADBEEF) validated on every kmalloc and kfree to detect corruption and double-free",
        "kmalloc() implements first-fit block search with block splitting when remaining fragment is larger than sizeof(heap_block_t) + 8 bytes; kfree() coalesces adjacent free blocks (both forward with next block and backward with prev block) to prevent fragmentation; all returned pointers are 8-byte aligned",
        "VGA text buffer at virtual 0xB8000 and serial port registers at I/O port 0x3F8 remain accessible after paging is enabled; the identity map covers physical 0x00000000-0x003FFFFF which includes the VGA buffer at 0xB8000; the kernel can write to VGA immediately after enabling paging without a page fault",
        "A post-initialization test sequence calls kmalloc() for at least two allocations of different sizes, writes a pattern to the allocated memory with memset (verified not to cause page faults), frees one allocation and reallocates a smaller size from it (verifying block reuse), and prints the PMM free frame count; all tests pass without faults in QEMU"
      ]
    },
    {
      "milestone_id": "build-os-m4",
      "criteria": [
        "Process Control Block (PCB) struct stores exactly: uint32_t pid, char name[32], process_state_t state (enum: READY/RUNNING/BLOCKED/DEAD), cpu_context_t context (containing edi, esi, ebx, ebp, esp, eip, eflags fields), uint32_t *page_directory (physical address), uint8_t *kernel_stack, uint32_t kernel_stack_top, and scheduler linkage (next pointer for circular linked list)",
        "cpu_context_t layout matches the exact push/pop order in context_switch_asm: EFLAGS at stack top (pushed last by pushfd), then EBP, EDI, ESI, EBX, then EIP (return address from call instruction at bottom), with ESP saved explicitly into context.esp field via mov instruction",
        "context_switch_asm is implemented in x86 assembly (not C) and correctly saves EFLAGS via pushfd, saves EBX/ESI/EDI/EBP, saves old ESP into old_ctx->esp (offset 16), loads new ESP from new_ctx->esp, restores new process's EBP/EDI/ESI/EBX/EFLAGS via popfd, and returns to new process's saved EIP via ret",
        "The ESP swap in context_switch_asm is the identity boundary: pushes before the swap go onto the old process's kernel stack, pops after the swap come from the new process's kernel stack \u2014 demonstrated by being able to trace a context switch in GDB and observe ESP change to a different memory region at that exact instruction",
        "TSS structure (tss_t) is correctly defined as a packed struct with prev_tss at offset 0, esp0 at offset 4, ss0 at offset 8 (set to 0x10 = kernel data selector), and iomap_base at offset 102; TSS is registered as GDT entry 5 with a system descriptor (access byte 0x89) and loaded into the Task Register via the ltr instruction with selector 0x28",
        "tss_set_kernel_stack() updates kernel_tss.esp0 on every context switch before context_switch_asm is called, so that any interrupt arriving while the new process runs uses the new process's kernel stack top \u2014 verified by causing a timer interrupt during a user process and confirming the interrupt handler runs on the correct kernel stack",
        "Fabricated initial kernel stack for new processes contains (top to bottom): EFLAGS = 0x00000202 (IF=1, reserved bit 1 set), EBP = 0, EDI = 0, ESI = 0, EBX = 0, entry function address (as EIP); context.esp points to this fabricated frame so context_switch_asm can pop it on first scheduling",
        "Round-robin scheduler maintains a circular singly-linked list of READY processes; scheduler_tick() is called from the timer IRQ handler, advances current_process to the next READY process in the list, transitions old state to READY and new state to RUNNING, and calls context_switch() \u2014 all with interrupts disabled (inside interrupt gate handler)",
        "At least 3 kernel-mode processes run concurrently, each writing distinct characters to non-overlapping VGA columns, with visible interleaved output demonstrating that no single process monopolizes the CPU across multiple screen rows",
        "Per-process page directories for user-mode processes copy kernel PDEs (indices 768-1023, covering 0xC0000000+) from the boot page directory so kernel mappings are always accessible, while user PDEs (indices 0-767) are unique per process; user pages mapped with PTE_PRESENT | PTE_WRITABLE | PTE_USER",
        "enter_user_mode() uses iretd with stack frame containing (top to bottom): SS=0x23 (user data, RPL=3), user_esp, EFLAGS with IF=1 (bit 9 set), CS=0x1B (user code, RPL=3), user_eip; all data segment registers (DS/ES/FS/GS) loaded with 0x23 before iretd executes",
        "User-mode process accessing a kernel virtual address (>= 0xC0000000, mapped without PTE_USER) triggers page fault #14 with error code bit 2 (U bit) = 1 and bit 0 (P bit) = 1, confirming hardware-enforced supervisor-only protection; page fault handler prints the faulting CR2 address and error code before halting the offending process",
        "System call gate at IDT vector 0x80 uses flags 0xEF (P=1, DPL=3, trap gate type=1111) so user-mode code can invoke int 0x80 without a general protection fault, and interrupts remain enabled during syscall handling (trap gate does not clear IF)",
        "Syscall dispatch reads syscall number from interrupt_frame->eax, argument 1 from frame->ebx, argument 2 from frame->ecx, argument 3 from frame->edx; return value is written back to frame->eax before iret restores user registers",
        "sys_write (syscall 4) validates that the user buffer pointer is below 0xC0000000 before dereferencing, returns -1 on invalid pointer, and successfully outputs the buffer contents to both VGA and serial for fd=1 (stdout)",
        "sys_exit (syscall 1) marks the current process as PROCESS_DEAD, removes it from the circular ready queue, updates process_list_head if needed, and immediately context-switches to the next READY process without saving the dead process's context",
        "TSS ESP0 field is verified to be non-zero and pointing to a valid kernel stack address after every context switch \u2014 confirmed by inspecting kernel_tss.esp0 in GDB after each scheduling event",
        "An idle process exists in the ready queue that executes 'hlt' in a loop; the scheduler switches to it when no other process is READY, preventing the scheduler from having no runnable process when all others are BLOCKED or DEAD"
      ]
    }
  ],
  "explained_concepts": [
    "segment-descriptors-flat-model",
    "linker-script-bare-metal",
    "real-mode-vs-protected-mode",
    "x86-privilege-rings"
  ],
  "system_diagram_d2": null,
  "system_diagram_iteration": 0,
  "system_diagram_done": false,
  "project_structure_md": "",
  "project_charter_md": ""
}