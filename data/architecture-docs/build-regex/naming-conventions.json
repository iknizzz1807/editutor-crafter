{
  "types": {
    "regex_error_t": "enum: REGEX_SUCCESS, REGEX_ERROR_SYNTAX, REGEX_ERROR_MEMORY, REGEX_ERROR_LIMIT_EXCEEDED, REGEX_ERROR_INTERNAL",
    "regex_error_context_t": "fields: code regex_error_t, position int, message char[256], expected char[64], actual char[64], line_number int, column_number int, context_start int, context_length int",
    "ast_node_t": "fields: type ast_node_type_t, ref_count int, data union",
    "ast_node_type_t": "enum AST_LITERAL, AST_CONCAT, AST_ALTERNATE, AST_STAR, AST_PLUS, AST_QUESTION, AST_CHARSET",
    "nfa_t": "fields: states dynamic_array_t, start_state int, accept_states state_set_t",
    "dfa_t": "fields: states dynamic_array_t, start_state int, minimized bool",
    "charset_t": "fields: bitmap uint32_t[8], negated bool",
    "dynamic_array_t": "fields: items void*, count int, capacity int, item_size int",
    "nfa_fragment_t": "fields: start_state int, accept_state int, parent_nfa nfa_t*",
    "lexer_state_t": "fields: pattern const char*, position int, length int, current_token token_t, line_number int, column_number int",
    "compilation_stats_t": "timing and size metrics",
    "pattern_test_case_t": "fields: pattern const char*, input const char*, should_match bool, test_description const char*, category const char*",
    "token_type_t": "enum TOKEN_LITERAL, TOKEN_STAR, TOKEN_PLUS, TOKEN_QUESTION, TOKEN_PIPE, TOKEN_LPAREN, TOKEN_RPAREN, TOKEN_LBRACKET, TOKEN_RBRACKET, TOKEN_CARET, TOKEN_DASH, TOKEN_ESCAPE, TOKEN_DOT, TOKEN_EOF, TOKEN_CHARSET",
    "token_t": "fields: type token_type_t, position int, data union",
    "state_set_t": "fields: states dynamic_array_t, sorted bool, hash_cache uint32_t, hash_valid bool",
    "nfa_state_t": "fields: state_id int, is_accept bool, char_transitions dynamic_array_t, epsilon_transitions dynamic_array_t",
    "char_transition_t": "fields: character char, destination_state int",
    "dfa_state_t": "fields: state_id int, nfa_states state_set_t, transitions dynamic_array_t, is_accept bool, processed bool",
    "cache_entry_t": "fields: state dfa_state_t*, last_access uint64_t, access_count int, prev cache_entry_t*, next cache_entry_t*",
    "equivalence_group_t": "fields: group_id int, states dynamic_array_t, is_accepting bool, signature_hash uint32_t",
    "minimization_context_t": "fields: groups dynamic_array_t, work_queue dynamic_array_t, partition_changed bool, next_group_id int",
    "lazy_dfa_context_t": "fields: cache_head cache_entry_t*, cache_tail cache_entry_t*, cache_size int, max_cache_size int",
    "compiled_regex_t": "compiled regex with execution engine selection",
    "execution_engine_t": "enum for engine selection",
    "match_result_t": "boolean result with execution details",
    "resource_monitor_t": "fields: ast_nodes_allocated int, nfa_states_allocated int, dfa_states_allocated int, memory_allocated size_t, compilation_start clock_t, recursion_depth int",
    "performance_metrics_t": "fields: compilation_start clock_t, compilation_end clock_t, execution_start clock_t, execution_end clock_t, memory_allocated size_t, nfa_states size_t, dfa_states size_t",
    "test_context_t": "fields: tests_run int, tests_passed int, tests_failed int, current_suite char[128], log_file FILE*",
    "debug_level_t": "enum for debug flags",
    "capture_context_t": "fields: boundaries dynamic_array_t, max_group_id int, tracking_enabled bool",
    "capture_result_t": "fields: group_count int, groups array with start_pos/end_pos/captured_text/was_captured",
    "group_boundary_t": "fields: group_id int, text_position int, is_start bool",
    "utf8_decoder_t": "fields: text const char*, position int, byte_length int, current_codepoint uint32_t, codepoint_bytes int",
    "nfa_group_marker_t": "fields: type nfa_marker_type_t, group_id int",
    "AST_GROUP": "AST node type for capture groups",
    "AST_GROUP_START": "AST node type for group start marker",
    "AST_GROUP_END": "AST node type for group end marker",
    "nfa_config_t": "fields: state_id int, captures capture_context_t*",
    "GROUP_START_STATE": "NFA state type for group entry",
    "GROUP_END_STATE": "NFA state type for group exit",
    "nfa_marker_type_t": "enum: NFA_GROUP_START_MARKER, NFA_GROUP_END_MARKER"
  },
  "methods": {
    "regex_compile(pattern)": "compile pattern string to regex object",
    "regex_match_nfa(regex, text)": "match using NFA simulation",
    "regex_match_dfa(regex, text)": "match using DFA execution",
    "parse_pattern(pattern, error_ctx)": "parse regex string into AST",
    "nfa_from_ast(ast)": "convert AST to NFA",
    "epsilon_closure(nfa, states)": "compute epsilon closure",
    "thompson_construct_recursive(node)": "build NFA fragment for AST node",
    "ast_create_literal(ch)": "create literal AST node",
    "charset_contains(set, ch)": "test character membership in character set",
    "regex_compile_with_stats(pattern, result, stats)": "compile with performance measurement",
    "tokenize_pattern": "convert string to tokens",
    "handle_escape_sequence": "process backslash escapes",
    "parse_character_class": "handle character ranges",
    "advance_lexer_position": "update position tracking",
    "dynamic_array_init": "initialize array",
    "dynamic_array_append": "add item to array",
    "charset_init": "initialize character set",
    "charset_add_char": "add character to set",
    "lexer_next_token": "advance tokenization",
    "ast_create_literal(char)": "create literal character AST node",
    "ast_create_binary(type, left, right)": "create binary operator AST node",
    "ast_create_unary(type, operand)": "create unary operator AST node",
    "charset_init(set)": "initialize empty character set",
    "charset_add_char(set, ch)": "add character to character set",
    "dynamic_array_init(array, item_size, initial_capacity, max_capacity)": "initialize growable array",
    "dynamic_array_append(array, item)": "add item to dynamic array",
    "state_set_add(set, state_id)": "add state to state set",
    "state_set_contains(set, state_id)": "test state membership in set",
    "nfa_add_state(nfa, is_accept)": "add new state to NFA and return state ID",
    "nfa_add_char_transition(nfa, from, ch, to)": "add character transition between states",
    "ast_node_retain(node)": "increment AST node reference count",
    "ast_node_release(node)": "decrement reference count and cleanup",
    "tokenize_pattern(pattern, tokens)": "convert regex string into token sequence",
    "handle_escape_sequence(lexer, token)": "process backslash escape sequences",
    "parse_character_class(lexer, charset)": "parse bracket expressions into character sets",
    "advance_lexer_position(lexer)": "update position and column tracking",
    "lexer_next_token(lexer, token)": "get next token from input stream",
    "charset_add_range(set, start, end)": "add character range to set",
    "charset_negate(set)": "toggle character set negation",
    "parse_pattern(pattern, result, error_ctx)": "parse complete regex into AST",
    "parse_expression(lexer, error_ctx)": "parse alternation expressions",
    "parse_term(lexer, error_ctx)": "parse concatenation terms",
    "parse_factor(lexer, error_ctx)": "parse quantified factors",
    "parse_atom(lexer, error_ctx)": "parse atomic elements",
    "error_context_init(ctx)": "initialize error context structure",
    "error_context_set(ctx, code, position, expected, actual, format, ...)": "set error with formatted message and context",
    "error_context_print(ctx, pattern)": "display formatted error with visual context",
    "nfa_from_ast(ast_root, result_nfa)": "convert complete AST to NFA using Thompson's construction",
    "thompson_construct_recursive(node, result_fragment, parent_nfa)": "build NFA fragment for AST node recursively",
    "nfa_add_char_transition(nfa, from_state, ch, to_state)": "add character transition between states",
    "nfa_add_epsilon_transition(nfa, from_state, to_state)": "add epsilon transition between states",
    "epsilon_closure(nfa, input_states, result_closure)": "compute complete epsilon closure of input state set",
    "epsilon_closure_single(nfa, state_id, result_closure)": "compute epsilon closure of single state",
    "nfa_init(nfa)": "initialize empty NFA structure",
    "nfa_destroy(nfa)": "clean up NFA memory",
    "collect_char_transitions(nfa, states, input_char, destinations)": "gather all valid character transitions from state set",
    "determine_match_success(nfa, final_states, result)": "check if final state set indicates successful match",
    "state_set_clear(set)": "reset state set to empty while preserving allocated capacity",
    "state_set_ensure_sorted(set)": "ensure state set is sorted for binary operations",
    "nfa_to_dfa(nfa, result_dfa) returns regex_error_t": "convert NFA to DFA using subset construction",
    "minimize_dfa_moore(dfa) returns regex_error_t": "minimize DFA using Moore's algorithm",
    "regex_match_lazy_dfa(nfa, text, match_result) returns regex_error_t": "match using lazy DFA construction",
    "state_set_add(set, state_id) returns regex_error_t": "add state to sorted state set",
    "state_set_contains(set, state_id) returns bool": "test state membership",
    "state_set_hash(set) returns uint32_t": "compute hash for fast comparison",
    "state_set_equal(set1, set2) returns bool": "test state set equality",
    "collect_reachable_states(nfa, current_states, input_char, reachable_states) returns regex_error_t": "gather states reachable on character",
    "compute_state_signature(dfa, state_id, partition_groups, signature_hash) returns regex_error_t": "compute transition signature",
    "manage_dfa_cache(context, new_state) returns regex_error_t": "handle LRU cache eviction",
    "epsilon_closure(nfa, states, result_closure) returns regex_error_t": "compute epsilon closure",
    "regex_match_with_engine(regex, text, engine_type, result)": "match using specified engine",
    "parse_pattern(tokens, ast, error_ctx)": "parse token sequence into AST",
    "nfa_from_ast(ast, nfa)": "convert AST to NFA",
    "nfa_to_dfa(nfa, dfa)": "convert NFA to DFA using subset construction",
    "minimize_dfa_moore(dfa)": "minimize DFA using Moore's algorithm",
    "determine_match_success(automaton, final_states, result)": "check if final state set indicates successful match",
    "error_context_init(ctx) returns regex_error_t": "initialize error context structure",
    "error_context_set(ctx, code, position, expected, actual, format, ...) returns regex_error_t": "set error with formatted message and context",
    "resource_monitor_init() returns regex_error_t": "initialize resource monitoring",
    "check_resource_limits(error_ctx) returns regex_error_t": "verify resource allocation within limits",
    "track_allocation(ast_nodes, nfa_states, dfa_states, memory)": "update resource allocation counters",
    "parse_pattern(pattern, result, error_ctx) returns regex_error_t": "parse regex string with comprehensive error handling",
    "analyze_pattern_complexity(ast, error_ctx) returns regex_error_t": "analyze AST for degenerate patterns",
    "nfa_from_ast(ast, result_nfa, error_ctx) returns regex_error_t": "convert AST to NFA with resource protection",
    "performance_start_compilation(metrics)": "begin compilation timing",
    "performance_end_compilation(metrics)": "end compilation timing",
    "performance_report(metrics, test_name)": "display performance results",
    "load_test_cases_from_file(filename, cases, count)": "load test data from file",
    "run_pattern_test_case(test_case)": "execute single test case",
    "compare_ast_structures(expected, actual)": "validate AST equivalence",
    "validate_engine_consistency(pattern, input)": "compare NFA and DFA execution results",
    "benchmark_engine_comparison(pattern, input, iterations)": "measure performance differences",
    "debug_init(level, output)": "initialize debug system with level and output file",
    "debug_print_ast(node, indent_level)": "print AST structure with indentation",
    "debug_print_charset(set, indent_level)": "print character set contents",
    "ast_to_string(node, buffer, buffer_size)": "convert AST back to regex string for verification",
    "trace_nfa_simulation(nfa, input, result)": "trace NFA simulation step by step",
    "analyze_pattern_complexity(ast, error_ctx)": "analyze AST for degenerate patterns",
    "epsilon_closure(nfa, states, result_closure)": "compute epsilon closure of state set",
    "DEBUG_LOG(level, format, ...)": "debug logging macro with level filtering",
    "capture_context_init(ctx)": "initialize capture context structure",
    "capture_boundary_add(ctx, group_id, position, is_start)": "record group boundary during matching",
    "capture_extract_results(ctx, input_text, result)": "extract final capture group results",
    "utf8_decoder_init(decoder, text)": "initialize UTF-8 decoder for input text",
    "utf8_decoder_next(decoder, codepoint)": "decode next UTF-8 character to Unicode code point",
    "unicode_is_letter(codepoint)": "test if Unicode code point is letter character",
    "nfa_simulate_with_captures(nfa, input, captures)": "NFA simulation with capture group tracking",
    "epsilon_closure_with_captures(nfa, states, context, position)": "epsilon closure that processes group markers",
    "nfa_to_dfa(nfa, result_dfa)": "convert NFA to DFA using subset construction",
    "regex_match_lazy_dfa(nfa, text, match_result)": "match using lazy DFA construction",
    "state_set_hash(set)": "compute hash for fast comparison",
    "state_set_equal(set1, set2)": "test state set equality",
    "collect_reachable_states(nfa, current_states, input_char, reachable_states)": "gather states reachable on character",
    "compute_state_signature(dfa, state_id, partition_groups, signature_hash)": "compute transition signature",
    "manage_dfa_cache(context, new_state)": "handle LRU cache eviction",
    "resource_monitor_init()": "initialize resource monitoring",
    "check_resource_limits(error_ctx)": "verify resource allocation within limits"
  },
  "constants": {
    "REGEX_SUCCESS": "successful operation",
    "REGEX_ERROR_SYNTAX": "malformed pattern syntax",
    "AST_LITERAL": "literal character AST node",
    "AST_CONCAT": "concatenation AST node",
    "AST_ALTERNATE": "alternation AST node",
    "AST_STAR": "* quantifier AST node",
    "AST_PLUS": "+ quantifier AST node",
    "AST_QUESTION": "? quantifier AST node",
    "REGEX_ERROR_MEMORY": "memory allocation failure",
    "REGEX_ERROR_LIMIT_EXCEEDED": "pattern exceeds limits",
    "MAX_AST_NODES": "200, maximum AST nodes",
    "MAX_NFA_STATES": "maximum NFA states limit",
    "MAX_DFA_STATES": "maximum DFA states limit",
    "MAX_PATTERN_LENGTH": "500, maximum pattern length",
    "TOKEN_LITERAL": "regular character token",
    "TOKEN_STAR": "* quantifier token",
    "TOKEN_PIPE": "| alternation token",
    "AST_CHARSET": "character class AST node",
    "AST_DOT": "any character node",
    "TOKEN_EOF": "end of input token",
    "DFA_CACHE_SIZE": "100, lazy construction cache size",
    "MAX_STATE_SET_SIZE": "64, maximum states in set",
    "NFA_SIMULATION": "execution engine type",
    "DFA_EXECUTION": "execution engine type",
    "LAZY_DFA_CONSTRUCTION": "execution engine type",
    "REGEX_ERROR_INTERNAL": "internal engine error",
    "DEBUG_NONE": "no debug output",
    "DEBUG_LEXER": "lexer debug flag",
    "DEBUG_PARSER": "parser debug flag",
    "DEBUG_AST": "AST debug flag",
    "DEBUG_NFA": "NFA debug flag",
    "DEBUG_DFA": "DFA debug flag",
    "DEBUG_SIMULATION": "simulation debug flag",
    "DEBUG_ALL": "all debug flags enabled",
    "MAX_CAPTURE_GROUPS": "maximum number of capture groups supported",
    "NFA_GROUP_START_MARKER": "marker type for group start boundary",
    "NFA_GROUP_END_MARKER": "marker type for group end boundary"
  },
  "terms": {
    "Thompson's construction": "algorithm for converting regex to NFA using composable fragments",
    "epsilon transition": "NFA transition without consuming input",
    "subset construction": "algorithm for converting NFA to equivalent DFA",
    "epsilon closure": "set of states reachable via epsilon transitions",
    "automaton": "mathematical model for pattern recognition",
    "catastrophic backtracking": "exponential performance degradation in backtracking regex engines",
    "state explosion": "exponential growth in automaton size due to feature complexity",
    "full string matching": "pattern must match entire input string",
    "compilation pipeline": "sequence of transformations from regex to executable automaton",
    "greedy quantifier semantics": "quantifiers match as many characters as possible",
    "recursive descent": "parsing technique with functions for each grammar rule",
    "operator precedence": "rules determining order of operations in expressions",
    "Abstract Syntax Tree": "hierarchical representation of parsed regex pattern",
    "discriminated union": "tagged union with type field indicating active variant",
    "reference counting": "memory management tracking number of references to object",
    "lexical analysis": "process of converting character stream into tokens",
    "escape sequence": "backslash-prefixed characters with special meanings",
    "character class": "bracket expression matching set of characters",
    "implicit concatenation": "automatic sequencing of adjacent pattern elements",
    "quantifier": "operator specifying repetition count for preceding element",
    "NFA fragment": "modular NFA piece with single start and accept state",
    "fragment composition": "process of connecting NFA fragments into larger automata",
    "non-deterministic": "multiple possible transitions from same state",
    "accept state": "final state indicating successful pattern match",
    "parallel state tracking": "managing multiple active states simultaneously during input processing",
    "state equivalence": "two states indistinguishable by future input",
    "partition refinement": "iterative process splitting equivalence classes",
    "lazy construction": "building DFA states on-demand during matching",
    "LRU eviction": "cache policy removing least recently used entries",
    "transition signature": "characterization of state's transition behavior",
    "execution engine": "component that performs pattern matching using specific algorithm",
    "component interface contract": "API boundaries and data exchange formats between engine components",
    "immutable handoffs": "data transfer between stages without modifying shared state",
    "resource ownership transfer": "explicit handoff of memory management responsibility",
    "resource exhaustion protection": "preventing excessive memory usage and infinite loops",
    "degenerate patterns": "regex patterns that could cause exponential behavior",
    "error recovery": "continuing compilation after syntax errors to find multiple issues",
    "synchronization points": "grammar locations where parsing can safely resume after errors",
    "risk assessment": "analyzing patterns for potential performance issues",
    "pattern normalization": "transforming high-risk patterns into equivalent but efficient forms",
    "compilation strategy selection": "choosing appropriate algorithms based on pattern complexity",
    "pattern equivalence": "different patterns accepting same language",
    "cross-engine validation": "comparing results across different execution engines",
    "performance regression": "degradation in performance characteristics over time",
    "boundary condition": "edge cases at limits of normal operation",
    "integration testing": "end-to-end validation across complete system",
    "pathological patterns": "regex patterns designed to trigger worst-case performance",
    "execution tracing": "logging every state transition and decision during matching",
    "simulation state invariants": "properties preserved during automaton execution",
    "transformation invariants": "properties preserved as data flows through processing stages",
    "state reachability analysis": "tracing paths through automaton to verify connectivity",
    "cross-validation": "comparing results across different execution engines",
    "capture groups": "parenthesized regex groups that extract matched substrings",
    "group markers": "special NFA states that record group boundary positions",
    "Unicode normalization": "converting equivalent Unicode representations to canonical form",
    "case folding": "Unicode-aware case conversion for case-insensitive matching",
    "lookaround assertions": "regex constructs that check conditions without consuming input",
    "backreferences": "regex feature referencing previously captured group content",
    "atomic groups": "non-backtracking regex groups that commit to first match",
    "UTF-8 decoding": "converting multi-byte UTF-8 sequences to Unicode code points",
    "Unicode properties": "character classification system like Letter, Digit, Script"
  }
}