{
  "project_id": "build-sqlite",
  "meta": {
    "id": "build-sqlite",
    "name": "Build Your Own SQLite",
    "description": "Embedded SQL database with tokenizer, parser, bytecode compiler (VDBE), page-based B-tree/B+tree storage, buffer pool, query planner, and ACID transactions via rollback journal and WAL.",
    "difficulty": "expert",
    "estimated_hours": 105,
    "essence": "SQL tokenization and recursive-descent parsing producing ASTs, compiled into bytecode instructions executed by a virtual machine (VDBE), operating over a page-based storage engine with B-trees for clustered table storage and B+trees for secondary indexes, managed through a buffer pool with LRU eviction, and ACID guarantees via either rollback journal or write-ahead logging.",
    "why_important": "Building a database from scratch teaches the fundamental data structures and algorithms underlying all modern databases. These skills\u2014disk I/O management, B-tree indexing, bytecode execution, query optimization, and crash recovery\u2014 are directly applicable to backend engineering, distributed systems, and performance optimization roles.",
    "learning_outcomes": [
      "Build a lexer and recursive-descent parser converting SQL text into an AST",
      "Compile ASTs into bytecode instructions for a virtual machine (VDBE)",
      "Implement a buffer pool manager with LRU eviction and dirty page tracking",
      "Design page-based B-tree storage for tables (clustered) and B+tree for indexes",
      "Implement row storage with variable-length record encoding",
      "Build a cost-based query planner with statistics-driven cardinality estimation",
      "Implement ACID transactions with rollback journal for crash recovery",
      "Design WAL mode for improved concurrent read/write performance"
    ],
    "skills": [
      "SQL Parsing",
      "Bytecode Compilation",
      "Virtual Machine Execution",
      "Buffer Pool Management",
      "B-tree/B+tree Indexing",
      "Query Optimization",
      "Page-Based Storage",
      "Write-Ahead Logging",
      "Transaction Management",
      "Binary File Formats"
    ],
    "tags": [
      "acid",
      "btree",
      "build-from-scratch",
      "databases",
      "expert",
      "persistence",
      "query-engine",
      "sql",
      "virtual-machine"
    ],
    "architecture_doc": "architecture-docs/build-sqlite/index.md",
    "languages": {
      "recommended": [
        "C",
        "Rust",
        "Go"
      ],
      "also_possible": [
        "Java"
      ]
    },
    "resources": [
      {
        "name": "Let's Build a Simple Database",
        "url": "https://cstack.github.io/db_tutorial/",
        "type": "tutorial"
      },
      {
        "name": "SQLite Architecture",
        "url": "https://www.sqlite.org/arch.html",
        "type": "documentation"
      },
      {
        "name": "SQLite File Format",
        "url": "https://www.sqlite.org/fileformat2.html",
        "type": "documentation"
      },
      {
        "name": "SQLite VDBE Documentation",
        "url": "https://www.sqlite.org/vdbe.html",
        "type": "documentation"
      },
      {
        "name": "CMU 15-445 Database Systems",
        "url": "https://15445.courses.cs.cmu.edu",
        "type": "course"
      },
      {
        "name": "CodeCrafters SQLite Challenge",
        "url": "https://app.codecrafters.io/courses/sqlite/overview",
        "type": "tool"
      }
    ],
    "prerequisites": [
      {
        "type": "skill",
        "name": "B-tree data structure"
      },
      {
        "type": "skill",
        "name": "SQL basics"
      },
      {
        "type": "skill",
        "name": "File I/O and binary formats"
      },
      {
        "type": "skill",
        "name": "Basic compiler concepts (lexer, parser, AST)"
      }
    ],
    "milestones": [
      {
        "id": "build-sqlite-m1",
        "name": "SQL Tokenizer",
        "description": "Build a lexer that converts SQL text into a stream of typed tokens.",
        "acceptance_criteria": [
          "Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.) case-insensitively",
          "String literals enclosed in single quotes are parsed including escaped quotes ('it''s' \u2192 it's)",
          "Numeric literals including integers and floating-point values (42, 3.14, -7) are recognized as distinct token types",
          "Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon) are tokenized as distinct tokens",
          "Identifiers (table names, column names) support quoted identifiers with double quotes (\"column name\")",
          "Tokenizer reports error position (line and column) for unrecognized characters",
          "Token stream correctly tokenizes at least 20 diverse SQL statements in a test suite",
          "Tokenizer recognizes keywords like SELECT and INSERT case-insensitively",
          "String literals correctly handle escaped single quotes ('') and preserve internal content",
          "Numeric literals distinguish between integers (42) and floats (3.14)",
          "Double-quoted identifiers ('\"Table Name\"') are correctly tokenized as identifiers",
          "The tokenizer returns a list or stream of objects containing type, value, line, and column",
          "A test suite of 20+ SQL statements produces the expected token sequence"
        ],
        "pitfalls": [
          "Not handling escaped quotes in string literals ('it''s') causes premature string termination",
          "Keywords must be case-insensitive but identifiers may be case-sensitive depending on quoting\u2014handle both",
          "Unicode identifiers require careful handling; start simple with ASCII and document limitations",
          "Negative numbers may be ambiguous with subtraction operator; handle at parser level, not tokenizer"
        ],
        "concepts": [
          "Lexical analysis converts character stream to token stream",
          "Finite state machine drives character-by-character token recognition",
          "Token types classify each lexeme (keyword, identifier, literal, operator)",
          "Error reporting with source location enables useful diagnostics"
        ],
        "skills": [
          "String parsing",
          "State machine implementation",
          "Token classification",
          "Error reporting"
        ],
        "deliverables": [
          "Lexer producing token stream from SQL input string",
          "Keyword recognition for all supported SQL words",
          "String and numeric literal parsing with escape handling",
          "Operator and punctuation tokenization",
          "Error reporting with line and column position",
          "Test suite with at least 20 SQL statements"
        ],
        "estimated_hours": 4
      },
      {
        "id": "build-sqlite-m2",
        "name": "SQL Parser (AST)",
        "description": "Build a recursive-descent parser that converts the token stream into an Abstract Syntax Tree (AST).",
        "acceptance_criteria": [
          "SELECT parser produces AST with column list (including *), FROM clause, optional WHERE, ORDER BY, and LIMIT",
          "INSERT parser produces AST with target table, optional column names, and VALUES clause",
          "CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT, REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison (=,<,>) > AND > OR",
          "Parenthesized expressions override default precedence",
          "Parser produces clear error messages with token position for syntax errors",
          "Test suite covers at least 15 valid and 10 invalid SQL statements",
          "SELECT parser produces AST with column list, FROM, and optional WHERE/LIMIT",
          "INSERT parser handles target table and VALUES mapping",
          "CREATE TABLE parser extracts column names, types, and constraints (PRIMARY KEY, NOT NULL)",
          "Expression parser correctly handles NOT > AND > OR precedence",
          "Parenthesized expressions correctly override default precedence levels",
          "Parser provides error position (line/column) for syntax errors",
          "Test suite passes for 15+ valid and 10+ invalid SQL edge cases",
          "Tokenizer output is correctly consumed as parser input (token stream integration)",
          "SELECT parser produces AST with column list including * wildcard support",
          "SELECT parser produces AST with FROM clause containing table name",
          "SELECT parser produces optional WHERE clause as Expression node",
          "SELECT parser produces optional ORDER BY clause with column names and ASC/DESC",
          "SELECT parser produces optional LIMIT clause with numeric value",
          "INSERT parser produces AST with target table name",
          "INSERT parser produces optional column name list before VALUES",
          "INSERT parser produces VALUES clause with one or more tuples of expressions",
          "CREATE TABLE parser extracts table name from statement",
          "CREATE TABLE parser extracts column definitions with names and data types (INTEGER, TEXT, REAL, BLOB)",
          "CREATE TABLE parser extracts column constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison > AND > OR",
          "Binary expressions (AND, OR, comparison operators) produce BinaryExpression AST nodes",
          "Unary NOT expression produces UnaryExpression AST node",
          "Parser provides error position (line and column) for syntax errors",
          "Parser reports meaningful error messages for unexpected tokens",
          "NULL keyword is parsed as LiteralExpression not IdentifierExpression",
          "String and numeric literals are parsed as LiteralExpression nodes",
          "Column references are parsed as IdentifierExpression nodes",
          "Test suite passes for 15+ valid SQL statements across SELECT, INSERT, and CREATE TABLE",
          "Test suite correctly rejects 10+ invalid SQL statements with position information"
        ],
        "pitfalls": [
          "Left recursion in expression grammar causes infinite recursion in recursive-descent parsers\u2014use precedence climbing or Pratt parsing",
          "AND binds tighter than OR in SQL (unlike some programming languages)\u2014get this wrong and WHERE clauses evaluate incorrectly",
          "Not handling parenthesized sub-expressions breaks complex WHERE clauses",
          "NULL is a keyword, not an identifier\u2014treat it specially in expression parsing"
        ],
        "concepts": [
          "Recursive descent parsing uses mutually recursive functions for grammar rules",
          "AST represents the syntactic structure of a SQL statement as a tree",
          "Operator precedence determines evaluation order of expressions",
          "Pratt parsing or precedence climbing handles binary operators elegantly"
        ],
        "skills": [
          "Recursive function design",
          "Tree data structures",
          "Grammar rule encoding",
          "Precedence handling"
        ],
        "deliverables": [
          "SELECT statement parser producing AST",
          "INSERT statement parser producing AST",
          "CREATE TABLE parser producing AST with column definitions and constraints",
          "Expression parser with correct operator precedence",
          "Error reporting with token position for parse errors",
          "Test suite for valid and invalid SQL inputs"
        ],
        "estimated_hours": 7
      },
      {
        "id": "build-sqlite-m3",
        "name": "Bytecode Compiler (VDBE)",
        "description": "Compile parsed AST into bytecode instructions executed by a virtual machine. This is the execution engine of the database.",
        "acceptance_criteria": [
          "Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable, Rewind, Column, ResultRow, Next, Halt",
          "Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord, Insert, Halt",
          "Virtual machine executes bytecode programs step-by-step, processing one opcode per cycle",
          "VM maintains a register file (array of typed values) for intermediate computation",
          "EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable format",
          "WHERE clause compiles to conditional jump opcodes that skip non-matching rows",
          "Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under 100ms",
          "Compiler translates SELECT AST into opcodes including OpenTable, Rewind, Column, ResultRow, Next, and Halt",
          "Compiler translates INSERT AST into opcodes including OpenTable, MakeRecord, and Insert",
          "VM executes bytecode in a fetch-decode-execute loop, processing one opcode per cycle",
          "VM manages a register file of typed values for intermediate calculations",
          "WHERE clauses are correctly compiled into conditional jump opcodes (e.g., Gt, Le, Ne)",
          "The EXPLAIN command displays the human-readable opcode sequence for any valid SQL statement",
          "The VM executes a full table scan of 10,000 rows in under 100ms",
          "Bytecode execution of SELECT * FROM t on a 10,000-row table completes in under 100ms"
        ],
        "pitfalls": [
          "Directly interpreting AST nodes (tree-walking interpreter) is simpler but significantly slower than bytecode\u2014commit to bytecode",
          "Register allocation must handle nested expressions without clobbering intermediate values",
          "Opcodes for cursor management (open, rewind, next, close) must match the storage engine's iterator interface",
          "Missing a Halt opcode causes the VM to run past the end of the program into garbage memory"
        ],
        "concepts": [
          "Bytecode compilation translates high-level AST into low-level instruction sequence",
          "Virtual machine executes bytecode using a fetch-decode-execute loop",
          "Register-based VM uses a register file for operand storage (vs stack-based)",
          "Cursor opcodes abstract B-tree traversal for the VM"
        ],
        "skills": [
          "Bytecode generation",
          "Virtual machine implementation",
          "Register allocation",
          "Instruction set design"
        ],
        "deliverables": [
          "Bytecode instruction set with opcodes for table operations, comparisons, jumps, and output",
          "Compiler translating SELECT, INSERT, CREATE TABLE ASTs into bytecode",
          "Virtual machine executing bytecode with register file and program counter",
          "EXPLAIN command displaying bytecode for any SQL statement",
          "WHERE clause compilation to conditional jumps"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m4",
        "name": "Buffer Pool Manager",
        "description": "Implement a page cache that sits between the B-tree layer and disk, managing fixed-size pages with LRU eviction and dirty page tracking.",
        "acceptance_criteria": [
          "Buffer pool manages a configurable number of in-memory page frames (default 1000 pages)",
          "Pages are fixed-size (4096 bytes by default, configurable)",
          "FetchPage loads a page from disk into a free frame, or returns the cached frame if already resident",
          "LRU eviction selects the least recently used unpinned page for replacement when no free frames exist",
          "Dirty page tracking marks pages modified in memory; eviction writes dirty pages to disk before replacement",
          "Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations",
          "FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)",
          "Buffer pool hit rate is measurable and logged for performance tuning",
          "Buffer pool initializes with a fixed number of 4096-byte frames",
          "FetchPage returns the correct page from memory if already loaded (hit)",
          "FetchPage loads page from disk if not in memory (miss)",
          "LRU algorithm correctly identifies the least recently used page for eviction",
          "Pinned pages (count > 0) are never selected for eviction",
          "Dirty pages are written back to disk only when evicted or on FlushAll",
          "Buffer pool hit rate is tracked and accessible for performance metrics"
        ],
        "pitfalls": [
          "Evicting a pinned page causes data corruption or use-after-free\u2014enforce pin counting",
          "Not flushing dirty pages before eviction loses committed data",
          "Page ID collisions if page numbering is not globally unique across the database file",
          "Buffer pool deadlocks when B-tree operations pin too many pages simultaneously\u2014set pin limits"
        ],
        "concepts": [
          "Buffer pool caches disk pages in memory for fast access",
          "LRU (Least Recently Used) eviction approximates optimal page replacement",
          "Pin counting prevents eviction of actively-used pages",
          "Dirty page tracking enables write-back caching"
        ],
        "skills": [
          "Page cache implementation",
          "LRU eviction algorithm",
          "Pin/unpin lifecycle management",
          "Disk I/O management"
        ],
        "deliverables": [
          "Buffer pool with configurable frame count and page size",
          "FetchPage loading pages from disk or returning cached frames",
          "LRU eviction selecting least recently used unpinned page",
          "Dirty page tracking and write-back on eviction",
          "Pin/Unpin API for B-tree layer",
          "FlushAll for shutdown and checkpoint"
        ],
        "estimated_hours": 8
      },
      {
        "id": "build-sqlite-m5",
        "name": "B-tree Page Format & Table Storage",
        "description": "Implement the on-disk page structure for B-trees (tables) and B+trees (indexes), with row serialization and node splitting.",
        "acceptance_criteria": [
          "Page header contains page type (leaf/internal, table/index), cell count, free space pointer, and right-child pointer (internal only)",
          "Table B-tree leaf pages store rows keyed by rowid with variable-length record encoding",
          "Table B-tree internal pages store rowid separator keys and child page numbers",
          "Index B+tree leaf pages store (indexed column value, rowid) pairs with data only in leaves",
          "Index B+tree internal pages store only separator keys and child pointers (no row data)",
          "CREATE TABLE creates a B-tree root page and records the schema in a system catalog (sqlite_master equivalent)",
          "INSERT serializes a row and inserts into the correct B-tree leaf; node splitting creates a new page and promotes a separator key",
          "Full table scan traverses all leaf pages in rowid order, returning all rows",
          "Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer pool",
          "Page header correctly identifies Leaf vs Internal and Table vs Index types",
          "Slotted page format implements bidirectional growth (pointers vs cells)",
          "Table B-tree stores full records in leaf nodes keyed by rowid",
          "Index B+tree stores key/rowid pairs only in leaf nodes",
          "Node split algorithm correctly rebalances the tree and promotes keys to parents",
          "Varint implementation handles 1-9 byte encoding for 64-bit integers",
          "System catalog (sqlite_master) persists table root page numbers",
          "Full table scan successfully iterates through all leaf pages in order"
        ],
        "pitfalls": [
          "Conflating B-tree (data in all nodes) with B+tree (data only in leaves)\u2014tables use B-tree, indexes use B+tree in SQLite",
          "Cell overflow when a row exceeds page capacity requires overflow pages\u2014handle or document the size limit",
          "Page fragmentation after deletions wastes space\u2014track free space within pages",
          "Endianness must be consistent between write and read (SQLite uses big-endian for portability)",
          "Variable-length integer encoding (varint) must handle the full range of 64-bit integers"
        ],
        "concepts": [
          "B-tree stores key-value pairs in all nodes (used for rowid-keyed tables)",
          "B+tree stores data only in leaf nodes with linked leaf pages (used for indexes)",
          "Slotted page format uses cell pointers for variable-length records",
          "Node splitting maintains B-tree balance on insert overflow",
          "System catalog stores table and index schema metadata"
        ],
        "skills": [
          "Binary page format design",
          "B-tree/B+tree implementation",
          "Variable-length record encoding",
          "Node splitting algorithms"
        ],
        "deliverables": [
          "Page format with header, cell pointer array, and cell content area",
          "Table B-tree with leaf (row storage) and internal (separator + child pointer) pages",
          "Index B+tree with leaf (key + rowid) and internal (separator + child pointer) pages",
          "Row serialization with variable-length encoding for column values",
          "Node splitting on insert overflow with separator key promotion",
          "System catalog table storing schema metadata",
          "Full table scan via leaf page traversal"
        ],
        "estimated_hours": 12
      },
      {
        "id": "build-sqlite-m6",
        "name": "SELECT Execution & DML",
        "description": "Execute SELECT, INSERT, UPDATE, and DELETE via the bytecode VM, with row deserialization, projection, and filtering.",
        "acceptance_criteria": [
          "SELECT * FROM table returns all rows in rowid order via B-tree leaf scan",
          "SELECT col1, col2 returns only specified columns (projection)",
          "WHERE clause filters rows during scan, evaluating boolean expressions on deserialized column values",
          "INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data",
          "UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes",
          "DELETE removes rows matching WHERE; subsequent SELECT no longer returns them",
          "NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null",
          "Operations on non-existent tables return an error with the table name",
          "SELECT * returns all rows by iterating through the B-tree leaf sequence",
          "SELECT with column names correctly projects only the requested fields",
          "WHERE clause correctly filters rows using Three-Valued Logic (handling NULLs)",
          "INSERT adds a new row and updates the B-tree structure correctly",
          "UPDATE and DELETE modify/remove rows while maintaining B-tree integrity",
          "NOT NULL constraints reject invalid writes with a descriptive error",
          "Attempting to query a table not in the System Catalog returns an 'undefined table' error",
          "WHERE clause correctly filters rows using Three-Valued Logic (NULL = NULL evaluates to NULL, not TRUE)",
          "Column projection correctly deserializes variable-length records to extract requested fields",
          "INSERT with NULL for INTEGER PRIMARY KEY triggers auto-increment behavior",
          "UPDATE cannot change the rowid (primary key) - must reject or handle as delete+insert",
          "DELETE during iteration uses two-pass approach to avoid cursor corruption"
        ],
        "pitfalls": [
          "Column name case sensitivity: SQL standard is case-insensitive for identifiers; be consistent",
          "NULL handling in WHERE: NULL = NULL evaluates to NULL (falsy), not TRUE\u2014use IS NULL for null checks",
          "B-tree rebalancing after DELETE is complex; initially, mark rows as deleted and reclaim space during compaction",
          "Updating the primary key (rowid) requires delete + re-insert at the new position",
          "Memory management for large result sets\u2014stream results row-by-row, don't buffer all in memory"
        ],
        "concepts": [
          "Cursor pattern abstracts B-tree traversal for the VM",
          "Projection selects a subset of columns from each row",
          "Predicate evaluation filters rows during scan",
          "Three-valued logic (TRUE, FALSE, NULL) for SQL boolean expressions"
        ],
        "skills": [
          "B-tree cursor implementation",
          "Row deserialization",
          "Expression evaluation",
          "DML execution"
        ],
        "deliverables": [
          "Table scan operator iterating all rows via B-tree cursor",
          "Row deserialization from binary page format to typed column values",
          "Column projection selecting specified fields",
          "WHERE clause evaluation with three-valued logic",
          "INSERT, UPDATE, DELETE execution via bytecode VM",
          "NOT NULL and UNIQUE constraint enforcement during writes"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m7",
        "name": "Secondary Indexes",
        "description": "Implement secondary indexes using B+trees and integrate index lookups into query execution.",
        "acceptance_criteria": [
          "CREATE INDEX builds a B+tree index mapping (indexed column value \u2192 rowid) from existing table data",
          "Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries added/removed/updated)",
          "Index lookup retrieves rows matching an equality predicate without full table scan, verified by counting pages read",
          "Range scan on index returns rows within a value range using B+tree leaf traversal",
          "Query execution uses index scan when an indexed column appears in WHERE with equality or range predicate",
          "UNIQUE index rejects INSERT or UPDATE that would create duplicate values",
          "CREATE INDEX builds a B+tree mapping column values to rowids",
          "INSERT/UPDATE/DELETE operations maintain all associated indexes synchronously",
          "Index lookup (equality) avoids full table scan and visits significantly fewer pages",
          "Index range scan (BETWEEN or < >) traverses linked leaf pages",
          "UNIQUE index correctly rejects duplicate value insertions",
          "Bytecode VM can perform a 'Double Lookup' from index cursor to table cursor"
        ],
        "pitfalls": [
          "Forgetting to update indexes on INSERT/UPDATE/DELETE causes stale or missing index entries",
          "Index on a column with many NULL values\u2014NULLs must be handled consistently (SQLite allows multiple NULLs in UNIQUE index)",
          "Composite indexes (multi-column) require careful key comparison\u2014leftmost prefix must match for index to be useful",
          "Index maintenance overhead can make writes slower; only create indexes that benefit read patterns"
        ],
        "concepts": [
          "Secondary index maps indexed column values to primary keys (rowids)",
          "B+tree leaf traversal enables efficient range scans",
          "Index maintenance ensures indexes stay consistent with table data",
          "Covering index can satisfy a query without table lookup if all needed columns are in the index"
        ],
        "skills": [
          "B+tree index implementation",
          "Index maintenance on DML",
          "Index scan execution",
          "Unique constraint enforcement"
        ],
        "deliverables": [
          "CREATE INDEX building B+tree from existing table data",
          "Index maintenance on INSERT, UPDATE, DELETE",
          "Index equality lookup avoiding full table scan",
          "Index range scan using B+tree leaf traversal",
          "UNIQUE index constraint enforcement"
        ],
        "estimated_hours": 8
      },
      {
        "id": "build-sqlite-m8",
        "name": "Query Planner & Statistics",
        "description": "Implement a cost-based query planner that chooses between table scan and index scan based on collected statistics.",
        "acceptance_criteria": [
          "ANALYZE command collects statistics: row count per table, distinct value count per indexed column",
          "Cost model estimates pages read for full table scan (total_pages) and index scan (estimated_rows / rows_per_page)",
          "Planner selects index scan when estimated selectivity (matching_rows / total_rows) is below a threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too low",
          "EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated row count",
          "For multi-table queries (JOIN), planner estimates join cardinality and selects join order to minimize intermediate result size",
          "ANALYZE command collects row count per table and distinct value count per indexed column",
          "Cost model estimates pages read for full table scan based on total pages",
          "Cost model estimates I/O cost for index scan based on selectivity and random I/O factor",
          "Planner selects index scan when estimated selectivity is below threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too high",
          "EXPLAIN displays the chosen plan including scan type, index name if used, and estimated row count",
          "For multi-table queries, planner estimates join cardinality and considers join order to minimize intermediate result size"
        ],
        "pitfalls": [
          "Without statistics (before ANALYZE), the planner has no data for cost estimation\u2014use default assumptions (e.g., assume 1M rows)",
          "Stale statistics after many inserts/deletes cause the planner to choose suboptimal plans\u2014recommend periodic ANALYZE",
          "Cardinality estimation errors compound through joins\u2014a 10x error in one table becomes 100x after a two-table join",
          "Plan search space explodes exponentially with number of tables in JOIN\u2014limit to dynamic programming for \u226410 tables"
        ],
        "concepts": [
          "Cost-based optimization compares estimated cost of alternative plans",
          "Cardinality estimation predicts the number of rows each operator produces",
          "Selectivity is the fraction of rows matching a predicate",
          "Statistics collection (ANALYZE) provides data for cost estimation",
          "Dynamic programming-based join ordering for multi-table queries"
        ],
        "skills": [
          "Statistics collection",
          "Cost model design",
          "Cardinality estimation",
          "Plan enumeration"
        ],
        "deliverables": [
          "ANALYZE command collecting table and index statistics",
          "Cost model estimating I/O for table scan vs index scan",
          "Plan selection choosing cheapest access path per table",
          "EXPLAIN command displaying chosen plan with cost estimates",
          "Join order optimization for multi-table queries"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m9",
        "name": "Transactions (Rollback Journal)",
        "description": "Implement ACID transactions using a rollback journal for crash recovery.",
        "acceptance_criteria": [
          "BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or ROLLBACK",
          "COMMIT makes all changes permanent by flushing dirty pages and removing the rollback journal",
          "ROLLBACK undoes all changes by restoring original pages from the rollback journal",
          "Rollback journal records original page contents BEFORE modification (for undo on crash)",
          "Changes are not visible to other connections until COMMIT (basic read isolation)",
          "Crash recovery on startup detects an existing rollback journal and automatically rolls back the incomplete transaction",
          "Journal file is fsync'd before modified pages are written to the database file (write ordering guarantee)",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the engine state",
          "A .db-journal file is created and contains original page data before any write to the main .db file",
          "The journal file is physically flushed to disk (fsync) before the main database is modified",
          "A manual ROLLBACK restores the state from the journal and clears the journal file",
          "Startup logic detects a 'Hot Journal' and automatically restores the database to a consistent state",
          "Writes are not visible to other database connections until the COMMIT is complete",
          "BEGIN starts a transaction and buffers all subsequent writes until COMMIT or ROLLBACK",
          "Rollback journal records original page contents BEFORE any modification to the database file",
          "Changes are not visible to other database connections until COMMIT completes",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the transaction manager state",
          "A manual ROLLBACK restores the database state from the journal and deletes the journal file"
        ],
        "pitfalls": [
          "Writing modified pages to database before journal is fsync'd causes unrecoverable corruption on crash",
          "Partial page writes (torn pages) on crash can corrupt the database\u2014journal must contain complete original pages",
          "Lock ordering between multiple connections must be consistent to prevent deadlocks",
          "Long-running transactions holding locks block all other writers\u2014document the locking behavior"
        ],
        "concepts": [
          {
            "ACID": "Atomicity (rollback journal), Consistency (constraints), Isolation (locking), Durability (fsync)"
          },
          "Rollback journal records undo information (original page images) before modification",
          {
            "Write ordering": "journal fsync \u2192 database write \u2192 journal delete"
          },
          {
            "Crash recovery": "hot journal detected \u2192 restore original pages \u2192 delete journal"
          }
        ],
        "skills": [
          "Rollback journal implementation",
          "Write ordering enforcement",
          "Crash recovery logic",
          "Lock management"
        ],
        "deliverables": [
          "BEGIN/COMMIT/ROLLBACK command implementation",
          "Rollback journal recording original page contents before modification",
          "Write ordering ensuring journal is durable before database pages are modified",
          "Crash recovery detecting hot journal and restoring database to pre-transaction state",
          "ACID guarantee verification test suite"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m10",
        "name": "WAL Mode",
        "description": "Implement Write-Ahead Logging as an alternative to rollback journal, enabling concurrent readers during writes.",
        "acceptance_criteria": [
          "WAL mode appends modified pages to a separate WAL file instead of modifying the main database file",
          "Writers append to WAL; readers check WAL for the most recent version of a page before reading from the main database",
          "Multiple readers can execute queries concurrently while a single writer appends to the WAL",
          "Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database file",
          "WAL checkpoint is required to prevent unbounded WAL growth\u2014auto-checkpoint triggers after configurable page count (default 1000)",
          "Readers see a consistent snapshot: a reader that starts before a commit does not see that commit's changes (snapshot isolation for reads)",
          "WAL file corruption is detected via page checksums",
          "Writers append to a separate WAL file instead of modifying the main .db file",
          "Readers search the WAL for the most recent page version before falling back to the main file",
          "Writers and multiple readers can operate simultaneously without blocking",
          "Checkpointing copies WAL pages to the main database and truncates the WAL",
          "Automatic checkpoint triggers after 1000 pages (configurable)",
          "Readers use a consistent snapshot based on the WAL state at their start time",
          "Checksums are used to detect and reject corrupted WAL frames"
        ],
        "pitfalls": [
          "WAL grows unbounded without checkpointing\u2014auto-checkpoint is not optional, it's required",
          "Readers pinning old WAL frames prevent checkpoint from truncating\u2014long-running reads block WAL cleanup",
          "WAL and rollback journal are mutually exclusive modes\u2014switching requires careful state management",
          "Checkpoint must not run while readers are using WAL frames that would be overwritten",
          "WAL file corruption must be detected (checksums) to prevent propagating bad data to the main database"
        ],
        "concepts": [
          "WAL appends redo information (new page images) to a log file",
          "Readers use WAL index (wal-index) to find most recent page version",
          "Checkpoint merges WAL changes back into the main database file",
          {
            "Snapshot isolation": "each reader sees the database as of its start time"
          },
          {
            "WAL vs rollback journal": "WAL enables concurrent readers, rollback journal does not"
          }
        ],
        "skills": [
          "Write-ahead log implementation",
          "Snapshot isolation for readers",
          "Checkpoint algorithm",
          "Concurrent read/write coordination"
        ],
        "deliverables": [
          "WAL file format appending modified pages with checksums",
          "WAL reader looking up most recent page version before main database",
          "Checkpoint process copying WAL pages into main database",
          "Auto-checkpoint triggered by WAL page count threshold",
          "Concurrent reader support during active write transactions",
          "WAL mode toggle (PRAGMA journal_mode=WAL)"
        ],
        "estimated_hours": 12
      },
      {
        "id": "build-sqlite-m11",
        "name": "Aggregate Functions & JOIN",
        "description": "Implement aggregate functions (COUNT, SUM, AVG, MIN, MAX), GROUP BY, and basic JOIN execution.",
        "acceptance_criteria": [
          "COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values",
          "SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY groups rows by specified columns before applying aggregate functions",
          "HAVING filters groups after aggregation",
          "INNER JOIN combines rows from two tables matching a join condition",
          "Nested loop join is implemented as the baseline join algorithm",
          "JOIN with WHERE clause filters correctly after join",
          "COUNT(*) accurately counts rows including NULLs",
          "COUNT(col) ignores NULL values in the target column",
          "AVG returns a REAL/float even if input column is INTEGER",
          "GROUP BY correctly partitions aggregate states into buckets",
          "HAVING filters out aggregated groups based on result values",
          "INNER JOIN correctly combines rows from two tables using a nested loop",
          "JOIN with WHERE correctly filters rows before or during the join process",
          "COUNT(*) returns the number of rows including NULLs",
          "COUNT(col) returns count of non-NULL values only",
          "SUM produces correct results over grouped and ungrouped queries",
          "AVG returns REAL/float even if input column is INTEGER",
          "AVG ignores NULL values in computation",
          "MIN and MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY partitions rows into groups before applying aggregate functions",
          "GROUP BY without ORDER BY can return groups in any order",
          "HAVING filters groups after aggregation based on aggregate values",
          "JOIN with WHERE clause filters correctly after or during join",
          "Empty table with aggregates returns appropriate default values (0 for COUNT, NULL for SUM/AVG)",
          "Multiple aggregates can be computed in a single query"
        ],
        "pitfalls": [
          "AVG must handle integer division correctly (return REAL, not INTEGER)",
          "NULL handling in aggregates: COUNT(*) counts NULLs, COUNT(col) does not; SUM/AVG ignore NULLs",
          "GROUP BY without aggregate function is valid SQL but confusing\u2014handle correctly",
          "Nested loop join is O(n*m)\u2014acceptable for small tables but document the limitation"
        ],
        "concepts": [
          "Aggregate functions accumulate values across row groups",
          "GROUP BY partitions rows into groups for aggregation",
          "Nested loop join iterates all combinations of rows from two tables",
          "HAVING filters groups after aggregation (vs WHERE which filters before)"
        ],
        "skills": [
          "Aggregate computation",
          "Group-by execution",
          "Join algorithms",
          "NULL handling in aggregates"
        ],
        "deliverables": [
          "COUNT, SUM, AVG, MIN, MAX aggregate functions",
          "GROUP BY execution with hash-based or sort-based grouping",
          "HAVING clause filtering groups after aggregation",
          "Nested loop INNER JOIN execution",
          "Test suite covering aggregates with NULLs, empty tables, and multi-table joins"
        ],
        "estimated_hours": 14
      }
    ],
    "domain": "data-storage"
  },
  "blueprint": {
    "title": "Build Your Own SQLite",
    "overview": "Build a complete embedded SQL database engine from scratch, implementing every layer from SQL tokenization to ACID transactions. This project teaches the fundamental architecture shared by all relational databases: how SQL text becomes executable bytecode, how B-trees organize data on disk, and how write-ahead logging enables crash recovery without losing committed transactions. You'll implement SQLite's virtual database engine (VDBE) \u2014 a register-based bytecode interpreter that executes compiled SQL statements \u2014 and understand why this design enables SQL's expressive power while maintaining predictable performance. The project culminates in two transaction modes: rollback journal for simple crash recovery and WAL mode for concurrent readers during writes.",
    "design_philosophy": "SQLite represents the purest expression of database fundamentals: a single library that does everything from parsing SQL to managing disk I/O. By building each layer yourself, you'll understand the deep connections between seemingly unrelated concepts \u2014 why the bytecode VM's register file mirrors the buffer pool's page frames, why B-tree node splits must be atomic, why WAL checkpoints are really just batched writes. This project prioritizes correctness over optimization: a slow database that survives crashes is infinitely more valuable than a fast one that corrupts data. Every milestone builds toward the durability soul \u2014 the unshakeable guarantee that committed transactions survive system failures.",
    "is_build_your_own": true,
    "prerequisites": {
      "assumed_known": [
        "B-tree data structure \u2014 insertion, deletion, balancing",
        "SQL basics \u2014 SELECT, INSERT, WHERE, JOIN syntax",
        "File I/O and binary formats \u2014 reading/writing bytes, endianness",
        "Basic compiler concepts \u2014 what a lexer/parser does, what an AST is",
        "Memory management in your chosen language",
        "Pointer/reference semantics"
      ],
      "must_teach_first": [
        {
          "concept": "Slotted page format",
          "depth": "intermediate",
          "when": "Milestone 5"
        },
        {
          "concept": "Varint encoding",
          "depth": "basic",
          "when": "Milestone 5"
        },
        {
          "concept": "Three-valued logic (NULL handling)",
          "depth": "basic",
          "when": "Milestone 6"
        },
        {
          "concept": "Write ordering and fsync semantics",
          "depth": "intermediate",
          "when": "Milestone 9"
        },
        {
          "concept": "Snapshot isolation",
          "depth": "basic",
          "when": "Milestone 10"
        }
      ]
    },
    "milestones": [
      {
        "id": "build-sqlite-m1",
        "title": "SQL Tokenizer",
        "anchor_id": "anc-tokenizer",
        "summary": "Build a lexer that converts SQL text into a stream of typed tokens, handling keywords, string literals, numeric literals, operators, and identifiers with proper escape sequences and error reporting.",
        "misconception": "Tokenization is just string splitting on whitespace \u2014 you can use split() and be done.",
        "reveal": "Tokenization requires a state machine because context determines meaning. The string 'SELECT' inside quotes is a literal, not a keyword. The character '-' might be a minus operator OR the start of a comment. Negative numbers aren't tokenized as negative \u2014 the parser decides if '-' is unary negation or subtraction. This is why real tokenizers are FSMs, not string splitters.",
        "cascade": [
          "Compiler frontends everywhere \u2014 the same FSM pattern powers JSON parsers, config file readers, and template engines",
          "UTF-8 handling \u2014 once you handle escaped quotes, you're one step from handling multi-byte characters correctly",
          "Error recovery \u2014 position tracking enables IDE features like 'go to definition' and squiggly error underlines",
          "Lexer generators (flex, ANTLR) \u2014 you'll understand what tools like Lex generate and why handwritten lexers can be faster",
          "SQL injection prevention \u2014 understanding token boundaries reveals why sanitization is harder than it looks"
        ],
        "yaml_acceptance_criteria": [
          "Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.) case-insensitively",
          "String literals enclosed in single quotes are parsed including escaped quotes ('it''s' \u2192 it's)",
          "Numeric literals including integers and floating-point values (42, 3.14, -7) are recognized as distinct token types",
          "Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon) are tokenized as distinct tokens",
          "Identifiers (table names, column names) support quoted identifiers with double quotes (\"column name\")",
          "Tokenizer reports error position (line and column) for unrecognized characters",
          "Token stream correctly tokenizes at least 20 diverse SQL statements in a test suite",
          "Tokenizer recognizes keywords like SELECT and INSERT case-insensitively",
          "String literals correctly handle escaped single quotes ('') and preserve internal content",
          "Numeric literals distinguish between integers (42) and floats (3.14)",
          "Double-quoted identifiers ('\"Table Name\"') are correctly tokenized as identifiers",
          "The tokenizer returns a list or stream of objects containing type, value, line, and column",
          "A test suite of 20+ SQL statements produces the expected token sequence"
        ]
      },
      {
        "id": "build-sqlite-m2",
        "title": "SQL Parser (AST)",
        "anchor_id": "anc-parser",
        "summary": "Build a recursive-descent parser that converts the token stream into an Abstract Syntax Tree, handling SELECT, INSERT, CREATE TABLE statements with correct operator precedence in expressions.",
        "misconception": "Parsing is just checking if the syntax is valid \u2014 the AST is basically the same as the token sequence.",
        "reveal": "The AST captures operator precedence through tree structure, not token order. 'a OR b AND c' produces different trees than '(a OR b) AND c' even though the token sequences are nearly identical. The parser doesn't just validate \u2014 it encodes meaning. This is why AND binds tighter than OR in SQL, and why Pratt parsing is elegant for expressions.",
        "cascade": [
          "Query optimization \u2014 the AST is where you can rewrite 'WHERE 1=1 AND col=5' to just 'WHERE col=5'",
          "Language servers \u2014 ASTs enable refactoring tools, code completion, and cross-reference navigation",
          "SQL dialect translation \u2014 the same AST can generate PostgreSQL, MySQL, or SQLite syntax",
          "Prepared statements \u2014 parsing once and executing many times is why prepared statements are faster",
          "Expression evaluation in spreadsheets \u2014 formula parsers use the same precedence climbing techniques"
        ],
        "yaml_acceptance_criteria": [
          "SELECT parser produces AST with column list (including *), FROM clause, optional WHERE, ORDER BY, and LIMIT",
          "INSERT parser produces AST with target table, optional column names, and VALUES clause",
          "CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT, REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison (=,<,>) > AND > OR",
          "Parenthesized expressions override default precedence",
          "Parser produces clear error messages with token position for syntax errors",
          "Test suite covers at least 15 valid and 10 invalid SQL statements",
          "SELECT parser produces AST with column list, FROM, and optional WHERE/LIMIT",
          "INSERT parser handles target table and VALUES mapping",
          "CREATE TABLE parser extracts column names, types, and constraints (PRIMARY KEY, NOT NULL)",
          "Expression parser correctly handles NOT > AND > OR precedence",
          "Parenthesized expressions correctly override default precedence levels",
          "Parser provides error position (line/column) for syntax errors",
          "Test suite passes for 15+ valid and 10+ invalid SQL edge cases",
          "Tokenizer output is correctly consumed as parser input (token stream integration)",
          "SELECT parser produces AST with column list including * wildcard support",
          "SELECT parser produces AST with FROM clause containing table name",
          "SELECT parser produces optional WHERE clause as Expression node",
          "SELECT parser produces optional ORDER BY clause with column names and ASC/DESC",
          "SELECT parser produces optional LIMIT clause with numeric value",
          "INSERT parser produces AST with target table name",
          "INSERT parser produces optional column name list before VALUES",
          "INSERT parser produces VALUES clause with one or more tuples of expressions",
          "CREATE TABLE parser extracts table name from statement",
          "CREATE TABLE parser extracts column definitions with names and data types (INTEGER, TEXT, REAL, BLOB)",
          "CREATE TABLE parser extracts column constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison > AND > OR",
          "Binary expressions (AND, OR, comparison operators) produce BinaryExpression AST nodes",
          "Unary NOT expression produces UnaryExpression AST node",
          "Parser provides error position (line and column) for syntax errors",
          "Parser reports meaningful error messages for unexpected tokens",
          "NULL keyword is parsed as LiteralExpression not IdentifierExpression",
          "String and numeric literals are parsed as LiteralExpression nodes",
          "Column references are parsed as IdentifierExpression nodes",
          "Test suite passes for 15+ valid SQL statements across SELECT, INSERT, and CREATE TABLE",
          "Test suite correctly rejects 10+ invalid SQL statements with position information"
        ]
      },
      {
        "id": "build-sqlite-m3",
        "title": "Bytecode Compiler (VDBE)",
        "anchor_id": "anc-vdbe",
        "summary": "Compile parsed AST into bytecode instructions executed by a virtual machine. This is SQLite's secret weapon \u2014 a register-based VM that makes query execution uniform, optimizable, and debuggable.",
        "misconception": "Interpreting the AST directly (tree-walking) is simpler and good enough for a learning project.",
        "reveal": "Bytecode is the enabler of everything that makes SQLite powerful: EXPLAIN shows you the execution plan, the query planner rewrites bytecode sequences, and prepared statements cache compiled bytecode. Tree-walking interpreters re-parse WHERE clauses for every row. Bytecode compiles once, executes millions of times. This is why Lua, Python, and the JVM all use bytecode.",
        "cascade": [
          "WebAssembly \u2014 the same 'compile to bytecode, execute in VM' pattern runs in every browser",
          "JIT compilation \u2014 bytecode is the starting point for tracing JITs like LuaJIT and V8's TurboFan",
          "Database portability \u2014 the VDBE abstracts storage engine differences, enabling SQLite to run on bizarre filesystems",
          "Query plan caching \u2014 prepared statements are fast because they skip parsing and compilation",
          "Garbage collector implementation \u2014 register-based VMs face the same value lifetime questions as GCs"
        ],
        "yaml_acceptance_criteria": [
          "Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable, Rewind, Column, ResultRow, Next, Halt",
          "Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord, Insert, Halt",
          "Virtual machine executes bytecode programs step-by-step, processing one opcode per cycle",
          "VM maintains a register file (array of typed values) for intermediate computation",
          "EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable format",
          "WHERE clause compiles to conditional jump opcodes that skip non-matching rows",
          "Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under 100ms",
          "Compiler translates SELECT AST into opcodes including OpenTable, Rewind, Column, ResultRow, Next, and Halt",
          "Compiler translates INSERT AST into opcodes including OpenTable, MakeRecord, and Insert",
          "VM executes bytecode in a fetch-decode-execute loop, processing one opcode per cycle",
          "VM manages a register file of typed values for intermediate calculations",
          "WHERE clauses are correctly compiled into conditional jump opcodes (e.g., Gt, Le, Ne)",
          "The EXPLAIN command displays the human-readable opcode sequence for any valid SQL statement",
          "The VM executes a full table scan of 10,000 rows in under 100ms"
        ]
      },
      {
        "id": "build-sqlite-m4",
        "title": "Buffer Pool Manager",
        "anchor_id": "anc-buffer-pool",
        "summary": "Implement a page cache that sits between the B-tree layer and disk, managing fixed-size pages with LRU eviction and dirty page tracking. This is where memory management meets durability.",
        "misconception": "The OS page cache handles this \u2014 just use mmap and let the kernel do the work.",
        "reveal": "mmap gives you neither the control nor the guarantees a database needs. You can't pin pages that the OS might evict mid-transaction. You can't fsync dirty pages selectively. You can't implement LRU better than the OS because you don't know the access pattern \u2014 but you DO know the access pattern, it's B-tree traversal! This is why databases implement their own buffer pools.",
        "cascade": [
          "Redis eviction policies \u2014 LRU/LFU in Redis are the same algorithms, just applied to keys instead of pages",
          "Operating system design \u2014 the buffer pool IS a page cache, identical in concept to the OS page cache",
          "CDN cache design \u2014 eviction policies, pinning for hot content, and dirty write-back are universal caching problems",
          "Game engine asset streaming \u2014 texture/geometry caches use the same pin/unpin pattern for streaming open worlds",
          "Browser cache eviction \u2014 the same LRU logic decides which HTTP responses stay in memory"
        ],
        "yaml_acceptance_criteria": [
          "Buffer pool manages a configurable number of in-memory page frames (default 1000 pages)",
          "Pages are fixed-size (4096 bytes by default, configurable)",
          "FetchPage loads a page from disk into a free frame, or returns the cached frame if already resident",
          "LRU eviction selects the least recently used unpinned page for replacement when no free frames exist",
          "Dirty page tracking marks pages modified in memory; eviction writes dirty pages to disk before replacement",
          "Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations",
          "FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)",
          "Buffer pool hit rate is measurable and logged for performance tuning",
          "Buffer pool initializes with a fixed number of 4096-byte frames",
          "FetchPage returns the correct page from memory if already loaded (hit)",
          "FetchPage loads page from disk if not in memory (miss)",
          "LRU algorithm correctly identifies the least recently used page for eviction",
          "Pinned pages (count > 0) are never selected for eviction",
          "Dirty pages are written back to disk only when evicted or on FlushAll",
          "Buffer pool hit rate is tracked and accessible for performance metrics"
        ]
      },
      {
        "id": "build-sqlite-m5",
        "title": "B-tree Page Format & Table Storage",
        "anchor_id": "anc-btree-storage",
        "summary": "Implement the on-disk page structure for B-trees (tables) and B+trees (indexes), with row serialization, variable-length encoding, and node splitting. This is where bytes become meaningful data.",
        "misconception": "B-tree is B-tree \u2014 tables and indexes use the same structure, just with different keys.",
        "reveal": "SQLite uses B-tree for tables (data in all nodes) and B+tree for indexes (data only in leaves). This isn't arbitrary \u2014 table B-trees key by rowid, so finding a row is O(log n) regardless of which level contains it. Index B+trees key by column value, and linking leaves enables range scans without tree traversal. The page format encodes this difference in the header byte.",
        "cascade": [
          "Filesystem design \u2014 ext4/HFS+ use B-trees for directory entries; the same split/merge logic applies",
          "LSM trees \u2014 understanding B-tree page formats reveals why LSM-trees exist (write amplification vs read amplification)",
          "Variable-length encoding \u2014 varints appear in protocol buffers, MessagePack, and any format that values space efficiency",
          "Slotted page design \u2014 PostgreSQL, MySQL, and Oracle all use cell pointer arrays for the same reason",
          "Database page size tuning \u2014 why 4KB vs 8KB vs 16KB matters for SSD vs HDD workloads"
        ],
        "yaml_acceptance_criteria": [
          "Page header contains page type (leaf/internal, table/index), cell count, free space pointer, and right-child pointer (internal only)",
          "Table B-tree leaf pages store rows keyed by rowid with variable-length record encoding",
          "Table B-tree internal pages store rowid separator keys and child page numbers",
          "Index B+tree leaf pages store (indexed column value, rowid) pairs with data only in leaves",
          "Index B+tree internal pages store only separator keys and child pointers (no row data)",
          "CREATE TABLE creates a B-tree root page and records the schema in a system catalog (sqlite_master equivalent)",
          "INSERT serializes a row and inserts into the correct B-tree leaf; node splitting creates a new page and promotes a separator key",
          "Full table scan traverses all leaf pages in rowid order, returning all rows",
          "Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer pool",
          "Page header correctly identifies Leaf vs Internal and Table vs Index types",
          "Slotted page format implements bidirectional growth (pointers vs cells)",
          "Table B-tree stores full records in leaf nodes keyed by rowid",
          "Index B+tree stores key/rowid pairs only in leaf nodes",
          "Node split algorithm correctly rebalances the tree and promotes keys to parents",
          "Varint implementation handles 1-9 byte encoding for 64-bit integers",
          "System catalog (sqlite_master) persists table root page numbers",
          "Full table scan successfully iterates through all leaf pages in order"
        ]
      },
      {
        "id": "build-sqlite-m6",
        "title": "SELECT Execution & DML",
        "anchor_id": "anc-dml-execution",
        "summary": "Execute SELECT, INSERT, UPDATE, and DELETE via the bytecode VM, with row deserialization, projection, and filtering. This is where the database becomes useful \u2014 actual data in, actual data out.",
        "misconception": "NULL is just zero or empty string \u2014 handle it like any other default value.",
        "reveal": "NULL is not a value, it's the absence of a value. NULL = NULL evaluates to NULL (unknown), not TRUE. NULL + 5 evaluates to NULL. This three-valued logic (TRUE/FALSE/NULL) infects everything: WHERE clauses, aggregate functions, JOIN conditions. Getting NULL handling wrong produces silently incorrect results, not crashes.",
        "cascade": [
          "Optional types in programming languages \u2014 Rust's Option<T> and Swift's T? are formalizations of NULL handling",
          "NaN propagation in floating-point \u2014 NULL propagates through SQL the way NaN propagates through math",
          "Missing data in data science \u2014 pandas/R handle NA values with the same three-valued logic",
          "JSON null vs missing key \u2014 APIs that distinguish null from absent face the same modeling questions",
          "Distributed systems partial failures \u2014 NULL represents 'unknown' the same way a timeout represents 'outcome unknown'"
        ],
        "yaml_acceptance_criteria": [
          "SELECT * FROM table returns all rows in rowid order via B-tree leaf scan",
          "SELECT col1, col2 returns only specified columns (projection)",
          "WHERE clause filters rows during scan, evaluating boolean expressions on deserialized column values",
          "INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data",
          "UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes",
          "DELETE removes rows matching WHERE; subsequent SELECT no longer returns them",
          "NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null",
          "Operations on non-existent tables return an error with the table name",
          "SELECT * returns all rows by iterating through the B-tree leaf sequence",
          "SELECT with column names correctly projects only the requested fields",
          "WHERE clause correctly filters rows using Three-Valued Logic (handling NULLs)",
          "INSERT adds a new row and updates the B-tree structure correctly",
          "UPDATE and DELETE modify/remove rows while maintaining B-tree integrity",
          "NOT NULL constraints reject invalid writes with a descriptive error",
          "Attempting to query a table not in the System Catalog returns an 'undefined table' error",
          "WHERE clause correctly filters rows using Three-Valued Logic (NULL = NULL evaluates to NULL, not TRUE)",
          "Column projection correctly deserializes variable-length records to extract requested fields",
          "INSERT with NULL for INTEGER PRIMARY KEY triggers auto-increment behavior",
          "UPDATE cannot change the rowid (primary key) - must reject or handle as delete+insert",
          "DELETE during iteration uses two-pass approach to avoid cursor corruption"
        ]
      },
      {
        "id": "build-sqlite-m7",
        "title": "Secondary Indexes",
        "anchor_id": "anc-indexes",
        "summary": "Implement secondary indexes using B+trees and integrate index lookups into query execution. Speed up queries by orders of magnitude without changing the query.",
        "misconception": "Indexes are just sorted copies of the data \u2014 create them for every column and queries will be fast.",
        "reveal": "Indexes are a trade-off, not a free lunch. Each index slows down INSERT/UPDATE/DELETE because the index must be maintained. A 10-column table with indexes on every column does 11 B-tree operations per insert. The query planner's job is to know when an index hurts more than it helps \u2014 and sometimes a full table scan IS the optimal choice.",
        "cascade": [
          "Search engine inverted indexes \u2014 the same B+tree structure powers full-text search, just with different keys",
          "Database covering indexes \u2014 when all query columns are in the index, you skip the table lookup entirely",
          "LSM-tree index files \u2014 LevelDB/RocksDB use the same index concepts with different file organizations",
          "Index-only scans in column stores \u2014 data warehouses exploit indexes differently because of columnar storage",
          "Hash indexes vs B-tree indexes \u2014 understanding when O(1) lookup beats O(log n) range capability"
        ],
        "yaml_acceptance_criteria": [
          "CREATE INDEX builds a B+tree index mapping (indexed column value \u2192 rowid) from existing table data",
          "Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries added/removed/updated)",
          "Index lookup retrieves rows matching an equality predicate without full table scan, verified by counting pages read",
          "Range scan on index returns rows within a value range using B+tree leaf traversal",
          "Query execution uses index scan when an indexed column appears in WHERE with equality or range predicate",
          "UNIQUE index rejects INSERT or UPDATE that would create duplicate values",
          "CREATE INDEX builds a B+tree mapping column values to rowids",
          "INSERT/UPDATE/DELETE operations maintain all associated indexes synchronously",
          "Index lookup (equality) avoids full table scan and visits significantly fewer pages",
          "Index range scan (BETWEEN or < >) traverses linked leaf pages",
          "UNIQUE index correctly rejects duplicate value insertions",
          "Bytecode VM can perform a 'Double Lookup' from index cursor to table cursor"
        ]
      },
      {
        "id": "build-sqlite-m8",
        "title": "Query Planner & Statistics",
        "anchor_id": "anc-query-planner",
        "summary": "Implement a cost-based query planner that chooses between table scan and index scan based on collected statistics. Transform 'how do I execute this?' into 'what's the cheapest way to execute this?'",
        "misconception": "The optimizer picks the best plan \u2014 if my query is slow, I wrote the wrong SQL.",
        "reveal": "Query optimizers are guessing machines. Without perfect knowledge of data distribution, they estimate cardinality using statistics that might be stale. A 10x cardinality error becomes 100x after a join. This is why production databases have 'optimizer hints' \u2014 sometimes the human knows better than the cost model. The planner doesn't find the best plan; it finds the least-bad plan it can discover quickly.",
        "cascade": [
          "Compiler optimization \u2014 SSA form, dead code elimination, and register allocation are the same optimization mindset",
          "Network routing \u2014 BGP path selection is cost-based optimization with different metrics",
          "Machine learning hyperparameter tuning \u2014 the same search space explosion problem, different domain",
          "Video game pathfinding \u2014 A* is cost-based search; query planners do the same for execution paths",
          "Logistics optimization \u2014 traveling salesman variants appear in query plan enumeration"
        ],
        "yaml_acceptance_criteria": [
          "ANALYZE command collects statistics: row count per table, distinct value count per indexed column",
          "Cost model estimates pages read for full table scan (total_pages) and index scan (estimated_rows / rows_per_page)",
          "Planner selects index scan when estimated selectivity (matching_rows / total_rows) is below a threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too low",
          "EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated row count",
          "For multi-table queries (JOIN), planner estimates join cardinality and selects join order to minimize intermediate result size",
          "ANALYZE command collects row count per table and distinct value count per indexed column",
          "Cost model estimates pages read for full table scan based on total pages",
          "Cost model estimates I/O cost for index scan based on selectivity and random I/O factor",
          "Planner selects index scan when estimated selectivity is below threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too high",
          "EXPLAIN displays the chosen plan including scan type, index name if used, and estimated row count",
          "For multi-table queries, planner estimates join cardinality and considers join order to minimize intermediate result size"
        ]
      },
      {
        "id": "build-sqlite-m9",
        "title": "Transactions (Rollback Journal)",
        "anchor_id": "anc-transactions-journal",
        "summary": "Implement ACID transactions using a rollback journal for crash recovery. This is the first milestone where 'works most of the time' is not acceptable \u2014 durability has no partial credit.",
        "misconception": "Transactions are about locking \u2014 the hard part is preventing concurrent modifications.",
        "reveal": "Transactions are about surviving crashes. The hard part isn't preventing concurrent access; it's ensuring that a power failure at ANY point leaves the database in a consistent state. The rollback journal's write ordering \u2014 journal fsync BEFORE database write \u2014 is the difference between recovery and corruption. This is why fsync exists and why databases are paranoid about it.",
        "cascade": [
          "Version control systems \u2014 Git's reflog is a rollback journal for repository state",
          "Installers and updaters \u2014 'restore previous version if update fails' requires journaling",
          "Distributed consensus \u2014 Raft's log is a write-ahead journal replicated across nodes",
          "Filesystem journaling \u2014 ext4's journal does for filesystems what rollback journals do for databases",
          "Undo/redo in text editors \u2014 the same 'save before state' pattern enables Ctrl+Z"
        ],
        "yaml_acceptance_criteria": [
          "BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or ROLLBACK",
          "COMMIT makes all changes permanent by flushing dirty pages and removing the rollback journal",
          "ROLLBACK undoes all changes by restoring original pages from the rollback journal",
          "Rollback journal records original page contents BEFORE modification (for undo on crash)",
          "Changes are not visible to other connections until COMMIT (basic read isolation)",
          "Crash recovery on startup detects an existing rollback journal and automatically rolls back the incomplete transaction",
          "Journal file is fsync'd before modified pages are written to the database file (write ordering guarantee)",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the engine state",
          "A .db-journal file is created and contains original page data before any write to the main .db file",
          "The journal file is physically flushed to disk (fsync) before the main database is modified",
          "A manual ROLLBACK restores the state from the journal and clears the journal file",
          "Startup logic detects a 'Hot Journal' and automatically restores the database to a consistent state",
          "Writes are not visible to other database connections until the COMMIT is complete",
          "BEGIN starts a transaction and buffers all subsequent writes until COMMIT or ROLLBACK",
          "Rollback journal records original page contents BEFORE any modification to the database file",
          "Changes are not visible to other database connections until COMMIT completes",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the transaction manager state",
          "A manual ROLLBACK restores the database state from the journal and deletes the journal file"
        ]
      },
      {
        "id": "build-sqlite-m10",
        "title": "WAL Mode",
        "anchor_id": "anc-wal-mode",
        "summary": "Implement Write-Ahead Logging as an alternative to rollback journal, enabling concurrent readers during writes. This is how real databases achieve read scalability without sacrificing durability.",
        "misconception": "WAL is just a journal with a different name \u2014 the same thing, optimized slightly differently.",
        "reveal": "WAL inverts the rollback journal's approach entirely. Rollback journals store OLD pages (for undo); WAL stores NEW pages (for redo). This inversion is why WAL enables concurrent readers: readers check the WAL for newer versions of pages while writers append without blocking. The checkpoint is where WAL and rollback journal converge \u2014 both eventually write to the main database file.",
        "cascade": [
          "Event sourcing \u2014 the WAL IS an event log; event sourcing systems use the same append-only pattern",
          "Database replication \u2014 PostgreSQL's logical replication is essentially shipping WAL records to replicas",
          "Time-travel debugging \u2014 record/replay debuggers use WAL techniques to restore program state",
          "CRDTs in distributed systems \u2014 append-only logs with merging are the distributed analog of WAL checkpoints",
          "Blockchain structure \u2014 blocks form a write-ahead log where the 'checkpoint' is finality"
        ],
        "yaml_acceptance_criteria": [
          "WAL mode appends modified pages to a separate WAL file instead of modifying the main database file",
          "Writers append to WAL; readers check WAL for the most recent version of a page before reading from the main database",
          "Multiple readers can execute queries concurrently while a single writer appends to the WAL",
          "Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database file",
          "WAL checkpoint is required to prevent unbounded WAL growth\u2014auto-checkpoint triggers after configurable page count (default 1000)",
          "Readers see a consistent snapshot: a reader that starts before a commit does not see that commit's changes (snapshot isolation for reads)",
          "WAL file corruption is detected via page checksums",
          "Writers append to a separate WAL file instead of modifying the main .db file",
          "Readers search the WAL for the most recent page version before falling back to the main file",
          "Writers and multiple readers can operate simultaneously without blocking",
          "Checkpointing copies WAL pages to the main database and truncates the WAL",
          "Automatic checkpoint triggers after 1000 pages (configurable)",
          "Readers use a consistent snapshot based on the WAL state at their start time",
          "Checksums are used to detect and reject corrupted WAL frames"
        ]
      },
      {
        "id": "build-sqlite-m11",
        "title": "Aggregate Functions & JOIN",
        "anchor_id": "anc-aggregates-joins",
        "summary": "Implement aggregate functions (COUNT, SUM, AVG, MIN, MAX), GROUP BY, and basic JOIN execution. Transform from single-table queries to the full relational model.",
        "misconception": "JOINs are just nested loops \u2014 for each row in A, find matching rows in B.",
        "reveal": "JOINs ARE nested loops in SQLite's default implementation \u2014 but the order matters enormously. Joining a 1000-row table to a 1M-row table is 1000 page lookups if the small table is outer, but potentially 1M lookups if reversed. This is why the query planner's join ordering is critical, and why hash joins and merge joins exist for large datasets.",
        "cascade": [
          "MapReduce and data processing pipelines \u2014 GROUP BY is MapReduce's reduce phase; the same grouping logic applies",
          "Stream processing \u2014 windowed aggregations in Kafka Streams/Flink are continuous GROUP BY operations",
          "Dataframe joins in pandas/Polars \u2014 the same join algorithms, optimized for in-memory data",
          "GraphQL query resolution \u2014 resolving nested selections is fundamentally a join problem",
          "Entity-Component-System in game engines \u2014 ECS queries are database joins over component tables"
        ],
        "yaml_acceptance_criteria": [
          "COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values",
          "SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY groups rows by specified columns before applying aggregate functions",
          "HAVING filters groups after aggregation",
          "INNER JOIN combines rows from two tables matching a join condition",
          "Nested loop join is implemented as the baseline join algorithm",
          "JOIN with WHERE clause filters correctly after join",
          "COUNT(*) accurately counts rows including NULLs",
          "COUNT(col) ignores NULL values in the target column",
          "AVG returns a REAL/float even if input column is INTEGER",
          "GROUP BY correctly partitions aggregate states into buckets",
          "HAVING filters out aggregated groups based on result values",
          "INNER JOIN correctly combines rows from two tables using a nested loop",
          "JOIN with WHERE correctly filters rows before or during the join process",
          "COUNT(*) returns the number of rows including NULLs",
          "COUNT(col) returns count of non-NULL values only",
          "SUM produces correct results over grouped and ungrouped queries",
          "AVG returns REAL/float even if input column is INTEGER",
          "AVG ignores NULL values in computation",
          "MIN and MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY partitions rows into groups before applying aggregate functions",
          "GROUP BY without ORDER BY can return groups in any order",
          "HAVING filters groups after aggregation based on aggregate values",
          "JOIN with WHERE clause filters correctly after or during join",
          "Empty table with aggregates returns appropriate default values (0 for COUNT, NULL for SUM/AVG)",
          "Multiple aggregates can be computed in a single query"
        ]
      }
    ],
    "diagrams": [
      {
        "id": "diag-satellite-overview",
        "title": "SQLite Architecture: Satellite Map",
        "description": "System-wide view showing all components from SQL input to disk storage. Each component is an anchor point linking to detailed diagrams. Shows the three-layer architecture: Query/API layer (tokenizer, parser, compiler, VM), Storage Engine layer (buffer pool, B-tree, query planner), and Disk/Recovery layer (page files, journal, WAL).",
        "anchor_target": "anc-system-overview",
        "level": "satellite"
      },
      {
        "id": "diag-tokenizer-fsm",
        "title": "Tokenizer State Machine",
        "description": "State diagram showing character-by-character FSM transitions. States include START, IN_KEYWORD, IN_STRING, IN_NUMBER, IN_IDENTIFIER, IN_OPERATOR. Shows how escaped quotes are handled by state transitions, and how context determines token type.",
        "anchor_target": "anc-tokenizer",
        "level": "microscopic"
      },
      {
        "id": "diag-token-stream-example",
        "title": "Token Stream: SELECT Statement",
        "description": "data_walk showing how 'SELECT * FROM users WHERE id = 42' becomes a stream of typed tokens with line/column positions. Each token shows type, value, and position.",
        "anchor_target": "anc-tokenizer",
        "level": "street"
      },
      {
        "id": "diag-ast-structure",
        "title": "AST Node Hierarchy",
        "description": "structure_layout showing the AST node types: Statement (Select/Insert/CreateTable), Expression (Binary/Unary/Literal/Identifier), and their relationships. Shows how 'a OR b AND c' produces a different tree than '(a OR b) AND c'.",
        "anchor_target": "anc-parser",
        "level": "street"
      },
      {
        "id": "diag-precedence-climbing",
        "title": "Operator Precedence: Before and After",
        "description": "before_after comparing naive left-to-right parsing with correct precedence handling. Shows how 'NOT a = b AND c OR d' should be parsed as '(NOT (a = b)) AND c) OR d'.",
        "anchor_target": "anc-parser",
        "level": "microscopic"
      },
      {
        "id": "diag-ast-to-bytecode",
        "title": "Compilation Pipeline: AST to Bytecode",
        "description": "data_walk showing a SELECT AST transformed into bytecode opcodes. Traces the path from SelectStatement node through the compiler to OpenTable, Rewind, Column, ResultRow, Next, Halt sequence.",
        "anchor_target": "anc-vdbe",
        "level": "street"
      },
      {
        "id": "diag-vdbe-architecture",
        "title": "VDBE Virtual Machine Architecture",
        "description": "structure_layout of the VM showing program counter, register file, cursor array, and execution loop. Shows how one opcode is fetched, decoded, and executed per cycle.",
        "anchor_target": "anc-vdbe",
        "level": "street"
      },
      {
        "id": "diag-bytecode-where",
        "title": "WHERE Clause Bytecode Pattern",
        "description": "trace_example showing how 'WHERE col > 5' compiles to: load column value, load constant 5, compare (Gt), conditional jump to Next if false. Shows the branch pattern for row filtering.",
        "anchor_target": "anc-vdbe",
        "level": "microscopic"
      },
      {
        "id": "diag-explain-output",
        "title": "EXPLAIN Command Output",
        "description": "Example of EXPLAIN output showing opcode, operands, and comments. Demonstrates how a user can inspect the execution plan for any SQL statement.",
        "anchor_target": "anc-vdbe",
        "level": "street"
      },
      {
        "id": "diag-buffer-pool-structure",
        "title": "Buffer Pool Internal Structure",
        "description": "structure_layout showing page frames array, page table (page_id \u2192 frame_index), LRU list with head/tail pointers, and pin counts per frame. Shows how these structures interact.",
        "anchor_target": "anc-buffer-pool",
        "level": "street"
      },
      {
        "id": "diag-buffer-pool-flow",
        "title": "FetchPage: Hit vs Miss Flow",
        "description": "state_evolution showing the flow when FetchPage is called: check page table (hit \u2192 return frame), find free frame or evict (miss \u2192 load from disk \u2192 return frame). Shows dirty page write-back on eviction.",
        "anchor_target": "anc-buffer-pool",
        "level": "street"
      },
      {
        "id": "diag-lru-list",
        "title": "LRU List Operations",
        "description": "before_after showing LRU list state when page is accessed (move to head) and when eviction occurs (remove from tail). Shows doubly-linked list structure.",
        "anchor_target": "anc-buffer-pool",
        "level": "microscopic"
      },
      {
        "id": "diag-page-layout",
        "title": "B-tree Page Format: Slotted Page",
        "description": "structure_layout of 4096-byte page: header (type, cell count, free space offset), cell pointer array growing down from header, free space, cell content area growing up from end. Shows bidirectional growth.",
        "anchor_target": "anc-btree-storage",
        "level": "microscopic"
      },
      {
        "id": "diag-btree-vs-bplustree",
        "title": "B-tree vs B+tree: Structural Difference",
        "description": "before_after comparison showing B-tree (data in all nodes, used for tables) vs B+tree (data only in leaves with linked list, used for indexes). Highlights the leaf linkage in B+tree.",
        "anchor_target": "anc-btree-storage",
        "level": "street"
      },
      {
        "id": "diag-varint-encoding",
        "title": "Variable-Length Integer Encoding",
        "description": "Bit-level diagram showing how varints encode 1-9 bytes. Shows the high-bit continuation flag and payload bits for each byte. Examples: 0x7F \u2192 1 byte, 0x80 \u2192 2 bytes.",
        "anchor_target": "anc-btree-storage",
        "level": "microscopic"
      },
      {
        "id": "diag-node-split",
        "title": "B-tree Node Split Sequence",
        "description": "state_evolution showing leaf overflow \u2192 create new page \u2192 redistribute cells \u2192 promote separator to parent \u2192 update parent's child pointer. Shows the tree before, during, and after split.",
        "anchor_target": "anc-btree-storage",
        "level": "street"
      },
      {
        "id": "diag-row-serialization",
        "title": "Row Record Format",
        "description": "structure_layout showing serialized row: header with column type codes, followed by column values. Shows how NULL is encoded (type 0) vs INTEGER (type 1-6 depending on size).",
        "anchor_target": "anc-btree-storage",
        "level": "microscopic"
      },
      {
        "id": "diag-three-valued-logic",
        "title": "Three-Valued Logic Truth Tables",
        "description": "Truth tables for AND, OR, NOT with TRUE, FALSE, NULL inputs. Shows why NULL = NULL is NULL, why NULL AND FALSE is FALSE, why NULL OR TRUE is TRUE.",
        "anchor_target": "anc-dml-execution",
        "level": "street"
      },
      {
        "id": "diag-table-scan-cursor",
        "title": "Cursor-Based Table Scan",
        "description": "data_walk showing cursor movement through B-tree leaf pages. Shows cursor state at each position: page number, cell index, current rowid.",
        "anchor_target": "anc-dml-execution",
        "level": "street"
      },
      {
        "id": "diag-delete-two-pass",
        "title": "DELETE: Two-Pass Algorithm",
        "description": "trace_example showing why deletion during iteration requires two passes: first pass marks matching rows, second pass removes marked rows. Shows cursor corruption if single-pass is attempted.",
        "anchor_target": "anc-dml-execution",
        "level": "microscopic"
      },
      {
        "id": "diag-index-structure",
        "title": "Secondary Index: B+tree Structure",
        "description": "structure_layout showing index B+tree with (column_value, rowid) pairs in leaves. Shows how equality lookup traverses to leaf, how range scan follows leaf links.",
        "anchor_target": "anc-indexes",
        "level": "street"
      },
      {
        "id": "diag-double-lookup",
        "title": "Index-to-Table Double Lookup",
        "description": "data_walk showing query with index on 'name' looking for 'Alice': index B+tree finds rowid, then table B-tree retrieves full row. Shows two B-tree traversals.",
        "anchor_target": "anc-indexes",
        "level": "street"
      },
      {
        "id": "diag-index-maintenance",
        "title": "INSERT: Index Maintenance",
        "description": "state_evolution showing INSERT into table with two indexes: table B-tree insert, then index1 B+tree insert, then index2 B+tree insert. Shows the write amplification.",
        "anchor_target": "anc-indexes",
        "level": "street"
      },
      {
        "id": "diag-cost-comparison",
        "title": "Cost Model: Table Scan vs Index Scan",
        "description": "Comparison diagram showing cost formula for table scan (pages read = total_pages) vs index scan (pages read = index_height + selectivity * leaf_pages + selectivity * table_pages). Shows crossover point.",
        "anchor_target": "anc-query-planner",
        "level": "street"
      },
      {
        "id": "diag-selectivity-estimation",
        "title": "Selectivity Estimation",
        "description": "Example showing how distinct value count from ANALYZE enables selectivity estimation: column with 10 distinct values, equality predicate has 10% selectivity.",
        "anchor_target": "anc-query-planner",
        "level": "microscopic"
      },
      {
        "id": "diag-join-order",
        "title": "Join Order Impact",
        "description": "before_after comparing join orders: small table outer (few lookups) vs large table outer (many lookups). Shows why join ordering matters for performance.",
        "anchor_target": "anc-query-planner",
        "level": "street"
      },
      {
        "id": "diag-rollback-journal-flow",
        "title": "Rollback Journal: Write Sequence",
        "description": "state_evolution showing transaction lifecycle: BEGIN \u2192 journal original pages \u2192 fsync journal \u2192 modify database \u2192 COMMIT \u2192 fsync database \u2192 delete journal. Shows crash points and recovery behavior.",
        "anchor_target": "anc-transactions-journal",
        "level": "street"
      },
      {
        "id": "diag-crash-recovery",
        "title": "Crash Recovery: Hot Journal Detection",
        "description": "Flow diagram showing startup recovery: check for journal file \u2192 if exists, restore original pages from journal \u2192 delete journal \u2192 database is consistent.",
        "anchor_target": "anc-transactions-journal",
        "level": "street"
      },
      {
        "id": "diag-fsync-ordering",
        "title": "Write Ordering: Why fsync Matters",
        "description": "before_after comparing correct ordering (journal fsync before database write) vs incorrect ordering (crash corrupts database). Shows why write ordering is a correctness issue, not performance.",
        "anchor_target": "anc-transactions-journal",
        "level": "microscopic"
      },
      {
        "id": "diag-wal-structure",
        "title": "WAL File Format",
        "description": "structure_layout showing WAL file: header, followed by frames (each frame = page number + checksum + page data). Shows how readers find the most recent version of a page.",
        "anchor_target": "anc-wal-mode",
        "level": "microscopic"
      },
      {
        "id": "diag-wal-vs-journal",
        "title": "Rollback Journal vs WAL: Architectural Difference",
        "description": "before_after comparing rollback journal (old pages for undo) vs WAL (new pages for redo). Shows why WAL enables concurrent readers.",
        "anchor_target": "anc-wal-mode",
        "level": "street"
      },
      {
        "id": "diag-wal-checkpoint",
        "title": "WAL Checkpoint Process",
        "description": "state_evolution showing checkpoint: read WAL frames \u2192 write pages to main database \u2192 truncate WAL. Shows auto-checkpoint threshold.",
        "anchor_target": "anc-wal-mode",
        "level": "street"
      },
      {
        "id": "diag-snapshot-isolation",
        "title": "Snapshot Isolation: Reader View",
        "description": "Example showing two readers starting at different times, each seeing a consistent snapshot of the database. Shows how reader tracks its 'end mark' in the WAL.",
        "anchor_target": "anc-wal-mode",
        "level": "street"
      },
      {
        "id": "diag-concurrent-read-write",
        "title": "WAL Mode: Concurrent Readers and Writer",
        "description": "Timeline showing writer appending to WAL while multiple readers query without blocking. Shows the read/write concurrency that rollback journal cannot provide.",
        "anchor_target": "anc-wal-mode",
        "level": "street"
      },
      {
        "id": "diag-group-by-execution",
        "title": "GROUP BY: Hash Aggregation",
        "description": "data_walk showing rows flowing into hash table keyed by GROUP BY columns, with aggregate state (COUNT, SUM) updated per row. Shows final result extraction.",
        "anchor_target": "anc-aggregates-joins",
        "level": "street"
      },
      {
        "id": "diag-nested-loop-join",
        "title": "Nested Loop Join Algorithm",
        "description": "trace_example showing outer loop iterating table A, inner loop finding matching rows in table B via index lookup. Shows O(n*m) complexity characteristic.",
        "anchor_target": "anc-aggregates-joins",
        "level": "street"
      },
      {
        "id": "diag-having-vs-where",
        "title": "HAVING vs WHERE: Filter Timing",
        "description": "before_after showing WHERE filtering rows before aggregation, HAVING filtering groups after aggregation. Shows why you can't use aggregate functions in WHERE.",
        "anchor_target": "anc-aggregates-joins",
        "level": "street"
      },
      {
        "id": "diag-null-in-aggregates",
        "title": "NULL Handling in Aggregate Functions",
        "description": "Example table with NULL values showing how COUNT(*), COUNT(col), SUM(col), AVG(col) each handle NULLs differently. Shows why COUNT(*) \u2260 COUNT(col) when NULLs exist.",
        "anchor_target": "anc-aggregates-joins",
        "level": "microscopic"
      }
    ]
  },
  "accumulated_md": "# Build Your Own SQLite\n\nBuild a complete embedded SQL database engine from scratch, implementing every layer from SQL tokenization to ACID transactions. This project teaches the fundamental architecture shared by all relational databases: how SQL text becomes executable bytecode, how B-trees organize data on disk, and how write-ahead logging enables crash recovery without losing committed transactions. You'll implement SQLite's virtual database engine (VDBE) \u2014 a register-based bytecode interpreter that executes compiled SQL statements \u2014 and understand why this design enables SQL's expressive power while maintaining predictable performance. The project culminates in two transaction modes: rollback journal for simple crash recovery and WAL mode for concurrent readers during writes.\n\n\n\n<!-- MS_ID: build-sqlite-m1 -->\n# SQL Tokenizer: The Frontline of Understanding\nYour database engine receives a raw string \u2014 a sequence of bytes with no inherent meaning. Before it can execute a query, before it can plan joins or traverse B-trees, it must answer a fundamental question: **what are the building blocks here?**\nThis is lexical analysis: transforming a character stream into a token stream. It's the first step in the compilation pipeline that converts SQL text into executable bytecode.\n```\n\"SELECT * FROM users WHERE id = 42\"\n         \u2193 Tokenizer\n[SELECT] [STAR] [FROM] [IDENTIFIER:users] [WHERE] [IDENTIFIER:id] [EQUALS] [INTEGER:42]\n```\nYou're building the eyes of your database. Let's make them sharp.\n---\n## The Tension: Context Determines Meaning\nHere's a puzzle. Consider these three SQL fragments:\n```sql\n-- Fragment 1\nSELECT name FROM users\n-- Fragment 2\n'It''s a SELECT statement'\n-- Fragment 3\n-- SELECT is commented out\nINSERT INTO logs VALUES ('ignored')\n```\nIn fragment 1, `SELECT` is a keyword. In fragment 2, `SELECT` is just characters inside a string literal \u2014 it has no special meaning. In fragment 3, `-- SELECT` is a comment, and the word should never even produce a token.\nNow consider the hyphen character `-`:\n```sql\n-- This is a comment\nSELECT 5 - 3\n```\nThe first `-` starts a comment that consumes everything until end-of-line. The second `-` is a subtraction operator. The **same character** means completely different things based on **what came before it**.\nThis is why tokenization requires a **finite state machine (FSM)** \u2014 a computational model where the system's behavior depends on its current state plus its input. You can't tokenize correctly by splitting on whitespace or scanning with simple pattern matching. Context matters.\n### The Revelation: Why String Splitting Fails\nYou might think: \"I'll just split on spaces and classify each piece.\" Let's watch that approach shatter:\n```sql\nSELECT * FROM users WHERE name = 'John Doe'\n```\nSplitting on spaces gives you `['SELECT', '*', 'FROM', 'users', 'WHERE', 'name', '=', \"'John\", \"Doe'\"]`. The string literal got torn apart. And this is the simple case.\n```sql\nSELECT 'it''s working' FROM \"my table\"\n```\nHere you have escaped quotes inside a string (`''` represents a single quote character), and a quoted identifier with a space in it. String splitting cannot handle this.\n```sql\nSELECT a<b FROM t\n```\nIs `<b` part of the token? No \u2014 `<` is an operator, `b` is an identifier. But `<` might also be the start of `<=` or `<>`. You can't know until you look at the **next** character.\nAnd then there's this:\n```sql\nSELECT -5 FROM t\n```\nIs `-5` a negative integer literal? Not necessarily. The tokenizer produces `[SELECT] [MINUS] [INTEGER:5]`. The **parser** decides whether that minus is unary negation or subtraction based on context. The tokenizer's job is to identify atomic units, not interpret their meaning.\nThis is the fundamental insight: **tokenization is character-by-character state tracking, not string manipulation.** Every real SQL tokenizer is a state machine. Some are generated by tools like `lex` or `flex`; you'll write one by hand, which means you'll understand exactly what those tools generate.\n---\n## Token Types: The Vocabulary of SQL\nBefore building the state machine, you need to define what you're looking for. SQL tokens fall into distinct categories:\n### Keywords\nReserved words with special meaning in SQL syntax:\n```\nSELECT, FROM, WHERE, INSERT, INTO, VALUES, UPDATE, SET, DELETE,\nCREATE, TABLE, INDEX, DROP, ALTER, AND, OR, NOT, NULL, IS, IN,\nLIKE, BETWEEN, JOIN, LEFT, RIGHT, INNER, OUTER, ON, GROUP, BY,\nORDER, ASC, DESC, LIMIT, OFFSET, DISTINCT, PRIMARY, KEY, UNIQUE,\nFOREIGN, REFERENCES, CHECK, DEFAULT, COLLATE, ASC, DESC, CAST,\nAS, CASE, WHEN, THEN, ELSE, END, EXISTS, UNION, EXCEPT, INTERSECT\n```\nKeywords are **case-insensitive** in SQL. `SELECT`, `select`, and `SeLeCt` are all the same keyword. Your tokenizer should normalize them (typically to uppercase) for consistency.\n### Identifiers\nNames for tables, columns, indexes, and other database objects:\n```sql\nusers          -- unquoted identifier\nmy_table       -- unquoted identifier  \n\"column name\"  -- quoted identifier (preserves case, allows spaces)\n\"Table\"        -- quoted identifier (case-sensitive)\n```\nUnquoted identifiers are typically case-insensitive (folded to lowercase or uppercase \u2014 SQLite folds to uppercase internally, PostgreSQL to lowercase). Quoted identifiers preserve case exactly and can contain any character including spaces and special symbols.\n### Literals\nFixed values in the SQL text:\n```sql\n42              -- integer literal\n3.14159         -- floating-point literal\n-17             -- NOT a negative literal; this is [MINUS] [INTEGER:17]\n'hello world'   -- string literal\n'it''s a test'  -- string with escaped quote ('' \u2192 ')\nX'4A6F686E'     -- blob literal (hex-encoded binary)\nNULL            -- null literal (keyword, but special)\nTRUE, FALSE     -- boolean literals (in some SQL dialects)\n```\nString literals use **single quotes**. To include a single quote inside a string, double it: `'it''s'` becomes the string `it's`.\n### Operators and Punctuation\n```sql\n+ - * / %       -- arithmetic operators\n= < > <= >= <> !=   -- comparison operators\n( ) , ; .       -- punctuation\n||              -- string concatenation (SQL standard)\n```\nNote: `<>` and `!=` both mean \"not equal\" \u2014 your tokenizer should recognize both. The two-character operators like `<=`, `>=`, `<>`, `!=` require looking ahead before committing to the first character.\n### Comments (Typically Discarded)\n```sql\n-- single line comment (until end of line)\n/* multi-line\n   comment */\n```\nComments are usually stripped during tokenization \u2014 they don't produce tokens. But you might preserve them for tooling (syntax highlighters, documentation generators).\n---\n## The Tokenizer State Machine\n{{DIAGRAM:diag-tokenizer-fsm}}\nAt its core, your tokenizer is a loop that examines the input character by character, maintains state, and emits tokens. Here's the structure:\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     TOKENIZER STATES                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    whitespace    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502  START  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502  START  \u2502 (self-loop)      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502       \u2502                                                     \u2502\n\u2502       \u2502 letter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502       \u2502                   \u2502 IDENTIFIER \u2502                   \u2502\n\u2502       \u2502                   \u2502 or KEYWORD \u2502                   \u2502\n\u2502       \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502       \u2502                                                     \u2502\n\u2502       \u2502 digit \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502       \u2502                   \u2502   NUMBER   \u2502                   \u2502\n\u2502       \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502       \u2502                                                     \u2502\n\u2502       \u2502 ' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    ''    \u250c\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502       \u2502                   \u2502   STRING   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502ESC \u2502 \u2502\n\u2502       \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2514\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502                          \u2502 '                       \u2502\n\u2502       \u2502                          \u25bc                         \u2502\n\u2502       \u2502                    emit STRING                     \u2502\n\u2502       \u2502                                                     \u2502\n\u2502       \u2502 \" \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \"\"    \u250c\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502       \u2502                   \u2502  QUOTED_ID  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502ESC \u2502 \u2502\n\u2502       \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2514\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502                                                     \u2502\n\u2502       \u2502 - \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502       \u2502                   \u2502  MINUS or   \u2502                  \u2502\n\u2502       \u2502                   \u2502  COMMENT    \u2502                  \u2502\n\u2502       \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502       \u2502                                                     \u2502\n\u2502       \u2502 operator chars \u2500\u2500\u2500\u25ba \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502       \u2502                   \u2502  OPERATOR   \u2502                 \u2502\n\u2502       \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n### The Token Data Structure\nEach token needs to capture:\n```c\ntypedef enum {\n    TOKEN_KEYWORD,      // SELECT, FROM, WHERE, etc.\n    TOKEN_IDENTIFIER,   // table_name, column_name\n    TOKEN_STRING,       // 'hello world'\n    TOKEN_INTEGER,      // 42\n    TOKEN_FLOAT,        // 3.14\n    TOKEN_OPERATOR,     // =, <, >, <=, >=, <>, !=\n    TOKEN_PUNCTUATION,  // (, ), ,, ;\n    TOKEN_EOF,          // end of input\n    TOKEN_ERROR         // lexical error\n} TokenType;\ntypedef struct {\n    TokenType type;\n    char* value;        // the text content (owned by token)\n    int line;           // 1-based line number\n    int column;         // 1-based column number\n} Token;\n```\nThe line and column are crucial for error messages. When the parser encounters a syntax error, it needs to tell the user **where** the problem is:\n```\nError: unexpected token at line 3, column 15\n  SELECT * FORM users\n                ^\n  Did you mean FROM?\n```\n### Position Tracking\nYou'll maintain position state as you scan:\n```c\ntypedef struct {\n    const char* input;      // the source SQL text\n    size_t input_length;    // total length\n    size_t pos;             // current position in input (0-based)\n    int line;               // current line (1-based)\n    int column;             // current column (1-based)\n} Lexer;\n```\nEvery time you advance `pos`, you update `line` and `column`:\n```c\nvoid advance(Lexer* lexer) {\n    if (lexer->pos >= lexer->input_length) return;\n    if (lexer->input[lexer->pos] == '\\n') {\n        lexer->line++;\n        lexer->column = 1;\n    } else {\n        lexer->column++;\n    }\n    lexer->pos++;\n}\n```\nFor multi-character advances (like consuming an entire string literal), you loop the single-character advance to keep position tracking correct.\n---\n## Implementing the State Machine\nLet's walk through each major state.\n### Start State: Character Classification\nAt the start of each token, you examine the current character and branch:\n```c\nToken next_token(Lexer* lexer) {\n    skip_whitespace(lexer);\n    if (lexer->pos >= lexer->input_length) {\n        return make_token(lexer, TOKEN_EOF, \"\");\n    }\n    char c = lexer->input[lexer->pos];\n    if (is_alpha(c) || c == '_') {\n        return read_identifier_or_keyword(lexer);\n    }\n    if (is_digit(c)) {\n        return read_number(lexer);\n    }\n    if (c == '\\'') {\n        return read_string(lexer);\n    }\n    if (c == '\"') {\n        return read_quoted_identifier(lexer);\n    }\n    if (c == '-') {\n        return handle_minus_or_comment(lexer);\n    }\n    if (is_operator_start(c)) {\n        return read_operator(lexer);\n    }\n    if (is_punctuation(c)) {\n        return read_punctuation(lexer);\n    }\n    // Unknown character\n    return make_error_token(lexer, \"unrecognized character\");\n}\n```\n### Identifier/Keyword State\nWhen you encounter a letter or underscore, you're reading either an identifier or a keyword. Read until you hit a non-identifier character, then check if the result is a keyword:\n```c\nToken read_identifier_or_keyword(Lexer* lexer) {\n    int start_line = lexer->line;\n    int start_col = lexer->column;\n    // Read alphanumeric + underscore sequence\n    size_t start = lexer->pos;\n    while (lexer->pos < lexer->input_length) {\n        char c = lexer->input[lexer->pos];\n        if (!is_alnum(c) && c != '_') break;\n        advance(lexer);\n    }\n    // Extract the text\n    size_t length = lexer->pos - start;\n    char* text = extract_substring(lexer->input, start, length);\n    // Check if it's a keyword (case-insensitive)\n    TokenType type = TOKEN_IDENTIFIER;\n    char* upper = to_uppercase(text);\n    if (is_keyword(upper)) {\n        type = TOKEN_KEYWORD;\n        // Keep uppercase form for keywords\n        free(text);\n        text = upper;\n    } else {\n        free(upper);\n        // For identifiers, preserve original case or normalize\n    }\n    return make_token_with_position(lexer, type, text, start_line, start_col);\n}\n```\nThe keyword lookup can be a hash table, a sorted array with binary search, or a trie. For a hand-rolled lexer, a sorted array with `strcmp` comparisons works well:\n```c\nstatic const char* KEYWORDS[] = {\n    \"ADD\", \"ALL\", \"ALTER\", \"AND\", \"AS\", \"ASC\", \"BETWEEN\", \"BY\",\n    \"CASE\", \"CHECK\", \"COLLATE\", \"COLUMN\", \"CONSTRAINT\", \"CREATE\",\n    \"DEFAULT\", \"DELETE\", \"DESC\", \"DISTINCT\", \"DROP\", \"ELSE\",\n    \"END\", \"ESCAPE\", \"EXCEPT\", \"EXISTS\", \"FOREIGN\", \"FROM\",\n    \"GROUP\", \"HAVING\", \"IN\", \"INDEX\", \"INNER\", \"INSERT\", \"INTO\",\n    \"IS\", \"JOIN\", \"KEY\", \"LEFT\", \"LIKE\", \"LIMIT\", \"NOT\", \"NULL\",\n    \"ON\", \"OR\", \"ORDER\", \"OUTER\", \"PRIMARY\", \"REFERENCES\", \"RIGHT\",\n    \"SELECT\", \"SET\", \"TABLE\", \"THEN\", \"UNION\", \"UNIQUE\", \"UPDATE\",\n    \"USING\", \"VALUES\", \"WHEN\", \"WHERE\", /* ... */\n};\nbool is_keyword(const char* text) {\n    // Binary search in sorted KEYWORDS array\n    int lo = 0, hi = KEYWORD_COUNT - 1;\n    while (lo <= hi) {\n        int mid = (lo + hi) / 2;\n        int cmp = strcmp(text, KEYWORDS[mid]);\n        if (cmp == 0) return true;\n        if (cmp < 0) hi = mid - 1;\n        else lo = mid + 1;\n    }\n    return false;\n}\n```\n### Number State\nNumbers in SQL can be integers or floating-point. The grammar:\n```\ninteger:     [0-9]+\nfloat:       [0-9]+ '.' [0-9]* \n           | [0-9]* '.' [0-9]+\n           | [0-9]+ ('e'|'E') ('+'|'-')? [0-9]+   // scientific notation\n```\n```c\nToken read_number(Lexer* lexer) {\n    int start_line = lexer->line;\n    int start_col = lexer->column;\n    size_t start = lexer->pos;\n    bool is_float = false;\n    // Read integer part\n    while (lexer->pos < lexer->input_length && is_digit(lexer->input[lexer->pos])) {\n        advance(lexer);\n    }\n    // Check for decimal point\n    if (lexer->pos < lexer->input_length && lexer->input[lexer->pos] == '.') {\n        is_float = true;\n        advance(lexer);\n        // Read fractional part\n        while (lexer->pos < lexer->input_length && is_digit(lexer->input[lexer->pos])) {\n            advance(lexer);\n        }\n    }\n    // Check for exponent\n    if (lexer->pos < lexer->input_length && \n        (lexer->input[lexer->pos] == 'e' || lexer->input[lexer->pos] == 'E')) {\n        is_float = true;\n        advance(lexer);\n        // Optional sign\n        if (lexer->pos < lexer->input_length && \n            (lexer->input[lexer->pos] == '+' || lexer->input[lexer->pos] == '-')) {\n            advance(lexer);\n        }\n        // Exponent digits (required)\n        if (lexer->pos < lexer->input_length && is_digit(lexer->input[lexer->pos])) {\n            while (lexer->pos < lexer->input_length && is_digit(lexer->input[lexer->pos])) {\n                advance(lexer);\n            }\n        } else {\n            return make_error_token(lexer, \"expected exponent digits\");\n        }\n    }\n    size_t length = lexer->pos - start;\n    char* text = extract_substring(lexer->input, start, length);\n    TokenType type = is_float ? TOKEN_FLOAT : TOKEN_INTEGER;\n    return make_token_with_position(lexer, type, text, start_line, start_col);\n}\n```\n**Why doesn't the tokenizer handle negative numbers?** Consider:\n```sql\nSELECT 5 - 3\nSELECT -3\n```\nIn the first query, `-` is the subtraction operator. In the second, `-3` could be a negative number, but it could also be parsed as unary negation applied to `3`. The tokenizer can't know which interpretation is correct \u2014 it lacks syntactic context. The safe choice: tokenize `-` as an operator and let the parser determine meaning.\n### String Literal State\nString literals are enclosed in single quotes with doubled quotes as escapes:\n```c\nToken read_string(Lexer* lexer) {\n    int start_line = lexer->line;\n    int start_col = lexer->column;\n    advance(lexer);  // consume opening quote\n    StringBuilder* sb = string_builder_new();\n    while (lexer->pos < lexer->input_length) {\n        char c = lexer->input[lexer->pos];\n        if (c == '\\'') {\n            // Check for escaped quote\n            if (lexer->pos + 1 < lexer->input_length && \n                lexer->input[lexer->pos + 1] == '\\'') {\n                // Escaped quote: add single quote to result\n                string_builder_append(sb, \"'\");\n                advance(lexer);\n                advance(lexer);\n            } else {\n                // End of string\n                advance(lexer);  // consume closing quote\n                break;\n            }\n        } else if (c == '\\n') {\n            // SQL allows multi-line strings\n            string_builder_append_char(sb, c);\n            advance(lexer);\n        } else {\n            string_builder_append_char(sb, c);\n            advance(lexer);\n        }\n    }\n    if (lexer->pos >= lexer->input_length && lexer->input[lexer->pos - 1] != '\\'') {\n        string_builder_free(sb);\n        return make_error_token(lexer, \"unterminated string literal\");\n    }\n    char* value = string_builder_to_string(sb);\n    string_builder_free(sb);\n    return make_token_with_position(lexer, TOKEN_STRING, value, start_line, start_col);\n}\n```\nThe key insight: `''` inside a string represents a single quote character. The string `'it''s working'` has the value `it's working` (11 characters, not 12).\n### Quoted Identifier State\nQuoted identifiers use double quotes and follow the same escape pattern:\n```sql\n\"My Table\"      -- identifier with space\n\"column\"\"name\"  -- identifier containing a quote character\n```\nThe implementation mirrors string literals but produces `TOKEN_IDENTIFIER` instead of `TOKEN_STRING`:\n```c\nToken read_quoted_identifier(Lexer* lexer) {\n    // Similar to read_string, but:\n    // 1. Uses double quotes as delimiters\n    // 2. Returns TOKEN_IDENTIFIER\n    // 3. Preserves case exactly (no normalization)\n}\n```\n### Comment Handling\nSQL supports two comment styles:\n```sql\n-- Single-line comment (until newline or EOF)\n/* Multi-line comment (can span lines) */\n```\nFor the tokenizer, comments are typically **skipped entirely** \u2014 they don't produce tokens:\n```c\nToken handle_minus_or_comment(Lexer* lexer) {\n    if (lexer->pos + 1 < lexer->input_length && \n        lexer->input[lexer->pos + 1] == '-') {\n        // Line comment: skip until newline or EOF\n        advance(lexer);\n        advance(lexer);\n        while (lexer->pos < lexer->input_length && \n               lexer->input[lexer->pos] != '\\n') {\n            advance(lexer);\n        }\n        // Skip the newline too (or we're at EOF)\n        if (lexer->pos < lexer->input_length) {\n            advance(lexer);\n        }\n        // Recursively get the next real token\n        return next_token(lexer);\n    } else {\n        // Just a minus operator\n        return read_operator(lexer);\n    }\n}\nvoid skip_block_comment(Lexer* lexer) {\n    // Assumes we've already consumed /*\n    while (lexer->pos + 1 < lexer->input_length) {\n        if (lexer->input[lexer->pos] == '*' && \n            lexer->input[lexer->pos + 1] == '/') {\n            advance(lexer);\n            advance(lexer);\n            return;\n        }\n        advance(lexer);\n    }\n    // Error: unterminated block comment\n}\n```\n### Operator Recognition\nOperators require lookahead because one character might be a complete operator or the start of a two-character operator:\n```c\nToken read_operator(Lexer* lexer) {\n    int start_line = lexer->line;\n    int start_col = lexer->column;\n    char c = lexer->input[lexer->pos];\n    // Check for two-character operators\n    if (lexer->pos + 1 < lexer->input_length) {\n        char next = lexer->input[lexer->pos + 1];\n        char two_char[3] = {c, next, '\\0'};\n        if (strcmp(two_char, \"<=\") == 0 ||\n            strcmp(two_char, \">=\") == 0 ||\n            strcmp(two_char, \"<>\") == 0 ||\n            strcmp(two_char, \"!=\") == 0 ||\n            strcmp(two_char, \"||\") == 0) {\n            advance(lexer);\n            advance(lexer);\n            return make_token_with_position(lexer, TOKEN_OPERATOR, \n                                            strdup(two_char), start_line, start_col);\n        }\n    }\n    // Single-character operator\n    advance(lexer);\n    char* value = char_to_string(c);\n    return make_token_with_position(lexer, TOKEN_OPERATOR, value, start_line, start_col);\n}\n```\nThe operators `<>` and `!=` are semantically equivalent (both mean \"not equal\"), but the tokenizer preserves the distinction. The parser or a later phase can normalize them.\n---\n## The Token Stream\n\n![Token Stream: SELECT Statement](./diagrams/diag-token-stream-example.svg)\n\nThe tokenizer produces a stream of tokens. For a database engine, you have two design choices:\n### 1. Eager Tokenization\nTokenize the entire input upfront, producing a list/array of tokens:\n```c\ntypedef struct {\n    Token* tokens;\n    size_t count;\n    size_t capacity;\n} TokenStream;\nTokenStream* tokenize(const char* input) {\n    Lexer lexer = init_lexer(input);\n    TokenStream* stream = create_token_stream();\n    while (true) {\n        Token token = next_token(&lexer);\n        token_stream_add(stream, token);\n        if (token.type == TOKEN_EOF || token.type == TOKEN_ERROR) {\n            break;\n        }\n    }\n    return stream;\n}\n```\n**Pros**: Easy to implement, enables lookahead anywhere in the parser, good error recovery (parser can skip tokens and continue).\n**Cons**: Memory overhead for large queries (though SQL queries are rarely that large), no streaming capability.\n### 2. Lazy/On-Demand Tokenization\nProduce tokens one at a time as the parser requests them:\n```c\ntypedef struct {\n    Lexer lexer;\n    Token current;      // current token (peeked or consumed)\n    Token next;         // one-token lookahead\n    bool has_next;      // is next valid?\n} TokenStream;\nToken peek_token(TokenStream* stream) {\n    if (!stream->has_next) {\n        stream->next = next_token(&stream->lexer);\n        stream->has_next = true;\n    }\n    return stream->next;\n}\nToken consume_token(TokenStream* stream) {\n    if (stream->has_next) {\n        stream->current = stream->next;\n        stream->has_next = false;\n    } else {\n        stream->current = next_token(&stream->lexer);\n    }\n    return stream->current;\n}\n```\n**Pros**: Memory efficient, natural for recursive-descent parsers that typically need one-token lookahead.\n**Cons**: Harder to implement multi-token lookahead, error recovery is more complex.\nFor your SQLite clone, **eager tokenization** is the simpler choice. SQL queries are bounded in size (typically kilobytes, not megabytes), so memory isn't a concern. The parser will thank you for random access to the token array.\n---\n## Error Handling: Position is Everything\nWhen tokenization fails, the error message must be precise:\n```sql\nSELECT name FROM users WHERE id = @value\n                                       ^\nError: unrecognized character '@' at line 1, column 30\n```\nThe caret (`^`) and the line/column information come directly from your token's position fields:\n```c\nToken make_error_token(Lexer* lexer, const char* message) {\n    Token token;\n    token.type = TOKEN_ERROR;\n    token.value = strdup(message);\n    token.line = lexer->line;\n    token.column = lexer->column;\n    return token;\n}\n```\nFor multi-character constructs like strings, record the **start position** when you begin reading, not the end position when you detect the error:\n```c\nToken read_string(Lexer* lexer) {\n    int start_line = lexer->line;\n    int start_col = lexer->column;\n    // ... read string ...\n    if (unterminated) {\n        // Report where the string STARTED, not where we ran out of input\n        return make_error_token_at(lexer, \"unterminated string literal\", \n                                   start_line, start_col);\n    }\n}\n```\nThis matters for long strings. If a 500-character string is missing its closing quote, you want the error pointing at the opening quote, not 500 characters later.\n---\n## The Three-Level View\nLet's examine tokenization at three levels of abstraction:\n### Level 1: Application (SQL Input \u2192 Token List)\nAt the API level, the tokenizer is a function:\n```c\nTokenStream* tokenize(const char* sql);\n```\nInput: a SQL string. Output: a list of tokens. The caller doesn't care about states or character loops \u2014 they want the tokens.\n### Level 2: Engine (State Machine Loop)\nInside the tokenizer, a state machine processes character by character:\n```c\nwhile (pos < length) {\n    char c = input[pos];\n    switch (current_state) {\n        case START:\n            if (is_alpha(c)) transition_to(IDENTIFIER);\n            else if (is_digit(c)) transition_to(NUMBER);\n            else if (c == '\\'') transition_to(STRING);\n            // ...\n            break;\n        case IDENTIFIER:\n            if (is_alnum(c) || c == '_') continue_reading();\n            else emit_token_and_return_to_start();\n            break;\n        // ...\n    }\n}\n```\nThe state machine encodes SQL's lexical grammar \u2014 the rules for what constitutes a valid token.\n### Level 3: Implementation (Memory and Pointers)\nAt the lowest level, the tokenizer is pointer manipulation:\n```c\n// Current position in input\nconst char* cursor = input;\n// Read a character\nchar c = *cursor;\n// Advance\ncursor++;\n// Check if more input\nif (cursor < input_end) { /* ... */ }\n// Extract substring for token value\nsize_t length = cursor - token_start;\nchar* value = malloc(length + 1);\nmemcpy(value, token_start, length);\nvalue[length] = '\\0';\n```\nMemory management is critical: each token owns its `value` string. When tokens are freed, their values must be freed too. A token stream needs a `free_token_stream()` function that walks all tokens and frees their strings.\n---\n## Testing Strategy\nYour tokenizer needs comprehensive tests. Here's a systematic approach:\n### 1. Keyword Recognition\n```c\nvoid test_keywords() {\n    assert_tokens(\"SELECT\", keyword(\"SELECT\"));\n    assert_tokens(\"select\", keyword(\"SELECT\"));  // case-insensitive\n    assert_tokens(\"sElEcT\", keyword(\"SELECT\"));\n    assert_tokens(\"FROM WHERE JOIN\", \n        keyword(\"FROM\"), keyword(\"WHERE\"), keyword(\"JOIN\"));\n}\n```\n### 2. String Literals\n```c\nvoid test_strings() {\n    assert_tokens(\"'hello'\", string(\"hello\"));\n    assert_tokens(\"'it''s'\", string(\"it's\"));  // escaped quote\n    assert_tokens(\"''''\", string(\"'\"));        // just a quote\n    assert_tokens(\"''\", string(\"\"));           // empty string\n    assert_tokens(\"'multi\\nline'\", string(\"multi\\nline\"));\n}\n```\n### 3. Numbers\n```c\nvoid test_numbers() {\n    assert_tokens(\"42\", integer(42));\n    assert_tokens(\"0\", integer(0));\n    assert_tokens(\"3.14\", float_val(3.14));\n    assert_tokens(\".5\", float_val(0.5));\n    assert_tokens(\"1e10\", float_val(1e10));\n    assert_tokens(\"1.5e-3\", float_val(0.0015));\n    assert_tokens(\"1e+\", error_token());  // incomplete exponent\n}\n```\n### 4. Operators and Punctuation\n```c\nvoid test_operators() {\n    assert_tokens(\"=\", operator(\"=\"));\n    assert_tokens(\"<=\", operator(\"<=\"));\n    assert_tokens(\"<>\", operator(\"<>\"));\n    assert_tokens(\"!=\", operator(\"!=\"));\n    assert_tokens(\"||\", operator(\"||\"));\n    assert_tokens(\"()\", punct(\"(\"), punct(\")\"));\n    assert_tokens(\";\", punct(\";\"));\n}\n```\n### 5. Identifiers\n```c\nvoid test_identifiers() {\n    assert_tokens(\"users\", identifier(\"users\"));\n    assert_tokens(\"_private\", identifier(\"_private\"));\n    assert_tokens(\"table123\", identifier(\"table123\"));\n    assert_tokens(\"\\\"my table\\\"\", identifier(\"my table\"));  // quoted\n    assert_tokens(\"\\\"col\\\"\\\"name\\\"\", identifier(\"col\\\"name\"));  // escaped quote\n}\n```\n### 6. Error Cases\n```c\nvoid test_errors() {\n    assert_tokens(\"@\", error_at(1, 1));\n    assert_tokens(\"SELECT @ FROM t\", \n        keyword(\"SELECT\"), error_at(1, 8), keyword(\"FROM\"), identifier(\"t\"));\n    assert_tokens(\"'unterminated\", error_at(1, 1));\n}\n```\n### 7. Complex Queries\n```c\nvoid test_complex_queries() {\n    const char* sql = \"SELECT id, name FROM users WHERE age >= 18 AND status = 'active'\";\n    assert_tokens(sql,\n        keyword(\"SELECT\"), identifier(\"id\"), punct(\",\"),\n        identifier(\"name\"), keyword(\"FROM\"), identifier(\"users\"),\n        keyword(\"WHERE\"), identifier(\"age\"), operator(\">=\"),\n        integer(18), keyword(\"AND\"), identifier(\"status\"),\n        operator(\"=\"), string(\"active\")\n    );\n}\n```\nBuild a test harness that compares token sequences:\n```c\nvoid assert_tokens(const char* input, Token expected[], size_t expected_count) {\n    TokenStream* stream = tokenize(input);\n    for (size_t i = 0; i < expected_count; i++) {\n        Token actual = stream->tokens[i];\n        Token expect = expected[i];\n        assert(actual.type == expect.type);\n        if (expect.value != NULL) {\n            assert(strcmp(actual.value, expect.value) == 0);\n        }\n        if (expect.line > 0) {\n            assert(actual.line == expect.line);\n            assert(actual.column == expect.column);\n        }\n    }\n    free_token_stream(stream);\n}\n```\n---\n## Performance Considerations\nYour tokenizer will be called for every SQL statement. It should be fast. Key optimizations:\n### 1. Minimize Allocations\nEach token's `value` string requires allocation. For keywords, you can avoid allocation by using a pointer to a static string:\n```c\n// For keywords, value points to a constant string (no allocation)\nToken make_keyword_token(const char* keyword) {\n    Token token;\n    token.type = TOKEN_KEYWORD;\n    token.value = (char*)keyword;  // cast away const for API consistency\n    token.line = /* ... */;\n    token.column = /* ... */;\n    return token;\n}\n```\n### 2. Buffer the Input\nIf reading from a file or network, buffer the entire input before tokenizing. Character-by-character I/O is slow.\n### 3. Use Pointer Arithmetic\nInstead of maintaining a `pos` index into the input string, use pointers directly:\n```c\nconst char* cursor;   // current position\nconst char* start;    // start of current token\nconst char* end;      // end of input\n// Read character\nchar c = *cursor;\n// Check for more input\nif (cursor < end) { /* ... */ }\n// Token length\nsize_t length = cursor - start;\n```\n### 4. Inline Hot Paths\nFunctions like `is_alpha`, `is_digit`, and `advance` should be inlined (or macros) since they're called for every character.\n---\n## What You've Built\nYour tokenizer is a character-by-character state machine that:\n1. **Tracks position** (line and column) for error reporting\n2. **Recognizes keywords** case-insensitively\n3. **Handles string literals** with escaped quote handling (`''` \u2192 `'`)\n4. **Distinguishes integers from floats** (but leaves negative numbers to the parser)\n5. **Supports quoted identifiers** for names with special characters\n6. **Skips comments** (both `--` and `/* */` styles)\n7. **Produces precise errors** with location information\nThis tokenizer is the foundation for your SQL parser. Every subsequent layer \u2014 the AST, the bytecode compiler, the virtual machine \u2014 depends on correct tokenization. A bug here will cascade into mysterious failures in query execution.\n---\n## Knowledge Cascade\nTokenization is a universal technique with connections across computing:\n### 1. Compiler Frontends Everywhere\nThe same FSM pattern powers every language processor:\n- **JSON parsers**: similar state machine for strings, numbers, and literals\n- **Config file readers**: INI, YAML, TOML all need tokenization\n- **Template engines**: Mustache, Jinja, and friends tokenize the template syntax\n- **Shell command parsers**: bash tokenizes with even more complexity (quoting rules, expansion)\nUnderstanding SQL tokenization means you can tokenize almost anything.\n### 2. Lexer Generators (flex, ANTLR, re2c)\nTools like `flex` generate state machine code from a regex-based specification:\n```flex\n[0-9]+          { return INTEGER; }\n[0-9]+\".\"[0-9]* { return FLOAT; }\n'[']*''[']*'*   { return STRING; }\nSELECT|FROM     { return KEYWORD; }\n```\nNow you understand what they generate and why. A handwritten lexer can be faster (no regex overhead) and more debuggable (you wrote the code).\n### 3. UTF-8 and Unicode Handling\nOnce you handle escaped quotes, you're one step from handling multi-byte characters. UTF-8 is a variable-length encoding:\n```\nASCII characters: 1 byte (0xxxxxxx)\nExtended chars:   2-4 bytes (11xxxxxx starts a multi-byte sequence)\n```\nA production tokenizer needs to track character positions (for error messages), not just byte positions. The `column` counter should increment per Unicode code point, not per byte.\n### 4. SQL Injection Prevention\nUnderstanding token boundaries reveals why sanitization is hard. Consider:\n```sql\nSELECT * FROM users WHERE name = 'Robert'); DROP TABLE users; --'\n```\nThe tokenizer sees this as:\n```\n[SELECT] [STAR] [FROM] [users] [WHERE] [name] [=] \n['Robert'); DROP TABLE users; --']\n```\nThe \"injection\" is just a string literal! The tokenizer correctly identifies the boundaries. The vulnerability happens when you concatenate strings instead of using parameterized queries \u2014 bypassing the tokenizer entirely for the attacker's input.\n### 5. IDE and Tooling Features\nPosition tracking enables:\n- **Syntax highlighting**: color tokens by type\n- **Go to definition**: resolve identifier tokens to declarations\n- **Error squiggles**: underline error tokens in red\n- **Auto-complete**: suggest keywords at the current token position\nThe tokenizer is the foundation for all language tooling.\n---\n## What's Next\nWith tokenization complete, you're ready for the **parser** \u2014 the next milestone that transforms your token stream into an Abstract Syntax Tree. The parser will consume the tokens you produce, building a tree structure that represents the SQL statement's meaning.\nYour tokenizer is the eyes of your database. The parser will be its understanding.\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m1\", \"criteria\": [\"Tokenizer implements a finite state machine (FSM) processing input character-by-character with state transitions\", \"Token data structure includes type, value (string), line number (1-based), and column number (1-based)\", \"All SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, FROM, AND, OR, NOT, NULL, etc.) are recognized case-insensitively and normalized to uppercase\", \"String literals enclosed in single quotes correctly handle escaped quotes ('' \u2192 ') and preserve internal content including whitespace\", \"Numeric literals are classified as TOKEN_INTEGER for values without decimal points (42, 0, 12345) and TOKEN_FLOAT for values with decimal points or exponents (3.14, .5, 1e10, 1.5e-3)\", \"Operators are correctly tokenized including single-character (=, <, >, +, -, *, /) and two-character forms (<=, >=, <>, !=, ||)\", \"Punctuation characters (comma, parentheses, semicolon, period) are tokenized as distinct TOKEN_PUNCTUATION tokens\", \"Quoted identifiers using double quotes support spaces and special characters, with doubled quotes as escapes (\\\"col\\\"\\\"name\\\" \u2192 col\\\"name)\", \"Single-line comments (--) and multi-line comments (/* */) are skipped and do not produce tokens\", \"Error tokens include precise line and column position of the error, with meaningful error messages\", \"Unterminated string literals report the position of the opening quote, not the end of input\", \"Test suite includes at least 20 diverse SQL statements covering all token types and edge cases\", \"Test suite validates correct token sequences for complex multi-clause queries like SELECT id, name FROM users WHERE age >= 18 AND status = 'active'\", \"Negative numbers are tokenized as [MINUS operator] [positive number], not as negative literals \u2014 unary negation is a parser concern\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m2 -->\n# SQL Parser (AST): From Token Stream to Meaning\nYour tokenizer hands you a sequence of tokens \u2014 `[SELECT] [STAR] [FROM] [users]`. But a token stream isn't understanding. It's just a list of words.\nConsider this: the token streams for `SELECT a OR b AND c` and `SELECT (a OR b) AND c` are almost identical. Same tokens, same types, same order. Yet these queries mean fundamentally different things. The first returns rows where `a` is true OR both `b` AND `c` are true. The second returns rows where both `a` OR `b` is true AND `c` is true.\nThe parser's job is to encode this meaning into structure \u2014 to transform a flat sequence into a tree that captures the relationships between tokens. This tree, the Abstract Syntax Tree (AST), is where your database begins to understand what the user wants.\n```\n\"a OR b AND c\"                    \"(a OR b) AND c\"\n       OR                               AND\n      /   \\                            /    \\\n     a    AND                         OR     c\n         /   \\                       /  \\\n        b     c                     a    b\n```\nSame tokens. Different trees. Different meanings. The parser doesn't just validate syntax \u2014 it **discovers structure**.\n---\n## The Tension: Ambiguity in Flat Sequences\nHere's the core problem: a token stream is inherently ambiguous. Consider:\n```sql\nSELECT * FROM users WHERE id = 5\n```\nYour tokenizer produces:\n```\n[SELECT] [STAR] [FROM] [IDENTIFIER:users] [WHERE] [IDENTIFIER:id] [=] [INTEGER:5]\n```\nBut what's the structure? How do you know that `*` belongs to `SELECT` and not to `FROM`? How do you know `id = 5` is the WHERE condition and not part of the FROM clause?\nThe answer: **grammar rules**. SQL has a defined grammar that specifies how tokens combine into meaningful structures. Your parser encodes these rules.\nBut here's where it gets tricky. Consider expressions:\n```sql\nSELECT * FROM t WHERE a = 1 OR b = 2 AND c = 3\n```\nIn SQL, `AND` binds tighter than `OR`. This means:\n- `a = 1 OR (b = 2 AND c = 3)` \u2014 correct interpretation\n- `(a = 1 OR b = 2) AND c = 3` \u2014 wrong interpretation\nThe parser must respect this precedence. If it doesn't, your database returns wrong results.\nAnd then there's this:\n```sql\nSELECT (a = 1 OR b = 2) AND c = 3\n```\nParentheses override precedence. The parser must handle this too.\nThe tension: **you need to recover hierarchical structure from a flat sequence, respecting complex binding rules, while producing helpful errors when the input doesn't match the grammar**.\n---\n## The Revelation: Trees Encode Meaning\nHere's a misconception that will break your parser: *parsing is just checking if syntax is valid \u2014 the AST is basically the same as the token sequence.*\nNo. The AST is **not** a linearization of tokens. It's a **structural transformation** that encodes semantics through tree shape.\n\n![Operator Precedence: Before and After](./diagrams/diag-precedence-climbing.svg)\n\nConsider this expression:\n```sql\nNOT a = 1 AND b = 2 OR c = 3\n```\nThe precedence rules (NOT > comparison > AND > OR) tell us this should be parsed as:\n```\n((NOT (a = 1)) AND (b = 2)) OR (c = 3)\n```\nThe AST reflects this structure:\n```\n           OR\n          /   \\\n        AND    c=3\n       /   \\\n    NOT    b=2\n     |\n    a=1\n```\nNotice: the tokens appear in a completely different arrangement in the tree than in the source. `OR` is at the top (evaluated last), even though it appears at the end of the expression. `NOT` is near the bottom (evaluated first), even though it appears first.\nThis inversion is the key insight: **operator precedence determines tree depth**. Tighter-binding operators appear deeper in the tree. Looser-binding operators appear closer to the root.\nWhy? Because you evaluate a tree bottom-up. The deeper sub-expressions evaluate first. If `AND` binds tighter than `OR`, then `AND` expressions must be deeper in the tree so they evaluate before the `OR` combines their results.\nThe AST doesn't just capture \"what tokens appeared\" \u2014 it captures \"how they combine.\"\n---\n## Recursive Descent: Parsing by Grammar Rules\nThe most intuitive parsing technique for SQL is **recursive descent**: a set of mutually recursive functions, one for each rule in the grammar.\nHere's the core idea: each grammar rule becomes a function. If a rule references another rule, the function calls that rule's function. The recursion naturally handles nested structures.\n### A Simple Grammar\nLet's define a simplified SQL grammar:\n```\nstatement    ::= select_stmt | insert_stmt | create_stmt\nselect_stmt  ::= \"SELECT\" column_list \"FROM\" identifier [where_clause]\ncolumn_list  ::= \"*\" | identifier (\",\" identifier)*\nwhere_clause ::= \"WHERE\" expression\nexpression   ::= identifier \"=\" literal\n```\nThis grammar says:\n- A `statement` is either a SELECT, INSERT, or CREATE\n- A `select_stmt` starts with SELECT, then columns, then FROM, then a table name, optionally a WHERE clause\n- A `column_list` is either `*` or a comma-separated list of identifiers\n- An `expression` is an identifier, equals, and a literal\nEach rule becomes a function:\n```c\nASTNode* parse_statement(Parser* parser) {\n    Token t = peek_token(parser);\n    if (token_is(t, TOKEN_KEYWORD, \"SELECT\")) {\n        return parse_select_stmt(parser);\n    } else if (token_is(t, TOKEN_KEYWORD, \"INSERT\")) {\n        return parse_insert_stmt(parser);\n    } else if (token_is(t, TOKEN_KEYWORD, \"CREATE\")) {\n        return parse_create_stmt(parser);\n    } else {\n        return error_node(parser, \"expected SELECT, INSERT, or CREATE\");\n    }\n}\nASTNode* parse_select_stmt(Parser* parser) {\n    expect_keyword(parser, \"SELECT\");\n    ASTNode* columns = parse_column_list(parser);\n    expect_keyword(parser, \"FROM\");\n    Token table = expect_identifier(parser);\n    ASTNode* where = NULL;\n    if (check_keyword(parser, \"WHERE\")) {\n        where = parse_where_clause(parser);\n    }\n    return make_select_node(columns, table.value, where);\n}\n```\nThe `expect_*` functions consume tokens and error if the token doesn't match. The `check_*` functions look ahead without consuming.\n\n![AST Node Hierarchy](./diagrams/diag-ast-structure.svg)\n\n### The AST Node Structure\nEvery AST node needs:\n- A type field identifying what kind of node it is\n- Type-specific data (column names, table names, child nodes)\n- Position information for error reporting\n```c\ntypedef enum {\n    AST_SELECT_STMT,\n    AST_INSERT_STMT,\n    AST_CREATE_STMT,\n    AST_COLUMN_LIST,\n    AST_WHERE_CLAUSE,\n    AST_BINARY_EXPR,\n    AST_UNARY_EXPR,\n    AST_LITERAL_EXPR,\n    AST_IDENTIFIER_EXPR,\n    AST_COLUMN_DEF,\n    // ...\n} ASTNodeType;\nstruct ASTNode {\n    ASTNodeType type;\n    int line;\n    int column;\n    union {\n        struct {\n            ASTNode* columns;\n            char* table_name;\n            ASTNode* where;\n            ASTNode* order_by;\n            ASTNode* limit;\n        } select_stmt;\n        struct {\n            char* table_name;\n            ASTNode* columns;  // NULL means all columns\n            ASTNode* values;\n        } insert_stmt;\n        struct {\n            char* table_name;\n            ASTNode* column_defs;\n        } create_stmt;\n        struct {\n            ASTNode* left;\n            char* operator;  // \"=\", \"<\", \"AND\", \"OR\", etc.\n            ASTNode* right;\n        } binary_expr;\n        struct {\n            char* operator;  // \"NOT\"\n            ASTNode* operand;\n        } unary_expr;\n        struct {\n            Token token;  // holds the literal value\n        } literal_expr;\n        struct {\n            char* name;\n        } identifier_expr;\n        // ... other node types\n    } data;\n};\n```\nThe union saves memory \u2014 each node only stores the data relevant to its type. The position fields (`line`, `column`) come from the token where the node started, enabling precise error messages later.\n---\n## Parsing SELECT Statements\nLet's build the SELECT parser incrementally, handling each clause.\n### Column List: Handling * and Multiple Columns\n```sql\nSELECT * FROM t\nSELECT a FROM t\nSELECT a, b, c FROM t\n```\nThe column list has two forms: `*` (all columns) or a comma-separated list of identifiers:\n```c\nASTNode* parse_column_list(Parser* parser) {\n    ASTNode* node = make_node(AST_COLUMN_LIST);\n    if (check_punctuation(parser, \"*\")) {\n        consume_token(parser);\n        node->data.column_list.star = true;\n        node->data.column_list.columns = NULL;\n        return node;\n    }\n    node->data.column_list.star = false;\n    node->data.column_list.columns = create_list();\n    // First column\n    Token col = expect_identifier(parser);\n    list_add(node->data.column_list.columns, strdup(col.value));\n    // Additional columns\n    while (check_punctuation(parser, \",\")) {\n        consume_token(parser);\n        col = expect_identifier(parser);\n        list_add(node->data.column_list.columns, strdup(col.value));\n    }\n    return node;\n}\n```\nThe `expect_identifier` function consumes a token and errors if it's not an identifier:\n```c\nToken expect_identifier(Parser* parser) {\n    Token t = peek_token(parser);\n    if (t.type != TOKEN_IDENTIFIER) {\n        error(parser, \"expected identifier, got %s at line %d, column %d\",\n              token_type_name(t.type), t.line, t.column);\n        return error_token();\n    }\n    return consume_token(parser);\n}\n```\n### FROM Clause: Table Reference\n```sql\nSELECT * FROM users\nSELECT * FROM my_schema.users  -- qualified name\n```\nFor now, handle simple table names. Qualified names (`schema.table`) require additional parsing:\n```c\nToken parse_table_name(Parser* parser) {\n    return expect_identifier(parser);\n}\n```\n### WHERE Clause: The Expression Problem\nHere's where things get interesting. The WHERE clause contains an expression:\n```sql\nWHERE id = 5\nWHERE id = 5 AND status = 'active'\nWHERE (id = 5 OR id = 10) AND status = 'active'\nWHERE NOT deleted\n```\nExpressions involve:\n- **Literals**: numbers, strings, NULL\n- **Identifiers**: column references\n- **Operators**: =, <, >, AND, OR, NOT\n- **Parentheses**: override precedence\nThe challenge: expressions can be arbitrarily nested. A `AND` can contain `OR`s which contain `NOT`s which contain comparisons. The parser must handle all this while respecting precedence.\nThis is the classic expression parsing problem, and it has elegant solutions.\n---\n## Expression Parsing: Precedence Climbing\nThe key insight for expression parsing: **precedence determines when to stop recursing**.\nHere's the algorithm, known as **precedence climbing**:\n1. Parse the first operand (the \"left\" side)\n2. Look at the next operator\n3. If its precedence is too low, stop \u2014 return what you have\n4. If its precedence is high enough, consume the operator\n5. Recursively parse the \"right\" side (operators with higher precedence)\n6. Build a binary expression node\n7. Repeat, treating the binary expression as the new \"left\" side\n### Precedence Table\nFirst, define your precedence levels (higher number = tighter binding):\n```c\ntypedef enum {\n    PREC_OR,          // 0 \u2014 lowest\n    PREC_AND,         // 1\n    PREC_NOT,         // 2 (unary)\n    PREC_COMPARISON,  // 3: =, <, >, <=, >=, <>, !=\n    PREC_ADDITIVE,    // 4: +, -\n    PREC_MULTIPLICATIVE, // 5: *, /, %\n    PREC_UNARY,       // 6: unary -, + (not implementing yet)\n    PREC_PRIMARY      // 7 \u2014 highest (literals, identifiers, parens)\n} Precedence;\nPrecedence get_precedence(Token token) {\n    if (token.type == TOKEN_KEYWORD) {\n        if (strcmp(token.value, \"OR\") == 0) return PREC_OR;\n        if (strcmp(token.value, \"AND\") == 0) return PREC_AND;\n        if (strcmp(token.value, \"NOT\") == 0) return PREC_NOT;\n    }\n    if (token.type == TOKEN_OPERATOR) {\n        if (strcmp(token.value, \"=\") == 0 ||\n            strcmp(token.value, \"<\") == 0 ||\n            strcmp(token.value, \">\") == 0 ||\n            strcmp(token.value, \"<=\") == 0 ||\n            strcmp(token.value, \">=\") == 0 ||\n            strcmp(token.value, \"<>\") == 0 ||\n            strcmp(token.value, \"!=\") == 0) {\n            return PREC_COMPARISON;\n        }\n        if (strcmp(token.value, \"+\") == 0 ||\n            strcmp(token.value, \"-\") == 0) {\n            return PREC_ADDITIVE;\n        }\n        if (strcmp(token.value, \"*\") == 0 ||\n            strcmp(token.value, \"/\") == 0 ||\n            strcmp(token.value, \"%\") == 0) {\n            return PREC_MULTIPLICATIVE;\n        }\n    }\n    return PREC_PRIMARY;\n}\n```\n### The Precedence Climbing Parser\n```c\nASTNode* parse_expression(Parser* parser) {\n    return parse_expression_with_precedence(parser, PREC_OR);\n}\nASTNode* parse_expression_with_precedence(Parser* parser, Precedence min_prec) {\n    ASTNode* left = parse_unary(parser);\n    while (true) {\n        Token op = peek_token(parser);\n        Precedence prec = get_precedence(op);\n        // Stop if operator has lower precedence than we're looking for\n        if (prec < min_prec) {\n            break;\n        }\n        // Handle left-associativity: >= for left-assoc, > for right-assoc\n        // All our operators are left-associative\n        consume_token(parser);\n        // Parse right side with higher precedence\n        ASTNode* right = parse_expression_with_precedence(parser, prec + 1);\n        // Build binary expression node\n        left = make_binary_expr(op.value, left, right);\n    }\n    return left;\n}\nASTNode* parse_unary(Parser* parser) {\n    Token t = peek_token(parser);\n    // Handle NOT (unary operator)\n    if (token_is(t, TOKEN_KEYWORD, \"NOT\")) {\n        consume_token(parser);\n        ASTNode* operand = parse_unary(parser);  // NOT is right-associative\n        return make_unary_expr(\"NOT\", operand);\n    }\n    // Handle unary minus (for things like SELECT -5)\n    if (t.type == TOKEN_OPERATOR && strcmp(t.value, \"-\") == 0) {\n        consume_token(parser);\n        ASTNode* operand = parse_unary(parser);\n        return make_unary_expr(\"-\", operand);\n    }\n    return parse_primary(parser);\n}\nASTNode* parse_primary(Parser* parser) {\n    Token t = peek_token(parser);\n    // Parenthesized expression\n    if (check_punctuation(parser, \"(\")) {\n        consume_token(parser);\n        ASTNode* expr = parse_expression(parser);\n        expect_punctuation(parser, \")\");\n        return expr;\n    }\n    // Literals\n    if (t.type == TOKEN_INTEGER || t.type == TOKEN_FLOAT || \n        t.type == TOKEN_STRING) {\n        consume_token(parser);\n        return make_literal_expr(t);\n    }\n    // NULL literal\n    if (token_is(t, TOKEN_KEYWORD, \"NULL\")) {\n        consume_token(parser);\n        return make_literal_expr(t);\n    }\n    // TRUE/FALSE literals\n    if (token_is(t, TOKEN_KEYWORD, \"TRUE\") || \n        token_is(t, TOKEN_KEYWORD, \"FALSE\")) {\n        consume_token(parser);\n        return make_literal_expr(t);\n    }\n    // Identifier (column reference)\n    if (t.type == TOKEN_IDENTIFIER) {\n        consume_token(parser);\n        return make_identifier_expr(t.value);\n    }\n    // Error\n    error(parser, \"expected expression at line %d, column %d\", t.line, t.column);\n    return make_error_node(parser);\n}\n```\n### Tracing Through an Example\nLet's trace `a OR b AND c`:\n```\nparse_expression(PREC_OR)\n  parse_unary()\n    parse_primary() \u2192 identifier \"a\"\n  left = identifier \"a\"\n  peek: OR, prec = 0, min_prec = 0, 0 >= 0, continue\n  consume OR\n  parse_expression_with_precedence(PREC_OR + 1 = PREC_AND)\n    parse_unary()\n      parse_primary() \u2192 identifier \"b\"\n    left = identifier \"b\"\n    peek: AND, prec = 1, min_prec = 1, 1 >= 1, continue\n    consume AND\n    parse_expression_with_precedence(PREC_AND + 1 = PREC_COMPARISON)\n      parse_unary()\n        parse_primary() \u2192 identifier \"c\"\n      left = identifier \"c\"\n      peek: EOF (or something), prec = PREC_PRIMARY, 7 < 3, break\n      return identifier \"c\"\n    right = identifier \"c\"\n    left = binary(AND, \"b\", \"c\")\n    peek: EOF, prec = PREC_PRIMARY, break\n    return binary(AND, \"b\", \"c\")\n  right = binary(AND, \"b\", \"c\")\n  left = binary(OR, \"a\", binary(AND, \"b\", \"c\"))\n  peek: EOF, break\n  return binary(OR, \"a\", binary(AND, \"b\", \"c\"))\n```\nThe result: `OR(a, AND(b, c))` \u2014 exactly the correct tree structure.\n### Why Parentheses Work\nConsider `(a OR b) AND c`:\nWhen we hit the `(`, `parse_primary` recursively calls `parse_expression`:\n```c\nif (check_punctuation(parser, \"(\")) {\n    consume_token(parser);\n    ASTNode* expr = parse_expression(parser);  // Fresh parse with min_prec = 0\n    expect_punctuation(parser, \")\");\n    return expr;\n}\n```\nThis fresh parse with `min_prec = 0` treats `a OR b` as a complete expression, returning `OR(a, b)`. Then the outer parse continues with `AND c`, producing `AND(OR(a, b), c)`.\nParentheses work because they **restart** precedence parsing at the lowest level.\n---\n## Parsing INSERT Statements\nINSERT syntax:\n```sql\nINSERT INTO table_name VALUES (1, 'Alice')\nINSERT INTO table_name (id, name) VALUES (1, 'Alice')\n```\n```c\nASTNode* parse_insert_stmt(Parser* parser) {\n    ASTNode* node = make_node(AST_INSERT_STMT);\n    expect_keyword(parser, \"INSERT\");\n    expect_keyword(parser, \"INTO\");\n    // Table name\n    Token table = expect_identifier(parser);\n    node->data.insert_stmt.table_name = strdup(table.value);\n    // Optional column list\n    if (check_punctuation(parser, \"(\")) {\n        consume_token(parser);\n        node->data.insert_stmt.columns = parse_column_list(parser);\n        expect_punctuation(parser, \")\");\n    } else {\n        node->data.insert_stmt.columns = NULL;\n    }\n    // VALUES clause\n    expect_keyword(parser, \"VALUES\");\n    node->data.insert_stmt.values = parse_value_list(parser);\n    return node;\n}\nASTNode* parse_value_list(Parser* parser) {\n    ASTNode* node = make_node(AST_VALUE_LIST);\n    node->data.value_list.values = create_list();\n    expect_punctuation(parser, \"(\");\n    // First value\n    ASTNode* value = parse_expression(parser);\n    list_add(node->data.value_list.values, value);\n    // Additional values\n    while (check_punctuation(parser, \",\")) {\n        consume_token(parser);\n        value = parse_expression(parser);\n        list_add(node->data.value_list.values, value);\n    }\n    expect_punctuation(parser, \")\");\n    return node;\n}\n```\nNote that we use `parse_expression` for each value. This allows expressions in VALUES:\n```sql\nINSERT INTO t VALUES (1 + 1, UPPER('hello'))\n```\n---\n## Parsing CREATE TABLE Statements\nCREATE TABLE syntax:\n```sql\nCREATE TABLE users (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE\n)\n```\n```c\nASTNode* parse_create_stmt(Parser* parser) {\n    ASTNode* node = make_node(AST_CREATE_STMT);\n    expect_keyword(parser, \"CREATE\");\n    expect_keyword(parser, \"TABLE\");\n    // Table name\n    Token table = expect_identifier(parser);\n    node->data.create_stmt.table_name = strdup(table.value);\n    // Column definitions\n    expect_punctuation(parser, \"(\");\n    node->data.create_stmt.column_defs = parse_column_def_list(parser);\n    expect_punctuation(parser, \")\");\n    return node;\n}\nASTNode* parse_column_def_list(Parser* parser) {\n    ASTNode* node = make_node(AST_COLUMN_DEF_LIST);\n    node->data.column_def_list.defs = create_list();\n    // First column definition\n    list_add(node->data.column_def_list.defs, parse_column_def(parser));\n    // Additional column definitions\n    while (check_punctuation(parser, \",\")) {\n        consume_token(parser);\n        list_add(node->data.column_def_list.defs, parse_column_def(parser));\n    }\n    return node;\n}\nASTNode* parse_column_def(Parser* parser) {\n    ASTNode* node = make_node(AST_COLUMN_DEF);\n    // Column name\n    Token name = expect_identifier(parser);\n    node->data.column_def.name = strdup(name.value);\n    // Data type\n    Token type = expect_keyword_in(parser, \n        (const char*[]){\"INTEGER\", \"TEXT\", \"REAL\", \"BLOB\", NULL});\n    node->data.column_def.type = strdup(type.value);\n    // Constraints (optional, can be multiple)\n    node->data.column_def.constraints = create_list();\n    while (true) {\n        Token t = peek_token(parser);\n        if (token_is(t, TOKEN_KEYWORD, \"PRIMARY\")) {\n            consume_token(parser);\n            expect_keyword(parser, \"KEY\");\n            list_add(node->data.column_def.constraints, \n                     make_constraint(CONSTRAINT_PRIMARY_KEY));\n        }\n        else if (token_is(t, TOKEN_KEYWORD, \"NOT\")) {\n            consume_token(parser);\n            expect_keyword(parser, \"NULL\");\n            list_add(node->data.column_def.constraints, \n                     make_constraint(CONSTRAINT_NOT_NULL));\n        }\n        else if (token_is(t, TOKEN_KEYWORD, \"UNIQUE\")) {\n            consume_token(parser);\n            list_add(node->data.column_def.constraints, \n                     make_constraint(CONSTRAINT_UNIQUE));\n        }\n        else {\n            break;  // No more constraints\n        }\n    }\n    return node;\n}\nToken expect_keyword_in(Parser* parser, const char** keywords) {\n    Token t = peek_token(parser);\n    if (t.type != TOKEN_KEYWORD) {\n        error(parser, \"expected keyword at line %d, column %d\", t.line, t.column);\n        return error_token();\n    }\n    for (int i = 0; keywords[i] != NULL; i++) {\n        if (strcmp(t.value, keywords[i]) == 0) {\n            return consume_token(parser);\n        }\n    }\n    error(parser, \"expected one of: INTEGER, TEXT, REAL, BLOB at line %d, column %d\",\n          t.line, t.column);\n    return error_token();\n}\n```\n### Constraint Representation\n```c\ntypedef enum {\n    CONSTRAINT_PRIMARY_KEY,\n    CONSTRAINT_NOT_NULL,\n    CONSTRAINT_UNIQUE,\n    CONSTRAINT_DEFAULT,\n    CONSTRAINT_CHECK,\n    CONSTRAINT_FOREIGN_KEY\n} ConstraintType;\nstruct Constraint {\n    ConstraintType type;\n    union {\n        struct {\n            ASTNode* expression;  // for DEFAULT, CHECK\n        } expr_constraint;\n        struct {\n            char* ref_table;\n            char* ref_column;\n        } foreign_key;\n    } data;\n};\n```\n---\n## Error Handling: Position and Recovery\nWhen parsing fails, the error message must point to the exact location:\n```c\nvoid error(Parser* parser, const char* format, ...) {\n    if (parser->has_error) return;  // Report only first error\n    parser->has_error = true;\n    Token t = peek_token(parser);\n    parser->error_line = t.line;\n    parser->error_column = t.column;\n    va_list args;\n    va_start(args, format);\n    vsnprintf(parser->error_message, sizeof(parser->error_message), format, args);\n    va_end(args);\n}\n```\n### Error Recovery Strategies\nFor a database, **fail fast** is usually best \u2014 report the first error and stop. Unlike an IDE's parser that tries to continue for more diagnostics, a database parser needs to reject invalid queries entirely.\nHowever, some recovery helps for better error messages:\n```c\n// After an error, skip to a synchronization point\nvoid synchronize(Parser* parser) {\n    while (!is_at_end(parser)) {\n        if (check_punctuation(parser, \";\")) {\n            return;  // Statement boundary\n        }\n        Token t = peek_token(parser);\n        if (t.type == TOKEN_KEYWORD) {\n            if (strcmp(t.value, \"SELECT\") == 0 ||\n                strcmp(t.value, \"INSERT\") == 0 ||\n                strcmp(t.value, \"CREATE\") == 0 ||\n                strcmp(t.value, \"UPDATE\") == 0 ||\n                strcmp(t.value, \"DELETE\") == 0 ||\n                strcmp(t.value, \"DROP\") == 0) {\n                return;  // Statement start\n            }\n        }\n        consume_token(parser);\n    }\n}\n```\nThis allows parsing multiple statements in a batch, where an error in one doesn't prevent parsing the next.\n---\n## The Parser Structure\nHere's the complete parser state:\n```c\ntypedef struct {\n    TokenStream* tokens;      // From the tokenizer\n    size_t current;           // Current position in token stream\n    bool has_error;\n    int error_line;\n    int error_column;\n    char error_message[256];\n} Parser;\nToken peek_token(Parser* parser) {\n    return parser->tokens->tokens[parser->current];\n}\nToken previous_token(Parser* parser) {\n    return parser->tokens->tokens[parser->current - 1];\n}\nToken consume_token(Parser* parser) {\n    return parser->tokens->tokens[parser->current++];\n}\nbool is_at_end(Parser* parser) {\n    return peek_token(parser).type == TOKEN_EOF;\n}\nbool check_keyword(Parser* parser, const char* keyword) {\n    Token t = peek_token(parser);\n    return t.type == TOKEN_KEYWORD && strcmp(t.value, keyword) == 0;\n}\nbool check_punctuation(Parser* parser, const char* punct) {\n    Token t = peek_token(parser);\n    return t.type == TOKEN_PUNCTUATION && strcmp(t.value, punct) == 0;\n}\n```\n---\n## The Three-Level View\n### Level 1: Application (SQL Text \u2192 AST)\nAt the API level, the parser is a single function:\n```c\nASTNode* parse(const char* sql);\n```\nInput: a SQL string. Output: an AST representing the statement. The caller (compiler, executor) doesn't care about tokens or grammar rules \u2014 they want the tree.\n### Level 2: Engine (Recursive Descent + Precedence Climbing)\nInside the parser, recursive functions implement grammar rules:\n```c\nASTNode* parse_statement(Parser* p) {\n    if (check_keyword(p, \"SELECT\")) return parse_select(p);\n    if (check_keyword(p, \"INSERT\")) return parse_insert(p);\n    if (check_keyword(p, \"CREATE\")) return parse_create(p);\n    // ...\n}\n```\nExpression parsing uses precedence climbing to handle operator binding:\n```c\nASTNode* parse_expression(Parser* p) {\n    return parse_expr_with_precedence(p, PREC_OR);\n}\n```\n### Level 3: Implementation (Token Consumption and Tree Construction)\nAt the lowest level, the parser manipulates tokens and builds nodes:\n```c\nToken t = consume_token(parser);\nASTNode* node = allocate_node(AST_LITERAL_EXPR);\nnode->data.literal_expr.token = t;\nreturn node;\n```\nMemory management is critical: each AST node is heap-allocated. The caller must free the tree after use. A `free_ast(ASTNode* node)` function recursively frees children.\n---\n## Testing Strategy\n### Valid Statement Tests\n```c\nvoid test_select_star() {\n    const char* sql = \"SELECT * FROM users\";\n    ASTNode* ast = parse(sql);\n    assert(ast->type == AST_SELECT_STMT);\n    assert(ast->data.select_stmt.columns->data.column_list.star);\n    assert(strcmp(ast->data.select_stmt.table_name, \"users\") == 0);\n    assert(ast->data.select_stmt.where == NULL);\n    free_ast(ast);\n}\nvoid test_select_with_where() {\n    const char* sql = \"SELECT id, name FROM users WHERE status = 'active'\";\n    ASTNode* ast = parse(sql);\n    assert(ast->type == AST_SELECT_STMT);\n    assert(!ast->data.select_stmt.columns->data.column_list.star);\n    assert(strcmp(ast->data.select_stmt.columns->data.column_list.columns->items[0], \"id\") == 0);\n    assert(strcmp(ast->data.select_stmt.columns->data.column_list.columns->items[1], \"name\") == 0);\n    assert(ast->data.select_stmt.where != NULL);\n    assert(ast->data.select_stmt.where->type == AST_WHERE_CLAUSE);\n    free_ast(ast);\n}\nvoid test_expression_precedence() {\n    const char* sql = \"SELECT * FROM t WHERE a OR b AND c\";\n    ASTNode* ast = parse(sql);\n    ASTNode* where_expr = ast->data.select_stmt.where->data.where_clause.expression;\n    // Root should be OR\n    assert(where_expr->type == AST_BINARY_EXPR);\n    assert(strcmp(where_expr->data.binary_expr.operator, \"OR\") == 0);\n    // Left side of OR should be identifier 'a'\n    assert(where_expr->data.binary_expr.left->type == AST_IDENTIFIER_EXPR);\n    // Right side of OR should be AND\n    ASTNode* and_node = where_expr->data.binary_expr.right;\n    assert(and_node->type == AST_BINARY_EXPR);\n    assert(strcmp(and_node->data.binary_expr.operator, \"AND\") == 0);\n    free_ast(ast);\n}\nvoid test_parentheses_override() {\n    const char* sql = \"SELECT * FROM t WHERE (a OR b) AND c\";\n    ASTNode* ast = parse(sql);\n    ASTNode* where_expr = ast->data.select_stmt.where->data.where_clause.expression;\n    // Root should be AND\n    assert(where_expr->type == AST_BINARY_EXPR);\n    assert(strcmp(where_expr->data.binary_expr.operator, \"AND\") == 0);\n    // Left side of AND should be OR\n    ASTNode* or_node = where_expr->data.binary_expr.left;\n    assert(or_node->type == AST_BINARY_EXPR);\n    assert(strcmp(or_node->data.binary_expr.operator, \"OR\") == 0);\n    free_ast(ast);\n}\n```\n### Invalid Statement Tests\n```c\nvoid test_missing_from() {\n    const char* sql = \"SELECT *\";\n    ASTNode* ast = parse(sql);\n    assert(ast == NULL || parser_has_error());\n    assert(strstr(parser_error_message(), \"FROM\") != NULL);\n}\nvoid test_unexpected_token() {\n    const char* sql = \"SELECT * FROM WHERE users\";\n    ASTNode* ast = parse(sql);\n    assert(ast == NULL || parser_has_error());\n}\nvoid test_mismatched_parentheses() {\n    const char* sql = \"SELECT * FROM t WHERE (a = 1\";\n    ASTNode* ast = parse(sql);\n    assert(ast == NULL || parser_has_error());\n    assert(strstr(parser_error_message(), \")\") != NULL);\n}\nvoid test_invalid_column_type() {\n    const char* sql = \"CREATE TABLE t (id NUMBER)\";\n    ASTNode* ast = parse(sql);\n    assert(ast == NULL || parser_has_error());\n    assert(strstr(parser_error_message(), \"INTEGER\") != NULL ||\n           strstr(parser_error_message(), \"TEXT\") != NULL);\n}\n```\n### Edge Cases\n```c\nvoid test_null_literal() {\n    const char* sql = \"SELECT * FROM t WHERE col IS NULL\";\n    ASTNode* ast = parse(sql);\n    // NULL should be a literal, not an identifier\n    assert(ast != NULL);\n    free_ast(ast);\n}\nvoid test_empty_values() {\n    const char* sql = \"INSERT INTO t VALUES ()\";\n    ASTNode* ast = parse(sql);\n    // Empty values list is valid (for tables with no columns or all defaults)\n    assert(ast != NULL);\n    free_ast(ast);\n}\nvoid test_multiple_constraints() {\n    const char* sql = \"CREATE TABLE t (id INTEGER PRIMARY KEY NOT NULL)\";\n    ASTNode* ast = parse(sql);\n    assert(ast != NULL);\n    ASTNode* col_def = ast->data.create_stmt.column_defs->data.column_def_list.defs->items[0];\n    assert(list_size(col_def->data.column_def.constraints) == 2);\n    free_ast(ast);\n}\n```\n---\n## Common Pitfalls\n### Left Recursion\nIf you define your grammar as:\n```\nexpression ::= expression \"+\" term | term\n```\nThe `parse_expression` function would call itself infinitely:\n```c\nASTNode* parse_expression() {\n    ASTNode* left = parse_expression();  // Infinite recursion!\n    // ...\n}\n```\nThis is **left recursion**. Recursive descent can't handle it directly.\n**Solution**: rewrite the grammar to be left-factored:\n```\nexpression ::= term (\"+\" term)*\n```\nOr use precedence climbing, which avoids the issue entirely by iterating instead of recursing on the left.\n### AND vs OR Precedence\nSQL's precedence: `AND` binds tighter than `OR`. This is **opposite** of some programming languages where `||` and `&&` have different relationships.\n```c\n// WRONG\nPrecedence get_precedence(Token t) {\n    if (is_and(t)) return PREC_OR;   // Swapped!\n    if (is_or(t)) return PREC_AND;   // Swapped!\n}\n// RIGHT\nPrecedence get_precedence(Token t) {\n    if (is_and(t)) return PREC_AND;  // Higher precedence\n    if (is_or(t)) return PREC_OR;    // Lower precedence\n}\n```\n### NULL is a Keyword\nIn expressions, `NULL` is a keyword (literal), not an identifier:\n```c\n// WRONG\nif (t.type == TOKEN_IDENTIFIER) {\n    return make_identifier_expr(t.value);\n}\n// RIGHT\nif (t.type == TOKEN_IDENTIFIER) {\n    return make_identifier_expr(t.value);\n}\nif (token_is(t, TOKEN_KEYWORD, \"NULL\")) {\n    return make_literal_expr(t);  // NULL is a literal\n}\n```\n### Column vs Expression Confusion\nIn `SELECT a, b FROM t`, `a` and `b` look like identifiers. But in `SELECT a + b FROM t`, `a + b` is an expression. Your column list parser should accept expressions:\n```c\nASTNode* parse_select_column(Parser* parser) {\n    return parse_expression(parser);  // Handles identifiers and expressions\n}\n```\n---\n## What You've Built\nYour parser is a recursive descent engine that:\n1. **Consumes token streams** produced by the tokenizer\n2. **Builds AST nodes** representing SQL statement structure\n3. **Handles SELECT, INSERT, CREATE TABLE** with all their clauses\n4. **Parses expressions with correct precedence** using precedence climbing\n5. **Supports parentheses** to override default binding\n6. **Reports errors with position** for helpful diagnostics\n7. **Distinguishes NULL from identifiers** correctly\nThe AST is the semantic representation of the query. The next milestone will compile this AST into bytecode for the virtual machine.\n---\n## Knowledge Cascade\nUnderstanding SQL parsing unlocks connections across computing:\n### 1. Query Optimization\nThe AST is where optimization begins. The query planner can rewrite:\n```sql\nWHERE 1=1 AND col=5\n```\nTo just:\n```sql\nWHERE col=5\n```\nBy recognizing that `1=1` is always true and pruning it from the AST. Constant folding, predicate pushdown, and join reordering all operate on the AST.\n### 2. Language Servers (LSP)\nThe same AST that your database uses powers IDE features:\n- **Go to definition**: resolve identifier nodes to schema objects\n- **Auto-complete**: suggest continuations at the current AST position\n- **Refactoring**: rename all references to a column by traversing the AST\n- **Diagnostics**: type checking walks the AST and validates expressions\nThe parser you build is the foundation for any SQL tooling.\n### 3. SQL Dialect Translation\nThe AST is dialect-agnostic. One AST can generate:\n```sql\n-- PostgreSQL\nSELECT * FROM t LIMIT 10 OFFSET 5\n-- SQL Server\nSELECT * FROM t ORDER BY col OFFSET 5 ROWS FETCH NEXT 10 ROWS ONLY\n-- Oracle (older)\nSELECT * FROM (SELECT a.*, ROWNUM rn FROM t a WHERE ROWNUM <= 15) WHERE rn > 5\n```\nThe AST captures the **intent** (pagination). Code generation produces the **syntax**.\n### 4. Prepared Statements\nWhy are prepared statements faster? Because parsing happens once:\n```\nFirst execution:\n  \"SELECT * FROM users WHERE id = ?\" \u2192 tokenize \u2192 parse \u2192 AST \u2192 compile \u2192 bytecode\nSubsequent executions:\n  Different ? values \u2192 skip tokenize/parse/compile \u2192 execute bytecode directly\n```\nThe AST enables this optimization. Without it, every execution would re-parse.\n### 5. Expression Evaluation Everywhere\nThe precedence climbing technique you learned applies to:\n- **Spreadsheet formulas**: `=IF(A1>0, A1*B1, 0)` uses the same expression grammar\n- **Configuration languages**: Terraform, Ansible, and others have expression sub-languages\n- **Template engines**: `{{ user.name | upper }}` involves parsing filter expressions\n- **Search queries**: Elasticsearch, Lucene query strings are expression languages\nOnce you can parse expressions with precedence, you can parse almost any expression language.\n---\n## What's Next\nWith parsing complete, you have an AST representing the SQL statement's meaning. The next milestone \u2014 **Bytecode Compiler (VDBE)** \u2014 transforms this tree into a linear sequence of instructions for a virtual machine.\nThe parser gave your database **understanding**. The compiler will give it **action**.\n\n![Compilation Pipeline: AST to Bytecode](./diagrams/diag-ast-to-bytecode.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m2\", \"criteria\": [\"Parser implements recursive descent with one function per grammar rule (parse_statement, parse_select_stmt, parse_insert_stmt, parse_create_stmt)\", \"AST node structure includes type field, position information (line/column), and a union for type-specific data\", \"SELECT parser produces AST with column_list node (supporting * wildcard and comma-separated identifiers), FROM clause with table name, and optional WHERE/ORDER BY/LIMIT clauses\", \"INSERT parser produces AST with target table name, optional column name list, and VALUES clause containing one or more expression tuples\", \"CREATE TABLE parser extracts table name and column definitions, where each column definition includes name, data type (INTEGER, TEXT, REAL, BLOB), and zero or more constraints (PRIMARY KEY, NOT NULL, UNIQUE)\", \"Expression parser uses precedence climbing algorithm to handle binary operators with correct precedence: OR (lowest) < AND < comparison (=,<,>,<=,>=,<>,!=) < additive (+,-) < multiplicative (*,/,%) < unary (NOT, -) < primary (highest)\", \"Parenthesized expressions trigger a fresh parse_expression call, correctly overriding operator precedence\", \"Binary expressions produce AST_BINARY_EXPR nodes with left operand, operator string, and right operand\", \"Unary expressions (NOT, unary minus) produce AST_UNARY_EXPR nodes with operator and operand\", \"NULL keyword is parsed as AST_LITERAL_EXPR, not AST_IDENTIFIER_EXPR\", \"TRUE and FALSE keywords are parsed as boolean AST_LITERAL_EXPR nodes\", \"String, integer, and float literals are parsed as AST_LITERAL_EXPR nodes containing the token value\", \"Column references are parsed as AST_IDENTIFIER_EXPR nodes containing the column name\", \"Error handling captures the first error with line/column position and a descriptive message\", \"Parser uses expect_* functions that consume tokens and report errors on mismatch\", \"Parser uses check_* functions for lookahead without consuming tokens\", \"Test suite validates at least 15 valid SQL statements covering SELECT, INSERT, and CREATE TABLE with various clause combinations\", \"Test suite validates at least 10 invalid SQL statements, confirming errors are reported with correct position information\", \"Expression precedence tests verify that 'a OR b AND c' produces OR at root with AND as right child\", \"Parentheses tests verify that '(a OR b) AND c' produces AND at root with OR as left child\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m3 -->\n# Bytecode Compiler (VDBE): Where Understanding Becomes Action\nYour parser hands you a beautiful AST \u2014 a tree representing the SQL statement's meaning. But a tree is just data. It doesn't *do* anything.\nConsider what happens when you execute `SELECT * FROM users WHERE age > 18`:\n1. Open the `users` table\n2. Position a cursor at the first row\n3. Read the `age` column from the current row\n4. Compare it to 18\n5. If the comparison is true, output the row\n6. Move to the next row\n7. Repeat steps 3-6 until no more rows\n8. Close the table and halt\nThis is a **program** \u2014 a sequence of instructions that executes in order, with loops and conditionals. Your AST encodes *what* the user wants. The bytecode encodes *how* to get it.\n\n![Compilation Pipeline: AST to Bytecode](./diagrams/diag-ast-to-bytecode.svg)\n\nHere's the fundamental insight that separates toy databases from production ones: **you don't execute the AST.** You compile it.\n---\n## The Tension: Tree-Walking is Slow\nLet's be honest. You *could* execute the AST directly. A tree-walking interpreter is simple: traverse the tree recursively, performing operations at each node. For `SELECT * FROM t WHERE a > 5`, you'd:\n1. Visit the SELECT node\n2. Visit the FROM clause, open the table\n3. For each row, visit the WHERE clause's expression tree\n4. If the expression evaluates to true, visit the column list and output\nThis works. It's also catastrophically slow for real workloads.\n### The Problem: Repeated Work\nConsider a table with 100,000 rows and a WHERE clause like `WHERE first_name = 'John' AND age > 25 AND status = 'active'`.\nThe expression tree looks like:\n```\n         AND\n        /   \\\n      AND    status='active'\n     /   \\\nfirst_name='John'  age>25\n```\nFor *each* of the 100,000 rows, your tree-walking interpreter must:\n- Traverse the entire expression tree (pointer chasing, cache misses)\n- Look up `first_name`, compare to `'John'`\n- Look up `age`, compare to `25`\n- Look up `status`, compare to `'active'`\n- Combine results with AND logic\nThe tree traversal happens 100,000 times. The pointer chasing happens 100,000 times. The function call overhead happens 100,000 times.\nA bytecode compiler does the tree traversal **once** \u2014 at compile time. It produces a flat sequence of instructions that can be executed with minimal overhead:\n```\nOpenRead     0  users\nRewind       0\nColumn       0  first_name  r1\nNe           r1  'John'     L1\nColumn       0  age         r2\nLe           r2  25         L1\nColumn       0  status      r3\nNe           r3  'active'   L1\nResultRow    ...\nL1:\nNext         0\nHalt\n```\nThe VM executes this in a tight loop \u2014 no pointer chasing, no recursive calls, no tree traversal per row. The difference isn't 2x or 3x; it's often 10x-100x for query-heavy workloads.\n### The Real Cost: Lost Opportunities\nBut there's a deeper problem with tree-walking: **you can't optimize what you can't see.**\nWhen the query planner wants to reorder AND conditions (put the most selective first), it needs to manipulate the execution plan. When you want to cache a compiled query for prepared statements, you need something serializable. When you want `EXPLAIN` to show the execution plan, you need a representation that makes sense to display.\nBytecode is the *unit of optimization*. It's a flat, linear representation that can be:\n- **Analyzed**: Count instructions, estimate cost, find hot paths\n- **Transformed**: Reorder operations, eliminate dead code, fuse loops\n- **Cached**: Store the compiled program for prepared statement reuse\n- **Displayed**: `EXPLAIN` just prints the instruction sequence\nThis is why SQLite, Lua, Python, the JVM, and WebAssembly all use bytecode. It's not about compilation speed \u2014 it's about having a representation that enables everything else.\n---\n## The Revelation: Bytecode is the Universal Enabler\n\n![VDBE Virtual Machine Architecture](./diagrams/diag-vdbe-architecture.svg)\n\nHere's what most learners miss: **bytecode isn't just an implementation detail.** It's the architectural backbone that makes modern database features possible.\n### EXPLAIN is Just Pretty-Printing\n```sql\nEXPLAIN SELECT * FROM users WHERE age > 18\n```\nOutput:\n```\naddr  opcode         p1    p2    p3    p4             p5  comment\n----  -------------  ----  ----  ----  -------------  --  -------\n0     Init           0     10    0                    0   \n1     OpenRead       0     2     0     3              0   users\n2     Rewind         0     9     0                    0   \n3       Column       0     1     1                    0   age\n4       Le           1     8     2     18             0   if age<=18 goto 8\n5       Column       0     0     2                    0   id\n6       Column       0     2     3                    0   name\n7       ResultRow    2     2     0                    0   \n8     Next          0     3     0                    0   \n9     Halt          0     0     0                    0   \n10    Noop          0     0     0                    0   \n```\nThis isn't a separate analysis pass. The compiler already produced this bytecode. `EXPLAIN` just formats it for display. Without bytecode, you'd need a separate \"plan representation\" that duplicates the execution logic.\n### Prepared Statements Cache Bytecode\n```c\n// First call: parse, compile, cache\nsqlite3_prepare_v2(db, \"SELECT * FROM users WHERE id = ?\", -1, &stmt, NULL);\n// Bind parameter and execute (bytecode already compiled)\nsqlite3_bind_int(stmt, 1, user_id);\nsqlite3_step(stmt);\n// Execute again with different parameter (no recompile!)\nsqlite3_reset(stmt);\nsqlite3_bind_int(stmt, 1, other_user_id);\nsqlite3_step(stmt);\n```\nThe `sqlite3_prepare_v2` call tokenizes, parses, and compiles the SQL once. The resulting bytecode program is stored in `stmt`. Subsequent executions skip all the frontend work \u2014 they just bind new parameters and run the VM.\n### Query Optimization Rewrites Bytecode\nWhen the optimizer decides to use an index instead of a table scan, it doesn't rebuild the AST. It emits different opcodes:\n```\n-- Without index (table scan):\nOpenRead     0  users\nRewind       0\nColumn       0  age  r1\n...\n-- With index (index seek):\nOpenRead     0  users\nOpenRead     1  idx_age\nSeekGE       1  r1  18     -- seek to first row where age >= 18\nColumn       0  age  r1\n...\n```\nSame logical query, different bytecode. The planner's job is to choose which bytecode sequence to emit.\n### Portability Through Abstraction\nSQLite runs on embedded devices, mobile phones, desktops, and servers. The bytecode VM provides a clean abstraction:\n- **Above the VM**: SQL parsing, AST compilation, query optimization \u2014 platform-independent\n- **Below the VM**: B-tree operations, file I/O, memory management \u2014 platform-specific\nThe VDBE (Virtual Database Engine) is the boundary. Port SQLite to a new platform, and you port the opcodes' implementations \u2014 not the parser, not the compiler, not the planner.\n---\n## The Register-Based VM Design\nYour VDBE is a **register-based virtual machine**. This is a critical design choice with performance implications.\n### Register-Based vs Stack-Based\nThere are two VM architectures:\n**Stack-based** (Java bytecode, WebAssembly, Python):\n- Operands are pushed onto a stack\n- Instructions pop operands, compute, push results\n- Example: `a + b` \u2192 `PUSH a, PUSH b, ADD` (result on stack)\n**Register-based** (Lua 5.0+, SQLite VDBE):\n- Operands are stored in numbered registers\n- Instructions reference registers directly\n- Example: `a + b` \u2192 `ADD r1, r2, r3` (r1 = r2 + r3)\n\n![VDBE Virtual Machine Architecture](./diagrams/diag-vdbe-architecture.svg)\n\nSQLite chose register-based because:\n1. **Fewer instructions**: `ADD r1, r2, r3` replaces `PUSH r2, PUSH r3, ADD, POP r1`\n2. **Better for expressions**: SQL queries involve many column references and intermediate results \u2014 registers are natural\n3. **Easier optimization**: Register allocation enables instruction combining and dead code elimination\n### The Register File\nYour VM maintains an array of registers \u2014 the **register file**:\n```c\ntypedef struct {\n    ValueType type;    // INTEGER, FLOAT, STRING, NULL, BLOB\n    union {\n        int64_t integer_val;\n        double float_val;\n        char* string_val;\n        void* blob_val;\n    } data;\n} Value;\ntypedef struct {\n    Value* registers;     // The register file (dynamically sized)\n    int register_count;   // Number of allocated registers\n    // ... other VM state\n} VDBE;\n```\nThe compiler decides how many registers a program needs and allocates them upfront. For `SELECT a + b FROM t`, you might use:\n- `r1`: column `a`\n- `r2`: column `b`  \n- `r3`: result of `a + b`\n### Instruction Format\nEach instruction is a struct with an opcode and operands:\n```c\ntypedef enum {\n    // Table operations\n    OP_OpenRead,      // Open a table or index for reading\n    OP_OpenWrite,     // Open a table or index for writing\n    OP_Close,         // Close a cursor\n    // Cursor movement\n    OP_Rewind,        // Position cursor at first row\n    OP_Next,          // Move cursor to next row\n    OP_Prev,          // Move cursor to previous row\n    // Data access\n    OP_Column,        // Read a column value into a register\n    OP_MakeRecord,    // Build a row record from registers\n    OP_Insert,        // Insert a record into a table\n    OP_Delete,        // Delete the current row\n    // Arithmetic and comparison\n    OP_Add,           // r[r1] = r[r2] + r[r3]\n    OP_Subtract,      // r[r1] = r[r2] - r[r3]\n    OP_Multiply,      // r[r1] = r[r2] * r[r3]\n    OP_Divide,        // r[r1] = r[r2] / r[r3]\n    // Comparisons (jump to target if condition true/false)\n    OP_Eq,            // if r[p1] == r[p2] goto p3\n    OP_Ne,            // if r[p1] != r[p2] goto p3\n    OP_Lt,            // if r[p1] < r[p2] goto p3\n    OP_Le,            // if r[p1] <= r[p2] goto p3\n    OP_Gt,            // if r[p1] > r[p2] goto p3\n    OP_Ge,            // if r[p1] >= r[p2] goto p3\n    // Control flow\n    OP_Goto,          // Unconditional jump\n    OP_Gosub,         // Call subroutine (push return address)\n    OP_Return,        // Return from subroutine\n    OP_Halt,          // Stop execution\n    // Output\n    OP_ResultRow,     // Output current result row\n    // Utility\n    OP_Integer,       // Load integer constant into register\n    OP_String8,       // Load string constant into register\n    OP_Null,          // Load NULL into register\n    OP_Copy,          // Copy register value\n    OP_SCopy,         // Shallow copy (for read-only use)\n    // Transaction\n    OP_Transaction,   // Begin a transaction\n    OP_AutoCommit,    // Set autocommit mode\n    // ... many more\n} OpCode;\ntypedef struct {\n    OpCode opcode;     // What operation to perform\n    int p1;            // First operand (often register number or cursor number)\n    int p2;            // Second operand (often register number or jump target)\n    int p3;            // Third operand (often register number)\n    char* p4;          // Fourth operand (often string constant or pointer)\n    int p5;            // Fifth operand (flags)\n} Instruction;\n```\nThe exact instruction format varies. SQLite uses a packed format for storage efficiency, but a struct is fine for your implementation.\n---\n## The Compiler: AST to Bytecode\nThe compiler walks the AST and emits instructions. It's a recursive process \u2014 each AST node type has a corresponding emit function.\n### Compiler State\n```c\ntypedef struct {\n    Instruction* code;        // Array of emitted instructions\n    int code_count;           // Number of instructions\n    int code_capacity;        // Capacity of code array\n    int next_register;        // Next available register number\n    int next_cursor;          // Next available cursor number\n    // For jump target resolution\n    int* pending_jumps;       // Instruction addresses needing target resolution\n    int pending_jump_count;\n    // Symbol table for table/index lookup\n    SymbolTable* symbols;\n    // Error handling\n    bool has_error;\n    char error_message[256];\n} Compiler;\n```\n### Emitting Instructions\nHelper functions make instruction emission clean:\n```c\nint emit(Compiler* c, OpCode opcode, int p1, int p2, int p3, const char* p4) {\n    if (c->code_count >= c->code_capacity) {\n        c->code_capacity *= 2;\n        c->code = realloc(c->code, c->code_capacity * sizeof(Instruction));\n    }\n    int addr = c->code_count++;\n    c->code[addr].opcode = opcode;\n    c->code[addr].p1 = p1;\n    c->code[addr].p2 = p2;\n    c->code[addr].p3 = p3;\n    c->code[addr].p4 = p4 ? strdup(p4) : NULL;\n    c->code[addr].p5 = 0;\n    return addr;\n}\nint allocate_register(Compiler* c) {\n    return c->next_register++;\n}\nint allocate_cursor(Compiler* c) {\n    return c->next_cursor++;\n}\n```\n### Compiling SELECT\nA SELECT statement compiles to a table scan with optional filtering:\n```c\nvoid compile_select(Compiler* c, ASTNode* select) {\n    // Step 1: Open the table\n    int cursor = allocate_cursor(c);\n    Table* table = symbol_table_lookup(c->symbols, select->data.select_stmt.table_name);\n    emit(c, OP_OpenRead, cursor, table->root_page, 0, NULL);\n    // Step 2: Position at start\n    int loop_start = c->code_count;\n    emit(c, OP_Rewind, cursor, 0, 0, NULL);\n    // Remember where to jump when done\n    int loop_end_placeholder = emit(c, OP_Goto, 0, 0, 0, NULL);  // Target filled in later\n    // Step 3: For each row, evaluate WHERE clause\n    int where_start = c->code_count;\n    if (select->data.select_stmt.where) {\n        // Compile WHERE expression, result goes in a register\n        int result_reg = compile_expression(c, select->data.select_stmt.where);\n        // If result is false/NULL, skip to Next\n        int skip_row_placeholder = emit(c, OP_IfNot, result_reg, 0, 0, NULL);\n    }\n    // Step 4: Output the row\n    compile_result_row(c, select, cursor);\n    // Step 5: Next iteration\n    int next_addr = emit(c, OP_Next, cursor, where_start, 0, NULL);\n    // Fix up jump targets\n    if (select->data.select_stmt.where) {\n        // If condition was false, jump here (after ResultRow, before Next)\n        // Actually, jump should go to Next\n        patch_jump(c, skip_row_placeholder, next_addr);\n    }\n    // When Rewind finds empty table, jump here\n    patch_jump(c, loop_end_placeholder, c->code_count);\n    // Step 6: Halt\n    emit(c, OP_Halt, 0, 0, 0, NULL);\n}\n```\nThis is the basic structure. Let's fill in the details.\n### Compiling Expressions\nExpressions are where registers come into play. Each expression is compiled to produce its result in a register:\n```c\n// Returns the register containing the expression result\nint compile_expression(Compiler* c, ASTNode* expr) {\n    switch (expr->type) {\n        case AST_LITERAL_EXPR:\n            return compile_literal(c, expr);\n        case AST_IDENTIFIER_EXPR:\n            return compile_identifier(c, expr);\n        case AST_BINARY_EXPR:\n            return compile_binary_expr(c, expr);\n        case AST_UNARY_EXPR:\n            return compile_unary_expr(c, expr);\n        default:\n            compiler_error(c, \"unknown expression type\");\n            return -1;\n    }\n}\nint compile_literal(Compiler* c, ASTNode* literal) {\n    int reg = allocate_register(c);\n    Token t = literal->data.literal_expr.token;\n    switch (t.type) {\n        case TOKEN_INTEGER:\n            emit(c, OP_Integer, atoi(t.value), reg, 0, NULL);\n            break;\n        case TOKEN_FLOAT:\n            // For floats, store the value and reference it\n            emit(c, OP_Real, 0, reg, 0, t.value);  // p4 holds string representation\n            break;\n        case TOKEN_STRING:\n            emit(c, OP_String8, 0, reg, 0, t.value);\n            break;\n        case TOKEN_KEYWORD:\n            if (strcmp(t.value, \"NULL\") == 0) {\n                emit(c, OP_Null, 0, reg, 0, NULL);\n            } else if (strcmp(t.value, \"TRUE\") == 0) {\n                emit(c, OP_Integer, 1, reg, 0, NULL);\n            } else if (strcmp(t.value, \"FALSE\") == 0) {\n                emit(c, OP_Integer, 0, reg, 0, NULL);\n            }\n            break;\n        default:\n            compiler_error(c, \"unknown literal type\");\n    }\n    return reg;\n}\nint compile_identifier(Compiler* c, ASTNode* ident) {\n    // Identifier in an expression context is a column reference\n    // We need to know which cursor and column index\n    // This requires context from the enclosing SELECT\n    // For now, assume we have current cursor context\n    int cursor = c->current_cursor;\n    const char* col_name = ident->data.identifier_expr.name;\n    // Look up column index in table schema\n    int col_index = table_column_index(c->current_table, col_name);\n    int reg = allocate_register(c);\n    emit(c, OP_Column, cursor, col_index, reg, NULL);\n    return reg;\n}\nint compile_binary_expr(Compiler* c, ASTNode* expr) {\n    const char* op = expr->data.binary_expr.operator;\n    // Special handling for AND/OR (short-circuit evaluation)\n    if (strcmp(op, \"AND\") == 0) {\n        return compile_and(c, expr);\n    }\n    if (strcmp(op, \"OR\") == 0) {\n        return compile_or(c, expr);\n    }\n    // Comparison operators\n    if (is_comparison_op(op)) {\n        return compile_comparison(c, expr);\n    }\n    // Arithmetic operators\n    int left_reg = compile_expression(c, expr->data.binary_expr.left);\n    int right_reg = compile_expression(c, expr->data.binary_expr.right);\n    int result_reg = allocate_register(c);\n    if (strcmp(op, \"+\") == 0) {\n        emit(c, OP_Add, left_reg, right_reg, result_reg, NULL);\n    } else if (strcmp(op, \"-\") == 0) {\n        emit(c, OP_Subtract, left_reg, right_reg, result_reg, NULL);\n    } else if (strcmp(op, \"*\") == 0) {\n        emit(c, OP_Multiply, left_reg, right_reg, result_reg, NULL);\n    } else if (strcmp(op, \"/\") == 0) {\n        emit(c, OP_Divide, left_reg, right_reg, result_reg, NULL);\n    }\n    return result_reg;\n}\n```\n### WHERE Clause: Conditional Jumps\n\n![WHERE Clause Bytecode Pattern](./diagrams/diag-bytecode-where.svg)\n\nThe WHERE clause is the interesting case. Consider:\n```sql\nSELECT * FROM users WHERE age > 18 AND status = 'active'\n```\nThe bytecode needs to skip rows that don't match. Here's the pattern:\n```\nOpenRead     cursor=0  table=users\nRewind       cursor=0  if-empty-goto=END\nLOOP_START:\n  Column     cursor=0  column=age      dest=r1\n  Le         r1        18              if-true-goto=SKIP_ROW   -- age <= 18? skip\n  Column     cursor=0  column=status   dest=r2\n  Ne         r2        'active'        if-true-goto=SKIP_ROW   -- status != 'active'? skip\n  Column     cursor=0  column=id       dest=r3\n  Column     cursor=0  column=name     dest=r4\n  Column     cursor=0  column=age      dest=r5\n  ResultRow  start=r3  count=3\nSKIP_ROW:\n  Next       cursor=0  if-more-goto=LOOP_START\nEND:\n  Halt\n```\nThe key insight: **comparisons are inverted**. The WHERE clause says `age > 18`, but the bytecode checks `age <= 18` and jumps to skip the row. This is because we're implementing \"skip if condition fails\" rather than \"continue if condition passes.\"\n```c\nvoid compile_where_clause(Compiler* c, ASTNode* where, int skip_target) {\n    // WHERE expression should evaluate to a boolean\n    // If false or NULL, jump to skip_target\n    if (where->type == AST_BINARY_EXPR) {\n        const char* op = where->data.binary_expr.operator;\n        // For AND: if left side is false, skip; else check right side\n        if (strcmp(op, \"AND\") == 0) {\n            // Compile left side, skip if false\n            compile_where_clause(c, where->data.binary_expr.left, skip_target);\n            // If we get here, left was true; check right side\n            compile_where_clause(c, where->data.binary_expr.right, skip_target);\n            return;\n        }\n        // For OR: if left side is true, continue; else check right side\n        if (strcmp(op, \"OR\") == 0) {\n            int check_right = allocate_label(c);\n            compile_or_condition(c, where, skip_target, check_right);\n            patch_label(c, check_right);\n            return;\n        }\n        // For comparisons: evaluate both sides, compare, jump if condition fails\n        if (is_comparison_op(op)) {\n            int left_reg = compile_expression(c, where->data.binary_expr.left);\n            int right_reg = compile_expression(c, where->data.binary_expr.right);\n            // Emit inverted comparison\n            OpCode jump_op = inverted_comparison_op(op);\n            emit(c, jump_op, left_reg, right_reg, skip_target, NULL);\n            return;\n        }\n    }\n    // General case: evaluate expression, check if true\n    int result_reg = compile_expression(c, where);\n    emit(c, OP_IfNot, result_reg, skip_target, 0, NULL);\n}\nOpCode inverted_comparison_op(const char* op) {\n    if (strcmp(op, \"=\") == 0)  return OP_Ne;   // if not equal, skip\n    if (strcmp(op, \"<\") == 0)  return OP_Ge;   // if >=, skip\n    if (strcmp(op, \">\") == 0)  return OP_Le;   // if <=, skip\n    if (strcmp(op, \"<=\") == 0) return OP_Gt;   // if >, skip\n    if (strcmp(op, \">=\") == 0) return OP_Lt;   // if <, skip\n    if (strcmp(op, \"<>\") == 0 || strcmp(op, \"!=\") == 0) return OP_Eq;  // if equal, skip\n    return OP_Noop;\n}\n```\n### Jump Target Resolution\nWhen you emit a jump instruction, you often don't know the target address yet. You need to **patch** it later:\n```c\ntypedef struct {\n    int instruction_addr;   // Address of the jump instruction\n    int target_addr;        // Target address (or -1 if not yet known)\n} JumpPatch;\nvoid patch_jump(Compiler* c, int instruction_addr, int target_addr) {\n    c->code[instruction_addr].p2 = target_addr;\n}\n// For forward jumps (jumping to code not yet emitted)\nint emit_placeholder_jump(Compiler* c, OpCode opcode) {\n    return emit(c, opcode, 0, -1, 0, NULL);  // p2 = -1 means \"patch later\"\n}\nvoid resolve_jumps(Compiler* c) {\n    // Second pass: resolve all placeholder jumps\n    // This is useful when you have labels instead of addresses\n}\n```\n---\n## The Virtual Machine: Fetch-Decode-Execute\n\n![VDBE Virtual Machine Architecture](./diagrams/diag-vdbe-architecture.svg)\n\nThe VM is the heart of execution. It's a simple loop:\n```c\ntypedef struct {\n    Instruction* code;\n    int code_length;\n    int pc;                  // Program counter\n    Value* registers;\n    int register_count;\n    Cursor* cursors;         // Array of open cursors\n    int cursor_count;\n    // Output callback\n    ResultCallback callback;\n    void* callback_data;\n    // Execution state\n    bool halted;\n    int error_code;\n    char error_message[256];\n} VDBE;\nint vdbe_execute(VDBE* vm) {\n    while (!vm->halted && vm->pc < vm->code_length) {\n        Instruction* instr = &vm->code[vm->pc];\n        int result = execute_instruction(vm, instr);\n        if (result != 0) {\n            return result;  // Error\n        }\n    }\n    return 0;  // Success\n}\nint execute_instruction(VDBE* vm, Instruction* instr) {\n    switch (instr->opcode) {\n        case OP_Halt:\n            vm->halted = true;\n            vm->pc++;\n            return 0;\n        case OP_Goto:\n            vm->pc = instr->p2;\n            return 0;\n        case OP_Integer:\n            vm->registers[instr->p2].type = VALUE_INTEGER;\n            vm->registers[instr->p2].data.integer_val = instr->p1;\n            vm->pc++;\n            return 0;\n        case OP_String8:\n            vm->registers[instr->p2].type = VALUE_STRING;\n            vm->registers[instr->p2].data.string_val = strdup(instr->p4);\n            vm->pc++;\n            return 0;\n        case OP_Null:\n            vm->registers[instr->p2].type = VALUE_NULL;\n            vm->pc++;\n            return 0;\n        case OP_OpenRead: {\n            int cursor_id = instr->p1;\n            int table_root = instr->p2;\n            vm->cursors[cursor_id].table = table_open_for_read(table_root);\n            vm->cursors[cursor_id].eof = false;\n            vm->pc++;\n            return 0;\n        }\n        case OP_Rewind: {\n            int cursor_id = instr->p1;\n            Cursor* cursor = &vm->cursors[cursor_id];\n            btree_rewind(cursor);\n            if (btree_eof(cursor)) {\n                // Table is empty, jump to p2\n                vm->pc = instr->p2;\n            } else {\n                vm->pc++;\n            }\n            return 0;\n        }\n        case OP_Column: {\n            int cursor_id = instr->p1;\n            int col_index = instr->p2;\n            int dest_reg = instr->p3;\n            Cursor* cursor = &vm->cursors[cursor_id];\n            Row row;\n            btree_fetch_row(cursor, &row);\n            // Copy column value to register\n            copy_value(&vm->registers[dest_reg], &row.columns[col_index]);\n            vm->pc++;\n            return 0;\n        }\n        case OP_Next: {\n            int cursor_id = instr->p1;\n            Cursor* cursor = &vm->cursors[cursor_id];\n            btree_next(cursor);\n            if (btree_eof(cursor)) {\n                // No more rows, continue to next instruction\n                vm->pc++;\n            } else {\n                // More rows, jump back to loop start (p2)\n                vm->pc = instr->p2;\n            }\n            return 0;\n        }\n        case OP_ResultRow: {\n            int start_reg = instr->p1;\n            int count = instr->p2;\n            // Build result row from registers\n            Row result;\n            result.column_count = count;\n            result.columns = malloc(count * sizeof(Value));\n            for (int i = 0; i < count; i++) {\n                copy_value(&result.columns[i], &vm->registers[start_reg + i]);\n            }\n            // Invoke callback\n            if (vm->callback) {\n                vm->callback(&result, vm->callback_data);\n            }\n            free_row(&result);\n            vm->pc++;\n            return 0;\n        }\n        // Comparison operators\n        case OP_Eq:\n        case OP_Ne:\n        case OP_Lt:\n        case OP_Le:\n        case OP_Gt:\n        case OP_Ge: {\n            Value* left = &vm->registers[instr->p1];\n            Value* right = &vm->registers[instr->p2];\n            int jump_target = instr->p3;\n            int cmp = compare_values(left, right);\n            bool condition = false;\n            switch (instr->opcode) {\n                case OP_Eq: condition = (cmp == 0); break;\n                case OP_Ne: condition = (cmp != 0); break;\n                case OP_Lt: condition = (cmp < 0); break;\n                case OP_Le: condition = (cmp <= 0); break;\n                case OP_Gt: condition = (cmp > 0); break;\n                case OP_Ge: condition = (cmp >= 0); break;\n            }\n            if (condition) {\n                vm->pc = jump_target;\n            } else {\n                vm->pc++;\n            }\n            return 0;\n        }\n        // Arithmetic\n        case OP_Add: {\n            Value* a = &vm->registers[instr->p1];\n            Value* b = &vm->registers[instr->p2];\n            Value* result = &vm->registers[instr->p3];\n            if (a->type == VALUE_INTEGER && b->type == VALUE_INTEGER) {\n                result->type = VALUE_INTEGER;\n                result->data.integer_val = a->data.integer_val + b->data.integer_val;\n            } else {\n                result->type = VALUE_FLOAT;\n                result->data.float_val = value_to_float(a) + value_to_float(b);\n            }\n            vm->pc++;\n            return 0;\n        }\n        // ... other opcodes\n        default:\n            vm->error_code = -1;\n            snprintf(vm->error_message, sizeof(vm->error_message),\n                     \"unknown opcode: %d\", instr->opcode);\n            return -1;\n    }\n}\n```\n### Comparison with NULL\nSQL uses **three-valued logic**: comparisons with NULL return NULL (unknown), not false. Your comparison opcodes must handle this:\n```c\nint compare_values(Value* a, Value* b) {\n    // NULL comparisons\n    if (a->type == VALUE_NULL || b->type == VALUE_NULL) {\n        // In SQL, NULL compared to anything is NULL (not true or false)\n        // The caller should check for NULL and handle accordingly\n        return CMP_NULL;\n    }\n    // Type coercion for mixed types\n    if (a->type == VALUE_INTEGER && b->type == VALUE_INTEGER) {\n        if (a->data.integer_val < b->data.integer_val) return -1;\n        if (a->data.integer_val > b->data.integer_val) return 1;\n        return 0;\n    }\n    // ... handle other type combinations\n    return 0;\n}\n```\nThe comparison opcodes need to treat NULL specially:\n```c\ncase OP_Eq: {\n    Value* left = &vm->registers[instr->p1];\n    Value* right = &vm->registers[instr->p2];\n    // If either operand is NULL, the comparison is NULL (unknown)\n    // In a WHERE context, NULL is treated as false (row is skipped)\n    if (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n        // Don't jump \u2014 treat NULL as \"condition not met\"\n        vm->pc++;\n        return 0;\n    }\n    int cmp = compare_values(left, right);\n    if (cmp == 0) {\n        vm->pc = instr->p3;  // Jump if equal\n    } else {\n        vm->pc++;\n    }\n    return 0;\n}\n```\n---\n## Compiling INSERT\nINSERT is simpler than SELECT \u2014 no loops, just build a record and insert it:\n```c\nvoid compile_insert(Compiler* c, ASTNode* insert) {\n    // Step 1: Open the table for writing\n    int cursor = allocate_cursor(c);\n    Table* table = symbol_table_lookup(c->symbols, insert->data.insert_stmt.table_name);\n    emit(c, OP_OpenWrite, cursor, table->root_page, 0, NULL);\n    // Step 2: Compile each value expression into registers\n    int value_count = list_size(insert->data.insert_stmt.values);\n    int start_reg = allocate_register(c);\n    for (int i = 0; i < value_count; i++) {\n        ASTNode* value_expr = list_get(insert->data.insert_stmt.values, i);\n        int reg = compile_expression(c, value_expr);\n        // Note: compile_expression allocates registers sequentially\n    }\n    // Step 3: Build the record\n    int record_reg = allocate_register(c);\n    emit(c, OP_MakeRecord, start_reg, value_count, record_reg, NULL);\n    // Step 4: Generate rowid (or use provided one)\n    int rowid_reg = allocate_register(c);\n    emit(c, OP_NewRowid, cursor, rowid_reg, 0, NULL);\n    // Step 5: Insert the record\n    emit(c, OP_Insert, cursor, record_reg, rowid_reg, NULL);\n    // Step 6: Close and halt\n    emit(c, OP_Close, cursor, 0, 0, NULL);\n    emit(c, OP_Halt, 0, 0, 0, NULL);\n}\n```\nThe bytecode for `INSERT INTO users VALUES (1, 'Alice', 30)`:\n```\nOpenWrite    cursor=0  table=users\nInteger      1         r1\nString8      'Alice'   r2\nInteger      30        r3\nMakeRecord   r1        3         r4\nNewRowid     cursor=0  r5\nInsert       cursor=0  r4        r5\nClose        cursor=0\nHalt\n```\n---\n## The Three-Level View\n### Level 1: Application (AST \u2192 Executed Query)\nAt the API level, the compiler + VM are hidden behind a single call:\n```c\nResult* execute_query(Database* db, const char* sql);\n```\nOr, for prepared statements:\n```c\nPreparedStatement* prepare(Database* db, const char* sql);\nResult* execute_prepared(PreparedStatement* stmt);\n```\nThe caller provides SQL text and receives results. The compilation and execution are invisible.\n### Level 2: Engine (Compiler + VM Loop)\nInside, the compiler transforms the AST:\n```c\nCompiledProgram* compile(ASTNode* ast) {\n    Compiler c = init_compiler();\n    compile_statement(&c, ast);\n    return get_program(&c);\n}\n```\nAnd the VM executes:\n```c\nint execute(CompiledProgram* program, Database* db, ResultCallback callback) {\n    VDBE vm = init_vdbe(program, db, callback);\n    return vdbe_execute(&vm);\n}\n```\nThe compiler is a one-time cost. The VM is the hot loop.\n### Level 3: Implementation (Instruction Dispatch)\nAt the lowest level, the VM is a `switch` statement in a `while` loop:\n```c\nwhile (!halted) {\n    Instruction* i = &code[pc++];\n    switch (i->opcode) {\n        case OP_Add: /* ... */ break;\n        case OP_Column: /* ... */ break;\n        // ...\n    }\n}\n```\nThis is the **interpreter pattern**. Every instruction dispatch involves:\n1. Fetch instruction from array\n2. Decode opcode (switch statement)\n3. Execute (run the case body)\n4. Increment PC (or jump)\nPerformance-critical VMs optimize this further with **direct threading** (computed gotos) or **JIT compilation**, but a switch-based interpreter is the foundation.\n---\n## EXPLAIN: Window Into Execution\n\n![EXPLAIN Command Output](./diagrams/diag-explain-output.svg)\n\nThe EXPLAIN command is trivial once you have bytecode:\n```c\nvoid explain_program(CompiledProgram* program, FILE* output) {\n    fprintf(output, \"addr  opcode           p1    p2    p3    p4\\n\");\n    fprintf(output, \"----  ---------------  ----  ----  ----  -----\\n\");\n    for (int i = 0; i < program->code_length; i++) {\n        Instruction* instr = &program->code[i];\n        fprintf(output, \"%-4d  %-15s  %-4d  %-4d  %-4d  %s\\n\",\n                i,\n                opcode_name(instr->opcode),\n                instr->p1,\n                instr->p2,\n                instr->p3,\n                instr->p4 ? instr->p4 : \"\");\n    }\n}\nconst char* opcode_name(OpCode op) {\n    static const char* names[] = {\n        [OP_OpenRead] = \"OpenRead\",\n        [OP_OpenWrite] = \"OpenWrite\",\n        [OP_Rewind] = \"Rewind\",\n        [OP_Column] = \"Column\",\n        [OP_Next] = \"Next\",\n        [OP_ResultRow] = \"ResultRow\",\n        [OP_Halt] = \"Halt\",\n        [OP_Insert] = \"Insert\",\n        [OP_MakeRecord] = \"MakeRecord\",\n        [OP_Eq] = \"Eq\",\n        [OP_Ne] = \"Ne\",\n        [OP_Lt] = \"Lt\",\n        [OP_Le] = \"Le\",\n        [OP_Gt] = \"Gt\",\n        [OP_Ge] = \"Ge\",\n        [OP_Goto] = \"Goto\",\n        [OP_Integer] = \"Integer\",\n        [OP_String8] = \"String8\",\n        [OP_Null] = \"Null\",\n        // ...\n    };\n    return names[op];\n}\n```\nWhen the parser sees `EXPLAIN SELECT ...`, it:\n1. Compiles the SELECT normally\n2. Calls `explain_program()` instead of `vdbe_execute()`\n3. Returns the formatted output\n---\n## Performance: The 100ms Target\nThe acceptance criterion states: \"Bytecode execution of `SELECT * FROM t` on a 10,000-row table completes in under 100ms.\"\nThis is achievable with a straightforward implementation. Here's why:\n### Instruction Cost Analysis\nFor a full table scan of 10,000 rows, each row executes:\n- 1x `Column` per output column (let's say 3 columns = 3 instructions)\n- 1x `ResultRow`\n- 1x `Next`\nThat's 5 instructions per row, plus setup/teardown. Total: ~50,000 instructions.\nAt 1 million instructions per second (very conservative for a switch-based interpreter), that's 50ms. You have headroom.\n### Where Time Goes\nThe real cost isn't instruction dispatch \u2014 it's:\n1. **B-tree traversal**: `Next` calls into the B-tree layer\n2. **Row deserialization**: `Column` decodes variable-length records\n3. **Memory allocation**: `ResultRow` allocates and copies data\nOptimization priorities:\n1. **Minimize allocations**: Reuse buffers, avoid per-row malloc\n2. **Inline hot paths**: Make `Column` and `Next` fast\n3. **Cache table metadata**: Don't look up column info per row\n### Benchmarking\n```c\nvoid benchmark_table_scan() {\n    Database* db = create_test_database();\n    // Create table with 10,000 rows\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, name TEXT, value REAL)\");\n    for (int i = 0; i < 10000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, 'name%d', %d.5)\", \n                 i, i, i);\n        execute_sql(db, sql);\n    }\n    // Benchmark SELECT\n    clock_t start = clock();\n    Result* result = execute_sql(db, \"SELECT * FROM test\");\n    clock_t end = clock();\n    double elapsed_ms = (double)(end - start) / CLOCKS_PER_SEC * 1000;\n    printf(\"Table scan of %d rows took %.2f ms\\n\", result->row_count, elapsed_ms);\n    assert(elapsed_ms < 100.0);\n    free_result(result);\n    free_database(db);\n}\n```\n---\n## Testing Strategy\n### Opcode Tests\nTest each opcode in isolation:\n```c\nvoid test_op_integer() {\n    VDBE vm = create_test_vm();\n    Instruction instr = {OP_Integer, 42, 0, 0, NULL};  // r0 = 42\n    execute_instruction(&vm, &instr);\n    assert(vm.registers[0].type == VALUE_INTEGER);\n    assert(vm.registers[0].data.integer_val == 42);\n}\nvoid test_op_add() {\n    VDBE vm = create_test_vm();\n    vm.registers[0].type = VALUE_INTEGER;\n    vm.registers[0].data.integer_val = 5;\n    vm.registers[1].type = VALUE_INTEGER;\n    vm.registers[1].data.integer_val = 3;\n    Instruction instr = {OP_Add, 0, 1, 2, NULL};  // r2 = r0 + r1\n    execute_instruction(&vm, &instr);\n    assert(vm.registers[2].type == VALUE_INTEGER);\n    assert(vm.registers[2].data.integer_val == 8);\n}\nvoid test_op_eq_jump() {\n    VDBE vm = create_test_vm();\n    vm.registers[0].type = VALUE_INTEGER;\n    vm.registers[0].data.integer_val = 5;\n    vm.registers[1].type = VALUE_INTEGER;\n    vm.registers[1].data.integer_val = 5;\n    Instruction instr = {OP_Eq, 0, 1, 10, NULL};  // if r0 == r1 goto 10\n    execute_instruction(&vm, &instr);\n    assert(vm.pc == 10);  // Jumped\n}\n```\n### Compiler Tests\nTest compilation of specific constructs:\n```c\nvoid test_compile_select_star() {\n    const char* sql = \"SELECT * FROM users\";\n    CompiledProgram* prog = compile_sql(sql);\n    // Check for expected opcodes\n    assert(has_opcode(prog, OP_OpenRead));\n    assert(has_opcode(prog, OP_Rewind));\n    assert(has_opcode(prog, OP_Column));\n    assert(has_opcode(prog, OP_ResultRow));\n    assert(has_opcode(prog, OP_Next));\n    assert(has_opcode(prog, OP_Halt));\n    // Check loop structure\n    int rewind_addr = find_opcode(prog, OP_Rewind);\n    int next_addr = find_opcode(prog, OP_Next);\n    Instruction* next = &prog->code[next_addr];\n    // Next should jump back to after Rewind\n    assert(next->p2 == rewind_addr + 1);\n    free_program(prog);\n}\nvoid test_compile_where_clause() {\n    const char* sql = \"SELECT * FROM users WHERE age > 18\";\n    CompiledProgram* prog = compile_sql(sql);\n    // Check for comparison\n    int gt_addr = find_opcode(prog, OP_Gt);\n    assert(gt_addr >= 0);\n    // Check for conditional jump\n    Instruction* gt = &prog->code[gt_addr];\n    assert(gt->p3 > gt_addr);  // Jump target is after the comparison\n    free_program(prog);\n}\nvoid test_compile_insert() {\n    const char* sql = \"INSERT INTO users VALUES (1, 'Alice', 30)\";\n    CompiledProgram* prog = compile_sql(sql);\n    assert(has_opcode(prog, OP_OpenWrite));\n    assert(has_opcode(prog, OP_Integer));\n    assert(has_opcode(prog, OP_String8));\n    assert(has_opcode(prog, OP_MakeRecord));\n    assert(has_opcode(prog, OP_Insert));\n    assert(has_opcode(prog, OP_Halt));\n    free_program(prog);\n}\n```\n### Integration Tests\nEnd-to-end tests with real execution:\n```c\nvoid test_execute_select() {\n    Database* db = create_in_memory_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, name TEXT)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'Alice')\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, 'Bob')\");\n    Result* result = execute_sql(db, \"SELECT * FROM test\");\n    assert(result->row_count == 2);\n    assert(strcmp(result->rows[0].columns[1].data.string_val, \"Alice\") == 0);\n    assert(strcmp(result->rows[1].columns[1].data.string_val, \"Bob\") == 0);\n    free_result(result);\n    free_database(db);\n}\nvoid test_execute_where() {\n    Database* db = create_in_memory_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 10)\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, 20)\");\n    execute_sql(db, \"INSERT INTO test VALUES (3, 30)\");\n    Result* result = execute_sql(db, \"SELECT * FROM test WHERE value > 15\");\n    assert(result->row_count == 2);\n    assert(result->rows[0].columns[0].data.integer_val == 2);\n    assert(result->rows[1].columns[0].data.integer_val == 3);\n    free_result(result);\n    free_database(db);\n}\n```\n---\n## Common Pitfalls\n### Forgetting the Halt Instruction\nIf you don't emit `OP_Halt`, the VM runs off the end of the program into garbage memory:\n```c\nvoid compile_select(Compiler* c, ASTNode* select) {\n    // ... compile the query ...\n    // DON'T FORGET:\n    emit(c, OP_Halt, 0, 0, 0, NULL);\n}\n```\n### Wrong Jump Targets\nJump targets are instruction addresses, not line numbers. If you insert instructions after emitting a jump, the target may be wrong:\n```c\n// WRONG\nint skip_target = c->code_count + 5;  // Assuming 5 instructions\nemit(c, OP_IfNot, reg, skip_target, 0, NULL);\n// ... emit 6 instructions instead of 5 ...\n// Jump target is now wrong!\n// RIGHT\nint skip_placeholder = emit(c, OP_IfNot, reg, -1, 0, NULL);\n// ... emit instructions ...\nint skip_target = c->code_count;\npatch_jump(c, skip_placeholder, skip_target);\n```\n### Register Clobbering\nIf you reuse a register too early, you lose the value:\n```c\n// WRONG: compiles a + (a * b) but reuses r1\nint r1 = compile_expression(c, left);   // r1 = a\nint r2 = compile_expression(c, right);  // r2 = a (clobbers if same register!)\n// r1 might be invalid now\n// RIGHT: use distinct registers\nint left_reg = compile_expression(c, left);\nint right_reg = compile_expression(c, right);\n// Both registers are valid\n```\nThe `compile_expression` function should always return a fresh register unless you're explicitly reusing.\n### NULL in Comparisons\nSQL's three-valued logic is tricky:\n```c\n// WRONG: treats NULL as comparable\ncase OP_Eq:\n    if (compare_values(left, right) == 0) {\n        vm->pc = instr->p3;\n    }\n    break;\n// RIGHT: NULL comparisons are NULL (neither true nor false)\ncase OP_Eq:\n    if (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n        vm->pc++;  // Don't jump, treat as false\n    } else if (compare_values(left, right) == 0) {\n        vm->pc = instr->p3;\n    } else {\n        vm->pc++;\n    }\n    break;\n```\n### Cursor vs Register Confusion\nCursors are for table traversal; registers are for values. Don't mix them:\n```c\n// WRONG: treating cursor number as register\nemit(c, OP_Column, cursor_id, col_index, cursor_id);  // Dest should be a register!\n// RIGHT: distinct spaces\nint reg = allocate_register(c);\nemit(c, OP_Column, cursor_id, col_index, reg);\n```\n---\n## What You've Built\nYour bytecode compiler and virtual machine are a complete execution engine:\n1. **Compiler** transforms AST into linear bytecode sequences\n2. **Register-based VM** executes bytecode in a fetch-decode-execute loop\n3. **Table scan opcodes** (`OpenRead`, `Rewind`, `Column`, `Next`) iterate over B-tree data\n4. **Comparison opcodes** with conditional jumps implement WHERE clause filtering\n5. **INSERT opcodes** (`MakeRecord`, `Insert`) build and store rows\n6. **EXPLAIN** reveals the execution plan by pretty-printing bytecode\n7. **Three-valued logic** handles NULL correctly in comparisons\nThis is the execution core shared by SQLite, Lua, the JVM, and countless other systems. You've built the foundation for query optimization, prepared statements, and portable database execution.\n---\n## Knowledge Cascade\n### 1. WebAssembly: The Same Pattern in Your Browser\nWebAssembly is bytecode for the web. The pattern is identical:\n```\nC++/Rust source \u2192 WASM bytecode \u2192 Browser VM execution\nSQL source      \u2192 VDBE bytecode \u2192 SQLite VM execution\n```\nWASM uses a stack-based VM (for simpler verification), but the principle is the same: compile once to a portable intermediate format, execute anywhere. The WASM MVP has ~170 opcodes; SQLite's VDBE has ~200.\n### 2. JIT Compilation: From Bytecode to Machine Code\nBytecode is the starting point for Just-In-Time compilation:\n- **LuaJIT** traces hot bytecode paths and compiles them to native code\n- **V8 (Chrome)** parses JavaScript to bytecode, then JITs to machine code\n- **SQLite** doesn't JIT, but the architecture would support it\nThe bytecode provides a clean IR (intermediate representation) for optimization passes before code generation.\n### 3. Query Plan Caching: Why Prepared Statements Are Fast\n```c\n// Without prepared statements: 5 steps per execution\nfor (int i = 0; i < 1000; i++) {\n    char* sql = format(\"SELECT * FROM users WHERE id = %d\", i);\n    execute(sql);  // tokenize \u2192 parse \u2192 compile \u2192 execute \u2192 free\n}\n// With prepared statements: 3 steps once, 2 steps 999 times\nPreparedStatement* stmt = prepare(\"SELECT * FROM users WHERE id = ?\");\nfor (int i = 0; i < 1000; i++) {\n    bind_int(stmt, 1, i);\n    execute(stmt);  // execute \u2192 reset\n}\n```\nThe bytecode is compiled once and cached. This is why parameterized queries are faster *and* safer (SQL injection prevention).\n### 4. Garbage Collection: Register Lifetime\nRegister-based VMs face the same lifetime questions as garbage collectors:\n- When can a register's value be freed?\n- Is a shallow copy safe, or do we need a deep copy?\n- How do we track which registers are \"live\" at each instruction?\nSQLite uses a simple approach: values are copied into registers, and the VM frees them on `Halt` or when overwritten. A more sophisticated VM might use reference counting or liveness analysis.\n### 5. Database Portability: The VDBE Abstraction\nThe VDBE is why SQLite runs on:\n- Standard filesystems (Linux, Windows, macOS)\n- Embedded systems (Arduino, ESP32)\n- Mobile platforms (iOS, Android)\n- In-memory environments\n- Custom VFS (Virtual File System) implementations\nThe bytecode is platform-independent. Porting SQLite means implementing the opcodes on a new platform \u2014 not rewriting the parser, compiler, or planner.\n---\n## What's Next\nWith the VDBE complete, you can execute SQL queries. But every `Column` opcode reads from disk through the B-tree layer \u2014 and that's slow. The next milestone, **Buffer Pool Manager**, adds the caching layer that keeps hot pages in memory, turning disk reads into memory accesses.\nYour VM is the brain. The buffer pool will be the memory.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m3\", \"criteria\": [\"Compiler defines a complete instruction set with opcodes for table operations (OpenRead, OpenWrite, Close), cursor movement (Rewind, Next, Prev), data access (Column, MakeRecord, Insert, Delete), arithmetic (Add, Subtract, Multiply, Divide), comparisons (Eq, Ne, Lt, Le, Gt, Ge), control flow (Goto, Halt), and value loading (Integer, String8, Null)\", \"Compiler implements an emit() function that appends instructions to a growing code array, returning the instruction address\", \"Compiler implements register allocation via allocate_register() that returns monotonically increasing register numbers\", \"Compiler translates SELECT AST into bytecode sequence: OpenRead \u2192 Rewind \u2192 [Column for each output column] \u2192 ResultRow \u2192 Next (looping) \u2192 Halt\", \"Compiler translates INSERT AST into bytecode sequence: OpenWrite \u2192 [value expressions to registers] \u2192 MakeRecord \u2192 NewRowid \u2192 Insert \u2192 Close \u2192 Halt\", \"WHERE clause expressions compile to conditional jump opcodes with inverted comparisons (e.g., 'age > 18' compiles to Le jump-if-age<=18)\", \"Compiler handles AND expressions in WHERE by chaining conditional jumps (if first condition false, skip; else check second condition)\", \"Compiler handles OR expressions in WHERE by implementing short-circuit logic with appropriate jump targets\", \"Compiler implements jump target patching for forward references (emit placeholder, patch target address later)\", \"VM implements a fetch-decode-execute loop: while (!halted) { fetch instruction; switch on opcode; execute; }\", \"VM maintains a register file as an array of Value structs with type discrimination (INTEGER, FLOAT, STRING, NULL, BLOB)\", \"VM maintains a cursor array for open table/index traversals, tracking position and EOF state\", \"VM comparison opcodes implement SQL three-valued logic: NULL compared to any value does NOT trigger the jump (treated as condition-not-met)\", \"VM arithmetic opcodes handle type coercion (INTEGER + INTEGER \u2192 INTEGER, mixed types \u2192 FLOAT)\", \"EXPLAIN command outputs human-readable bytecode listing with columns: addr, opcode, p1, p2, p3, p4, comment\", \"EXPLAIN output correctly shows jump targets as instruction addresses that match the actual target line\", \"Bytecode execution of SELECT * FROM t on a 10,000-row table completes in under 100ms\", \"VM correctly handles empty tables: Rewind detects EOF and jumps past the loop body without executing ResultRow\", \"ResultRow instruction extracts values from consecutive registers and invokes a callback or populates a result structure\", \"Test suite validates opcode execution in isolation (e.g., OP_Integer sets register correctly, OP_Add produces correct sum)\", \"Test suite validates compiler output for SELECT, INSERT, and WHERE constructs (checking for expected opcodes and jump targets)\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m4 -->\n# Buffer Pool Manager: Where Memory Meets Durability\nYour virtual machine executes bytecode, reading columns and inserting rows. Every `Column` opcode reaches through the B-tree layer to fetch data. But where does that data live?\nOn disk. Every column read, every row insert, every index lookup \u2014 they all touch the disk. A spinning hard drive takes 5-10 milliseconds per random seek. An SSD is faster at 0.1ms, but that's still 100,000 nanoseconds. Your CPU executes billions of operations per second. Waiting for disk is like a Formula 1 car stopped at a red light that lasts for months.\nYou need a cache. Not a simple cache \u2014 a **buffer pool**: a managed collection of in-memory page frames that tracks what's loaded, what's been modified, and what can be safely evicted. This is where your database transitions from \"slow but correct\" to \"fast and still correct.\"\n---\n## The Tension: You Can't Fit Everything in Memory\nA database might store terabytes of data. Your server might have 64 gigabytes of RAM. Even if you could fit everything in memory, the operating system claims its share, other processes claim theirs, and your database gets what's left.\n\n![Buffer Pool Internal Structure](./diagrams/diag-buffer-pool-structure.svg)\n\nThe fundamental constraint: **you must choose what stays in memory and what gets evicted.** This choice has consequences:\n- **Evict the wrong page**: The next query needs it, forcing a disk read (slow)\n- **Never evict**: Memory fills up, no room for new pages, system grinds to a halt\n- **Evict a dirty page without writing**: Lost updates, corrupted data\nThe buffer pool is a **resource allocation problem** with a durability constraint. You're not just caching for speed \u2014 you're mediating between the volatile world of RAM and the persistent world of disk, ensuring that every committed change survives.\n### The Numbers That Matter\nLet's make this concrete with real latencies:\n| Operation | Latency | CPU Cycles (3GHz) |\n|-----------|---------|-------------------|\n| L1 cache hit | 1 ns | 3 |\n| L2 cache hit | 4 ns | 12 |\n| L3 cache hit | 12 ns | 36 |\n| DRAM access | 100 ns | 300 |\n| SSD random read | 100,000 ns | 300,000 |\n| HDD random read | 10,000,000 ns | 30,000,000 |\nA buffer pool hit (page already in RAM) costs ~100ns. A miss (read from SSD) costs ~100,000ns. That's a **1000x difference** per page access. A query that touches 1000 pages:\n- **100% hit rate**: 0.1 milliseconds\n- **0% hit rate**: 100 milliseconds\nThe buffer pool's job is to maximize that hit rate. A well-tuned buffer pool achieves 95-99% hit rates on realistic workloads, meaning most page accesses avoid the disk entirely.\n---\n## The Revelation: Why Not mmap?\nHere's a seductive idea: *The operating system already has a page cache. Why not just `mmap()` the database file and let the kernel handle everything?*\nThis seems reasonable. The OS knows how to cache pages. It has sophisticated eviction algorithms. It manages memory efficiently. Why reinvent the wheel?\n\n![FetchPage: Hit vs Miss Flow](./diagrams/diag-buffer-pool-flow.svg)\n\n**Because the OS doesn't know what you know.**\n### What the OS Doesn't Understand\n**1. Transaction Boundaries**\nWhen you're in the middle of a transaction, you've modified pages A, B, and C. The transaction isn't committed yet. But the OS doesn't know this \u2014 it might decide to evict page A to make room for something else. If you need page A again, you read it from disk... but wait, the disk version doesn't have your uncommitted changes!\nWith your own buffer pool, you **pin** pages that are part of an active transaction. The OS has no equivalent mechanism.\n**2. Selective fsync**\nWhen you commit a transaction, you need to ensure specific dirty pages reach disk. With mmap, you call `msync()` on a memory range, but the OS may write back *other* dirty pages in that range \u2014 pages from *other* transactions that aren't ready to commit. This violates write-ahead logging protocols.\nYour buffer pool lets you `fsync()` exactly the pages you need, when you need them.\n**3. Access Pattern Knowledge**\nThe OS uses a general-purpose algorithm (typically LRU or CLOCK) that works okay for everything but optimizes for nothing. But you *know* your access pattern:\n- B-tree root pages are accessed on *every* query \u2014 they should never be evicted\n- Internal nodes are hotter than leaf nodes\n- A sequential scan should use MRU (most recently used) eviction, not LRU, because you won't revisit those pages\nThe OS can't distinguish a B-tree root from a random leaf page. You can.\n**4. Pinning for Iterators**\nWhen your B-tree iterator holds a cursor pointing at a page, that page cannot be evicted \u2014 the cursor would become invalid. The OS has no way to express \"this page is in use by an iterator.\" Your buffer pool's pin count handles this naturally.\n### The Real Cost of mmap\nSQLite actually tried this. In 2006, they experimented with mmap-based I/O. The results were disappointing:\n- No performance improvement (the kernel's page cache isn't faster than a user-space buffer pool)\n- Loss of control over eviction\n- Complexity in handling edge cases (what if the OS evicts a page you're writing to?)\n- Portability issues (different OSes handle mmap differently)\nSQLite reverted to its own buffer pool. PostgreSQL, MySQL, Oracle, and virtually every production database does the same thing. The lesson: **when you need transactional guarantees, you can't outsource memory management to a general-purpose OS.**\n---\n## The Buffer Pool Architecture\nYour buffer pool manages a fixed set of **frames** \u2014 slots in memory that hold disk pages. Each frame is exactly one page (typically 4096 bytes, matching disk block size and OS page size).\n```c\n#define PAGE_SIZE 4096\n#define DEFAULT_POOL_SIZE 1000\ntypedef struct {\n    PageId page_id;           // Which disk page is in this frame (-1 if empty)\n    char data[PAGE_SIZE];     // The actual page data\n    int pin_count;            // How many threads/cursors are using this page\n    bool is_dirty;            // Has this page been modified?\n    // LRU list pointers\n    Frame* lru_prev;\n    Frame* lru_next;\n} Frame;\ntypedef struct {\n    Frame* frames;            // Array of frames\n    int frame_count;          // Number of frames\n    // Page ID \u2192 Frame index mapping\n    HashTable* page_table;    // For O(1) lookup by page ID\n    // LRU list\n    Frame* lru_head;          // Most recently used\n    Frame* lru_tail;          // Least recently used\n    // Statistics\n    int hits;\n    int misses;\n    // Disk I/O\n    FileManager* file_manager;\n} BufferPool;\n```\n### The Page Table\nWhen you need page 42, you don't scan all frames looking for it. You use a hash table mapping `PageId \u2192 Frame*`:\n```c\nFrame* lookup_frame(BufferPool* pool, PageId page_id) {\n    return hash_table_get(pool->page_table, page_id);\n}\n```\nThis gives O(1) lookup for pages already in memory.\n### The LRU List\nWhen you need to evict a page, you need to know which page was used longest ago. A doubly-linked list maintains this ordering:\n- **Head**: Most recently used page\n- **Tail**: Least recently used page\nEvery time a page is accessed, it moves to the head. When eviction is necessary, the tail page is chosen.\n\n![LRU List Operations](./diagrams/diag-lru-list.svg)\n\n### Pin Counts: The Safety Mechanism\nA page with `pin_count > 0` cannot be evicted. The pin count represents \"this page is currently in use\":\n- B-tree operation reads a page \u2192 pin count +1\n- Cursor positioned on a page \u2192 pin count +1\n- Operation finishes \u2192 pin count -1\nThis prevents the catastrophic scenario of evicting a page that an active operation is still modifying.\n### Dirty Flag: The Write-Back Signal\nWhen a page is modified (insert, update, delete), the `is_dirty` flag is set. Before evicting a dirty page, you must write it to disk. Clean pages can be evicted immediately \u2014 their disk versions are identical.\n---\n## FetchPage: The Core Operation\nThe `FetchPage` function is the buffer pool's primary interface. Given a page ID, it returns a pointer to the page data, loading from disk if necessary.\n```c\nFrame* fetch_page(BufferPool* pool, PageId page_id) {\n    // Step 1: Check if page is already in memory\n    Frame* frame = lookup_frame(pool, page_id);\n    if (frame != NULL) {\n        // HIT: Page is already loaded\n        pool->hits++;\n        frame->pin_count++;\n        move_to_lru_head(pool, frame);\n        return frame;\n    }\n    // MISS: Page is not in memory\n    pool->misses++;\n    // Step 2: Find a frame to hold the page\n    frame = find_free_frame(pool);\n    if (frame == NULL) {\n        // No free frames \u2014 must evict\n        frame = evict_page(pool);\n        if (frame == NULL) {\n            // All pages are pinned \u2014 fatal error\n            return NULL;\n        }\n    }\n    // Step 3: Load page from disk\n    disk_read(pool->file_manager, page_id, frame->data);\n    // Step 4: Update frame metadata\n    frame->page_id = page_id;\n    frame->pin_count = 1;\n    frame->is_dirty = false;\n    // Step 5: Add to page table and LRU list\n    hash_table_insert(pool->page_table, page_id, frame);\n    add_to_lru_head(pool, frame);\n    return frame;\n}\n```\n### Hit Path (Fast)\nWhen the page is already in memory:\n1. Look up in hash table (O(1))\n2. Increment pin count\n3. Move to LRU head\n4. Return pointer\nThis path never touches disk and takes nanoseconds.\n### Miss Path (Slow)\nWhen the page is not in memory:\n1. Find a free frame OR evict a victim (may require disk write)\n2. Read page from disk (milliseconds)\n3. Update metadata and return\nThis path involves disk I/O and takes milliseconds.\n\n![FetchPage: Hit vs Miss Flow](./diagrams/diag-buffer-pool-flow.svg)\n\n---\n## LRU Eviction: Choosing the Victim\nWhen the buffer pool is full and you need to load a new page, you must evict an existing page. The **Least Recently Used (LRU)** algorithm chooses the page that was accessed longest ago.\n### Why LRU Works\nLRU exploits **temporal locality**: pages accessed recently are likely to be accessed again soon. Conversely, pages not accessed in a while are likely to remain unused. This isn't perfect, but it's effective for most database workloads.\n### The LRU List Operations\n```c\nvoid move_to_lru_head(BufferPool* pool, Frame* frame) {\n    // Remove from current position\n    if (frame->lru_prev != NULL) {\n        frame->lru_prev->lru_next = frame->lru_next;\n    }\n    if (frame->lru_next != NULL) {\n        frame->lru_next->lru_prev = frame->lru_prev;\n    }\n    if (pool->lru_tail == frame) {\n        pool->lru_tail = frame->lru_prev;\n    }\n    // Insert at head\n    frame->lru_prev = NULL;\n    frame->lru_next = pool->lru_head;\n    if (pool->lru_head != NULL) {\n        pool->lru_head->lru_prev = frame;\n    }\n    pool->lru_head = frame;\n    if (pool->lru_tail == NULL) {\n        pool->lru_tail = frame;\n    }\n}\nFrame* find_lru_victim(BufferPool* pool) {\n    Frame* victim = pool->lru_tail;\n    // Scan from tail to head, looking for unpinned page\n    while (victim != NULL) {\n        if (victim->pin_count == 0) {\n            return victim;\n        }\n        victim = victim->lru_prev;\n    }\n    // All pages are pinned\n    return NULL;\n}\n```\n### Eviction with Dirty Write-Back\n```c\nFrame* evict_page(BufferPool* pool) {\n    Frame* victim = find_lru_victim(pool);\n    if (victim == NULL) {\n        return NULL;  // All pages pinned\n    }\n    // If dirty, write to disk first\n    if (victim->is_dirty) {\n        disk_write(pool->file_manager, victim->page_id, victim->data);\n        victim->is_dirty = false;\n    }\n    // Remove from page table\n    hash_table_remove(pool->page_table, victim->page_id);\n    // Remove from LRU list\n    remove_from_lru_list(pool, victim);\n    // Frame is now free\n    victim->page_id = INVALID_PAGE_ID;\n    return victim;\n}\n```\nThe critical ordering: **write before evict**. If you evict first and then crash, the dirty page is lost forever.\n---\n## Pin and Unpin: Managing Active References\nThe pin count is the buffer pool's protection mechanism. A page with `pin_count > 0` cannot be evicted, even if it's at the LRU tail.\n### When to Pin\nPin a page whenever you need to guarantee it stays in memory:\n```c\n// B-tree operation reading a page\nFrame* frame = fetch_page(pool, page_id);\n// frame->pin_count is already 1 from fetch_page\n// ... perform operations on frame->data ...\n// If you need to hold the page across multiple operations:\npin_page(pool, frame);  // pin_count = 2\n```\n### When to Unpin\nUnpin when you're done with the page:\n```c\nvoid unpin_page(BufferPool* pool, Frame* frame) {\n    if (frame->pin_count > 0) {\n        frame->pin_count--;\n    } else {\n        // Error: unbalanced pin/unpin\n    }\n}\n```\n### The Pinning Protocol\nEvery `fetch_page` should have a matching `unpin_page`. Unbalanced pinning leads to:\n- **Under-pinning**: Page evicted while in use \u2192 use-after-free \u2192 corruption\n- **Over-pinning**: Page never evictable \u2192 memory leak \u2192 buffer pool fills up\n```c\n// CORRECT pattern\nFrame* frame = fetch_page(pool, page_id);\nif (frame == NULL) {\n    return ERROR_NO_MEMORY;\n}\n// ... use frame->data ...\nunpin_page(pool, frame);\n```\n### Handling the \"All Pinned\" Scenario\nIf every frame has `pin_count > 0`, eviction is impossible. This indicates a bug (unbalanced pin/unpin) or an under-provisioned buffer pool (too many concurrent operations for the available memory).\n```c\nFrame* fetch_page(BufferPool* pool, PageId page_id) {\n    // ... lookup ...\n    frame = find_free_frame(pool);\n    if (frame == NULL) {\n        frame = evict_page(pool);\n        if (frame == NULL) {\n            // All pages pinned \u2014 this is a fatal error\n            log_error(\"Buffer pool exhausted: all %d frames are pinned\", \n                      pool->frame_count);\n            return NULL;\n        }\n    }\n    // ... load page ...\n}\n```\nIn production systems, this might block until a page becomes available. For your implementation, returning NULL (error) is acceptable.\n---\n## Dirty Page Tracking and Flush\nWhen a page is modified, it becomes **dirty** \u2014 the in-memory version differs from the disk version. Dirty pages must eventually be written back to disk.\n### Marking Pages Dirty\n```c\nvoid mark_dirty(BufferPool* pool, Frame* frame) {\n    frame->is_dirty = true;\n}\n```\nThe B-tree layer calls `mark_dirty` after any insert, update, or delete operation:\n```c\nvoid btree_insert(BufferPool* pool, PageId page_id, Row* row) {\n    Frame* frame = fetch_page(pool, page_id);\n    // ... perform insertion into frame->data ...\n    mark_dirty(pool, frame);\n    unpin_page(pool, frame);\n}\n```\n### FlushAll: Committing Everything\nBefore shutdown or checkpoint, you need to ensure all dirty pages are on disk:\n```c\nvoid flush_all(BufferPool* pool) {\n    for (int i = 0; i < pool->frame_count; i++) {\n        Frame* frame = &pool->frames[i];\n        if (frame->page_id != INVALID_PAGE_ID && frame->is_dirty) {\n            disk_write(pool->file_manager, frame->page_id, frame->data);\n            frame->is_dirty = false;\n        }\n    }\n    // Ensure writes are durable\n    disk_sync(pool->file_manager);\n}\n```\nThe `disk_sync` (fsync) ensures the operating system's write cache is flushed to physical disk. Without it, the data might sit in the OS cache, vulnerable to power loss.\n### Selective Flush\nFor transaction commit, you might want to flush only specific pages:\n```c\nvoid flush_page(BufferPool* pool, Frame* frame) {\n    if (frame->is_dirty) {\n        disk_write(pool->file_manager, frame->page_id, frame->data);\n        frame->is_dirty = false;\n    }\n}\n```\nThis is used in write-ahead logging (WAL) mode, where only specific dirty pages need to reach disk at commit time.\n---\n## Hit Rate: Measuring Buffer Pool Effectiveness\nThe **hit rate** is the fraction of page requests served from memory:\n```\nhit_rate = hits / (hits + misses)\n```\nA well-tuned buffer pool achieves 95-99% hit rate on typical workloads. Lower hit rates indicate:\n- Buffer pool too small for working set\n- Poor access pattern (random access, no locality)\n- Inefficient eviction algorithm\n### Tracking Statistics\n```c\ntypedef struct {\n    int hits;\n    int misses;\n    int evictions;\n    int dirty_writes;\n} BufferPoolStats;\nBufferPoolStats get_stats(BufferPool* pool) {\n    BufferPoolStats stats;\n    stats.hits = pool->hits;\n    stats.misses = pool->misses;\n    stats.evictions = pool->evictions;\n    stats.dirty_writes = pool->dirty_writes;\n    return stats;\n}\ndouble get_hit_rate(BufferPool* pool) {\n    int total = pool->hits + pool->misses;\n    if (total == 0) return 0.0;\n    return (double)pool->hits / total;\n}\n```\n### Logging for Tuning\nPeriodically log the hit rate to identify when the buffer pool needs tuning:\n```\n[INFO] Buffer pool hit rate: 97.3% (hits=9730, misses=270, evictions=265)\n[INFO] Buffer pool hit rate: 94.1% (hits=9410, misses=590, evictions=585)  // Warning!\n```\nA sudden drop in hit rate might indicate:\n- New query touching cold data\n- Working set grew beyond buffer pool capacity\n- Data distribution changed (skewed access)\n---\n## The Three-Level View\n### Level 1: Application (Page Request \u2192 Page Pointer)\nAt the API level, the buffer pool is simple:\n```c\n// Fetch a page\nFrame* frame = fetch_page(pool, page_id);\nchar* page_data = frame->data;\n// ... use page_data ...\n// Release the page\nunpin_page(pool, frame);\n```\nThe caller doesn't know about LRU lists, hash tables, or disk I/O. They request a page, get a pointer, and release it when done.\n### Level 2: Engine (Hit/Miss Logic + Eviction)\nInside `fetch_page`, the buffer pool orchestrates:\n1. **Hash table lookup**: Is the page already loaded?\n2. **LRU management**: Update access order\n3. **Eviction decision**: Find victim if necessary\n4. **Disk I/O coordination**: Read/write as needed\n5. **Metadata tracking**: Pin counts, dirty flags\nThis is the \"brain\" of the buffer pool \u2014 the logic that decides what stays and what goes.\n### Level 3: Implementation (Memory + Pointers + Disk)\nAt the lowest level, the buffer pool is:\n- **Memory allocation**: A large contiguous buffer for frames\n- **Pointer manipulation**: LRU list threading, hash table chaining\n- **System calls**: `read()`, `write()`, `fsync()`\n```c\n// Frame array is a single allocation\npool->frames = malloc(pool->frame_count * sizeof(Frame));\n// Each frame's data is a fixed-size buffer within the struct\n// (Or could be a separate allocation for alignment)\n// Disk I/O wraps system calls\nvoid disk_read(FileManager* fm, PageId page_id, char* buffer) {\n    off_t offset = page_id * PAGE_SIZE;\n    pread(fm->fd, buffer, PAGE_SIZE, offset);\n}\n```\n---\n## Design Decisions: Why This, Not That\n### LRU vs CLOCK\n| Aspect | LRU (Chosen) | CLOCK |\n|--------|--------------|-------|\n| Accuracy | Exact: always evicts least recent | Approximate: may evict suboptimal page |\n| Overhead | O(1) per access (list manipulation) | O(1) per access (bit check) |\n| Memory | 2 pointers per frame | 1 bit per frame |\n| Implementation | Linked list, more complex | Circular scan, simpler |\n| Real systems | PostgreSQL, InnoDB | OS page cache, some DBs |\nLRU is more accurate for database workloads where access patterns are predictable. CLOCK is simpler and works well for general-purpose OS caching where patterns vary widely.\n### Fixed-Size vs Variable-Size Pages\n| Aspect | Fixed-Size (Chosen) | Variable-Size |\n|--------|---------------------|---------------|\n| Allocation | Simple: array of frames | Complex: memory management |\n| Fragmentation | None within pages | Possible |\n| Alignment | Guaranteed | Must manage manually |\n| Disk mapping | Direct: page N \u2192 offset N\u00d7size | Indirect: requires index |\n| Real systems | Virtually all databases | Memcached, some KV stores |\nDatabases use fixed-size pages because they map directly to disk blocks and simplify all aspects of memory management.\n### Write-Through vs Write-Back\n| Aspect | Write-Back (Chosen) | Write-Through |\n|--------|---------------------|---------------|\n| Performance | Fast: batch writes | Slow: write on every modification |\n| Complexity | Must track dirty pages | Simpler: no dirty tracking |\n| Crash risk | Lost updates if not flushed | No lost updates |\n| Real systems | All production databases | Simple caches, prototypes |\nWrite-back is essential for performance. The risk of lost updates is mitigated by WAL and checkpointing.\n---\n## Common Pitfalls\n### Forgetting to Unpin\n```c\n// WRONG: page never unpinned\nFrame* frame = fetch_page(pool, page_id);\nprocess_page(frame->data);\n// Missing: unpin_page(pool, frame);\n// After many such operations, all pages are pinned\n// Buffer pool can't evict anything \u2192 out of memory errors\n```\n**Fix**: Use RAII patterns or explicit cleanup sections:\n```c\nFrame* frame = fetch_page(pool, page_id);\nif (frame == NULL) return ERROR;\nint result = process_page(frame->data);\nunpin_page(pool, frame);\nreturn result;\n```\n### Evicting a Dirty Page Without Writing\n```c\n// WRONG: evict without checking dirty flag\nFrame* victim = find_lru_victim(pool);\nremove_from_page_table(pool, victim->page_id);\n// Missing: if (victim->is_dirty) disk_write(...);\n// The modifications in victim->data are lost forever\n```\n**Fix**: Always check and write dirty pages:\n```c\nif (victim->is_dirty) {\n    disk_write(pool->file_manager, victim->page_id, victim->data);\n}\n```\n### Not Calling fsync After Flush\n```c\n// WRONG: flush without sync\nflush_all(pool);\n// Missing: disk_sync(pool->file_manager);\n// Data is in OS cache, not on disk\n// Power loss = data loss\n```\n**Fix**: Always sync after flush:\n```c\nflush_all(pool);\ndisk_sync(pool->file_manager);\n```\n### Using Stale Pointers After Unpin\n```c\n// WRONG: using frame after unpin\nFrame* frame = fetch_page(pool, page_id);\nchar* data = frame->data;\nunpin_page(pool, frame);\n// ... later ...\nprocess(data);  // DANGER: frame may have been evicted!\n// data now points to freed or reused memory\n```\n**Fix**: Copy data if needed after unpin, or extend the pin lifetime:\n```c\nFrame* frame = fetch_page(pool, page_id);\nprocess(frame->data);  // Process while pinned\nunpin_page(pool, frame);  // Unpin after use\n```\n### Hash Table Collision Handling\nIf two page IDs hash to the same bucket, the page table must handle it. Using separate chaining (linked lists) or open addressing (linear probing) correctly is essential. A buggy hash table returns the wrong frame for a page ID, leading to subtle corruption.\n---\n## Testing Strategy\n### Hit/Miss Tests\n```c\nvoid test_fetch_page_hit() {\n    BufferPool* pool = create_pool(10);\n    // First fetch: miss, loads from disk\n    Frame* frame1 = fetch_page(pool, 42);\n    assert(pool->misses == 1);\n    assert(pool->hits == 0);\n    // Second fetch: hit, returns cached page\n    Frame* frame2 = fetch_page(pool, 42);\n    assert(pool->hits == 1);\n    assert(frame1 == frame2);  // Same frame\n    unpin_page(pool, frame1);\n    unpin_page(pool, frame2);\n    free_pool(pool);\n}\nvoid test_fetch_page_miss_eviction() {\n    BufferPool* pool = create_pool(3);  // Only 3 frames\n    // Fill the pool\n    Frame* f1 = fetch_page(pool, 1);\n    Frame* f2 = fetch_page(pool, 2);\n    Frame* f3 = fetch_page(pool, 3);\n    unpin_page(pool, f1);\n    unpin_page(pool, f2);\n    unpin_page(pool, f3);\n    // Fetch a new page \u2014 should evict page 1 (LRU)\n    Frame* f4 = fetch_page(pool, 4);\n    assert(pool->evictions == 1);\n    // Page 1 should no longer be in memory\n    Frame* f1_again = lookup_frame(pool, 1);\n    assert(f1_again == NULL);\n    unpin_page(pool, f4);\n    free_pool(pool);\n}\n```\n### LRU Tests\n```c\nvoid test_lru_ordering() {\n    BufferPool* pool = create_pool(3);\n    Frame* f1 = fetch_page(pool, 1);\n    Frame* f2 = fetch_page(pool, 2);\n    Frame* f3 = fetch_page(pool, 3);\n    // Access order: 1, 2, 3\n    // LRU order (tail to head): 1, 2, 3\n    unpin_page(pool, f1);\n    unpin_page(pool, f2);\n    unpin_page(pool, f3);\n    // Access page 1 again \u2014 moves to head\n    f1 = fetch_page(pool, 1);\n    unpin_page(pool, f1);\n    // LRU order now: 2, 3, 1\n    // Page 2 should be evicted next\n    Frame* victim = find_lru_victim(pool);\n    assert(victim->page_id == 2);\n    free_pool(pool);\n}\nvoid test_pinned_not_evicted() {\n    BufferPool* pool = create_pool(2);\n    Frame* f1 = fetch_page(pool, 1);\n    Frame* f2 = fetch_page(pool, 2);\n    unpin_page(pool, f2);\n    // f1 is still pinned\n    // Need to load page 3, but f1 is pinned\n    // Should evict f2 (unpinned), not f1\n    Frame* f3 = fetch_page(pool, 3);\n    assert(f3 != NULL);\n    assert(lookup_frame(pool, 1) != NULL);  // f1 still present\n    assert(lookup_frame(pool, 2) == NULL);  // f2 evicted\n    unpin_page(pool, f1);\n    unpin_page(pool, f3);\n    free_pool(pool);\n}\n```\n### Dirty Page Tests\n```c\nvoid test_dirty_page_write_back() {\n    BufferPool* pool = create_pool(2);\n    FileManager* fm = create_test_file_manager();\n    pool->file_manager = fm;\n    // Load and modify page 1\n    Frame* f1 = fetch_page(pool, 1);\n    f1->data[0] = 'X';\n    mark_dirty(pool, f1);\n    unpin_page(pool, f1);\n    // Load page 2 (no eviction needed yet)\n    Frame* f2 = fetch_page(pool, 2);\n    unpin_page(pool, f2);\n    // Load page 3 \u2014 evicts page 1, should write dirty page\n    Frame* f3 = fetch_page(pool, 3);\n    assert(pool->dirty_writes == 1);  // Dirty page was written\n    // Verify disk has the modification\n    char disk_data[PAGE_SIZE];\n    disk_read(fm, 1, disk_data);\n    assert(disk_data[0] == 'X');\n    unpin_page(pool, f3);\n    free_pool(pool);\n}\nvoid test_flush_all() {\n    BufferPool* pool = create_pool(10);\n    FileManager* fm = create_test_file_manager();\n    pool->file_manager = fm;\n    // Load and modify multiple pages\n    Frame* f1 = fetch_page(pool, 1);\n    f1->data[0] = 'A';\n    mark_dirty(pool, f1);\n    unpin_page(pool, f1);\n    Frame* f2 = fetch_page(pool, 2);\n    f2->data[0] = 'B';\n    mark_dirty(pool, f2);\n    unpin_page(pool, f2);\n    // Flush all\n    flush_all(pool);\n    // Both pages should be written\n    char disk_data[PAGE_SIZE];\n    disk_read(fm, 1, disk_data);\n    assert(disk_data[0] == 'A');\n    disk_read(fm, 2, disk_data);\n    assert(disk_data[0] == 'B');\n    // Dirty flags should be cleared\n    assert(f1->is_dirty == false);\n    assert(f2->is_dirty == false);\n    free_pool(pool);\n}\n```\n### Statistics Tests\n```c\nvoid test_hit_rate_calculation() {\n    BufferPool* pool = create_pool(10);\n    // 3 misses\n    fetch_page(pool, 1);\n    fetch_page(pool, 2);\n    fetch_page(pool, 3);\n    // 7 hits (re-access pages 1, 2, 3 multiple times)\n    fetch_page(pool, 1); fetch_page(pool, 1); fetch_page(pool, 1);\n    fetch_page(pool, 2); fetch_page(pool, 2);\n    fetch_page(pool, 3);\n    double hit_rate = get_hit_rate(pool);\n    assert(hit_rate == 7.0 / 10.0);  // 70%\n    // Unpin all\n    for (int i = 0; i < pool->frame_count; i++) {\n        while (pool->frames[i].pin_count > 0) {\n            unpin_page(pool, &pool->frames[i]);\n        }\n    }\n    free_pool(pool);\n}\n```\n---\n## Performance Considerations\n### LRU List Optimization\nMoving a frame to the LRU head requires pointer manipulation. For high-throughput systems, this can be a bottleneck. Optimizations:\n1. **Approximate LRU (CLOCK)**: Use a reference bit instead of exact ordering\n2. **LRU-K**: Track last K accesses, evict based on oldest K-th access\n3. **Segmented LRU**: Separate hot and cold regions\nFor your implementation, exact LRU is fine \u2014 the overhead is minimal compared to disk I/O.\n### Hash Table Sizing\nThe page table should be sized to minimize collisions:\n```c\n// Rule of thumb: hash table size = 2x frame count\npool->page_table = hash_table_create(pool->frame_count * 2);\n```\nA well-sized hash table gives O(1) average lookup. A poorly-sized one degrades to O(n).\n### Batch Operations\nWhen inserting many rows, the B-tree layer will fetch many pages. Batch optimizations:\n1. **Prefetch**: Anticipate needed pages and load them in parallel\n2. **Group commit**: Flush dirty pages in batches rather than individually\n3. **Background writer**: Continuously flush dirty pages to reduce eviction latency\n### Memory Alignment\nPage frames should be aligned to the system page size for optimal I/O:\n```c\n// POSIX aligned allocation\npool->frames = aligned_alloc(PAGE_SIZE, pool->frame_count * sizeof(Frame));\n```\nAligned buffers enable direct I/O (O_DIRECT) if you want to bypass the OS cache entirely.\n---\n## What You've Built\nYour buffer pool is a complete page cache:\n1. **Fixed-size frames** hold disk pages in memory\n2. **Hash table** enables O(1) page lookup\n3. **LRU eviction** selects the best victim when memory is full\n4. **Pin counts** protect pages in active use\n5. **Dirty tracking** ensures modifications reach disk\n6. **FlushAll** commits all dirty pages for shutdown/checkpoint\n7. **Hit rate statistics** enable performance tuning\nThis is the memory management layer that makes your database fast. Every B-tree operation goes through the buffer pool. A well-tuned buffer pool can serve 95%+ of requests from memory, making disk I/O the exception rather than the rule.\n---\n## Knowledge Cascade\n### 1. Redis Eviction Policies\nRedis faces the same problem: more data than memory. Its eviction policies are directly analogous:\n- **LRU**: Same as your buffer pool \u2014 evict least recently accessed\n- **LFU** (Least Frequently Used): Track access frequency, not recency\n- **TTL-based**: Evict keys closest to expiration\nThe algorithms are identical; only the \"unit of eviction\" differs (pages vs. keys).\n### 2. Operating System Page Cache\nThe OS page cache IS a buffer pool. The kernel maintains:\n- A set of page frames (physical memory)\n- A page table mapping (virtual \u2192 physical)\n- LRU/CLOCK eviction\n- Dirty page tracking (write-back)\nThe difference: the OS doesn't know your transaction boundaries. Your buffer pool does.\n### 3. CDN Cache Design\nContent Delivery Networks cache HTTP responses. They face:\n- **Capacity limits**: Can't cache the entire web\n- **Eviction decisions**: LRU, LFU, or size-based\n- **Staleness**: Cached content may be outdated (analogous to dirty pages)\n- **Pinning**: \"Sticky\" content that should never be evicted\nThe buffer pool pattern is universal: limited fast storage, unlimited slow storage, and intelligent mediation between them.\n### 4. Game Engine Asset Streaming\nOpen-world games stream textures and geometry from disk to GPU memory:\n- **Pin count**: An asset in use by the renderer can't be evicted\n- **LRU**: Distant/unseen assets are evicted first\n- **Prefetch**: Anticipate player movement, load assets proactively\nThe pin/unpin pattern is identical \u2014 the GPU can't render an evicted texture.\n### 5. Browser Cache Eviction\nYour browser's HTTP cache uses the same logic:\n- **Capacity limit**: Disk space is finite\n- **LRU eviction**: Old responses are purged\n- **Validation**: Check if cached response is still fresh (analogous to dirty page write-back)\n- **Pinning**: Pinned sites (frequently visited) are preserved\nEvery caching system reinvents the buffer pool. Now you understand the pattern at its core.\n---\n## What's Next\nYour buffer pool manages pages in memory, but what's *in* those pages? The next milestone, **B-tree Page Format & Table Storage**, defines how rows are serialized into bytes, how variable-length records fit into fixed-size pages, and how B-tree nodes are structured for efficient traversal.\nThe buffer pool is the memory. The page format is the language spoken within it.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m4\", \"criteria\": [\"Buffer pool initializes with a configurable number of fixed-size frames (default 1000 frames, 4096 bytes per frame)\", \"Frame structure contains page_id (identifying which disk page is loaded), data buffer (PAGE_SIZE bytes), pin_count (integer \u2265 0), is_dirty flag (boolean), and LRU list pointers\", \"Page table (hash map) provides O(1) lookup from PageId to Frame pointer for fast cache hit detection\", \"FetchPage operation checks page table first: if page is resident, increment pin_count, move to LRU head, increment hit counter, and return frame (cache hit path)\", \"FetchPage on cache miss finds a free frame (or evicts a victim), reads page from disk into frame, initializes metadata (pin_count=1, is_dirty=false), adds to page table and LRU head, increments miss counter, and returns frame\", \"LRU eviction scans from tail of LRU list to find the first unpinned page (pin_count == 0)\", \"Eviction of a dirty page writes the page to disk BEFORE removing from page table (write-back ordering guarantee)\", \"Pin count mechanism prevents eviction: pages with pin_count > 0 are never selected as eviction victims\", \"Unpin operation decrements pin_count; error is reported if pin_count would go negative (unbalanced pin/unpin)\", \"MarkDirty function sets the is_dirty flag; B-tree layer calls this after any insert/update/delete operation\", \"FlushAll iterates all frames and writes dirty pages to disk, then calls fsync to ensure durability, and clears is_dirty flags\", \"Hit rate is calculated as hits / (hits + misses) and is accessible for performance monitoring\", \"Statistics tracking includes: hit count, miss count, eviction count, dirty write count\", \"All-pinned scenario is handled gracefully: eviction returns NULL when no unpinned pages exist, and FetchPage returns NULL (error) rather than crashing\", \"Test suite validates cache hit returns same frame on repeated FetchPage calls\", \"Test suite validates cache miss loads from disk and increments miss counter\", \"Test suite validates LRU ordering: least recently accessed unpinned page is evicted first\", \"Test suite validates pinned pages are never evicted even when at LRU tail\", \"Test suite validates dirty pages are written to disk before eviction\", \"Test suite validates FlushAll writes all dirty pages and clears dirty flags\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m5 -->\n# B-tree Page Format & Table Storage: Where Bytes Become Meaning\nYour buffer pool hands you 4096 bytes of raw memory. It knows this is \"page 42\" \u2014 but what does that *mean*? What lives at byte 0? Where does the first row start? How do you find row 17? How do you know if this page is a leaf or an internal node?\nThe buffer pool manages *frames*. The page format gives those frames *structure*.\nThis is where your database confronts a fundamental constraint: **disk doesn't understand rows.** Disk understands bytes in fixed-size blocks. Your rows are variable-length: an INTEGER is 8 bytes, a TEXT might be 3 characters or 3000. You need to pack these irregular shapes into regular containers, find them again, modify them, and maintain the B-tree structure that enables O(log n) lookups.\nThe page format is the translation layer between logical rows and physical bytes.\n---\n## The Tension: Variable Rows in Fixed Pages\nHere's the core problem. Your database stores rows like:\n```sql\nINSERT INTO users VALUES (1, 'Alice', 30);\nINSERT INTO users VALUES (2, 'Bob', 25);\nINSERT INTO users VALUES (3, 'Charlotte', 28);\n```\nRow 1: 8 bytes (INTEGER) + 5 bytes ('Alice') + 8 bytes (INTEGER) = 21 bytes\nRow 2: 8 bytes (INTEGER) + 3 bytes ('Bob') + 8 bytes (INTEGER) = 19 bytes\nRow 3: 8 bytes (INTEGER) + 9 bytes ('Charlotte') + 8 bytes (INTEGER) = 25 bytes\nThese rows have **different sizes**. You can't just say \"row 3 starts at offset 40\" because the offset depends on how big rows 1 and 2 are.\nNow consider what happens when you update row 2:\n```sql\nUPDATE users SET name = 'Robert' WHERE id = 2;\n```\n'Bob' is 3 bytes. 'Robert' is 6 bytes. The new row doesn't fit in the old slot. You'd have to shift everything after it \u2014 potentially thousands of rows across multiple pages.\nAnd then there's the B-tree constraint. When you insert a row, it must go into the correct leaf page based on its rowid (for tables) or key value (for indexes). If that leaf page is full, you **must split it** \u2014 there's no \"insert somewhere else\" option.\n\n![B-tree Page Format: Slotted Page](./diagrams/diag-page-layout.svg)\n\nThe tension: **you need variable-length records, efficient updates, and B-tree structural integrity \u2014 all within fixed-size pages that must serialize to exactly 4096 bytes.**\n---\n## The Revelation: B-tree \u2260 B+tree\nHere's a misconception that will silently corrupt your database: *B-tree is B-tree \u2014 tables and indexes use the same structure, just with different keys.*\n**No.** SQLite uses **two different tree structures** for two different access patterns.\n### Table B-trees: Data Everywhere\nA table B-tree stores **complete row data** in every node \u2014 leaves and internal nodes both contain rows. The tree is keyed by `rowid` (the implicit INTEGER PRIMARY KEY).\n```\nTable B-tree (users table):\n                    [rowid=50, full row data]\n                           /    \\\n          [rowid=25, full row]   [rowid=75, full row]\n              /         \\              /        \\\n    [rowid=10]    [rowid=40]    [rowid=60]    [rowid=90]\n```\nWhen you query `SELECT * FROM users WHERE id = 60`, the B-tree traversal finds the node containing rowid 60 \u2014 and the complete row is right there. No extra lookup needed.\n### Index B+trees: Data Only in Leaves\nAn index B+tree stores **only keys and rowids** in internal nodes \u2014 no row data. The leaves contain (key, rowid) pairs and are linked together for range scans.\n{{DIAGRAM:diag-btree-vs-bplustree}}\n```\nIndex B+tree (index on users.name):\n                    [separator: 'M']\n                       /        \\\n          ['D']              ['T']\n          /    \\              /    \\\n    ['Alice',1] ['Bob',2]  ['Sam',4] ['Zoe',5]\n         \u2193___________\u2193___________\u2193__________\u2193\n              (linked leaves)\n```\nWhen you query `SELECT * FROM users WHERE name = 'Bob'`, the B+tree traversal finds the leaf containing ('Bob', 2). The `2` is the rowid \u2014 now you must look up rowid 2 in the *table* B-tree to get the full row. This is the **double lookup** pattern.\n### Why This Difference Matters\n**Point lookups by rowid**: Table B-tree is perfect. You traverse O(log n) nodes and find the complete row at the destination. If internal nodes stored only keys, you'd have to traverse to the leaf *every time* \u2014 even when the row you want is in an internal node.\n**Range scans on indexed column**: B+tree is perfect. The leaves are linked, so once you find 'Bob', you can scan forward to 'Charlie', 'David', etc. without climbing back up the tree. And internal nodes don't waste space storing row data \u2014 they maximize fan-out, reducing tree height.\nThis isn't arbitrary. The page format **encodes this difference in the header byte**:\n```c\n// Page type byte (first byte of page)\n#define BTREE_PAGE_TYPE_TABLE_LEAF      0x0D\n#define BTREE_PAGE_TYPE_TABLE_INTERNAL  0x05\n#define BTREE_PAGE_TYPE_INDEX_LEAF      0x0A\n#define BTREE_PAGE_TYPE_INDEX_INTERNAL  0x02\n```\nWhen your code reads a page, the first byte tells it exactly what to expect.\n---\n## The Slotted Page: Bidirectional Growth\nYou have 4096 bytes. Rows have variable sizes. Rows get inserted, deleted, and updated. How do you organize this chaos?\nThe answer is the **slotted page** format \u2014 used by PostgreSQL, MySQL, Oracle, and SQLite.\n\n![B-tree Page Format: Slotted Page](./diagrams/diag-page-layout.svg)\n\n### Page Structure\n```\n+------------------+  offset 0\n| Page Header      |  (8-12 bytes: type, cell count, free space, etc.)\n+------------------+\n| Cell Pointer 0   |  2 bytes: offset to cell 0\n| Cell Pointer 1   |  2 bytes: offset to cell 1\n| Cell Pointer 2   |  2 bytes: offset to cell 2\n| ...              |\n+------------------+  offset = header_size + 2 * cell_count\n|                  |\n| FREE SPACE       |  grows toward middle\n|                  |\n+------------------+\n| Cell N           |  variable size, grows from end\n| ...              |\n| Cell 1           |\n| Cell 0           |\n+------------------+  offset 4095\n```\nThe page grows in **two directions**:\n- **Cell pointers** grow from the beginning, after the header\n- **Cell content** grows from the end, toward the beginning\nThe middle is free space. As you add cells, the pointer array grows down and the cell content grows up. When they meet, the page is full.\n### Why This Works\n**Insert**: Add a cell pointer at the end of the pointer array, write the cell at the end of the page, update free space pointer. O(1) pointer manipulation.\n**Delete**: Clear the cell pointer (or mark it deleted), update free space pointer. The cell's space becomes a gap, but you can compact later.\n**Variable size**: Each cell can be any size. The pointer array gives you random access by cell index, regardless of cell sizes.\n**Compact**: Move cells to eliminate gaps, update pointers. This is the only time you touch the cell data itself.\n### The Page Header\nEvery page starts with a header that identifies its type and state:\n```c\ntypedef struct {\n    uint8_t page_type;        // 0x0D, 0x05, 0x0A, or 0x02\n    uint16_t first_freeblock; // offset to first free block, or 0\n    uint16_t cell_count;      // number of cells on this page\n    uint16_t cell_content_start; // offset where cell content begins\n    uint8_t fragmented_bytes; // bytes in fragmented free space\n    // For internal pages only:\n    uint32_t right_child;     // rightmost child page number\n} BTreePageHeader;\n```\nThe header is 8 bytes for leaf pages, 12 bytes for internal pages (the right_child field).\n### Cell Pointer Array\nImmediately after the header, an array of 2-byte offsets points to each cell:\n```c\nuint16_t* cell_pointers = (uint16_t*)(page_data + header_size);\n// Cell i is at offset cell_pointers[i] from page start\n```\nThis is why cell access is O(1): you index into an array, read an offset, and jump directly to the cell.\n---\n## Varint: Variable-Length Integers\nBefore we can talk about cell contents, we need to discuss how integers are stored. SQLite uses **variable-length integers** (varints) to save space.\nConsider rowids. Most tables have fewer than 16 million rows, which fits in 24 bits. But rowids *can* be 64-bit integers. Storing all 8 bytes for every rowid would waste 5 bytes per row on average.\nVarints solve this: use 1 byte for small values, up to 9 bytes for the largest.\n\n![Variable-Length Integer Encoding](./diagrams/diag-varint-encoding.svg)\n\n### Varint Encoding\n```\nValue Range              | Bytes | Encoding\n-------------------------|-------|----------------------------------\n0 to 127                 | 1     | 0xxxxxxx\n128 to 16,383            | 2     | 1xxxxxxx 0xxxxxxx\n16,384 to 2,097,151      | 3     | 1xxxxxxx 1xxxxxxx 0xxxxxxx\n...                      | ...   | ...\n2^56 to 2^64-1           | 9     | 1xxxxxxx ... 1xxxxxxx xxxxxxxx\n```\nThe high bit of each byte indicates continuation:\n- `0` = last byte of this varint\n- `1` = more bytes follow\n```c\n// Read a varint from buffer, return value and bytes consumed\nint read_varint(const uint8_t* buf, uint64_t* value) {\n    uint64_t result = 0;\n    int bytes_read = 0;\n    for (int i = 0; i < 9; i++) {\n        uint8_t byte = buf[i];\n        bytes_read++;\n        if (i < 8) {\n            result = (result << 7) | (byte & 0x7F);\n            if ((byte & 0x80) == 0) {\n                break;  // Last byte\n            }\n        } else {\n            // 9th byte: use all 8 bits\n            result = (result << 8) | byte;\n        }\n    }\n    *value = result;\n    return bytes_read;\n}\n// Write a varint to buffer, return bytes written\nint write_varint(uint8_t* buf, uint64_t value) {\n    if (value <= 127) {\n        buf[0] = (uint8_t)value;\n        return 1;\n    }\n    // Calculate how many bytes we need\n    int bytes_needed = 0;\n    uint64_t temp = value;\n    while (temp > 0) {\n        bytes_needed++;\n        temp >>= (bytes_needed < 9) ? 7 : 8;\n    }\n    // Write bytes from most significant to least\n    for (int i = bytes_needed - 1; i >= 0; i--) {\n        if (i > 0) {\n            buf[bytes_needed - 1 - i] = ((value >> (7 * i)) & 0x7F) | 0x80;\n        } else {\n            buf[bytes_needed - 1] = value & 0x7F;\n        }\n    }\n    return bytes_needed;\n}\n```\n### Why 9 Bytes for 64 Bits?\nThe first 8 bytes contribute 7 bits each (56 bits total). The 9th byte contributes 8 bits. Together: 56 + 8 = 64 bits.\nThis is the tradeoff: we sacrifice 1 bit per byte (the continuation bit) to achieve variable length. For values 0-127, we save 7 bytes compared to a fixed 8-byte encoding.\n---\n## Row Serialization: The Record Format\nA row is a sequence of column values. To store it in a page, you serialize it to bytes. SQLite uses a **type-length-value** encoding.\n\n![Row Record Format](./diagrams/diag-row-serialization.svg)\n\n### Record Header\nEvery record starts with a header that describes the types and sizes of each column:\n```\n+----------------+\n| Header Size    |  varint: total bytes in header (including this size)\n+----------------+\n| Serial Type 0  |  varint: type of column 0\n| Serial Type 1  |  varint: type of column 1\n| ...            |\n+----------------+\n| Column 0 Data  |  (size determined by serial type)\n| Column 1 Data  |\n| ...            |\n+----------------+\n```\n### Serial Types\nThe serial type encodes both the type and size:\n| Serial Type | Meaning | Size |\n|-------------|---------|------|\n| 0 | NULL | 0 bytes |\n| 1 | 8-bit signed integer | 1 byte |\n| 2 | 16-bit big-endian signed integer | 2 bytes |\n| 3 | 24-bit big-endian signed integer | 3 bytes |\n| 4 | 32-bit big-endian signed integer | 4 bytes |\n| 5 | 48-bit big-endian signed integer | 6 bytes |\n| 6 | 64-bit big-endian signed integer | 8 bytes |\n| 7 | IEEE 754 64-bit float | 8 bytes |\n| 8 | Integer 0 | 0 bytes |\n| 9 | Integer 1 | 0 bytes |\n| 10, 11 | Reserved | - |\n| N \u2265 12, even | BLOB of (N-12)/2 bytes | (N-12)/2 bytes |\n| N \u2265 13, odd | TEXT of (N-13)/2 bytes | (N-13)/2 bytes |\nThe clever trick: for BLOB and TEXT, the serial type encodes the length. A 5-byte string has serial type 13 + 5*2 = 23. Decoding: `(23 - 13) / 2 = 5` bytes.\n```c\ntypedef struct {\n    ValueType type;\n    union {\n        int64_t integer_val;\n        double float_val;\n        struct {\n            char* data;\n            size_t length;\n        } string_val;\n        struct {\n            uint8_t* data;\n            size_t length;\n        } blob_val;\n    } data;\n} ColumnValue;\nint encode_serial_type(ColumnValue* val) {\n    switch (val->type) {\n        case VALUE_NULL:\n            return 0;\n        case VALUE_INTEGER:\n            if (val->data.integer_val == 0) return 8;\n            if (val->data.integer_val == 1) return 9;\n            if (val->data.integer_val >= -128 && val->data.integer_val <= 127) return 1;\n            if (val->data.integer_val >= -32768 && val->data.integer_val <= 32767) return 2;\n            if (val->data.integer_val >= -8388608 && val->data.integer_val <= 8388607) return 3;\n            if (val->data.integer_val >= -2147483648 && val->data.integer_val <= 2147483647) return 4;\n            if (val->data.integer_val >= -140737488355328LL && val->data.integer_val <= 140737488355327LL) return 5;\n            return 6;\n        case VALUE_FLOAT:\n            return 7;\n        case VALUE_TEXT:\n            return 13 + (int)val->data.string_val.length * 2;\n        case VALUE_BLOB:\n            return 12 + (int)val->data.blob_val.length * 2;\n    }\n    return 0;\n}\n```\n### Row Serialization Example\n```sql\nINSERT INTO users VALUES (42, 'Alice', 30);\n```\nColumn values:\n- id: INTEGER 42 \u2192 serial type 1 (1 byte)\n- name: TEXT 'Alice' (5 bytes) \u2192 serial type 13 + 5*2 = 23\n- age: INTEGER 30 \u2192 serial type 1 (1 byte)\nHeader:\n- Header size: 1 (size) + 1 (serial type for 42) + 1 (serial type for 'Alice') + 1 (serial type for 30) = 4 bytes\n- Serial types: [1, 23, 1]\nData:\n- id: 0x2A (42 in 1 byte)\n- name: 'Alice' (5 bytes: 0x41 0x6C 0x69 0x63 0x65)\n- age: 0x1E (30 in 1 byte)\nComplete record:\n```\n04 01 17 01  2A  41 6C 69 63 65  1E\n\u2502  \u2502  \u2502  \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502  \u2502  \u2502  \u2502   \u2502       name       age\n\u2502  \u2502  \u2502  \u2502   id\n\u2502  \u2502  \u2502  \u2514\u2500 age serial type (1)\n\u2502  \u2502  \u2514\u2500 name serial type (23)\n\u2502  \u2514\u2500 id serial type (1)\n\u2514\u2500 header size (4)\n```\nTotal: 11 bytes for a row with 3 columns. The varint encoding and type-specific sizes minimize wasted space.\n---\n## Cell Format: Different for Each Page Type\nNow we can talk about cells \u2014 the actual entries stored in B-tree pages. The format differs by page type.\n### Table B-tree Leaf Cell\nStores a complete row, keyed by rowid:\n```\n+----------------+\n| Payload Size   |  varint: total bytes of payload\n+----------------+\n| Rowid          |  varint: the rowid (primary key)\n+----------------+\n| Payload        |  (payload_size bytes): the serialized row\n+----------------+\n| Overflow Page  |  (optional, 4 bytes): if payload doesn't fit\n+----------------+\n```\nThe payload is the serialized record (header + column data). If it fits in the page, great. If not, it spills to overflow pages.\n### Table B-tree Internal Cell\nStores a separator key and child pointer:\n```\n+----------------+\n| Left Child     |  4 bytes: page number of left child subtree\n+----------------+\n| Rowid          |  varint: separator key (largest rowid in left subtree)\n+----------------+\n```\nInternal nodes don't store row data \u2014 they guide the search to the correct child page.\n### Index B+tree Leaf Cell\nStores (key, rowid) pair:\n```\n+----------------+\n| Payload Size   |  varint: size of key + rowid\n+----------------+\n| Payload        |  serialized key columns + rowid at the end\n+----------------+\n```\nThe payload includes the indexed column values (serialized) followed by the rowid. No separate rowid field \u2014 it's part of the payload.\n### Index B+tree Internal Cell\nStores separator key and child pointer:\n```\n+----------------+\n| Left Child     |  4 bytes: page number of left child subtree\n+----------------+\n| Payload Size   |  varint: size of separator key\n+----------------+\n| Payload        |  serialized separator key (no rowid)\n+----------------+\n```\nInternal index cells don't include rowid \u2014 just the key values needed to guide the search.\n---\n## Node Splitting: Maintaining Balance\nWhen a page is full and you need to insert, you **split** the page: move half the cells to a new page, and insert a separator key into the parent.\n\n![B-tree Node Split Sequence](./diagrams/diag-node-split.svg)\n\n### The Split Algorithm\n```\n1. Allocate a new page (sibling)\n2. Copy upper half of cells from original to sibling\n3. Update cell counts in both pages\n4. If original is a leaf:\n   - Insert new cell into appropriate page\n   - Copy up the separator key (first key of sibling)\n5. If original is internal:\n   - The middle key goes up to parent\n   - Remaining keys are split between children\n6. Insert separator into parent:\n   - If parent is full, recursively split parent\n   - If parent doesn't exist (root split), create new root\n```\n### Root Split: The Special Case\nWhen the root page splits, you can't \"insert into parent\" \u2014 the root has no parent. Instead:\n1. Allocate a new page (will become left child)\n2. Copy all cells from root to new page\n3. Clear root, set as internal page\n4. Insert left child pointer into root\n5. Split the new page (which is now a child of root)\n6. Root now has 2 children, tree height increased by 1\nThe root page number never changes \u2014 only its contents and type change from leaf to internal.\n```c\nvoid split_leaf_page(BufferPool* pool, PageId page_id, Cell* new_cell) {\n    Frame* frame = fetch_page(pool, page_id);\n    BTreePage* page = (BTreePage*)frame->data;\n    // Allocate sibling\n    PageId sibling_id = allocate_page(pool);\n    Frame* sibling_frame = fetch_page(pool, sibling_id);\n    BTreePage* sibling = (BTreePage*)sibling_frame->data;\n    sibling->page_type = page->page_type;\n    // Collect all cells including new one\n    Cell* cells[MAX_CELLS];\n    int cell_count = collect_cells(page, cells);\n    insert_cell_sorted(cells, &cell_count, new_cell);\n    // Split point: roughly half\n    int split_point = cell_count / 2;\n    // Distribute cells\n    clear_cells(page);\n    for (int i = 0; i < split_point; i++) {\n        add_cell(page, cells[i]);\n    }\n    for (int i = split_point; i < cell_count; i++) {\n        add_cell(sibling, cells[i]);\n    }\n    // Separator key: first key of sibling\n    int64_t separator_key = get_first_key(sibling);\n    // Insert separator into parent\n    if (is_root_page(page_id)) {\n        // Root split: create new root\n        create_new_root(pool, page_id, sibling_id, separator_key);\n    } else {\n        // Insert into existing parent\n        insert_into_parent(pool, page_id, sibling_id, separator_key);\n    }\n    mark_dirty(pool, frame);\n    mark_dirty(pool, sibling_frame);\n    unpin_page(pool, frame);\n    unpin_page(pool, sibling_frame);\n}\n```\n### Maintaining Order\nAfter a split, the B-tree property must hold: all keys in the left subtree are less than the separator, and all keys in the right subtree are greater than or equal to the separator.\nFor table B-trees, the separator is the **first rowid** in the right (sibling) page. For index B+trees, the separator is the **first key** in the sibling.\n---\n## The System Catalog: sqlite_master\nTables don't magically exist \u2014 they're stored as rows in a special table called the system catalog. SQLite calls it `sqlite_master`.\n\n![Cursor-Based Table Scan](./diagrams/diag-table-scan-cursor.svg)\n\n### Schema of sqlite_master\n```sql\nCREATE TABLE sqlite_master (\n    type TEXT,        -- 'table', 'index', 'trigger', 'view'\n    name TEXT,        -- object name\n    tbl_name TEXT,    -- table name (for indexes, triggers)\n    rootpage INTEGER, -- root page number of B-tree\n    sql TEXT          -- CREATE statement that created this object\n);\n```\nWhen you execute `CREATE TABLE users (...)`, your database:\n1. Allocates a new page (page N) for the users table's root\n2. Inserts a row into sqlite_master:\n   ```\n   type='table', name='users', tbl_name='users', rootpage=N, sql='CREATE TABLE...'\n   ```\n3. The sqlite_master table itself has a known root page (always page 1)\n### Lookup During Query Execution\nWhen the parser sees `SELECT * FROM users`, it needs to find the table's root page:\n```c\nTable* lookup_table(Database* db, const char* name) {\n    // Query sqlite_master for this table\n    const char* sql = \"SELECT rootpage, sql FROM sqlite_master WHERE type='table' AND name=?\";\n    PreparedStatement* stmt = prepare(db, sql);\n    bind_text(stmt, 1, name);\n    execute(stmt);\n    if (!has_result(stmt)) {\n        return NULL;  // Table doesn't exist\n    }\n    int rootpage = get_int(stmt, 0);\n    const char* create_sql = get_text(stmt, 1);\n    // Parse CREATE statement to get column definitions\n    Table* table = parse_create_statement(create_sql);\n    table->root_page = rootpage;\n    return table;\n}\n```\nThe first page of the database file is always the sqlite_master root page. This is how you bootstrap the entire schema lookup system.\n---\n## Full Table Scan: Walking the Leaves\nA full table scan reads every row in a table, in rowid order. For a table B-tree, this means traversing all leaf pages.\n\n![Cursor-Based Table Scan](./diagrams/diag-table-scan-cursor.svg)\n\n### The Scan Algorithm\n```c\nvoid full_table_scan(Database* db, Table* table, RowCallback callback) {\n    BufferPool* pool = db->buffer_pool;\n    PageId current_page = table->root_page;\n    // First, find the leftmost leaf\n    while (true) {\n        Frame* frame = fetch_page(pool, current_page);\n        BTreePage* page = (BTreePage*)frame->data;\n        if (is_leaf_page(page)) {\n            // Found leftmost leaf\n            unpin_page(pool, frame);\n            break;\n        }\n        // Internal page: follow leftmost child\n        uint32_t leftmost_child = get_leftmost_child(page);\n        unpin_page(pool, frame);\n        current_page = leftmost_child;\n    }\n    // Now scan all leaves, following right-sibling pointers\n    current_page = leftmost_leaf;\n    while (current_page != 0) {\n        Frame* frame = fetch_page(pool, current_page);\n        BTreePage* page = (BTreePage*)frame->data;\n        // Read all cells in this leaf\n        for (int i = 0; i < page->cell_count; i++) {\n            Cell* cell = get_cell(page, i);\n            Row row;\n            deserialize_row(cell->payload, &row);\n            callback(&row);\n            free_row(&row);\n        }\n        // Move to right sibling (stored in page header for leaves)\n        PageId next_page = page->right_sibling;\n        unpin_page(pool, frame);\n        current_page = next_page;\n    }\n}\n```\n### Right-Sibling Pointers\nTable B-tree leaves have a `right_sibling` field in the header that points to the next leaf in rowid order. This enables efficient sequential scans without climbing the tree.\nFor index B+trees, the leaves are **doubly-linked** (both left and right sibling pointers), enabling both forward and backward range scans.\n---\n## Endianness: A Critical Detail\nThe SQLite file format uses **big-endian** encoding for all multi-byte integers in the page format (page numbers, cell pointers, etc.). This is for portability \u2014 a database file created on a little-endian x86 system can be read on a big-endian ARM system.\nHowever, the record payload (column values) uses **native byte order** for integers, which means it's typically little-endian on modern systems.\n```c\n// Reading a 4-byte page number (big-endian)\nuint32_t read_page_number(const uint8_t* buf) {\n    return ((uint32_t)buf[0] << 24) |\n           ((uint32_t)buf[1] << 16) |\n           ((uint32_t)buf[2] << 8) |\n           ((uint32_t)buf[3]);\n}\n// Writing a 4-byte page number (big-endian)\nvoid write_page_number(uint8_t* buf, uint32_t value) {\n    buf[0] = (value >> 24) & 0xFF;\n    buf[1] = (value >> 16) & 0xFF;\n    buf[2] = (value >> 8) & 0xFF;\n    buf[3] = value & 0xFF;\n}\n```\nGet this wrong, and your database files will be unreadable on different architectures \u2014 or worse, silently corrupted.\n---\n## The Three-Level View\n### Level 1: Application (Row \u2192 Stored \u2192 Retrieved)\nAt the API level, storage is invisible:\n```sql\nINSERT INTO users VALUES (1, 'Alice', 30);\nSELECT * FROM users WHERE id = 1;\n```\nThe user provides a row, gets a row back. They don't know about pages, cells, varints, or B-tree splits.\n### Level 2: Engine (Serialization + B-tree Operations)\nInside the storage engine, each operation translates to specific page manipulations:\n```\nINSERT:\n1. Serialize row to bytes (record format)\n2. Build cell (payload size + rowid + payload)\n3. Find target leaf page (B-tree traversal)\n4. If page has space: insert cell, update pointers\n5. If page full: split page, insert separator in parent\nSELECT:\n1. Traverse B-tree from root to leaf (comparing rowids)\n2. Read cell from leaf page\n3. Deserialize payload to row\n4. Return row\n```\n### Level 3: Implementation (Bytes, Offsets, Pointers)\nAt the lowest level, every operation is byte manipulation:\n```c\n// Reading a cell pointer\nuint16_t cell_ptr = *(uint16_t*)(page + HEADER_SIZE + cell_index * 2);\n// Reading a varint from cell\nuint64_t rowid;\nint varint_len = read_varint(page + cell_ptr + payload_size_offset, &rowid);\n// Following a child pointer\nuint32_t child_page = read_page_number(page + cell_ptr);\n```\nThe correctness of your database depends on every byte being in the right place.\n---\n## Common Pitfalls\n### Forgetting to Update Both Pages During Split\nWhen splitting a page, you modify both the original and the sibling. If you crash between the two writes, the tree is corrupted.\n**Solution**: Use write-ahead logging (WAL) to ensure atomicity of multi-page modifications. The WAL milestone will cover this.\n### Overflow Pages Not Handled\nIf a row is larger than a page (e.g., a 10KB TEXT column), it won't fit in a leaf cell. You need overflow pages.\n**Solution**: Check payload size before inserting. If too large, allocate overflow pages and store a pointer in the cell.\n```c\n#define USABLE_SPACE(page_size) ((page_size) - HEADER_SIZE - 2)  // Approximate\nif (payload_size > USABLE_SPACE(page_size)) {\n    // Allocate overflow pages\n    int overflow_pages_needed = (payload_size + PAGE_SIZE - 1) / PAGE_SIZE;\n    PageId first_overflow = allocate_overflow_chain(pool, payload, payload_size);\n    cell.overflow_page = first_overflow;\n    cell.local_payload_size = calculate_local_payload(page_size);\n}\n```\n### Varint Encoding Edge Cases\nThe varint encoding has edge cases:\n- Value 0 and 1 have special serial types (8 and 9) with 0 bytes of data\n- The 9th byte of a varint uses all 8 bits (no continuation bit)\n- Negative integers need careful handling (sign extension)\nTest your varint implementation with: 0, 1, 127, 128, 16383, 16384, 2^56-1, 2^63-1.\n### Page Fragmentation After Deletes\nDeleting a cell leaves a gap in the cell content area. The free space becomes fragmented.\n**Solution**: Periodic compaction (during vacuum or when fragmentation exceeds a threshold). Or use a freeblock list within the page.\n```c\nvoid compact_page(BTreePage* page) {\n    uint8_t temp[PAGE_SIZE];\n    int offset = PAGE_SIZE;\n    // Copy cells to temp buffer, compacted\n    for (int i = 0; i < page->cell_count; i++) {\n        Cell* cell = get_cell(page, i);\n        int cell_size = cell_total_size(cell);\n        offset -= cell_size;\n        memcpy(temp + offset, cell, cell_size);\n        page->cell_pointers[i] = offset;  // Update pointer\n    }\n    // Copy back to page\n    memcpy(page + page->cell_content_start, \n           temp + offset, \n           PAGE_SIZE - offset);\n    page->cell_content_start = offset;\n    page->fragmented_bytes = 0;\n}\n```\n---\n## Testing Strategy\n### Varint Tests\n```c\nvoid test_varint_roundtrip() {\n    uint64_t test_values[] = {0, 1, 127, 128, 16383, 16384, \n                               2097151, 2097152, \n                               0xFFFFFFFFFFFFFFFFULL};\n    for (int i = 0; i < sizeof(test_values)/sizeof(test_values[0]); i++) {\n        uint8_t buf[9];\n        int written = write_varint(buf, test_values[i]);\n        uint64_t decoded;\n        int read = read_varint(buf, &decoded);\n        assert(decoded == test_values[i]);\n        assert(written == read);\n    }\n}\n```\n### Page Header Tests\n```c\nvoid test_page_header_roundtrip() {\n    BTreePageHeader header;\n    header.page_type = BTREE_PAGE_TYPE_TABLE_LEAF;\n    header.first_freeblock = 0;\n    header.cell_count = 5;\n    header.cell_content_start = 4000;\n    header.fragmented_bytes = 0;\n    uint8_t page[PAGE_SIZE];\n    serialize_header(&header, page);\n    BTreePageHeader decoded;\n    deserialize_header(page, &decoded);\n    assert(decoded.page_type == header.page_type);\n    assert(decoded.cell_count == header.cell_count);\n    assert(decoded.cell_content_start == header.cell_content_start);\n}\n```\n### Row Serialization Tests\n```c\nvoid test_row_serialization() {\n    ColumnValue columns[3];\n    columns[0].type = VALUE_INTEGER;\n    columns[0].data.integer_val = 42;\n    columns[1].type = VALUE_TEXT;\n    columns[1].data.string_val.data = \"Alice\";\n    columns[1].data.string_val.length = 5;\n    columns[2].type = VALUE_INTEGER;\n    columns[2].data.integer_val = 30;\n    uint8_t buffer[1024];\n    int size = serialize_row(columns, 3, buffer);\n    ColumnValue decoded[3];\n    int decoded_count = deserialize_row(buffer, size, decoded);\n    assert(decoded_count == 3);\n    assert(decoded[0].type == VALUE_INTEGER);\n    assert(decoded[0].data.integer_val == 42);\n    assert(decoded[1].type == VALUE_TEXT);\n    assert(strcmp(decoded[1].data.string_val.data, \"Alice\") == 0);\n}\n```\n### B-tree Insert and Split Tests\n```c\nvoid test_insert_and_split() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value TEXT)\");\n    // Insert enough rows to cause multiple splits\n    for (int i = 0; i < 1000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, 'value%d')\", i, i);\n        execute_sql(db, sql);\n    }\n    // Verify all rows can be retrieved\n    for (int i = 0; i < 1000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"SELECT value FROM test WHERE id = %d\", i);\n        Result* result = execute_sql(db, sql);\n        assert(result->row_count == 1);\n        char expected[20];\n        snprintf(expected, sizeof(expected), \"value%d\", i);\n        assert(strcmp(result->rows[0].columns[0].data.string_val, expected) == 0);\n        free_result(result);\n    }\n    free_database(db);\n}\n```\n---\n## Design Decisions: Why This, Not That\n### Page Size: 4KB vs 8KB vs 16KB\n| Page Size | Pros | Cons | Best For |\n|-----------|------|------|----------|\n| **4KB (chosen)** | Matches OS page size, SSD block size; minimal read amplification | Lower fan-out, deeper trees | OLTP (many small transactions) |\n| 8KB | Higher fan-out, shallower trees | More wasted space for small rows | Mixed workloads |\n| 16KB | Maximum fan-out, minimal tree height | High read amplification for point lookups | OLAP (analytical queries) |\nSQLite uses 4KB by default. PostgreSQL defaults to 8KB. MySQL InnoDB defaults to 16KB. The choice depends on your workload's read/write ratio and row sizes.\n### Big-Endian vs Little-Endian\n| Choice | Pros | Cons |\n|--------|------|------|\n| **Big-Endian (chosen)** | Platform-independent files; human-readable in hex dumps | Conversion overhead on little-endian CPUs (most modern CPUs) |\n| Little-Endian | Native on x86/ARM; no conversion overhead | Database files are architecture-specific |\nSQLite chose big-endian for the page format (not record payload) because portability mattered more than avoiding byte swaps.\n### Slotted Page vs Log-Structured\n| Design | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **Slotted Page (chosen)** | In-place updates; good for random writes | Fragmentation; complex update logic | SQLite, PostgreSQL, MySQL |\n| Log-Structured | Append-only; fast writes | Compaction needed; complex reads | LSM-trees (RocksDB, LevelDB) |\nThe slotted page format is optimized for read-heavy workloads with occasional writes. LSM-trees are optimized for write-heavy workloads.\n---\n## What You've Built\nYour B-tree page format is a complete storage engine:\n1. **Page header** identifies page type (table/index, leaf/internal) and tracks free space\n2. **Slotted page format** enables variable-length cells with O(1) access via pointer array\n3. **Varint encoding** compresses integers from 1-9 bytes based on magnitude\n4. **Record serialization** uses type-length-value encoding for efficient column storage\n5. **Table B-trees** store full rows in all nodes, keyed by rowid\n6. **Index B+trees** store (key, rowid) pairs in leaves only, with linked leaves for range scans\n7. **Node splitting** maintains B-tree balance when pages overflow\n8. **System catalog** (sqlite_master) persists schema metadata in a known location\n9. **Full table scan** traverses leaf pages via sibling pointers\nThis is the physical storage layer that makes your database persistent. Every query touches this code path \u2014 a bug here corrupts user data.\n---\n## Knowledge Cascade\n### 1. Filesystem Design: B-trees Everywhere\nThe ext4, HFS+, and NTFS filesystems all use B-tree variants for directory entries and extent maps. The same split/merge logic you implemented appears in:\n- **Directory indexing**: Finding a file by name in a directory with 100,000 files\n- **Extent allocation**: Tracking which blocks belong to a file\n- **Free space management**: B-trees of free block ranges\nYour page format could, with minor modifications, implement a simple filesystem.\n### 2. LSM Trees: The Write-Optimized Alternative\nB-trees optimize for reads: O(log n) lookups with O(1) page reads. But they're write-unfriendly: every insert may cause a page split, and every update is an in-place write.\nLSM-trees (Log-Structured Merge-trees) flip this tradeoff:\n- **Writes are appends** to an in-memory table (fast)\n- **Background compaction** merges sorted runs (amortized cost)\n- **Reads check multiple levels** (slower, but cached hot data is fast)\nUnderstanding B-tree page formats reveals *why* LSM-trees exist: to solve the write amplification problem inherent in B-tree splits.\n### 3. Variable-Length Encoding: A Universal Pattern\nVarints appear everywhere space efficiency matters:\n- **Protocol Buffers**: All integers are varints by default\n- **MessagePack**: Compact binary serialization format\n- **UTF-8**: Variable-length character encoding (1-4 bytes per code point)\n- **MIDI files**: Variable-length delta times\nThe principle is universal: don't waste bytes on values that don't need them.\n### 4. Slotted Page: The Standard Pattern\nPostgreSQL, MySQL InnoDB, Oracle, and SQLite all use the slotted page format with minor variations:\n| System | Pointer Size | Page Size | Special Features |\n|--------|--------------|-----------|------------------|\n| SQLite | 2 bytes | 4KB default | Overflow pages for large rows |\n| PostgreSQL | 4 bytes (item pointer) | 8KB default | TOAST for large values |\n| MySQL InnoDB | 2 bytes | 16KB default | Page directory for binary search |\n| Oracle | Variable | 8KB default | Row directory with chaining |\nThe bidirectional growth pattern (pointers from top, cells from bottom) is universal because it works.\n### 5. Page Size Tuning: SSD vs HDD\nThe \"right\" page size depends on storage characteristics:\n**HDD (spinning disk)**:\n- Random read: 5-10ms (seek time dominates)\n- Larger pages amortize seek cost: 16KB-64KB is common\n- Sequential reads are much faster than random\n**SSD**:\n- Random read: 0.1ms (no seek)\n- Write amplification matters: SSDs erase in large blocks (256KB-4MB)\n- Smaller pages reduce write amplification: 4KB-8KB is common\n**NVM (Intel Optane, future tech)**:\n- Byte-addressable, no erase cycle\n- Cache-line sized (64 bytes) access may be optimal\n- B-tree page size could shrink dramatically\n---\n## What's Next\nWith the page format complete, you can store and retrieve rows. But your VDBE's `Column` opcode still needs to read specific columns from specific rows, and `WHERE` clauses need to evaluate expressions on those rows.\nThe next milestone, **SELECT Execution & DML**, connects your bytecode VM to the storage layer, implementing row deserialization, column projection, and WHERE clause filtering with three-valued logic.\nYour storage engine is the foundation. The execution layer is where queries come to life.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m5\", \"criteria\": [\"Page header is serialized at offset 0 and contains: page_type (1 byte identifying table/index leaf/internal), first_freeblock offset (2 bytes), cell_count (2 bytes), cell_content_start offset (2 bytes), fragmented_bytes (1 byte), and right_child page number (4 bytes, internal pages only)\", \"Page type byte values: 0x0D for table leaf, 0x05 for table internal, 0x0A for index leaf, 0x02 for index internal\", \"Slotted page format implements bidirectional growth: cell pointer array grows from header toward middle, cell content grows from end toward middle, with cell_content_start tracking the boundary\", \"Cell pointer array is a contiguous sequence of 2-byte big-endian offsets immediately following the header, with cell_count entries\", \"Varint encoding reads/writes integers in 1-9 bytes using continuation bit (high bit = 1 means more bytes follow, 0 means last byte), with 9th byte using all 8 bits\", \"Varint implementation correctly handles all edge cases: 0, 1, 127, 128, 16383, 16384, 2^56-1, 2^63-1, and maximum 64-bit value\", \"Row record format consists of header (varint header_size followed by varint serial types for each column) followed by column data in declaration order\", \"Serial type encoding: 0=NULL (0 bytes), 1=8-bit int, 2=16-bit int, 3=24-bit int, 4=32-bit int, 5=48-bit int, 6=64-bit int, 7=64-bit float, 8=integer 0 (0 bytes), 9=integer 1 (0 bytes), N>=12 even=BLOB of (N-12)/2 bytes, N>=13 odd=TEXT of (N-13)/2 bytes\", \"Table B-tree leaf cell format: payload_size (varint), rowid (varint), payload (serialized row), optional overflow_page (4 bytes)\", \"Table B-tree internal cell format: left_child_page (4 bytes big-endian), rowid (varint separator key)\", \"Index B+tree leaf cell format: payload_size (varint), payload (serialized key columns + rowid)\", \"Index B+tree internal cell format: left_child_page (4 bytes), payload_size (varint), payload (serialized separator key without rowid)\", \"Node split algorithm: when page is full, allocate sibling, move upper half of cells to sibling, update cell counts, promote separator key to parent, handle root split by creating new root with two children\", \"Root split creates new internal root page, copies old root contents to new child page, updates old root to point to children, increments tree height by 1\", \"System catalog (sqlite_master) is stored in page 1 with columns: type (TEXT), name (TEXT), tbl_name (TEXT), rootpage (INTEGER), sql (TEXT)\", \"CREATE TABLE allocates a new page for the table's root, inserts metadata row into sqlite_master with rootpage number and CREATE statement\", \"Full table scan finds leftmost leaf by traversing leftmost child pointers from root, then iterates all leaf cells in rowid order following right_sibling pointers\", \"Page serialization produces exactly PAGE_SIZE (4096) bytes with all multi-byte integers in big-endian format for portability\", \"Page deserialization correctly interprets big-endian integers and reconstructs header and cell pointer array from raw bytes\", \"Test suite validates varint roundtrip for all edge case values\", \"Test suite validates row serialization/deserialization preserves all column values and types\", \"Test suite validates page header roundtrip maintains all fields\", \"Test suite validates node split maintains B-tree invariants (all keys in left subtree < separator, all keys in right subtree >= separator)\", \"Test suite validates full table scan returns all rows in rowid order after multiple inserts and splits\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m6 -->\n# SELECT Execution & DML: Where Queries Come to Life\nYour bytecode compiler produces elegant instruction sequences. Your buffer pool caches pages efficiently. Your B-tree layer stores and retrieves rows. But these are *infrastructure* \u2014 they don't answer the user's question.\nWhen someone types `SELECT name FROM users WHERE age > 30`, they want *answers*: a list of names. The infrastructure exists to serve this query. Now you must connect the pieces. The VDBE executes opcodes that call into the B-tree layer, which fetches pages from the buffer pool. Rows are deserialized from bytes into values. Expressions are evaluated against those values. Results flow back to the caller.\nThis is where your database becomes *useful* \u2014 where SQL text becomes actual data.\n---\n## The Tension: NULL Breaks Everything\nHere's a query that looks innocent:\n```sql\nSELECT * FROM users WHERE email = email\n```\nYou'd expect this to return every row where `email` is equal to itself. And it does \u2014 except for rows where `email` is NULL. Those rows vanish from the result.\nWhy? Because `NULL = NULL` doesn't evaluate to `TRUE`. It evaluates to `NULL`.\n{{DIAGRAM:diag-three-valued-logic}}\nThis isn't a bug. It's the **three-valued logic** at the heart of SQL, and it will silently corrupt your results if you don't understand it. Consider:\n```sql\n-- What does this return?\nSELECT * FROM users WHERE NOT (status = 'inactive')\n```\nIf `status` is `'active'`, the condition is `NOT(FALSE)` = `TRUE`. \u2713\nIf `status` is `'inactive'`, the condition is `NOT(TRUE)` = `FALSE`. \u2713\nIf `status` is `NULL`, the condition is `NOT(NULL)` = `NULL`. \u2717\nRows with `NULL` status are excluded \u2014 probably not what the query author intended.\n### NULL Propagation\nNULL doesn't just affect comparisons. It *propagates*:\n```sql\nSELECT 5 + NULL    -- Result: NULL\nSELECT NULL || 'text'  -- Result: NULL (string concatenation)\nSELECT LENGTH(NULL)    -- Result: NULL\n```\nAny operation involving NULL yields NULL. This is like **NaN propagation** in floating-point arithmetic: once NaN enters a calculation, it infects everything downstream.\nThe tension: **you cannot treat NULL as a value.** It's the *absence* of a value \u2014 unknown, missing, inapplicable. Your comparison operators, arithmetic, and expression evaluator must all be NULL-aware, or you'll produce silently incorrect results.\n---\n## The Revelation: NULL is Not Zero or Empty\nHere's the misconception that will corrupt your data: *NULL is just zero for numbers and empty string for text \u2014 handle it like any other default value.*\n**No.** NULL is fundamentally different:\n| Expression | Result | Why |\n|------------|--------|-----|\n| `0 = 0` | TRUE | Zero equals zero |\n| `NULL = NULL` | NULL | Unknown equals unknown? We don't know |\n| `'' = ''` | TRUE | Empty string equals empty string |\n| `NULL = ''` | NULL | Does unknown equal empty? We don't know |\n| `0 + NULL` | NULL | Zero plus unknown = unknown |\n| `0 + 0` | 0 | Zero plus zero = zero |\nThe key insight: **NULL represents \"unknown\" \u2014 not \"zero\" or \"empty\".** If you compare two unknowns, the result is unknown. If you add an unknown to a known, the result is unknown.\nThis has profound implications for every part of your execution engine:\n- **WHERE clauses**: `WHERE x = 5` excludes rows where `x` is NULL (NULL \u2260 TRUE means don't include)\n- **JOIN conditions**: `ON a.id = b.id` fails to match when either side is NULL\n- **CHECK constraints**: `CHECK (status IN ('active', 'inactive'))` allows NULL (NULL IN (...) is NULL, not FALSE)\n- **Aggregate functions**: `COUNT(*)` counts NULL rows, `COUNT(x)` doesn't\n### The IS NULL Exception\nSQL provides special operators for NULL handling because `=` doesn't work:\n```sql\n-- WRONG: Never matches NULL\nSELECT * FROM users WHERE email = NULL\n-- RIGHT: Correctly matches NULL\nSELECT * FROM users WHERE email IS NULL\n-- Also works: IS NOT NULL\nSELECT * FROM users WHERE email IS NOT NULL\n```\n`IS NULL` and `IS NOT NULL` are the only ways to reliably test for NULL. Your expression evaluator must treat these as special cases, not as comparisons.\n---\n## Cursor-Based Table Scan\nThe `SELECT * FROM table` query is the simplest execution pattern: scan every row in the table and output it.\n\n![Cursor-Based Table Scan](./diagrams/diag-table-scan-cursor.svg)\n\n### The Cursor Abstraction\nA **cursor** abstracts B-tree traversal into a simple iterator interface:\n```c\ntypedef struct {\n    BufferPool* pool;\n    PageId root_page;\n    PageId current_page;\n    int current_cell_index;\n    bool eof;\n    // For internal node traversal\n    PageId* page_stack;\n    int stack_depth;\n} Cursor;\n```\nThe cursor maintains position within the B-tree:\n- `current_page`: Which leaf page we're on\n- `current_cell_index`: Which cell within that page\n- `eof`: Have we exhausted all rows?\n### Cursor Operations\n```c\n// Initialize cursor at first row\nvoid cursor_first(Cursor* cursor) {\n    cursor->current_page = cursor->root_page;\n    cursor->current_cell_index = 0;\n    cursor->eof = false;\n    // Navigate to leftmost leaf\n    while (true) {\n        Frame* frame = fetch_page(cursor->pool, cursor->current_page);\n        BTreePage* page = (BTreePage*)frame->data;\n        if (is_leaf_page(page)) {\n            // Found leftmost leaf\n            if (page->cell_count == 0) {\n                cursor->eof = true;  // Empty table\n            }\n            unpin_page(cursor->pool, frame);\n            break;\n        }\n        // Internal page: follow leftmost child\n        uint32_t leftmost_child = get_leftmost_child(page);\n        unpin_page(cursor->pool, frame);\n        cursor->current_page = leftmost_child;\n    }\n}\n// Advance to next row\nvoid cursor_next(Cursor* cursor) {\n    Frame* frame = fetch_page(cursor->pool, cursor->current_page);\n    BTreePage* page = (BTreePage*)frame->data;\n    cursor->current_cell_index++;\n    if (cursor->current_cell_index >= page->cell_count) {\n        // End of current leaf, move to next leaf\n        PageId next_leaf = page->right_sibling;\n        unpin_page(cursor->pool, frame);\n        if (next_leaf == 0) {\n            cursor->eof = true;  // No more leaves\n            return;\n        }\n        cursor->current_page = next_leaf;\n        cursor->current_cell_index = 0;\n        return;\n    }\n    unpin_page(cursor->pool, frame);\n}\n// Get current row\nRow* cursor_get_row(Cursor* cursor) {\n    Frame* frame = fetch_page(cursor->pool, cursor->current_page);\n    BTreePage* page = (BTreePage*)frame->data;\n    Cell* cell = get_cell(page, cursor->current_cell_index);\n    Row* row = deserialize_row(cell->payload, cell->payload_size);\n    unpin_page(cursor->pool, frame);\n    return row;\n}\n```\n### VDBE Integration\nThe VDBE opcodes for table scan:\n```c\n// OP_OpenRead: Open a cursor on a table\ncase OP_OpenRead: {\n    int cursor_id = instr->p1;\n    int root_page = instr->p2;\n    cursor_init(&vm->cursors[cursor_id], vm->buffer_pool, root_page);\n    vm->pc++;\n    break;\n}\n// OP_Rewind: Position cursor at first row\ncase OP_Rewind: {\n    int cursor_id = instr->p1;\n    Cursor* cursor = &vm->cursors[cursor_id];\n    cursor_first(cursor);\n    if (cursor->eof) {\n        // Empty table, jump to p2\n        vm->pc = instr->p2;\n    } else {\n        vm->pc++;\n    }\n    break;\n}\n// OP_Column: Read a column value into a register\ncase OP_Column: {\n    int cursor_id = instr->p1;\n    int col_index = instr->p2;\n    int dest_reg = instr->p3;\n    Cursor* cursor = &vm->cursors[cursor_id];\n    Row* row = cursor_get_row(cursor);\n    // Copy column value to register\n    if (col_index < row->column_count) {\n        copy_value(&vm->registers[dest_reg], &row->columns[col_index]);\n    } else {\n        // Column doesn't exist (schema mismatch?)\n        vm->registers[dest_reg].type = VALUE_NULL;\n    }\n    free_row(row);\n    vm->pc++;\n    break;\n}\n// OP_Next: Move to next row\ncase OP_Next: {\n    int cursor_id = instr->p1;\n    Cursor* cursor = &vm->cursors[cursor_id];\n    cursor_next(cursor);\n    if (cursor->eof) {\n        vm->pc++;  // End of table\n    } else {\n        vm->pc = instr->p2;  // Loop back\n    }\n    break;\n}\n```\n---\n## Column Projection\n`SELECT *` returns all columns. `SELECT name, email` returns only specific columns. This is **projection** \u2014 extracting a subset of columns from each row.\n### Deserialization with Projection\nWhen you deserialize a row, you parse the record header to find each column's offset and type:\n```c\ntypedef struct {\n    int header_size;\n    int* serial_types;  // Serial type for each column\n    int column_count;\n    const uint8_t* data_start;  // Where column data begins\n} RecordHeader;\nint parse_record_header(const uint8_t* payload, RecordHeader* header) {\n    const uint8_t* ptr = payload;\n    // Read header size\n    uint64_t header_size_varint;\n    int bytes_read = read_varint(ptr, &header_size_varint);\n    header->header_size = (int)header_size_varint;\n    ptr += bytes_read;\n    // Read serial types\n    header->column_count = 0;\n    header->serial_types = malloc(MAX_COLUMNS * sizeof(int));\n    int header_bytes_read = bytes_read;\n    while (header_bytes_read < header->header_size) {\n        uint64_t serial_type;\n        bytes_read = read_varint(ptr, &serial_type);\n        header->serial_types[header->column_count++] = (int)serial_type;\n        ptr += bytes_read;\n        header_bytes_read += bytes_read;\n    }\n    header->data_start = payload + header->header_size;\n    return 0;\n}\nColumnValue* get_column_value(const uint8_t* payload, RecordHeader* header, int col_index) {\n    if (col_index >= header->column_count) {\n        return NULL;  // Column doesn't exist\n    }\n    // Find the offset of this column\n    const uint8_t* ptr = header->data_start;\n    for (int i = 0; i < col_index; i++) {\n        int col_size = serial_type_to_size(header->serial_types[i]);\n        ptr += col_size;\n    }\n    // Deserialize this column\n    ColumnValue* value = malloc(sizeof(ColumnValue));\n    deserialize_value(ptr, header->serial_types[col_index], value);\n    return value;\n}\n```\n### Projection in the VDBE\nThe compiler generates `Column` opcodes only for requested columns:\n```sql\nSELECT name, email FROM users\n```\nBytecode:\n```\nOpenRead     cursor=0  table=users\nRewind       cursor=0\nLOOP:\n  Column     cursor=0  col=name_index   r1\n  Column     cursor=0  col=email_index  r2\n  ResultRow  r1        2\n  Next       cursor=0  LOOP\nHalt\n```\nOnly `name` and `email` are extracted. Other columns are skipped entirely \u2014 no deserialization cost.\n---\n## WHERE Clause Evaluation\nThe WHERE clause is a boolean expression evaluated for each row. If the expression evaluates to TRUE, the row is included. If it evaluates to FALSE or NULL, the row is excluded.\n\n![WHERE Clause Bytecode Pattern](./diagrams/diag-bytecode-where.svg)\n\n### Expression Evaluation with NULL\nYour expression evaluator must handle NULL at every step:\n```c\nValue evaluate_binary_expr(Value* left, const char* op, Value* right) {\n    Value result;\n    // Arithmetic operators: NULL propagates\n    if (strcmp(op, \"+\") == 0) {\n        if (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n            result.type = VALUE_NULL;\n            return result;\n        }\n        result.type = VALUE_FLOAT;  // Promote to float for mixed types\n        result.data.float_val = value_to_float(left) + value_to_float(right);\n        return result;\n    }\n    // Comparison operators: NULL yields NULL (unknown)\n    if (is_comparison_op(op)) {\n        if (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n            result.type = VALUE_NULL;\n            return result;\n        }\n        int cmp = compare_values(left, right);\n        result.type = VALUE_INTEGER;\n        if (strcmp(op, \"=\") == 0)  result.data.integer_val = (cmp == 0);\n        else if (strcmp(op, \"<\") == 0)  result.data.integer_val = (cmp < 0);\n        else if (strcmp(op, \">\") == 0)  result.data.integer_val = (cmp > 0);\n        else if (strcmp(op, \"<=\") == 0) result.data.integer_val = (cmp <= 0);\n        else if (strcmp(op, \">=\") == 0) result.data.integer_val = (cmp >= 0);\n        else if (strcmp(op, \"<>\") == 0 || strcmp(op, \"!=\") == 0) \n            result.data.integer_val = (cmp != 0);\n        return result;\n    }\n    // Logical operators: special NULL handling\n    if (strcmp(op, \"AND\") == 0) {\n        return evaluate_and(left, right);\n    }\n    if (strcmp(op, \"OR\") == 0) {\n        return evaluate_or(left, right);\n    }\n    // Unknown operator\n    result.type = VALUE_NULL;\n    return result;\n}\n```\n### Three-Valued Logic for AND/OR\nThe truth tables for AND and OR with NULL:\n| AND | TRUE | FALSE | NULL |\n|-----|------|-------|------|\n| TRUE | TRUE | FALSE | NULL |\n| FALSE | FALSE | FALSE | FALSE |\n| NULL | NULL | FALSE | NULL |\n| OR | TRUE | FALSE | NULL |\n|----|------|-------|------|\n| TRUE | TRUE | TRUE | TRUE |\n| FALSE | TRUE | FALSE | NULL |\n| NULL | TRUE | NULL | NULL |\n```c\nValue evaluate_and(Value* left, Value* right) {\n    Value result;\n    result.type = VALUE_INTEGER;\n    // If either is FALSE, result is FALSE\n    if (left->type == VALUE_INTEGER && left->data.integer_val == 0) {\n        result.data.integer_val = 0;\n        return result;\n    }\n    if (right->type == VALUE_INTEGER && right->data.integer_val == 0) {\n        result.data.integer_val = 0;\n        return result;\n    }\n    // If either is NULL, result is NULL\n    if (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n        result.type = VALUE_NULL;\n        return result;\n    }\n    // Both must be TRUE\n    result.data.integer_val = \n        (left->data.integer_val != 0) && (right->data.integer_val != 0);\n    return result;\n}\nValue evaluate_or(Value* left, Value* right) {\n    Value result;\n    result.type = VALUE_INTEGER;\n    // If either is TRUE, result is TRUE\n    if (left->type == VALUE_INTEGER && left->data.integer_val != 0) {\n        result.data.integer_val = 1;\n        return result;\n    }\n    if (right->type == VALUE_INTEGER && right->data.integer_val != 0) {\n        result.data.integer_val = 1;\n        return result;\n    }\n    // If either is NULL, result is NULL\n    if (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n        result.type = VALUE_NULL;\n        return result;\n    }\n    // Both must be FALSE\n    result.data.integer_val = 0;\n    return result;\n}\n```\n### NOT with NULL\n```c\nValue evaluate_not(Value* operand) {\n    Value result;\n    if (operand->type == VALUE_NULL) {\n        result.type = VALUE_NULL;\n        return result;\n    }\n    result.type = VALUE_INTEGER;\n    result.data.integer_val = (operand->data.integer_val == 0) ? 1 : 0;\n    return result;\n}\n```\n### IS NULL and IS NOT NULL\nThese are the only operators that produce TRUE/FALSE for NULL:\n```c\nValue evaluate_is_null(Value* operand) {\n    Value result;\n    result.type = VALUE_INTEGER;\n    result.data.integer_val = (operand->type == VALUE_NULL) ? 1 : 0;\n    return result;\n}\nValue evaluate_is_not_null(Value* operand) {\n    Value result;\n    result.type = VALUE_INTEGER;\n    result.data.integer_val = (operand->type != VALUE_NULL) ? 1 : 0;\n    return result;\n}\n```\n### WHERE Clause Bytecode Pattern\nFor `SELECT * FROM users WHERE age > 30 AND status = 'active'`:\n```\nOpenRead     cursor=0  table=users\nRewind       cursor=0  if-empty-goto=END\nLOOP:\n  Column     cursor=0  col=age      r1\n  Integer    30        r2\n  Le         r1        r2           SKIP_ROW  -- age <= 30? skip (inverted)\n  Column     cursor=0  col=status   r3\n  String8    'active'  r4\n  Ne         r3        r4           SKIP_ROW  -- status != 'active'? skip\n  Column     cursor=0  col=id       r5\n  Column     cursor=0  col=name     r6\n  Column     cursor=0  col=age      r7\n  Column     cursor=0  col=status   r8\n  ResultRow  r5        4\nSKIP_ROW:\n  Next       cursor=0  LOOP\nEND:\n  Halt\n```\nThe comparisons are **inverted**: we check if the condition *fails* and jump to skip the row. If the comparison yields NULL (e.g., `age` is NULL), the condition fails and the row is skipped.\n---\n## INSERT Execution\nINSERT adds a new row to a table. The process:\n1. Open the table for writing\n2. Evaluate value expressions\n3. Build a serialized record\n4. Insert into the B-tree\n5. Maintain any indexes\n### INSERT Bytecode\n```sql\nINSERT INTO users (id, name, age) VALUES (1, 'Alice', 30)\n```\n```\nOpenWrite    cursor=0  table=users\nInteger      1         r1\nString8      'Alice'   r2\nInteger      30        r3\nMakeRecord   r1        3            r4       -- Build record from r1, r2, r3\nNewRowid     cursor=0  r5                    -- Generate new rowid\nInsert       cursor=0  r4           r5       -- Insert record with rowid\nClose        cursor=0\nHalt\n```\n### MakeRecord Opcode\n```c\ncase OP_MakeRecord: {\n    int start_reg = instr->p1;\n    int count = instr->p2;\n    int dest_reg = instr->p3;\n    // Serialize columns into a record\n    uint8_t buffer[MAX_RECORD_SIZE];\n    int size = serialize_row(&vm->registers[start_reg], count, buffer);\n    vm->registers[dest_reg].type = VALUE_BLOB;\n    vm->registers[dest_reg].data.blob_val.data = malloc(size);\n    memcpy(vm->registers[dest_reg].data.blob_val.data, buffer, size);\n    vm->registers[dest_reg].data.blob_val.length = size;\n    vm->pc++;\n    break;\n}\n```\n### NewRowid Opcode\nFor tables with `INTEGER PRIMARY KEY`, the rowid can be specified. Otherwise, generate one:\n```c\ncase OP_NewRowid: {\n    int cursor_id = instr->p1;\n    int dest_reg = instr->p2;\n    Cursor* cursor = &vm->cursors[cursor_id];\n    // Find the maximum rowid and add 1\n    int64_t max_rowid = btree_find_max_rowid(cursor);\n    int64_t new_rowid = max_rowid + 1;\n    // If rowid is already taken (due to deletions), find a free one\n    // (Simplified: in production, use a more sophisticated algorithm)\n    vm->registers[dest_reg].type = VALUE_INTEGER;\n    vm->registers[dest_reg].data.integer_val = new_rowid;\n    vm->pc++;\n    break;\n}\n```\n### Insert Opcode\n```c\ncase OP_Insert: {\n    int cursor_id = instr->p1;\n    int record_reg = instr->p2;\n    int rowid_reg = instr->p3;\n    Cursor* cursor = &vm->cursors[cursor_id];\n    Value* record = &vm->registers[record_reg];\n    int64_t rowid = vm->registers[rowid_reg].data.integer_val;\n    // Build the cell\n    Cell cell;\n    cell.rowid = rowid;\n    cell.payload = record->data.blob_val.data;\n    cell.payload_size = record->data.blob_val.length;\n    // Insert into B-tree\n    int result = btree_insert(cursor, &cell);\n    if (result != 0) {\n        vm->error_code = result;\n        return -1;\n    }\n    vm->pc++;\n    break;\n}\n```\n### Auto-Increment Behavior\nWhen `INSERT` specifies NULL for the INTEGER PRIMARY KEY column:\n```sql\nINSERT INTO users (id, name, age) VALUES (NULL, 'Bob', 25)\n```\nThe compiler detects this and uses `NewRowid` to generate a rowid:\n```\nOpenWrite    cursor=0\nNull         r1                      -- NULL for id\nString8      'Bob'    r2\nInteger      25       r3\nNewRowid     cursor=0 r4             -- Generate rowid\nCopy         r4       r1             -- Replace NULL with generated rowid\nMakeRecord   r1       3     r5\nInsert       cursor=0 r5    r4\nClose        cursor=0\nHalt\n```\n---\n## UPDATE Execution\nUPDATE modifies existing rows. The process:\n1. Scan for rows matching WHERE clause\n2. For each matching row, modify specified columns\n3. Delete old row, insert new row (B-tree modification)\n### UPDATE Bytecode\n```sql\nUPDATE users SET age = 31, status = 'senior' WHERE id = 1\n```\n```\nOpenWrite    cursor=0  table=users\nRewind       cursor=0  if-empty-goto=END\nLOOP:\n  Column     cursor=0  col=id       r1\n  Integer    1         r2\n  Ne         r1        r2           SKIP_ROW  -- id != 1? skip\n  Column     cursor=0  col=id       r3        -- Keep id\n  Column     cursor=0  col=name     r4        -- Keep name\n  Integer    31        r5                     -- New age\n  String8    'senior'  r6                     -- New status\n  MakeRecord r3        4            r7        -- Build new record\n  Delete     cursor=0\n  Insert     cursor=0  r7           r3        -- Insert with same rowid\nSKIP_ROW:\n  Next       cursor=0  LOOP\nEND:\n  Close      cursor=0\n  Halt\n```\n### The Delete + Insert Pattern\nIn a B-tree, you can't modify a row in place because:\n1. The row size might change (new values don't fit)\n2. The rowid might change position in the tree\nSo UPDATE is implemented as DELETE + INSERT:\n```c\ncase OP_Delete: {\n    int cursor_id = instr->p1;\n    Cursor* cursor = &vm->cursors[cursor_id];\n    btree_delete(cursor);\n    vm->pc++;\n    break;\n}\n```\n### Rowid Immutability\nThe rowid (INTEGER PRIMARY KEY) cannot be changed by UPDATE. If you try:\n```sql\nUPDATE users SET id = 999 WHERE id = 1\n```\nThis should be rejected or handled as a delete + insert with a new rowid. The compiler should detect attempts to update the rowid column and either:\n- Report an error\n- Generate delete + insert code\n---\n## DELETE Execution\nDELETE removes rows matching the WHERE clause. The challenge: you're modifying the B-tree while iterating over it.\n\n![DELETE: Two-Pass Algorithm](./diagrams/diag-delete-two-pass.svg)\n\n### The Two-Pass Algorithm\nIf you delete during iteration, the cursor position becomes invalid:\n```c\n// WRONG: Delete during iteration\nwhile (cursor_next(cursor)) {\n    if (matches_where(cursor)) {\n        btree_delete(cursor);  // Cursor position corrupted!\n    }\n}\n```\nThe solution: **two-pass approach**:\n1. **First pass**: Scan and collect rowids of rows to delete\n2. **Second pass**: Delete each rowid\n```c\ncase OP_DeleteRows: {\n    // This opcode assumes rowids were collected in previous opcodes\n    int cursor_id = instr->p1;\n    int rowid_list_reg = instr->p2;  // Register containing rowid list\n    Value* rowid_list = &vm->registers[rowid_list_reg];\n    for (int i = 0; i < rowid_list->data.array.count; i++) {\n        int64_t rowid = rowid_list->data.array.items[i].data.integer_val;\n        btree_delete_by_rowid(vm->buffer_pool, cursor_id, rowid);\n    }\n    vm->pc++;\n    break;\n}\n```\n### DELETE Bytecode\n```sql\nDELETE FROM users WHERE age < 18\n```\n```\nOpenWrite    cursor=0  table=users\nOpenWrite    cursor=1  temp_rowids  -- Temporary table for rowids\nRewind       cursor=0  if-empty-goto=DELETE_PHASE\nCOLLECT_LOOP:\n  Column     cursor=0  col=age      r1\n  Integer    18        r2\n  Ge         r1        r2           SKIP_COLLECT  -- age >= 18? skip\n  Column     cursor=0  col=rowid    r3\n  Insert     cursor=1  r3           r3     -- Collect rowid\nSKIP_COLLECT:\n  Next       cursor=0  COLLECT_LOOP\nDELETE_PHASE:\n  Rewind     cursor=1  if-empty-goto=END\nDELETE_LOOP:\n  Column     cursor=1  col=0        r4       -- Get rowid to delete\n  DeleteRow  cursor=0  r4                    -- Delete by rowid\n  Next       cursor=1  DELETE_LOOP\nEND:\n  Close      cursor=0\n  Close      cursor=1\n  Halt\n```\n### Simpler Single-Pass with Careful Positioning\nFor simple cases, you can delete in a single pass if you're careful:\n```c\nvoid delete_with_cursor(Cursor* cursor, bool (*predicate)(Row*)) {\n    cursor_first(cursor);\n    while (!cursor->eof) {\n        Row* row = cursor_get_row(cursor);\n        bool should_delete = predicate(row);\n        int64_t rowid = row->rowid;\n        free_row(row);\n        if (should_delete) {\n            // Delete and reset cursor position\n            btree_delete_by_rowid(cursor, rowid);\n            cursor_first(cursor);  // Reset to beginning\n            continue;  // Don't advance, re-check current position\n        }\n        cursor_next(cursor);\n    }\n}\n```\nThis is inefficient (O(n\u00b2) worst case) but correct. The two-pass approach is preferred for production.\n---\n## Constraint Enforcement\nConstraints are checked during INSERT and UPDATE execution.\n### NOT NULL Constraint\n```c\nint check_not_null_constraint(Table* table, ColumnValue* values, int count) {\n    for (int i = 0; i < count; i++) {\n        ColumnDef* col = &table->columns[i];\n        if (col->not_null && values[i].type == VALUE_NULL) {\n            return ERROR_NOT_NULL_CONSTRAINT;\n        }\n    }\n    return 0;\n}\n// In OP_Insert handler:\ncase OP_Insert: {\n    // ... get record ...\n    Table* table = get_table(vm, cursor_id);\n    ColumnValue* values = deserialize_to_values(record);\n    int constraint_result = check_not_null_constraint(table, values, record_count);\n    if (constraint_result != 0) {\n        vm->error_code = constraint_result;\n        snprintf(vm->error_message, sizeof(vm->error_message),\n                 \"NOT NULL constraint failed: %s.%s\",\n                 table->name, table->columns[failed_column].name);\n        return -1;\n    }\n    // ... proceed with insert ...\n}\n```\n### UNIQUE Constraint\n```c\nint check_unique_constraint(Database* db, Table* table, int col_index, ColumnValue* value) {\n    // Check if value already exists in unique index\n    Index* index = find_unique_index(table, col_index);\n    if (index == NULL) return 0;  // No unique constraint\n    // Look up in index\n    bool exists = index_lookup(db, index, value);\n    if (exists) {\n        return ERROR_UNIQUE_CONSTRAINT;\n    }\n    return 0;\n}\n```\n### PRIMARY KEY Constraint\nThe INTEGER PRIMARY KEY is special:\n- It's the rowid\n- It's automatically NOT NULL\n- It's automatically UNIQUE\n```c\nint check_primary_key_constraint(Database* db, Table* table, int64_t rowid) {\n    // Check if rowid already exists\n    bool exists = btree_rowid_exists(db, table->root_page, rowid);\n    if (exists) {\n        return ERROR_PRIMARY_KEY_CONSTRAINT;\n    }\n    return 0;\n}\n```\n---\n## Error Handling\n### Table Not Found\nWhen the parser encounters a table reference, it looks up the table in the system catalog:\n```c\nTable* lookup_table(Database* db, const char* name) {\n    // Query sqlite_master\n    const char* sql = \"SELECT rootpage, sql FROM sqlite_master \"\n                      \"WHERE type='table' AND name=?\";\n    PreparedStatement* stmt = prepare(db, sql);\n    bind_text(stmt, 1, name);\n    if (!execute_step(stmt)) {\n        // Table not found\n        finalize(stmt);\n        return NULL;\n    }\n    // ... build table from result ...\n}\n// In compiler:\nTable* table = lookup_table(compiler->db, table_name);\nif (table == NULL) {\n    compiler_error(compiler, \"no such table: %s\", table_name);\n    return NULL;\n}\n```\n### Error Propagation\nErrors at any layer must propagate up to the caller:\n```c\nint execute_query(Database* db, const char* sql) {\n    // Tokenize\n    TokenStream* tokens = tokenize(sql);\n    if (tokens->error) {\n        db->error_code = ERROR_TOKENIZE;\n        snprintf(db->error_message, sizeof(db->error_message),\n                 \"tokenize error at line %d, column %d: %s\",\n                 tokens->error_line, tokens->error_column, tokens->error_message);\n        return -1;\n    }\n    // Parse\n    ASTNode* ast = parse(tokens);\n    if (parser_has_error()) {\n        db->error_code = ERROR_PARSE;\n        snprintf(db->error_message, sizeof(db->error_message),\n                 \"parse error: %s\", parser_error_message());\n        return -1;\n    }\n    // Compile\n    CompiledProgram* prog = compile(ast);\n    if (compiler_has_error()) {\n        db->error_code = ERROR_COMPILE;\n        snprintf(db->error_message, sizeof(db->error_message),\n                 \"compile error: %s\", compiler_error_message());\n        return -1;\n    }\n    // Execute\n    int result = vdbe_execute(prog, db);\n    free_program(prog);\n    free_ast(ast);\n    free_tokens(tokens);\n    return result;\n}\n```\n---\n## The Three-Level View\n### Level 1: Application (SQL Text \u2192 Result Rows)\nAt the API level, execution is a single call:\n```c\nResult* result = execute_query(db, \"SELECT * FROM users WHERE age > 30\");\nfor (int i = 0; i < result->row_count; i++) {\n    printf(\"%s\\n\", result->rows[i].columns[1].data.string_val);\n}\nfree_result(result);\n```\nThe caller provides SQL, gets rows. They don't know about cursors, registers, or three-valued logic.\n### Level 2: Engine (Bytecode + Expression Evaluation + B-tree Operations)\nInside the VDBE, each opcode manipulates state:\n```c\nwhile (!halted) {\n    Instruction* instr = &code[pc];\n    switch (instr->opcode) {\n        case OP_Column:\n            // Read from B-tree via cursor\n            // Deserialize column value\n            // Store in register\n            break;\n        case OP_Eq:\n            // Compare registers with NULL handling\n            // Jump if equal\n            break;\n        case OP_ResultRow:\n            // Build result from registers\n            // Invoke callback\n            break;\n    }\n}\n```\n### Level 3: Implementation (Bytes, Pages, Values)\nAt the lowest level, every operation is byte manipulation:\n```c\n// Reading a column from a page\nint col_offset = calculate_column_offset(page, cell_index, col_index);\nuint8_t serial_type = header->serial_types[col_index];\nColumnValue* value = deserialize_value(page + col_offset, serial_type);\n// Comparing values with NULL check\nif (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n    // NULL comparison yields NULL\n    return CMP_NULL;\n}\n```\n---\n## Common Pitfalls\n### NULL Comparison Bugs\n```c\n// WRONG: Treating NULL as equal to NULL\nif (compare_values(left, right) == 0) {\n    // This never matches when either is NULL\n}\n// RIGHT: Use IS NULL for null checks\nif (left->type == VALUE_NULL && right->type == VALUE_NULL) {\n    // Both NULL\n}\n```\n### Forgetting to Handle NULL in Arithmetic\n```c\n// WRONG: NULL + 5 crashes or returns garbage\nint64_t result = left->data.integer_val + right->data.integer_val;\n// RIGHT: NULL propagates\nif (left->type == VALUE_NULL || right->type == VALUE_NULL) {\n    result.type = VALUE_NULL;\n} else {\n    result.data.integer_val = left->data.integer_val + right->data.integer_val;\n}\n```\n### Cursor Corruption During DELETE\n```c\n// WRONG: Delete invalidates cursor\ncursor_get_row(cursor, &row);\nbtree_delete(cursor);\ncursor_next(cursor);  // Cursor position is garbage!\n// RIGHT: Two-pass or reset cursor\nbtree_delete(cursor);\ncursor_first(cursor);  // Reset to beginning\n```\n### NOT NULL Check Before Type Coercion\n```c\n// WRONG: Check NULL after coercion\nValue* coerced = coerce_to_integer(value);\nif (coerced->type == VALUE_NULL) { ... }\n// RIGHT: Check NULL first\nif (value->type == VALUE_NULL) {\n    return check_constraint_failed();\n}\nValue* coerced = coerce_to_integer(value);\n```\n### Column Index Mismatch\nThe column index in `Column` opcode must match the table schema. If the schema changes after the statement is prepared, the indices may be wrong:\n```c\n// Solution: Store column names in prepared statement, resolve at execution\nint col_index = table_find_column_index(table, stmt->column_names[i]);\n```\n---\n## Testing Strategy\n### Three-Valued Logic Tests\n```c\nvoid test_null_comparison() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, NULL)\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, 5)\");\n    // NULL = NULL should not match\n    Result* r1 = execute_sql(db, \"SELECT * FROM test WHERE value = NULL\");\n    assert(r1->row_count == 0);\n    // IS NULL should match\n    Result* r2 = execute_sql(db, \"SELECT * FROM test WHERE value IS NULL\");\n    assert(r2->row_count == 1);\n    assert(r2->rows[0].columns[0].data.integer_val == 1);\n    // NULL > 5 should not match\n    Result* r3 = execute_sql(db, \"SELECT * FROM test WHERE value > 5\");\n    assert(r3->row_count == 0);\n    free_result(r1);\n    free_result(r2);\n    free_result(r3);\n    free_database(db);\n}\nvoid test_null_and_or() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (a INTEGER, b INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, NULL)\");\n    execute_sql(db, \"INSERT INTO test VALUES (NULL, 1)\");\n    execute_sql(db, \"INSERT INTO test VALUES (NULL, NULL)\");\n    execute_sql(db, \"INSERT INTO test VALUES (0, 0)\");\n    // FALSE AND NULL = FALSE\n    Result* r1 = execute_sql(db, \"SELECT * FROM test WHERE a = 0 AND b = 1\");\n    assert(r1->row_count == 0);\n    // TRUE OR NULL = TRUE\n    Result* r2 = execute_sql(db, \"SELECT * FROM test WHERE a = 1 OR b = NULL\");\n    assert(r2->row_count == 1);\n    // NULL OR NULL = NULL (not matched)\n    Result* r3 = execute_sql(db, \"SELECT * FROM test WHERE a = NULL OR b = NULL\");\n    assert(r3->row_count == 0);\n    free_result(r1);\n    free_result(r2);\n    free_result(r3);\n    free_database(db);\n}\n```\n### Projection Tests\n```c\nvoid test_column_projection() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (a INTEGER, b TEXT, c INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'hello', 3)\");\n    Result* r = execute_sql(db, \"SELECT b, a FROM test\");\n    assert(r->row_count == 1);\n    assert(r->column_count == 2);\n    assert(strcmp(r->rows[0].columns[0].data.string_val, \"hello\") == 0);\n    assert(r->rows[0].columns[1].data.integer_val == 1);\n    free_result(r);\n    free_database(db);\n}\n```\n### INSERT/UPDATE/DELETE Tests\n```c\nvoid test_insert_select() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, name TEXT)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'Alice')\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, 'Bob')\");\n    Result* r = execute_sql(db, \"SELECT * FROM test ORDER BY id\");\n    assert(r->row_count == 2);\n    assert(strcmp(r->rows[0].columns[1].data.string_val, \"Alice\") == 0);\n    assert(strcmp(r->rows[1].columns[1].data.string_val, \"Bob\") == 0);\n    free_result(r);\n    free_database(db);\n}\nvoid test_update() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 10)\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, 20)\");\n    execute_sql(db, \"UPDATE test SET value = 99 WHERE id = 1\");\n    Result* r = execute_sql(db, \"SELECT value FROM test WHERE id = 1\");\n    assert(r->row_count == 1);\n    assert(r->rows[0].columns[0].data.integer_val == 99);\n    free_result(r);\n    free_database(db);\n}\nvoid test_delete() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1)\");\n    execute_sql(db, \"INSERT INTO test VALUES (2)\");\n    execute_sql(db, \"INSERT INTO test VALUES (3)\");\n    execute_sql(db, \"DELETE FROM test WHERE id = 2\");\n    Result* r = execute_sql(db, \"SELECT * FROM test ORDER BY id\");\n    assert(r->row_count == 2);\n    assert(r->rows[0].columns[0].data.integer_val == 1);\n    assert(r->rows[1].columns[0].data.integer_val == 3);\n    free_result(r);\n    free_database(db);\n}\n```\n### Constraint Tests\n```c\nvoid test_not_null_constraint() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, name TEXT NOT NULL)\");\n    int result = execute_sql(db, \"INSERT INTO test VALUES (1, NULL)\");\n    assert(result != 0);  // Should fail\n    assert(strstr(db->error_message, \"NOT NULL\") != NULL);\n    free_database(db);\n}\nvoid test_unique_constraint() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, email TEXT UNIQUE)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'alice@example.com')\");\n    int result = execute_sql(db, \"INSERT INTO test VALUES (2, 'alice@example.com')\");\n    assert(result != 0);  // Should fail\n    assert(strstr(db->error_message, \"UNIQUE\") != NULL);\n    free_database(db);\n}\n```\n---\n## What You've Built\nYour execution layer is a complete query engine:\n1. **Cursor-based table scan** iterates through B-tree leaves in rowid order\n2. **Column projection** extracts only requested columns from serialized rows\n3. **WHERE clause evaluation** with three-valued logic handles NULL correctly\n4. **INSERT execution** serializes rows and inserts into B-tree\n5. **UPDATE execution** modifies rows via delete + insert\n6. **DELETE execution** uses two-pass algorithm to avoid cursor corruption\n7. **Constraint enforcement** rejects invalid data with descriptive errors\n8. **Error handling** propagates errors from all layers to the caller\nThis is where SQL becomes data. Every query flows through this execution path.\n---\n## Knowledge Cascade\n### 1. Optional Types in Programming Languages\nThe three-valued logic you implemented is the same concept formalized in programming languages:\n- **Rust's `Option<T>`**: `Some(value)` or `None` \u2014 you must explicitly handle the None case\n- **Swift's `T?`**: Optional type with forced unwrapping\n- **Haskell's `Maybe`**: Monadic handling of potential absence\nThe difference: SQL implicit NULL handling vs. explicit Option handling. SQL's approach is more ergonomic for queries but easier to get wrong. Language designers chose explicitness to force programmers to think about absence.\n### 2. NaN Propagation in Floating-Point\nNULL propagates through SQL operations exactly like NaN propagates through math:\n```\nNULL + 5 = NULL       NaN + 5 = NaN\nNULL = NULL = NULL    NaN == NaN = false\n```\nIEEE 754 designed NaN as \"Not a Number\" \u2014 an invalid or indeterminate result. SQL's NULL is \"Not a Value\" \u2014 an unknown or missing value. The semantics are nearly identical: an invalid input yields an invalid output.\n### 3. Missing Data in Data Science\nPandas, R, and other data tools face the same NULL problem:\n```python\n# Pandas\ndf[df['value'].notna()]  # Filter out NULL/NaN\n# R\nna.omit(data)  # Remove rows with NA values\n```\nThe same three-valued logic applies: comparisons with NA yield NA. Data scientists spend significant time handling missing data \u2014 your database must handle it correctly at the source.\n### 4. JSON null vs. Missing Key\nAPIs face a modeling question: should `null` and missing be different?\n```json\n{\"name\": \"Alice\", \"email\": null}     // Email exists but is null\n{\"name\": \"Alice\"}                     // Email doesn't exist\n```\nSome APIs distinguish these (PATCH semantics: null = clear, missing = unchanged). SQL doesn't \u2014 NULL represents both \"unknown\" and \"doesn't exist.\" The modeling question is the same: how do you represent absence?\n### 5. Distributed Systems Partial Failures\nIn distributed systems, a timeout means \"outcome unknown\" \u2014 the request may have succeeded or failed. This is analogous to NULL:\n```\nLocal query:  value = 5        (known)\nLocal query:  value = NULL     (unknown)\nDistributed:  response = 200   (known success)\nDistributed:  timeout          (unknown)\n```\nBoth represent \"we don't know.\" The handling strategies are similar: retry (re-query), assume failure (exclude), or propagate uncertainty (return NULL/timeout to caller).\n---\n## What's Next\nWith SELECT, INSERT, UPDATE, and DELETE working, you have a functional database. You can store data, query it, modify it, and delete it.\nBut every query so far has been a full table scan. For a table with a million rows, `WHERE id = 42` reads all million rows to find one. The next milestone, **Secondary Indexes**, adds B+tree indexes that enable O(log n) lookups instead of O(n) scans.\nYour database can store and retrieve data. Now it's time to make it fast.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m6\", \"criteria\": [\"Cursor abstraction provides first(), next(), get_row() operations for iterating B-tree leaves in rowid order\", \"OP_Rewind positions cursor at first row of table; jumps to specified address if table is empty\", \"OP_Column reads a specific column from the current row, deserializing the value into a target register\", \"OP_Next advances cursor to next row; jumps to loop start if more rows, continues past loop if EOF\", \"Column projection extracts only requested columns from serialized records, skipping unrequested columns entirely\", \"WHERE clause evaluation implements three-valued logic: NULL comparisons yield NULL, which is treated as FALSE for filtering\", \"Expression evaluator handles NULL propagation: NULL + value = NULL, NULL || string = NULL\", \"AND operator with NULL: FALSE AND NULL = FALSE (short-circuit), TRUE AND NULL = NULL, NULL AND NULL = NULL\", \"OR operator with NULL: TRUE OR NULL = TRUE (short-circuit), FALSE OR NULL = NULL, NULL OR NULL = NULL\", \"NOT operator with NULL: NOT NULL = NULL (not TRUE, not FALSE)\", \"IS NULL and IS NOT NULL operators correctly return TRUE or FALSE (never NULL) for NULL values\", \"OP_MakeRecord serializes column values from consecutive registers into a B-tree record format\", \"OP_Insert inserts a serialized record into the B-tree at the specified rowid position\", \"OP_NewRowid generates a new unique rowid for auto-increment columns when NULL is provided\", \"OP_Delete removes the current row from the B-tree\", \"UPDATE execution implements delete + insert pattern: old row deleted, new row with modified columns inserted\", \"UPDATE rejects attempts to modify the rowid (INTEGER PRIMARY KEY) column\", \"DELETE execution uses two-pass algorithm: first pass collects rowids, second pass deletes by rowid to avoid cursor corruption\", \"NOT NULL constraint enforcement rejects INSERT or UPDATE that sets a NOT NULL column to NULL\", \"UNIQUE constraint enforcement rejects INSERT or UPDATE that would create duplicate values\", \"Table lookup failure returns error with table name: 'no such table: users'\", \"Error messages include constraint name and column name: 'NOT NULL constraint failed: users.name'\", \"Test suite validates NULL = NULL returns no rows (NULL comparison)\", \"Test suite validates IS NULL correctly matches NULL values\", \"Test suite validates NULL propagation through arithmetic and string operations\", \"Test suite validates three-valued logic for AND, OR, NOT with NULL operands\", \"Test suite validates projection returns only requested columns in correct order\", \"Test suite validates INSERT followed by SELECT returns inserted data\", \"Test suite validates UPDATE modifies only specified columns, preserves others\", \"Test suite validates DELETE removes only rows matching WHERE clause\", \"Test suite validates NOT NULL constraint rejection with descriptive error\", \"Test suite validates operations on non-existent tables return appropriate error\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m7 -->\n# Secondary Indexes: When O(n) Becomes O(log n)\nYour database can store millions of rows. It can execute SELECT, INSERT, UPDATE, and DELETE. But there's a problem hiding in plain sight.\nConsider a table with one million users:\n```sql\nSELECT * FROM users WHERE email = 'alice@example.com';\n```\nYour current implementation performs a **full table scan**. It reads every single row, checks if `email` matches, and returns the one that does. For a million-row table spread across thousands of B-tree pages, this means reading thousands of pages from disk to find one row.\nOne row. Thousands of page reads. This isn't slow \u2014 it's catastrophic.\nThe solution is **secondary indexes**: auxiliary B+trees that map column values to row locations. With an index on `email`, the same query reads perhaps 3-4 pages instead of thousands. The difference isn't incremental \u2014 it's the difference between a database that works and one that doesn't.\nBut here's what most learners miss: **indexes are not free**. Every INSERT, UPDATE, and DELETE must now update every index on the table. A table with ten indexes requires eleven B-tree operations per insert (one for the table, ten for the indexes). Indexes are a trade-off \u2014 faster reads in exchange for slower writes.\n---\n## The Tension: The Write-Read Trade-off\nLet's make this concrete with numbers.\n### The Read Cost Without Indexes\nA B-tree with 1 million rows, assuming 100 rows per leaf page:\n- **Leaf pages**: 10,000 pages\n- **Tree height**: 3-4 levels\n- **Full table scan**: 10,000 page reads\nA query like `WHERE email = 'alice@example.com'` that matches one row still reads all 10,000 leaf pages. The cost is **O(n)** where n is the number of rows.\n### The Read Cost With Index\nCreate an index on `email`:\n```sql\nCREATE INDEX idx_email ON users(email);\n```\nNow the same query:\n1. **Index lookup**: Traverse the index B+tree to find `email = 'alice@example.com'`\n2. **Get rowid**: The index leaf contains `(email, rowid)` pairs\n3. **Table lookup**: Use the rowid to fetch the actual row from the table B-tree\n{{DIAGRAM:diag-index-structure}}\nFor a 1 million row index with the same fanout:\n- **Index leaf pages**: ~10,000 (but each entry is smaller, so maybe 20,000 entries per page \u2192 50 leaf pages)\n- **Index tree height**: 2-3 levels\n- **Index traversal**: ~3 page reads\n- **Table lookup**: ~3 page reads (separate B-tree)\n- **Total**: ~6 page reads\n6 page reads vs 10,000 page reads. That's a **1,666x improvement**.\n### The Write Cost With Index\nBut now consider an INSERT:\n```sql\nINSERT INTO users (email, name) VALUES ('bob@example.com', 'Bob');\n```\nWithout indexes:\n- **1 B-tree operation**: Insert into table B-tree\nWith one index on `email`:\n- **2 B-tree operations**: Insert into table, insert into index\n- **Write amplification**: 2x\nWith indexes on `email`, `name`, `created_at`, `status`:\n- **5 B-tree operations**: Insert into table + 4 indexes\n- **Write amplification**: 5x\n\n![INSERT: Index Maintenance](./diagrams/diag-index-maintenance.svg)\n\nFor a table with 10 indexes, every write becomes 11x more expensive. This is the **write-read trade-off**: indexes make reads faster but writes slower.\n### The Hidden Cost: Index Maintenance\nThe cost isn't just the extra B-tree insert. It's also:\n1. **Lock contention**: Each index is a separate B-tree, requiring separate locks\n2. **Buffer pool pressure**: Hot indexes compete for buffer pool frames\n3. **Checkpoint overhead**: WAL mode must checkpoint more dirty pages\n4. **Fragmentation**: Many small inserts into different B-trees cause page splits\nThis is why production databases don't index everything. The query planner's job is to know when an index **hurts more than it helps** \u2014 and sometimes, a full table scan IS the optimal choice.\n---\n## The Revelation: Indexes Are Not Sorted Copies\nHere's the misconception that will lead you astray: *An index is just a sorted copy of the column data. Create one for every column and queries will be fast.*\n**No.** An index is a **completely separate data structure** with its own B-tree, its own pages, and its own maintenance cost. Let's trace through what actually happens.\n### What CREATE INDEX Actually Does\nWhen you execute:\n```sql\nCREATE INDEX idx_email ON users(email);\n```\nYour database:\n1. **Allocates a new root page** for the index B+tree\n2. **Scans the entire table**, reading every row\n3. **Extracts the indexed column** (email) from each row\n4. **Inserts into the index B+tree**: `(email_value, rowid)` pairs\n5. **Records metadata** in sqlite_master\nThis is an **O(n log n)** operation for a table with n rows. For a million-row table, that's millions of B-tree operations.\n### The Index B+tree Structure\n{{DIAGRAM:diag-btree-vs-bplustree}}\nUnlike the table B-tree (which stores complete rows in all nodes), an index B+tree:\n- **Internal nodes** store only separator keys and child pointers\n- **Leaf nodes** store `(key, rowid)` pairs\n- **Leaves are linked** for range scans\n```\nIndex B+tree on users.email:\n                    ['m' separator]\n                       /        \\\n            ['d']                 ['t']\n            /    \\                 /    \\\n['alice@...' \u2192 1] ['bob@...' \u2192 2] ['sam@...' \u2192 4] ['zoe@...' \u2192 5]\n        \u2193__________________\u2193__________________\u2193________________\u2193\n                    (linked leaves for range scans)\n```\nThe leaf contains:\n- `email` value (the key)\n- `rowid` (the pointer back to the table)\nWhen you query `WHERE email = 'bob@example.com'`:\n1. Traverse the index B+tree to find the leaf containing 'bob@...'\n2. Extract the rowid (2 in this example)\n3. Traverse the **table** B-tree using rowid 2 to get the full row\nThis is the **double lookup** pattern: index \u2192 rowid \u2192 table.\n\n![Index-to-Table Double Lookup](./diagrams/diag-double-lookup.svg)\n\n### Why B+tree, Not B-tree?\nYou might wonder: why do indexes use B+trees (data only in leaves) while tables use B-trees (data in all nodes)?\n**Answer: fanout and range scans.**\nAn internal node in a B+tree stores only keys and child pointers. For a 4KB page:\n- Key (email string): ~25 bytes average\n- Child pointer: 4 bytes\n- Entries per internal node: ~140\nAn internal node in a B-tree stores complete rows. For a 4KB page:\n- Complete row: ~100 bytes average\n- Entries per internal node: ~40\nHigher fanout means:\n- **Shallower trees**: 140^3 = 2.7M rows in a 3-level B+tree vs 40^3 = 64K rows in a 3-level B-tree\n- **Fewer disk reads**: Each level traversal is one page read\nFor range scans (e.g., `WHERE email BETWEEN 'a' AND 'z'`), B+trees are even better:\n- Find the first matching leaf\n- Follow sibling pointers through all matching leaves\n- No need to climb back up the tree\n---\n## The Double Lookup Pattern\nHere's where performance gets interesting. Consider:\n```sql\nSELECT email FROM users WHERE email = 'alice@example.com';\n```\nYou only need `email`, which is already in the index. Do you need to look up the table?\n**No.** This is a **covering index** scan \u2014 the index contains all requested columns.\nBut for:\n```sql\nSELECT * FROM users WHERE email = 'alice@example.com';\n```\nYou need all columns. The index only has `email` and `rowid`. You **must** look up the table.\n### When to Use the Index\nThe query planner (next milestone) decides between:\n1. **Full table scan**: Read all pages, filter rows\n2. **Index scan + table lookup**: Read index, then look up matching rows in table\n3. **Covering index scan**: Read index only (if all columns are in the index)\n\n![Cost Model: Table Scan vs Index Scan](./diagrams/diag-cost-comparison.svg)\n\nThe decision depends on **selectivity**: what fraction of rows match the predicate?\n| Selectivity | Rows Matching (1M total) | Table Scan Pages | Index + Lookup Pages | Better Choice |\n|-------------|--------------------------|------------------|---------------------|---------------|\n| 0.001% (10 rows) | 10 | 10,000 | ~60 | Index |\n| 1% (10K rows) | 10,000 | 10,000 | ~60,000 | Table scan |\n| 10% (100K rows) | 100,000 | 10,000 | ~600,000 | Table scan |\n**The surprise**: for low-selectivity queries (many matching rows), a full table scan is **faster** than an index scan. This is because:\n- Sequential I/O (table scan) is faster than random I/O (many row lookups)\n- The double lookup cost dominates when you're looking up many rows\nThe rule of thumb: if more than ~10-20% of rows match, a table scan is usually faster.\n---\n## CREATE INDEX Implementation\nNow let's build it.\n### Index Metadata\nFirst, track indexes in the system catalog:\n```sql\n-- sqlite_master already stores index metadata\nINSERT INTO sqlite_master (type, name, tbl_name, rootpage, sql)\nVALUES ('index', 'idx_email', 'users', 42, 'CREATE INDEX idx_email ON users(email)');\n```\n### CREATE INDEX Execution\n```c\nvoid execute_create_index(Database* db, ASTNode* create_index) {\n    const char* index_name = create_index->data.create_index.index_name;\n    const char* table_name = create_index->data.create_index.table_name;\n    const char* column_name = create_index->data.create_index.column_name;\n    bool is_unique = create_index->data.create_index.is_unique;\n    // Step 1: Look up the table\n    Table* table = lookup_table(db, table_name);\n    if (table == NULL) {\n        db_error(db, \"no such table: %s\", table_name);\n        return;\n    }\n    // Step 2: Find the column index\n    int col_index = table_find_column(table, column_name);\n    if (col_index < 0) {\n        db_error(db, \"no such column: %s.%s\", table_name, column_name);\n        return;\n    }\n    // Step 3: Allocate root page for the index\n    PageId root_page = allocate_page(db->buffer_pool);\n    // Step 4: Initialize the root page as an index leaf\n    Frame* frame = fetch_page(db->buffer_pool, root_page);\n    BTreePage* page = (BTreePage*)frame->data;\n    init_index_leaf_page(page);\n    mark_dirty(db->buffer_pool, frame);\n    unpin_page(db->buffer_pool, frame);\n    // Step 5: Scan the table and insert into the index\n    Cursor cursor;\n    cursor_init(&cursor, db->buffer_pool, table->root_page);\n    cursor_first(&cursor);\n    while (!cursor.eof) {\n        Row* row = cursor_get_row(&cursor);\n        ColumnValue* col_value = &row->columns[col_index];\n        // Build index key: (column_value, rowid)\n        IndexEntry entry;\n        entry.key = *col_value;\n        entry.rowid = row->rowid;\n        // Insert into index B+tree\n        int result = btree_insert_index(db->buffer_pool, root_page, &entry);\n        if (result == ERROR_UNIQUE_CONSTRAINT && is_unique) {\n            db_error(db, \"UNIQUE constraint failed during index creation: duplicate value in column %s\", \n                     column_name);\n            free_row(row);\n            return;\n        }\n        free_row(row);\n        cursor_next(&cursor);\n    }\n    // Step 6: Record index metadata\n    char* sql = generate_create_index_sql(create_index);\n    insert_into_sqlite_master(db, \"index\", index_name, table_name, root_page, sql);\n    free(sql);\n    // Step 7: Store index info in memory\n    Index* index = malloc(sizeof(Index));\n    index->name = strdup(index_name);\n    index->table_name = strdup(table_name);\n    index->root_page = root_page;\n    index->column_index = col_index;\n    index->is_unique = is_unique;\n    add_index_to_table(db, table, index);\n}\n```\n### Index Entry Serialization\nFor the index B+tree, leaf cells contain `(key, rowid)` pairs:\n```c\ntypedef struct {\n    ColumnValue key;    // The indexed column value\n    int64_t rowid;      // Pointer to the table row\n} IndexEntry;\nint serialize_index_entry(IndexEntry* entry, uint8_t* buffer) {\n    // Payload format: [key_value] [rowid]\n    // The key is serialized using the same record format as table rows\n    uint8_t* ptr = buffer;\n    // Serialize the key (single column)\n    int key_size = serialize_column_value(&entry->key, ptr);\n    ptr += key_size;\n    // Append the rowid (varint)\n    int rowid_size = write_varint(ptr, entry->rowid);\n    ptr += rowid_size;\n    return ptr - buffer;\n}\nint deserialize_index_entry(const uint8_t* buffer, int size, IndexEntry* entry) {\n    const uint8_t* ptr = buffer;\n    // Deserialize the key\n    int key_size = deserialize_column_value(ptr, &entry->key);\n    ptr += key_size;\n    // Deserialize the rowid\n    uint64_t rowid;\n    int rowid_size = read_varint(ptr, &rowid);\n    entry->rowid = (int64_t)rowid;\n    return key_size + rowid_size;\n}\n```\n### Index B+tree Insertion\nThe index B+tree insertion is similar to table B-tree, but with different cell format:\n```c\nint btree_insert_index(BufferPool* pool, PageId root_page, IndexEntry* entry) {\n    // Traverse to find the correct leaf\n    PageId current = root_page;\n    PageId* path = malloc(MAX_TREE_DEPTH * sizeof(PageId));\n    int path_depth = 0;\n    while (true) {\n        Frame* frame = fetch_page(pool, current);\n        BTreePage* page = (BTreePage*)frame->data;\n        path[path_depth++] = current;\n        if (is_leaf_page(page)) {\n            // Found the leaf\n            unpin_page(pool, frame);\n            break;\n        }\n        // Internal page: find the child to descend into\n        int child_index = find_child_for_key(page, &entry->key);\n        PageId child_page = get_child_page(page, child_index);\n        unpin_page(pool, frame);\n        current = child_page;\n    }\n    // Insert into the leaf\n    PageId leaf_page = path[path_depth - 1];\n    int result = insert_into_index_leaf(pool, leaf_page, entry);\n    // Handle splits propagating up the tree\n    if (result == ERROR_PAGE_FULL) {\n        result = split_and_propagate_index(pool, path, path_depth, entry);\n    }\n    free(path);\n    return result;\n}\n```\n---\n## Index Maintenance on DML\nEvery INSERT, UPDATE, and DELETE must now maintain all indexes on the table.\n### INSERT with Index Maintenance\n```c\nvoid execute_insert_with_indexes(Database* db, Table* table, ColumnValue* values, int count) {\n    // Step 1: Insert into the table B-tree (as before)\n    int64_t rowid = generate_rowid(db, table);\n    int result = btree_insert_table(db->buffer_pool, table->root_page, values, count, rowid);\n    if (result != 0) {\n        db_error(db, \"insert failed: %s\", error_message(result));\n        return;\n    }\n    // Step 2: Insert into all indexes\n    for (int i = 0; i < table->index_count; i++) {\n        Index* index = table->indexes[i];\n        ColumnValue* indexed_value = &values[index->column_index];\n        IndexEntry entry;\n        entry.key = *indexed_value;\n        entry.rowid = rowid;\n        result = btree_insert_index(db->buffer_pool, index->root_page, &entry);\n        if (result == ERROR_UNIQUE_CONSTRAINT) {\n            // Rollback the table insert! (In reality, use transactions)\n            btree_delete_by_rowid(db->buffer_pool, table->root_page, rowid);\n            db_error(db, \"UNIQUE constraint failed: %s.%s\", \n                     table->name, table->columns[index->column_index].name);\n            return;\n        }\n    }\n}\n```\n\n![INSERT: Index Maintenance](./diagrams/diag-index-maintenance.svg)\n\n### DELETE with Index Maintenance\n```c\nvoid execute_delete_with_indexes(Database* db, Table* table, int64_t rowid) {\n    // Step 1: Read the row to get indexed values BEFORE deleting\n    Row* row = btree_fetch_row(db->buffer_pool, table->root_page, rowid);\n    if (row == NULL) {\n        return;  // Row doesn't exist\n    }\n    // Step 2: Delete from all indexes\n    for (int i = 0; i < table->index_count; i++) {\n        Index* index = table->indexes[i];\n        ColumnValue* indexed_value = &row->columns[index->column_index];\n        // Delete the index entry (key, rowid)\n        btree_delete_index_entry(db->buffer_pool, index->root_page, indexed_value, rowid);\n    }\n    // Step 3: Delete from the table\n    btree_delete_by_rowid(db->buffer_pool, table->root_page, rowid);\n    free_row(row);\n}\n```\n### UPDATE with Index Maintenance\nUPDATE is the most complex case because the indexed column might change:\n```c\nvoid execute_update_with_indexes(Database* db, Table* table, int64_t rowid, \n                                  int* col_indices, ColumnValue* new_values, int count) {\n    // Step 1: Read the old row\n    Row* old_row = btree_fetch_row(db->buffer_pool, table->root_page, rowid);\n    if (old_row == NULL) {\n        return;\n    }\n    // Step 2: Build the new row\n    Row* new_row = copy_row(old_row);\n    for (int i = 0; i < count; i++) {\n        copy_value(&new_row->columns[col_indices[i]], &new_values[i]);\n    }\n    // Step 3: Update indexes for any changed indexed columns\n    for (int i = 0; i < table->index_count; i++) {\n        Index* index = table->indexes[i];\n        int col_idx = index->column_index;\n        // Check if this indexed column is being updated\n        bool column_updated = false;\n        for (int j = 0; j < count; j++) {\n            if (col_indices[j] == col_idx) {\n                column_updated = true;\n                break;\n            }\n        }\n        if (!column_updated) {\n            continue;  // Index not affected\n        }\n        ColumnValue* old_value = &old_row->columns[col_idx];\n        ColumnValue* new_value = &new_row->columns[col_idx];\n        // Check for NULL changes\n        if (old_value->type == VALUE_NULL && new_value->type == VALUE_NULL) {\n            continue;  // Both NULL, no index change\n        }\n        // Delete old index entry\n        if (old_value->type != VALUE_NULL) {\n            IndexEntry old_entry = { *old_value, rowid };\n            btree_delete_index_entry(db->buffer_pool, index->root_page, &old_entry);\n        }\n        // Insert new index entry\n        if (new_value->type != VALUE_NULL) {\n            IndexEntry new_entry = { *new_value, rowid };\n            int result = btree_insert_index(db->buffer_pool, index->root_page, &new_entry);\n            if (result == ERROR_UNIQUE_CONSTRAINT) {\n                free_row(old_row);\n                free_row(new_row);\n                db_error(db, \"UNIQUE constraint failed: %s.%s\", \n                         table->name, table->columns[col_idx].name);\n                return;\n            }\n        }\n    }\n    // Step 4: Update the table (delete old, insert new)\n    btree_delete_by_rowid(db->buffer_pool, table->root_page, rowid);\n    btree_insert_table(db->buffer_pool, table->root_page, new_row->columns, \n                       new_row->column_count, rowid);\n    free_row(old_row);\n    free_row(new_row);\n}\n```\n---\n## Index Lookup: Equality Predicate\nNow for the payoff: using the index for fast lookups.\n### Index Lookup Bytecode\nFor `SELECT * FROM users WHERE email = 'alice@example.com'`:\n```\nOpenRead       cursor=0  table=users\nOpenRead       cursor=1  index=idx_email\nString8        'alice@example.com'  r1\nSeekGE         cursor=1  r1         r2       -- Seek to first key >= r1\nColumn         cursor=1  col=0      r3       -- Read key from index\nNe             r1        r3         END      -- If key != target, not found\nColumn         cursor=1  col=1      r4       -- Read rowid from index\nSeekRowid      cursor=0  r4                  -- Seek table to rowid\nColumn         cursor=0  col=id     r5       -- Read table columns\nColumn         cursor=0  col=name   r6\nColumn         cursor=0  col=email  r7\nResultRow      r5        3\nEND:\nHalt\n```\nThe key new opcodes:\n### SeekGE Opcode\n```c\ncase OP_SeekGE: {\n    int cursor_id = instr->p1;\n    int key_reg = instr->p2;\n    int result_reg = instr->p3;  // Optional: store comparison result\n    Cursor* cursor = &vm->cursors[cursor_id];\n    Value* key = &vm->registers[key_reg];\n    // Traverse the index B+tree to find key >= target\n    bool found = index_seek_ge(cursor, key);\n    if (!found) {\n        cursor->eof = true;\n    }\n    vm->pc++;\n    break;\n}\n```\n### SeekRowid Opcode\n```c\ncase OP_SeekRowid: {\n    int cursor_id = instr->p1;\n    int rowid_reg = instr->p2;\n    Cursor* cursor = &vm->cursors[cursor_id];\n    int64_t rowid = vm->registers[rowid_reg].data.integer_val;\n    // Seek the table B-tree to the specific rowid\n    bool found = btree_seek_rowid(cursor, rowid);\n    if (!found) {\n        cursor->eof = true;\n    }\n    vm->pc++;\n    break;\n}\n```\n### Index Seek Implementation\n```c\nbool index_seek_ge(Cursor* cursor, Value* key) {\n    PageId current = cursor->root_page;\n    while (true) {\n        Frame* frame = fetch_page(cursor->pool, current);\n        BTreePage* page = (BTreePage*)frame->data;\n        if (is_leaf_page(page)) {\n            // Binary search for key >= target\n            int cell_index = binary_search_leaf(page, key);\n            if (cell_index >= page->cell_count) {\n                // All keys < target, check right sibling\n                if (page->right_sibling != 0) {\n                    current = page->right_sibling;\n                    unpin_page(cursor->pool, frame);\n                    continue;\n                }\n                unpin_page(cursor->pool, frame);\n                return false;  // No key >= target\n            }\n            cursor->current_page = current;\n            cursor->current_cell_index = cell_index;\n            unpin_page(cursor->pool, frame);\n            return true;\n        }\n        // Internal page: find the child to descend into\n        int child_index = find_child_for_key(page, key);\n        PageId child_page = get_child_page(page, child_index);\n        unpin_page(cursor->pool, frame);\n        current = child_page;\n    }\n}\nint binary_search_leaf(BTreePage* page, Value* key) {\n    int lo = 0, hi = page->cell_count - 1;\n    while (lo <= hi) {\n        int mid = (lo + hi) / 2;\n        Cell* cell = get_cell(page, mid);\n        IndexEntry entry;\n        deserialize_index_entry(cell->payload, cell->payload_size, &entry);\n        int cmp = compare_values(&entry.key, key);\n        if (cmp < 0) {\n            lo = mid + 1;\n        } else if (cmp > 0) {\n            hi = mid - 1;\n        } else {\n            return mid;  // Exact match\n        }\n    }\n    return lo;  // First cell with key >= target\n}\n```\n---\n## Index Range Scan\nFor range predicates like `BETWEEN`, `<`, `>`, the index can scan multiple leaves:\n```sql\nSELECT * FROM users WHERE email BETWEEN 'a' AND 'm';\n```\n```\nOpenRead       cursor=0  table=users\nOpenRead       cursor=1  index=idx_email\nString8        'a'       r1\nString8        'm'       r2\nSeekGE         cursor=1  r1                  -- Seek to first key >= 'a'\nRANGE_LOOP:\n  Column       cursor=1  col=0      r3       -- Read key from index\n  Gt           r3        r2         END      -- If key > 'm', done\n  Column       cursor=1  col=1      r4       -- Read rowid\n  SeekRowid    cursor=0  r4\n  Column       cursor=0  col=id     r5\n  Column       cursor=0  col=name   r6\n  Column       cursor=0  col=email  r7\n  ResultRow    r5        3\n  Next         cursor=1  RANGE_LOOP         -- Next leaf entry (follows sibling pointers)\nEND:\nHalt\n```\nThe `Next` opcode on an index cursor follows the right-sibling pointer in the leaf page header, enabling efficient range scans.\n---\n## UNIQUE Index Constraint\nA UNIQUE index rejects duplicate values:\n```sql\nCREATE UNIQUE INDEX idx_email ON users(email);\n```\n### Enforcement During Insert\n```c\nint btree_insert_index_unique(BufferPool* pool, PageId root_page, IndexEntry* entry) {\n    // First, check if the key already exists\n    Cursor cursor;\n    cursor_init(&cursor, pool, root_page);\n    bool found = index_seek_ge(&cursor, &entry->key);\n    if (found) {\n        // Check if this is an exact match\n        IndexEntry existing;\n        index_cursor_get_entry(&cursor, &existing);\n        if (compare_values(&existing.key, &entry->key) == 0) {\n            // Duplicate key!\n            return ERROR_UNIQUE_CONSTRAINT;\n        }\n    }\n    // No duplicate, proceed with insert\n    return btree_insert_index(pool, root_page, entry);\n}\n```\n### NULL Handling in UNIQUE Indexes\nSQL has a quirk: `NULL != NULL` evaluates to NULL (unknown), not TRUE. This means:\n```sql\nINSERT INTO users (email) VALUES (NULL);\nINSERT INTO users (email) VALUES (NULL);  -- This is ALLOWED in most databases!\n```\nSQLite allows multiple NULLs in a UNIQUE index because NULL is \"unknown\" \u2014 we can't say two unknowns are equal, so we can't say they're duplicates.\nSome databases (PostgreSQL with `NULLS NOT DISTINCT`) treat NULLs as equal for uniqueness. Your implementation should document which behavior it chooses.\n```c\nint compare_for_uniqueness(Value* a, Value* b) {\n    // NULL values are not considered duplicates\n    if (a->type == VALUE_NULL && b->type == VALUE_NULL) {\n        return -1;  // Not equal (allow multiple NULLs)\n    }\n    return compare_values(a, b);\n}\n```\n---\n## Covering Index Optimization\nWhen all requested columns are in the index, you can skip the table lookup:\n```sql\n-- Requires table lookup\nSELECT * FROM users WHERE email = 'alice@example.com';\n-- Covering index scan (no table lookup needed)\nSELECT email FROM users WHERE email = 'alice@example.com';\n-- Also covering if we create: CREATE INDEX idx_email_name ON users(email, name)\nSELECT email, name FROM users WHERE email = 'alice@example.com';\n```\n### Composite Indexes\nA composite index on multiple columns:\n```sql\nCREATE INDEX idx_status_created ON users(status, created_at);\n```\nThis index can serve:\n- `WHERE status = 'active'` \u2713\n- `WHERE status = 'active' AND created_at > '2024-01-01'` \u2713\n- `WHERE created_at > '2024-01-01'` \u2717 (leftmost prefix rule)\nThe **leftmost prefix rule**: an index on (A, B, C) can serve queries on A, (A, B), or (A, B, C), but not B alone or C alone.\n```c\nbool index_can_serve_predicate(Index* index, ColumnRef* columns, int count) {\n    // Check if the query columns form a leftmost prefix of the index\n    for (int i = 0; i < count; i++) {\n        if (i >= index->column_count) {\n            return false;  // More query columns than index columns\n        }\n        if (columns[i].column_index != index->column_indices[i]) {\n            return false;  // Not a leftmost prefix\n        }\n    }\n    return true;\n}\n```\n---\n## The Three-Level View\n### Level 1: Application (SQL \u2192 Faster Query)\nAt the API level, indexes are invisible:\n```sql\nCREATE INDEX idx_email ON users(email);  -- One-time setup\nSELECT * FROM users WHERE email = 'alice@example.com';  -- Automatically faster\n```\nThe user doesn't know about B+trees, double lookups, or page counts. They just see faster queries.\n### Level 2: Engine (Index Scan vs Table Scan)\nInside the query executor:\n```c\nif (has_usable_index(query, table)) {\n    execute_index_scan(db, query, table, index);\n} else {\n    execute_table_scan(db, query, table);\n}\n```\nThe index scan path:\n1. Traverse index B+tree to find matching keys\n2. For each match, extract rowid\n3. Traverse table B-tree to fetch the row\n4. Output result\n### Level 3: Implementation (B+tree Operations)\nAt the lowest level, index operations are page manipulations:\n```c\n// Index leaf cell\nstruct {\n    varint payload_size;\n    uint8_t[] key_bytes;    // Serialized key\n    varint rowid;\n}\n// Index internal cell\nstruct {\n    uint32_t left_child_page;\n    varint payload_size;\n    uint8_t[] separator_key;\n}\n```\nThe correctness of your index depends on every byte being in the right place, just like the table B-tree.\n---\n## Common Pitfalls\n### Forgetting Index Maintenance on UPDATE\n```c\n// WRONG: Update table but forget to update index\nbtree_delete_by_rowid(pool, table->root_page, rowid);\nbtree_insert_table(pool, table->root_page, new_values, count, rowid);\n// Index still has old key value!\n// RIGHT: Update index entries for changed columns\nif (indexed_column_changed) {\n    btree_delete_index_entry(pool, index->root_page, old_key, rowid);\n    btree_insert_index(pool, index->root_page, new_key, rowid);\n}\n```\n### Not Handling NULL in Indexes\n```c\n// WRONG: Crash when indexed column is NULL\nint cmp = compare_values(&entry->key, &target_key);\n// RIGHT: Handle NULL keys specially\nif (entry->key.type == VALUE_NULL) {\n    // NULL sorts before all non-NULL values\n    return (target_key.type == VALUE_NULL) ? 0 : -1;\n}\nif (target_key.type == VALUE_NULL) {\n    return 1;\n}\nint cmp = compare_values(&entry->key, &target_key);\n```\n### Unique Constraint Check Race Condition\n```c\n// WRONG: Check and insert in separate operations\nbool exists = index_lookup(pool, root_page, key);\nif (!exists) {\n    btree_insert_index(pool, root_page, entry);  // Race! Another thread might have inserted\n}\n// RIGHT: Use atomic check-and-insert or locks\nlock_index(pool, root_page);\nbool exists = index_lookup(pool, root_page, key);\nif (!exists) {\n    btree_insert_index(pool, root_page, entry);\n}\nunlock_index(pool, root_page);\n```\n### Incorrect Selectivity Estimation\n```c\n// WRONG: Always use index if available\nif (has_index(table, column)) {\n    use_index = true;\n}\n// RIGHT: Estimate selectivity and compare costs\ndouble selectivity = estimate_selectivity(table, column, value);\ndouble index_cost = selectivity * table->row_count * INDEX_LOOKUP_COST;\ndouble table_scan_cost = table->page_count * SEQUENTIAL_READ_COST;\nuse_index = (index_cost < table_scan_cost);\n```\n---\n## Testing Strategy\n### Index Creation Tests\n```c\nvoid test_create_index() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, name TEXT)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'Alice')\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, 'Bob')\");\n    execute_sql(db, \"INSERT INTO test VALUES (3, 'Charlie')\");\n    int result = execute_sql(db, \"CREATE INDEX idx_name ON test(name)\");\n    assert(result == 0);\n    // Verify index exists in sqlite_master\n    Result* r = execute_sql(db, \"SELECT name FROM sqlite_master WHERE type='index' AND tbl_name='test'\");\n    assert(r->row_count == 1);\n    assert(strcmp(r->rows[0].columns[0].data.string_val, \"idx_name\") == 0);\n    free_result(r);\n    free_database(db);\n}\n```\n### Index Lookup Tests\n```c\nvoid test_index_lookup() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    execute_sql(db, \"CREATE INDEX idx_value ON test(value)\");\n    // Insert 1000 rows\n    for (int i = 0; i < 1000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, %d)\", i, i * 10);\n        execute_sql(db, sql);\n    }\n    // Reset page read counter\n    int pages_before = db->buffer_pool->misses;\n    // Lookup by indexed column\n    Result* r = execute_sql(db, \"SELECT * FROM test WHERE value = 500\");\n    int pages_read = db->buffer_pool->misses - pages_before;\n    assert(r->row_count == 1);\n    assert(r->rows[0].columns[0].data.integer_val == 50);  // id = 50 when value = 500\n    assert(pages_read < 20);  // Should read far fewer pages than full table scan\n    free_result(r);\n    free_database(db);\n}\n```\n### Index Maintenance Tests\n```c\nvoid test_index_maintained_on_insert() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, name TEXT)\");\n    execute_sql(db, \"CREATE INDEX idx_name ON test(name)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'Alice')\");\n    // New row should be findable via index\n    Result* r = execute_sql(db, \"SELECT * FROM test WHERE name = 'Alice'\");\n    assert(r->row_count == 1);\n    free_result(r);\n    free_database(db);\n}\nvoid test_index_maintained_on_delete() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, name TEXT)\");\n    execute_sql(db, \"CREATE INDEX idx_name ON test(name)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'Alice')\");\n    execute_sql(db, \"DELETE FROM test WHERE id = 1\");\n    // Deleted row should not be findable via index\n    Result* r = execute_sql(db, \"SELECT * FROM test WHERE name = 'Alice'\");\n    assert(r->row_count == 0);\n    free_result(r);\n    free_database(db);\n}\nvoid test_index_maintained_on_update() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, name TEXT)\");\n    execute_sql(db, \"CREATE INDEX idx_name ON test(name)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'Alice')\");\n    execute_sql(db, \"UPDATE test SET name = 'Bob' WHERE id = 1\");\n    // Old name should not be findable\n    Result* r1 = execute_sql(db, \"SELECT * FROM test WHERE name = 'Alice'\");\n    assert(r1->row_count == 0);\n    // New name should be findable\n    Result* r2 = execute_sql(db, \"SELECT * FROM test WHERE name = 'Bob'\");\n    assert(r2->row_count == 1);\n    free_result(r1);\n    free_result(r2);\n    free_database(db);\n}\n```\n### Unique Index Tests\n```c\nvoid test_unique_index_rejects_duplicate() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, email TEXT)\");\n    execute_sql(db, \"CREATE UNIQUE INDEX idx_email ON test(email)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'alice@example.com')\");\n    int result = execute_sql(db, \"INSERT INTO test VALUES (2, 'alice@example.com')\");\n    assert(result != 0);\n    assert(strstr(db->error_message, \"UNIQUE\") != NULL);\n    free_database(db);\n}\nvoid test_unique_index_allows_multiple_nulls() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, email TEXT)\");\n    execute_sql(db, \"CREATE UNIQUE INDEX idx_email ON test(email)\");\n    int r1 = execute_sql(db, \"INSERT INTO test VALUES (1, NULL)\");\n    int r2 = execute_sql(db, \"INSERT INTO test VALUES (2, NULL)\");\n    assert(r1 == 0);\n    assert(r2 == 0);  // Multiple NULLs allowed\n    free_database(db);\n}\n```\n### Range Scan Tests\n```c\nvoid test_index_range_scan() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    execute_sql(db, \"CREATE INDEX idx_value ON test(value)\");\n    for (int i = 0; i < 100; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, %d)\", i, i);\n        execute_sql(db, sql);\n    }\n    Result* r = execute_sql(db, \"SELECT * FROM test WHERE value BETWEEN 20 AND 30\");\n    assert(r->row_count == 11);  // 20, 21, ..., 30\n    // Verify order (index returns in key order)\n    for (int i = 0; i < 11; i++) {\n        assert(r->rows[i].columns[1].data.integer_val == 20 + i);\n    }\n    free_result(r);\n    free_database(db);\n}\n```\n---\n## What You've Built\nYour secondary index system provides:\n1. **CREATE INDEX** builds a B+tree index from existing table data\n2. **Index B+tree structure** stores (key, rowid) pairs with linked leaves\n3. **Automatic index maintenance** on INSERT, UPDATE, DELETE\n4. **Equality lookup** via index seek + table lookup (double lookup)\n5. **Range scan** via leaf traversal following sibling pointers\n6. **UNIQUE constraint** enforcement during index insert\n7. **NULL handling** in indexes (NULLs sort first, multiple NULLs allowed in UNIQUE)\nThis is the difference between a database that crawls and one that flies. A well-indexed query on a million-row table can execute in milliseconds instead of seconds.\n---\n## Knowledge Cascade\n### 1. Search Engine Inverted Indexes\nThe B+tree index you built is the same structure powering full-text search, just with different keys:\n- **Database index**: `email \u2192 rowid`\n- **Search engine index**: `word \u2192 [doc_id, position, ...]`\nBoth use ordered structures for fast lookup. Both trade write cost for read speed. The difference: search engines often use **postings lists** (sorted arrays of doc IDs) instead of B+trees, because postings are append-optimized and compress well.\nWhen you build a search engine, you're building a specialized index structure \u2014 the principles are identical.\n### 2. Covering Indexes in Query Optimization\nA **covering index** includes all columns needed by a query:\n```sql\nCREATE INDEX idx_covering ON users(email, name, created_at);\nSELECT email, name FROM users WHERE email = 'alice@example.com';\n-- This query never touches the table!\n```\nColumn-oriented databases (Snowflake, Redshift, ClickHouse) take this further: every column is essentially its own index. \"Covering index scan\" becomes the default execution mode.\n### 3. LSM-Tree Index Files\nLevelDB and RocksDB use **Log-Structured Merge-trees** instead of B-trees:\n- **B-tree index**: In-place updates, good for reads, expensive writes\n- **LSM-tree index**: Append-only writes, good for writes, multiple levels to read\nBoth solve the same problem (ordered key-value storage) with opposite tradeoffs. Understanding B-tree indexes reveals why LSM-trees exist: to optimize for write-heavy workloads where B-tree write amplification is prohibitive.\n### 4. Hash Indexes vs B-tree Indexes\nSome databases offer **hash indexes** for equality predicates:\n| Aspect | B-tree Index | Hash Index |\n|--------|--------------|------------|\n| Lookup | O(log n) | O(1) |\n| Range scan | \u2713 Supported | \u2717 Not supported |\n| Ordering | Preserved | Random |\n| Collisions | None | Possible |\n| Use case | General purpose | Exact equality only |\nPostgreSQL has both. Redis uses hash tables exclusively. The choice depends on your access pattern: if you never need range scans, hash indexes are faster.\n### 5. Database Statistics and Cost Estimation\nThe query planner (next milestone) needs to know **how many rows match** a predicate to choose between index scan and table scan. This is **cardinality estimation**:\n```sql\n-- How many rows match?\nSELECT COUNT(*) FROM users WHERE email LIKE '%gmail.com';\n```\nThe database maintains statistics (histograms, distinct value counts) to estimate this without scanning. Your index makes these statistics more accurate \u2014 the index knows how many distinct values exist.\n---\n## What's Next\nWith secondary indexes, your database can find rows efficiently. But who decides when to use an index? Right now, your executor always uses an index if one exists \u2014 even when a table scan would be faster.\nThe next milestone, **Query Planner & Statistics**, builds the brain that makes these decisions. It estimates costs, compares alternatives, and chooses the optimal execution plan. Without it, indexes are a blunt instrument. With it, they become a precision tool.\nYour database can store, retrieve, and index data. Now it needs to think.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m7\", \"criteria\": [\"CREATE INDEX builds a B+tree index structure with a new root page recorded in sqlite_master (type='index')\", \"Index leaf cells contain serialized (key, rowid) pairs where key is the indexed column value\", \"Index internal cells contain (left_child_page, separator_key) for tree traversal\", \"Index B+tree leaves are linked via right_sibling pointers in the page header for range scans\", \"CREATE INDEX scans the entire table, extracting indexed column values and inserting into the index B+tree\", \"INSERT operations maintain all table indexes by inserting (key, rowid) entries into each index B+tree after the table insert\", \"DELETE operations maintain all indexes by reading the row before deletion and removing corresponding index entries\", \"UPDATE operations detect when indexed columns change, delete old index entries, and insert new ones for affected indexes only\", \"Index equality lookup uses SeekGE opcode to find the first matching key, then verifies exact match\", \"Index range scan uses SeekGE to find start of range, then Next opcode to traverse linked leaves until end of range\", \"SeekRowid opcode performs table B-tree lookup using rowid extracted from index entry (double lookup pattern)\", \"UNIQUE index constraint checks for existing key before insert and returns ERROR_UNIQUE_CONSTRAINT on duplicate\", \"UNIQUE index allows multiple NULL values (NULL is not considered equal to NULL for uniqueness)\", \"Index lookup on a 1000-row table with selective predicate (1 matching row) reads fewer than 20 pages, verified by buffer pool miss counter\", \"Full table scan on same 1000-row table reads significantly more pages than index lookup (at least 10x difference)\", \"Test suite validates CREATE INDEX populates index with correct (key, rowid) pairs from existing table data\", \"Test suite validates index maintenance on INSERT (new rows findable via index)\", \"Test suite validates index maintenance on DELETE (deleted rows not findable via index)\", \"Test suite validates index maintenance on UPDATE (old key not findable, new key findable)\", \"Test suite validates UNIQUE index rejects duplicate non-NULL values with appropriate error message\", \"Test suite validates UNIQUE index allows multiple NULL values\", \"Test suite validates range scan returns all matching rows in key order\", \"Test suite validates equality lookup returns exactly one row when unique key matches\", \"Bytecode for index scan includes OpenRead for both index and table cursors, SeekGE/SeekRowid opcodes, and comparison to verify match\"]}] ]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m8 -->\n# Query Planner & Statistics: The Art of Educated Guessing\nYour database can execute SQL. It can scan tables, traverse indexes, and join data. But here's a uncomfortable truth: **there are many ways to execute the same query, and most of them are terrible.**\nConsider this innocent-looking query:\n```sql\nSELECT * FROM orders WHERE customer_id = 42 AND total > 1000;\n```\nYour database could execute this in several ways:\n1. **Full table scan**: Read every order, filter both conditions\n2. **Index on customer_id**: Find customer 42's orders, filter by total\n3. **Index on total**: Find orders > $1000, filter by customer_id\n4. **Index intersection**: Find customer 42's orders AND orders > $1000, intersect\nWhich is fastest? It depends entirely on your data:\n- If customer 42 has 3 orders, the customer_id index wins\n- If only 5 orders exceed $1000, the total index wins\n- If customer 42 has 50,000 orders, a table scan might be faster than 50,000 index lookups\nThe query planner's job is to choose. Not randomly, not based on rules of thumb, but based on a **cost model** that estimates which plan will execute fastest.\nHere's the problem: **the planner doesn't know your data.** It has to guess. And those guesses are often wrong.\n---\n## The Tension: Optimization Under Uncertainty\nThe query planner operates under a fundamental constraint: **it must make decisions without perfect information.**\nWhen you execute a query, the planner has milliseconds to choose an execution plan. It cannot read your entire table to understand the data distribution. It cannot run experiments to measure actual costs. It must decide based on **statistics** \u2014 summary information collected previously.\n### The Information Gap\nConsider what the planner needs to know for `WHERE customer_id = 42`:\n1. **How many rows have customer_id = 42?** (selectivity)\n2. **How many distinct customer_ids exist?** (distribution)\n3. **Are customer_ids evenly distributed or skewed?** (histogram)\n4. **How many pages does the index span?** (physical layout)\n5. **What's the cache hit rate for this index?** (runtime state)\nThe planner knows some of this from statistics. It guesses the rest. And the guesses compound:\n\n![Selectivity Estimation](./diagrams/diag-selectivity-estimation.svg)\n\n```\nEstimated selectivity for customer_id = 42: 0.01% (1 in 10,000)\nActual selectivity: 15% (customer 42 is a major account)\nError: 1500x\n```\nA 1500x error in cardinality estimation might not matter for a single-table query. But for a join:\n```sql\nSELECT * FROM orders o\nJOIN order_items i ON o.id = i.order_id\nWHERE o.customer_id = 42;\n```\nIf the planner underestimates orders for customer 42 by 1500x, it will:\n1. Choose a nested loop join (good for few rows)\n2. Actually iterate over 1500x more rows than expected\n3. Execute 1500x more index lookups\n4. Run for minutes instead of milliseconds\n**Cardinality estimation errors compound through joins.** A 10x error in one table becomes 100x after joining with another table. This is why query performance can suddenly collapse when data distributions change.\n### The Time Budget\nThe planner has another constraint: **it must decide quickly.**\nFor a simple query that executes in 1 millisecond, spending 100 milliseconds planning is unacceptable. The planner must find a \"good enough\" plan within a strict time budget.\nThis is why production databases use **heuristic optimization** rather than exhaustive search:\n- They don't consider all possible join orderings (n! possibilities for n tables)\n- They don't try every index combination\n- They prune the search space aggressively\nThe planner doesn't find the *optimal* plan. It finds the **least-bad plan it can discover quickly**.\n---\n## The Revelation: Optimizers Are Guessing Machines\nHere's the misconception that will lead you astray: *The query optimizer picks the best plan. If my query is slow, I wrote the wrong SQL.*\n**No.** The optimizer is a guessing machine operating under uncertainty. Let's trace through what actually happens.\n### What Statistics Actually Tell You\nWhen you run `ANALYZE`, your database collects:\n```c\ntypedef struct {\n    int64_t row_count;           // Total rows in table\n    int32_t page_count;          // Total pages\n    // Per-column statistics\n    int32_t distinct_count;      // Number of distinct values\n    ColumnValue min_value;       // Minimum value (for ordered types)\n    ColumnValue max_value;       // Maximum value\n    // For indexed columns\n    int32_t* histogram_buckets;  // Value distribution (optional)\n    double avg_column_width;     // Average size in bytes\n} ColumnStatistics;\n```\nThis is **summary information**, not complete knowledge. From \"customer_id has 10,000 distinct values and the table has 1,000,000 rows\", the planner estimates:\n- Average selectivity: 1/10,000 = 0.01%\n- Expected rows matching `customer_id = X`: 100 rows\nBut this assumes **uniform distribution**. What if:\n- Customer 1 has 500,000 orders (50% of the table!)\n- The remaining 9,999 customers average 50 orders each\nThe statistics don't capture this skew. The planner's 100-row estimate for `customer_id = 1` is off by 5000x.\n### Why Production Databases Have Optimizer Hints\nEver wonder why databases have syntax like:\n```sql\nSELECT /*+ INDEX(orders idx_customer) */ * FROM orders WHERE customer_id = 42;\n```\nThis is an **optimizer hint** \u2014 a way to override the planner's decision. It exists because **sometimes the human knows better than the cost model.**\nThe planner's statistics might be stale. Its assumptions might be wrong. Its cost model might not match reality. When the planner chooses poorly, hints let you force the right plan.\nThis is the dirty secret of query optimization: **the optimizer is not omniscient.** It's an educated guesser that's right most of the time but catastrophically wrong sometimes.\n---\n## The Cost Model: Translating Plans to Numbers\nThe planner's core mechanism is the **cost model**: a function that estimates the execution cost of a plan.\n### Cost Components\n```c\ntypedef struct {\n    double cpu_cost;      // CPU operations (comparisons, function calls)\n    double io_cost;       // Disk I/O (page reads)\n    double memory_cost;   // Memory usage (sorting, hashing)\n    double network_cost;  // Network transfer (distributed only)\n} PlanCost;\n```\nFor a single-node database like SQLite, I/O cost dominates. A simple model:\n```c\ndouble estimate_table_scan_cost(Table* table) {\n    // Sequential I/O is faster than random I/O\n    double seq_io_cost = table->page_count * SEQ_IO_COST_PER_PAGE;\n    double cpu_cost = table->row_count * CPU_COST_PER_ROW_FILTER;\n    return seq_io_cost + cpu_cost;\n}\ndouble estimate_index_scan_cost(Index* index, double selectivity, Table* table) {\n    // Index traversal: log_fanout(tree_size) page reads\n    double index_traversal_cost = log(index->page_count) / log(INDEX_FANOUT);\n    // Matching entries\n    int64_t matching_rows = (int64_t)(table->row_count * selectivity);\n    // Random I/O for table lookups (one per matching row)\n    double table_lookup_cost = matching_rows * RANDOM_IO_COST_PER_PAGE;\n    // Index leaf scans are sequential\n    double index_leaf_pages = (matching_rows / ENTRIES_PER_LEAF_PAGE);\n    double index_scan_cost = index_leaf_pages * SEQ_IO_COST_PER_PAGE;\n    return index_traversal_cost + table_lookup_cost + index_scan_cost;\n}\n```\n### Real Cost Constants\nWhere do `SEQ_IO_COST_PER_PAGE` and `RANDOM_IO_COST_PER_PAGE` come from? From measurement:\n| Operation | Latency | Relative Cost |\n|-----------|---------|---------------|\n| Sequential page read (SSD) | 0.025 ms | 1.0 (baseline) |\n| Random page read (SSD) | 0.1 ms | 4.0 |\n| Sequential page read (HDD) | 0.5 ms | 20.0 |\n| Random page read (HDD) | 10 ms | 400.0 |\n| CPU comparison | 0.000001 ms | 0.00004 |\n| Row deserialization | 0.001 ms | 0.04 |\nOn SSD, random I/O is 4x sequential. On HDD, it's 20x. This is why:\n- **SSD-friendly plans**: Many random lookups are acceptable\n- **HDD-friendly plans**: Sequential scans preferred even for moderate selectivity\n\n![Cost Model: Table Scan vs Index Scan](./diagrams/diag-cost-comparison.svg)\n\n### The Crossover Point\nGiven these costs, there's a **selectivity threshold** where index scan becomes more expensive than table scan:\n```c\ndouble find_crossover_selectivity(Table* table, Index* index) {\n    // table_scan_cost = page_count * seq_io_cost\n    double table_scan = table->page_count * SEQ_IO_COST;\n    // index_scan_cost = traversal + (matching_rows * random_io)\n    // At crossover: table_scan = index_scan\n    // table_scan = log(pages) * seq_io + selectivity * rows * random_io\n    // Solve for selectivity:\n    double traversal = log(index->page_count) / log(INDEX_FANOUT) * SEQ_IO_COST;\n    double remaining_budget = table_scan - traversal;\n    // remaining_budget = selectivity * rows * random_io\n    double selectivity = remaining_budget / (table->row_count * RANDOM_IO_COST);\n    return selectivity;\n}\n```\nFor a 100,000-row, 1,000-page table on SSD:\n- Table scan cost: 1000 * 0.025 = 25 ms\n- Crossover selectivity: ~6%\nIf more than 6% of rows match, a table scan is faster. This is the **magic number** your planner uses \u2014 though it varies by hardware, row size, and index structure.\n---\n## Statistics Collection: ANALYZE\nBefore the planner can estimate costs, it needs statistics. The `ANALYZE` command collects them.\n### What ANALYZE Does\n```sql\nANALYZE;                    -- Analyze all tables\nANALYZE users;              -- Analyze specific table\nANALYZE users(email);       -- Analyze specific column\n```\nFor each analyzed object:\n```c\nvoid analyze_table(Database* db, Table* table) {\n    TableStatistics stats;\n    stats.row_count = 0;\n    stats.page_count = 0;\n    // Count rows and pages via B-tree scan\n    Cursor cursor;\n    cursor_init(&cursor, db->buffer_pool, table->root_page);\n    cursor_first(&cursor);\n    while (!cursor.eof) {\n        stats.row_count++;\n        cursor_next(&cursor);\n    }\n    stats.page_count = count_btree_pages(db->buffer_pool, table->root_page);\n    // Store statistics\n    store_table_statistics(db, table->name, &stats);\n    // Analyze each index\n    for (int i = 0; i < table->index_count; i++) {\n        analyze_index(db, table, table->indexes[i]);\n    }\n}\nvoid analyze_index(Database* db, Table* table, Index* index) {\n    IndexStatistics stats;\n    stats.distinct_count = 0;\n    stats.null_count = 0;\n    stats.min_value = NULL_VALUE;\n    stats.max_value = NULL_VALUE;\n    // Scan the index to collect statistics\n    Cursor cursor;\n    cursor_init(&cursor, db->buffer_pool, index->root_page);\n    cursor_first(&cursor);\n    ColumnValue prev_value = NULL_VALUE;\n    bool first = true;\n    while (!cursor.eof) {\n        IndexEntry entry;\n        index_cursor_get_entry(&cursor, &entry);\n        // Count NULLs\n        if (entry.key.type == VALUE_NULL) {\n            stats.null_count++;\n        } else {\n            // Track distinct values\n            if (first || compare_values(&entry.key, &prev_value) != 0) {\n                stats.distinct_count++;\n                prev_value = entry.key;\n                first = false;\n            }\n            // Track min/max\n            if (stats.min_value.type == VALUE_NULL || \n                compare_values(&entry.key, &stats.min_value) < 0) {\n                stats.min_value = entry.key;\n            }\n            if (stats.max_value.type == VALUE_NULL || \n                compare_values(&entry.key, &stats.max_value) > 0) {\n                stats.max_value = entry.key;\n            }\n        }\n        cursor_next(&cursor);\n    }\n    stats.page_count = count_btree_pages(db->buffer_pool, index->root_page);\n    // Store statistics\n    store_index_statistics(db, index->name, &stats);\n}\n```\n### Storing Statistics\nStatistics are stored in a system table (SQLite uses `sqlite_stat1`):\n```sql\nCREATE TABLE sqlite_stat1 (\n    tbl TEXT,    -- Table name\n    idx TEXT,    -- Index name (NULL for table)\n    stat TEXT    -- Encoded statistics\n);\n```\nThe `stat` column encodes multiple values:\n```\n\"100000 500 1\"  -- 100,000 rows, 500 distinct values in first index column,\n                 -- 1 distinct value in second (for composite indexes)\n```\n### Sampling for Large Tables\nFor tables with billions of rows, a full scan is too expensive. Production databases use **sampling**:\n```c\nvoid analyze_table_sampled(Database* db, Table* table, double sample_rate) {\n    int64_t sampled_rows = 0;\n    int64_t total_estimate = 0;\n    // Random sampling of pages\n    int sample_pages = (int)(table->page_count * sample_rate);\n    for (int i = 0; i < sample_pages; i++) {\n        int page_num = random() % table->page_count;\n        PageId page_id = get_page_by_number(table, page_num);\n        Frame* frame = fetch_page(db->buffer_pool, page_id);\n        BTreePage* page = (BTreePage*)frame->data;\n        sampled_rows += page->cell_count;\n        unpin_page(db->buffer_pool, frame);\n    }\n    // Extrapolate\n    total_estimate = (sampled_rows / sample_pages) * table->page_count;\n    // Store estimated statistics\n    // ...\n}\n```\nSampling introduces error but makes ANALYZE feasible for massive tables. A 1% sample is usually sufficient for reasonable estimates.\n---\n## Selectivity Estimation\nWith statistics in hand, the planner estimates **selectivity**: the fraction of rows matching a predicate.\n### Equality Predicate\n```c\ndouble estimate_equality_selectivity(ColumnStatistics* stats, ColumnValue* value) {\n    if (value->type == VALUE_NULL) {\n        // NULL comparison matches nothing (unless IS NULL)\n        return 0.0;\n    }\n    if (stats->distinct_count == 0) {\n        // No statistics; use default assumption\n        return DEFAULT_EQUALITY_SELECTIVITY;  // 0.1 (10%)\n    }\n    // Assume uniform distribution\n    return 1.0 / stats->distinct_count;\n}\n```\n### Range Predicate\n```c\ndouble estimate_range_selectivity(ColumnStatistics* stats, ColumnValue* low, \n                                   ColumnValue* high, bool include_low, bool include_high) {\n    if (stats->min_value.type == VALUE_NULL || stats->max_value.type == VALUE_NULL) {\n        // No statistics; use default\n        return DEFAULT_RANGE_SELECTIVITY;  // 0.33 (33%)\n    }\n    // Calculate what fraction of the value range is covered\n    double total_range = value_to_double(&stats->max_value) - \n                         value_to_double(&stats->min_value);\n    double pred_range;\n    if (low->type == VALUE_NULL && high->type == VALUE_NULL) {\n        return 1.0;  // No bounds = all rows\n    } else if (low->type == VALUE_NULL) {\n        // WHERE col < high\n        pred_range = value_to_double(high) - value_to_double(&stats->min_value);\n        if (!include_high) pred_range -= 1;  // Adjust for exclusive bound\n    } else if (high->type == VALUE_NULL) {\n        // WHERE col > low\n        pred_range = value_to_double(&stats->max_value) - value_to_double(low);\n        if (!include_low) pred_range -= 1;\n    } else {\n        // WHERE col BETWEEN low AND high\n        pred_range = value_to_double(high) - value_to_double(low);\n    }\n    double selectivity = pred_range / total_range;\n    // Clamp to [0, 1]\n    if (selectivity < 0) selectivity = 0;\n    if (selectivity > 1) selectivity = 1;\n    return selectivity;\n}\n```\n### AND Predicate\n```c\ndouble estimate_and_selectivity(double sel_a, double sel_b) {\n    // Assume independence: P(A AND B) = P(A) * P(B)\n    // This is often wrong but is the standard assumption\n    return sel_a * sel_b;\n}\n```\nThe independence assumption is the planner's Achilles' heel. If `status = 'active'` and `last_login > '2024-01-01'` are correlated (active users log in more), the actual selectivity might be much higher than the product.\n### OR Predicate\n```c\ndouble estimate_or_selectivity(double sel_a, double sel_b) {\n    // P(A OR B) = P(A) + P(B) - P(A AND B)\n    // Using independence for the intersection:\n    return sel_a + sel_b - (sel_a * sel_b);\n}\n```\n### Combining Selectivities\n```c\ndouble estimate_predicate_selectivity(Predicate* pred, TableStatistics* table_stats, \n                                       ColumnStatistics** col_stats) {\n    switch (pred->type) {\n        case PREDICATE_EQUALITY:\n            return estimate_equality_selectivity(\n                col_stats[pred->column_index], &pred->value);\n        case PREDICATE_RANGE:\n            return estimate_range_selectivity(\n                col_stats[pred->column_index], \n                &pred->range_low, &pred->range_high,\n                pred->include_low, pred->include_high);\n        case PREDICATE_AND:\n            return estimate_and_selectivity(\n                estimate_predicate_selectivity(pred->left, table_stats, col_stats),\n                estimate_predicate_selectivity(pred->right, table_stats, col_stats));\n        case PREDICATE_OR:\n            return estimate_or_selectivity(\n                estimate_predicate_selectivity(pred->left, table_stats, col_stats),\n                estimate_predicate_selectivity(pred->right, table_stats, col_stats));\n        case PREDICATE_NOT:\n            return 1.0 - estimate_predicate_selectivity(pred->operand, table_stats, col_stats);\n        default:\n            return DEFAULT_SELECTIVITY;  // 0.5 (50%) for unknown predicates\n    }\n}\n```\n---\n## Plan Enumeration\nWith cost estimation in place, the planner enumerates possible plans and chooses the cheapest.\n### Single-Table Query\nFor a query like:\n```sql\nSELECT * FROM users WHERE email = 'alice@example.com' AND age > 25;\n```\nThe planner considers:\n```c\ntypedef struct {\n    ScanType type;          // TABLE_SCAN, INDEX_SCAN, COVERING_INDEX_SCAN\n    Index* index;           // NULL for table scan\n    double estimated_cost;\n    int64_t estimated_rows;\n    List* predicates;       // Predicates applied during scan\n} AccessPath;\nAccessPath* find_cheapest_path(Query* query, Table* table) {\n    List* paths = create_list();\n    // Option 1: Full table scan\n    AccessPath* table_scan = create_table_scan_path(table, query->predicates);\n    list_add(paths, table_scan);\n    // Option 2-N: Index scans for each applicable index\n    for (int i = 0; i < table->index_count; i++) {\n        Index* index = table->indexes[i];\n        if (index_can_serve_predicates(index, query->predicates)) {\n            AccessPath* index_scan = create_index_scan_path(table, index, query->predicates);\n            list_add(paths, index_scan);\n        }\n    }\n    // Find the cheapest\n    AccessPath* cheapest = NULL;\n    double min_cost = INFINITY;\n    for (int i = 0; i < paths->count; i++) {\n        AccessPath* path = paths->items[i];\n        if (path->estimated_cost < min_cost) {\n            min_cost = path->estimated_cost;\n            cheapest = path;\n        }\n    }\n    return cheapest;\n}\n```\n### Index Selection Logic\n```c\nbool index_can_serve_predicates(Index* index, List* predicates) {\n    // Check if any predicate uses the indexed column\n    for (int i = 0; i < predicates->count; i++) {\n        Predicate* pred = predicates->items[i];\n        if (predicates_uses_column(pred, index->column_index)) {\n            // Check leftmost prefix rule for composite indexes\n            if (index->column_count == 1) {\n                return true;\n            }\n            // For composite indexes, more complex logic needed\n            // ...\n            return true;\n        }\n    }\n    return false;\n}\nAccessPath* create_index_scan_path(Table* table, Index* index, List* predicates) {\n    AccessPath* path = malloc(sizeof(AccessPath));\n    path->type = INDEX_SCAN;\n    path->index = index;\n    // Find the predicate that uses this index\n    Predicate* index_pred = find_index_predicate(index, predicates);\n    // Estimate selectivity\n    double selectivity = estimate_predicate_selectivity(index_pred, \n                                                         table->stats, \n                                                         table->column_stats);\n    path->estimated_rows = (int64_t)(table->stats->row_count * selectivity);\n    // Estimate cost\n    path->estimated_cost = estimate_index_scan_cost(index, selectivity, table);\n    return path;\n}\nAccessPath* create_table_scan_path(Table* table, List* predicates) {\n    AccessPath* path = malloc(sizeof(AccessPath));\n    path->type = TABLE_SCAN;\n    path->index = NULL;\n    // Assume all predicates are applied (filter after scan)\n    // Estimate combined selectivity\n    double selectivity = 1.0;\n    for (int i = 0; i < predicates->count; i++) {\n        Predicate* pred = predicates->items[i];\n        double pred_sel = estimate_predicate_selectivity(pred, \n                                                          table->stats,\n                                                          table->column_stats);\n        selectivity *= pred_sel;\n    }\n    path->estimated_rows = (int64_t)(table->stats->row_count * selectivity);\n    path->estimated_cost = estimate_table_scan_cost(table);\n    return path;\n}\n```\n### The Selection Decision\n```c\nAccessPath* choose_access_path(Table* table, List* predicates) {\n    AccessPath* table_scan = create_table_scan_path(table, predicates);\n    AccessPath* best_index_scan = NULL;\n    double best_index_cost = INFINITY;\n    for (int i = 0; i < table->index_count; i++) {\n        Index* index = table->indexes[i];\n        if (index_can_serve_predicates(index, predicates)) {\n            AccessPath* index_path = create_index_scan_path(table, index, predicates);\n            if (index_path->estimated_cost < best_index_cost) {\n                best_index_cost = index_path->estimated_cost;\n                best_index_scan = index_path;\n            }\n        }\n    }\n    // Compare table scan vs best index scan\n    if (best_index_scan == NULL || \n        table_scan->estimated_cost < best_index_scan->estimated_cost) {\n        free(best_index_scan);\n        return table_scan;\n    }\n    free(table_scan);\n    return best_index_scan;\n}\n```\n---\n## Join Planning: Order Matters\nFor multi-table queries, join order dramatically affects performance.\n\n![Join Order Impact](./diagrams/diag-join-order.svg)\n\n### The Join Order Problem\nConsider:\n```sql\nSELECT * FROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN order_items i ON o.id = i.order_id\nWHERE c.country = 'US' AND i.quantity > 10;\n```\nPossible join orders:\n1. `(customers \u22c8 orders) \u22c8 order_items`\n2. `(customers \u22c8 order_items) \u22c8 orders` \u2014 Invalid! No join condition\n3. `(orders \u22c8 order_items) \u22c8 customers`\n4. `(orders \u22c8 customers) \u22c8 order_items`\n5. etc.\nThe number of possible orderings is n! for n tables. For 10 tables, that's 3.6 million orderings \u2014 too many to enumerate.\n### Dynamic Programming Approach\nProduction databases use **dynamic programming** to find the optimal join order efficiently:\n```c\ntypedef struct {\n    TableSet* tables;       // Set of tables in this join\n    double best_cost;\n    JoinPlan* best_plan;\n} JoinMemoEntry;\nJoinPlan* find_optimal_join_order(Query* query) {\n    int table_count = query->table_count;\n    // Memoization table: DP[subset of tables] = best plan for that subset\n    HashTable* memo = hash_table_create();\n    // Base case: single tables\n    for (int i = 0; i < table_count; i++) {\n        Table* table = query->tables[i];\n        JoinMemoEntry* entry = malloc(sizeof(JoinMemoEntry));\n        entry->tables = table_set_create();\n        table_set_add(entry->tables, table);\n        entry->best_plan = create_single_table_plan(table, query->predicates);\n        entry->best_cost = entry->best_plan->cost;\n        hash_table_put(memo, entry->tables, entry);\n    }\n    // Build up: 2 tables, 3 tables, ..., n tables\n    for (int size = 2; size <= table_count; size++) {\n        // Generate all subsets of size 'size'\n        List* subsets = generate_subsets(query->tables, table_count, size);\n        for (int i = 0; i < subsets->count; i++) {\n            TableSet* subset = subsets->items[i];\n            // Try all ways to split this subset into two parts\n            JoinMemoEntry* best_entry = NULL;\n            double best_cost = INFINITY;\n            List* splits = generate_splits(subset);\n            for (int j = 0; j < splits->count; j++) {\n                Split* split = splits->items[j];\n                JoinMemoEntry* left = hash_table_get(memo, split->left);\n                JoinMemoEntry* right = hash_table_get(memo, split->right);\n                if (left && right && tables_can_join(split->left, split->right, query)) {\n                    double join_cost = estimate_join_cost(left->best_plan, \n                                                          right->best_plan,\n                                                          query);\n                    double total_cost = left->best_cost + right->best_cost + join_cost;\n                    if (total_cost < best_cost) {\n                        best_cost = total_cost;\n                        best_entry = malloc(sizeof(JoinMemoEntry));\n                        best_entry->tables = table_set_copy(subset);\n                        best_entry->best_cost = total_cost;\n                        best_entry->best_plan = create_join_plan(\n                            left->best_plan, right->best_plan, join_cost);\n                    }\n                }\n            }\n            if (best_entry) {\n                hash_table_put(memo, subset, best_entry);\n            }\n        }\n    }\n    // Extract the best plan for all tables\n    TableSet* all_tables = table_set_from_query(query);\n    JoinMemoEntry* result = hash_table_get(memo, all_tables);\n    return result ? result->best_plan : NULL;\n}\n```\n### Join Cardinality Estimation\nThe key to join planning is estimating the size of join results:\n```c\ndouble estimate_join_cardinality(JoinPlan* left, JoinPlan* right, Query* query) {\n    // Find the join condition\n    JoinCondition* cond = find_join_condition(left->tables, right->tables, query);\n    if (cond == NULL) {\n        // Cartesian product (bad!)\n        return left->estimated_rows * right->estimated_rows;\n    }\n    // Assume foreign key relationship: many-to-one\n    // Join size = (larger table) / (distinct values in smaller table)\n    double left_distinct = get_distinct_count(left, cond->left_column);\n    double right_distinct = get_distinct_count(right, cond->right_column);\n    // The smaller distinct count is the \"one\" side of the relationship\n    double selectivity = 1.0 / MIN(left_distinct, right_distinct);\n    return left->estimated_rows * right->estimated_rows * selectivity;\n}\n```\nThis is a simplification. Real databases use more sophisticated models that consider:\n- Foreign key constraints (guaranteed many-to-one)\n- Histogram intersection (actual value overlap)\n- Join skew (values with many matches)\n---\n## EXPLAIN: Window Into the Planner\nThe `EXPLAIN` command reveals the planner's decisions:\n```sql\nEXPLAIN SELECT * FROM users WHERE email = 'alice@example.com';\n```\nOutput:\n```\nQUERY PLAN\n`--SEARCH users USING INDEX idx_email (email=?)\n   |--Estimated rows: 1\n   |--Estimated cost: 12.5\n```\n### Implementing EXPLAIN\n```c\nvoid explain_query_plan(Query* query, FILE* output) {\n    fprintf(output, \"QUERY PLAN\\n\");\n    explain_plan_recursive(query->root_plan, output, 0);\n}\nvoid explain_plan_recursive(PlanNode* node, FILE* output, int indent) {\n    char prefix[100];\n    for (int i = 0; i < indent; i++) {\n        prefix[i*2] = ' ';\n        prefix[i*2+1] = ' ';\n    }\n    prefix[indent*2] = '\\0';\n    switch (node->type) {\n        case PLAN_TABLE_SCAN:\n            fprintf(output, \"%s`--SCAN %s\\n\", prefix, node->table->name);\n            break;\n        case PLAN_INDEX_SCAN:\n            fprintf(output, \"%s`--SEARCH %s USING INDEX %s (%s)\\n\", \n                    prefix, \n                    node->table->name, \n                    node->index->name,\n                    format_index_condition(node));\n            break;\n        case PLAN_NESTED_LOOP_JOIN:\n            fprintf(output, \"%s`--NESTED LOOP JOIN\\n\", prefix);\n            explain_plan_recursive(node->left, output, indent + 1);\n            explain_plan_recursive(node->right, output, indent + 1);\n            break;\n        // ... other plan types\n    }\n    // Show estimates\n    fprintf(output, \"%s   |--Estimated rows: %lld\\n\", prefix, node->estimated_rows);\n    fprintf(output, \"%s   |--Estimated cost: %.1f\\n\", prefix, node->estimated_cost);\n}\n```\n### EXPLAIN QUERY PLAN vs EXPLAIN\nSQLite distinguishes two modes:\n- **EXPLAIN**: Shows bytecode (low-level)\n- **EXPLAIN QUERY PLAN**: Shows the logical plan (high-level)\nYour implementation should support both:\n```c\n// In the parser\nif (check_keyword(parser, \"EXPLAIN\")) {\n    consume_token(parser);\n    if (check_keyword(parser, \"QUERY\")) {\n        consume_token(parser);\n        expect_keyword(parser, \"PLAN\");\n        query->explain_mode = EXPLAIN_PLAN;\n    } else {\n        query->explain_mode = EXPLAIN_BYTECODE;\n    }\n}\n// In the executor\nswitch (query->explain_mode) {\n    case EXPLAIN_NONE:\n        return execute_query_plan(plan, db);\n    case EXPLAIN_PLAN:\n        explain_query_plan(query, stdout);\n        return NULL;\n    case EXPLAIN_BYTECODE:\n        explain_bytecode(program, stdout);\n        return NULL;\n}\n```\n---\n## The Three-Level View\n### Level 1: Application (SQL \u2192 Fast Execution)\nAt the API level, the planner is invisible:\n```sql\nSELECT * FROM users WHERE email = 'alice@example.com';\n-- Planner chooses index scan automatically\n-- Returns in 0.5ms instead of 500ms\n```\nThe user just sees fast queries. They don't know about selectivity, cost models, or join ordering.\n### Level 2: Engine (Statistics \u2192 Cost Estimation \u2192 Plan Selection)\nInside the planner:\n```\n1. Parse query, extract predicates\n2. Look up statistics for referenced tables/columns\n3. Enumerate possible access paths\n4. Estimate cost for each path\n5. Select the cheapest path\n6. Generate bytecode for the chosen plan\n```\n### Level 3: Implementation (Numbers \u2192 Comparisons \u2192 Decisions)\nAt the lowest level, the planner is arithmetic:\n```c\n// Selectivity calculation\ndouble sel = 1.0 / stats->distinct_count;  // 0.0001 for 10,000 distinct values\n// Cardinality estimation\nint64_t rows = table->row_count * sel;  // 1,000,000 * 0.0001 = 100 rows\n// Cost comparison\ndouble index_cost = 3.0 + (100 * 4.0);  // traversal + lookups = 403\ndouble scan_cost = 10000 * 1.0;         // sequential pages = 10,000\n// Decision\nuse_index = (index_cost < scan_cost);   // true\n```\n---\n## Design Decisions: Why This, Not That\n### Cost Model: Simple vs Complex\n| Aspect | Simple Model (Chosen) | Complex Model |\n|--------|----------------------|---------------|\n| I/O cost | Sequential = 1x, Random = 4x | Detailed model based on disk type, cache state |\n| CPU cost | Constant per row | Detailed model for each operator |\n| Memory | Ignored | Tracked, spills to disk considered |\n| Accuracy | 2-5x error typical | 1.5-3x error typical |\n| Complexity | Easy to implement | Requires extensive calibration |\n| Used by | SQLite, early PostgreSQL | Oracle, modern PostgreSQL |\nSimple models are sufficient for most workloads. Complex models help for edge cases but require ongoing calibration.\n### Statistics: Exact vs Sampled\n| Aspect | Exact Statistics | Sampled Statistics |\n|--------|-----------------|-------------------|\n| Accuracy | Perfect | Error proportional to 1/\u221asample_size |\n| ANALYZE time | O(n) for n rows | O(n * sample_rate) |\n| Frequency | Run rarely | Can run frequently |\n| Used by | Small databases | Large databases |\nFor your implementation, exact statistics are fine. Production systems switch to sampling when ANALYZE becomes too slow.\n### Join Enumeration: Exhaustive vs Heuristic\n| Aspect | Dynamic Programming | Heuristic |\n|--------|---------------------|-----------|\n| Optimality | Guaranteed optimal | May miss optimal plan |\n| Tables supported | ~10-15 | Unlimited |\n| Time complexity | O(3^n) | O(n log n) |\n| Used by | All major databases | Very large queries |\nDynamic programming is standard for queries with reasonable table counts. Heuristics kick in for massive join queries.\n---\n## Common Pitfalls\n### Stale Statistics\n```c\n// Statistics collected at T=0: 1,000 rows\nANALYZE users;\n// Data loaded at T=1: 1,000,000 rows\nINSERT INTO users SELECT ... FROM external_source;\n// Query at T=2: Planner estimates 1 row, actually 1000\nSELECT * FROM users WHERE status = 'active';\n```\n**Solution**: Re-run ANALYZE after significant data changes. Some databases auto-analyze when tables change by >10%.\n### Correlated Columns\n```c\n// Independence assumption: P(A AND B) = P(A) * P(B)\n// Actual: P(A AND B) might be much higher if A and B are correlated\n// Example: status='active' AND last_login > '2024-01-01'\n// Active users log in frequently, so these are highly correlated\n// Planner estimates: 0.1 * 0.3 = 0.03 (3%)\n// Actual: 0.28 (28%)\n```\n**Solution**: Collect multi-column statistics (column groups) for frequently co-queried columns.\n### Skewed Data\n```c\n// Statistics say: 10,000 distinct customer_ids, 1,000,000 rows\n// Average: 100 rows per customer\n// Reality: Customer 1 has 500,000 rows, others have ~50 each\n// Query for Customer 1:\nSELECT * FROM orders WHERE customer_id = 1;\n// Planner: Index scan (expects 100 rows)\n// Reality: Should be table scan (500,000 rows)\n```\n**Solution**: Collect histograms that capture value frequency, not just distinct counts.\n### Missing Statistics\n```c\n// Table created, data inserted, but ANALYZE never run\nCREATE TABLE orders (...);\nINSERT INTO orders SELECT ...;\n// Query without statistics\nSELECT * FROM orders WHERE customer_id = 42;\n// Planner uses defaults: 10% selectivity, likely wrong\n```\n**Solution**: Use sensible defaults when statistics are missing. Consider auto-analyze on first query.\n---\n## Testing Strategy\n### Statistics Collection Tests\n```c\nvoid test_analyze_collects_row_count() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    for (int i = 0; i < 1000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, %d)\", i, i % 10);\n        execute_sql(db, sql);\n    }\n    execute_sql(db, \"ANALYZE test\");\n    TableStatistics* stats = get_table_statistics(db, \"test\");\n    assert(stats->row_count == 1000);\n    assert(stats->page_count > 0);\n    free_database(db);\n}\nvoid test_analyze_collects_index_statistics() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, category TEXT)\");\n    execute_sql(db, \"CREATE INDEX idx_category ON test(category)\");\n    // Insert 100 rows with 5 distinct categories\n    for (int i = 0; i < 100; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, 'cat%d')\", \n                 i, i % 5);\n        execute_sql(db, sql);\n    }\n    execute_sql(db, \"ANALYZE test\");\n    IndexStatistics* stats = get_index_statistics(db, \"idx_category\");\n    assert(stats->distinct_count == 5);\n    free_database(db);\n}\n```\n### Selectivity Estimation Tests\n```c\nvoid test_equality_selectivity() {\n    TableStatistics table_stats = { .row_count = 10000 };\n    ColumnStatistics col_stats = { .distinct_count = 100 };\n    double sel = estimate_equality_selectivity(&col_stats, &some_value);\n    assert(sel == 0.01);  // 1/100 = 1%\n}\nvoid test_range_selectivity() {\n    ColumnStatistics col_stats = {\n        .min_value = int_value(0),\n        .max_value = int_value(1000)\n    };\n    // WHERE col BETWEEN 200 AND 300\n    double sel = estimate_range_selectivity(&col_stats, \n                                            int_value(200), int_value(300),\n                                            true, true);\n    assert(sel == 0.1);  // 100/1000 = 10%\n}\nvoid test_and_selectivity() {\n    double sel_a = 0.1;  // 10%\n    double sel_b = 0.2;  // 20%\n    double combined = estimate_and_selectivity(sel_a, sel_b);\n    assert(combined == 0.02);  // 10% * 20% = 2%\n}\n```\n### Plan Selection Tests\n```c\nvoid test_chooses_index_for_low_selectivity() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    execute_sql(db, \"CREATE INDEX idx_value ON test(value)\");\n    // Insert 10,000 rows with 10,000 distinct values (0.01% selectivity)\n    for (int i = 0; i < 10000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, %d)\", i, i);\n        execute_sql(db, sql);\n    }\n    execute_sql(db, \"ANALYZE test\");\n    // Query with equality predicate\n    QueryPlan* plan = plan_query(db, \"SELECT * FROM test WHERE value = 5000\");\n    assert(plan->scan_type == INDEX_SCAN);\n    assert(strcmp(plan->index_name, \"idx_value\") == 0);\n    free_database(db);\n}\nvoid test_chooses_table_scan_for_high_selectivity() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, status TEXT)\");\n    execute_sql(db, \"CREATE INDEX idx_status ON test(status)\");\n    // Insert 10,000 rows with only 2 distinct values (50% selectivity)\n    for (int i = 0; i < 10000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, '%s')\", \n                 i, i < 5000 ? \"active\" : \"inactive\");\n        execute_sql(db, sql);\n    }\n    execute_sql(db, \"ANALYZE test\");\n    // Query that matches 50% of rows\n    QueryPlan* plan = plan_query(db, \"SELECT * FROM test WHERE status = 'active'\");\n    assert(plan->scan_type == TABLE_SCAN);\n    free_database(db);\n}\n```\n### EXPLAIN Output Tests\n```c\nvoid test_explain_shows_index_usage() {\n    Database* db = create_test_database();\n    // ... setup ...\n    char* output = capture_explain_output(db, \n        \"SELECT * FROM test WHERE value = 5\");\n    assert(strstr(output, \"USING INDEX idx_value\") != NULL);\n    assert(strstr(output, \"Estimated rows:\") != NULL);\n    free(output);\n    free_database(db);\n}\nvoid test_explain_shows_table_scan() {\n    Database* db = create_test_database();\n    // ... setup with no index or high selectivity ...\n    char* output = capture_explain_output(db, \n        \"SELECT * FROM test WHERE value > 0\");\n    assert(strstr(output, \"SCAN test\") != NULL);\n    free(output);\n    free_database(db);\n}\n```\n---\n## What You've Built\nYour query planner is a complete optimization engine:\n1. **ANALYZE command** collects table and index statistics\n2. **Statistics storage** persists cardinality and distribution information\n3. **Selectivity estimation** predicts predicate match rates\n4. **Cost model** translates plans to estimated execution time\n5. **Plan enumeration** generates candidate access paths\n6. **Plan selection** chooses the cheapest path\n7. **Join ordering** (for multi-table queries) minimizes intermediate result sizes\n8. **EXPLAIN output** reveals the planner's decisions\nThis is the brain of your database. Without it, queries execute but slowly. With it, the same queries can be orders of magnitude faster \u2014 or, if the planner guesses wrong, catastrophically slower.\n---\n## Knowledge Cascade\n### 1. Compiler Optimization: The Same Mindset\nQuery optimization is compiler optimization applied to data:\n- **Constant folding**: `WHERE 1=1 AND col=5` \u2192 `WHERE col=5`\n- **Dead code elimination**: Unreachable branches pruned\n- **Common subexpression elimination**: Repeated predicates computed once\n- **Register allocation**: Minimizing temporary storage (like minimizing intermediate results)\nThe query planner IS an optimizing compiler, just with a different target (execution plans instead of machine code).\n### 2. Network Routing: BGP and Cost-Based Decisions\nBGP (Border Gateway Protocol) routes internet traffic using cost-based decisions:\n- **Metrics**: AS path length, policy preferences, congestion\n- **Uncertainty**: Routes change, metrics are estimates\n- **Time budget**: Must converge quickly\nSound familiar? BGP faces the same challenges as query planning: optimize under uncertainty with time constraints. The internet's routing tables are the ultimate \"database\" with billions of rows.\n### 3. Machine Learning Hyperparameter Tuning\nFinding the best ML hyperparameters is search space optimization:\n- **Explosion**: 10 parameters with 10 values each = 10^10 combinations\n- **Heuristics**: Grid search, random search, Bayesian optimization\n- **Cost evaluation**: Each trial requires training a model (expensive!)\nQuery planners face the same explosion. Both domains use intelligent pruning to find good-enough solutions quickly.\n### 4. Video Game Pathfinding: A* is Query Planning\nA* pathfinding finds the shortest path through a game map:\n- **Cost function**: f(n) = g(n) + h(n) (actual cost + heuristic estimate)\n- **Admissible heuristic**: Never overestimates (like cost model)\n- **Pruning**: Skip obviously bad paths\nQuery planning IS pathfinding \u2014 through the space of possible execution plans instead of game terrain. The \"shortest path\" is the lowest-cost plan.\n### 5. Logistics Optimization: The Traveling Salesman\nDelivery route optimization (TSP variants) appears in:\n- **Join ordering**: Which order to join tables?\n- **Index selection**: Which indexes to use?\n- **Data placement**: Which server stores which shard?\nAll are combinatorial optimization under uncertainty. The techniques (dynamic programming, branch and bound, genetic algorithms) transfer directly.\n---\n## What's Next\nWith query planning complete, your database can execute SQL efficiently. But there's a looming threat: **what happens when the power goes out?**\nEvery query so far has assumed the database stays running. But real databases crash. Power fails. Disks corrupt. The next milestone, **Transactions (Rollback Journal)**, introduces ACID guarantees \u2014 ensuring that committed data survives crashes and uncommitted changes are rolled back.\nYour database can think. Now it needs to be reliable.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m8\", \"criteria\": [\"ANALYZE command scans table B-tree to count total rows and pages, storing results in statistics system table (e.g., sqlite_stat1)\", \"ANALYZE command scans each index B-tree to count distinct values, null count, min value, max value, and index page count\", \"Statistics storage persists table statistics (row_count, page_count) and index statistics (distinct_count, null_count, min_value, max_value)\", \"Selectivity estimation for equality predicate (col = value) returns 1/distinct_count, or default 0.1 if no statistics available\", \"Selectivity estimation for range predicate (col BETWEEN low AND high) calculates fraction of value range covered using min/max statistics\", \"Selectivity estimation for AND predicates multiplies individual selectivities assuming column independence\", \"Selectivity estimation for OR predicates uses formula: sel_a + sel_b - (sel_a * sel_b)\", \"Cost model for table scan estimates: page_count * SEQ_IO_COST + row_count * CPU_COST_PER_ROW\", \"Cost model for index scan estimates: traversal_cost + (matching_rows * RANDOM_IO_COST) + index_leaf_scan_cost\", \"Cost constants differentiate sequential I/O (baseline) from random I/O (typically 4x cost on SSD, 20x on HDD)\", \"Plan enumeration considers table scan plus all applicable index scans for single-table queries\", \"Plan selection compares estimated costs and chooses the cheapest access path\", \"Planner selects index scan when estimated selectivity is below configurable threshold (default 10-20%)\", \"Planner falls back to table scan when no index exists, no index is applicable, or selectivity is too high\", \"Join planning estimates join cardinality using formula: left_rows * right_rows / min(distinct_left, distinct_right)\", \"Join planning uses dynamic programming to find optimal join order for queries with up to 10 tables\", \"EXPLAIN QUERY PLAN output shows scan type (SCAN for table scan, SEARCH for index scan), index name if used, and estimated row count\", \"EXPLAIN output includes estimated cost to enable plan comparison\", \"Test suite validates ANALYZE collects correct row count and distinct value count\", \"Test suite validates selectivity estimation produces correct values for equality, range, AND, and OR predicates\", \"Test suite validates planner chooses index scan for low-selectivity queries (<1% matching rows)\", \"Test suite validates planner chooses table scan for high-selectivity queries (>20% matching rows)\", \"Test suite validates planner falls back to table scan when no suitable index exists\", \"Test suite validates EXPLAIN output shows correct scan type and index name\", \"Test suite validates join cardinality estimation produces reasonable estimates\", \"Statistics have sensible defaults when ANALYZE has not been run (e.g., assume 10% selectivity for equality)\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m9 -->\n# Transactions (Rollback Journal): When the Power Goes Out\nYour database can parse SQL, compile bytecode, execute queries, manage memory, store data in B-trees, maintain indexes, and plan efficient execution paths. It's a complete database engine \u2014 as long as nothing goes wrong.\nBut something *will* go wrong. The power will fail. The operating system will crash. A user will yank the power cord. A disk will corrupt a sector. The question isn't *if* these things happen \u2014 it's *when*, and whether your database survives.\nConsider what happens during this innocent-looking transaction:\n```sql\nBEGIN;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;  -- Deduct $100 from Alice\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;  -- Add $100 to Bob\nCOMMIT;\n```\nBetween the two UPDATE statements, your database has modified Alice's balance but not Bob's. If the power fails at this exact moment, what's on disk? Alice has lost $100. Bob never received it. The money vanished \u2014 $100 destroyed because of a power outage.\nThis is unacceptable. Your database must guarantee that **either both updates happen, or neither happens**. There is no middle ground. This is the ACID promise: Atomicity, Consistency, Isolation, Durability. And this milestone is where you make it real.\n---\n## The Tension: Every Write is a Crash Risk\nHere's the fundamental problem: **writing to disk is not atomic.**\nWhen you modify a page in your buffer pool and flush it to disk, the operating system doesn't guarantee atomicity. A 4KB page write might:\n- Complete successfully (best case)\n- Never start (power fails before write)\n- Partially complete (power fails during write \u2014 a \"torn page\")\n- Complete but sit in the disk's write cache (power fails before cache flushes to platter)\nFor a transaction that modifies 10 pages, you have 10 opportunities for partial failure. If page 5's write is interrupted, your database is corrupted: half the transaction is on disk, half isn't.\n### The Numbers That Matter\nLet's make this concrete with write latencies:\n| Operation | Latency | Failure Window |\n|-----------|---------|----------------|\n| Page write (4KB to SSD) | 0.025 ms | 25 microseconds of vulnerability |\n| fsync (force to disk) | 1-10 ms | Milliseconds of vulnerability |\n| Full transaction (10 pages) | 0.25 ms writes + 10 ms fsync | ~10 ms total vulnerability window |\nIn a database handling 1000 transactions per second, that's 10 seconds of cumulative vulnerability per second \u2014 your database is in a crash-vulnerable state essentially continuously.\nBut here's the deeper insight: **the vulnerability window isn't random.** It's determined by *when* you write what. Write the wrong thing at the wrong time, and a crash corrupts your database. Write the right thing at the right time, and a crash is recoverable.\nThis is the art of transaction logging: **ordering your writes so that any crash leaves the database in a known, recoverable state.**\n---\n## The Revelation: Transactions Are About Surviving Crashes, Not Preventing Them\nHere's the misconception that will corrupt your data: *Transactions are about locking \u2014 the hard part is preventing concurrent modifications.*\n**No.** Transactions are about **surviving crashes**. Locking is the easy part \u2014 it's just mutual exclusion. The hard part is ensuring that a power failure at *any* point leaves the database in a consistent state.\nConsider the lifecycle of a simple transaction:\n```\nT0: BEGIN\nT1: Modify page 42 (Alice's balance)\nT2: Modify page 17 (Bob's balance)\nT3: COMMIT\n```\nAt each point in time, a crash has different implications:\n| Crash Time | What's On Disk | Consequence |\n|------------|----------------|-------------|\n| Before T1 | Original state | Safe \u2014 transaction never started |\n| After T1, before T2 | Page 42 modified, page 17 original | **CORRUPTED** \u2014 Alice lost $100, Bob unchanged |\n| After T2, before T3 | Both pages modified | **CORRUPTED** \u2014 transaction not committed, but changes visible |\n| After T3 | Both pages modified, commit marker written | Safe \u2014 transaction is durable |\nThe rollback journal solves this by capturing the **original state** before any modification. If a crash occurs, the original pages can be restored. The key insight:\n\n![Rollback Journal: Write Sequence](./diagrams/diag-rollback-journal-flow.svg)\n\n> **The journal must be on disk BEFORE the database pages are modified.**\nThis ordering guarantee is the difference between recovery and corruption. If you write the database pages first and then the journal, a crash between them leaves you with modified pages and no way to undo them. But if you write the journal first, any crash is recoverable: either the journal is complete (restore from it) or incomplete (ignore it).\n### Why fsync Matters\nThe `fsync()` system call is the most important operation in database implementation. It tells the operating system: *flush all buffered writes for this file to physical storage.* Without fsync, your writes sit in RAM \u2014 fast, but vulnerable to power loss.\nHere's the critical sequence:\n```\n1. Write original page to journal file\n2. fsync(journal)  \u2190 Journal is now durable\n3. Write modified page to database file\n4. fsync(database)  \u2190 Database is now durable\n5. Delete journal file\n```\nIf a crash occurs after step 2 but before step 4, the journal contains the original page. Recovery restores it. If a crash occurs after step 4, the database has the modified page \u2014 and the journal can be deleted.\nBut if you skip step 2 (no fsync on journal), a crash after step 3 might leave the journal unwritten while the database is modified. **Corruption.**\n\n![Write Ordering: Why fsync Matters](./diagrams/diag-fsync-ordering.svg)\n\nThis is why databases are paranoid about fsync. SQLite famously calls fsync multiple times during commit, and PostgreSQL has extensive documentation on filesystem behavior. The rollback journal's correctness depends on write ordering that the operating system doesn't guarantee without explicit fsync.\n---\n## The Rollback Journal Architecture\nA rollback journal is a separate file that stores original page images before they're modified. Its structure is simple but precise:\n### Journal File Format\n```\n+------------------+\n| Journal Header   |  28 bytes\n+------------------+\n| Page Record 1    |  4 bytes (page number) + 4 bytes (page size) + N bytes (page data)\n+------------------+\n| Page Record 2    |\n+------------------+\n| ...              |\n+------------------+\n| Page Record N    |\n+------------------+\n```\nThe journal header contains:\n```c\ntypedef struct {\n    uint32_t magic;           // Magic number: 0xd9d505f9\n    uint32_t format_version;  // Format version: 1\n    uint32_t page_size;       // Database page size\n    uint32_t sequence_number; // Incremented on each transaction\n    uint32_t salt_1;          // Random value for validation\n    uint32_t salt_2;          // Random value for validation\n    uint32_t checksum_1;      // Checksum of first 24 bytes\n    uint32_t checksum_2;      // Checksum of first 24 bytes (different algorithm)\n} JournalHeader;\n```\nEach page record contains:\n```c\ntypedef struct {\n    uint32_t page_number;     // Which page in the database\n    // Followed by page_size bytes of original page content\n} PageRecord;\n```\n### Transaction State Machine\nYour transaction manager tracks the current state:\n```c\ntypedef enum {\n    TXN_AUTOCOMMIT,    // No active transaction (each statement commits immediately)\n    TXN_STARTED,       // BEGIN executed, no writes yet\n    TXN_DIRTY,         // Writes have occurred, journal exists\n    TXN_COMMITTING,    // COMMIT in progress\n    TXN_ROLLING_BACK   // ROLLBACK in progress\n} TransactionState;\ntypedef struct {\n    TransactionState state;\n    const char* journal_path;    // Path to journal file\n    int journal_fd;              // File descriptor for journal\n    uint32_t* modified_pages;    // List of page numbers modified in this transaction\n    int modified_count;\n    int modified_capacity;\n    uint32_t sequence_number;    // Incremented each transaction\n} TransactionManager;\n```\n### The Transaction Lifecycle\nA complete transaction flows through these states:\n```\nAUTOCOMMIT \u2500\u2500[BEGIN]\u2500\u2500\u25ba STARTED \u2500\u2500[first write]\u2500\u2500\u25ba DIRTY \u2500\u2500[COMMIT]\u2500\u2500\u25ba AUTOCOMMIT\n                              \u2502                       \u2502\n                              \u2502                       \u2514\u2500\u2500[ROLLBACK]\u2500\u2500\u25ba AUTOCOMMIT\n                              \u2502\n                              \u2514\u2500\u2500[crash]\u2500\u2500\u25ba [recovery restores from journal]\n```\n---\n## BEGIN: Starting a Transaction\nThe `BEGIN` statement transitions from autocommit mode to explicit transaction mode:\n```c\nint txn_begin(TransactionManager* txn, const char* db_path) {\n    if (txn->state != TXN_AUTOCOMMIT) {\n        return ERROR_NESTED_TRANSACTION;  // Nested transactions not supported\n    }\n    // Generate journal path: database.db \u2192 database.db-journal\n    txn->journal_path = generate_journal_path(db_path);\n    txn->journal_fd = -1;  // Not created until first write\n    txn->modified_count = 0;\n    txn->state = TXN_STARTED;\n    return 0;\n}\nconst char* generate_journal_path(const char* db_path) {\n    size_t len = strlen(db_path);\n    char* journal_path = malloc(len + 8);  // \"-journal\" suffix\n    strcpy(journal_path, db_path);\n    strcat(journal_path, \"-journal\");\n    return journal_path;\n}\n```\nNote that the journal file isn't created yet \u2014 it's created lazily on the first write. This is an optimization for read-only transactions.\n---\n## The Critical Path: First Write Creates the Journal\nWhen the first write occurs in a transaction, the journal must be created and the original page recorded:\n```c\nint txn_before_page_modify(TransactionManager* txn, BufferPool* pool, \n                           PageId page_id, Frame* frame) {\n    if (txn->state == TXN_AUTOCOMMIT) {\n        // Autocommit mode: each statement is its own transaction\n        // Create a temporary transaction\n        txn_begin(txn, pool->db_path);\n        txn->auto_started = true;\n    }\n    if (txn->state == TXN_STARTED) {\n        // First write: create the journal\n        int result = create_journal(txn, pool->page_size);\n        if (result != 0) return result;\n        txn->state = TXN_DIRTY;\n    }\n    // Check if we've already recorded this page\n    for (int i = 0; i < txn->modified_count; i++) {\n        if (txn->modified_pages[i] == page_id) {\n            // Already recorded, no need to journal again\n            return 0;\n        }\n    }\n    // Record the original page in the journal\n    int result = write_page_to_journal(txn, page_id, frame->data);\n    if (result != 0) return result;\n    // Track this page as modified\n    if (txn->modified_count >= txn->modified_capacity) {\n        txn->modified_capacity *= 2;\n        txn->modified_pages = realloc(txn->modified_pages, \n                                       txn->modified_capacity * sizeof(uint32_t));\n    }\n    txn->modified_pages[txn->modified_count++] = page_id;\n    return 0;\n}\n```\n### Creating the Journal File\n```c\nint create_journal(TransactionManager* txn, uint32_t page_size) {\n    // Open journal file\n    txn->journal_fd = open(txn->journal_path, O_RDWR | O_CREAT | O_TRUNC, 0644);\n    if (txn->journal_fd < 0) {\n        return ERROR_CANNOT_CREATE_JOURNAL;\n    }\n    // Initialize and write header\n    JournalHeader header;\n    header.magic = 0xd9d505f9;\n    header.format_version = 1;\n    header.page_size = page_size;\n    header.sequence_number = txn->sequence_number++;\n    // Generate random salt values\n    header.salt_1 = random();\n    header.salt_2 = random();\n    // Calculate checksums (SQLite uses a specific algorithm)\n    calculate_checksums(&header);\n    // Write header\n    ssize_t written = write(txn->journal_fd, &header, sizeof(header));\n    if (written != sizeof(header)) {\n        close(txn->journal_fd);\n        unlink(txn->journal_path);\n        return ERROR_JOURNAL_WRITE_FAILED;\n    }\n    return 0;\n}\n```\n### Writing a Page to the Journal\n```c\nint write_page_to_journal(TransactionManager* txn, uint32_t page_id, \n                          const char* page_data) {\n    // Seek to end of journal\n    off_t offset = lseek(txn->journal_fd, 0, SEEK_END);\n    // Write page number (big-endian)\n    uint32_t page_id_be = htonl(page_id);\n    ssize_t written = write(txn->journal_fd, &page_id_be, sizeof(page_id_be));\n    if (written != sizeof(page_id_be)) {\n        return ERROR_JOURNAL_WRITE_FAILED;\n    }\n    // Write page data\n    written = write(txn->journal_fd, page_data, txn->page_size);\n    if (written != (ssize_t)txn->page_size) {\n        return ERROR_JOURNAL_WRITE_FAILED;\n    }\n    return 0;\n}\n```\n---\n## COMMIT: Making Changes Permanent\nThe `COMMIT` statement is where the write ordering becomes critical:\n```c\nint txn_commit(TransactionManager* txn, BufferPool* pool, FileManager* fm) {\n    if (txn->state == TXN_STARTED) {\n        // No writes occurred: just end the transaction\n        txn->state = TXN_AUTOCOMMIT;\n        return 0;\n    }\n    if (txn->state != TXN_DIRTY) {\n        return ERROR_INVALID_TRANSACTION_STATE;\n    }\n    txn->state = TXN_COMMITTING;\n    // Step 1: Sync the journal to disk\n    // This ensures all original pages are durable BEFORE we modify the database\n    int result = fsync(txn->journal_fd);\n    if (result != 0) {\n        // Cannot guarantee recovery \u2014 abort\n        txn_rollback(txn, pool);\n        return ERROR_JOURNAL_SYNC_FAILED;\n    }\n    // Step 2: Flush all dirty pages to the database file\n    // The buffer pool has been tracking which pages are dirty\n    result = buffer_pool_flush_dirty_pages(pool, txn->modified_pages, txn->modified_count);\n    if (result != 0) {\n        txn_rollback(txn, pool);\n        return ERROR_DATABASE_WRITE_FAILED;\n    }\n    // Step 3: Sync the database file\n    // This ensures all modified pages are durable on disk\n    result = file_manager_sync(fm);\n    if (result != 0) {\n        txn_rollback(txn, pool);\n        return ERROR_DATABASE_SYNC_FAILED;\n    }\n    // Step 4: Close and delete the journal\n    // Once the journal is deleted, the transaction is committed\n    close(txn->journal_fd);\n    txn->journal_fd = -1;\n    result = unlink(txn->journal_path);\n    if (result != 0) {\n        // Journal deletion failed, but transaction is committed\n        // This is non-fatal but should be logged\n        log_warning(\"Failed to delete journal file: %s\", txn->journal_path);\n    }\n    // Step 5: Clean up transaction state\n    free(txn->modified_pages);\n    txn->modified_pages = NULL;\n    txn->modified_count = 0;\n    txn->state = TXN_AUTOCOMMIT;\n    return 0;\n}\n```\n### The Critical Ordering\nLet's emphasize why the ordering matters:\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    COMMIT SEQUENCE                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  1. Write journal (original pages)                              \u2502\n\u2502  2. fsync(journal)           \u2190 CRITICAL: journal must be on disk\u2502\n\u2502  3. Write database (modified pages)                             \u2502\n\u2502  4. fsync(database)          \u2190 CRITICAL: database must be on disk\u2502\n\u2502  5. Delete journal                                              \u2502\n\u2502                                                                 \u2502\n\u2502  If crash after step 2: Journal exists, database unchanged      \u2502\n\u2502    \u2192 Recovery restores from journal (or ignores if we delete)   \u2502\n\u2502                                                                 \u2502\n\u2502  If crash after step 4: Both journal and database on disk       \u2502\n\u2502    \u2192 Transaction is committed (journal can be deleted)          \u2502\n\u2502                                                                 \u2502\n\u2502  WRONG ORDERING (step 3 before step 2):                         \u2502\n\u2502  If crash after step 3: Database modified, journal NOT on disk  \u2502\n\u2502    \u2192 NO RECOVERY POSSIBLE: original pages lost                  \u2502\n\u2502    \u2192 DATABASE CORRUPTED                                         \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\nThis ordering is why the rollback journal is sometimes called a \"undo log\" \u2014 it contains the information needed to *undo* an incomplete transaction.\n---\n## ROLLBACK: Undoing Changes\nThe `ROLLBACK` statement restores the database to its pre-transaction state:\n```c\nint txn_rollback(TransactionManager* txn, BufferPool* pool) {\n    if (txn->state == TXN_STARTED) {\n        // No writes occurred: just end the transaction\n        txn->state = TXN_AUTOCOMMIT;\n        return 0;\n    }\n    if (txn->state != TXN_DIRTY && txn->state != TXN_COMMITTING) {\n        return ERROR_INVALID_TRANSACTION_STATE;\n    }\n    txn->state = TXN_ROLLING_BACK;\n    // Read the journal and restore each page\n    off_t journal_size = lseek(txn->journal_fd, 0, SEEK_END);\n    lseek(txn->journal_fd, sizeof(JournalHeader), SEEK_SET);  // Skip header\n    while (lseek(txn->journal_fd, 0, SEEK_CUR) < journal_size) {\n        // Read page number\n        uint32_t page_id_be;\n        ssize_t bytes_read = read(txn->journal_fd, &page_id_be, sizeof(page_id_be));\n        if (bytes_read != sizeof(page_id_be)) {\n            break;  // Incomplete record\n        }\n        uint32_t page_id = ntohl(page_id_be);\n        // Read original page data\n        char page_data[txn->page_size];\n        bytes_read = read(txn->journal_fd, page_data, txn->page_size);\n        if (bytes_read != (ssize_t)txn->page_size) {\n            break;  // Incomplete record\n        }\n        // Restore the page in the buffer pool\n        Frame* frame = buffer_pool_fetch_page(pool, page_id);\n        if (frame != NULL) {\n            memcpy(frame->data, page_data, txn->page_size);\n            buffer_pool_mark_dirty(pool, frame);\n            buffer_pool_unpin_page(pool, frame);\n        }\n        // Also write directly to the database file\n        // This ensures rollback persists even if buffer pool isn't flushed\n        file_manager_write_page(pool->file_manager, page_id, page_data);\n    }\n    // Sync the database to ensure rollback is durable\n    file_manager_sync(pool->file_manager);\n    // Close and delete the journal\n    close(txn->journal_fd);\n    txn->journal_fd = -1;\n    unlink(txn->journal_path);\n    // Discard any remaining dirty pages in the buffer pool\n    buffer_pool_discard_dirty_pages(pool, txn->modified_pages, txn->modified_count);\n    // Clean up transaction state\n    free(txn->modified_pages);\n    txn->modified_pages = NULL;\n    txn->modified_count = 0;\n    txn->state = TXN_AUTOCOMMIT;\n    return 0;\n}\n```\n### Why Rollback Writes to Disk\nYou might wonder: why does rollback write directly to the database file instead of just modifying the buffer pool?\nThe answer: **crash safety during rollback itself.**\nIf rollback only modifies the buffer pool and the system crashes before those pages are flushed, the database file still contains the modified (uncommitted) pages. On restart, recovery sees no journal (it was deleted during the failed rollback) and assumes the database is consistent \u2014 but it's not.\nBy writing restored pages directly to disk during rollback, we ensure that even a crash during rollback leaves the database in a recoverable state.\n---\n## Crash Recovery: The Hot Journal Protocol\nThe most important part of the rollback journal system is what happens when the database starts up after a crash:\n\n![Crash Recovery: Hot Journal Detection](./diagrams/diag-crash-recovery.svg)\n\n```c\nint recover_from_crash(Database* db) {\n    // Check for hot journal\n    char* journal_path = generate_journal_path(db->file_path);\n    if (!file_exists(journal_path)) {\n        // No journal: database is clean\n        free(journal_path);\n        return 0;\n    }\n    // Hot journal exists: a crash occurred during a transaction\n    log_info(\"Hot journal detected: %s\", journal_path);\n    // Validate the journal\n    int journal_fd = open(journal_path, O_RDWR);\n    if (journal_fd < 0) {\n        free(journal_path);\n        return ERROR_CANNOT_OPEN_JOURNAL;\n    }\n    JournalHeader header;\n    ssize_t bytes_read = read(journal_fd, &header, sizeof(header));\n    if (bytes_read != sizeof(header)) {\n        close(journal_fd);\n        free(journal_path);\n        return ERROR_JOURNAL_INVALID;\n    }\n    // Verify magic number\n    if (header.magic != 0xd9d505f9) {\n        log_error(\"Invalid journal magic number\");\n        close(journal_fd);\n        unlink(journal_path);  // Delete corrupted journal\n        free(journal_path);\n        return ERROR_JOURNAL_INVALID;\n    }\n    // Verify checksums\n    if (!verify_journal_checksums(&header)) {\n        log_error(\"Journal checksum mismatch\");\n        close(journal_fd);\n        unlink(journal_path);  // Delete corrupted journal\n        free(journal_path);\n        return ERROR_JOURNAL_INVALID;\n    }\n    // Perform rollback recovery\n    log_info(\"Performing crash recovery rollback...\");\n    off_t journal_size = lseek(journal_fd, 0, SEEK_END);\n    lseek(journal_fd, sizeof(JournalHeader), SEEK_SET);\n    int pages_restored = 0;\n    while (lseek(journal_fd, 0, SEEK_CUR) < journal_size) {\n        uint32_t page_id_be;\n        bytes_read = read(journal_fd, &page_id_be, sizeof(page_id_be));\n        if (bytes_read != sizeof(page_id_be)) {\n            break;\n        }\n        uint32_t page_id = ntohl(page_id_be);\n        char page_data[header.page_size];\n        bytes_read = read(journal_fd, page_data, header.page_size);\n        if (bytes_read != (ssize_t)header.page_size) {\n            break;\n        }\n        // Restore page to database file\n        file_manager_write_page(db->file_manager, page_id, page_data);\n        pages_restored++;\n    }\n    // Sync the database\n    file_manager_sync(db->file_manager);\n    log_info(\"Crash recovery complete: %d pages restored\", pages_restored);\n    // Delete the journal\n    close(journal_fd);\n    unlink(journal_path);\n    free(journal_path);\n    return 0;\n}\n```\n### The Hot Journal Detection Logic\nA \"hot journal\" is a journal file that exists when the database starts up. This indicates that a transaction was in progress when the system crashed. The recovery process:\n1. **Detect** the journal file's existence\n2. **Validate** the journal header (magic number, checksums)\n3. **Restore** each page from the journal to the database file\n4. **Sync** the database file to ensure durability\n5. **Delete** the journal file\nAfter recovery, the database is in exactly the state it was before the crashed transaction began. The transaction is effectively rolled back.\n---\n## Integration with the Buffer Pool\nThe rollback journal must integrate with your buffer pool's dirty page tracking. When a page is modified within a transaction:\n```c\n// In buffer_pool_mark_dirty()\nvoid buffer_pool_mark_dirty(BufferPool* pool, Frame* frame) {\n    frame->is_dirty = true;\n    // If we're in a transaction, journal this page first\n    if (pool->txn_manager->state != TXN_AUTOCOMMIT) {\n        txn_before_page_modify(pool->txn_manager, pool, frame->page_id, frame);\n    }\n}\n```\nThis ensures that every dirty page is journaled before it's modified in memory. The journal write happens synchronously \u2014 the transaction can't proceed until the journal is written.\n### The Page Flow During Transaction\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              PAGE MODIFICATION IN TRANSACTION                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  1. Fetch page from disk into buffer pool                       \u2502\n\u2502  2. Check if page already journaled in this transaction         \u2502\n\u2502     - If yes: proceed to step 4                                 \u2502\n\u2502     - If no: continue                                           \u2502\n\u2502  3. Write original page content to journal file                 \u2502\n\u2502  4. Modify page content in buffer pool memory                   \u2502\n\u2502  5. Mark page as dirty                                          \u2502\n\u2502                                                                 \u2502\n\u2502  At COMMIT:                                                     \u2502\n\u2502  6. fsync journal (ensures original pages are durable)          \u2502\n\u2502  7. Flush dirty pages to database file                          \u2502\n\u2502  8. fsync database (ensures modified pages are durable)         \u2502\n\u2502  9. Delete journal                                              \u2502\n\u2502                                                                 \u2502\n\u2502  At ROLLBACK:                                                   \u2502\n\u2502  6. Read original pages from journal                            \u2502\n\u2502  7. Restore pages in buffer pool and database file              \u2502\n\u2502  8. fsync database                                              \u2502\n\u2502  9. Delete journal                                              \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n---\n## Basic Read Isolation\nWhile the rollback journal primarily provides atomicity and durability, it also enables basic read isolation. Changes made within a transaction should not be visible to other connections until commit.\n### Connection-Level Visibility\n```c\ntypedef struct {\n    Database* db;\n    TransactionManager* txn;\n    bool sees_uncommitted_changes;  // For this connection only\n} Connection;\nRow* connection_execute_query(Connection* conn, const char* sql) {\n    // During query execution, this connection sees its own uncommitted changes\n    // because they're in the buffer pool\n    // Other connections only see committed changes\n    // This is achieved by not flushing dirty pages until commit\n}\n```\nIn SQLite's architecture, each connection has its own view of the database. Uncommitted changes exist only in the buffer pool's dirty pages. Other connections, reading through their own buffer pool access, see only the on-disk state (committed changes).\nThis is a simple isolation model \u2014 it doesn't prevent all isolation anomalies (like phantom reads), but it provides the basic guarantee that uncommitted changes aren't visible to other connections.\n---\n## The Three-Level View\n### Level 1: Application (SQL \u2192 ACID Guarantee)\nAt the API level, transactions are simple:\n```sql\nBEGIN;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT;  -- Either both updates happen, or neither does\n```\nThe user gets the ACID promise: atomicity (all or nothing), consistency (constraints preserved), isolation (uncommitted changes invisible), durability (committed changes survive crashes).\n### Level 2: Engine (Journal Management + Write Ordering)\nInside the transaction manager:\n```\nBEGIN:\n  - Transition to transaction mode\n  - Prepare journal path\nFirst Write:\n  - Create journal file with header\n  - Write original page to journal\n  - Modify page in buffer pool\nCOMMIT:\n  - fsync journal (CRITICAL)\n  - Flush dirty pages to database\n  - fsync database (CRITICAL)\n  - Delete journal\nROLLBACK:\n  - Read pages from journal\n  - Restore to database file\n  - Delete journal\n```\n### Level 3: Implementation (File Descriptors + fsync + Byte Copies)\nAt the lowest level, transactions are file operations:\n```c\n// Journal write\nwrite(journal_fd, &page_number, 4);\nwrite(journal_fd, page_data, page_size);\n// Critical sync\nfsync(journal_fd);\n// Database write\npwrite(database_fd, modified_page, page_size, page_number * page_size);\n// Critical sync\nfsync(database_fd);\n// Cleanup\nunlink(journal_path);\n```\nThe correctness of your transactions depends on every `fsync()` being called at the right time.\n---\n## Common Pitfalls\n### Forgetting to fsync the Journal\n```c\n// WRONG: No fsync on journal\nwrite(journal_fd, page_data, page_size);\nwrite(database_fd, modified_page, page_size);  // Danger!\nfsync(database_fd);\nunlink(journal_path);\n// RIGHT: fsync journal first\nwrite(journal_fd, page_data, page_size);\nfsync(journal_fd);  // Journal is durable\nwrite(database_fd, modified_page, page_size);\nfsync(database_fd);\nunlink(journal_path);\n```\nWithout the journal fsync, a crash after the database write but before the journal reaches disk leaves you with modified pages and no way to undo them.\n### Torn Pages (Partial Writes)\nA 4KB page write might be interrupted mid-write, leaving a \"torn page\" \u2014 part old data, part new data:\n```c\n// Before write: page = \"AAAA...AAAA\" (4KB)\n// After torn write: page = \"AAAABBBB...BBBAAAA\" (corrupted)\n```\n**Solution**: Some databases write pages twice or use checksums to detect torn pages. SQLite relies on the journal containing the complete original page \u2014 if the database page is corrupted, recovery restores it from the journal.\n### Journal Deletion Failure\n```c\n// If unlink() fails, the journal persists\nunlink(journal_path);  // Might fail due to permissions, etc.\n// On next startup, recovery sees journal and rolls back a committed transaction!\n```\n**Solution**: SQLite uses a \"master journal\" or sets a flag in the journal header to indicate a committed transaction. Recovery checks this flag before rolling back.\n### Double Journaling the Same Page\n```c\n// WRONG: Journal the same page twice in one transaction\ntxn_before_page_modify(txn, pool, page_42, frame);\n// ... modify page ...\ntxn_before_page_modify(txn, pool, page_42, frame);  // Writes original AGAIN\n// RIGHT: Check if already journaled\nif (!page_already_journaled(txn, page_42)) {\n    txn_before_page_modify(txn, pool, page_42, frame);\n}\n```\nDouble journaling wastes space and time but doesn't affect correctness. However, it's inefficient for transactions that modify the same page multiple times.\n### Crash During Recovery\nWhat if the system crashes *during* crash recovery?\n```c\n// Recovery is in progress, restoring pages...\nrestore_page_from_journal(page_42);\n// CRASH!\n// page_42 is restored, but page_17 is not\n```\n**Solution**: Recovery is idempotent. If recovery is interrupted, the next startup runs recovery again, restoring all pages from the (still-present) journal. The journal is only deleted after all pages are restored and the database is synced.\n---\n## Testing Strategy\n### Basic Transaction Tests\n```c\nvoid test_begin_commit() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 100)\");\n    // Begin transaction\n    assert(execute_sql(db, \"BEGIN\") == 0);\n    assert(db->txn_manager->state == TXN_STARTED);\n    // Modify data\n    execute_sql(db, \"UPDATE test SET value = 200 WHERE id = 1\");\n    assert(db->txn_manager->state == TXN_DIRTY);\n    // Commit\n    assert(execute_sql(db, \"COMMIT\") == 0);\n    assert(db->txn_manager->state == TXN_AUTOCOMMIT);\n    // Verify change persisted\n    Result* r = execute_sql(db, \"SELECT value FROM test WHERE id = 1\");\n    assert(r->rows[0].columns[0].data.integer_val == 200);\n    free_database(db);\n}\nvoid test_begin_rollback() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 100)\");\n    // Begin transaction\n    execute_sql(db, \"BEGIN\");\n    // Modify data\n    execute_sql(db, \"UPDATE test SET value = 200 WHERE id = 1\");\n    // Rollback\n    assert(execute_sql(db, \"ROLLBACK\") == 0);\n    assert(db->txn_manager->state == TXN_AUTOCOMMIT);\n    // Verify change was NOT persisted\n    Result* r = execute_sql(db, \"SELECT value FROM test WHERE id = 1\");\n    assert(r->rows[0].columns[0].data.integer_val == 100);  // Original value\n    free_database(db);\n}\n```\n### Journal File Tests\n```c\nvoid test_journal_created_on_first_write() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY)\");\n    execute_sql(db, \"BEGIN\");\n    assert(!file_exists(db->journal_path));  // No journal yet\n    execute_sql(db, \"INSERT INTO test VALUES (1)\");\n    assert(file_exists(db->journal_path));  // Journal created\n    execute_sql(db, \"COMMIT\");\n    assert(!file_exists(db->journal_path));  // Journal deleted\n    free_database(db);\n}\nvoid test_journal_contains_original_pages() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value TEXT)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'original')\");\n    execute_sql(db, \"BEGIN\");\n    execute_sql(db, \"UPDATE test SET value = 'modified' WHERE id = 1\");\n    // Read journal and verify it contains original page\n    int fd = open(db->journal_path, O_RDONLY);\n    JournalHeader header;\n    read(fd, &header, sizeof(header));\n    uint32_t page_id;\n    read(fd, &page_id, sizeof(page_id));\n    char page_data[PAGE_SIZE];\n    read(fd, page_data, PAGE_SIZE);\n    // The original page should contain 'original', not 'modified'\n    // (This requires deserializing the page to check)\n    close(fd);\n    execute_sql(db, \"ROLLBACK\");\n    free_database(db);\n}\n```\n### Crash Recovery Tests\n```c\nvoid test_crash_recovery_rollback() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 100)\");\n    // Begin transaction and modify\n    execute_sql(db, \"BEGIN\");\n    execute_sql(db, \"UPDATE test SET value = 200 WHERE id = 1\");\n    // Simulate crash: don't commit, just close\n    char* journal_path = strdup(db->journal_path);\n    simulate_crash(db);  // Closes database without cleanup\n    // Reopen database - should trigger recovery\n    db = reopen_database(db);\n    // Recovery should have rolled back the transaction\n    Result* r = execute_sql(db, \"SELECT value FROM test WHERE id = 1\");\n    assert(r->rows[0].columns[0].data.integer_val == 100);  // Original value\n    // Journal should be deleted after recovery\n    assert(!file_exists(journal_path));\n    free(journal_path);\n    free_database(db);\n}\nvoid test_no_recovery_when_journal_absent() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1)\");\n    execute_sql(db, \"BEGIN\");\n    execute_sql(db, \"INSERT INTO test VALUES (2)\");\n    execute_sql(db, \"COMMIT\");  // Journal deleted on commit\n    // Simulate crash\n    simulate_crash(db);\n    // Reopen - no recovery needed\n    db = reopen_database(db);\n    // All data should be intact\n    Result* r = execute_sql(db, \"SELECT COUNT(*) FROM test\");\n    assert(r->rows[0].columns[0].data.integer_val == 2);\n    free_database(db);\n}\n```\n### fsync Verification Tests\n```c\nvoid test_journal_synced_before_database_write() {\n    // This test requires instrumenting the code to track fsync order\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY)\");\n    // Enable fsync tracking\n    FsyncTracker* tracker = create_fsync_tracker();\n    install_fsync_tracker(tracker);\n    execute_sql(db, \"BEGIN\");\n    execute_sql(db, \"INSERT INTO test VALUES (1)\");\n    execute_sql(db, \"COMMIT\");\n    // Verify fsync order: journal sync must come before database sync\n    assert(tracker->sync_count >= 2);\n    assert(tracker->syncs[0].type == SYNC_JOURNAL);\n    assert(tracker->syncs[1].type == SYNC_DATABASE);\n    uninstall_fsync_tracker();\n    free_fsync_tracker(tracker);\n    free_database(db);\n}\n```\n---\n## What You've Built\nYour rollback journal system provides:\n1. **BEGIN** starts a transaction, deferring journal creation until the first write\n2. **Journal file** captures original page contents before any modification\n3. **Write ordering** ensures journal is fsync'd before database pages are written\n4. **COMMIT** flushes dirty pages to disk, syncs the database, and deletes the journal\n5. **ROLLBACK** restores original pages from the journal and deletes it\n6. **Crash recovery** detects hot journals on startup and automatically rolls back incomplete transactions\n7. **Basic isolation** keeps uncommitted changes invisible to other connections\nThis is the foundation of ACID transactions. With this system, your database can survive crashes without data corruption. Every committed transaction is durable; every uncommitted transaction is rolled back.\n---\n## Knowledge Cascade\n### 1. Git's Reflog: A Rollback Journal for Repository State\nGit's reflog is essentially a rollback journal for your repository:\n```bash\ngit reflog  # Shows history of HEAD movements\ngit reset HEAD@{1}  # \"Rollback\" to previous state\n```\nWhen you run `git commit`, Git:\n1. Creates new objects (like writing to journal)\n2. Updates refs (like writing to database)\n3. Records in reflog (like journal deletion would confirm commit)\nA crash mid-rebase? The reflog lets you recover. It's the same undo-log pattern, applied to version control.\n### 2. Installers and System Updates\nOperating system updaters use journaling:\n1. Create restore point (journal original state)\n2. Apply updates (modify system)\n3. If update fails, restore from restore point (rollback)\nWindows System Restore, macOS Time Machine snapshots, and Linux package managers all use this pattern. The \"restore previous version\" option in installers is a user-visible rollback.\n### 3. Distributed Consensus: Raft's Log\nRaft (and Paxos) uses a write-ahead log for distributed consensus:\n1. Leader appends entry to log (like journaling)\n2. Replicates log to followers\n3. Commits when majority have entry (like fsync)\n4. Applies to state machine (like writing to database)\nThe log provides durability and ordering \u2014 the same properties your rollback journal provides for a single-node database, but replicated across nodes.\n### 4. Filesystem Journaling: ext4, HFS+, NTFS\nModern filesystems use journaling to survive crashes:\n- **Metadata journaling**: Log directory changes before applying\n- **Full data journaling**: Log file contents before writing\nWhen the system crashes, the filesystem replays or discards journal entries. ext4's `data=ordered` mode is similar to your rollback journal: data is written before metadata is committed.\n### 5. Text Editor Undo/Redo\nYour text editor's undo stack is a journal:\n```\nOriginal: \"Hello\"\nEdit 1:   \"Hello World\"  (undo record: delete \" World\")\nEdit 2:   \"Hello World!\" (undo record: delete \"!\")\n```\nCtrl+Z replays undo records \u2014 rolling back your edits. The pattern is identical: record the inverse operation before applying the change.\n---\n## What's Next\nWith rollback journal transactions, your database can survive crashes. But there's a limitation: **only one writer at a time.** The rollback journal requires exclusive access to the database file during writes, blocking all readers.\nThe next milestone, **WAL Mode**, introduces write-ahead logging \u2014 a different approach that allows concurrent readers while a writer is active. Instead of undo information, WAL stores redo information (new page images) in a separate log, enabling readers to access the old database version while the writer appends to the log.\nYour database is now crash-safe. Next, it becomes concurrent.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m9\", \"criteria\": [\"Transaction manager tracks state: AUTOCOMMIT, STARTED, DIRTY, COMMITTING, ROLLING_BACK\", \"BEGIN statement transitions from AUTOCOMMIT to STARTED state, preparing journal path but not creating file\", \"Journal file (.db-journal) is created lazily on first write within a transaction\", \"Journal file format includes 28-byte header with magic number (0xd9d505f9), format version, page size, sequence number, salt values, and checksums\", \"Each page record in journal contains 4-byte page number (big-endian) followed by page_size bytes of original page content\", \"Before modifying any page, original content is written to journal (write-before-modify guarantee)\", \"Duplicate journaling of the same page within one transaction is prevented (check if page already journaled)\", \"COMMIT executes sequence: fsync(journal) \u2192 flush dirty pages to database \u2192 fsync(database) \u2192 delete journal\", \"fsync on journal file is called BEFORE any modified pages are written to the database file (critical write ordering)\", \"COMMIT deletes journal file only after database fsync completes successfully\", \"ROLLBACK reads all page records from journal and restores original content to database file\", \"ROLLBACK writes restored pages directly to disk (not just buffer pool) to ensure durability of the rollback itself\", \"ROLLBACK deletes journal file after restoration is complete\", \"Crash recovery on startup checks for existence of hot journal file\", \"Hot journal detection validates magic number and checksums before attempting recovery\", \"Crash recovery restores all pages from valid journal to database file, then deletes the journal\", \"Recovery is idempotent: if interrupted, next restart continues recovery from the same journal\", \"Basic read isolation: uncommitted changes in dirty buffer pool pages are not visible to other connections\", \"Transaction state machine prevents invalid transitions (e.g., COMMIT without BEGIN returns error)\", \"Nested transactions return appropriate error (not supported in basic implementation)\", \"Autocommit mode: each statement outside BEGIN/COMMIT is its own transaction\", \"Test suite validates BEGIN/COMMIT persists changes\", \"Test suite validates BEGIN/ROLLBACK discards changes\", \"Test suite validates journal file is created on first write and deleted on commit/rollback\", \"Test suite validates crash recovery restores original state from journal\", \"Test suite validates no recovery occurs when journal is absent (clean shutdown)\", \"Test suite validates journal fsync occurs before database write (order verification)\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m10 -->\n# WAL Mode: Inverting the Recovery Model for Concurrency\nYour database can now survive crashes. The rollback journal ensures atomicity \u2014 every transaction is all-or-nothing. But there's a problem hiding in plain sight, and it's not about durability.\nIt's about *concurrency*.\nConsider what happens when a long-running analytical query meets a write transaction:\n```sql\n-- Connection A: Analytical query reading 10 million rows\nSELECT category, AVG(price) FROM products GROUP BY category;\n-- Connection B: Simultaneous write\nUPDATE products SET price = price * 1.1 WHERE category = 'electronics';\nCOMMIT;\n```\nWith the rollback journal, Connection B's UPDATE must acquire an exclusive lock on the database file. Why? Because the journal contains the *original* pages \u2014 the pages that Connection A might still be reading. If Connection B modifies those pages while Connection A is scanning them, Connection A sees inconsistent data: some rows at the old price, some at the new price.\nThe rollback journal's solution: **block all readers while writing.**\nThis works, but it's catastrophic for workloads with mixed reads and writes. A 30-second analytical query blocks all writes for 30 seconds. A high-frequency write workload blocks all reads continuously. The database becomes a serialization point \u2014 one operation at a time.\nYou need a different model. Not \"write to database, journal the old pages.\" But \"write to a log, checkpoint later.\" This is **Write-Ahead Logging**, and it inverts everything you know about transaction logging.\n---\n## The Tension: Readers and Writers Cannot Share the Same File\nHere's the fundamental constraint: **readers need a consistent view of the database, and writers need to modify it.**\nWith the rollback journal, these needs conflict:\n- **Writer**: Must modify pages to commit changes\n- **Reader**: Must read pages without them changing mid-query\nThe rollback journal resolves this by forcing writers to wait for readers to finish. This is **pessimistic concurrency** \u2014 assume conflict will happen, prevent it preemptively.\nBut consider what a writer actually needs to do:\n1. Modify pages in memory\n2. Ensure those modifications survive a crash\n3. Make them visible to future readers\nStep 2 is about durability \u2014 the rollback journal handles this. But step 3, making changes visible, doesn't require modifying the main database file *immediately*. What if we could delay that?\n### The Key Insight: Readers Don't Need the Latest Version\nWhen Connection A starts its analytical query at time T=0, it needs to see a consistent snapshot of the database *as of T=0*. It doesn't need to see changes committed at T=1, T=2, or T=10. It just needs *one* consistent view.\nWhat if:\n- Writers append modified pages to a separate log file (the WAL)\n- Readers check the log for page updates *before* reading the main database\n- Each reader remembers which log entries existed when it started\n- A background process periodically copies log entries back to the main database (checkpoint)\nNow writers never modify the main database file directly. They append to the WAL \u2014 an append-only operation that doesn't conflict with readers. Readers get a consistent snapshot by ignoring WAL entries newer than their start time.\n\n![Rollback Journal vs WAL: Architectural Difference](./diagrams/diag-wal-vs-journal.svg)\n\nThis is **optimistic concurrency** \u2014 assume conflict won't happen, detect it if it does. The WAL enables this by giving each reader its own temporal view of the database.\n### The Numbers That Matter\nLet's quantify the difference:\n| Scenario | Rollback Journal | WAL Mode |\n|----------|-----------------|----------|\n| Read during write | BLOCKED | ALLOWED |\n| Write during read | BLOCKED | ALLOWED |\n| Multiple writers | BLOCKED | BLOCKED (one at a time) |\n| Concurrent readers | ALLOWED | ALLOWED |\n| Read throughput (mixed workload) | ~100/sec | ~10,000/sec |\n| Write latency (with active readers) | 0-30 seconds | 1-5 milliseconds |\nThe difference isn't incremental \u2014 it's two orders of magnitude for read-heavy workloads. A database that handles 100 reads/second with rollback journal can handle 10,000 reads/second with WAL, simply because reads don't block on writes.\n---\n## The Revelation: WAL Stores NEW Pages, Not OLD Pages\nHere's the misconception that will prevent you from understanding WAL: *WAL is just a journal with a different name \u2014 the same thing, optimized slightly differently.*\n**No.** WAL inverts the rollback journal's approach entirely.\n| Aspect | Rollback Journal | Write-Ahead Log |\n|--------|-----------------|-----------------|\n| Stores | Original pages (before modification) | Modified pages (after modification) |\n| Purpose | Undo incomplete transactions | Redo committed transactions |\n| Recovery | Roll back from journal | Roll forward from WAL |\n| Readers | Read main database (blocked during writes) | Check WAL first, then main database |\n| Writes | Modify main database, journal originals | Append to WAL, checkpoint later |\nThis inversion is why WAL enables concurrent readers.\n### The Rollback Journal Model\n```\n1. Write original page to journal\n2. Modify page in main database\n3. Delete journal on commit\nRecovery: If journal exists, restore original pages \u2192 UNDO\n```\nThe journal exists to *undo* incomplete transactions. The main database always contains the latest committed state (after commit). Readers read the main database, so they see only committed changes.\nBut this means the main database is being modified *during* the transaction. Readers must be blocked.\n### The WAL Model\n```\n1. Modify page in memory\n2. Append modified page to WAL\n3. On checkpoint: copy WAL pages to main database\nRecovery: If WAL exists, replay entries \u2192 REDO\n```\nThe WAL exists to *redo* committed transactions. The main database lags behind \u2014 it contains the state as of the last checkpoint. Readers check the WAL for newer page versions.\nThe main database is only modified during checkpoint, not during normal transactions. Readers can proceed without blocking.\n\n![WAL File Format](./diagrams/diag-wal-structure.svg)\n\n### The \"Aha!\" Moment: Time Travel for Pages\nHere's the key insight: **a page in the WAL is a newer version of a page in the database.**\nWhen you read page 42:\n1. Check the WAL for entries with page_id = 42\n2. If found, use the most recent entry (up to your snapshot time)\n3. If not found, read from the main database\nThe WAL is like a version history for each page. Readers choose which \"version\" to see based on when they started. A reader that started before a commit sees the old version (in the main database). A reader that started after sees the new version (in the WAL).\nThis is **snapshot isolation** \u2014 each transaction sees a consistent snapshot of the database as of its start time.\n---\n## WAL File Structure\nThe WAL file is an append-only log of page modifications. Its structure is designed for efficient reads and safe recovery.\n### WAL File Format\n```\n+------------------+\n| WAL Header       |  32 bytes\n+------------------+\n| Frame 1          |  24-byte header + page_size bytes\n+------------------+\n| Frame 2          |\n+------------------+\n| ...              |\n+------------------+\n| Frame N          |\n+------------------+\n```\n### WAL Header (32 bytes)\n```c\ntypedef struct {\n    uint32_t magic;           // 0x377f0682 or 0x377f0683 (big/little endian)\n    uint32_t format_version;  // Format version: 3007000\n    uint32_t page_size;       // Database page size\n    uint32_t checkpoint_seq;  // Checkpoint sequence number\n    uint32_t salt_1;          // Random value for frame validation\n    uint32_t salt_2;          // Random value for frame validation\n    uint32_t checksum_1;      // Checksum of first 24 bytes\n    uint32_t checksum_2;      // Checksum of first 24 bytes (different algorithm)\n} WALHeader;\n```\nThe magic number indicates byte order. Salt values change on each checkpoint and are used to validate that frames belong to the current WAL.\n### Frame Header (24 bytes)\n```c\ntypedef struct {\n    uint32_t page_number;     // Which page in the database\n    uint32_t commit_size;     // For commit frames: size of database in pages\n                              // For non-commit frames: 0\n    uint32_t salt_1;          // Must match WAL header salt_1\n    uint32_t salt_2;          // Must match WAL header salt_2\n    uint32_t checksum_1;      // Cumulative checksum including this frame\n    uint32_t checksum_2;      // Cumulative checksum including this frame\n} FrameHeader;\n```\nEach frame contains one modified page. The checksums are **cumulative** \u2014 each frame's checksum includes all previous frames. This enables detection of corruption at any point in the WAL.\n### Commit Markers\nA frame with `commit_size > 0` marks a transaction commit. All frames from the previous commit marker to this frame belong to the same transaction.\n```\nFrame 1: page=42, commit_size=0  (part of transaction 1)\nFrame 2: page=17, commit_size=0  (part of transaction 1)\nFrame 3: page=42, commit_size=100 (commits transaction 1, db size = 100 pages)\nFrame 4: page=55, commit_size=0  (part of transaction 2)\nFrame 5: page=55, commit_size=100 (commits transaction 2)\n```\nReaders use commit markers to determine which frames are \"complete\" transactions. A frame after the last commit marker is an incomplete transaction \u2014 readers ignore it.\n---\n## The WAL Index: Finding Pages Fast\nReaders need to find the most recent version of a page in the WAL. Scanning the entire WAL for every page read would be catastrophically slow \u2014 O(n) per page, where n is the number of WAL frames.\nThe solution is the **WAL Index** \u2014 an in-memory hash table mapping page numbers to their most recent frame locations.\n### WAL Index Structure\n```c\ntypedef struct {\n    uint32_t page_number;     // Page ID\n    uint32_t frame_offset;    // Offset in WAL file where frame starts\n    uint32_t frame_count;     // Number of times this page appears in WAL\n                              // (for tracking multiple versions)\n} WALIndexEntry;\ntypedef struct {\n    WALIndexEntry* entries;   // Hash table entries\n    int capacity;\n    int count;\n    uint32_t max_frame;       // Highest frame number in WAL\n    uint32_t min_frame;       // Oldest valid frame (after checkpoint)\n} WALIndex;\n```\n### Building the Index\nThe WAL index is rebuilt on each database open:\n```c\nint wal_build_index(WAL* wal) {\n    // Read WAL header\n    WALHeader header;\n    pread(wal->fd, &header, sizeof(header), 0);\n    // Validate magic number\n    if (header.magic != WAL_MAGIC_BE && header.magic != WAL_MAGIC_LE) {\n        return ERROR_INVALID_WAL;\n    }\n    // Scan all frames\n    off_t offset = sizeof(WALHeader);\n    uint32_t cumulative_checksum_1 = 0;\n    uint32_t cumulative_checksum_2 = 0;\n    while (true) {\n        FrameHeader frame;\n        ssize_t bytes_read = pread(wal->fd, &frame, sizeof(frame), offset);\n        if (bytes_read < sizeof(frame)) {\n            break;  // End of WAL\n        }\n        // Validate salts\n        if (frame.salt_1 != header.salt_1 || frame.salt_2 != header.salt_2) {\n            // Frame from previous checkpoint cycle \u2014 stop here\n            break;\n        }\n        // Validate checksums\n        if (!validate_frame_checksum(&frame, cumulative_checksum_1, \n                                      cumulative_checksum_2)) {\n            log_warning(\"WAL frame checksum mismatch at offset %ld\", offset);\n            break;  // Corrupted frame \u2014 stop here\n        }\n        // Update cumulative checksums\n        cumulative_checksum_1 = frame.checksum_1;\n        cumulative_checksum_2 = frame.checksum_2;\n        // Add to index\n        wal_index_add(wal->index, frame.page_number, offset);\n        // Track commit markers\n        if (frame.commit_size > 0) {\n            wal->last_commit_frame = offset;\n        }\n        // Move to next frame\n        offset += sizeof(FrameHeader) + header.page_size;\n    }\n    wal->wal_size = offset;\n    return 0;\n}\n```\n### Reading Through the WAL Index\n```c\nint wal_read_page(WAL* wal, uint32_t page_id, uint32_t max_frame, char* buffer) {\n    // Check WAL index for this page\n    WALIndexEntry* entry = wal_index_lookup(wal->index, page_id);\n    if (entry == NULL || entry->frame_offset > max_frame) {\n        // Page not in WAL (or version too new for this reader)\n        return WAL_NOT_FOUND;\n    }\n    // Read the frame\n    off_t frame_offset = entry->frame_offset;\n    FrameHeader header;\n    pread(wal->fd, &header, sizeof(header), frame_offset);\n    // Read page data\n    pread(wal->fd, buffer, wal->page_size, frame_offset + sizeof(FrameHeader));\n    return WAL_FOUND;\n}\n```\nThe `max_frame` parameter enables snapshot isolation \u2014 a reader only sees frames that existed when it started.\n---\n## Reading with Snapshot Isolation\nWhen a reader starts a transaction, it captures the current WAL state:\n```c\ntypedef struct {\n    uint32_t snapshot_max_frame;  // Highest frame visible to this reader\n    uint32_t snapshot_commit;     // Last commit frame visible\n} ReadSnapshot;\nReadSnapshot* wal_begin_read(WAL* wal) {\n    ReadSnapshot* snapshot = malloc(sizeof(ReadSnapshot));\n    // Lock WAL briefly to get consistent view\n    pthread_mutex_lock(&wal->lock);\n    snapshot->snapshot_max_frame = wal->index->max_frame;\n    snapshot->snapshot_commit = wal->last_commit_frame;\n    pthread_mutex_unlock(&wal->lock);\n    return snapshot;\n}\n```\n### Page Read with Snapshot\n```c\nint buffer_pool_fetch_page_wal(BufferPool* pool, PageId page_id, \n                                ReadSnapshot* snapshot, char* buffer) {\n    // First, try the WAL\n    WAL* wal = pool->wal;\n    if (wal != NULL && wal->mode == WAL_MODE_ON) {\n        int result = wal_read_page(wal, page_id, \n                                    snapshot->snapshot_max_frame, buffer);\n        if (result == WAL_FOUND) {\n            return 0;  // Page found in WAL\n        }\n    }\n    // Fall back to main database\n    return disk_read(pool->file_manager, page_id, buffer);\n}\n```\n\n![Snapshot Isolation: Reader View](./diagrams/diag-snapshot-isolation.svg)\n\nThis is the magic of WAL: **readers never block on writers.** A writer can append to the WAL while a reader is querying. The reader simply ignores frames newer than its snapshot.\n### Multiple Versions of the Same Page\nWhat if a page is modified multiple times within the WAL?\n```\nFrame 1: page 42 version 1\nFrame 2: page 17 version 1\nFrame 3: page 42 version 2  (newer version of page 42)\nFrame 4: page 42 version 3  (even newer)\n```\nThe WAL index only stores the *most recent* version of each page. But for snapshot isolation, we need to track all versions:\n```c\ntypedef struct {\n    uint32_t page_number;\n    uint32_t* frame_offsets;  // Array of frame offsets (newest first)\n    int frame_count;\n} WALIndexEntryMulti;\nuint32_t wal_find_frame_for_snapshot(WALIndexEntryMulti* entry, \n                                      uint32_t max_frame) {\n    // Find the most recent frame that is <= max_frame\n    for (int i = 0; i < entry->frame_count; i++) {\n        if (entry->frame_offsets[i] <= max_frame) {\n            return entry->frame_offsets[i];\n        }\n    }\n    return 0;  // No version in snapshot \u2014 read from main database\n}\n```\nFor simplicity, many implementations (including SQLite) only track the most recent version and rely on checkpointing to keep the WAL small. If a reader needs an older version that's been checkpointed away, it reads from the main database.\n---\n## Writing to the WAL\nWriters append modified pages to the WAL instead of modifying the main database:\n```c\ntypedef struct {\n    WAL* wal;\n    uint32_t first_frame_offset;  // Offset of first frame in this transaction\n    uint32_t current_frame_offset;\n    uint32_t pages_written;\n    bool active;\n} WALWriteTransaction;\nint wal_begin_write(WAL* wal, WALWriteTransaction* txn) {\n    // Wait for exclusive write lock\n    pthread_mutex_lock(&wal->write_lock);\n    txn->wal = wal;\n    txn->first_frame_offset = 0;\n    txn->current_frame_offset = wal->wal_size;\n    txn->pages_written = 0;\n    txn->active = true;\n    return 0;\n}\nint wal_write_page(WALWriteTransaction* txn, uint32_t page_id, \n                   const char* page_data) {\n    WAL* wal = txn->wal;\n    // Build frame header\n    FrameHeader header;\n    header.page_number = page_id;\n    header.commit_size = 0;  // Not a commit frame\n    header.salt_1 = wal->header.salt_1;\n    header.salt_2 = wal->header.salt_2;\n    // Calculate checksums (cumulative)\n    calculate_frame_checksum(&header, wal->cumulative_checksum_1, \n                             wal->cumulative_checksum_2);\n    // Write frame header\n    pwrite(wal->fd, &header, sizeof(header), txn->current_frame_offset);\n    // Write page data\n    pwrite(wal->fd, page_data, wal->page_size, \n           txn->current_frame_offset + sizeof(FrameHeader));\n    // Update index\n    wal_index_add(wal->index, page_id, txn->current_frame_offset);\n    // Update state\n    if (txn->first_frame_offset == 0) {\n        txn->first_frame_offset = txn->current_frame_offset;\n    }\n    txn->current_frame_offset += sizeof(FrameHeader) + wal->page_size;\n    txn->pages_written++;\n    return 0;\n}\n```\n### Committing a Write Transaction\n```c\nint wal_commit_transaction(WALWriteTransaction* txn, uint32_t db_size) {\n    WAL* wal = txn->wal;\n    if (txn->pages_written == 0) {\n        // No pages written \u2014 nothing to commit\n        pthread_mutex_unlock(&wal->write_lock);\n        txn->active = false;\n        return 0;\n    }\n    // The last frame is the commit marker\n    // Rewrite its header with commit_size set\n    off_t last_frame_offset = txn->current_frame_offset - \n                              (sizeof(FrameHeader) + wal->page_size);\n    FrameHeader commit_header;\n    pread(wal->fd, &commit_header, sizeof(commit_header), last_frame_offset);\n    commit_header.commit_size = db_size;\n    // Recalculate checksums\n    calculate_frame_checksum(&commit_header, wal->cumulative_checksum_1,\n                             wal->cumulative_checksum_2);\n    pwrite(wal->fd, &commit_header, sizeof(commit_header), last_frame_offset);\n    // Sync the WAL to ensure durability\n    fsync(wal->fd);\n    // Update WAL state\n    wal->wal_size = txn->current_frame_offset;\n    wal->last_commit_frame = last_frame_offset;\n    // Release write lock\n    pthread_mutex_unlock(&wal->write_lock);\n    txn->active = false;\n    // Check if auto-checkpoint is needed\n    if (wal_should_checkpoint(wal)) {\n        wal_checkpoint(wal, CHECKPOINT_PASSIVE);\n    }\n    return 0;\n}\n```\n---\n## The Checkpoint: WAL Meets Database\nThe WAL cannot grow forever. Periodically, modified pages must be copied back to the main database file. This is the **checkpoint**.\n\n![WAL Checkpoint Process](./diagrams/diag-wal-checkpoint.svg)\n\n### Why Checkpoint is Necessary\nWithout checkpointing:\n1. WAL grows unbounded \u2014 eventually consuming all disk space\n2. Recovery takes longer \u2014 more frames to replay\n3. Read performance degrades \u2014 more WAL entries to check per page\nWith checkpointing:\n1. WAL is truncated after copying to database\n2. Recovery only replays post-checkpoint frames\n3. Readers find most pages in the main database\n### Checkpoint Modes\nSQLite defines several checkpoint modes:\n| Mode | Behavior | Blocking |\n|------|----------|----------|\n| PASSIVE | Checkpoint as much as possible without blocking | No blocking |\n| FULL | Wait for readers, checkpoint all frames, block new readers | Blocks briefly |\n| RESTART | Like FULL, but also blocks writers until complete | Blocks longer |\n| TRUNCATE | Like RESTART, but also truncates WAL file | Blocks longest |\nFor your implementation, start with PASSIVE and FULL.\n### Checkpoint Implementation\n```c\nint wal_checkpoint(WAL* wal, CheckpointMode mode) {\n    // Get list of pages to checkpoint\n    uint32_t checkpoint_up_to = wal->last_commit_frame;\n    if (checkpoint_up_to == 0) {\n        return 0;  // Nothing to checkpoint\n    }\n    // For FULL mode, wait for readers to finish\n    if (mode == CHECKPOINT_FULL || mode == CHECKPOINT_RESTART) {\n        pthread_mutex_lock(&wal->reader_lock);\n        while (wal->active_reader_count > 0) {\n            pthread_cond_wait(&wal->reader_cond, &wal->reader_lock);\n        }\n    }\n    // Collect pages to checkpoint\n    // We need to write each page only once (the latest version)\n    HashTable* pages_to_checkpoint = hash_table_create();\n    off_t offset = sizeof(WALHeader);\n    while (offset <= checkpoint_up_to) {\n        FrameHeader frame;\n        pread(wal->fd, &frame, sizeof(frame), offset);\n        // Remember the latest offset for each page\n        hash_table_put(pages_to_checkpoint, frame.page_number, \n                       (void*)(uint64_t)offset);\n        offset += sizeof(FrameHeader) + wal->page_size;\n    }\n    // Write pages to main database\n    HashTableIterator* iter = hash_table_iterator(pages_to_checkpoint);\n    while (hash_table_next(iter)) {\n        uint32_t page_id = iter->key;\n        off_t frame_offset = (off_t)(uint64_t)iter->value;\n        // Read page from WAL\n        char page_data[wal->page_size];\n        pread(wal->fd, page_data, wal->page_size, \n              frame_offset + sizeof(FrameHeader));\n        // Write to main database\n        disk_write(wal->db_file_manager, page_id, page_data);\n    }\n    // Sync the database\n    fsync(wal->db_file_manager->fd);\n    // Update checkpoint state\n    wal->header.checkpoint_seq++;\n    // Generate new salts (invalidates old frames)\n    wal->header.salt_1 = random();\n    wal->header.salt_2 = random();\n    // Truncate or reset WAL\n    if (mode == CHECKPOINT_TRUNCATE) {\n        ftruncate(wal->fd, sizeof(WALHeader));\n        wal->wal_size = sizeof(WALHeader);\n    } else {\n        // Keep WAL but reset write position\n        wal->wal_size = sizeof(WALHeader);\n    }\n    // Clear WAL index\n    wal_index_clear(wal->index);\n    if (mode == CHECKPOINT_FULL || mode == CHECKPOINT_RESTART) {\n        pthread_mutex_unlock(&wal->reader_lock);\n    }\n    hash_table_free(pages_to_checkpoint);\n    free(iter);\n    return 0;\n}\n```\n### Auto-Checkpoint\nTo prevent unbounded WAL growth, checkpoint automatically after a configurable number of pages:\n```c\nbool wal_should_checkpoint(WAL* wal) {\n    uint32_t frames_since_checkpoint = \n        (wal->wal_size - sizeof(WALHeader)) / (sizeof(FrameHeader) + wal->page_size);\n    return frames_since_checkpoint >= wal->auto_checkpoint_threshold;\n}\n// Configuration\nvoid wal_set_auto_checkpoint(WAL* wal, uint32_t page_threshold) {\n    wal->auto_checkpoint_threshold = page_threshold;\n}\n// Default: checkpoint every 1000 pages\n#define DEFAULT_AUTO_CHECKPOINT 1000\n```\n---\n## Concurrent Readers and Writers\n\n![WAL Mode: Concurrent Readers and Writer](./diagrams/diag-concurrent-read-write.svg)\n\nThe key to WAL concurrency is that readers and writers access different things:\n- **Writers** append to the end of the WAL file\n- **Readers** search the WAL index and read existing frames\nThese operations don't conflict because:\n1. Appending to a file doesn't affect existing content\n2. Reading existing content doesn't block appends\n3. The WAL index is updated *after* the frame is written\n### Reader Registration\n```c\nint wal_begin_read_transaction(WAL* wal, ReadSnapshot* snapshot) {\n    pthread_mutex_lock(&wal->lock);\n    // Capture current state\n    snapshot->snapshot_max_frame = wal->index->max_frame;\n    snapshot->snapshot_commit = wal->last_commit_frame;\n    // Register as active reader (prevents checkpoint from truncating needed frames)\n    wal->active_readers++;\n    wal->reader_min_frame = MIN(wal->reader_min_frame, \n                                 snapshot->snapshot_max_frame);\n    pthread_mutex_unlock(&wal->lock);\n    return 0;\n}\nint wal_end_read_transaction(WAL* wal, ReadSnapshot* snapshot) {\n    pthread_mutex_lock(&wal->lock);\n    wal->active_readers--;\n    if (wal->active_readers == 0) {\n        // Signal any waiting checkpoint\n        pthread_cond_signal(&wal->reader_cond);\n    }\n    pthread_mutex_unlock(&wal->lock);\n    return 0;\n}\n```\n### Writer Blocking\nOnly one writer at a time:\n```c\nint wal_begin_write(WAL* wal) {\n    pthread_mutex_lock(&wal->write_lock);\n    // Wait if another write is in progress\n    while (wal->write_in_progress) {\n        pthread_cond_wait(&wal->write_cond, &wal->write_lock);\n    }\n    wal->write_in_progress = true;\n    pthread_mutex_unlock(&wal->write_lock);\n    return 0;\n}\n```\n### The Coordination Dance\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           CONCURRENT READ/WRITE COORDINATION                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  Writer:                                                        \u2502\n\u2502  1. Acquire write lock                                          \u2502\n\u2502  2. Append frames to WAL                                        \u2502\n\u2502  3. Update WAL index                                            \u2502\n\u2502  4. Release write lock                                          \u2502\n\u2502  5. (Auto-checkpoint if needed)                                 \u2502\n\u2502                                                                 \u2502\n\u2502  Reader:                                                        \u2502\n\u2502  1. Capture snapshot (max_frame)                                \u2502\n\u2502  2. Register as active reader                                   \u2502\n\u2502  3. For each page needed:                                       \u2502\n\u2502     a. Check WAL index for page <= max_frame                    \u2502\n\u2502     b. If found, read from WAL                                  \u2502\n\u2502     c. If not found, read from main database                    \u2502\n\u2502  4. Unregister as active reader                                 \u2502\n\u2502                                                                 \u2502\n\u2502  Checkpoint (PASSIVE):                                          \u2502\n\u2502  1. Identify frames to checkpoint                               \u2502\n\u2502  2. Copy frames to main database                                \u2502\n\u2502  3. Update checkpoint marker                                    \u2502\n\u2502  4. (Do NOT truncate if readers might need old frames)          \u2502\n\u2502                                                                 \u2502\n\u2502  Checkpoint (FULL):                                             \u2502\n\u2502  1. Wait for all readers to finish                              \u2502\n\u2502  2. Copy all frames to main database                            \u2502\n\u2502  3. Reset WAL (new salts, truncate optional)                    \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n---\n## WAL Recovery\nWhen the database starts up with a WAL file present, recovery replays committed frames:\n```c\nint wal_recover(Database* db) {\n    WAL* wal = db->wal;\n    // Check if WAL file exists\n    if (!file_exists(wal->path)) {\n        return 0;  // No WAL, nothing to recover\n    }\n    // Open WAL file\n    wal->fd = open(wal->path, O_RDWR);\n    if (wal->fd < 0) {\n        return ERROR_CANNOT_OPEN_WAL;\n    }\n    // Read and validate header\n    WALHeader header;\n    read(wal->fd, &header, sizeof(header));\n    if (header.magic != WAL_MAGIC_BE && header.magic != WAL_MAGIC_LE) {\n        close(wal->fd);\n        return ERROR_INVALID_WAL;\n    }\n    // Scan WAL and replay committed transactions\n    off_t offset = sizeof(WALHeader);\n    off_t last_commit_offset = 0;\n    uint32_t checksum_1 = 0, checksum_2 = 0;\n    while (true) {\n        FrameHeader frame;\n        ssize_t bytes = read(wal->fd, &frame, sizeof(frame));\n        if (bytes < sizeof(frame)) {\n            break;\n        }\n        // Validate salts\n        if (frame.salt_1 != header.salt_1 || frame.salt_2 != header.salt_2) {\n            break;\n        }\n        // Validate checksums\n        if (!validate_frame_checksum(&frame, checksum_1, checksum_2)) {\n            log_warning(\"Corrupt WAL frame at offset %ld, stopping recovery\", offset);\n            break;\n        }\n        checksum_1 = frame.checksum_1;\n        checksum_2 = frame.checksum_2;\n        // Read page data\n        char page_data[header.page_size];\n        read(wal->fd, page_data, header.page_size);\n        // Track commit markers\n        if (frame.commit_size > 0) {\n            last_commit_offset = offset;\n        }\n        offset += sizeof(FrameHeader) + header.page_size;\n    }\n    // Replay only committed frames\n    if (last_commit_offset > 0) {\n        log_info(\"WAL recovery: replaying up to offset %ld\", last_commit_offset);\n        offset = sizeof(WALHeader);\n        while (offset <= last_commit_offset) {\n            FrameHeader frame;\n            pread(wal->fd, &frame, sizeof(frame), offset);\n            char page_data[header.page_size];\n            pread(wal->fd, page_data, header.page_size, \n                  offset + sizeof(FrameHeader));\n            // Write page to main database\n            disk_write(db->file_manager, frame.page_number, page_data);\n            offset += sizeof(FrameHeader) + header.page_size;\n        }\n        // Sync database\n        fsync(db->file_manager->fd);\n        log_info(\"WAL recovery complete\");\n    }\n    // Build WAL index for remaining frames (after last commit)\n    // ... (index building code) ...\n    return 0;\n}\n```\n### Recovery Semantics\nThe key insight: **only committed frames are replayed.**\nIf a crash occurred during a write transaction, the uncommitted frames (after the last commit marker) are ignored. This is the same atomicity guarantee as the rollback journal, achieved through a different mechanism (redo instead of undo).\n---\n## Switching Between Modes\nSQLite allows switching between rollback journal and WAL modes:\n```sql\nPRAGMA journal_mode=DELETE;  -- Rollback journal (default)\nPRAGMA journal_mode=WAL;     -- Write-ahead logging\nPRAGMA journal_mode=MEMORY;  -- In-memory journal (no durability)\n```\n### Switching to WAL Mode\n```c\nint switch_to_wal_mode(Database* db) {\n    if (db->wal != NULL && db->wal->mode == WAL_MODE_ON) {\n        return 0;  // Already in WAL mode\n    }\n    // Ensure no active transactions\n    if (db->txn_manager->state != TXN_AUTOCOMMIT) {\n        return ERROR_ACTIVE_TRANSACTION;\n    }\n    // Create WAL file\n    WAL* wal = wal_create(db->file_path, db->page_size);\n    // Initialize WAL header\n    wal->header.magic = WAL_MAGIC_BE;\n    wal->header.format_version = 3007000;\n    wal->header.page_size = db->page_size;\n    wal->header.checkpoint_seq = 0;\n    wal->header.salt_1 = random();\n    wal->header.salt_2 = random();\n    calculate_wal_header_checksum(&wal->header);\n    write(wal->fd, &wal->header, sizeof(wal->header));\n    fsync(wal->fd);\n    // Delete old rollback journal if exists\n    char* journal_path = generate_journal_path(db->file_path);\n    if (file_exists(journal_path)) {\n        unlink(journal_path);\n    }\n    free(journal_path);\n    // Update database mode\n    db->wal = wal;\n    db->journal_mode = JOURNAL_MODE_WAL;\n    return 0;\n}\n```\n### Switching to Rollback Journal\n```c\nint switch_to_rollback_mode(Database* db) {\n    if (db->wal == NULL || db->wal->mode != WAL_MODE_ON) {\n        return 0;  // Already in rollback mode\n    }\n    // Checkpoint all WAL content to database\n    wal_checkpoint(db->wal, CHECKPOINT_TRUNCATE);\n    // Close and delete WAL file\n    close(db->wal->fd);\n    unlink(db->wal->path);\n    // Free WAL structures\n    wal_free(db->wal);\n    db->wal = NULL;\n    db->journal_mode = JOURNAL_MODE_DELETE;\n    return 0;\n}\n```\n---\n## The Three-Level View\n### Level 1: Application (SQL \u2192 Concurrent Access)\nAt the API level, WAL mode is transparent:\n```sql\nPRAGMA journal_mode=WAL;\n-- Now multiple readers and one writer can operate concurrently\nSELECT * FROM large_table;  -- Doesn't block writes\nINSERT INTO large_table VALUES (...);  -- Doesn't block reads\n```\nThe user sees improved concurrency without changing their queries.\n### Level 2: Engine (WAL File + Index + Checkpoint)\nInside the database:\n- **WAL file**: Append-only log of modified pages\n- **WAL index**: In-memory hash table for fast page lookup\n- **Snapshot mechanism**: Readers capture a point-in-time view\n- **Checkpoint process**: Background copy of WAL to database\nThe coordination between these components enables concurrent access without sacrificing ACID guarantees.\n### Level 3: Implementation (File Operations + Checksums + Locks)\nAt the lowest level:\n```c\n// Append to WAL\npwrite(wal_fd, &frame_header, sizeof(frame_header), offset);\npwrite(wal_fd, page_data, page_size, offset + sizeof(frame_header));\n// Checkpoint\npread(wal_fd, page_data, page_size, frame_offset + sizeof(FrameHeader));\npwrite(db_fd, page_data, page_size, page_id * page_size);\nfsync(db_fd);\n// Coordination\npthread_mutex_lock(&lock);\n// ... update shared state ...\npthread_mutex_unlock(&lock);\n```\nThe correctness of WAL mode depends on:\n1. Atomic frame writes (each frame is written completely or not at all)\n2. Cumulative checksums (detect corruption at any point)\n3. Proper locking (readers don't see partial transactions)\n---\n## Common Pitfalls\n### Readers Pinning Old Frames\nIf a reader holds a snapshot for a long time, it prevents checkpoint from truncating frames:\n```c\n// Reader starts, captures snapshot at frame 100\nReadSnapshot* snapshot = wal_begin_read(wal);\n// Writer commits 10,000 frames\n// Auto-checkpoint wants to truncate, but reader might need frame 100\n// Checkpoint cannot proceed \u2014 WAL grows unbounded\n```\n**Solution**: Set a limit on WAL size. If exceeded, either:\n- Block new readers until old ones finish\n- Force-checkpoint and break old snapshots (they'll get errors)\n### Checksum Validation on Big-Endian vs Little-Endian\nThe WAL magic number indicates byte order, but checksums are still tricky:\n```c\n// WRONG: Assuming native byte order\nuint32_t checksum = calculate_checksum(data);\n// RIGHT: Use consistent byte order for checksums\nuint32_t checksum = calculate_checksum_be(data);  // Always big-endian\n```\n### WAL File Growing Without Checkpoint\nIf checkpoint never runs (PASSIVE mode with continuous readers):\n```\nDay 1: WAL = 100 MB\nDay 2: WAL = 500 MB\nDay 3: WAL = 2 GB\n...\n```\n**Solution**: Enforce auto-checkpoint with FULL mode when WAL exceeds a threshold.\n### Crash During Checkpoint\nIf the system crashes during checkpoint:\n- Some pages may have been written to the database\n- Others may not have been\n**Solution**: Checkpoint is idempotent. On recovery, the WAL is replayed, which includes re-writing pages that were partially checkpointed. The cumulative checksums ensure we only replay valid frames.\n---\n## Testing Strategy\n### WAL Read/Write Tests\n```c\nvoid test_wal_write_and_read() {\n    Database* db = create_test_database_wal();\n    // Write a page through WAL\n    char page_data[PAGE_SIZE];\n    memset(page_data, 'A', PAGE_SIZE);\n    WALWriteTransaction txn;\n    wal_begin_write(db->wal, &txn);\n    wal_write_page(&txn, 42, page_data);\n    wal_commit_transaction(&txn, 100);\n    // Read the page back\n    ReadSnapshot* snapshot = wal_begin_read(db->wal);\n    char read_buffer[PAGE_SIZE];\n    int result = wal_read_page(db->wal, 42, snapshot->snapshot_max_frame, \n                                read_buffer);\n    assert(result == WAL_FOUND);\n    assert(memcmp(read_buffer, page_data, PAGE_SIZE) == 0);\n    wal_end_read(db->wal, snapshot);\n    free_database(db);\n}\nvoid test_snapshot_isolation() {\n    Database* db = create_test_database_wal();\n    // Write version 1\n    WALWriteTransaction txn1;\n    wal_begin_write(db->wal, &txn1);\n    char v1[PAGE_SIZE]; memset(v1, 'A', PAGE_SIZE);\n    wal_write_page(&txn1, 42, v1);\n    wal_commit_transaction(&txn1, 100);\n    // Start reader (sees version 1)\n    ReadSnapshot* snapshot = wal_begin_read(db->wal);\n    // Write version 2\n    WALWriteTransaction txn2;\n    wal_begin_write(db->wal, &txn2);\n    char v2[PAGE_SIZE]; memset(v2, 'B', PAGE_SIZE);\n    wal_write_page(&txn2, 42, v2);\n    wal_commit_transaction(&txn2, 100);\n    // Reader should still see version 1\n    char read_buffer[PAGE_SIZE];\n    wal_read_page(db->wal, 42, snapshot->snapshot_max_frame, read_buffer);\n    assert(read_buffer[0] == 'A');  // Version 1\n    // New reader sees version 2\n    ReadSnapshot* snapshot2 = wal_begin_read(db->wal);\n    wal_read_page(db->wal, 42, snapshot2->snapshot_max_frame, read_buffer);\n    assert(read_buffer[0] == 'B');  // Version 2\n    wal_end_read(db->wal, snapshot);\n    wal_end_read(db->wal, snapshot2);\n    free_database(db);\n}\n```\n### Checkpoint Tests\n```c\nvoid test_checkpoint_copies_to_database() {\n    Database* db = create_test_database_wal();\n    // Write to WAL\n    WALWriteTransaction txn;\n    wal_begin_write(db->wal, &txn);\n    char page_data[PAGE_SIZE]; memset(page_data, 'X', PAGE_SIZE);\n    wal_write_page(&txn, 42, page_data);\n    wal_commit_transaction(&txn, 100);\n    // Page should NOT be in main database yet\n    char db_buffer[PAGE_SIZE];\n    disk_read(db->file_manager, 42, db_buffer);\n    assert(db_buffer[0] != 'X');  // Old data\n    // Checkpoint\n    wal_checkpoint(db->wal, CHECKPOINT_FULL);\n    // Now page should be in main database\n    disk_read(db->file_manager, 42, db_buffer);\n    assert(db_buffer[0] == 'X');  // Checkpointed data\n    free_database(db);\n}\nvoid test_auto_checkpoint() {\n    Database* db = create_test_database_wal();\n    wal_set_auto_checkpoint(db->wal, 100);  // Checkpoint every 100 pages\n    // Write 150 pages\n    for (int i = 0; i < 150; i++) {\n        WALWriteTransaction txn;\n        wal_begin_write(db->wal, &txn);\n        char page_data[PAGE_SIZE];\n        memset(page_data, i, PAGE_SIZE);\n        wal_write_page(&txn, i, page_data);\n        wal_commit_transaction(&txn, 100);\n    }\n    // Auto-checkpoint should have triggered\n    // WAL should be small (only ~50 pages since last checkpoint)\n    off_t wal_size = lseek(db->wal->fd, 0, SEEK_END);\n    assert(wal_size < 100 * (sizeof(FrameHeader) + PAGE_SIZE));\n    free_database(db);\n}\n```\n### Recovery Tests\n```c\nvoid test_wal_recovery_replays_committed() {\n    Database* db = create_test_database();\n    // Switch to WAL mode\n    execute_sql(db, \"PRAGMA journal_mode=WAL\");\n    execute_sql(db, \"CREATE TABLE test (id INTEGER PRIMARY KEY, value TEXT)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 'committed')\");\n    // Simulate crash (don't close properly)\n    char* wal_path = strdup(db->wal->path);\n    simulate_crash(db);\n    // Reopen and verify recovery\n    db = reopen_database(db);\n    Result* r = execute_sql(db, \"SELECT value FROM test WHERE id = 1\");\n    assert(r->row_count == 1);\n    assert(strcmp(r->rows[0].columns[0].data.string_val, \"committed\") == 0);\n    free_result(r);\n    free(wal_path);\n    free_database(db);\n}\nvoid test_wal_recovery_ignores_uncommitted() {\n    Database* db = create_test_database_wal();\n    // Write and commit\n    WALWriteTransaction txn1;\n    wal_begin_write(db->wal, &txn1);\n    char page_data[PAGE_SIZE]; memset(page_data, 'A', PAGE_SIZE);\n    wal_write_page(&txn1, 42, page_data);\n    wal_commit_transaction(&txn1, 100);\n    // Start another transaction but don't commit\n    WALWriteTransaction txn2;\n    wal_begin_write(db->wal, &txn2);\n    memset(page_data, 'B', PAGE_SIZE);\n    wal_write_page(&txn2, 42, page_data);\n    // DON'T commit\n    // Simulate crash\n    simulate_crash(db);\n    // Reopen and recover\n    db = reopen_database(db);\n    // Should see version 'A' (committed), not 'B' (uncommitted)\n    char read_buffer[PAGE_SIZE];\n    ReadSnapshot* snapshot = wal_begin_read(db->wal);\n    wal_read_page(db->wal, 42, snapshot->snapshot_max_frame, read_buffer);\n    assert(read_buffer[0] == 'A');\n    wal_end_read(db->wal, snapshot);\n    free_database(db);\n}\n```\n### Checksum Tests\n```c\nvoid test_checksum_detects_corruption() {\n    Database* db = create_test_database_wal();\n    // Write a frame\n    WALWriteTransaction txn;\n    wal_begin_write(db->wal, &txn);\n    char page_data[PAGE_SIZE]; memset(page_data, 'A', PAGE_SIZE);\n    wal_write_page(&txn, 42, page_data);\n    wal_commit_transaction(&txn, 100);\n    // Corrupt the WAL file\n    off_t frame_offset = sizeof(WALHeader);\n    char corrupt_byte = 'X';\n    pwrite(db->wal->fd, &corrupt_byte, 1, \n           frame_offset + sizeof(FrameHeader) + 100);\n    // Try to read \u2014 should fail validation\n    db->wal->index = wal_index_create();  // Rebuild index\n    int result = wal_build_index(db->wal);\n    assert(result != 0);  // Should detect corruption\n    free_database(db);\n}\n```\n---\n## What You've Built\nYour WAL mode implementation provides:\n1. **Concurrent readers and writers** \u2014 the primary benefit over rollback journal\n2. **WAL file format** with frames containing modified pages\n3. **Cumulative checksums** for corruption detection\n4. **WAL index** for fast page lookup\n5. **Snapshot isolation** \u2014 each reader sees a consistent point-in-time view\n6. **Checkpoint process** that copies WAL to main database\n7. **Auto-checkpoint** to prevent unbounded WAL growth\n8. **Recovery** that replays committed transactions\n9. **Mode switching** between rollback journal and WAL\nThis is the architecture used by SQLite (in WAL mode), PostgreSQL (with its WAL), and most modern databases. The append-only log pattern is fundamental to database concurrency.\n---\n## Knowledge Cascade\n### 1. Event Sourcing \u2014 The WAL IS an Event Log\nEvent sourcing systems store state changes as a sequence of events:\n```\nEvent 1: AccountCreated { id: 1, balance: 0 }\nEvent 2: MoneyDeposited { id: 1, amount: 100 }\nEvent 3: MoneyWithdrawn { id: 1, amount: 50 }\n```\nThe current state is derived by replaying events \u2014 exactly like WAL recovery. The \"checkpoint\" in event sourcing is a snapshot: a materialized state that allows replaying only post-snapshot events.\nSystems like Kafka, EventStoreDB, and Axon Framework are essentially distributed WALs with event semantics.\n### 2. Database Replication \u2014 Shipping WAL Records\nPostgreSQL's logical replication works by shipping WAL records to replicas:\n```\nPrimary: Write to WAL \u2192 Ship WAL record to replica\nReplica: Receive WAL record \u2192 Replay to local database\n```\nThe replica is always slightly behind (replication lag), but it has a consistent view. This is the same snapshot isolation pattern \u2014 the replica sees a point-in-time view of the primary.\nMySQL's binlog, Oracle's redo log shipping, and SQL Server's transaction log shipping all use the same pattern.\n### 3. Time-Travel Debugging \u2014 Record/Replay\nDebuggers like `rr` (Mozilla) and `UndoDB` record program execution as a stream of state changes:\n```\nRecord: Memory write at 0x1234 = 42\nRecord: Register rax = 100\nRecord: Memory write at 0x5678 = 'A'\n```\nTo \"reverse\" execution, replay from the beginning to the desired point. This is WAL recovery for program state. The \"checkpoint\" is a snapshot that allows starting replay from an intermediate point.\n### 4. CRDTs in Distributed Systems \u2014 Append-Only with Merging\nConflict-free Replicated Data Types (CRDTs) often use append-only logs:\n```\nReplica A: [op1, op2, op3]\nReplica B: [op1, op2, op4]  -- Different concurrent ops\nMerge: [op1, op2, op3, op4]  -- All ops, in causal order\n```\nThe merge is the distributed analog of checkpoint \u2014 reconciling multiple logs into consistent state. Systems like Redis CRDB, Automerge, and Yjs use this pattern.\n### 5. Blockchain Structure \u2014 Blocks as WAL Frames\nA blockchain is a WAL where each frame (block) contains a batch of transactions:\n```\nBlock N: [tx1, tx2, tx3] + hash(previous_block)\nBlock N+1: [tx4, tx5] + hash(block_N)\n```\nThe \"checkpoint\" is finality \u2014 when the network agrees a block won't be replaced. Recovery is replay from genesis (or from a snapshot). The cryptographic hashes are cumulative checksums, exactly like WAL frame checksums.\n---\n## What's Next\nWith WAL mode, your database achieves concurrent reads during writes \u2014 a critical capability for production workloads. But there's still a dimension missing: **analytical queries.**\nSo far, all your queries have returned individual rows. But real analytical work involves aggregation: `COUNT`, `SUM`, `AVG`, `MIN`, `MAX`. It involves grouping: `GROUP BY`. It involves combining tables: `JOIN`.\nThe next milestone, **Aggregate Functions & JOIN**, adds these capabilities. You'll implement hash-based aggregation for `GROUP BY`, nested loop joins for combining tables, and the subtle semantics of NULL handling in aggregate functions.\nYour database can now handle concurrent users. Next, it learns to answer analytical questions.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m10\", \"criteria\": [\"WAL file format includes 32-byte header with magic number (0x377f0682 or 0x377f0683 for endianness), format version, page size, checkpoint sequence, salt values, and cumulative checksums\", \"WAL frame format includes 24-byte header with page number, commit size (non-zero for commit frames), salt values matching header, and cumulative checksums\", \"WAL stores modified (new) page images, not original pages \u2014 this is the inversion from rollback journal\", \"WAL index is an in-memory hash table mapping page numbers to their most recent frame offset in the WAL file\", \"Writers append frames to the end of the WAL file without modifying the main database file\", \"Commit markers (commit_size > 0 in frame header) define transaction boundaries in the WAL\", \"Readers capture a snapshot (max_frame) at transaction start and only see frames with offset <= max_frame\", \"Page read checks WAL index first for recent page version, falls back to main database if not found or version too new\", \"Snapshot isolation: a reader that starts before a commit does not see that commit's changes\", \"Multiple readers can execute queries concurrently while a single writer appends to the WAL\", \"Readers never block writers (append-only WAL access) and writers never block readers (read-only access to existing frames)\", \"Checkpoint (PRAGMA wal_checkpoint) copies WAL frames to the main database file\", \"Checkpoint modes: PASSIVE (no blocking), FULL (wait for readers), RESTART (wait for readers and writers), TRUNCATE (truncate WAL after checkpoint)\", \"Auto-checkpoint triggers after configurable page count threshold (default 1000 pages) to prevent unbounded WAL growth\", \"Cumulative checksums validate frame integrity: each frame's checksum includes all previous frames\", \"Checksum validation detects and rejects corrupted WAL frames\", \"WAL recovery on startup replays committed frames (those before the last commit marker) to the main database\", \"Uncommitted frames (after the last commit marker) are ignored during recovery\", \"Recovery is idempotent: replaying the same WAL multiple times produces the same result\", \"PRAGMA journal_mode=WAL switches from rollback journal to WAL mode\", \"PRAGMA journal_mode=DELETE switches from WAL to rollback journal mode (requires checkpoint first)\", \"Readers register and unregister with the WAL to prevent checkpoint from truncating frames they might need\", \"Long-running readers can prevent WAL truncation \u2014 system should detect and handle this scenario\", \"WAL mode and rollback journal mode are mutually exclusive \u2014 database uses one or the other\", \"Test suite validates snapshot isolation (reader sees consistent point-in-time view)\", \"Test suite validates concurrent read/write (no blocking)\", \"Test suite validates checkpoint copies WAL pages to main database\", \"Test suite validates auto-checkpoint triggers at threshold\", \"Test suite validates recovery replays committed transactions\", \"Test suite validates recovery ignores uncommitted transactions\", \"Test suite validates checksums detect frame corruption\", \"Test suite validates mode switching between WAL and rollback journal\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m11 -->\n# Aggregate Functions & JOIN: From Rows to Insights\nYour database can select, insert, update, and delete. It can filter with WHERE clauses, scan indexes efficiently, and survive crashes. But every query so far has answered a simple question: \"Which rows match these criteria?\"\nReal questions are harder:\n- \"How many users signed up last month?\" (COUNT)\n- \"What's the average order value?\" (AVG)\n- \"Who are the top 10 customers by total spend?\" (SUM + ORDER BY)\n- \"Which products were purchased by users in California?\" (JOIN)\nThese require **aggregation** (combining multiple rows into a single result) and **joining** (combining multiple tables into a unified view). They transform your database from a row store into an analytical engine.\nThis is where SQL earns its power. A single query like:\n```sql\nSELECT c.name, COUNT(o.id), SUM(o.total)\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\nWHERE o.status = 'completed'\nGROUP BY c.id\nHAVING SUM(o.total) > 1000\nORDER BY SUM(o.total) DESC\nLIMIT 10;\n```\n...expresses a complete analytical pipeline: filter, join, aggregate, filter again, sort, and limit. Understanding how each piece works \u2014 and how they compose \u2014 is the difference between a database that stores data and one that answers questions.\n---\n## The Tension: Computation Grows Faster Than Data\nHere's the fundamental problem: **aggregation and joins can produce results far larger than their inputs.**\nConsider joining two tables with 10,000 rows each:\n- **Best case** (1:1 relationship): 10,000 result rows\n- **Worst case** (Cartesian product): 100,000,000 result rows\nA query that runs in milliseconds with the right join order can take hours with the wrong one. The database doesn't just execute \u2014 it must plan.\nAnd aggregation has its own trap: **intermediate state explosion.**\n```sql\nSELECT category, COUNT(*), AVG(price), SUM(quantity)\nFROM products\nGROUP BY category;\n```\nThis query needs to track running state for each group: count, sum of prices, sum of quantities. For 1,000 categories, that's 3,000 values. But what if you GROUP BY a column with 1,000,000 distinct values? Now you're tracking 3,000,000 values in memory.\nThe tension: **you must compute results that might be orders of magnitude larger than the data you're reading.** The naive approach \u2014 materializing all intermediate results \u2014 exhausts memory. The solution requires streaming algorithms and careful memory management.\n### The Numbers That Matter\nLet's make this concrete with operation counts:\n| Operation | Input Size | Output Size | Time Complexity |\n|-----------|------------|-------------|-----------------|\n| Full table scan | N rows | N rows | O(N) |\n| GROUP BY (K groups) | N rows | K rows | O(N) with hash |\n| Aggregate (no GROUP BY) | N rows | 1 row | O(N) |\n| Nested loop join | N + M rows | N\u00d7M (worst) | O(N\u00d7M) |\n| Hash join | N + M rows | O(N+M) (matches) | O(N+M) |\nThe join complexity is the killer. A nested loop join of a 100,000-row table with a 1,000,000-row table:\n- Small table outer: 100,000 \u00d7 1 = 100,000 inner lookups (fast with index)\n- Large table outer: 1,000,000 \u00d7 1 = 1,000,000 inner lookups (10x slower!)\nThis is why join ordering matters. The query planner doesn't just optimize for fun \u2014 it prevents catastrophically slow queries.\n---\n## The Revelation: JOIN Order is Everything\nHere's the misconception that will produce slow queries: *JOINs are just nested loops \u2014 for each row in A, find matching rows in B.*\n**Technically true, but dangerously incomplete.** Yes, SQLite's default JOIN implementation is a nested loop. But **which table is the outer loop** determines whether your query takes milliseconds or minutes.\nConsider:\n```sql\nSELECT *\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE c.country = 'US';\n```\nTwo execution strategies:\n**Strategy 1: Customers outer, Orders inner**\n```\nFor each customer in customers:\n    If customer.country = 'US':\n        For each order in orders:\n            If order.customer_id = customer.id:\n                Output (customer, order)\n```\nIf 10,000 customers are in the US and there are 1,000,000 orders:\n- Outer loop: 10,000 iterations (filtered)\n- Inner lookups: 10,000 \u00d7 (index seek on orders.customer_id) \u2248 50,000 page reads\n**Strategy 2: Orders outer, Customers inner**\n```\nFor each order in orders:\n    For each customer in customers:\n        If order.customer_id = customer.id AND customer.country = 'US':\n            Output (customer, order)\n```\n- Outer loop: 1,000,000 iterations (no filtering!)\n- Inner lookups: 1,000,000 \u00d7 (index seek on customers.id) \u2248 3,000,000 page reads\nThe difference: **60x more I/O.** Same query, same data, same algorithm \u2014 but the order makes the difference between fast and catastrophic.\n\n![Join Order Impact](./diagrams/diag-join-order.svg)\n\n### Why This Matters for Aggregation Too\nThe same principle applies to GROUP BY. Consider:\n```sql\nSELECT customer_id, COUNT(*)\nFROM orders\nWHERE status = 'completed'\nGROUP BY customer_id;\n```\nTwo strategies:\n**Strategy 1: Filter first, then aggregate**\n```\ncompleted_orders = filter(orders, status = 'completed')  -- 500,000 rows\nresult = group_by(completed_orders, customer_id)          -- 10,000 groups\n```\n**Strategy 2: Aggregate first, then filter groups**\n```\ngroups = group_by(orders, customer_id)                    -- 10,000 groups\nresult = filter(groups, status = 'completed')             -- WRONG! Lost row-level info\n```\nThe second strategy is wrong because you can't filter by `status` after grouping \u2014 the status information is aggregated away. The execution order matters: **WHERE before GROUP BY, HAVING after.**\n{{DIAGRAM:diag-having-vs-where}}\n---\n## Aggregate Functions: Combining Rows into Values\nAn aggregate function takes multiple rows and produces a single value. The canonical five: COUNT, SUM, AVG, MIN, MAX.\n### The Execution Model\nFor a query without GROUP BY:\n```sql\nSELECT COUNT(*), AVG(price), MIN(created_at)\nFROM products;\n```\nThe execution engine maintains **aggregate state** \u2014 a set of accumulators that are updated as rows stream through:\n```c\ntypedef struct {\n    // COUNT(*)\n    int64_t count_all;\n    // COUNT(col) - tracks separately for NULL handling\n    int64_t count_col;\n    // SUM\n    double sum_value;\n    bool sum_valid;  // Becomes true after first non-NULL\n    // AVG (computed from sum / count at end)\n    // Stored as sum and count\n    // MIN\n    Value min_value;\n    bool min_valid;\n    // MAX\n    Value max_value;\n    bool max_valid;\n} AggregateState;\n```\nThe VDBE executes a single pass through the table:\n```c\n// Bytecode for: SELECT COUNT(*), AVG(price) FROM products\nOpenRead     cursor=0  table=products\nRewind       cursor=0\nInteger      0         r1         // count_all = 0\nNull         r2                   // sum_price = NULL\nLOOP:\n  Column     cursor=0  col=price  r3\n  Add        r2        r3         r2   // sum_price += price\n  Integer    1         r4\n  Add        r1        r4         r1   // count_all += 1\n  Next       cursor=0  LOOP\nDivide       r2        r1         r5   // avg = sum / count\nResultRow    r1        2               // Output count, avg\nHalt\n```\n### NULL Handling: The Subtle Trap\nNULL handling in aggregates is specified by SQL standard but often misunderstood:\n\n![NULL Handling in Aggregate Functions](./diagrams/diag-null-in-aggregates.svg)\n\n| Function | Behavior with NULL |\n|----------|-------------------|\n| `COUNT(*)` | Counts all rows, including those with NULL columns |\n| `COUNT(col)` | Counts only rows where `col` is NOT NULL |\n| `SUM(col)` | Sums non-NULL values; returns NULL if all values are NULL |\n| `AVG(col)` | Averages non-NULL values; returns NULL if all values are NULL |\n| `MIN(col)` | Returns minimum non-NULL value; returns NULL if all values are NULL |\n| `MAX(col)` | Returns maximum non-NULL value; returns NULL if all values are NULL |\nThis is why `COUNT(*)` can differ from `COUNT(column)`:\n```sql\n-- Table: products (3 rows, 1 with NULL price)\nSELECT COUNT(*), COUNT(price) FROM products;\n-- Result: 3, 2\n```\nImplementation:\n```c\nvoid update_aggregate_state(AggregateState* state, Value* value, AggregateType type) {\n    switch (type) {\n        case AGG_COUNT_ALL:\n            state->count_all++;\n            break;\n        case AGG_COUNT_COL:\n            if (value->type != VALUE_NULL) {\n                state->count_col++;\n            }\n            break;\n        case AGG_SUM:\n            if (value->type != VALUE_NULL) {\n                if (!state->sum_valid) {\n                    state->sum_value = value_to_double(value);\n                    state->sum_valid = true;\n                } else {\n                    state->sum_value += value_to_double(value);\n                }\n            }\n            break;\n        case AGG_MIN:\n            if (value->type != VALUE_NULL) {\n                if (!state->min_valid || compare_values(value, &state->min_value) < 0) {\n                    copy_value(&state->min_value, value);\n                    state->min_valid = true;\n                }\n            }\n            break;\n        case AGG_MAX:\n            if (value->type != VALUE_NULL) {\n                if (!state->max_valid || compare_values(value, &state->max_value) > 0) {\n                    copy_value(&state->max_value, value);\n                    state->max_valid = true;\n                }\n            }\n            break;\n    }\n}\nValue finalize_aggregate(AggregateState* state, AggregateType type) {\n    Value result;\n    switch (type) {\n        case AGG_COUNT_ALL:\n            result.type = VALUE_INTEGER;\n            result.data.integer_val = state->count_all;\n            break;\n        case AGG_COUNT_COL:\n            result.type = VALUE_INTEGER;\n            result.data.integer_val = state->count_col;\n            break;\n        case AGG_SUM:\n            if (!state->sum_valid) {\n                result.type = VALUE_NULL;\n            } else {\n                result.type = VALUE_FLOAT;  // SUM returns float for consistency\n                result.data.float_val = state->sum_value;\n            }\n            break;\n        case AGG_AVG:\n            if (state->count_col == 0) {\n                result.type = VALUE_NULL;\n            } else {\n                result.type = VALUE_FLOAT;\n                result.data.float_val = state->sum_value / state->count_col;\n            }\n            break;\n        case AGG_MIN:\n        case AGG_MAX:\n            if (!state->min_valid) {\n                result.type = VALUE_NULL;\n            } else {\n                copy_value(&result, type == AGG_MIN ? &state->min_value : &state->max_value);\n            }\n            break;\n    }\n    return result;\n}\n```\n### Empty Tables: The Edge Case\nWhat do aggregates return for an empty table?\n```sql\nSELECT COUNT(*), SUM(price), AVG(price), MIN(price) FROM empty_table;\n```\nResult: `0, NULL, NULL, NULL`\nCOUNT returns 0 because there are zero rows. SUM, AVG, MIN, MAX return NULL because there are no values to aggregate. This is the SQL standard, and your implementation must match:\n```c\n// For empty table (count_all = 0 after scanning):\nresult.count = 0;\nresult.sum = NULL;   // sum_valid = false\nresult.avg = NULL;   // count_col = 0\nresult.min = NULL;   // min_valid = false\n```\n---\n## GROUP BY: Partitioning for Aggregation\nGROUP BY partitions rows into groups, then applies aggregate functions to each group independently.\n### Hash-Based Aggregation\nThe standard implementation uses a **hash table** to group rows:\n{{DIAGRAM:diag-group-by-execution}}\n```c\ntypedef struct {\n    Value* group_key;       // The GROUP BY column values\n    int key_count;\n    AggregateState* states; // One state per aggregate function\n    int aggregate_count;\n} GroupEntry;\ntypedef struct {\n    HashTable* groups;      // group_key \u2192 GroupEntry\n    int group_count;\n    int key_column_count;\n    AggregateType* aggregate_types;\n    int aggregate_count;\n} GroupByState;\nGroupByState* create_group_by_state(int key_count, AggregateType* types, int agg_count) {\n    GroupByState* state = malloc(sizeof(GroupByState));\n    state->groups = hash_table_create(1024);\n    state->group_count = 0;\n    state->key_column_count = key_count;\n    state->aggregate_types = types;\n    state->aggregate_count = agg_count;\n    return state;\n}\nvoid process_row_for_group_by(GroupByState* state, Value* group_keys, Value* agg_values) {\n    // Compute hash of group key\n    uint32_t hash = hash_values(group_keys, state->key_column_count);\n    // Look up existing group\n    GroupEntry* entry = hash_table_get(state->groups, group_keys, hash);\n    if (entry == NULL) {\n        // New group: create entry\n        entry = malloc(sizeof(GroupEntry));\n        entry->group_key = copy_values(group_keys, state->key_column_count);\n        entry->key_count = state->key_column_count;\n        entry->states = calloc(state->aggregate_count, sizeof(AggregateState));\n        entry->aggregate_count = state->aggregate_count;\n        hash_table_put(state->groups, entry->group_key, entry, hash);\n        state->group_count++;\n    }\n    // Update aggregate states for this group\n    for (int i = 0; i < state->aggregate_count; i++) {\n        update_aggregate_state(&entry->states[i], &agg_values[i], state->aggregate_types[i]);\n    }\n}\n```\n### GROUP BY Bytecode\nFor `SELECT category, COUNT(*), AVG(price) FROM products GROUP BY category`:\n```\nOpenRead     cursor=0  table=products\nRewind       cursor=0\nNull         r1                   // Initialize group-by state in r1\nGROUP_LOOP:\n  Column     cursor=0  col=category  r2  // Group key\n  Column     cursor=0  col=price      r3  // Aggregate value\n  AggStep    r1        r2         r3     // Update group state with key/value\n  Next       cursor=0  GROUP_LOOP\nAggFinal     r1                         // Finalize all groups\nAggRewind    r1                         // Rewind to first group\nOUTPUT_LOOP:\n  AggColumn  r1        key        r4    // Get group key\n  AggColumn  r1        count      r5    // Get count\n  AggColumn  r1        avg        r6    // Get avg\n  ResultRow  r4        3\n  AggNext    r1        OUTPUT_LOOP      // Next group\nHalt\n```\nThe key insight: **GROUP BY is two-pass.** First pass builds the hash table. Second pass outputs results.\n### Multiple Grouping Columns\nFor `GROUP BY category, region`:\n```c\n// Composite key creation\nValue* create_composite_key(Value* category, Value* region) {\n    Value* key = malloc(2 * sizeof(Value));\n    copy_value(&key[0], category);\n    copy_value(&key[1], region);\n    return key;\n}\n// Hash function for composite keys\nuint32_t hash_composite_key(Value* key, int count) {\n    uint32_t hash = 0;\n    for (int i = 0; i < count; i++) {\n        hash = hash_combine(hash, hash_value(&key[i]));\n    }\n    return hash;\n}\n// Comparison for composite keys\nbool keys_equal(Value* a, Value* b, int count) {\n    for (int i = 0; i < count; i++) {\n        if (compare_values(&a[i], &b[i]) != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```\n---\n## HAVING: Filtering After Aggregation\nHAVING filters groups based on aggregate results. It's WHERE's sibling that operates after GROUP BY.\n```sql\nSELECT category, COUNT(*) as cnt\nFROM products\nGROUP BY category\nHAVING COUNT(*) > 10;\n```\nThe execution adds a filter after aggregation:\n```c\n// After AggFinal, before output loop:\nwhile (agg_cursor_has_more(state)) {\n    GroupEntry* entry = agg_cursor_get_current(state);\n    // Compute HAVING condition\n    Value count_val = finalize_aggregate(&entry->states[0], AGG_COUNT_ALL);\n    bool passes = (count_val.data.integer_val > 10);\n    if (passes) {\n        // Output this group\n        output_group(entry);\n    }\n    agg_cursor_next(state);\n}\n```\n### HAVING vs WHERE: The Critical Distinction\n{{DIAGRAM:diag-having-vs-where}}\n```sql\n-- WHERE filters rows before grouping\nSELECT category, COUNT(*)\nFROM products\nWHERE price > 100    -- Only expensive products\nGROUP BY category;\n-- HAVING filters groups after aggregation\nSELECT category, COUNT(*)\nFROM products\nGROUP BY category\nHAVING COUNT(*) > 10;  -- Only categories with 10+ products\n```\nA query can have both:\n```sql\nSELECT category, AVG(price) as avg_price, COUNT(*) as cnt\nFROM products\nWHERE price > 0          -- Filter rows first\nGROUP BY category\nHAVING COUNT(*) > 5      -- Filter groups after\n   AND AVG(price) < 1000;\n```\nExecution order:\n1. Scan products table\n2. Apply WHERE filter (price > 0)\n3. Group surviving rows by category\n4. Compute AVG(price) and COUNT(*) for each group\n5. Apply HAVING filter (COUNT(*) > 5 AND AVG(price) < 1000)\n6. Output surviving groups\n---\n## INNER JOIN: Combining Tables\nAn INNER JOIN combines rows from two tables where a join condition is true.\n### The Nested Loop Join Algorithm\nThe default JOIN implementation in SQLite is the **nested loop join**:\n{{DIAGRAM:diag-nested-loop-join}}\n```c\nvoid execute_nested_loop_join(\n    Database* db,\n    Table* outer_table,\n    Table* inner_table,\n    JoinCondition* condition,\n    ResultCallback callback\n) {\n    Cursor outer_cursor, inner_cursor;\n    cursor_init(&outer_cursor, db->buffer_pool, outer_table->root_page);\n    cursor_first(&outer_cursor);\n    while (!outer_cursor.eof) {\n        Row* outer_row = cursor_get_row(&outer_cursor);\n        // For each outer row, scan inner table\n        cursor_init(&inner_cursor, db->buffer_pool, inner_table->root_page);\n        cursor_first(&inner_cursor);\n        while (!inner_cursor.eof) {\n            Row* inner_row = cursor_get_row(&inner_cursor);\n            // Evaluate join condition\n            if (evaluate_join_condition(condition, outer_row, inner_row)) {\n                // Combine rows and output\n                Row* combined = combine_rows(outer_row, inner_row);\n                callback(combined);\n                free_row(combined);\n            }\n            free_row(inner_row);\n            cursor_next(&inner_cursor);\n        }\n        free_row(outer_row);\n        cursor_next(&outer_cursor);\n    }\n}\n```\n### Join Bytecode\nFor `SELECT * FROM orders o JOIN customers c ON o.customer_id = c.id`:\n```\nOpenRead     cursor=0  table=orders\nOpenRead     cursor=1  table=customers\nRewind       cursor=0\nOUTER_LOOP:\n  Column     cursor=0  col=customer_id  r1\n  Rewind     cursor=1\nINNER_LOOP:\n  Column     cursor=1  col=id           r2\n  Ne         r1        r2         SKIP_MATCH  -- customer_id != id? skip\n  Column     cursor=0  col=*           r3-r6  -- All order columns\n  Column     cursor=1  col=*           r7-r10 -- All customer columns\n  ResultRow  r3        8\nSKIP_MATCH:\n  Next       cursor=1  INNER_LOOP\n  Next       cursor=0  OUTER_LOOP\nHalt\n```\n### The Inner Table Scan Optimization\nThe naive nested loop scans the entire inner table for each outer row. This is O(N\u00d7M). But if the inner table has an index on the join column, we can use an **index seek** instead of a full scan:\n```c\nvoid execute_nested_loop_join_with_index(\n    Database* db,\n    Table* outer_table,\n    Index* inner_index,\n    JoinCondition* condition,\n    ResultCallback callback\n) {\n    Cursor outer_cursor;\n    cursor_init(&outer_cursor, db->buffer_pool, outer_table->root_page);\n    cursor_first(&outer_cursor);\n    while (!outer_cursor.eof) {\n        Row* outer_row = cursor_get_row(&outer_cursor);\n        // Extract join key from outer row\n        Value* join_key = get_column_value(outer_row, condition->outer_column);\n        // Seek in inner index instead of scanning\n        Cursor inner_cursor;\n        cursor_init(&inner_cursor, db->buffer_pool, inner_index->root_page);\n        index_seek_equality(&inner_cursor, join_key);\n        while (!inner_cursor.eof) {\n            // Get rowid from index\n            int64_t inner_rowid = index_cursor_get_rowid(&inner_cursor);\n            // Fetch actual row from inner table\n            Row* inner_row = fetch_row_by_rowid(db, inner_index->table, inner_rowid);\n            // Output combined row\n            Row* combined = combine_rows(outer_row, inner_row);\n            callback(combined);\n            free_row(combined);\n            free_row(inner_row);\n            cursor_next(&inner_cursor);\n        }\n        free_row(outer_row);\n        cursor_next(&outer_cursor);\n    }\n}\n```\nWith an index, the complexity drops to **O(N \u00d7 log M)** for point lookups, or **O(N \u00d7 K)** where K is the average number of matching inner rows.\n### The Query Planner's Job\nThe query planner chooses:\n1. **Which table is outer** (smaller is usually better)\n2. **Whether to use an index** on the inner table\n3. **Whether to use a different join algorithm** (hash join, merge join)\nFor your implementation, nested loop with optional index seek is sufficient. The planner's join ordering (from milestone 8) handles the optimization.\n---\n## JOIN with WHERE: Filter Before or During?\nWhen a query has both JOIN and WHERE, the execution order matters:\n```sql\nSELECT *\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE c.country = 'US' AND o.total > 100;\n```\nTwo valid execution strategies:\n**Strategy 1: Filter during join**\n```\nFor each order:\n    If order.total > 100:\n        For each customer:\n            If customer.id = order.customer_id AND customer.country = 'US':\n                Output (order, customer)\n```\n**Strategy 2: Filter before join**\n```\nUS_customers = filter(customers, country = 'US')\nlarge_orders = filter(orders, total > 100)\nFor each order in large_orders:\n    For each customer in US_customers:\n        If customer.id = order.customer_id:\n            Output (order, customer)\n```\nStrategy 2 is often faster because:\n- Filtering reduces the input size to the join\n- An index on `customers.country` makes filtering fast\n- Smaller inputs mean fewer join iterations\nThe query planner (milestone 8) makes this decision. Your executor just needs to handle both:\n- **Predicates pushed to scan**: Apply during table scan (WHERE on single table)\n- **Join predicates**: Apply during join (ON condition)\n- **Post-join predicates**: Apply after join (WHERE on columns from both tables)\n---\n## Multiple Aggregates in One Query\nA single query can compute multiple aggregates:\n```sql\nSELECT \n    COUNT(*) as total,\n    COUNT(price) as priced_items,\n    SUM(price) as total_price,\n    AVG(price) as avg_price,\n    MIN(price) as min_price,\n    MAX(price) as max_price\nFROM products;\n```\nThe aggregate state tracks all of them:\n```c\ntypedef struct {\n    AggregateState count_all;\n    AggregateState count_price;\n    AggregateState sum_price;\n    AggregateState min_price;\n    AggregateState max_price;\n    // AVG is computed from sum_price / count_price at finalization\n} MultiAggregateState;\nvoid update_multi_aggregate(MultiAggregateState* state, Row* row) {\n    // COUNT(*)\n    state->count_all.count_all++;\n    // Other aggregates depend on price column\n    Value* price = get_column_value(row, PRICE_COLUMN);\n    // COUNT(price)\n    if (price->type != VALUE_NULL) {\n        state->count_price.count_col++;\n        state->sum_price.sum_value += value_to_double(price);\n        state->sum_price.sum_valid = true;\n        // MIN\n        if (!state->min_price.min_valid || \n            compare_values(price, &state->min_price.min_value) < 0) {\n            copy_value(&state->min_price.min_value, price);\n            state->min_price.min_valid = true;\n        }\n        // MAX\n        if (!state->max_price.max_valid || \n            compare_values(price, &state->max_price.max_value) > 0) {\n            copy_value(&state->max_price.max_value, price);\n            state->max_price.max_valid = true;\n        }\n    }\n}\n```\n### The AVg Trap: Integer Division\nAVG must return a floating-point result even if the input column is INTEGER:\n```sql\nSELECT AVG(quantity) FROM orders;  -- quantity is INTEGER\n-- Must return FLOAT, not INTEGER\n```\n```c\nValue finalize_avg(AggregateState* state) {\n    Value result;\n    if (state->count_col == 0) {\n        result.type = VALUE_NULL;\n    } else {\n        result.type = VALUE_FLOAT;  // Always FLOAT\n        result.data.float_val = state->sum_value / state->count_col;\n    }\n    return result;\n}\n```\n---\n## The Three-Level View\n### Level 1: Application (SQL \u2192 Aggregated Results)\nAt the API level, aggregation and joins are invisible:\n```sql\nSELECT c.name, COUNT(o.id), SUM(o.total)\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\nGROUP BY c.id\nHAVING SUM(o.total) > 1000;\n```\nThe user provides SQL and receives aggregated, joined results. They don't know about hash tables, nested loops, or aggregate states.\n### Level 2: Engine (Aggregate State + Hash Grouping + Nested Loop)\nInside the executor:\n```\n1. Open cursors on customers and orders\n2. For each customer:\n   a. For each order:\n      - Evaluate join condition\n      - If match, add to group-by hash table\n3. Finalize aggregates for each group\n4. Filter groups by HAVING condition\n5. Output surviving groups\n```\nThe hash table enables O(N) grouping. The nested loop enables joins.\n### Level 3: Implementation (Hash Table + State Accumulation)\nAt the lowest level:\n```c\n// Hash table for GROUP BY\nentry = hash_table_lookup(groups, group_key);\nif (!entry) {\n    entry = create_group_entry(group_key);\n    hash_table_insert(groups, group_key, entry);\n}\nentry->count++;\nentry->sum += value;\n// Nested loop for JOIN\nfor (outer_row in outer_table) {\n    for (inner_row in inner_table) {\n        if (matches_condition(outer_row, inner_row)) {\n            output(combine(outer_row, inner_row));\n        }\n    }\n}\n```\nThe correctness depends on:\n- Proper NULL handling in aggregates\n- Consistent hash function for grouping\n- Correct join condition evaluation\n---\n## Design Decisions: Why This, Not That\n### Hash Aggregation vs Sort-Based Aggregation\n| Aspect | Hash Aggregation (Chosen) | Sort-Based Aggregation |\n|--------|--------------------------|----------------------|\n| Time complexity | O(N) average | O(N log N) |\n| Memory usage | O(K) for K groups | O(N) for sorting |\n| Ordered output | No (unless added) | Yes (by group key) |\n| Large number of groups | May spill to disk | Handles naturally |\n| Used by | SQLite, PostgreSQL (default) | PostgreSQL (for sorted input) |\nHash aggregation is faster for typical workloads. Sort-based handles massive group counts better.\n### Nested Loop vs Hash Join vs Merge Join\n| Aspect | Nested Loop (Chosen) | Hash Join | Merge Join |\n|--------|---------------------|-----------|------------|\n| Time complexity | O(N \u00d7 M) worst, O(N \u00d7 K) with index | O(N + M) | O(N log N + M log M) |\n| Memory usage | O(1) | O(smaller table) | O(1) |\n| Requires index | Optional for optimization | No | No |\n| Requires sorted input | No | No | Yes |\n| Best for | Small tables, indexed joins | Large unsorted tables | Pre-sorted data |\n| Used by | SQLite (only option) | PostgreSQL, MySQL | PostgreSQL, Oracle |\nSQLite only implements nested loop. This is a design choice for simplicity \u2014 it works well with proper indexing. Production databases offer multiple algorithms.\n---\n## Common Pitfalls\n### COUNT(*) vs COUNT(col) Confusion\n```sql\n-- WRONG: Expecting count of all rows\nSELECT COUNT(email) FROM users;  -- Returns 90 if 10 users have NULL email\n-- RIGHT: Counting all rows\nSELECT COUNT(*) FROM users;      -- Returns 100\n```\n### AVG Returns NULL for Empty Tables\n```sql\nSELECT AVG(price) FROM products WHERE 1 = 0;  -- Returns NULL, not 0\n```\nThis is correct SQL semantics but can surprise application code expecting a numeric result.\n### Integer Division in AVG\n```sql\n-- WRONG: Using integer division\navg = sum / count;  -- Integer division if both are int\n-- RIGHT: Promote to float\navg = (double)sum / count;\n```\n### JOIN Without Index on Large Tables\n```sql\nSELECT * FROM large_table_a a\nJOIN large_table_b b ON a.foreign_key = b.id;\n-- Without index on b.id or a.foreign_key, this is O(N\u00d7M)\n```\nThe query planner should warn about this, but the execution will be catastrophically slow.\n### HAVING References Non-Aggregate Column\n```sql\n-- WRONG: HAVING can't reference non-aggregated columns\nSELECT category, COUNT(*)\nFROM products\nGROUP BY category\nHAVING price > 100;  -- ERROR: price not in GROUP BY or aggregate\n-- RIGHT: Use WHERE for row-level filtering\nSELECT category, COUNT(*)\nFROM products\nWHERE price > 100\nGROUP BY category;\n```\n### NULL Group Keys\nSQL treats NULL as a distinct group key. All rows with NULL in the GROUP BY column form one group:\n```sql\nSELECT category, COUNT(*)\nFROM products\nGROUP BY category;\n-- Rows with NULL category are grouped together\n```\n---\n## Testing Strategy\n### Aggregate Function Tests\n```c\nvoid test_count_all() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 10)\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, NULL)\");\n    execute_sql(db, \"INSERT INTO test VALUES (3, 30)\");\n    Result* r = execute_sql(db, \"SELECT COUNT(*) FROM test\");\n    assert(r->row_count == 1);\n    assert(r->rows[0].columns[0].data.integer_val == 3);\n    free_result(r);\n    free_database(db);\n}\nvoid test_count_col_ignores_null() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1, 10)\");\n    execute_sql(db, \"INSERT INTO test VALUES (2, NULL)\");\n    execute_sql(db, \"INSERT INTO test VALUES (3, 30)\");\n    Result* r = execute_sql(db, \"SELECT COUNT(value) FROM test\");\n    assert(r->row_count == 1);\n    assert(r->rows[0].columns[0].data.integer_val == 2);  // NULL not counted\n    free_result(r);\n    free_database(db);\n}\nvoid test_avg_returns_float() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES (1)\");\n    execute_sql(db, \"INSERT INTO test VALUES (2)\");\n    execute_sql(db, \"INSERT INTO test VALUES (3)\");\n    Result* r = execute_sql(db, \"SELECT AVG(value) FROM test\");\n    assert(r->row_count == 1);\n    assert(r->rows[0].columns[0].type == VALUE_FLOAT);\n    assert(fabs(r->rows[0].columns[0].data.float_val - 2.0) < 0.001);\n    free_result(r);\n    free_database(db);\n}\nvoid test_aggregates_empty_table() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (value INTEGER)\");\n    Result* r = execute_sql(db, \"SELECT COUNT(*), SUM(value), AVG(value), MIN(value) FROM test\");\n    assert(r->row_count == 1);\n    assert(r->rows[0].columns[0].data.integer_val == 0);  // COUNT(*) = 0\n    assert(r->rows[0].columns[1].type == VALUE_NULL);    // SUM = NULL\n    assert(r->rows[0].columns[2].type == VALUE_NULL);    // AVG = NULL\n    assert(r->rows[0].columns[3].type == VALUE_NULL);    // MIN = NULL\n    free_result(r);\n    free_database(db);\n}\n```\n### GROUP BY Tests\n```c\nvoid test_group_by_basic() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (category TEXT, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('A', 10)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('B', 20)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('A', 30)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('B', 40)\");\n    Result* r = execute_sql(db, \"SELECT category, SUM(value) FROM test GROUP BY category ORDER BY category\");\n    assert(r->row_count == 2);\n    assert(strcmp(r->rows[0].columns[0].data.string_val, \"A\") == 0);\n    assert(r->rows[0].columns[1].data.integer_val == 40);  // 10 + 30\n    assert(strcmp(r->rows[1].columns[0].data.string_val, \"B\") == 0);\n    assert(r->rows[1].columns[1].data.integer_val == 60);  // 20 + 40\n    free_result(r);\n    free_database(db);\n}\nvoid test_having_filters_groups() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (category TEXT, value INTEGER)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('A', 10)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('A', 20)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('B', 100)\");\n    execute_sql(db, \"INSERT INTO test VALUES ('B', 200)\");\n    Result* r = execute_sql(db, \n        \"SELECT category, SUM(value) FROM test GROUP BY category HAVING SUM(value) > 200\");\n    assert(r->row_count == 1);\n    assert(strcmp(r->rows[0].columns[0].data.string_val, \"B\") == 0);\n    assert(r->rows[0].columns[1].data.integer_val == 300);\n    free_result(r);\n    free_database(db);\n}\n```\n### JOIN Tests\n```c\nvoid test_inner_join_basic() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE orders (id INTEGER, customer_id INTEGER, total REAL)\");\n    execute_sql(db, \"CREATE TABLE customers (id INTEGER, name TEXT)\");\n    execute_sql(db, \"INSERT INTO orders VALUES (1, 100, 50.0)\");\n    execute_sql(db, \"INSERT INTO orders VALUES (2, 100, 75.0)\");\n    execute_sql(db, \"INSERT INTO orders VALUES (3, 200, 100.0)\");\n    execute_sql(db, \"INSERT INTO customers VALUES (100, 'Alice')\");\n    execute_sql(db, \"INSERT INTO customers VALUES (200, 'Bob')\");\n    Result* r = execute_sql(db, \n        \"SELECT o.id, c.name, o.total \"\n        \"FROM orders o \"\n        \"JOIN customers c ON o.customer_id = c.id \"\n        \"ORDER BY o.id\");\n    assert(r->row_count == 3);\n    assert(r->rows[0].columns[0].data.integer_val == 1);\n    assert(strcmp(r->rows[0].columns[1].data.string_val, \"Alice\") == 0);\n    assert(r->rows[2].columns[1].data.string_val[0] == 'B');  // Bob\n    free_result(r);\n    free_database(db);\n}\nvoid test_join_with_where() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE orders (id INTEGER, customer_id INTEGER, status TEXT)\");\n    execute_sql(db, \"CREATE TABLE customers (id INTEGER, country TEXT)\");\n    execute_sql(db, \"INSERT INTO orders VALUES (1, 100, 'completed')\");\n    execute_sql(db, \"INSERT INTO orders VALUES (2, 100, 'pending')\");\n    execute_sql(db, \"INSERT INTO orders VALUES (3, 200, 'completed')\");\n    execute_sql(db, \"INSERT INTO customers VALUES (100, 'US')\");\n    execute_sql(db, \"INSERT INTO customers VALUES (200, 'CA')\");\n    Result* r = execute_sql(db, \n        \"SELECT o.id FROM orders o \"\n        \"JOIN customers c ON o.customer_id = c.id \"\n        \"WHERE o.status = 'completed' AND c.country = 'US'\");\n    assert(r->row_count == 1);\n    assert(r->rows[0].columns[0].data.integer_val == 1);\n    free_result(r);\n    free_database(db);\n}\n```\n---\n## What You've Built\nYour aggregate and join system provides:\n1. **Aggregate functions** \u2014 COUNT(*), COUNT(col), SUM, AVG, MIN, MAX\n2. **NULL handling** \u2014 Correct semantics for each aggregate\n3. **GROUP BY** \u2014 Hash-based aggregation with multiple grouping columns\n4. **HAVING** \u2014 Post-aggregation filtering\n5. **INNER JOIN** \u2014 Nested loop join with optional index optimization\n6. **JOIN + WHERE** \u2014 Correct filter ordering (WHERE before join, HAVING after)\n7. **Multiple aggregates** \u2014 Computing several aggregates in one query\n8. **Empty table handling** \u2014 Appropriate defaults (0 for COUNT, NULL for others)\nThis transforms your database from a row store into a relational engine. You can now answer analytical questions, not just retrieve data.\n---\n## Knowledge Cascade\n### 1. MapReduce \u2014 GROUP BY IS MapReduce\nGoogle's MapReduce is fundamentally a distributed GROUP BY:\n```\nMap:    (row) \u2192 emit(group_key, value)    -- Like extracting GROUP BY columns\nShuffle: Group by key                      -- Like hash table partitioning\nReduce:  (key, [values]) \u2192 aggregate      -- Like SUM, COUNT, AVG\n```\nHadoop, Spark, and Flink all implement this pattern. When you understand GROUP BY, you understand distributed aggregation. The difference is scale: single-machine hash tables vs. distributed shuffle.\n### 2. Stream Processing \u2014 Continuous Aggregation\nKafka Streams and Flink extend aggregation to unbounded streams:\n```sql\n-- Batch query: one-time aggregation\nSELECT user_id, COUNT(*) FROM events GROUP BY user_id;\n-- Stream query: continuous aggregation with windows\nSELECT user_id, COUNT(*) \nFROM events \nGROUP BY user_id, TUMBLE(event_time, INTERVAL '1' HOUR);\n```\nThe aggregate state is maintained continuously, updated as new events arrive. Windowing adds time-based grouping \u2014 \"GROUP BY user, hour\" instead of \"GROUP BY user.\"\n### 3. Dataframe Joins in pandas/Polars\nPython dataframes implement the same join algorithms:\n```python\n# pandas (Python)\nmerged = pd.merge(orders, customers, on='customer_id')\n# Your database\nSELECT * FROM orders JOIN customers ON orders.customer_id = customers.id\n```\nThe difference: dataframes operate in memory, databases spill to disk. Dataframes use hash joins by default; SQLite uses nested loops. The semantics are identical.\n### 4. GraphQL Query Resolution\nResolving a GraphQL query is fundamentally a join problem:\n```graphql\nquery {\n    users {\n        name\n        orders {\n            total\n        }\n    }\n}\n```\nThe resolver must:\n1. Fetch users (one table)\n2. For each user, fetch orders (another table)\n3. Combine into nested result\nThis is a left outer join. GraphQL resolvers are essentially join executors, often using the same batching and dataloader techniques databases use for optimization.\n### 5. Entity-Component-System in Game Engines\nECS (Entity-Component-System) is a database architecture for games:\n```\nEntity:    Just an ID (like a rowid)\nComponent: Data attached to entity (like a column in a sparse table)\nSystem:    Query over components (like a JOIN + filter)\n```\nA query like \"all entities with Position AND Velocity components\" is a join:\n```sql\nSELECT e.entity_id, p.x, p.y, v.dx, v.dy\nFROM entities e\nJOIN positions p ON e.entity_id = p.entity_id\nJOIN velocities v ON e.entity_id = v.entity_id\n```\nGame engines optimize these joins for cache locality, using the same principles as database join algorithms.\n---\n## What's Next\nWith aggregates and joins, your database is a complete relational engine. You can store data, query it with complex predicates, join across tables, and compute analytical summaries.\nBut this project has a natural endpoint. You've built:\n1. **Tokenizer & Parser** \u2014 SQL text to AST\n2. **Bytecode Compiler** \u2014 AST to executable instructions\n3. **Virtual Machine** \u2014 Instruction execution\n4. **Buffer Pool** \u2014 Page caching with LRU eviction\n5. **B-tree Storage** \u2014 Page format and tree operations\n6. **Query Execution** \u2014 SELECT, INSERT, UPDATE, DELETE\n7. **Secondary Indexes** \u2014 B+tree indexes for fast lookups\n8. **Query Planner** \u2014 Cost-based optimization\n9. **Transactions** \u2014 Rollback journal for ACID\n10. **WAL Mode** \u2014 Concurrent readers and writers\n11. **Aggregates & JOINs** \u2014 Relational completeness\nThis is a complete database. Not a toy \u2014 a real, functional, ACID-compliant, SQL-speaking database engine. You've implemented the same architecture used by SQLite, PostgreSQL, MySQL, and every relational database.\nThe skills you've learned \u2014 B-tree management, query optimization, transaction logging, buffer pool design \u2014 are the foundation of all data infrastructure. Whether you're building a cache, a message queue, a file system, or a distributed database, these patterns repeat.\nYou now understand what happens when you type `SELECT * FROM table`. You've built it.\n\n![SQLite Architecture: Satellite Map](./diagrams/diag-satellite-overview.svg)\n\n---\n[[CRITERIA_JSON: {\"milestone_id\": \"build-sqlite-m11\", \"criteria\": [\"COUNT(*) returns the total number of rows in the result set, including rows with NULL values in any column\", \"COUNT(col) returns the number of rows where col is NOT NULL, excluding rows with NULL in the specified column\", \"SUM(col) returns the sum of non-NULL values in the column; returns NULL if all values are NULL or table is empty\", \"AVG(col) returns the arithmetic mean of non-NULL values as a FLOAT/REAL type, even if input column is INTEGER\", \"AVG(col) returns NULL if all values are NULL or table is empty (no division by zero)\", \"MIN(col) returns the minimum non-NULL value; returns NULL if all values are NULL or table is empty\", \"MAX(col) returns the maximum non-NULL value; returns NULL if all values are NULL or table is empty\", \"Aggregate functions correctly handle empty tables: COUNT(*)=0, others return NULL\", \"Multiple aggregate functions can be computed in a single query (e.g., SELECT COUNT(*), AVG(price), SUM(quantity))\", \"GROUP BY partitions input rows into groups based on equality of grouping column values\", \"GROUP BY uses hash-based aggregation with a hash table mapping group keys to aggregate states\", \"Aggregate state is maintained per group, updated incrementally as rows are processed\", \"NULL values in GROUP BY columns form a single group (NULL is treated as equal to NULL for grouping)\", \"Multiple grouping columns are supported via composite keys in the hash table\", \"GROUP BY without ORDER BY returns groups in arbitrary order (hash table iteration order)\", \"HAVING clause filters groups after aggregation based on aggregate function results\", \"HAVING can reference aggregate functions that are not in the SELECT list\", \"INNER JOIN produces rows where the join condition evaluates to TRUE for both tables\", \"Nested loop join is the baseline implementation: for each outer row, scan/seek inner table for matches\", \"Join condition is evaluated for each combination of outer and inner rows\", \"JOIN with WHERE clause applies row-level filtering before or during join (predicates pushed to table scans)\", \"Join output combines columns from both tables in the order specified (outer columns first, then inner)\", \"Empty table in JOIN produces empty result (no matching rows possible)\", \"Join on NULL values produces no matches (NULL = NULL is NULL, not TRUE)\", \"Test suite validates COUNT(*) vs COUNT(col) with NULL values\", \"Test suite validates AVG returns FLOAT for INTEGER input\", \"Test suite validates all aggregates return appropriate values for empty tables\", \"Test suite validates GROUP BY correctly partitions rows into groups\", \"Test suite validates HAVING filters groups based on aggregate results\", \"Test suite validates INNER JOIN produces correct combined rows\", \"Test suite validates JOIN with WHERE filters correctly (both join condition and row filter)\", \"Test suite validates multiple aggregates in single query produce correct results\"]}] ]\n<!-- END_MS -->\n",
  "current_ms_index": 11,
  "diagrams_to_generate": [],
  "diagram_attempt": 0,
  "current_diagram_code": null,
  "current_diagram_meta": null,
  "last_error": null,
  "status": "tdd_visualizing",
  "phase": "tdd",
  "knowledge_map": [
    "expression parsing",
    "index range scan",
    "transaction manager",
    "Next opcode",
    "hash table lookup",
    "cursor corruption during iteration",
    "SeekGE opcode",
    "UNIQUE index constraint",
    "UPDATE with index maintenance",
    "frame header",
    "fetch-decode-execute loop",
    "secondary indexes",
    "magic number",
    "statistics collection",
    "write-read trade-off",
    "NaN propagation analogy",
    "background writer",
    "floating-point literals",
    "query planner",
    "OP_Column",
    "cardinalinality estimation",
    "missing statistics",
    "NOT operator with NULL",
    "eviction victim selection",
    "COMMIT statement",
    "page cache architecture",
    "cursor abstraction",
    "group commit",
    "two-character operators",
    "WHERE clause",
    "page table",
    "table scan",
    "serialized records",
    "foreign key constraint",
    "join cardinality",
    "serial types",
    "crossover selectivity",
    "checkpoint overhead",
    "EXPLAIN",
    "correlated columns",
    "jump patching",
    "language servers (LSP)",
    "page fragmentation",
    "FlushAll operation",
    "selective flush",
    "full table scan",
    "index B+tree leaf cell",
    "selectivity",
    "NULL handling in comparisons",
    "freeblock list",
    "ROLLBACK statement",
    "root split",
    "LRU list",
    "column groups",
    "max_frame",
    "next",
    "cell pointer array",
    "B+tree",
    "PRAGMA journal_mode",
    "WHERE clause evaluation",
    "plan selection",
    "IS NOT NULL operator",
    "rollback journal",
    "comparison operators with NULL",
    "write lock",
    "expression evaluation",
    "RESTART",
    "page records",
    "index lookup bytecode",
    "stale pointers",
    "recursive descent parsing",
    "predicate selectivity",
    "buffer pool statistics",
    "register-based VM execution",
    "CREATE INDEX execution",
    "linked leaves",
    "AST",
    "index B+tree structure",
    "read snapshot",
    "page versions",
    "version history",
    "UPDATE execution",
    "page frames",
    "selectivity threshold",
    "magic number validation",
    "SQL dialect translation",
    "covering index scan",
    "index selection",
    "sqlite_master",
    "unary negation",
    "cache hit path",
    "OP_Insert",
    "line and column numbers",
    "write-before-modify",
    "crash safety",
    "system catalog lookup",
    "frame validation",
    "three-valued logic",
    "I/O cost",
    "RAII patterns",
    "torn pages",
    "cumulative checksums",
    "index B+tree internal cell",
    "varint encoding",
    "page journaling",
    "lazy tokenization",
    "buffer pool pressure",
    "serialization",
    "TRUNCATE)",
    "checksum validation",
    "result row emission",
    "write amplification",
    "journal file format",
    "integer literals",
    "journal mode switching",
    "database statistics",
    "INSERT execution",
    "AND predicate",
    "postings lists",
    "stack-based virtual machine",
    "O(1) lookup",
    "LRU vs CLOCK",
    "dirty page write-back",
    "right-sibling pointers",
    "Cartesian product",
    "AST node structure",
    "OR predicate",
    "autocommit mode",
    "ANALYZE command",
    "index maintenance",
    "direct I/O",
    "AND operator truth table with NULL",
    "buffer pool integration",
    "cursor",
    "expression compilation",
    "hash table",
    "position tracking",
    "UNIQUE constraint",
    "identifier recognition",
    "concurrent writers",
    "endianness",
    "case-insensitive matching",
    "buffer pool",
    "record deserialization",
    "Abstract Syntax Tree (AST)",
    "browser cache eviction",
    "optimizer hints",
    "unary expressions",
    "frame offset",
    "system catalog",
    "MarkDirty function",
    "segmented LRU",
    "double lookup pattern",
    "multi-column statistics",
    "lazy journal creation",
    "prepared statements",
    "lexical analysis",
    "random I/O",
    "left recursion",
    "short-circuit evaluation",
    "join planning",
    "punctuation",
    "fixed-size pages",
    "test harness",
    "compaction",
    "OP_NewRowid",
    "EXPLAIN QUERY PLAN",
    "checkpoint",
    "VDBE",
    "uniform distribution assumption",
    "histogram",
    "data skew",
    "VALUE_BLOB)",
    "crash recovery",
    "LSM-trees",
    "atomicity",
    "CPU cost",
    "index seek implementation",
    "page header",
    "quoted identifiers",
    "token data structure",
    "table B-tree leaf cell",
    "lock contention",
    "inverted comparisons",
    "grammar rules",
    "internal nodes",
    "append-only log",
    "uncommitted frames",
    "NULL literal handling",
    "B-tree traversal",
    "finite state machine (FSM)",
    "SQL keywords",
    "string literals",
    "cardinality estimation",
    "operator precedence",
    "approximate LRU",
    "memoization",
    "BEGIN statement",
    "tree-walking interpreter",
    "escaped quotes",
    "all-pinned scenario",
    "hot journal detection",
    "VALUE_INTEGER",
    "byte order",
    "cost constants",
    "frame structure",
    "plan enumeration",
    "NULL propagation",
    "character classification",
    "journal header",
    "WAL file format",
    "WAL header",
    "query optimization",
    "CDN cache design",
    "fragmentation",
    "two-pass delete algorithm",
    "dirty page tracking",
    "node splitting",
    "game engine asset streaming",
    "constraints",
    "memory alignment",
    "checkpoint modes (PASSIVE",
    "page lookup",
    "independence assumption",
    "concurrent readers",
    "sequential I/O",
    "compiler state",
    "exclusive lock",
    "INSERT statement parsing",
    "scientific notation",
    "ACID guarantees",
    "selective fsync",
    "range scans",
    "reader registration",
    "NULL comparison semantics",
    "jump target resolution",
    "token stream",
    "instruction format",
    "sampling",
    "B-tree",
    "bytecode compilation",
    "index entry serialization",
    "cost model",
    "INSERT with index maintenance",
    "hit rate",
    "index scan",
    "undo log",
    "get_row)",
    "EXPLAIN command",
    "delete + insert pattern",
    "equality predicate",
    "join ordering",
    "register file",
    "WHERE clause compilation",
    "SQL injection prevention",
    "error recovery",
    "index B+tree insertion",
    "composite indexes",
    "VALUE_FLOAT",
    "buffer optimization",
    "overflow pages",
    "LRU-K",
    "precedence climbing algorithm",
    "page fetching",
    "DELETE execution",
    "ACID properties",
    "prefetch",
    "big-endian",
    "cache miss path",
    "selectivity estimation",
    "pin/unpin protocol",
    "pin count",
    "covering index optimization",
    "fsync",
    "big-endian encoding",
    "OP_Rewind",
    "type coercion",
    "B+trees",
    "binary expressions",
    "DELETE with index maintenance",
    "auto-increment behavior",
    "batch operations",
    "eager tokenization",
    "OP_Next",
    "OS page cache",
    "register-based virtual machine",
    "single-line comments",
    "conditional jumps",
    "read isolation",
    "use-after-free",
    "range predicate",
    "idempotent recovery",
    "auto-checkpoint",
    "durability guarantee",
    "write-back caching",
    "cursor operations (first",
    "slotted page format",
    "transaction state machine",
    "index scan vs table scan",
    "OP_MakeRecord",
    "doubly-linked list",
    "checksum verification",
    "UTF-8 handling",
    "value types (VALUE_NULL",
    "WAL recovery",
    "iterator pinning",
    "LRU eviction",
    "write ordering",
    "commit markers",
    "covering index",
    "dirty flag",
    "table B-tree internal cell",
    "hash table collisions",
    "operators",
    "lookahead",
    "row serialization",
    "NULL handling in UNIQUE indexes",
    "database durability",
    "SeekRowid opcode",
    "Write-Ahead Logging (WAL)",
    "leftmost prefix rule",
    "temporal locality",
    "fanout",
    "WAL mode",
    "committed frames",
    "FetchPage operation",
    "rowid immutability",
    "B-tree leaf iteration",
    "tokenization",
    "OR operator truth table with NULL",
    "inverted comparison bytecode pattern",
    "PRIMARY KEY constraint",
    "hash indexes",
    "VDBE opcodes (OP_OpenRead",
    "leaf nodes",
    "keyword lookup",
    "inverted indexes",
    "multi-line comments",
    "pointer arithmetic",
    "parenthesized expressions",
    "instruction emission",
    "temporal view",
    "optimistic concurrency",
    "OP_Delete)",
    "column list parsing",
    "memory management",
    "access path",
    "stale statistics",
    "table not found error",
    "little-endian",
    "opcodes",
    "row deserialization",
    "nested loop join",
    "IS NULL operator",
    "type-length-value encoding",
    "column projection",
    "FULL",
    "column definitions",
    "transaction boundaries",
    "NOT NULL constraint",
    "access pattern knowledge",
    "binary search",
    "statistics",
    "salt values",
    "WAL index",
    "error propagation",
    "write-through vs write-back",
    "record format",
    "error handling",
    "constraint enforcement",
    "snapshot isolation",
    "dynamic programming",
    "heuristic optimization",
    "record header parsing",
    "index metadata",
    "bytecode",
    "lexer state machine",
    "mmap limitations",
    "CREATE TABLE parsing",
    "unterminated string detection",
    "left associativity",
    "pessimistic concurrency"
  ],
  "advanced_contexts": [],
  "tdd_blueprint": {
    "project_title": "Build Your Own SQLite",
    "design_vision": "A complete embedded SQL database engine implementing SQLite's architecture: lexical analysis through recursive-descent parsing, bytecode compilation for a register-based virtual machine (VDBE), page-based B-tree/B+tree storage with buffer pool management, cost-based query optimization, and ACID transactions via rollback journal and write-ahead logging. The system demonstrates the fundamental patterns underlying all relational databases.",
    "modules": [
      {
        "id": "build-sqlite-m1",
        "name": "SQL Tokenizer",
        "description": "Converts SQL text into a typed token stream using a finite state machine. Handles keywords (case-insensitive), string literals with escaped quotes, numeric literals (integers and floats), operators (single and multi-character), identifiers (quoted and unquoted), and comments. Provides precise error location reporting.",
        "specs": {
          "inputs": "Raw SQL string (character stream)",
          "outputs": "Token stream: list of Token objects with type, value, line, column",
          "abstractions": "Token types: KEYWORD, IDENTIFIER, STRING, INTEGER, FLOAT, OPERATOR, PUNCTUATION, EOF, ERROR. State machine states: START, IN_IDENTIFIER, IN_NUMBER, IN_STRING, IN_QUOTED_ID, IN_COMMENT, IN_OPERATOR.",
          "error_categories": [
            "UNRECOGNIZED_CHARACTER",
            "UNTERMINATED_STRING",
            "UNTERMINATED_QUOTED_IDENTIFIER",
            "UNTERMINATED_COMMENT"
          ],
          "concurrency_model": "Single-threaded, stateless transformation",
          "performance_targets": [
            "Tokenize 10KB SQL in <1ms",
            "Support streaming tokenization for large scripts",
            "Keyword lookup O(log n) via binary search"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Token types and Lexer struct",
            "estimated_hours": "0.5"
          },
          {
            "phase": 2,
            "name": "Keyword recognition (case-insensitive)",
            "estimated_hours": "0.5"
          },
          {
            "phase": 3,
            "name": "String literal parsing with escape handling",
            "estimated_hours": "1"
          },
          {
            "phase": 4,
            "name": "Numeric literal parsing (int/float)",
            "estimated_hours": "0.5"
          },
          {
            "phase": 5,
            "name": "Operator and punctuation tokenization",
            "estimated_hours": "0.5"
          },
          {
            "phase": 6,
            "name": "Comment handling (single and multi-line)",
            "estimated_hours": "0.5"
          },
          {
            "phase": 7,
            "name": "Error reporting with position tracking",
            "estimated_hours": "0.5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m1-1",
            "title": "Tokenizer State Machine",
            "description": "FSM showing all states and transitions for token recognition: START\u2192IN_IDENTIFIER\u2192emit, START\u2192IN_NUMBER\u2192emit, START\u2192IN_STRING\u2192escape_check\u2192emit, etc.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m1"
          },
          {
            "id": "tdd-diag-m1-2",
            "title": "Token Data Structure",
            "description": "Token struct with type enum, value string pointer, line/column integers. TokenStream as array of tokens with count and capacity.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m1"
          },
          {
            "id": "tdd-diag-m1-3",
            "title": "Keyword Lookup Table",
            "description": "Sorted array of keyword strings with binary search index. Shows comparison flow for case-insensitive matching.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m1"
          },
          {
            "id": "tdd-diag-m1-4",
            "title": "String Literal Escape Handling",
            "description": "Step-by-step state changes when processing 'it''s': read ', enter string state, read chars, encounter '', recognize escape, output single ', continue.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m1"
          }
        ]
      },
      {
        "id": "build-sqlite-m2",
        "name": "SQL Parser (AST)",
        "description": "Transforms token stream into Abstract Syntax Tree using recursive descent parsing. Handles SELECT, INSERT, CREATE TABLE statements with full clause support. Implements precedence climbing for expression parsing with correct operator binding.",
        "specs": {
          "inputs": "Token stream from tokenizer",
          "outputs": "AST root node representing complete statement",
          "abstractions": "AST node types: SELECT_STMT, INSERT_STMT, CREATE_STMT, COLUMN_LIST, WHERE_CLAUSE, BINARY_EXPR, UNARY_EXPR, LITERAL_EXPR, IDENTIFIER_EXPR, COLUMN_DEF. Precedence levels: OR < AND < NOT < comparison < additive < multiplicative < unary < primary.",
          "error_categories": [
            "SYNTAX_ERROR",
            "UNEXPECTED_TOKEN",
            "MISSING_CLAUSE",
            "INVALID_EXPRESSION"
          ],
          "concurrency_model": "Single-threaded, recursive descent",
          "performance_targets": [
            "Parse 1KB SQL in <2ms",
            "AST memory proportional to statement complexity",
            "Error recovery for multi-statement batches"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "AST node types and structures",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "Parser core with token consumption",
            "estimated_hours": "1"
          },
          {
            "phase": 3,
            "name": "SELECT statement parsing",
            "estimated_hours": "1.5"
          },
          {
            "phase": 4,
            "name": "INSERT statement parsing",
            "estimated_hours": "1"
          },
          {
            "phase": 5,
            "name": "CREATE TABLE parsing with constraints",
            "estimated_hours": "1"
          },
          {
            "phase": 6,
            "name": "Expression parsing with precedence climbing",
            "estimated_hours": "1.5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m2-1",
            "title": "AST Node Hierarchy",
            "description": "Class hierarchy showing ASTNode base with SELECT_STMT, INSERT_STMT, CREATE_STMT, BINARY_EXPR, UNARY_EXPR, LITERAL_EXPR, IDENTIFIER_EXPR subclasses. Each shows type-specific fields.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m2"
          },
          {
            "id": "tdd-diag-m2-2",
            "title": "Expression Precedence Tree",
            "description": "Shows 'a OR b AND c' parsed as OR(a, AND(b, c)) vs '(a OR b) AND c' parsed as AND(OR(a, b), c). Visualizes tree depth correlation with operator binding.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m2"
          },
          {
            "id": "tdd-diag-m2-3",
            "title": "Precedence Climbing Algorithm",
            "description": "Step-by-step execution trace for parsing 'NOT a = 1 AND b > 2': parse_unary(NOT), parse_primary(a), check precedence, build comparisons, combine with AND at root.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m2"
          },
          {
            "id": "tdd-diag-m2-4",
            "title": "SELECT AST Structure",
            "description": "Complete AST for 'SELECT id, name FROM users WHERE age > 18': SELECT_STMT node with columns list, FROM clause (table ref), WHERE clause (BINARY_EXPR tree).",
            "type": "architecture",
            "anchor_target": "build-sqlite-m2"
          },
          {
            "id": "tdd-diag-m2-5",
            "title": "Parser Error Recovery",
            "description": "Sequence showing error detection, synchronization point search (semicolon or statement keyword), continuation to next statement.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m2"
          }
        ]
      },
      {
        "id": "build-sqlite-m3",
        "name": "Bytecode Compiler (VDBE)",
        "description": "Compiles AST into bytecode instructions for a register-based virtual machine. Implements opcodes for table operations, cursor management, expression evaluation, and control flow. Generates EXPLAIN output for query introspection.",
        "specs": {
          "inputs": "AST root node from parser",
          "outputs": "Compiled program: array of Instructions with opcode and operands",
          "abstractions": "OpCodes: OpenRead, OpenWrite, Close, Rewind, Next, Column, MakeRecord, Insert, Delete, Eq/Ne/Lt/Le/Gt/Ge, Goto, Halt, Integer, String8, Null, Add/Sub/Mul/Div, ResultRow. Register file: array of typed Values. Cursor: B-tree position tracker.",
          "error_categories": [
            "COMPILATION_ERROR",
            "UNKNOWN_TABLE",
            "UNKNOWN_COLUMN",
            "TYPE_MISMATCH"
          ],
          "concurrency_model": "Single-threaded compilation, single-threaded VM execution",
          "performance_targets": [
            "Compile 100-statement batch in <10ms",
            "Execute SELECT * FROM 10K-row table in <100ms",
            "Support at least 100 registers per program"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Instruction set definition",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "Compiler framework with emit()",
            "estimated_hours": "1"
          },
          {
            "phase": 3,
            "name": "SELECT compilation (table scan)",
            "estimated_hours": "2"
          },
          {
            "phase": 4,
            "name": "WHERE clause to conditional jumps",
            "estimated_hours": "2"
          },
          {
            "phase": 5,
            "name": "INSERT compilation",
            "estimated_hours": "1.5"
          },
          {
            "phase": 6,
            "name": "VM fetch-decode-execute loop",
            "estimated_hours": "1.5"
          },
          {
            "phase": 7,
            "name": "EXPLAIN command output",
            "estimated_hours": "1"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m3-1",
            "title": "VDBE Architecture",
            "description": "Virtual machine components: code array, PC register, register file (Value array), cursor array, halt flag. Arrows showing fetch from code, decode opcode, execute, update PC.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m3"
          },
          {
            "id": "tdd-diag-m3-2",
            "title": "Instruction Format",
            "description": "Instruction struct: opcode (enum), p1-p5 (int/pointer operands). Memory layout showing packed vs unpacked representations.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m3"
          },
          {
            "id": "tdd-diag-m3-3",
            "title": "SELECT Compilation Flow",
            "description": "AST SELECT_STMT \u2192 OpenRead \u2192 Rewind \u2192 [Column ops] \u2192 ResultRow \u2192 Next (loop) \u2192 Halt. Shows jump target patching for loop.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m3"
          },
          {
            "id": "tdd-diag-m3-4",
            "title": "WHERE Clause Bytecode Pattern",
            "description": "Bytecode sequence for 'WHERE age > 18': Column(age) \u2192 Integer(18) \u2192 Le (inverted, jump to SKIP) \u2192 [output row] \u2192 SKIP: Next. Shows inverted comparison logic.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m3"
          },
          {
            "id": "tdd-diag-m3-5",
            "title": "Register Allocation During Compilation",
            "description": "Compiler state showing next_register counter, allocation for expression intermediates, result row registers. Example: r1=age, r2=18, r3=result.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m3"
          },
          {
            "id": "tdd-diag-m3-6",
            "title": "VM Execution Loop State Machine",
            "description": "States: FETCH, DECODE, EXECUTE, HALT. Transitions on each instruction type. Error state for unknown opcodes.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m3"
          }
        ]
      },
      {
        "id": "build-sqlite-m4",
        "name": "Buffer Pool Manager",
        "description": "Manages fixed-size page cache between B-tree layer and disk. Implements LRU eviction with pin counting for safe concurrent access. Tracks dirty pages for write-back caching and provides FlushAll for checkpoint operations.",
        "specs": {
          "inputs": "Page requests (PageId), disk I/O through FileManager",
          "outputs": "Frame pointers containing page data, eviction statistics",
          "abstractions": "Frame: page_id, data[PAGE_SIZE], pin_count, is_dirty, LRU pointers. BufferPool: frames array, page_table (hash map), LRU list head/tail. Page replacement: LRU with pin protection.",
          "error_categories": [
            "BUFFER_POOL_EXHAUSTED",
            "INVALID_PAGE_ID",
            "PIN_COUNT_UNDERFLOW"
          ],
          "concurrency_model": "Single-threaded with pin counting (thread-safe extension possible)",
          "performance_targets": [
            "Cache hit rate >95% on typical workloads",
            "FetchPage (hit) in <100ns",
            "FetchPage (miss) dominated by disk I/O",
            "Default 1000 frames, 4096-byte pages"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Frame and BufferPool structures",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "Page table hash map",
            "estimated_hours": "1"
          },
          {
            "phase": 3,
            "name": "FetchPage with hit/miss paths",
            "estimated_hours": "2"
          },
          {
            "phase": 4,
            "name": "LRU list management",
            "estimated_hours": "1.5"
          },
          {
            "phase": 5,
            "name": "Eviction with dirty write-back",
            "estimated_hours": "1"
          },
          {
            "phase": 6,
            "name": "Pin/Unpin and FlushAll",
            "estimated_hours": "1.5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m4-1",
            "title": "Buffer Pool Internal Structure",
            "description": "BufferPool struct showing: frames array (contiguous), page_table hash map (PageId\u2192Frame*), LRU doubly-linked list (head/tail pointers), statistics counters.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m4"
          },
          {
            "id": "tdd-diag-m4-2",
            "title": "Frame Memory Layout",
            "description": "Frame struct: page_id (4 bytes), data[4096] (4096 bytes), pin_count (4 bytes), is_dirty (1 byte), lru_prev/lru_next pointers (8 bytes each). Total with padding.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m4"
          },
          {
            "id": "tdd-diag-m4-3",
            "title": "FetchPage Flow: Hit vs Miss",
            "description": "Two paths: HIT (hash lookup \u2192 pin++ \u2192 move to LRU head \u2192 return), MISS (find free/evict \u2192 disk read \u2192 init frame \u2192 hash insert \u2192 LRU head \u2192 return).",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m4"
          },
          {
            "id": "tdd-diag-m4-4",
            "title": "LRU List Operations",
            "description": "Doubly-linked list showing: move_to_head (unlink, insert at front), find_victim (scan from tail, skip pinned), eviction (write if dirty, unlink, remove from hash).",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m4"
          },
          {
            "id": "tdd-diag-m4-5",
            "title": "Pin Count Lifecycle",
            "description": "State diagram: pin_count=0 (evictable) \u2192 pin++ (in use) \u2192 pin++ (nested use) \u2192 pin-- \u2192 pin-- \u2192 pin_count=0 (evictable again). Error state for underflow.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m4"
          }
        ]
      },
      {
        "id": "build-sqlite-m5",
        "name": "B-tree Page Format & Table Storage",
        "description": "Implements on-disk page structure for B-trees (tables) and B+trees (indexes). Handles variable-length record encoding with varints, slotted page format for bidirectional growth, and node splitting for tree maintenance. Manages system catalog for schema persistence.",
        "specs": {
          "inputs": "Row data (ColumnValue arrays), page read/write requests",
          "outputs": "Serialized pages (exactly 4096 bytes), deserialized rows",
          "abstractions": "Page types: TABLE_LEAF (0x0D), TABLE_INTERNAL (0x05), INDEX_LEAF (0x0A), INDEX_INTERNAL (0x02). Slotted page: header \u2192 cell pointers \u2192 free space \u2190 cell content. Varint: 1-9 byte variable-length integers. Serial types: 0=NULL, 1-6=integers, 7=float, 8-9=0/1, N>=12=BLOB/TEXT.",
          "error_categories": [
            "PAGE_OVERFLOW",
            "INVALID_PAGE_TYPE",
            "DESERIALIZATION_ERROR",
            "VARINT_OUT_OF_RANGE"
          ],
          "concurrency_model": "Single-threaded page operations, buffer pool coordinates access",
          "performance_targets": [
            "Varint encode/decode in <50ns",
            "Row serialization proportional to column count",
            "Page split maintains >50% fill factor"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Page header format and serialization",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "Slotted page cell management",
            "estimated_hours": "2"
          },
          {
            "phase": 3,
            "name": "Varint encoding/decoding",
            "estimated_hours": "1"
          },
          {
            "phase": 4,
            "name": "Row record format (serial types)",
            "estimated_hours": "2"
          },
          {
            "phase": 5,
            "name": "Table B-tree cell formats",
            "estimated_hours": "1.5"
          },
          {
            "phase": 6,
            "name": "Index B+tree cell formats",
            "estimated_hours": "1.5"
          },
          {
            "phase": 7,
            "name": "Node splitting algorithm",
            "estimated_hours": "2"
          },
          {
            "phase": 8,
            "name": "System catalog (sqlite_master)",
            "estimated_hours": "1"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m5-1",
            "title": "Slotted Page Layout",
            "description": "4096-byte page: header (8-12 bytes) \u2192 cell pointers (2 bytes each) \u2192 free space (grows toward middle) \u2190 cell content (grows from end). Cell_content_start pointer marks boundary.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-2",
            "title": "Page Header Fields",
            "description": "Header bytes: page_type (1), first_freeblock (2), cell_count (2), cell_content_start (2), fragmented_bytes (1), right_child (4, internal only). Byte offsets labeled.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-3",
            "title": "Varint Encoding Algorithm",
            "description": "Step-by-step for encoding 16384: calculate bytes needed (3), write continuation bits (1xxxxxx 1xxxxxx 0xxxxxxx), big-endian encoding. Decode shows reverse process.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-4",
            "title": "Row Record Format",
            "description": "Record bytes: header_size varint \u2192 serial_type per column (varints) \u2192 column data (variable). Example row (42, 'Alice', 30) with byte-level breakdown.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-5",
            "title": "B-tree vs B+tree Structure",
            "description": "Side-by-side comparison: Table B-tree (data in all nodes, keyed by rowid) vs Index B+tree (data only in leaves, linked leaves, separator keys in internal nodes).",
            "type": "architecture",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-6",
            "title": "Node Split Sequence",
            "description": "Before/after split: full page \u2192 allocate sibling \u2192 redistribute cells \u2192 promote separator \u2192 update parent (or create new root). Shows cell movement and key promotion.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-7",
            "title": "System Catalog Schema",
            "description": "sqlite_master table: type (TEXT), name (TEXT), tbl_name (TEXT), rootpage (INTEGER), sql (TEXT). Shows how CREATE TABLE populates this table.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m5"
          }
        ]
      },
      {
        "id": "build-sqlite-m6",
        "name": "SELECT Execution & DML",
        "description": "Implements cursor-based table scans, column projection, WHERE clause evaluation with three-valued logic, and INSERT/UPDATE/DELETE execution. Enforces NOT NULL and UNIQUE constraints with descriptive error messages.",
        "specs": {
          "inputs": "Bytecode programs from compiler, table schema from catalog",
          "outputs": "Result rows (Value arrays), execution status, constraint errors",
          "abstractions": "Cursor: B-tree position tracker with eof flag. Three-valued logic: TRUE, FALSE, NULL. Projection: extracting subset of columns. DML patterns: INSERT (MakeRecord + Insert), UPDATE (delete + re-insert), DELETE (two-pass to avoid cursor corruption).",
          "error_categories": [
            "CONSTRAINT_NOT_NULL",
            "CONSTRAINT_UNIQUE",
            "CONSTRAINT_PRIMARY_KEY",
            "TABLE_NOT_FOUND",
            "COLUMN_NOT_FOUND"
          ],
          "concurrency_model": "Single-threaded execution, transaction isolation via dirty page visibility",
          "performance_targets": [
            "Table scan of 10K rows in <50ms",
            "WHERE evaluation with NULL handling adds <10% overhead",
            "Constraint checking in <1\u03bcs per row"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Cursor abstraction and operations",
            "estimated_hours": "1.5"
          },
          {
            "phase": 2,
            "name": "Column projection in VDBE",
            "estimated_hours": "1"
          },
          {
            "phase": 3,
            "name": "Three-valued logic for comparisons",
            "estimated_hours": "1.5"
          },
          {
            "phase": 4,
            "name": "WHERE clause evaluation",
            "estimated_hours": "1.5"
          },
          {
            "phase": 5,
            "name": "INSERT execution",
            "estimated_hours": "1.5"
          },
          {
            "phase": 6,
            "name": "UPDATE execution (delete+insert)",
            "estimated_hours": "1"
          },
          {
            "phase": 7,
            "name": "DELETE two-pass algorithm",
            "estimated_hours": "1"
          },
          {
            "phase": 8,
            "name": "Constraint enforcement",
            "estimated_hours": "1"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m6-1",
            "title": "Cursor-Based Table Scan",
            "description": "Cursor state machine: INIT \u2192 positioned at first row \u2192 GET_ROW (return current) \u2192 NEXT \u2192 (more rows?) \u2192 GET_ROW or EOF. Shows page traversal for B-tree.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m6"
          },
          {
            "id": "tdd-diag-m6-2",
            "title": "Three-Valued Logic Truth Tables",
            "description": "AND/OR/NOT truth tables with NULL column. Shows: NULL AND FALSE = FALSE, NULL OR TRUE = TRUE, NOT NULL = NULL. Comparison with NULL yields NULL.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m6"
          },
          {
            "id": "tdd-diag-m6-3",
            "title": "WHERE Clause Bytecode Execution",
            "description": "Trace of 'WHERE age > 18 AND status = active': read age \u2192 compare \u2192 (fail?) skip \u2192 read status \u2192 compare \u2192 (fail?) skip \u2192 output. Shows jump targets.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m6"
          },
          {
            "id": "tdd-diag-m6-4",
            "title": "INSERT Bytecode Sequence",
            "description": "OpenWrite \u2192 Integer/String8 (values) \u2192 MakeRecord \u2192 NewRowid \u2192 Insert \u2192 Close \u2192 Halt. Shows register usage for each value.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m6"
          },
          {
            "id": "tdd-diag-m6-5",
            "title": "DELETE Two-Pass Algorithm",
            "description": "Pass 1: scan table, collect rowids to delete into temp array. Pass 2: iterate array, delete by rowid. Shows why single-pass corrupts cursor.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m6"
          },
          {
            "id": "tdd-diag-m6-6",
            "title": "Constraint Checking Flow",
            "description": "INSERT path: serialize row \u2192 check NOT NULL per column \u2192 check UNIQUE via index lookup \u2192 (pass?) Insert \u2192 (fail?) return error with column name.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m6"
          }
        ]
      },
      {
        "id": "build-sqlite-m7",
        "name": "Secondary Indexes",
        "description": "Implements B+tree secondary indexes mapping column values to rowids. Handles automatic index maintenance on INSERT/UPDATE/DELETE. Supports equality lookups and range scans via linked leaf traversal. Enforces UNIQUE constraint at index level.",
        "specs": {
          "inputs": "CREATE INDEX statement, DML operations on indexed tables",
          "outputs": "Index B+tree structures, faster query execution via index scans",
          "abstractions": "Index entry: (key, rowid) pair in leaf cells. Double lookup: index seek \u2192 get rowid \u2192 table seek. Covering index: all needed columns in index (skip table lookup). Range scan: follow right_sibling pointers through leaves.",
          "error_categories": [
            "INDEX_CREATION_FAILED",
            "UNIQUE_CONSTRAINT_VIOLATION",
            "INDEX_CORRUPTION"
          ],
          "concurrency_model": "Index updates synchronized with table updates in same transaction",
          "performance_targets": [
            "CREATE INDEX on 100K rows in <500ms",
            "Index lookup (equality) in <1ms for 1M-row table",
            "Range scan proportional to matching rows, not table size"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Index B+tree cell formats",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "CREATE INDEX execution",
            "estimated_hours": "2"
          },
          {
            "phase": 3,
            "name": "Index maintenance on INSERT",
            "estimated_hours": "1.5"
          },
          {
            "phase": 4,
            "name": "Index maintenance on DELETE",
            "estimated_hours": "1"
          },
          {
            "phase": 5,
            "name": "Index maintenance on UPDATE",
            "estimated_hours": "1.5"
          },
          {
            "phase": 6,
            "name": "Index equality lookup",
            "estimated_hours": "1"
          },
          {
            "phase": 7,
            "name": "Index range scan",
            "estimated_hours": "1"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m7-1",
            "title": "Index B+tree Structure",
            "description": "Three-level B+tree: root (separators) \u2192 internal (separators) \u2192 leaves (key+rowid pairs, linked via right_sibling). Shows key distribution and leaf linking.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m7"
          },
          {
            "id": "tdd-diag-m7-2",
            "title": "Double Lookup Pattern",
            "description": "Query flow: index cursor seek \u2192 find (key, rowid) \u2192 extract rowid \u2192 table cursor seek by rowid \u2192 fetch full row. Shows two B-tree traversals.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m7"
          },
          {
            "id": "tdd-diag-m7-3",
            "title": "CREATE INDEX Execution",
            "description": "Steps: allocate root page \u2192 scan table \u2192 for each row: extract indexed column \u2192 insert (key, rowid) into index B+tree \u2192 record metadata in catalog.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m7"
          },
          {
            "id": "tdd-diag-m7-4",
            "title": "Index Maintenance on DML",
            "description": "INSERT: after table insert, insert into all indexes. DELETE: before table delete, read row, delete from all indexes. UPDATE: detect changed indexed columns, delete old entries, insert new entries.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m7"
          },
          {
            "id": "tdd-diag-m7-5",
            "title": "Index Range Scan Bytecode",
            "description": "OpenRead (index) \u2192 SeekGE (start of range) \u2192 LOOP: Column (key) \u2192 check bounds \u2192 (in range?) seek table \u2192 output \u2192 Next (follow sibling) \u2192 LOOP.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m7"
          },
          {
            "id": "tdd-diag-m7-6",
            "title": "Covering Index Optimization",
            "description": "Query 'SELECT email FROM users WHERE email = X' with index on email: index contains all needed columns, skip table lookup. Shows bytecode without SeekRowid.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m7"
          }
        ]
      },
      {
        "id": "build-sqlite-m8",
        "name": "Query Planner & Statistics",
        "description": "Implements cost-based query optimization using collected statistics. ANALYZE command gathers table and index statistics. Selectivity estimation predicts predicate match rates. Plan enumeration compares table scan vs index scan costs. Join planning optimizes multi-table query order.",
        "specs": {
          "inputs": "Query AST, statistics from sqlite_stat1",
          "outputs": "Optimal execution plan (access path selection)",
          "abstractions": "Statistics: row_count, page_count, distinct_count, min/max values. Selectivity: fraction of rows matching predicate. Cost model: I/O cost (sequential vs random) + CPU cost. Plan: access path (scan type, index choice) + estimated rows + estimated cost.",
          "error_categories": [
            "STATISTICS_OUT_OF_DATE",
            "NO_STATISTICS_AVAILABLE",
            "PLAN_GENERATION_FAILED"
          ],
          "concurrency_model": "Planning is single-threaded, statistics shared read-only",
          "performance_targets": [
            "Plan generation in <1ms for 10-table queries",
            "Selectivity estimates within 10x of actual",
            "Join ordering via dynamic programming for \u226410 tables"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Statistics storage format",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "ANALYZE command implementation",
            "estimated_hours": "2"
          },
          {
            "phase": 3,
            "name": "Selectivity estimation functions",
            "estimated_hours": "1.5"
          },
          {
            "phase": 4,
            "name": "Cost model implementation",
            "estimated_hours": "1.5"
          },
          {
            "phase": 5,
            "name": "Access path enumeration",
            "estimated_hours": "1.5"
          },
          {
            "phase": 6,
            "name": "Plan selection logic",
            "estimated_hours": "1"
          },
          {
            "phase": 7,
            "name": "Join planning (basic)",
            "estimated_hours": "1.5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m8-1",
            "title": "Statistics Collection Flow",
            "description": "ANALYZE: scan table \u2192 count rows/pages \u2192 for each index: scan \u2192 count distinct values, nulls, min/max \u2192 store in sqlite_stat1 table.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m8"
          },
          {
            "id": "tdd-diag-m8-2",
            "title": "Selectivity Estimation Logic",
            "description": "Equality: 1/distinct_count. Range: (high-low)/(max-min). AND: sel_a \u00d7 sel_b. OR: sel_a + sel_b - (sel_a \u00d7 sel_b). Shows formula application.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m8"
          },
          {
            "id": "tdd-diag-m8-3",
            "title": "Cost Model Comparison",
            "description": "Table scan cost: pages \u00d7 seq_io + rows \u00d7 cpu. Index scan cost: log(index_pages) + matching_rows \u00d7 random_io. Graph showing crossover point at ~10% selectivity.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m8"
          },
          {
            "id": "tdd-diag-m8-4",
            "title": "Plan Enumeration Tree",
            "description": "For query with 2 indexes: show 3 paths (table scan, index A scan, index B scan). Each path shows estimated cost. Minimum selected.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m8"
          },
          {
            "id": "tdd-diag-m8-5",
            "title": "Join Order Impact",
            "description": "Two join orders for same query: (small \u22c8 large) vs (large \u22c8 small). Show intermediate result sizes and total I/O. Demonstrate 10x+ difference.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m8"
          },
          {
            "id": "tdd-diag-m8-6",
            "title": "EXPLAIN Output Format",
            "description": "EXPLAIN QUERY PLAN output: tree structure with SCAN/SEARCH nodes, index names, estimated rows and cost. Shows readable plan representation.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m8"
          }
        ]
      },
      {
        "id": "build-sqlite-m9",
        "name": "Transactions (Rollback Journal)",
        "description": "Implements ACID transactions using rollback journal for crash recovery. BEGIN starts transaction, first write creates journal file with original pages, COMMIT ensures write ordering (journal fsync \u2192 database write \u2192 database fsync \u2192 journal delete), ROLLBACK restores from journal. Hot journal detection on startup triggers automatic recovery.",
        "specs": {
          "inputs": "BEGIN/COMMIT/ROLLBACK statements, page modifications",
          "outputs": "Durable transactions, crash recovery to consistent state",
          "abstractions": "Transaction states: AUTOCOMMIT, STARTED, DIRTY, COMMITTING, ROLLING_BACK. Journal file: header (magic, salts, checksums) + page records (page_id + original data). Write ordering: journal fsync BEFORE database write. Hot journal: journal exists on startup (crash indicator).",
          "error_categories": [
            "TRANSACTION_ACTIVE",
            "JOURNAL_WRITE_FAILED",
            "JOURNAL_SYNC_FAILED",
            "DATABASE_SYNC_FAILED",
            "RECOVERY_FAILED"
          ],
          "concurrency_model": "Single writer, readers see only committed state (dirty pages not flushed)",
          "performance_targets": [
            "BEGIN/COMMIT overhead <1ms for read-only",
            "Journal write: 1 page in <1ms",
            "Crash recovery in <100ms for 1000-page journal"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Transaction state machine",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "Journal file format and header",
            "estimated_hours": "1"
          },
          {
            "phase": 3,
            "name": "BEGIN and lazy journal creation",
            "estimated_hours": "1"
          },
          {
            "phase": 4,
            "name": "Write-before-modify journaling",
            "estimated_hours": "1.5"
          },
          {
            "phase": 5,
            "name": "COMMIT with write ordering",
            "estimated_hours": "2"
          },
          {
            "phase": 6,
            "name": "ROLLBACK implementation",
            "estimated_hours": "1.5"
          },
          {
            "phase": 7,
            "name": "Hot journal detection and recovery",
            "estimated_hours": "2"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m9-1",
            "title": "Transaction State Machine",
            "description": "States: AUTOCOMMIT \u2192 (BEGIN) \u2192 STARTED \u2192 (first write) \u2192 DIRTY \u2192 (COMMIT) \u2192 COMMITTING \u2192 (done) \u2192 AUTOCOMMIT. Alternative: DIRTY \u2192 (ROLLBACK) \u2192 ROLLING_BACK \u2192 AUTOCOMMIT.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m9"
          },
          {
            "id": "tdd-diag-m9-2",
            "title": "Journal File Format",
            "description": "Bytes 0-27: header (magic, version, page_size, seq, salt1, salt2, checksums). Then: [page_id (4) + page_data (4096)] repeated. Shows record alignment.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m9"
          },
          {
            "id": "tdd-diag-m9-3",
            "title": "COMMIT Write Ordering",
            "description": "Sequence diagram: write journal \u2192 fsync(journal) \u2713 \u2192 write database pages \u2192 fsync(database) \u2713 \u2192 delete journal. Shows crash points and recovery behavior at each stage.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m9"
          },
          {
            "id": "tdd-diag-m9-4",
            "title": "ROLLBACK Page Restoration",
            "description": "Steps: open journal \u2192 skip header \u2192 for each record: read page_id \u2192 read original data \u2192 write to database at page_id \u2192 sync database \u2192 delete journal.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m9"
          },
          {
            "id": "tdd-diag-m9-5",
            "title": "Hot Journal Recovery Flow",
            "description": "Startup: check for .db-journal file \u2192 (exists?) validate header \u2192 restore pages \u2192 sync database \u2192 delete journal \u2192 proceed. Shows automatic recovery.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m9"
          },
          {
            "id": "tdd-diag-m9-6",
            "title": "fsync Ordering Guarantee",
            "description": "Timeline: T1 write journal \u2192 T2 fsync (durable) \u2192 T3 write database \u2192 T4 fsync (durable) \u2192 T5 delete journal. Crash at any point: before T2 (no recovery needed), after T2 (recoverable), after T4 (committed).",
            "type": "sequence",
            "anchor_target": "build-sqlite-m9"
          }
        ]
      },
      {
        "id": "build-sqlite-m10",
        "name": "WAL Mode",
        "description": "Implements Write-Ahead Logging for concurrent readers during writes. Writers append modified pages to WAL file (not main database). Readers check WAL for recent page versions before falling back to database. Snapshot isolation gives each reader a consistent point-in-time view. Checkpoint process copies WAL to database periodically.",
        "specs": {
          "inputs": "Page modifications, PRAGMA journal_mode=WAL",
          "outputs": "Concurrent read/write capability, WAL file, checkpointed database",
          "abstractions": "WAL file: header (magic, salts, checksums) + frames (page + header with cumulative checksum). WAL index: hash map page_id \u2192 frame offset. Snapshot: max_frame at transaction start. Checkpoint: copy WAL frames to database, reset WAL. Commit marker: frame with commit_size > 0.",
          "error_categories": [
            "WAL_CORRUPTION",
            "CHECKPOINT_FAILED",
            "WAL_WRITE_FAILED",
            "SNAPSHOT_TOO_OLD"
          ],
          "concurrency_model": "Multiple concurrent readers (snapshots), single writer (append-only), checkpoint coordinates with readers",
          "performance_targets": [
            "Read during write: no blocking",
            "WAL append: <1ms per page",
            "Checkpoint (1000 pages): <100ms",
            "Auto-checkpoint at 1000 pages"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "WAL file format and header",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "Frame writing with checksums",
            "estimated_hours": "1.5"
          },
          {
            "phase": 3,
            "name": "WAL index construction",
            "estimated_hours": "1.5"
          },
          {
            "phase": 4,
            "name": "Snapshot isolation for readers",
            "estimated_hours": "2"
          },
          {
            "phase": 5,
            "name": "Page read through WAL index",
            "estimated_hours": "1"
          },
          {
            "phase": 6,
            "name": "Write transaction to WAL",
            "estimated_hours": "1.5"
          },
          {
            "phase": 7,
            "name": "Checkpoint implementation",
            "estimated_hours": "2"
          },
          {
            "phase": 8,
            "name": "WAL recovery on startup",
            "estimated_hours": "1.5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m10-1",
            "title": "WAL vs Rollback Journal Architecture",
            "description": "Side-by-side: Rollback (modify database, journal originals, undo on crash) vs WAL (append to log, checkpoint later, redo on crash). Shows fundamental inversion.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m10"
          },
          {
            "id": "tdd-diag-m10-2",
            "title": "WAL File Structure",
            "description": "Header (32 bytes): magic, version, page_size, checkpoint_seq, salt1, salt2, checksums. Frames: [header (24 bytes) + page_data] repeated. Commit markers indicated.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m10"
          },
          {
            "id": "tdd-diag-m10-3",
            "title": "Snapshot Isolation Mechanism",
            "description": "Reader R1 starts at frame 100, sees frames 1-100. Writer W appends frames 101-150. Reader R2 starts at frame 150, sees frames 1-150. R1 still sees only 1-100. Shows temporal views.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m10"
          },
          {
            "id": "tdd-diag-m10-4",
            "title": "WAL Read Path",
            "description": "Page read: check WAL index for page_id \u2192 (found and frame \u2264 snapshot?) read from WAL \u2192 (not found or too new?) read from database. Shows fallback logic.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m10"
          },
          {
            "id": "tdd-diag-m10-5",
            "title": "Checkpoint Process",
            "description": "Steps: identify committed frames \u2192 wait for readers (FULL mode) \u2192 copy frames to database \u2192 sync database \u2192 update checkpoint marker \u2192 (TRUNCATE?) reset WAL. Shows coordination.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m10"
          },
          {
            "id": "tdd-diag-m10-6",
            "title": "Concurrent Reader/Writer Coordination",
            "description": "Writer: append frame \u2192 update index \u2192 release. Reader: capture snapshot \u2192 read pages (no locks). Checkpoint: wait for readers with old snapshots \u2192 proceed. Shows non-blocking reads.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m10"
          },
          {
            "id": "tdd-diag-m10-7",
            "title": "WAL Recovery Replay",
            "description": "Startup: open WAL \u2192 validate header \u2192 scan frames \u2192 find last commit marker \u2192 replay committed frames to database \u2192 sync \u2192 build index for remaining. Shows partial transaction handling.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m10"
          }
        ]
      },
      {
        "id": "build-sqlite-m11",
        "name": "Aggregate Functions & JOIN",
        "description": "Implements aggregate functions (COUNT, SUM, AVG, MIN, MAX) with correct NULL handling. GROUP BY partitions rows using hash-based aggregation. HAVING filters groups after aggregation. INNER JOIN combines tables via nested loop algorithm with optional index optimization.",
        "specs": {
          "inputs": "Queries with aggregates, GROUP BY, HAVING, JOIN clauses",
          "outputs": "Aggregated results, joined result sets",
          "abstractions": "Aggregate state: running accumulators (count, sum, min, max). Hash aggregation: group_key \u2192 aggregate_state map. NULL handling: COUNT(*) counts all, COUNT(col) excludes NULLs, SUM/AVG/MIN/MAX ignore NULLs. Nested loop join: for each outer row, scan/seek inner for matches.",
          "error_categories": [
            "AGGREGATE_TYPE_ERROR",
            "GROUP_BY_COLUMN_INVALID",
            "JOIN_CONDITION_INVALID"
          ],
          "concurrency_model": "Single-threaded execution, hash table for grouping",
          "performance_targets": [
            "Aggregate 100K rows in <50ms",
            "GROUP BY with 1000 groups in <100ms",
            "JOIN with index: O(N log M) for N outer, M inner rows"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Aggregate state structures",
            "estimated_hours": "1"
          },
          {
            "phase": 2,
            "name": "COUNT/SUM/AVG/MIN/MAX implementation",
            "estimated_hours": "2"
          },
          {
            "phase": 3,
            "name": "NULL handling in aggregates",
            "estimated_hours": "1"
          },
          {
            "phase": 4,
            "name": "Hash-based GROUP BY",
            "estimated_hours": "2"
          },
          {
            "phase": 5,
            "name": "HAVING clause filtering",
            "estimated_hours": "1"
          },
          {
            "phase": 6,
            "name": "Nested loop JOIN execution",
            "estimated_hours": "2"
          },
          {
            "phase": 7,
            "name": "JOIN with index optimization",
            "estimated_hours": "2"
          },
          {
            "phase": 8,
            "name": "JOIN + WHERE predicate pushdown",
            "estimated_hours": "1"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m11-1",
            "title": "Aggregate State Accumulation",
            "description": "For each row: update count, check NULL for col-based aggregates, update sum (if not NULL), update min/max (if not NULL and better). Shows incremental state changes.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-2",
            "title": "NULL Handling in Aggregates",
            "description": "Table with NULL values showing: COUNT(*)=5, COUNT(col)=3, SUM= sum of 3 non-NULL, AVG= sum/3, MIN/MAX from non-NULL only. Visualizes exclusion.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-3",
            "title": "Hash-Based GROUP BY",
            "description": "Input rows \u2192 extract group key \u2192 hash \u2192 lookup in hash table \u2192 (found?) update existing aggregate state : create new entry. Shows hash table structure.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-4",
            "title": "GROUP BY Execution Flow",
            "description": "Two-phase: Phase 1 scan table, build hash table of groups. Phase 2 iterate hash table, finalize aggregates, output results. Shows separation.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-5",
            "title": "HAVING vs WHERE Execution",
            "description": "WHERE filters rows before grouping (row-level). HAVING filters groups after aggregation (group-level). Shows execution order: scan \u2192 WHERE \u2192 GROUP BY \u2192 HAVING \u2192 output.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-6",
            "title": "Nested Loop Join Algorithm",
            "description": "For each outer row: for each inner row: evaluate condition \u2192 (match?) output combined row. Shows O(N\u00d7M) complexity and result row construction.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-7",
            "title": "Join with Index Optimization",
            "description": "For each outer row: extract join key \u2192 seek in inner index \u2192 (found?) fetch inner row by rowid \u2192 output combined. Shows O(N \u00d7 log M) with index.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-8",
            "title": "JOIN Bytecode Structure",
            "description": "OpenRead outer \u2192 Rewind outer \u2192 OUTER_LOOP: OpenRead inner \u2192 Rewind inner \u2192 INNER_LOOP: [compare keys] \u2192 (match?) ResultRow \u2192 Next inner \u2192 INNER_LOOP \u2192 Next outer \u2192 OUTER_LOOP \u2192 Halt.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m11"
          }
        ]
      }
    ]
  },
  "tdd_accumulated_md": "\n\n# TDD\n\nA complete embedded SQL database engine implementing SQLite's architecture: lexical analysis through recursive-descent parsing, bytecode compilation for a register-based virtual machine (VDBE), page-based B-tree/B+tree storage with buffer pool management, cost-based query optimization, and ACID transactions via rollback journal and write-ahead logging. The system demonstrates the fundamental patterns underlying all relational databases.\n\n\n\n<!-- TDD_MOD_ID: build-sqlite-m1 -->\n# SQL Tokenizer: Technical Design Specification\n## Module Charter\nThe SQL Tokenizer transforms a raw SQL character stream into a typed token sequence using a deterministic finite state machine. It recognizes SQL keywords (case-insensitively), string literals with escaped quote handling (`''` \u2192 `'`), numeric literals (integers and floats with scientific notation), operators (single and multi-character like `<=`, `<>`, `||`), identifiers (both plain and double-quoted), and comments (both `--` single-line and `/* */` multi-line). The tokenizer does NOT parse syntax\u2014that's the parser's job\u2014nor does it evaluate semantic meaning; it only classifies character sequences into token types.\n**Upstream dependencies**: None\u2014the tokenizer receives raw string input.\n**Downstream dependencies**: The SQL Parser consumes the token stream.\n**Invariants**:\n1. Every character in the input belongs to exactly one token (or is whitespace/comments that are consumed).\n2. Token positions (line, column) accurately reflect the source location where the token begins.\n3. Keywords are normalized to uppercase in the token value; identifiers preserve original case.\n4. String literal token values contain the *unescaped* content (the actual string, not the SQL representation).\n5. The tokenizer never modifies the input string; all token values are newly allocated.\n---\n## File Structure\nCreate files in this order:\n```\n1. src/tokenizer/token.h          -- Token type enum and Token struct definition\n2. src/tokenizer/token.c          -- Token constructor/destructor utilities\n3. src/tokenizer/keywords.h       -- Keyword list and lookup function declaration\n4. src/tokenizer/keywords.c       -- Sorted keyword array with binary search\n5. src/tokenizer/lexer.h          -- Lexer struct and public API declaration\n6. src/tokenizer/lexer.c          -- Core lexer implementation (state machine)\n7. src/tokenizer/lexer_string.c   -- String literal parsing state machine\n8. src/tokenizer/lexer_number.c   -- Numeric literal parsing\n9. src/tokenizer/lexer_identifier.c -- Identifier and keyword recognition\n10. src/tokenizer/lexer_operator.c -- Operator and punctuation handling\n11. src/tokenizer/lexer_comment.c -- Comment skipping logic\n12. tests/test_tokenizer.c        -- Test suite with 20+ SQL statements\n```\n---\n## Complete Data Model\n### Token Type Enumeration\n```c\n// File: src/tokenizer/token.h\ntypedef enum {\n    TOKEN_EOF = 0,           // End of input\n    TOKEN_ERROR,             // Lexical error (check error_message)\n    // Literals\n    TOKEN_INTEGER,           // 42, 0, 1234567890\n    TOKEN_FLOAT,             // 3.14, .5, 1e10, 2.5e-3\n    TOKEN_STRING,            // 'hello', 'it''s'\n    // Identifiers and Keywords\n    TOKEN_IDENTIFIER,        // users, my_table, \"column name\"\n    TOKEN_KEYWORD,           // SELECT, FROM, WHERE, etc.\n    // Operators (multi-char first for precedence in output)\n    TOKEN_OPERATOR,          // =, <, >, <=, >=, <>, !=, ||, +, -, *, /, %\n    // Punctuation\n    TOKEN_PUNCTUATION,       // (, ), ,, ;, .\n    // Special\n    TOKEN_NULL_LITERAL,      // NULL keyword treated as literal\n    TOKEN_BLOB_LITERAL,      // X'4A6F686E' (hex-encoded binary)\n} TokenType;\n```\n**Why each type exists**:\n- `TOKEN_EOF`: Signals end of stream to parser without special null checks.\n- `TOKEN_ERROR`: Carries error information without throwing; parser can attempt recovery.\n- `TOKEN_INTEGER` vs `TOKEN_FLOAT`: Parser needs to distinguish for type inference without re-scanning.\n- `TOKEN_STRING`: Contains unescaped value; original SQL representation is lost (by design).\n- `TOKEN_IDENTIFIER` vs `TOKEN_KEYWORD`: Parser needs to know if a word is reserved.\n- `TOKEN_OPERATOR`: Grouped together because all have same precedence within category.\n- `TOKEN_PUNCTUATION`: Separators that structure syntax but aren't operators.\n- `TOKEN_NULL_LITERAL`: SQL's NULL is a keyword but behaves as a literal in expressions.\n- `TOKEN_BLOB_LITERAL`: Special syntax `X'...'` for binary data.\n### Token Structure\n```c\n// File: src/tokenizer/token.h\ntypedef struct {\n    TokenType type;\n    char* value;           // Owned string (malloc'd). NULL for EOF.\n                           // Keywords: uppercase normalized\n                           // Identifiers: original case\n                           // Strings: unescaped content\n                           // Integers: decimal string representation\n                           // Floats: decimal string representation\n                           // Operators: the operator string\n                           // Punctuation: single character\n    int line;              // 1-based line number where token starts\n    int column;            // 1-based column number where token starts\n    int length;            // Number of characters in source (for error highlighting)\n} Token;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       4     type (enum, padded to 4)\n// 4       4     padding\n// 8       8     value (pointer)\n// 16      4     line\n// 20      4     column\n// 24      4     length\n// 28      4     padding\n// Total: 32 bytes\n```\n**Field justification**:\n- `type`: Required for parser dispatch.\n- `value`: String form needed for identifiers, literals. Stored as string (not union) for simplicity; parser converts as needed.\n- `line`, `column`: Required for error messages pointing to source location.\n- `length`: Enables highlighting the full token in error messages (not just start position).\n### Lexer State Structure\n```c\n// File: src/tokenizer/lexer.h\ntypedef struct {\n    const char* input;       // Input SQL string (not owned)\n    size_t input_length;     // Total length of input\n    size_t pos;              // Current byte position (0-based)\n    int line;                // Current line (1-based)\n    int column;              // Current column (1-based)\n    // Error state\n    char error_message[256]; // Last error message\n    int error_line;          // Line where error occurred\n    int error_column;        // Column where error occurred\n    // Lookahead cache (for peek functionality)\n    Token lookahead;         // Next token (if has_lookahead is true)\n    bool has_lookahead;\n} Lexer;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       8     input (pointer)\n// 8       8     input_length\n// 16      8     pos\n// 24      4     line\n// 28      4     column\n// 32      256   error_message\n// 288     4     error_line\n// 292     4     error_column\n// 296     32    lookahead (Token struct)\n// 328     1     has_lookahead\n// 329     7     padding\n// Total: 336 bytes\n```\n### Keyword Table\n```c\n// File: src/tokenizer/keywords.c\n// Sorted alphabetically for binary search\nstatic const char* const KEYWORDS[] = {\n    \"ADD\", \"ALL\", \"ALTER\", \"AND\", \"AS\", \"ASC\",\n    \"BETWEEN\", \"BY\",\n    \"CASE\", \"CHECK\", \"COLLATE\", \"COLUMN\", \"CONSTRAINT\", \"CREATE\", \"CROSS\",\n    \"DEFAULT\", \"DELETE\", \"DESC\", \"DISTINCT\", \"DROP\",\n    \"ELSE\", \"END\", \"ESCAPE\", \"EXCEPT\", \"EXISTS\",\n    \"FALSE\", \"FOREIGN\", \"FROM\", \"FULL\",\n    \"GROUP\",\n    \"HAVING\",\n    \"IN\", \"INDEX\", \"INNER\", \"INSERT\", \"INTERSECT\", \"INTO\", \"IS\",\n    \"JOIN\",\n    \"KEY\",\n    \"LEFT\", \"LIKE\", \"LIMIT\",\n    \"NOT\", \"NULL\",\n    \"ON\", \"OR\", \"ORDER\", \"OUTER\",\n    \"PRIMARY\",\n    \"REFERENCES\", \"RIGHT\",\n    \"SELECT\", \"SET\",\n    \"TABLE\", \"THEN\", \"TRUE\",\n    \"UNION\", \"UNIQUE\", \"UPDATE\", \"USING\",\n    \"VALUES\",\n    \"WHEN\", \"WHERE\",\n    // Total: ~60 keywords\n};\nstatic const int KEYWORD_COUNT = sizeof(KEYWORDS) / sizeof(KEYWORDS[0]);\n```\n\n![Tokenizer State Machine](./diagrams/tdd-diag-m1-1.svg)\n\n---\n## Interface Contracts\n### Token Lifecycle\n```c\n// File: src/tokenizer/token.c\n// Create a token with all fields\nToken token_create(TokenType type, const char* value, int line, int column, int length);\n// Create an error token\nToken token_create_error(const char* message, int line, int column);\n// Free a token's value (does not free Token struct itself)\nvoid token_free(Token* token);\n// Deep copy a token (value is newly allocated)\nToken token_copy(const Token* src);\n```\n**Constraints**:\n- `token_create`: `value` must not be NULL (except for EOF). String is copied.\n- `token_free`: Safe to call on token with NULL value. Idempotent.\n- `token_copy`: Returns token with independently allocated value.\n**Edge cases**:\n- Copying an EOF token: Returns token with NULL value.\n- Freeing a token twice: Second call is no-op.\n### Lexer Lifecycle\n```c\n// File: src/tokenizer/lexer.h\n// Initialize lexer with input string\nLexer* lexer_create(const char* input);\n// Free lexer and any cached lookahead token\nvoid lexer_free(Lexer* lexer);\n// Reset lexer to beginning of input\nvoid lexer_reset(Lexer* lexer);\n```\n**Constraints**:\n- `lexer_create`: `input` must be a valid null-terminated string. Lexer does NOT take ownership.\n- `lexer_free`: Safe to call with NULL pointer.\n- `lexer_reset`: Clears error state and lookahead cache.\n### Token Stream Access\n```c\n// File: src/tokenizer/lexer.h\n// Get next token (caller owns the returned token's value)\nToken lexer_next_token(Lexer* lexer);\n// Peek at next token without consuming (returns copy)\nToken lexer_peek_token(Lexer* lexer);\n// Check if at end of input\nbool lexer_is_eof(Lexer* lexer);\n// Check if last operation produced an error\nbool lexer_has_error(const Lexer* lexer);\n// Get error message (valid until next lexer operation or free)\nconst char* lexer_get_error(const Lexer* lexer);\n// Get error position\nvoid lexer_get_error_position(const Lexer* lexer, int* line, int* column);\n```\n**Return values**:\n- `lexer_next_token`: Returns next token. On error, returns `TOKEN_ERROR`. On EOF, returns `TOKEN_EOF`.\n- `lexer_peek_token`: Returns copy of next token without advancing. Safe to call repeatedly.\n- `lexer_is_eof`: Returns true if next token would be EOF.\n- `lexer_has_error`: Returns true if last tokenization encountered an error.\n**Error variants**:\n| Error | Detected By | Recovery | User-Visible? |\n|-------|-------------|----------|---------------|\n| `UNRECOGNIZED_CHARACTER` | Character doesn't match any token start | Return ERROR token, advance 1 char, continue | Yes, with position |\n| `UNTERMINATED_STRING` | EOF reached inside string literal | Return ERROR token, position at string start | Yes, with position |\n| `UNTERMINATED_QUOTED_IDENTIFIER` | EOF reached inside quoted identifier | Return ERROR token, position at quote start | Yes, with position |\n| `UNTERMINATED_COMMENT` | EOF reached inside multi-line comment | Return ERROR token, position at comment start | Yes, with position |\n| `INVALID_NUMBER_FORMAT` | Malformed numeric literal (e.g., `1e+` without exponent) | Return ERROR token at number start | Yes, with position |\n**Edge cases**:\n- Empty input: Returns single `TOKEN_EOF`.\n- Input is only whitespace: Returns `TOKEN_EOF`.\n- Input is only comments: Returns `TOKEN_EOF`.\n- Multiple consecutive errors: Each error produces separate ERROR token.\n- NULL input to `lexer_create`: Returns NULL (caller must check).\n---\n## Algorithm Specification\n### Main Tokenization Loop\n```\nALGORITHM: next_token\nINPUT: Lexer* lexer (current position in input)\nOUTPUT: Token (next token from input)\nINVARIANTS: \n  - lexer->pos is at start of next token (after whitespace/comments)\n  - lexer->line and lexer->column reflect current position\nPROCEDURE:\n1. skip_whitespace_and_comments(lexer)\n2. IF lexer->pos >= lexer->input_length THEN\n     RETURN token_create(TOKEN_EOF, NULL, lexer->line, lexer->column, 0)\n3. char c = lexer->input[lexer->pos]\n4. start_line = lexer->line\n   start_column = lexer->column\n5. SWITCH on c:\n   CASE c IN ('a'..'z', 'A'..'Z', '_'):\n     RETURN read_identifier_or_keyword(lexer, start_line, start_column)\n   CASE c IN ('0'..'9'):\n     RETURN read_number(lexer, start_line, start_column)\n   CASE '.':\n     IF next char is digit THEN\n       RETURN read_number(lexer, start_line, start_column)  // .5 case\n     ELSE\n       RETURN read_punctuation(lexer, start_line, start_column)\n   CASE '\\'':\n     RETURN read_string_literal(lexer, start_line, start_column)\n   CASE '\"':\n     RETURN read_quoted_identifier(lexer, start_line, start_column)\n   CASE 'X', 'x':\n     IF next char is '\\'' THEN\n       RETURN read_blob_literal(lexer, start_line, start_column)\n     ELSE\n       RETURN read_identifier_or_keyword(lexer, start_line, start_column)\n   CASE '-':\n     IF next char is '-' THEN\n       skip_line_comment(lexer)\n       GOTO step 1  // Recurse to get next token\n     ELSE\n       RETURN read_operator(lexer, start_line, start_column)\n   CASE '/':\n     IF next char is '*' THEN\n       result = skip_block_comment(lexer)\n       IF result IS ERROR THEN RETURN result\n       GOTO step 1\n     ELSE\n       RETURN read_operator(lexer, start_line, start_column)\n   CASE c IN ('=', '<', '>', '!', '|', '+', '*', '%'):\n     RETURN read_operator(lexer, start_line, start_column)\n   CASE c IN ('(', ')', ',', ';'):\n     RETURN read_punctuation(lexer, start_line, start_column)\n   DEFAULT:\n     advance(lexer)\n     RETURN token_create_error(\"unrecognized character\", start_line, start_column)\n```\n### Character Advancement with Position Tracking\n```\nALGORITHM: advance\nINPUT: Lexer* lexer\nOUTPUT: char (current character before advancing)\nSIDE EFFECTS: increments pos, updates line/column\nPROCEDURE:\n1. IF lexer->pos >= lexer->input_length THEN\n     RETURN '\\0'\n2. char c = lexer->input[lexer->pos]\n3. lexer->pos++\n4. IF c == '\\n' THEN\n     lexer->line++\n     lexer->column = 1\n   ELSE\n     lexer->column++\n5. RETURN c\n```\n### String Literal Parsing\n```\nALGORITHM: read_string_literal\nINPUT: Lexer* lexer, int start_line, int start_column\nOUTPUT: Token (TOKEN_STRING or TOKEN_ERROR)\nSTATE MACHINE STATES: IN_STRING, ESCAPE_QUOTE\nPROCEDURE:\n1. start_pos = lexer->pos\n2. advance(lexer)  // consume opening quote\n3. CREATE empty StringBuilder for result\n4. WHILE lexer->pos < lexer->input_length:\n   a. char c = advance(lexer)\n   b. IF c == '\\'' THEN\n        IF peek(lexer) == '\\'' THEN\n          // Escaped quote: '' -> '\n          advance(lexer)  // consume second quote\n          StringBuilder.append('\\'')\n        ELSE\n          // End of string\n          BREAK\n      ELSE\n        StringBuilder.append(c)\n5. IF lexer->pos >= lexer->input_length AND last char was not '\\'' THEN\n     RETURN token_create_error(\"unterminated string literal\", start_line, start_column)\n6. length = lexer->pos - start_pos\n7. value = StringBuilder.to_string()\n8. RETURN token_create(TOKEN_STRING, value, start_line, start_column, length)\n```\n**Edge cases**:\n- Empty string `''`: Returns `TOKEN_STRING` with empty value.\n- String with only escaped quotes `''''`: Returns `TOKEN_STRING` with value `'`.\n- Multi-line string `'line1\\nline2'`: Newline preserved in value.\n- String at EOF without closing quote: Error at opening quote position.\n\n![Token Data Structure](./diagrams/tdd-diag-m1-2.svg)\n\n### Numeric Literal Parsing\n```\nALGORITHM: read_number\nINPUT: Lexer* lexer, int start_line, int start_column\nOUTPUT: Token (TOKEN_INTEGER, TOKEN_FLOAT, or TOKEN_ERROR)\nGRAMMAR:\n  integer: [0-9]+\n  float:   [0-9]+ '.' [0-9]*\n         | [0-9]* '.' [0-9]+\n         | [0-9]+ ('e'|'E') ('+'|'-')? [0-9]+\n         | [0-9]+ '.' [0-9]* ('e'|'E') ('+'|'-')? [0-9]+\nPROCEDURE:\n1. start_pos = lexer->pos\n2. is_float = false\n3. // Read integer part (or handle leading dot)\n   IF current char == '.' THEN\n     is_float = true\n     advance(lexer)\n     IF !is_digit(peek(lexer)) THEN\n       RETURN token_create_error(\"expected digit after decimal point\", ...)\n     WHILE is_digit(peek(lexer)):\n       advance(lexer)\n   ELSE\n     WHILE is_digit(peek(lexer)):\n       advance(lexer)\n4. // Check for decimal point\n   IF peek(lexer) == '.' AND !is_float THEN\n     is_float = true\n     advance(lexer)\n     WHILE is_digit(peek(lexer)):\n       advance(lexer)\n5. // Check for exponent\n   IF peek(lexer) IN ('e', 'E') THEN\n     is_float = true\n     advance(lexer)\n     IF peek(lexer) IN ('+', '-') THEN\n       advance(lexer)\n     IF !is_digit(peek(lexer)) THEN\n       RETURN token_create_error(\"expected exponent digits\", ...)\n     WHILE is_digit(peek(lexer)):\n       advance(lexer)\n6. length = lexer->pos - start_pos\n7. value = substring(lexer->input, start_pos, length)\n8. type = is_float ? TOKEN_FLOAT : TOKEN_INTEGER\n9. RETURN token_create(type, value, start_line, start_column, length)\n```\n**Edge cases**:\n- `0`: Returns `TOKEN_INTEGER`.\n- `.5`: Returns `TOKEN_FLOAT`.\n- `5.`: Returns `TOKEN_FLOAT`.\n- `1e10`: Returns `TOKEN_FLOAT`.\n- `1e+10`: Returns `TOKEN_FLOAT`.\n- `1e-10`: Returns `TOKEN_FLOAT`.\n- `1e`: Error\u2014\"expected exponent digits\".\n- `1e+`: Error\u2014\"expected exponent digits\".\n- Negative numbers: NOT handled here. `-5` is `[OPERATOR: -] [INTEGER: 5]`. Parser interprets unary minus.\n### Keyword Recognition\n```\nALGORITHM: is_keyword\nINPUT: const char* text (null-terminated, will be uppercased for comparison)\nOUTPUT: bool (true if text is a keyword)\nPROCEDURE:\n1. text_upper = to_uppercase(text)  // Stack-allocated buffer\n2. lo = 0, hi = KEYWORD_COUNT - 1\n3. WHILE lo <= hi:\n   a. mid = (lo + hi) / 2\n   b. cmp = strcmp(text_upper, KEYWORDS[mid])\n   c. IF cmp == 0 THEN RETURN true\n   d. IF cmp < 0 THEN hi = mid - 1\n   e. ELSE lo = mid + 1\n4. RETURN false\nALGORITHM: read_identifier_or_keyword\nINPUT: Lexer* lexer, int start_line, int start_column\nOUTPUT: Token (TOKEN_IDENTIFIER or TOKEN_KEYWORD)\nPROCEDURE:\n1. start_pos = lexer->pos\n2. WHILE is_alnum(peek(lexer)) OR peek(lexer) == '_':\n     advance(lexer)\n3. length = lexer->pos - start_pos\n4. text = substring(lexer->input, start_pos, length)\n5. IF is_keyword(text) THEN\n     text_upper = to_uppercase(text)\n     free(text)\n     RETURN token_create(TOKEN_KEYWORD, text_upper, start_line, start_column, length)\n   ELSE\n     RETURN token_create(TOKEN_IDENTIFIER, text, start_line, start_column, length)\n```\n### Operator Recognition\n```\nALGORITHM: read_operator\nINPUT: Lexer* lexer, int start_line, int start_column\nOUTPUT: Token (TOKEN_OPERATOR)\nTWO_CHAR_OPERATORS = [\"<=\", \">=\", \"<>\", \"!=\", \"||\"]\nPROCEDURE:\n1. start_pos = lexer->pos\n2. first = advance(lexer)\n3. // Check for two-character operator\n   IF lexer->pos < lexer->input_length THEN\n     second = peek(lexer)\n     two_char = string(first, second)\n     IF two_char IN TWO_CHAR_OPERATORS THEN\n       advance(lexer)  // consume second character\n       length = 2\n       RETURN token_create(TOKEN_OPERATOR, two_char, start_line, start_column, length)\n4. // Single-character operator\n   length = 1\n   RETURN token_create(TOKEN_OPERATOR, string(first), start_line, start_column, length)\n```\n**Operator list**:\n- Single-char: `=`, `<`, `>`, `+`, `-`, `*`, `/`, `%`\n- Two-char: `<=`, `>=`, `<>`, `!=`, `||`\nNote: `<>` and `!=` are semantically equivalent (both mean \"not equal\"). Tokenizer preserves the distinction; parser or executor normalizes.\n### Comment Handling\n```\nALGORITHM: skip_line_comment\nINPUT: Lexer* lexer\nOUTPUT: void (lexer advanced past comment)\nPROCEDURE:\n1. advance(lexer)  // consume first '-'\n2. advance(lexer)  // consume second '-'\n3. WHILE lexer->pos < lexer->input_length AND peek(lexer) != '\\n':\n     advance(lexer)\n4. // Don't consume the newline (it's whitespace for next token)\nALGORITHM: skip_block_comment\nINPUT: Lexer* lexer\nOUTPUT: Token or NULL (NULL on success, ERROR token on unterminated)\nPROCEDURE:\n1. start_line = lexer->line\n   start_column = lexer->column\n2. advance(lexer)  // consume '/'\n3. advance(lexer)  // consume '*'\n4. WHILE lexer->pos < lexer->input_length - 1:\n   a. IF peek(lexer) == '*' AND peek_next(lexer) == '/' THEN\n        advance(lexer)  // consume '*'\n        advance(lexer)  // consume '/'\n        RETURN NULL  // Success\n   b. advance(lexer)\n5. // Reached EOF without closing\n   RETURN token_create_error(\"unterminated block comment\", start_line, start_column)\n```\n\n![Keyword Lookup Table](./diagrams/tdd-diag-m1-3.svg)\n\n---\n## State Machine\nThe lexer implements a deterministic finite state machine:\n```\nSTATES:\n  START           -- Initial state, classifying next character\n  IN_IDENTIFIER   -- Reading alphanumeric sequence\n  IN_NUMBER       -- Reading numeric literal\n  IN_STRING       -- Inside single-quoted string\n  IN_QUOTED_ID    -- Inside double-quoted identifier\n  IN_LINE_COMMENT -- After --, consuming until newline\n  IN_BLOCK_COMMENT -- Inside /* ... */\n  DONE            -- Token complete\nTRANSITIONS:\n  START --letter/underscore--> IN_IDENTIFIER\n  START --digit--> IN_NUMBER\n  START --'.' followed by digit--> IN_NUMBER\n  START --single quote--> IN_STRING\n  START --double quote--> IN_QUOTED_ID\n  START --'-' followed by '-'--> IN_LINE_COMMENT\n  START --'/' followed by '*'--> IN_BLOCK_COMMENT\n  START --operator char--> DONE (emit operator)\n  START --punctuation char--> DONE (emit punctuation)\n  START --whitespace--> START (skip)\n  START --EOF--> DONE (emit EOF)\n  START --other--> DONE (emit error)\n  IN_IDENTIFIER --alnum/underscore--> IN_IDENTIFIER\n  IN_IDENTIFIER --other--> DONE (emit identifier/keyword)\n  IN_NUMBER --digit--> IN_NUMBER\n  IN_NUMBER --'.' (first time)--> IN_NUMBER (mark as float)\n  IN_NUMBER --'e'/'E'--> IN_NUMBER (mark as float, expect exponent)\n  IN_NUMBER --other--> DONE (emit number)\n  IN_STRING --non-quote--> IN_STRING\n  IN_STRING --single quote, next is quote--> IN_STRING (escape)\n  IN_STRING --single quote, next not quote--> DONE (emit string)\n  IN_STRING --EOF--> DONE (emit error: unterminated)\n  IN_QUOTED_ID --non-quote--> IN_QUOTED_ID\n  IN_QUOTED_ID --double quote, next is quote--> IN_QUOTED_ID (escape)\n  IN_QUOTED_ID --double quote, next not quote--> DONE (emit identifier)\n  IN_QUOTED_ID --EOF--> DONE (emit error: unterminated)\n  IN_LINE_COMMENT --newline--> START\n  IN_LINE_COMMENT --other--> IN_LINE_COMMENT\n  IN_LINE_COMMENT --EOF--> START (emit EOF next)\n  IN_BLOCK_COMMENT --'*' followed by '/'--> START\n  IN_BLOCK_COMMENT --other--> IN_BLOCK_COMMENT\n  IN_BLOCK_COMMENT --EOF--> DONE (emit error: unterminated)\n```\nILLEGAL transitions: None\u2014all inputs have defined behavior. Invalid inputs produce ERROR tokens.\n---\n## Error Handling Matrix\n| Error | Detected By | Recovery | User-Visible? |\n|-------|-------------|----------|---------------|\n| `UNRECOGNIZED_CHARACTER` | START state, no valid transition | Advance 1 char, return ERROR token, continue tokenizing | Yes: \"unrecognized character '@' at line 3, column 15\" |\n| `UNTERMINATED_STRING` | IN_STRING state reaches EOF | Return ERROR token at string start position | Yes: \"unterminated string literal at line 1, column 20\" |\n| `UNTERMINATED_QUOTED_ID` | IN_QUOTED_ID state reaches EOF | Return ERROR token at quote start position | Yes: \"unterminated quoted identifier at line 2, column 5\" |\n| `UNTERMINATED_COMMENT` | IN_BLOCK_COMMENT state reaches EOF | Return ERROR token at comment start position | Yes: \"unterminated block comment at line 1, column 1\" |\n| `INVALID_NUMBER_FORMAT` | IN_NUMBER state, invalid exponent | Return ERROR token at number start position | Yes: \"invalid numeric literal at line 1, column 10\" |\n| `NULL_INPUT` | lexer_create receives NULL | Return NULL from lexer_create | Yes (caller must check) |\n| `MEMORY_ALLOCATION_FAILURE` | malloc returns NULL | Set error flag, return EOF token | Yes: \"memory allocation failed\" |\n**Recovery guarantees**:\n- After any error, lexer is at a valid position (not in the middle of a token).\n- Error tokens include position of where the error began, not where it was detected.\n- Tokenization continues after errors (doesn't abort entire process).\n---\n## Implementation Sequence with Checkpoints\n### Phase 1: Token Types and Lexer Struct (0.5 hours)\n**Files to create**: `token.h`, `token.c`, `lexer.h`\n**Implementation**:\n1. Define `TokenType` enum with all token types.\n2. Define `Token` struct with all fields.\n3. Implement `token_create`, `token_free`, `token_copy`.\n4. Define `Lexer` struct with all fields.\n5. Implement `lexer_create`, `lexer_free`, `lexer_reset`.\n**Checkpoint**: Compile successfully. Create and free a lexer without crashes.\n```bash\ngcc -c token.c lexer.c -o /dev/null\n# Test: lexer_create(\"SELECT 1\") returns non-NULL\n```\n### Phase 2: Keyword Recognition (0.5 hours)\n**Files to create**: `keywords.h`, `keywords.c`\n**Implementation**:\n1. Define sorted `KEYWORDS` array.\n2. Implement `is_keyword` with binary search.\n3. Implement `to_uppercase` utility.\n**Checkpoint**: Keyword lookup works correctly.\n```bash\n# Test: is_keyword(\"SELECT\") == true\n# Test: is_keyword(\"select\") == true  (case-insensitive)\n# Test: is_keyword(\"users\") == false\n```\n### Phase 3: String Literal Parsing (1 hour)\n**Files to create**: `lexer_string.c`\n**Implementation**:\n1. Implement `read_string_literal` state machine.\n2. Handle escaped quotes (`''` \u2192 `'`).\n3. Track position for error reporting.\n4. Implement `read_quoted_identifier` (similar logic, different token type).\n5. Implement `read_blob_literal` for `X'...'` syntax.\n**Checkpoint**: String literals tokenize correctly.\n```bash\n# Test: \"'hello'\" \u2192 TOKEN_STRING(\"hello\")\n# Test: \"'it''s'\" \u2192 TOKEN_STRING(\"it's\")\n# Test: \"''\" \u2192 TOKEN_STRING(\"\")\n# Test: \"'unterminated\" \u2192 TOKEN_ERROR\n```\n### Phase 4: Numeric Literal Parsing (0.5 hours)\n**Files to create**: `lexer_number.c`\n**Implementation**:\n1. Implement `read_number` with integer/float distinction.\n2. Handle decimal points.\n3. Handle scientific notation.\n4. Handle edge cases (`.5`, `5.`, `1e+10`).\n**Checkpoint**: Numbers tokenize correctly.\n```bash\n# Test: \"42\" \u2192 TOKEN_INTEGER(\"42\")\n# Test: \"3.14\" \u2192 TOKEN_FLOAT(\"3.14\")\n# Test: \".5\" \u2192 TOKEN_FLOAT(\".5\")\n# Test: \"1e10\" \u2192 TOKEN_FLOAT(\"1e10\")\n# Test: \"1e\" \u2192 TOKEN_ERROR\n```\n### Phase 5: Operator and Punctuation Tokenization (0.5 hours)\n**Files to create**: `lexer_operator.c`\n**Implementation**:\n1. Implement `read_operator` with two-character lookahead.\n2. Implement `read_punctuation` for single characters.\n3. Distinguish operators from punctuation in token type.\n**Checkpoint**: Operators tokenize correctly.\n```bash\n# Test: \"<=\" \u2192 TOKEN_OPERATOR(\"<=\")\n# Test: \"<>\" \u2192 TOKEN_OPERATOR(\"<>\")\n# Test: \"(\" \u2192 TOKEN_PUNCTUATION(\"(\")\n# Test: \";\" \u2192 TOKEN_PUNCTUATION(\";\")\n```\n### Phase 6: Comment Handling (0.5 hours)\n**Files to create**: `lexer_comment.c`\n**Implementation**:\n1. Implement `skip_line_comment`.\n2. Implement `skip_block_comment` with error handling.\n3. Integrate with main tokenization loop.\n**Checkpoint**: Comments are skipped correctly.\n```bash\n# Test: \"-- comment\\nSELECT\" \u2192 TOKEN_KEYWORD(\"SELECT\")\n# Test: \"/* comment */ SELECT\" \u2192 TOKEN_KEYWORD(\"SELECT\")\n# Test: \"/* unterminated\" \u2192 TOKEN_ERROR\n```\n### Phase 7: Error Reporting with Position Tracking (0.5 hours)\n**Files to create**: Complete `lexer.c`\n**Implementation**:\n1. Implement main `next_token` loop.\n2. Implement position tracking (`advance` function).\n3. Implement `peek_token` with lookahead cache.\n4. Ensure all error paths include position.\n**Checkpoint**: Complete tokenizer passes all tests.\n```bash\n# Run full test suite\n./test_tokenizer\n# All 20+ test cases pass\n```\n### Phase 8: Integration Testing (0.5 hours)\n**Implementation**:\n1. Test complex SQL statements.\n2. Test error recovery.\n3. Test boundary conditions.\n**Checkpoint**: Tokenizer handles real SQL.\n```bash\n# Test: \"SELECT * FROM users WHERE id = 42\" produces correct token sequence\n# Test: Error position is accurate\n```\n---\n## Test Specification\n### Token Structure Tests\n```c\nvoid test_token_create_and_free() {\n    Token t = token_create(TOKEN_INTEGER, \"42\", 1, 5, 2);\n    assert(t.type == TOKEN_INTEGER);\n    assert(strcmp(t.value, \"42\") == 0);\n    assert(t.line == 1);\n    assert(t.column == 5);\n    assert(t.length == 2);\n    token_free(&t);\n    // No crash = pass\n}\nvoid test_token_copy() {\n    Token t1 = token_create(TOKEN_STRING, \"hello\", 1, 1, 7);\n    Token t2 = token_copy(&t1);\n    assert(t2.value != t1.value);  // Different memory\n    assert(strcmp(t2.value, t1.value) == 0);  // Same content\n    token_free(&t1);\n    token_free(&t2);\n}\nvoid test_token_error() {\n    Token t = token_create_error(\"test error\", 3, 10);\n    assert(t.type == TOKEN_ERROR);\n    assert(strcmp(t.value, \"test error\") == 0);\n    assert(t.line == 3);\n    assert(t.column == 10);\n    token_free(&t);\n}\n```\n### Keyword Recognition Tests\n```c\nvoid test_keyword_uppercase() {\n    assert(is_keyword(\"SELECT\") == true);\n    assert(is_keyword(\"FROM\") == true);\n    assert(is_keyword(\"WHERE\") == true);\n}\nvoid test_keyword_case_insensitive() {\n    assert(is_keyword(\"select\") == true);\n    assert(is_keyword(\"Select\") == true);\n    assert(is_keyword(\"sElEcT\") == true);\n}\nvoid test_non_keyword() {\n    assert(is_keyword(\"users\") == false);\n    assert(is_keyword(\"my_table\") == false);\n    assert(is_keyword(\"column123\") == false);\n}\nvoid test_all_keywords() {\n    // Verify every keyword in KEYWORDS array is recognized\n    for (int i = 0; i < KEYWORD_COUNT; i++) {\n        assert(is_keyword(KEYWORDS[i]) == true);\n    }\n}\n```\n### String Literal Tests\n```c\nvoid test_simple_string() {\n    Lexer* lexer = lexer_create(\"'hello'\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_STRING);\n    assert(strcmp(t.value, \"hello\") == 0);\n    assert(t.line == 1);\n    assert(t.column == 1);\n    assert(t.length == 7);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_escaped_quote() {\n    Lexer* lexer = lexer_create(\"'it''s'\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_STRING);\n    assert(strcmp(t.value, \"it's\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_empty_string() {\n    Lexer* lexer = lexer_create(\"''\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_STRING);\n    assert(strcmp(t.value, \"\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_multiline_string() {\n    Lexer* lexer = lexer_create(\"'line1\\nline2'\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_STRING);\n    assert(strcmp(t.value, \"line1\\nline2\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_unterminated_string() {\n    Lexer* lexer = lexer_create(\"'unterminated\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_ERROR);\n    assert(t.line == 1);\n    assert(t.column == 1);  // Error at opening quote\n    token_free(&t);\n    lexer_free(lexer);\n}\n```\n### Numeric Literal Tests\n```c\nvoid test_integer() {\n    Lexer* lexer = lexer_create(\"42\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_INTEGER);\n    assert(strcmp(t.value, \"42\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_zero() {\n    Lexer* lexer = lexer_create(\"0\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_INTEGER);\n    assert(strcmp(t.value, \"0\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_float_decimal() {\n    Lexer* lexer = lexer_create(\"3.14\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_FLOAT);\n    assert(strcmp(t.value, \"3.14\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_float_leading_dot() {\n    Lexer* lexer = lexer_create(\".5\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_FLOAT);\n    assert(strcmp(t.value, \".5\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_float_trailing_dot() {\n    Lexer* lexer = lexer_create(\"5.\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_FLOAT);\n    assert(strcmp(t.value, \"5.\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_float_scientific() {\n    Lexer* lexer = lexer_create(\"1e10\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_FLOAT);\n    assert(strcmp(t.value, \"1e10\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_float_scientific_with_sign() {\n    Lexer* lexer = lexer_create(\"1.5e-3\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_FLOAT);\n    assert(strcmp(t.value, \"1.5e-3\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_invalid_exponent() {\n    Lexer* lexer = lexer_create(\"1e+\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_ERROR);\n    token_free(&t);\n    lexer_free(lexer);\n}\n```\n### Operator and Punctuation Tests\n```c\nvoid test_single_char_operators() {\n    const char* ops[] = {\"=\", \"<\", \">\", \"+\", \"-\", \"*\", \"/\", \"%\"};\n    for (int i = 0; i < 8; i++) {\n        Lexer* lexer = lexer_create(ops[i]);\n        Token t = lexer_next_token(lexer);\n        assert(t.type == TOKEN_OPERATOR);\n        assert(strcmp(t.value, ops[i]) == 0);\n        token_free(&t);\n        lexer_free(lexer);\n    }\n}\nvoid test_two_char_operators() {\n    const char* ops[] = {\"<=\", \">=\", \"<>\", \"!=\", \"||\"};\n    for (int i = 0; i < 5; i++) {\n        Lexer* lexer = lexer_create(ops[i]);\n        Token t = lexer_next_token(lexer);\n        assert(t.type == TOKEN_OPERATOR);\n        assert(strcmp(t.value, ops[i]) == 0);\n        assert(t.length == 2);\n        token_free(&t);\n        lexer_free(lexer);\n    }\n}\nvoid test_punctuation() {\n    const char* puncts[] = {\"(\", \")\", \",\", \";\", \".\"};\n    for (int i = 0; i < 5; i++) {\n        Lexer* lexer = lexer_create(puncts[i]);\n        Token t = lexer_next_token(lexer);\n        assert(t.type == TOKEN_PUNCTUATION);\n        assert(strcmp(t.value, puncts[i]) == 0);\n        token_free(&t);\n        lexer_free(lexer);\n    }\n}\n```\n### Identifier Tests\n```c\nvoid test_simple_identifier() {\n    Lexer* lexer = lexer_create(\"users\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_IDENTIFIER);\n    assert(strcmp(t.value, \"users\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_identifier_with_underscore() {\n    Lexer* lexer = lexer_create(\"my_table\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_IDENTIFIER);\n    assert(strcmp(t.value, \"my_table\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_quoted_identifier() {\n    Lexer* lexer = lexer_create(\"\\\"column name\\\"\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_IDENTIFIER);\n    assert(strcmp(t.value, \"column name\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_quoted_identifier_with_escape() {\n    Lexer* lexer = lexer_create(\"\\\"col\\\"\\\"name\\\"\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_IDENTIFIER);\n    assert(strcmp(t.value, \"col\\\"name\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\n```\n### Comment Tests\n```c\nvoid test_line_comment() {\n    Lexer* lexer = lexer_create(\"-- comment\\nSELECT\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_KEYWORD);\n    assert(strcmp(t.value, \"SELECT\") == 0);\n    assert(t.line == 2);  // After newline\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_block_comment() {\n    Lexer* lexer = lexer_create(\"/* comment */ SELECT\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_KEYWORD);\n    assert(strcmp(t.value, \"SELECT\") == 0);\n    token_free(&t);\n    lexer_free(lexer);\n}\nvoid test_unterminated_block_comment() {\n    Lexer* lexer = lexer_create(\"/* unterminated\");\n    Token t = lexer_next_token(lexer);\n    assert(t.type == TOKEN_ERROR);\n    token_free(&t);\n    lexer_free(lexer);\n}\n```\n### Complex Query Tests\n```c\nvoid test_select_statement() {\n    Lexer* lexer = lexer_create(\"SELECT * FROM users WHERE id = 42\");\n    Token tokens[10];\n    int count = 0;\n    while (true) {\n        Token t = lexer_next_token(lexer);\n        tokens[count++] = t;\n        if (t.type == TOKEN_EOF || t.type == TOKEN_ERROR) break;\n    }\n    assert(count == 10);  // 9 tokens + EOF\n    assert(tokens[0].type == TOKEN_KEYWORD && strcmp(tokens[0].value, \"SELECT\") == 0);\n    assert(tokens[1].type == TOKEN_OPERATOR && strcmp(tokens[1].value, \"*\") == 0);\n    assert(tokens[2].type == TOKEN_KEYWORD && strcmp(tokens[2].value, \"FROM\") == 0);\n    assert(tokens[3].type == TOKEN_IDENTIFIER && strcmp(tokens[3].value, \"users\") == 0);\n    assert(tokens[4].type == TOKEN_KEYWORD && strcmp(tokens[4].value, \"WHERE\") == 0);\n    assert(tokens[5].type == TOKEN_IDENTIFIER && strcmp(tokens[5].value, \"id\") == 0);\n    assert(tokens[6].type == TOKEN_OPERATOR && strcmp(tokens[6].value, \"=\") == 0);\n    assert(tokens[7].type == TOKEN_INTEGER && strcmp(tokens[7].value, \"42\") == 0);\n    assert(tokens[8].type == TOKEN_EOF);\n    for (int i = 0; i < count; i++) {\n        token_free(&tokens[i]);\n    }\n    lexer_free(lexer);\n}\nvoid test_insert_statement() {\n    Lexer* lexer = lexer_create(\"INSERT INTO users (id, name) VALUES (1, 'Alice')\");\n    // Verify token sequence...\n    lexer_free(lexer);\n}\nvoid test_complex_where() {\n    Lexer* lexer = lexer_create(\n        \"SELECT * FROM users WHERE age >= 18 AND status = 'active' OR premium = TRUE\"\n    );\n    // Verify token sequence including operators, strings, and keywords...\n    lexer_free(lexer);\n}\n```\n### Error Position Tests\n```c\nvoid test_error_position_multiline() {\n    Lexer* lexer = lexer_create(\"SELECT\\n  'unterminated\\nFROM\");\n    lexer_next_token(lexer);  // SELECT\n    lexer_next_token(lexer);  // newline\n    Token t = lexer_next_token(lexer);  // Error\n    assert(t.type == TOKEN_ERROR);\n    assert(t.line == 2);  // Line where string started\n    assert(t.column == 3);  // Column where string started\n    token_free(&t);\n    lexer_free(lexer);\n}\n```\n\n![String Literal Escape Handling](./diagrams/tdd-diag-m1-4.svg)\n\n---\n## Performance Targets\n| Operation | Target | How to Measure |\n|-----------|--------|----------------|\n| Tokenize 10KB SQL | < 1ms | `time ./tokenizer < 10kb_query.sql` |\n| Keyword lookup | O(log n) | Binary search on sorted array; benchmark with all keywords |\n| String allocation | Minimize | Profile with valgrind; avoid redundant copies |\n| Memory per token | 32 bytes | `sizeof(Token)` on 64-bit system |\n| Peak memory for 10KB input | < 50KB | All tokens + values in memory simultaneously |\n**Benchmark methodology**:\n```c\nvoid benchmark_tokenization() {\n    // Generate 10KB of SQL\n    char sql[10240];\n    generate_test_sql(sql, sizeof(sql));\n    clock_t start = clock();\n    Lexer* lexer = lexer_create(sql);\n    int token_count = 0;\n    while (true) {\n        Token t = lexer_next_token(lexer);\n        token_count++;\n        token_free(&t);\n        if (t.type == TOKEN_EOF) break;\n    }\n    clock_t end = clock();\n    double elapsed_ms = (double)(end - start) / CLOCKS_PER_SEC * 1000;\n    printf(\"Tokenized %d tokens in %.2f ms\\n\", token_count, elapsed_ms);\n    assert(elapsed_ms < 1.0);\n    lexer_free(lexer);\n}\n```\n---\n[[CRITERIA_JSON: {\"module_id\": \"build-sqlite-m1\", \"criteria\": [\"Tokenizer implements a finite state machine (FSM) processing input character-by-character with state transitions for START, IN_IDENTIFIER, IN_NUMBER, IN_STRING, IN_QUOTED_ID, IN_LINE_COMMENT, IN_BLOCK_COMMENT\", \"Token data structure includes type (TokenType enum), value (char* string), line (int, 1-based), column (int, 1-based), length (int, characters in source)\", \"All SQL keywords recognized case-insensitively using binary search on sorted keyword array; keywords normalized to uppercase in token value\", \"String literals enclosed in single quotes correctly handle escaped quotes ('' \u2192 ') and preserve internal content including whitespace and newlines\", \"Numeric literals classified as TOKEN_INTEGER (no decimal point or exponent) or TOKEN_FLOAT (contains decimal point and/or scientific notation)\", \"Float parsing handles all forms: 3.14, .5, 5., 1e10, 1.5e-3, 1e+10\", \"Invalid numeric literals (e.g., 1e without exponent) produce TOKEN_ERROR with position\", \"Operators tokenized including single-character (=, <, >, +, -, *, /, %) and two-character forms (<=, >=, <>, !=, ||)\", \"Two-character operators recognized by looking ahead before committing to single-character operator\", \"Punctuation characters (comma, parentheses, semicolon, period) tokenized as TOKEN_PUNCTUATION\", \"Quoted identifiers using double quotes support spaces and special characters with doubled quotes as escapes\", \"Single-line comments (--) consume characters until newline without producing tokens\", \"Multi-line comments (/* */) consume characters until */ without producing tokens\", \"Unterminated string literal reports error at opening quote position, not end of input\", \"Unterminated block comment reports error at /* position\", \"Error tokens include precise line and column position with descriptive message\", \"Unrecognized characters produce TOKEN_ERROR with position, tokenization continues\", \"Test suite includes at least 20 diverse SQL statements covering all token types\", \"Test suite validates complex multi-clause queries produce correct token sequence\", \"Test suite validates negative numbers are tokenized as [OPERATOR: -] [INTEGER: positive], not as negative literal\", \"Keyword lookup uses O(log n) binary search on alphabetically sorted keyword array\", \"Lexer provides peek_token for one-token lookahead without consuming\", \"Lexer tracks position (line, column) accurately across newlines and multi-line constructs\", \"Token values are newly allocated strings; caller owns and must free\", \"Empty input returns single TOKEN_EOF\", \"Input containing only whitespace/comments returns TOKEN_EOF\", \"NULL input to lexer_create returns NULL (caller must check)\"]}]\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m2 -->\n# SQL Parser (AST): Technical Design Specification\n## Module Charter\nThe SQL Parser transforms a token stream into an Abstract Syntax Tree (AST) using recursive descent parsing. It handles SELECT, INSERT, and CREATE TABLE statements with full clause support (WHERE, ORDER BY, LIMIT, column definitions, constraints). Expression parsing uses the precedence climbing algorithm to correctly handle operator binding: OR < AND < NOT < comparison (=, <, >, <=, >=, <>, !=) < additive (+, -) < multiplicative (*, /, %) < unary (-, NOT) < primary (literals, identifiers, parenthesized expressions). The parser does NOT validate semantics (table existence, column types) \u2014 that's the compiler's job. It only validates syntactic correctness and produces a tree structure that represents the SQL statement's grammatical form.\n**Upstream dependencies**: Token stream from the SQL Tokenizer (milestone 1).\n**Downstream dependencies**: Bytecode Compiler (milestone 3) consumes the AST.\n**Invariants**:\n1. Every valid token sequence matching the grammar produces exactly one AST; every invalid sequence produces exactly one error.\n2. All AST nodes include position information (line, column) from their first token for error propagation.\n3. The AST preserves the original syntactic structure \u2014 no semantic transformation occurs during parsing.\n4. Operator precedence is encoded in the AST shape: tighter-binding operators appear deeper in expression trees.\n5. NULL is represented as a LITERAL node, never as an IDENTIFIER node.\n6. Parser never modifies the token stream; it only consumes tokens sequentially.\n---\n## File Structure\nCreate files in this order:\n```\n1. src/parser/ast.h              -- AST node types and struct definitions\n2. src/parser/ast.c              -- AST node constructors and destructors\n3. src/parser/parser.h           -- Parser struct and public API\n4. src/parser/parser.c           -- Core parser with token consumption utilities\n5. src/parser/parse_select.c     -- SELECT statement parsing\n6. src/parser/parse_insert.c     -- INSERT statement parsing\n7. src/parser/parse_create.c     -- CREATE TABLE statement parsing\n8. src/parser/parse_expression.c -- Precedence climbing expression parser\n9. src/parser/precedence.h       -- Precedence levels and operator classification\n10. src/parser/precedence.c      -- Precedence lookup functions\n11. tests/test_parser_select.c   -- SELECT parsing tests\n12. tests/test_parser_insert.c   -- INSERT parsing tests\n13. tests/test_parser_create.c   -- CREATE TABLE parsing tests\n14. tests/test_parser_expression.c -- Expression parsing and precedence tests\n15. tests/test_parser_errors.c   -- Error handling and recovery tests\n```\n---\n## Complete Data Model\n### AST Node Type Enumeration\n```c\n// File: src/parser/ast.h\ntypedef enum {\n    // Statements\n    AST_SELECT_STMT,\n    AST_INSERT_STMT,\n    AST_CREATE_STMT,\n    AST_UPDATE_STMT,\n    AST_DELETE_STMT,\n    // Clauses\n    AST_COLUMN_LIST,        // List of column names or expressions\n    AST_FROM_CLAUSE,        // Table reference\n    AST_WHERE_CLAUSE,       // Filter expression\n    AST_ORDER_BY_CLAUSE,    // Sorting specification\n    AST_LIMIT_CLAUSE,       // Row limit\n    AST_VALUES_CLAUSE,      // INSERT values\n    // Expressions\n    AST_BINARY_EXPR,        // Binary operator: a OP b\n    AST_UNARY_EXPR,         // Unary operator: OP a\n    AST_LITERAL_EXPR,       // Literal value: 42, 'hello', NULL\n    AST_IDENTIFIER_EXPR,    // Column or table reference\n    AST_FUNCTION_CALL,      // Function call: COUNT(*), UPPER(name)\n    // Schema definitions\n    AST_COLUMN_DEF,         // Column definition in CREATE TABLE\n    AST_COLUMN_DEF_LIST,    // List of column definitions\n    AST_CONSTRAINT,         // Column constraint: PRIMARY KEY, NOT NULL, etc.\n    // Order by item\n    AST_ORDER_BY_ITEM,      // Single column in ORDER BY with ASC/DESC\n    // Special\n    AST_STAR_EXPR,          // SELECT * wildcard\n    AST_ERROR_NODE          // Placeholder for error recovery\n} ASTNodeType;\n```\n### Literal Value Types\n```c\n// File: src/parser/ast.h\ntypedef enum {\n    LITERAL_INTEGER,\n    LITERAL_FLOAT,\n    LITERAL_STRING,\n    LITERAL_NULL,\n    LITERAL_BOOLEAN,        // TRUE, FALSE\n    LITERAL_BLOB            // X'...'\n} LiteralType;\n```\n### Constraint Types\n```c\n// File: src/parser/ast.h\ntypedef enum {\n    CONSTRAINT_PRIMARY_KEY,\n    CONSTRAINT_NOT_NULL,\n    CONSTRAINT_UNIQUE,\n    CONSTRAINT_DEFAULT,\n    CONSTRAINT_CHECK,\n    CONSTRAINT_FOREIGN_KEY\n} ConstraintType;\n```\n### AST Node Structure\n```c\n// File: src/parser/ast.h\ntypedef struct ASTNode {\n    ASTNodeType type;\n    int line;               // 1-based line number of first token\n    int column;             // 1-based column number of first token\n    union {\n        // SELECT_STMT\n        struct {\n            struct ASTNode* columns;      // COLUMN_LIST node\n            char* table_name;             // FROM table\n            struct ASTNode* where;        // WHERE_CLAUSE or NULL\n            struct ASTNode* order_by;     // ORDER_BY_CLAUSE or NULL\n            struct ASTNode* limit;        // LIMIT_CLAUSE or NULL\n            bool distinct;                // SELECT DISTINCT\n        } select_stmt;\n        // INSERT_STMT\n        struct {\n            char* table_name;\n            struct ASTNode* columns;      // COLUMN_LIST or NULL (all columns)\n            struct ASTNode* values;       // VALUES_CLAUSE\n        } insert_stmt;\n        // CREATE_STMT\n        struct {\n            char* table_name;\n            struct ASTNode* column_defs;  // COLUMN_DEF_LIST\n            bool if_not_exists;           // CREATE TABLE IF NOT EXISTS\n        } create_stmt;\n        // UPDATE_STMT\n        struct {\n            char* table_name;\n            struct ASTNode* assignments;  // List of assignment nodes\n            struct ASTNode* where;        // WHERE_CLAUSE or NULL\n        } update_stmt;\n        // DELETE_STMT\n        struct {\n            char* table_name;\n            struct ASTNode* where;        // WHERE_CLAUSE or NULL\n        } delete_stmt;\n        // COLUMN_LIST\n        struct {\n            struct ASTNode** items;       // Array of expression nodes\n            int count;\n            int capacity;\n            bool star;                    // true if SELECT *\n        } column_list;\n        // FROM_CLAUSE\n        struct {\n            char* table_name;\n            char* alias;                  // Optional table alias\n        } from_clause;\n        // WHERE_CLAUSE\n        struct {\n            struct ASTNode* expression;\n        } where_clause;\n        // ORDER_BY_CLAUSE\n        struct {\n            struct ASTNode** items;       // Array of ORDER_BY_ITEM nodes\n            int count;\n        } order_by_clause;\n        // ORDER_BY_ITEM\n        struct {\n            struct ASTNode* expression;\n            bool ascending;               // true for ASC, false for DESC\n        } order_by_item;\n        // LIMIT_CLAUSE\n        struct {\n            struct ASTNode* limit_expr;\n            struct ASTNode* offset_expr;  // Optional OFFSET\n        } limit_clause;\n        // VALUES_CLAUSE\n        struct {\n            struct ASTNode** rows;        // Array of COLUMN_LIST nodes (each row)\n            int count;\n        } values_clause;\n        // BINARY_EXPR\n        struct {\n            struct ASTNode* left;\n            char* operator;               // \"=\", \"<\", \">\", \"AND\", \"OR\", etc.\n            struct ASTNode* right;\n        } binary_expr;\n        // UNARY_EXPR\n        struct {\n            char* operator;               // \"NOT\", \"-\"\n            struct ASTNode* operand;\n        } unary_expr;\n        // LITERAL_EXPR\n        struct {\n            LiteralType literal_type;\n            char* value;                  // String representation\n            // For integers: \"42\"\n            // For floats: \"3.14\"\n            // For strings: unescaped content\n            // For NULL: NULL\n            // For booleans: \"TRUE\" or \"FALSE\"\n        } literal_expr;\n        // IDENTIFIER_EXPR\n        struct {\n            char* name;                   // Column or table name\n            char* qualifier;              // Optional: table.column\n        } identifier_expr;\n        // FUNCTION_CALL\n        struct {\n            char* name;                   // Function name\n            struct ASTNode* args;         // COLUMN_LIST of arguments\n            bool distinct;                // COUNT(DISTINCT x)\n        } function_call;\n        // STAR_EXPR\n        struct {\n            char* qualifier;              // Optional: table.*\n        } star_expr;\n        // COLUMN_DEF\n        struct {\n            char* name;                   // Column name\n            char* data_type;              // \"INTEGER\", \"TEXT\", \"REAL\", \"BLOB\"\n            struct ASTNode** constraints; // Array of CONSTRAINT nodes\n            int constraint_count;\n        } column_def;\n        // COLUMN_DEF_LIST\n        struct {\n            struct ASTNode** defs;        // Array of COLUMN_DEF nodes\n            int count;\n        } column_def_list;\n        // CONSTRAINT\n        struct {\n            ConstraintType constraint_type;\n            struct ASTNode* expression;   // For DEFAULT, CHECK\n            char* ref_table;              // For FOREIGN KEY\n            char* ref_column;             // For FOREIGN KEY\n        } constraint;\n        // ERROR_NODE\n        struct {\n            char* message;\n        } error_node;\n    } data;\n} ASTNode;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       4     type (enum, 4-byte aligned)\n// 4       4     line\n// 8       4     column\n// 12      4     padding\n// 16      N     data union (largest variant ~64 bytes)\n// Total: ~80 bytes per node (varies by union content)\n```\n\n![AST Node Hierarchy](./diagrams/tdd-diag-m2-1.svg)\n\n### Parser State Structure\n```c\n// File: src/parser/parser.h\ntypedef struct {\n    Token* tokens;           // Array of tokens from tokenizer\n    int token_count;         // Total number of tokens\n    int current;             // Current position in token array (0-based)\n    // Error state\n    bool has_error;\n    char error_message[512];\n    int error_line;\n    int error_column;\n    // Recovery state\n    int sync_points_count;   // Number of synchronization points found\n    bool in_recovery;        // Currently skipping to sync point\n} Parser;\n// Memory layout (64-bit system):\n// Offset  Size   Field\n// 0       8      tokens (pointer)\n// 8       4      token_count\n// 12      4      current\n// 16      1      has_error\n// 17      1      in_recovery\n// 18      2      padding\n// 20      4      sync_points_count\n// 24      512    error_message\n// 536     4      error_line\n// 540     4      error_column\n// Total: 544 bytes\n```\n### Precedence Levels\n```c\n// File: src/parser/precedence.h\ntypedef enum {\n    PREC_LOWEST = 0,\n    PREC_OR,                 // OR\n    PREC_AND,                // AND\n    PREC_NOT,                // NOT (unary)\n    PREC_COMPARISON,         // =, <, >, <=, >=, <>, !=, IS, IN, LIKE, BETWEEN\n    PREC_ADDITIVE,           // +, -\n    PREC_MULTIPLICATIVE,     // *, /, %\n    PREC_UNARY,              // unary -, unary +\n    PREC_PRIMARY             // literals, identifiers, parenthesized\n} Precedence;\n// Higher number = tighter binding = evaluated first\n```\n---\n## Interface Contracts\n### AST Node Lifecycle\n```c\n// File: src/parser/ast.c\n// Create an AST node with type and position\nASTNode* ast_create(ASTNodeType type, int line, int column);\n// Constraints: type must be valid enum value, line >= 1, column >= 1\n// Returns: newly allocated node with zeroed data union\n// Caller owns the returned pointer\n// Free an AST node and all its children recursively\nvoid ast_free(ASTNode* node);\n// Constraints: node may be NULL (no-op if NULL)\n// Side effects: frees all owned strings and child nodes\n// Deep copy an AST node\nASTNode* ast_copy(const ASTNode* node);\n// Constraints: node must not be NULL\n// Returns: independently allocated copy with copied strings\n```\n### Specific Node Constructors\n```c\n// File: src/parser/ast.c\nASTNode* ast_create_select(int line, int column);\nASTNode* ast_create_insert(int line, int column);\nASTNode* ast_create_create(int line, int column);\nASTNode* ast_create_binary_expr(const char* op, ASTNode* left, ASTNode* right, \n                                  int line, int column);\nASTNode* ast_create_unary_expr(const char* op, ASTNode* operand, \n                                 int line, int column);\nASTNode* ast_create_literal(LiteralType type, const char* value, \n                             int line, int column);\nASTNode* ast_create_identifier(const char* name, int line, int column);\nASTNode* ast_create_column_def(const char* name, const char* data_type,\n                                int line, int column);\nASTNode* ast_create_constraint(ConstraintType type, int line, int column);\nASTNode* ast_create_column_list(int line, int column);\nASTNode* ast_create_error(const char* message, int line, int column);\n// Column list operations\nvoid ast_column_list_add(ASTNode* list, ASTNode* item);\n// Constraints: list must be AST_COLUMN_LIST type, item must not be NULL\n// Side effects: may reallocate list->data.column_list.items array\n// Constraint list operations\nvoid ast_column_def_add_constraint(ASTNode* col_def, ASTNode* constraint);\n// Constraints: col_def must be AST_COLUMN_DEF type\n```\n### Parser Lifecycle\n```c\n// File: src/parser/parser.h\n// Create parser with token array (takes ownership of tokens)\nParser* parser_create(Token* tokens, int token_count);\n// Constraints: tokens must be valid array, token_count >= 1 (at least EOF)\n// Returns: newly allocated parser\n// The parser DOES NOT take ownership of tokens array (caller must free)\n// Free parser (does NOT free tokens array)\nvoid parser_free(Parser* parser);\n// Constraints: parser may be NULL (no-op)\n// Parse a complete SQL statement\nASTNode* parser_parse(Parser* parser);\n// Returns: AST root node, or NULL on error\n// On error: check parser->error_message for details\n// Caller owns the returned AST node\n// Check for errors\nbool parser_has_error(const Parser* parser);\nconst char* parser_get_error(const Parser* parser);\nvoid parser_get_error_position(const Parser* parser, int* line, int* column);\n```\n### Token Consumption Utilities\n```c\n// File: src/parser/parser.c (internal interface)\n// Peek at current token without consuming\nToken parser_peek(const Parser* parser);\n// Returns: current token (never NULL; returns EOF token at end)\n// Peek at next token (one ahead)\nToken parser_peek_next(const Parser* parser);\n// Returns: next token, or EOF if at end\n// Consume current token and advance\nToken parser_consume(Parser* parser);\n// Returns: consumed token\n// Side effects: increments parser->current\n// Check if current token matches type and/or value\nbool parser_check_type(const Parser* parser, TokenType type);\nbool parser_check_keyword(const Parser* parser, const char* keyword);\nbool parser_check_punctuation(const Parser* parser, const char* punct);\nbool parser_check_operator(const Parser* parser, const char* op);\n// Consume if matches, otherwise error\nbool parser_match_keyword(Parser* parser, const char* keyword);\nbool parser_match_punctuation(Parser* parser, const char* punct);\nbool parser_match_operator(Parser* parser, const char* op);\n// Expect specific token (error if not found)\nToken parser_expect_keyword(Parser* parser, const char* keyword);\nToken parser_expect_punctuation(Parser* parser, const char* punct);\nToken parser_expect_identifier(Parser* parser);\n// On mismatch: sets error state, returns ERROR token\n// Recovery: does NOT consume token on error\n// Check for end of input\nbool parser_is_at_end(const Parser* parser);\n```\n### Statement Parsing Functions\n```c\n// File: src/parser/parse_select.c\nASTNode* parser_parse_select(Parser* parser);\n// Grammar: SELECT [DISTINCT] column_list FROM identifier [WHERE expr] [ORDER BY ...] [LIMIT ...]\n// Returns: AST_SELECT_STMT node or NULL on error\n// File: src/parser/parse_insert.c\nASTNode* parser_parse_insert(Parser* parser);\n// Grammar: INSERT INTO identifier [(column_list)] VALUES (value_list)\n// Returns: AST_INSERT_STMT node or NULL on error\n// File: src/parser/parse_create.c\nASTNode* parser_parse_create(Parser* parser);\n// Grammar: CREATE TABLE [IF NOT EXISTS] identifier (column_def_list)\n// Returns: AST_CREATE_STMT node or NULL on error\n```\n### Expression Parsing\n```c\n// File: src/parser/parse_expression.c\n// Parse expression with minimum precedence\nASTNode* parser_parse_expression(Parser* parser);\n// Equivalent to parser_parse_expression_with_precedence(parser, PREC_LOWEST)\n// Parse expression with specified minimum precedence\nASTNode* parser_parse_expression_with_precedence(Parser* parser, Precedence min_prec);\n// Uses precedence climbing algorithm\n// Returns: expression AST node or NULL on error\n// Parse primary expression (literals, identifiers, parenthesized)\nASTNode* parser_parse_primary(Parser* parser);\n// Returns: AST_LITERAL_EXPR, AST_IDENTIFIER_EXPR, or parenthesized expression\n```\n---\n## Algorithm Specification\n### Main Parse Loop\n```\nALGORITHM: parser_parse\nINPUT: Parser* parser (positioned at start of statement)\nOUTPUT: ASTNode* (root of statement AST) or NULL on error\nINVARIANTS:\n  - parser->current points to first token of statement\n  - On return, parser->current points past last consumed token\n  - On error, parser->has_error is true, parser->error_message is set\nPROCEDURE:\n1. IF parser_has_error(parser) THEN RETURN NULL\n2. Token t = parser_peek(parser)\n3. IF t.type == TOKEN_EOF THEN\n     parser_set_error(parser, \"unexpected end of input\", t.line, t.column)\n     RETURN NULL\n4. SWITCH on t.type:\n   CASE TOKEN_KEYWORD:\n     IF t.value == \"SELECT\" THEN\n       RETURN parser_parse_select(parser)\n     ELSE IF t.value == \"INSERT\" THEN\n       RETURN parser_parse_insert(parser)\n     ELSE IF t.value == \"CREATE\" THEN\n       RETURN parser_parse_create(parser)\n     ELSE IF t.value == \"UPDATE\" THEN\n       RETURN parser_parse_update(parser)\n     ELSE IF t.value == \"DELETE\" THEN\n       RETURN parser_parse_delete(parser)\n     ELSE\n       parser_set_error(parser, \"unexpected keyword: %s\", t.line, t.column, t.value)\n       RETURN NULL\n   DEFAULT:\n     parser_set_error(parser, \"expected statement\", t.line, t.column)\n     RETURN NULL\n```\n### SELECT Statement Parsing\n```\nALGORITHM: parser_parse_select\nINPUT: Parser* parser (current token is SELECT keyword)\nOUTPUT: ASTNode* (AST_SELECT_STMT) or NULL on error\nGRAMMAR:\n  select_stmt ::= SELECT [DISTINCT] column_list \n                   FROM identifier \n                   [WHERE expression]\n                   [ORDER BY order_by_list]\n                   [LIMIT expression [OFFSET expression]]\nPROCEDURE:\n1. start_line = parser_peek(parser).line\n   start_column = parser_peek(parser).column\n2. parser_consume(parser)  // consume SELECT\n3. ASTNode* select = ast_create_select(start_line, start_column)\n4. // Check for DISTINCT\n   IF parser_match_keyword(parser, \"DISTINCT\") THEN\n     select->data.select_stmt.distinct = true\n   ELSE\n     select->data.select_stmt.distinct = false\n5. // Parse column list\n   select->data.select_stmt.columns = parser_parse_column_list(parser)\n   IF select->data.select_stmt.columns == NULL THEN\n     ast_free(select)\n     RETURN NULL\n6. // Parse FROM clause\n   IF NOT parser_match_keyword(parser, \"FROM\") THEN\n     parser_set_error(parser, \"expected FROM after column list\", ...)\n     ast_free(select)\n     RETURN NULL\n   Token table = parser_expect_identifier(parser)\n   IF table.type == TOKEN_ERROR THEN\n     ast_free(select)\n     RETURN NULL\n   select->data.select_stmt.table_name = strdup(table.value)\n7. // Optional WHERE\n   IF parser_check_keyword(parser, \"WHERE\") THEN\n     parser_consume(parser)  // consume WHERE\n     select->data.select_stmt.where = ast_create(AST_WHERE_CLAUSE, ...)\n     select->data.select_stmt.where->data.where_clause.expression = \n         parser_parse_expression(parser)\n     IF select->data.select_stmt.where->data.where_clause.expression == NULL THEN\n       ast_free(select)\n       RETURN NULL\n8. // Optional ORDER BY\n   IF parser_check_keyword(parser, \"ORDER\") THEN\n     select->data.select_stmt.order_by = parser_parse_order_by(parser)\n     IF select->data.select_stmt.order_by == NULL THEN\n       ast_free(select)\n       RETURN NULL\n9. // Optional LIMIT\n   IF parser_check_keyword(parser, \"LIMIT\") THEN\n     select->data.select_stmt.limit = parser_parse_limit(parser)\n     IF select->data.select_stmt.limit == NULL THEN\n       ast_free(select)\n       RETURN NULL\n10. RETURN select\n```\n### Column List Parsing\n```\nALGORITHM: parser_parse_column_list\nINPUT: Parser* parser (positioned at first column expression)\nOUTPUT: ASTNode* (AST_COLUMN_LIST) or NULL on error\nGRAMMAR:\n  column_list ::= '*' | expression (',' expression)*\nPROCEDURE:\n1. start_line = parser_peek(parser).line\n   start_column = parser_peek(parser).column\n2. ASTNode* list = ast_create_column_list(start_line, start_column)\n3. // Check for star\n   IF parser_check_operator(parser, \"*\") THEN\n     parser_consume(parser)\n     list->data.column_list.star = true\n     RETURN list\n4. list->data.column_list.star = false\n5. // Parse first expression\n   ASTNode* expr = parser_parse_expression(parser)\n   IF expr == NULL THEN\n     ast_free(list)\n     RETURN NULL\n   ast_column_list_add(list, expr)\n6. // Parse additional columns\n   WHILE parser_match_punctuation(parser, \",\") DO\n     expr = parser_parse_expression(parser)\n     IF expr == NULL THEN\n       ast_free(list)\n       RETURN NULL\n     ast_column_list_add(list, expr)\n7. RETURN list\n```\n\n![Expression Precedence Tree](./diagrams/tdd-diag-m2-2.svg)\n\n### Precedence Climbing Expression Parser\n```\nALGORITHM: parser_parse_expression_with_precedence\nINPUT: Parser* parser, Precedence min_prec (minimum precedence to continue)\nOUTPUT: ASTNode* (expression tree) or NULL on error\nCORE INSIGHT: Precedence determines tree depth. Tighter binding = deeper.\nPROCEDURE:\n1. // Parse left operand (primary or unary)\n   ASTNode* left = parser_parse_unary(parser)\n   IF left == NULL THEN RETURN NULL\n2. // Loop while next operator has sufficient precedence\n   WHILE true DO\n   a. Token op_token = parser_peek(parser)\n   b. IF op_token.type != TOKEN_OPERATOR AND \n        NOT is_keyword_operator(op_token.value) THEN\n        BREAK  // Not an operator, stop\n   c. Precedence op_prec = get_operator_precedence(op_token.value)\n   d. IF op_prec < min_prec THEN\n        BREAK  // Operator too weak, let caller handle it\n   e. // For left-associative operators, use > min_prec for right side\n      // For right-associative operators, use >= min_prec\n      // All SQL binary operators are left-associative\n      Precedence next_min_prec = op_prec + 1\n   f. parser_consume(parser)  // consume operator\n   g. // Parse right operand with higher precedence\n      ASTNode* right = parser_parse_expression_with_precedence(parser, next_min_prec)\n      IF right == NULL THEN\n        ast_free(left)\n        RETURN NULL\n   h. // Build binary expression node\n      ASTNode* binary = ast_create_binary_expr(op_token.value, left, right,\n                                                left->line, left->column)\n      left = binary  // Result becomes new left operand\n3. RETURN left\nALGORITHM: parser_parse_unary\nINPUT: Parser* parser\nOUTPUT: ASTNode* (unary expression or primary) or NULL on error\nPROCEDURE:\n1. Token t = parser_peek(parser)\n2. // Handle NOT\n   IF parser_match_keyword(parser, \"NOT\") THEN\n     ASTNode* operand = parser_parse_unary(parser)  // NOT is right-associative\n     IF operand == NULL THEN RETURN NULL\n     RETURN ast_create_unary_expr(\"NOT\", operand, t.line, t.column)\n3. // Handle unary minus\n   IF parser_check_operator(parser, \"-\") THEN\n     parser_consume(parser)\n     ASTNode* operand = parser_parse_unary(parser)\n     IF operand == NULL THEN RETURN NULL\n     RETURN ast_create_unary_expr(\"-\", operand, t.line, t.column)\n4. // Handle unary plus (no-op, just parse operand)\n   IF parser_check_operator(parser, \"+\") THEN\n     parser_consume(parser)\n     RETURN parser_parse_unary(parser)\n5. // Not a unary operator, parse primary\n   RETURN parser_parse_primary(parser)\nALGORITHM: parser_parse_primary\nINPUT: Parser* parser\nOUTPUT: ASTNode* (literal, identifier, or parenthesized expression) or NULL on error\nPROCEDURE:\n1. Token t = parser_peek(parser)\n2. // Parenthesized expression\n   IF parser_match_punctuation(parser, \"(\") THEN\n     ASTNode* expr = parser_parse_expression(parser)\n     IF expr == NULL THEN RETURN NULL\n     IF NOT parser_match_punctuation(parser, \")\") THEN\n       parser_set_error(parser, \"expected ')' after expression\", ...)\n       ast_free(expr)\n       RETURN NULL\n     RETURN expr\n3. // Integer literal\n   IF t.type == TOKEN_INTEGER THEN\n     parser_consume(parser)\n     RETURN ast_create_literal(LITERAL_INTEGER, t.value, t.line, t.column)\n4. // Float literal\n   IF t.type == TOKEN_FLOAT THEN\n     parser_consume(parser)\n     RETURN ast_create_literal(LITERAL_FLOAT, t.value, t.line, t.column)\n5. // String literal\n   IF t.type == TOKEN_STRING THEN\n     parser_consume(parser)\n     RETURN ast_create_literal(LITERAL_STRING, t.value, t.line, t.column)\n6. // NULL literal\n   IF parser_match_keyword(parser, \"NULL\") THEN\n     RETURN ast_create_literal(LITERAL_NULL, \"NULL\", t.line, t.column)\n7. // TRUE/FALSE literals\n   IF parser_match_keyword(parser, \"TRUE\") THEN\n     RETURN ast_create_literal(LITERAL_BOOLEAN, \"TRUE\", t.line, t.column)\n   IF parser_match_keyword(parser, \"FALSE\") THEN\n     RETURN ast_create_literal(LITERAL_BOOLEAN, \"FALSE\", t.line, t.column)\n8. // Identifier (column reference)\n   IF t.type == TOKEN_IDENTIFIER THEN\n     parser_consume(parser)\n     RETURN ast_create_identifier(t.value, t.line, t.column)\n9. // Error: unexpected token\n   parser_set_error(parser, \"expected expression\", t.line, t.column)\n   RETURN NULL\n```\n### Precedence Lookup\n```\nALGORITHM: get_operator_precedence\nINPUT: const char* operator\nOUTPUT: Precedence level\nPROCEDURE:\n1. IF operator == \"OR\" THEN RETURN PREC_OR\n2. IF operator == \"AND\" THEN RETURN PREC_AND\n3. IF operator IN (\"=\", \"<\", \">\", \"<=\", \">=\", \"<>\", \"!=\", \n                   \"IS\", \"IN\", \"LIKE\", \"BETWEEN\") THEN\n     RETURN PREC_COMPARISON\n4. IF operator IN (\"+\", \"-\") THEN RETURN PREC_ADDITIVE\n5. IF operator IN (\"*\", \"/\", \"%\") THEN RETURN PREC_MULTIPLICATIVE\n6. RETURN PREC_PRIMARY  // Unknown operator (should not happen)\nHELPER: is_keyword_operator\nINPUT: const char* keyword\nOUTPUT: bool (true if keyword is also an operator)\nPROCEDURE:\n1. RETURN keyword IN (\"AND\", \"OR\", \"NOT\", \"IS\", \"IN\", \"LIKE\", \"BETWEEN\")\n```\n\n![Precedence Climbing Algorithm](./diagrams/tdd-diag-m2-3.svg)\n\n### INSERT Statement Parsing\n```\nALGORITHM: parser_parse_insert\nINPUT: Parser* parser (current token is INSERT keyword)\nOUTPUT: ASTNode* (AST_INSERT_STMT) or NULL on error\nGRAMMAR:\n  insert_stmt ::= INSERT INTO identifier [(column_list)] VALUES (value_list)\nPROCEDURE:\n1. start_line = parser_peek(parser).line\n   start_column = parser_peek(parser).column\n2. parser_consume(parser)  // consume INSERT\n3. IF NOT parser_match_keyword(parser, \"INTO\") THEN\n     parser_set_error(parser, \"expected INTO after INSERT\", ...)\n     RETURN NULL\n4. Token table = parser_expect_identifier(parser)\n   IF table.type == TOKEN_ERROR THEN RETURN NULL\n5. ASTNode* insert = ast_create_insert(start_line, start_column)\n   insert->data.insert_stmt.table_name = strdup(table.value)\n6. // Optional column list\n   IF parser_match_punctuation(parser, \"(\") THEN\n     insert->data.insert_stmt.columns = parser_parse_column_list(parser)\n     IF insert->data.insert_stmt.columns == NULL THEN\n       ast_free(insert)\n       RETURN NULL\n     IF NOT parser_match_punctuation(parser, \")\") THEN\n       parser_set_error(parser, \"expected ')' after column list\", ...)\n       ast_free(insert)\n       RETURN NULL\n   ELSE\n     insert->data.insert_stmt.columns = NULL\n7. // VALUES clause\n   IF NOT parser_match_keyword(parser, \"VALUES\") THEN\n     parser_set_error(parser, \"expected VALUES\", ...)\n     ast_free(insert)\n     RETURN NULL\n8. insert->data.insert_stmt.values = parser_parse_values_clause(parser)\n   IF insert->data.insert_stmt.values == NULL THEN\n     ast_free(insert)\n     RETURN NULL\n9. RETURN insert\nALGORITHM: parser_parse_values_clause\nINPUT: Parser* parser\nOUTPUT: ASTNode* (AST_VALUES_CLAUSE) or NULL on error\nGRAMMAR:\n  values_clause ::= '(' expression (',' expression)* ')'\n                   (',' '(' expression (',' expression)* ')')*\nPROCEDURE:\n1. start_line = parser_peek(parser).line\n   start_column = parser_peek(parser).column\n2. ASTNode* values = ast_create(AST_VALUES_CLAUSE, start_line, start_column)\n   values->data.values_clause.rows = malloc(INITIAL_CAPACITY * sizeof(ASTNode*))\n   values->data.values_clause.count = 0\n3. // Parse first row\n   IF NOT parser_match_punctuation(parser, \"(\") THEN\n     parser_set_error(parser, \"expected '(' before values\", ...)\n     ast_free(values)\n     RETURN NULL\n   ASTNode* row = parser_parse_column_list(parser)  // Reuse column list for values\n   IF row == NULL THEN\n     ast_free(values)\n     RETURN NULL\n   values->data.values_clause.rows[values->data.values_clause.count++] = row\n   IF NOT parser_match_punctuation(parser, \")\") THEN\n     parser_set_error(parser, \"expected ')' after values\", ...)\n     ast_free(values)\n     RETURN NULL\n4. // Parse additional rows\n   WHILE parser_match_punctuation(parser, \",\") DO\n     IF NOT parser_match_punctuation(parser, \"(\") THEN\n       parser_set_error(parser, \"expected '(' before values\", ...)\n       ast_free(values)\n       RETURN NULL\n     row = parser_parse_column_list(parser)\n     IF row == NULL THEN\n       ast_free(values)\n       RETURN NULL\n     values->data.values_clause.rows[values->data.values_clause.count++] = row\n     IF NOT parser_match_punctuation(parser, \")\") THEN\n       parser_set_error(parser, \"expected ')' after values\", ...)\n       ast_free(values)\n       RETURN NULL\n5. RETURN values\n```\n### CREATE TABLE Parsing\n```\nALGORITHM: parser_parse_create\nINPUT: Parser* parser (current token is CREATE keyword)\nOUTPUT: ASTNode* (AST_CREATE_STMT) or NULL on error\nGRAMMAR:\n  create_stmt ::= CREATE TABLE [IF NOT EXISTS] identifier '(' column_def_list ')'\nPROCEDURE:\n1. start_line = parser_peek(parser).line\n   start_column = parser_peek(parser).column\n2. parser_consume(parser)  // consume CREATE\n3. IF NOT parser_match_keyword(parser, \"TABLE\") THEN\n     parser_set_error(parser, \"expected TABLE after CREATE\", ...)\n     RETURN NULL\n4. ASTNode* create = ast_create_create(start_line, start_column)\n5. // Check for IF NOT EXISTS\n   IF parser_match_keyword(parser, \"IF\") THEN\n     IF NOT parser_match_keyword(parser, \"NOT\") THEN\n       parser_set_error(parser, \"expected NOT after IF\", ...)\n       ast_free(create)\n       RETURN NULL\n     IF NOT parser_match_keyword(parser, \"EXISTS\") THEN\n       parser_set_error(parser, \"expected EXISTS after NOT\", ...)\n       ast_free(create)\n       RETURN NULL\n     create->data.create_stmt.if_not_exists = true\n   ELSE\n     create->data.create_stmt.if_not_exists = false\n6. // Table name\n   Token table = parser_expect_identifier(parser)\n   IF table.type == TOKEN_ERROR THEN\n     ast_free(create)\n     RETURN NULL\n   create->data.create_stmt.table_name = strdup(table.value)\n7. // Column definitions\n   IF NOT parser_match_punctuation(parser, \"(\") THEN\n     parser_set_error(parser, \"expected '(' after table name\", ...)\n     ast_free(create)\n     RETURN NULL\n8. create->data.create_stmt.column_defs = parser_parse_column_def_list(parser)\n   IF create->data.create_stmt.column_defs == NULL THEN\n     ast_free(create)\n     RETURN NULL\n9. IF NOT parser_match_punctuation(parser, \")\") THEN\n     parser_set_error(parser, \"expected ')' after column definitions\", ...)\n     ast_free(create)\n     RETURN NULL\n10. RETURN create\nALGORITHM: parser_parse_column_def_list\nINPUT: Parser* parser\nOUTPUT: ASTNode* (AST_COLUMN_DEF_LIST) or NULL on error\nGRAMMAR:\n  column_def_list ::= column_def (',' column_def)*\nPROCEDURE:\n1. start_line = parser_peek(parser).line\n   start_column = parser_peek(parser).column\n2. ASTNode* list = ast_create(AST_COLUMN_DEF_LIST, start_line, start_column)\n   list->data.column_def_list.defs = malloc(INITIAL_CAPACITY * sizeof(ASTNode*))\n   list->data.column_def_list.count = 0\n3. // Parse first column definition\n   ASTNode* col_def = parser_parse_column_def(parser)\n   IF col_def == NULL THEN\n     ast_free(list)\n     RETURN NULL\n   list->data.column_def_list.defs[list->data.column_def_list.count++] = col_def\n4. // Parse additional columns\n   WHILE parser_match_punctuation(parser, \",\") DO\n     col_def = parser_parse_column_def(parser)\n     IF col_def == NULL THEN\n       ast_free(list)\n       RETURN NULL\n     list->data.column_def_list.defs[list->data.column_def_list.count++] = col_def\n5. RETURN list\nALGORITHM: parser_parse_column_def\nINPUT: Parser* parser\nOUTPUT: ASTNode* (AST_COLUMN_DEF) or NULL on error\nGRAMMAR:\n  column_def ::= identifier data_type [constraint]*\n  data_type ::= \"INTEGER\" | \"TEXT\" | \"REAL\" | \"BLOB\"\n  constraint ::= \"PRIMARY\" \"KEY\" | \"NOT\" \"NULL\" | \"UNIQUE\" | \n                 \"DEFAULT\" literal | \"CHECK\" '(' expression ')'\nPROCEDURE:\n1. Token name = parser_expect_identifier(parser)\n   IF name.type == TOKEN_ERROR THEN RETURN NULL\n2. Token type = parser_expect_data_type(parser)\n   IF type.type == TOKEN_ERROR THEN RETURN NULL\n3. ASTNode* col_def = ast_create_column_def(name.value, type.value, \n                                             name.line, name.column)\n4. // Parse constraints\n   WHILE true DO\n     ASTNode* constraint = NULL\n     IF parser_check_keyword(parser, \"PRIMARY\") THEN\n       constraint = parser_parse_primary_key_constraint(parser)\n     ELSE IF parser_check_keyword(parser, \"NOT\") THEN\n       constraint = parser_parse_not_null_constraint(parser)\n     ELSE IF parser_check_keyword(parser, \"UNIQUE\") THEN\n       constraint = parser_parse_unique_constraint(parser)\n     ELSE IF parser_check_keyword(parser, \"DEFAULT\") THEN\n       constraint = parser_parse_default_constraint(parser)\n     ELSE IF parser_check_keyword(parser, \"CHECK\") THEN\n       constraint = parser_parse_check_constraint(parser)\n     ELSE\n       BREAK  // No more constraints\n     IF constraint == NULL THEN\n       ast_free(col_def)\n       RETURN NULL\n     ast_column_def_add_constraint(col_def, constraint)\n5. RETURN col_def\n```\n{{DIAGRAM:tdd-diag-m2-4}}\n---\n## State Machine\nThe parser has implicit states based on what it's currently parsing:\n```\nSTATES (implicit in call stack):\n  PARSE_STATEMENT    -- Determining statement type\n  PARSE_SELECT       -- Inside SELECT statement\n  PARSE_INSERT       -- Inside INSERT statement\n  PARSE_CREATE       -- Inside CREATE TABLE statement\n  PARSE_EXPRESSION   -- Inside expression (recursive)\n  PARSE_COLUMN_LIST  -- Inside column list\n  PARSE_COLUMN_DEF   -- Inside column definition\n  ERROR_RECOVERY     -- After error, skipping to sync point\nTRANSITIONS (via function calls):\n  PARSE_STATEMENT --keyword SELECT--> PARSE_SELECT\n  PARSE_STATEMENT --keyword INSERT--> PARSE_INSERT\n  PARSE_STATEMENT --keyword CREATE--> PARSE_CREATE\n  PARSE_SELECT --keyword WHERE--> PARSE_EXPRESSION\n  PARSE_SELECT --keyword ORDER--> PARSE_ORDER_BY\n  PARSE_SELECT --keyword LIMIT--> PARSE_LIMIT\n  PARSE_EXPRESSION --operator--> PARSE_EXPRESSION (recursive)\n  PARSE_EXPRESSION --'('--> PARSE_EXPRESSION (parenthesized)\n  ANY_STATE --error--> ERROR_RECOVERY\nERROR_RECOVERY:\n  Skip tokens until sync point found:\n  - Semicolon (;)\n  - Statement keywords: SELECT, INSERT, CREATE, UPDATE, DELETE\n  - EOF\n  After sync point:\n  - Clear error state\n  - Resume parsing\n```\n---\n## Error Handling Matrix\n| Error | Detected By | Recovery | User-Visible? |\n|-------|-------------|----------|---------------|\n| `UNEXPECTED_TOKEN` | `parser_expect_*` functions | Set error, do NOT consume, return NULL | Yes: \"expected identifier, got keyword 'FROM' at line 3, column 15\" |\n| `MISSING_FROM` | `parser_parse_select` after column list | Set error, return NULL | Yes: \"expected FROM after column list at line 1, column 20\" |\n| `MISSING_CLAUSE` | Clause-specific parsers | Set error with clause name, return NULL | Yes: \"expected VALUES after column list at line 2, column 5\" |\n| `INVALID_EXPRESSION` | `parser_parse_primary` | Set error at unexpected token, return NULL | Yes: \"expected expression at line 4, column 10\" |\n| `UNTERMINATED_PAREN` | Expression parser after `(` | Set error at opening paren position | Yes: \"expected ')' after expression at line 1, column 5\" |\n| `INVALID_DATA_TYPE` | `parser_expect_data_type` | Set error with expected types, return NULL | Yes: \"expected data type (INTEGER, TEXT, REAL, BLOB) at line 2, column 15\" |\n| `EMPTY_COLUMN_LIST` | `parser_parse_column_list` with immediate `)` | Set error, return NULL | Yes: \"expected column expression at line 1, column 10\" |\n| `DUPLICATE_CONSTRAINT` | Column definition parser | Non-fatal warning (semantic check) | Yes (warning): \"duplicate NOT NULL constraint at line 3, column 5\" |\n**Recovery guarantees**:\n- After any error, parser position is at a valid token boundary.\n- Error includes position of where the error was detected (not where it might have started).\n- Parser can continue parsing after error recovery (multi-statement batches).\n- Memory is not leaked on error paths (all partial AST nodes freed).\n---\n## Implementation Sequence with Checkpoints\n### Phase 1: AST Node Types and Structures (1 hour)\n**Files to create**: `ast.h`, `ast.c`\n**Implementation**:\n1. Define `ASTNodeType` enum with all node types.\n2. Define `LiteralType` and `ConstraintType` enums.\n3. Define `ASTNode` struct with union for all variants.\n4. Implement `ast_create`, `ast_free`, `ast_copy`.\n5. Implement specific constructors: `ast_create_select`, `ast_create_literal`, etc.\n6. Implement list operations: `ast_column_list_add`, `ast_column_def_add_constraint`.\n**Checkpoint**: Create and free AST nodes without memory leaks.\n```bash\ngcc -c ast.c -o ast.o\n# Test: Create SELECT node, add columns, free without crash or leak\nvalgrind --leak-check=full ./test_ast_basics\n```\n### Phase 2: Parser Core with Token Consumption (1 hour)\n**Files to create**: `parser.h`, `parser.c`, `precedence.h`, `precedence.c`\n**Implementation**:\n1. Define `Parser` struct with all fields.\n2. Implement `parser_create`, `parser_free`.\n3. Implement token peek/consume utilities.\n4. Implement `parser_check_*` and `parser_match_*` functions.\n5. Implement `parser_expect_*` functions with error setting.\n6. Implement precedence lookup functions.\n**Checkpoint**: Token consumption works correctly.\n```bash\n# Test: Consume tokens in sequence, peek without consuming\n./test_parser_tokens\n# SELECT * FROM users -> [SELECT, *, FROM, users, EOF]\n```\n### Phase 3: SELECT Statement Parsing (1.5 hours)\n**Files to create**: `parse_select.c`\n**Implementation**:\n1. Implement `parser_parse_select` main function.\n2. Implement `parser_parse_column_list` (including * wildcard).\n3. Implement `parser_parse_order_by` (with ASC/DESC).\n4. Implement `parser_parse_limit` (with optional OFFSET).\n5. Integrate expression parsing for WHERE clause.\n6. Handle DISTINCT keyword.\n**Checkpoint**: SELECT statements parse to correct AST.\n```bash\n# Test: SELECT * FROM users -> AST with star=true, table_name=\"users\"\n# Test: SELECT a, b FROM t WHERE x = 5 ORDER BY a DESC LIMIT 10\n./test_parser_select\n```\n### Phase 4: INSERT Statement Parsing (1 hour)\n**Files to create**: `parse_insert.c`\n**Implementation**:\n1. Implement `parser_parse_insert` main function.\n2. Implement `parser_parse_values_clause`.\n3. Handle optional column list.\n4. Handle multiple value rows.\n**Checkpoint**: INSERT statements parse correctly.\n```bash\n# Test: INSERT INTO users VALUES (1, 'Alice')\n# Test: INSERT INTO users (id, name) VALUES (1, 'Alice'), (2, 'Bob')\n./test_parser_insert\n```\n### Phase 5: CREATE TABLE Parsing with Constraints (1 hour)\n**Files to create**: `parse_create.c`\n**Implementation**:\n1. Implement `parser_parse_create` main function.\n2. Implement `parser_parse_column_def_list`.\n3. Implement `parser_parse_column_def`.\n4. Implement constraint parsers: PRIMARY KEY, NOT NULL, UNIQUE, DEFAULT, CHECK.\n5. Handle IF NOT EXISTS clause.\n**Checkpoint**: CREATE TABLE statements parse correctly.\n```bash\n# Test: CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT NOT NULL)\n./test_parser_create\n```\n### Phase 6: Expression Parsing with Precedence Climbing (1.5 hours)\n**Files to create**: `parse_expression.c`\n**Implementation**:\n1. Implement `parser_parse_expression` entry point.\n2. Implement `parser_parse_expression_with_precedence` (core algorithm).\n3. Implement `parser_parse_unary` (NOT, unary minus).\n4. Implement `parser_parse_primary` (literals, identifiers, parentheses).\n5. Test all operator precedences.\n**Checkpoint**: Expression parsing handles all precedences correctly.\n```bash\n# Test: a OR b AND c -> OR at root, AND as right child\n# Test: (a OR b) AND c -> AND at root, OR as left child\n# Test: NOT a = b -> NOT at root, = as child\n./test_parser_expression\n```\n---\n## Test Specification\n### AST Node Tests\n```c\nvoid test_ast_create_select() {\n    ASTNode* node = ast_create_select(1, 5);\n    assert(node->type == AST_SELECT_STMT);\n    assert(node->line == 1);\n    assert(node->column == 5);\n    assert(node->data.select_stmt.columns == NULL);\n    assert(node->data.select_stmt.table_name == NULL);\n    ast_free(node);\n}\nvoid test_ast_create_binary_expr() {\n    ASTNode* left = ast_create_identifier(\"a\", 1, 1);\n    ASTNode* right = ast_create_identifier(\"b\", 1, 5);\n    ASTNode* bin = ast_create_binary_expr(\"AND\", left, right, 1, 1);\n    assert(bin->type == AST_BINARY_EXPR);\n    assert(strcmp(bin->data.binary_expr.operator, \"AND\") == 0);\n    assert(bin->data.binary_expr.left == left);\n    assert(bin->data.binary_expr.right == right);\n    ast_free(bin);  // Should free left and right too\n}\nvoid test_ast_column_list_add() {\n    ASTNode* list = ast_create_column_list(1, 1);\n    ASTNode* col1 = ast_create_identifier(\"a\", 1, 1);\n    ASTNode* col2 = ast_create_identifier(\"b\", 1, 4);\n    ast_column_list_add(list, col1);\n    ast_column_list_add(list, col2);\n    assert(list->data.column_list.count == 2);\n    assert(list->data.column_list.star == false);\n    ast_free(list);\n}\n```\n### SELECT Parsing Tests\n```c\nvoid test_parse_select_star() {\n    Token tokens[] = {\n        {TOKEN_KEYWORD, \"SELECT\", 1, 1},\n        {TOKEN_OPERATOR, \"*\", 1, 8},\n        {TOKEN_KEYWORD, \"FROM\", 1, 10},\n        {TOKEN_IDENTIFIER, \"users\", 1, 15},\n        {TOKEN_EOF, NULL, 1, 20}\n    };\n    Parser* parser = parser_create(tokens, 5);\n    ASTNode* ast = parser_parse(parser);\n    assert(ast != NULL);\n    assert(ast->type == AST_SELECT_STMT);\n    assert(ast->data.select_stmt.columns->data.column_list.star == true);\n    assert(strcmp(ast->data.select_stmt.table_name, \"users\") == 0);\n    assert(ast->data.select_stmt.where == NULL);\n    parser_free(parser);\n    ast_free(ast);\n}\nvoid test_parse_select_columns() {\n    // SELECT id, name FROM users\n    // Verify column list has two items: \"id\" and \"name\"\n}\nvoid test_parse_select_with_where() {\n    // SELECT * FROM users WHERE age > 18\n    // Verify where clause exists and contains binary expression\n}\nvoid test_parse_select_with_order_by() {\n    // SELECT * FROM users ORDER BY name ASC, id DESC\n    // Verify order_by clause with two items\n}\nvoid test_parse_select_with_limit() {\n    // SELECT * FROM users LIMIT 10 OFFSET 5\n    // Verify limit clause with limit and offset\n}\nvoid test_parse_select_distinct() {\n    // SELECT DISTINCT category FROM products\n    // Verify distinct == true\n}\n```\n### INSERT Parsing Tests\n```c\nvoid test_parse_insert_values() {\n    // INSERT INTO users VALUES (1, 'Alice')\n    // Verify table_name, values clause with one row\n}\nvoid test_parse_insert_columns() {\n    // INSERT INTO users (id, name) VALUES (1, 'Alice')\n    // Verify columns list and values clause\n}\nvoid test_parse_insert_multiple_rows() {\n    // INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob')\n    // Verify values clause has two rows\n}\n```\n### CREATE TABLE Parsing Tests\n```c\nvoid test_parse_create_simple() {\n    // CREATE TABLE users (id INTEGER, name TEXT)\n    // Verify table_name and column_defs\n}\nvoid test_parse_create_with_constraints() {\n    // CREATE TABLE users (\n    //   id INTEGER PRIMARY KEY,\n    //   name TEXT NOT NULL UNIQUE\n    // )\n    // Verify constraints on each column\n}\nvoid test_parse_create_if_not_exists() {\n    // CREATE TABLE IF NOT EXISTS users (id INTEGER)\n    // Verify if_not_exists == true\n}\n```\n### Expression Parsing Tests\n```c\nvoid test_parse_literal_integer() {\n    // Expression: 42\n    // Verify AST_LITERAL_EXPR with LITERAL_INTEGER\n}\nvoid test_parse_literal_string() {\n    // Expression: 'hello'\n    // Verify AST_LITERAL_EXPR with LITERAL_STRING\n}\nvoid test_parse_literal_null() {\n    // Expression: NULL\n    // Verify AST_LITERAL_EXPR with LITERAL_NULL\n}\nvoid test_parse_identifier() {\n    // Expression: column_name\n    // Verify AST_IDENTIFIER_EXPR\n}\nvoid test_parse_parenthesized() {\n    // Expression: (a + b)\n    // Verify parentheses are NOT in AST (just affect parsing)\n}\nvoid test_parse_precedence_and_or() {\n    // Expression: a OR b AND c\n    // Verify: OR at root, AND as right child of OR\n    ASTNode* ast = parse_expression(\"a OR b AND c\");\n    assert(ast->type == AST_BINARY_EXPR);\n    assert(strcmp(ast->data.binary_expr.operator, \"OR\") == 0);\n    assert(ast->data.binary_expr.right->type == AST_BINARY_EXPR);\n    assert(strcmp(ast->data.binary_expr.right->data.binary_expr.operator, \"AND\") == 0);\n}\nvoid test_parse_precedence_comparison_additive() {\n    // Expression: a + b = c\n    // Verify: = at root, + as left child of =\n    // Because comparison < additive in precedence\n}\nvoid test_parse_precedence_parentheses() {\n    // Expression: (a OR b) AND c\n    // Verify: AND at root, OR as left child of AND\n    ASTNode* ast = parse_expression(\"(a OR b) AND c\");\n    assert(ast->type == AST_BINARY_EXPR);\n    assert(strcmp(ast->data.binary_expr.operator, \"AND\") == 0);\n    assert(ast->data.binary_expr.left->type == AST_BINARY_EXPR);\n    assert(strcmp(ast->data.binary_expr.left->data.binary_expr.operator, \"OR\") == 0);\n}\nvoid test_parse_unary_not() {\n    // Expression: NOT a = b\n    // Verify: NOT at root, = as child\n}\nvoid test_parse_unary_minus() {\n    // Expression: -5\n    // Verify: UNARY_EXPR with operator \"-\"\n}\nvoid test_parse_nested_unary() {\n    // Expression: NOT NOT a\n    // Verify: NOT(NOT(a))\n}\n```\n### Error Handling Tests\n```c\nvoid test_error_missing_from() {\n    // SELECT * users\n    // Verify error: \"expected FROM after column list\"\n}\nvoid test_error_unexpected_token() {\n    // SELECT * FROM ,\n    // Verify error at comma position\n}\nvoid test_error_unterminated_paren() {\n    // SELECT * FROM users WHERE (a = 1\n    // Verify error at opening paren position\n}\nvoid test_error_invalid_data_type() {\n    // CREATE TABLE t (id NUMBER)\n    // Verify error: expected data type (INTEGER, TEXT, REAL, BLOB)\n}\nvoid test_error_empty_column_list() {\n    // SELECT FROM users\n    // Verify error: expected column expression\n}\n```\n\n![Parser Error Recovery](./diagrams/tdd-diag-m2-5.svg)\n\n---\n## Performance Targets\n| Operation | Target | How to Measure |\n|-----------|--------|----------------|\n| Parse 1KB SQL | < 2ms | `time ./parser_benchmark < 1kb.sql` |\n| Parse 10KB SQL | < 15ms | Benchmark with large complex queries |\n| AST memory per simple SELECT | < 500 bytes | `sizeof` all nodes for \"SELECT * FROM t\" |\n| AST memory per complex query | Proportional to complexity | Linear scaling with statement size |\n| Expression parsing | O(n) for n tokens | Single pass with precedence climbing |\n| Keyword lookup in precedence | O(1) | Hash table or switch statement |\n**Benchmark methodology**:\n```c\nvoid benchmark_parser() {\n    const char* complex_sql = \n        \"SELECT a, b, c FROM t1 \"\n        \"JOIN t2 ON t1.id = t2.t1_id \"\n        \"WHERE a > 5 AND b LIKE '%test%' \"\n        \"ORDER BY c DESC LIMIT 100\";\n    // Tokenize\n    Lexer* lexer = lexer_create(complex_sql);\n    Token tokens[100];\n    int count = 0;\n    while (true) {\n        tokens[count] = lexer_next_token(lexer);\n        if (tokens[count].type == TOKEN_EOF) break;\n        count++;\n    }\n    // Parse\n    clock_t start = clock();\n    Parser* parser = parser_create(tokens, count);\n    ASTNode* ast = parser_parse(parser);\n    clock_t end = clock();\n    double elapsed_ms = (double)(end - start) / CLOCKS_PER_SEC * 1000;\n    printf(\"Parsed in %.2f ms\\n\", elapsed_ms);\n    assert(elapsed_ms < 2.0);\n    ast_free(ast);\n    parser_free(parser);\n    lexer_free(lexer);\n}\n```\n---\n[[CRITERIA_JSON: {\"module_id\": \"build-sqlite-m2\", \"criteria\": [\"Parser implements recursive descent with one function per grammar rule: parse_statement, parse_select_stmt, parse_insert_stmt, parse_create_stmt\", \"AST node structure includes type field (ASTNodeType enum), position information (line, column integers), and a union for type-specific data\", \"SELECT parser produces AST with column_list node supporting * wildcard and comma-separated identifiers\", \"SELECT parser produces FROM clause with table name as string\", \"SELECT parser produces optional WHERE clause as Expression node\", \"SELECT parser produces optional ORDER BY clause with column names and ASC/DESC flags\", \"SELECT parser produces optional LIMIT clause with numeric value\", \"INSERT parser produces AST with target table name as string\", \"INSERT parser produces optional column name list before VALUES as column_list node\", \"INSERT parser produces VALUES clause with one or more tuples of expressions\", \"CREATE TABLE parser extracts table name from statement as string\", \"CREATE TABLE parser extracts column definitions with names and data types (INTEGER, TEXT, REAL, BLOB)\", \"CREATE TABLE parser extracts column constraints (PRIMARY KEY, NOT NULL, UNIQUE)\", \"Expression parser uses precedence climbing algorithm to handle binary operators\", \"Precedence order from lowest to highest: OR < AND < comparison (=,<,>,<=,>=,<>,!=) < additive (+,-) < multiplicative (*,/,%) < unary (NOT,-) < primary\", \"Parenthesized expressions trigger fresh parse_expression call, correctly overriding operator precedence\", \"Binary expressions produce AST_BINARY_EXPR nodes with left operand, operator string, and right operand\", \"Unary expressions (NOT, unary minus) produce AST_UNARY_EXPR nodes with operator and operand\", \"NULL keyword is parsed as AST_LITERAL_EXPR with LITERAL_NULL type, not AST_IDENTIFIER_EXPR\", \"TRUE and FALSE keywords are parsed as AST_LITERAL_EXPR with LITERAL_BOOLEAN type\", \"String, integer, and float literals are parsed as AST_LITERAL_EXPR nodes containing the token value\", \"Column references are parsed as AST_IDENTIFIER_EXPR nodes containing the column name\", \"Error handling captures first error with line/column position and descriptive message\", \"Parser uses expect_* functions that consume tokens and report errors on mismatch\", \"Parser uses check_* functions for lookahead without consuming tokens\", \"Test suite validates at least 15 valid SQL statements covering SELECT, INSERT, and CREATE TABLE with various clause combinations\", \"Test suite validates at least 10 invalid SQL statements with correct position information in errors\", \"Expression precedence tests verify that 'a OR b AND c' produces OR at root with AND as right child\", \"Parentheses tests verify that '(a OR b) AND c' produces AND at root with OR as left child\", \"Parser handles qualified identifiers (table.column) if supported, or documents limitation\", \"Parser provides error recovery for multi-statement batches by skipping to synchronization points (semicolon, statement keywords)\"]}]\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m3 -->\n# Bytecode Compiler (VDBE): Technical Design Specification\n## Module Charter\nThe Bytecode Compiler transforms SQL AST nodes into a linear sequence of bytecode instructions for the Virtual Database Engine (VDBE)\u2014a register-based virtual machine. It implements approximately 25 opcodes covering table operations (OpenRead, OpenWrite, Close), cursor movement (Rewind, Next, Prev), data access (Column, MakeRecord, Insert, Delete), arithmetic (Add, Subtract, Multiply, Divide), comparisons (Eq, Ne, Lt, Le, Gt, Ge), control flow (Goto, Halt, Gosub, Return), and value loading (Integer, String8, Null, Copy). The compiler performs register allocation, manages jump target resolution through forward references, and produces EXPLAIN output for query introspection. The compiler does NOT execute queries\u2014that's the VDBE's job\u2014nor does it perform query optimization (that's the planner's job); it faithfully translates the AST into executable instructions.\n**Upstream dependencies**: AST from SQL Parser (milestone 2), table/index metadata from system catalog.\n**Downstream dependencies**: VDBE (this milestone) executes the compiled bytecode.\n**Invariants**:\n1. Every valid AST produces exactly one complete bytecode program ending with Halt.\n2. All jump targets resolve to valid instruction addresses within the program.\n3. Register allocation never clobbers live values; each expression result occupies a unique register.\n4. Cursors are opened before use and closed before program termination.\n5. Comparison operators handle NULL correctly via three-valued logic (NULL comparisons never trigger jumps).\n6. The instruction array is growable; no fixed limits on program size.\n---\n## File Structure\nCreate files in this order:\n```\n1. src/vdbe/opcode.h            -- OpCode enumeration and instruction struct\n2. src/vdbe/instruction.h       -- Instruction struct with all operands\n3. src/vdbe/value.h             -- Value union for register file entries\n4. src/vdbe/value.c             -- Value constructors, comparisons, arithmetic\n5. src/vdbe/cursor.h            -- Cursor struct for B-tree traversal\n6. src/vdbe/vm.h                -- VDBE struct with registers, cursors, code\n7. src/vdbe/vm.c                -- VM lifecycle and execution loop\n8. src/compiler/compiler.h      -- Compiler struct with emit/allocate functions\n9. src/compiler/compiler.c      -- Core compiler framework\n10. src/compiler/compile_select.c -- SELECT statement compilation\n11. src/compiler/compile_insert.c -- INSERT statement compilation\n12. src/compiler/compile_expression.c -- Expression to bytecode compilation\n13. src/compiler/jump_resolver.h -- Forward jump resolution utilities\n14. src/compiler/jump_resolver.c -- Patch pending jumps when targets known\n15. src/vdbe/opcodes_table.c    -- Execution implementation for each opcode\n16. src/vdbe/explain.c          -- EXPLAIN formatting output\n17. tests/test_compiler.c       -- Compilation tests\n18. tests/test_vm.c             -- VM execution tests\n19. tests/test_explain.c        -- EXPLAIN output tests\n```\n---\n## Complete Data Model\n### OpCode Enumeration\n```c\n// File: src/vdbe/opcode.h\ntypedef enum {\n    // Control flow\n    OP_Halt = 0,            // Stop execution\n    OP_Goto,                // Unconditional jump: p2 = target address\n    OP_Gosub,               // Call subroutine: p2 = target, push return addr\n    OP_Return,              // Return from subroutine\n    OP_Noop,                // Do nothing (for padding/debugging)\n    // Value loading\n    OP_Integer,             // p1 = value, p2 = dest register\n    OP_Real,                // p1 = 0, p2 = dest, p4 = string representation\n    OP_String8,             // p1 = length (or 0), p2 = dest, p4 = string\n    OP_Null,                // p1 = 0, p2 = dest register\n    OP_Blob,                // p1 = length, p2 = dest, p4 = blob data\n    OP_Copy,                // p1 = src reg, p2 = dest reg, p3 = count\n    OP_SCopy,               // Shallow copy (for read-only)\n    // Table/cursor operations\n    OP_OpenRead,            // p1 = cursor id, p2 = root page, p4 = table ptr\n    OP_OpenWrite,           // p1 = cursor id, p2 = root page, p4 = table ptr\n    OP_Close,               // p1 = cursor id\n    OP_SeekRowid,           // p1 = cursor, p2 = rowid register, p3 = jump if not found\n    OP_SeekGE,              // p1 = cursor, p2 = key register, p3 = jump target\n    OP_SeekGT,              // p1 = cursor, p2 = key register, p3 = jump target\n    // Cursor movement\n    OP_Rewind,              // p1 = cursor, p2 = jump if empty\n    OP_Next,                // p1 = cursor, p2 = jump target if more rows\n    OP_Prev,                // p1 = cursor, p2 = jump target if more rows\n    // Data access\n    OP_Column,              // p1 = cursor, p2 = column index, p3 = dest register\n    OP_MakeRecord,          // p1 = start reg, p2 = count, p3 = dest reg\n    OP_Insert,              // p1 = cursor, p2 = record reg, p3 = rowid reg\n    OP_Delete,              // p1 = cursor\n    OP_NewRowid,            // p1 = cursor, p2 = dest reg for new rowid\n    // Comparisons (jump if condition TRUE)\n    OP_Eq,                  // p1 = reg1, p2 = reg2, p3 = jump target\n    OP_Ne,                  // p1 = reg1, p2 = reg2, p3 = jump target\n    OP_Lt,                  // p1 = reg1, p2 = reg2, p3 = jump target\n    OP_Le,                  // p1 = reg1, p2 = reg2, p3 = jump target\n    OP_Gt,                  // p1 = reg1, p2 = reg2, p3 = jump target\n    OP_Ge,                  // p1 = reg1, p2 = reg2, p3 = jump target\n    // Conditional jump (generic)\n    OP_If,                  // p1 = condition reg, p2 = jump target\n    OP_IfNot,               // p1 = condition reg, p2 = jump target\n    // Arithmetic\n    OP_Add,                 // p1 = reg1, p2 = reg2, p3 = dest\n    OP_Subtract,            // p1 = reg1, p2 = reg2, p3 = dest\n    OP_Multiply,            // p1 = reg1, p2 = reg2, p3 = dest\n    OP_Divide,              // p1 = reg1, p2 = reg2, p3 = dest\n    OP_Remainder,           // p1 = reg1, p2 = reg2, p3 = dest (modulo)\n    // String operations\n    OP_Concat,              // p1 = reg1, p2 = reg2, p3 = dest\n    // Output\n    OP_ResultRow,           // p1 = start reg, p2 = count\n    // Transaction\n    OP_Transaction,         // p1 = db number, p2 = write flag\n    OP_AutoCommit,          // p1 = 0/1 for off/on\n    // Total: ~35 opcodes for initial implementation\n} OpCode;\n```\n### Instruction Structure\n```c\n// File: src/vdbe/instruction.h\ntypedef struct {\n    OpCode opcode;\n    int p1;                 // First operand (often register or cursor)\n    int p2;                 // Second operand (often register or jump target)\n    int p3;                 // Third operand (often register)\n    char* p4;               // Fourth operand (string, or pointer cast to char*)\n    int p5;                 // Fifth operand (flags, rarely used)\n} Instruction;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       4     opcode (enum)\n// 4       4     p1\n// 8       4     p2\n// 12      4     p3\n// 16      8     p4 (pointer)\n// 24      4     p5\n// 28      4     padding\n// Total: 32 bytes per instruction\n```\n### Value Union (Register Entry)\n```c\n// File: src/vdbe/value.h\ntypedef enum {\n    VALUE_NULL = 0,\n    VALUE_INTEGER,\n    VALUE_FLOAT,\n    VALUE_STRING,\n    VALUE_BLOB\n} ValueType;\ntypedef struct {\n    ValueType type;\n    union {\n        int64_t integer_val;\n        double float_val;\n        struct {\n            char* data;\n            size_t length;\n            bool owned;         // true if malloc'd, false if static/borrowed\n        } string_val;\n        struct {\n            uint8_t* data;\n            size_t length;\n            bool owned;\n        } blob_val;\n    } data;\n} Value;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       4     type (enum)\n// 4       4     padding\n// 8       8     integer_val OR\n// 8       8     float_val OR\n// 8       8     string_val.data pointer\n// 16      8     string_val.length\n// 24      1     string_val.owned\n// 25      7     padding\n// Total: 32 bytes per register entry\n```\n### Cursor Structure\n```c\n// File: src/vdbe/cursor.h\ntypedef struct {\n    int cursor_id;          // Cursor identifier (matches p1 in opcodes)\n    PageId root_page;       // Root page of B-tree\n    PageId current_page;    // Current page in traversal\n    int current_cell;       // Cell index within current page\n    bool eof;               // End of traversal flag\n    bool is_table;          // true for table B-tree, false for index\n    Table* table;           // Table metadata (for schema lookup)\n    Index* index;           // Index metadata (NULL for table cursors)\n} Cursor;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       4     cursor_id\n// 4       4     root_page\n// 8       4     current_page\n// 12      4     current_cell\n// 16      1     eof\n// 17      1     is_table\n// 18      2     padding\n// 20      4     padding\n// 24      8     table pointer\n// 32      8     index pointer\n// Total: 40 bytes per cursor\n```\n### VDBE Structure\n```c\n// File: src/vdbe/vm.h\ntypedef struct {\n    // Code\n    Instruction* code;\n    int code_length;\n    int code_capacity;\n    // Registers\n    Value* registers;\n    int register_count;\n    int register_capacity;\n    // Cursors\n    Cursor* cursors;\n    int cursor_count;\n    int cursor_capacity;\n    // Execution state\n    int pc;                 // Program counter (instruction index)\n    bool halted;\n    // Error state\n    int error_code;\n    char error_message[256];\n    // Database reference\n    Database* db;\n    BufferPool* buffer_pool;\n    // Output callback\n    void (*result_callback)(const Value* row, int count, void* user_data);\n    void* callback_user_data;\n} VDBE;\n// Memory layout (64-bit system):\n// Offset   Size   Field\n// 0        8      code pointer\n// 8        4      code_length\n// 12       4      code_capacity\n// 16       8      registers pointer\n// 24       4      register_count\n// 28       4      register_capacity\n// 32       8      cursors pointer\n// 40       4      cursor_count\n// 44       4      cursor_capacity\n// 48       4      pc\n// 52       1      halted\n// 53       3      padding\n// 56       4      error_code\n// 60       256    error_message\n// 316      4      padding\n// 320      8      db pointer\n// 328      8      buffer_pool pointer\n// 336      8      result_callback pointer\n// 344      8      callback_user_data\n// Total: 352 bytes (base structure)\n```\n### Compiler Structure\n```c\n// File: src/compiler/compiler.h\ntypedef struct {\n    // Code generation\n    Instruction* code;\n    int code_length;\n    int code_capacity;\n    // Register allocation\n    int next_register;      // Monotonically increasing register counter\n    int max_register;       // Highest register used\n    // Cursor allocation\n    int next_cursor;\n    // Jump resolution\n    int* pending_jumps;     // Instruction addresses with unresolved targets\n    int pending_jump_count;\n    int pending_jump_capacity;\n    // Symbol table for table/column lookup\n    Database* db;\n    // Current context (for column resolution)\n    Table* current_table;\n    int current_cursor;\n    // Error state\n    bool has_error;\n    char error_message[512];\n    int error_line;\n    int error_column;\n} Compiler;\n// Memory layout (64-bit system):\n// Offset   Size   Field\n// 0        8      code pointer\n// 8        4      code_length\n// 12       4      code_capacity\n// 16       4      next_register\n// 20       4      max_register\n// 24       4      next_cursor\n// 28       4      padding\n// 32       8      pending_jumps pointer\n// 40       4      pending_jump_count\n// 44       4      pending_jump_capacity\n// 48       8      db pointer\n// 56       8      current_table pointer\n// 64       4      current_cursor\n// 68       1      has_error\n// 69       3      padding\n// 72       4      error_line\n// 76       4      error_column\n// 80       512    error_message\n// Total: 592 bytes\n```\n\n![VDBE Architecture](./diagrams/tdd-diag-m3-1.svg)\n\n---\n## Interface Contracts\n### Value Lifecycle\n```c\n// File: src/vdbe/value.c\n// Initialize a null value\nvoid value_init(Value* v);\n// Constraints: v must not be NULL\n// Set integer value\nvoid value_set_integer(Value* v, int64_t val);\n// Constraints: v must not be NULL\n// Side effects: frees any existing owned data\n// Set float value\nvoid value_set_float(Value* v, double val);\n// Constraints: v must not be NULL\n// Set string value (copies the string)\nvoid value_set_string(Value* v, const char* str, size_t len);\n// Constraints: v must not be NULL, str must not be NULL if len > 0\n// Side effects: allocates memory for string copy\n// Set string value (takes ownership)\nvoid value_set_string_owned(Value* v, char* str, size_t len);\n// Constraints: v must not be NULL, str must be malloc'd\n// Side effects: v takes ownership of str\n// Set blob value (copies the data)\nvoid value_set_blob(Value* v, const uint8_t* data, size_t len);\n// Set null value\nvoid value_set_null(Value* v);\n// Free owned data (does not free Value struct)\nvoid value_free(Value* v);\n// Constraints: v must not be NULL\n// Side effects: frees owned string/blob data\n// Deep copy\nvoid value_copy(Value* dest, const Value* src);\n// Constraints: dest and src must not be NULL, must not overlap\n// Side effects: allocates memory for copied owned data\n// Comparison (returns -1, 0, 1 for <, ==, >)\nint value_compare(const Value* a, const Value* b);\n// Constraints: a and b must not be NULL\n// Special: returns 0 if both are NULL, -2 if either is NULL (for three-valued logic)\n// Type coercion\ndouble value_to_float(const Value* v);\n// Returns: 0.0 for NULL, integer as double, float as-is, string parsed as double\n```\n### Compiler Lifecycle\n```c\n// File: src/compiler/compiler.h\n// Create compiler with database reference\nCompiler* compiler_create(Database* db);\n// Constraints: db must not be NULL\n// Returns: newly allocated compiler with empty code array\n// Free compiler (does NOT free database)\nvoid compiler_free(Compiler* compiler);\n// Constraints: compiler may be NULL (no-op)\n// Compile AST to bytecode\nint compiler_compile(Compiler* compiler, const ASTNode* ast);\n// Constraints: compiler and ast must not be NULL\n// Returns: 0 on success, non-zero error code on failure\n// On error: compiler->has_error is true, error_message is set\n// Get compiled program (transfers ownership)\nInstruction* compiler_get_code(const Compiler* compiler, int* length);\n// Constraints: compiler must not be NULL\n// Returns: pointer to instruction array (caller must free)\n// The returned array is a copy; compiler retains its own array\n// Get required register count\nint compiler_get_register_count(const Compiler* compiler);\n```\n### Instruction Emission\n```c\n// File: src/compiler/compiler.c (internal)\n// Emit instruction with all operands\nint emit(Compiler* compiler, OpCode opcode, int p1, int p2, int p3, const char* p4);\n// Returns: instruction address (index in code array)\n// Side effects: may grow code array\n// Emit instruction with integer p4 (for convenience)\nint emit_int_p4(Compiler* compiler, OpCode opcode, int p1, int p2, int p3, int64_t p4);\n// Allocate a register\nint allocate_register(Compiler* compiler);\n// Returns: next available register number\n// The register is not initialized; caller must emit a load instruction\n// Allocate a cursor\nint allocate_cursor(Compiler* compiler);\n// Returns: next available cursor number\n// Emit jump with placeholder target (for forward jumps)\nint emit_jump_placeholder(Compiler* compiler, OpCode opcode, int p1, int p2, int p3);\n// Returns: instruction address (for later patching)\n// p2 is set to -1 as placeholder\n// Patch jump target\nvoid patch_jump(Compiler* compiler, int instruction_addr, int target_addr);\n// Constraints: instruction_addr must be valid, instruction must be a jump\n// Side effects: modifies instruction->p2\n```\n### VDBE Lifecycle\n```c\n// File: src/vdbe/vm.h\n// Create VM with database and buffer pool\nVDBE* vdbe_create(Database* db, BufferPool* pool);\n// Constraints: db and pool must not be NULL\n// Returns: newly allocated VM with empty state\n// Free VM (does NOT free database or buffer pool)\nvoid vdbe_free(VDBE* vm);\n// Constraints: vm may be NULL (no-op)\n// Load program into VM\nint vdbe_load_program(VDBE* vm, const Instruction* code, int length, int register_count);\n// Constraints: vm and code must not be NULL, length > 0, register_count > 0\n// Returns: 0 on success, non-zero on allocation failure\n// Side effects: allocates register array\n// Set result callback\nvoid vdbe_set_callback(VDBE* vm, \n                       void (*callback)(const Value* row, int count, void* user_data),\n                       void* user_data);\n// Constraints: callback may be NULL (no output)\n// Execute program\nint vdbe_execute(VDBE* vm);\n// Returns: 0 on success (halt), error code on failure\n// Execution: runs until Halt opcode or error\n// For each ResultRow, invokes callback with current register values\n// Reset VM for re-execution\nvoid vdbe_reset(VDBE* vm);\n// Side effects: clears registers, closes cursors, resets PC to 0\n```\n### EXPLAIN Output\n```c\n// File: src/vdbe/explain.c\n// Format program as human-readable EXPLAIN output\nvoid explain_program(const Instruction* code, int length, FILE* output);\n// Constraints: code must not be NULL, output must be open for writing\n// Output format:\n//   addr  opcode           p1    p2    p3    p4\n//   ----  ---------------  ----  ----  ----  -----\n//   0     Integer          42    0           \n//   1     Halt             \n// Get opcode name string\nconst char* opcode_name(OpCode opcode);\n// Returns: static string for opcode name\n```\n---\n## Algorithm Specification\n### Main Compile Dispatch\n```\nALGORITHM: compiler_compile\nINPUT: Compiler* compiler, const ASTNode* ast\nOUTPUT: int (0 = success, non-zero = error code)\nINVARIANTS:\n  - compiler->code is empty at start\n  - On success, compiler->code contains complete program ending with Halt\n  - On error, compiler->has_error is true\nPROCEDURE:\n1. IF compiler == NULL OR ast == NULL THEN\n     RETURN ERROR_NULL_ARGUMENT\n2. SWITCH on ast->type:\n   CASE AST_SELECT_STMT:\n     result = compile_select(compiler, ast)\n   CASE AST_INSERT_STMT:\n     result = compile_insert(compiler, ast)\n   CASE AST_CREATE_STMT:\n     result = compile_create(compiler, ast)\n   DEFAULT:\n     compiler_set_error(compiler, \"unsupported statement type\", ast->line, ast->column)\n     RETURN ERROR_UNSUPPORTED_STATEMENT\n3. IF result != 0 THEN RETURN result\n4. // Ensure program ends with Halt\n   IF compiler->code_length == 0 OR \n      compiler->code[compiler->code_length - 1].opcode != OP_Halt THEN\n     emit(compiler, OP_Halt, 0, 0, 0, NULL)\n5. RETURN 0\n```\n### SELECT Compilation\n```\nALGORITHM: compile_select\nINPUT: Compiler* compiler, const ASTNode* select_ast\nOUTPUT: int (0 = success, error code on failure)\nGRAMMAR: SELECT [DISTINCT] columns FROM table [WHERE expr] [ORDER BY ...] [LIMIT ...]\nPROCEDURE:\n1. // Lookup table in system catalog\n   Table* table = database_lookup_table(compiler->db, select_ast->data.select_stmt.table_name)\n   IF table == NULL THEN\n     compiler_set_error(compiler, \"no such table: %s\", \n                        select_ast->line, select_ast->column,\n                        select_ast->data.select_stmt.table_name)\n     RETURN ERROR_UNKNOWN_TABLE\n2. compiler->current_table = table\n3. // Open cursor on table\n   int cursor = allocate_cursor(compiler)\n   compiler->current_cursor = cursor\n   emit(compiler, OP_OpenRead, cursor, table->root_page, 0, (char*)table)\n4. // Position at first row\n   int loop_start = compiler->code_length\n   int empty_jump = emit(compiler, OP_Rewind, cursor, 0, 0, NULL)  // p2 patched later\n5. // Compile WHERE clause (if present)\n   int skip_row_jump = -1\n   IF select_ast->data.select_stmt.where != NULL THEN\n     skip_row_jump = compile_where_clause(compiler, \n                                           select_ast->data.select_stmt.where,\n                                           cursor)\n     IF skip_row_jump < 0 THEN RETURN ERROR_COMPILATION_FAILED\n6. // Compile column projection\n   result = compile_column_list(compiler, select_ast->data.select_stmt.columns, cursor)\n   IF result != 0 THEN RETURN result\n7. // Close cursor\n   emit(compiler, OP_Close, cursor, 0, 0, NULL)\n8. // Emit Halt\n   emit(compiler, OP_Halt, 0, 0, 0, NULL)\n9. // Patch jump targets\n   // Rewind jumps to end if empty\n   patch_jump(compiler, empty_jump, compiler->code_length - 1)\n   // WHERE clause jumps to Next if condition fails\n   IF skip_row_jump >= 0 THEN\n     patch_jump(compiler, skip_row_jump, loop_start + 1)  // Jump to Next\n10. RETURN 0\n```\n### Column List Compilation\n```\nALGORITHM: compile_column_list\nINPUT: Compiler* compiler, const ASTNode* columns, int cursor\nOUTPUT: int (0 = success, error code on failure)\nPROCEDURE:\n1. IF columns->data.column_list.star THEN\n     // SELECT * - emit Column for all table columns\n     Table* table = compiler->current_table\n     int start_reg = allocate_register(compiler)\n     FOR i = 0 TO table->column_count - 1:\n       emit(compiler, OP_Column, cursor, i, start_reg + i, NULL)\n     emit(compiler, OP_ResultRow, start_reg, table->column_count, 0, NULL)\n   ELSE\n     // SELECT col1, col2, ...\n     int count = columns->data.column_list.count\n     int start_reg = allocate_register(compiler)\n     FOR i = 0 TO count - 1:\n       ASTNode* col_expr = columns->data.column_list.items[i]\n       result = compile_expression_to_register(compiler, col_expr, start_reg + i)\n       IF result != 0 THEN RETURN result\n     emit(compiler, OP_ResultRow, start_reg, count, 0, NULL)\n2. // Emit Next instruction for loop\n   emit(compiler, OP_Next, cursor, loop_start, 0, NULL)\n   // Note: loop_start is passed from caller or tracked in compiler\n3. RETURN 0\n```\n### WHERE Clause Compilation\n```\nALGORITHM: compile_where_clause\nINPUT: Compiler* compiler, const ASTNode* where_expr, int cursor\nOUTPUT: int (instruction address of skip jump, or negative on error)\nINSIGHT: WHERE clauses are compiled as inverted comparisons that skip non-matching rows\nPROCEDURE:\n1. // The where_expr is an expression tree\n   // We need to emit code that evaluates to TRUE/FALSE and jumps if FALSE\n   int cond_reg = compile_expression(compiler, where_expr)\n   IF cond_reg < 0 THEN RETURN cond_reg  // Error\n2. // Emit conditional jump (if condition is FALSE or NULL, skip row)\n   int skip_addr = emit(compiler, OP_IfNot, cond_reg, 0, 0, NULL)  // p2 patched later\n3. RETURN skip_addr\nALGORITHM: compile_expression\nINPUT: Compiler* compiler, const ASTNode* expr\nOUTPUT: int (register containing expression result, or negative on error)\nPROCEDURE:\n1. SWITCH on expr->type:\n   CASE AST_LITERAL_EXPR:\n     RETURN compile_literal(compiler, expr)\n   CASE AST_IDENTIFIER_EXPR:\n     RETURN compile_identifier(compiler, expr)\n   CASE AST_BINARY_EXPR:\n     RETURN compile_binary_expr(compiler, expr)\n   CASE AST_UNARY_EXPR:\n     RETURN compile_unary_expr(compiler, expr)\n   DEFAULT:\n     compiler_set_error(compiler, \"unknown expression type\", expr->line, expr->column)\n     RETURN ERROR_UNKNOWN_EXPRESSION\nALGORITHM: compile_literal\nINPUT: Compiler* compiler, const ASTNode* literal\nOUTPUT: int (register containing literal value)\nPROCEDURE:\n1. int reg = allocate_register(compiler)\n2. SWITCH on literal->data.literal_expr.literal_type:\n   CASE LITERAL_INTEGER:\n     int64_t val = parse_integer(literal->data.literal_expr.value)\n     emit(compiler, OP_Integer, (int)val, reg, 0, NULL)  // Note: p1 limited to int\n     // For values > INT_MAX, use p4 string representation\n   CASE LITERAL_FLOAT:\n     emit(compiler, OP_Real, 0, reg, 0, strdup(literal->data.literal_expr.value))\n   CASE LITERAL_STRING:\n     emit(compiler, OP_String8, 0, reg, 0, strdup(literal->data.literal_expr.value))\n   CASE LITERAL_NULL:\n     emit(compiler, OP_Null, 0, reg, 0, NULL)\n   CASE LITERAL_BOOLEAN:\n     int val = (strcmp(literal->data.literal_expr.value, \"TRUE\") == 0) ? 1 : 0\n     emit(compiler, OP_Integer, val, reg, 0, NULL)\n3. RETURN reg\nALGORITHM: compile_identifier\nINPUT: Compiler* compiler, const ASTNode* ident\nOUTPUT: int (register containing column value)\nPROCEDURE:\n1. // Look up column in current table\n   const char* col_name = ident->data.identifier_expr.name\n   Table* table = compiler->current_table\n   int col_index = table_find_column(table, col_name)\n   IF col_index < 0 THEN\n     compiler_set_error(compiler, \"no such column: %s\", ident->line, ident->column, col_name)\n     RETURN ERROR_UNKNOWN_COLUMN\n2. int reg = allocate_register(compiler)\n3. emit(compiler, OP_Column, compiler->current_cursor, col_index, reg, NULL)\n4. RETURN reg\nALGORITHM: compile_binary_expr\nINPUT: Compiler* compiler, const ASTNode* expr\nOUTPUT: int (register containing result)\nPROCEDURE:\n1. const char* op = expr->data.binary_expr.operator\n2. int left_reg = compile_expression(compiler, expr->data.binary_expr.left)\n   IF left_reg < 0 THEN RETURN left_reg\n3. int right_reg = compile_expression(compiler, expr->data.binary_expr.right)\n   IF right_reg < 0 THEN RETURN right_reg\n4. int result_reg = allocate_register(compiler)\n5. // Emit appropriate opcode based on operator\n   IF strcmp(op, \"+\") == 0 THEN\n     emit(compiler, OP_Add, left_reg, right_reg, result_reg, NULL)\n   ELSE IF strcmp(op, \"-\") == 0 THEN\n     emit(compiler, OP_Subtract, left_reg, right_reg, result_reg, NULL)\n   ELSE IF strcmp(op, \"*\") == 0 THEN\n     emit(compiler, OP_Multiply, left_reg, right_reg, result_reg, NULL)\n   ELSE IF strcmp(op, \"/\") == 0 THEN\n     emit(compiler, OP_Divide, left_reg, right_reg, result_reg, NULL)\n   ELSE IF strcmp(op, \"%\") == 0 THEN\n     emit(compiler, OP_Remainder, left_reg, right_reg, result_reg, NULL)\n   ELSE IF strcmp(op, \"||\") == 0 THEN\n     emit(compiler, OP_Concat, left_reg, right_reg, result_reg, NULL)\n   ELSE IF is_comparison_op(op) THEN\n     // Comparisons are typically used in WHERE, not as standalone expressions\n     // But if we reach here, we need to compute a boolean\n     // This requires a more complex sequence with conditional jumps\n     RETURN compile_comparison_to_boolean(compiler, op, left_reg, right_reg, result_reg)\n   ELSE\n     compiler_set_error(compiler, \"unknown operator: %s\", expr->line, expr->column, op)\n     RETURN ERROR_UNKNOWN_OPERATOR\n6. RETURN result_reg\n```\n\n![Instruction Format](./diagrams/tdd-diag-m3-2.svg)\n\n### Comparison to Boolean (for expressions like `a > b` in SELECT list)\n```\nALGORITHM: compile_comparison_to_boolean\nINPUT: Compiler* compiler, const char* op, int left_reg, int right_reg, int result_reg\nOUTPUT: int (result_reg on success, negative on error)\nINSIGHT: Comparisons need jump instructions; we emit compare, two jumps, and loads\nPROCEDURE:\n1. // Emit comparison (inverted for WHERE pattern, normal here)\n   // We need: if (left op right) then result = 1 else result = 0\n   int true_label = compiler->code_length  // Will emit Integer 1 here\n   int false_label = emit(compiler, OP_Goto, 0, 0, 0, NULL)  // Placeholder\n2. // Emit code to load 1 (true)\n   emit(compiler, OP_Integer, 1, result_reg, 0, NULL)\n   int end_label = emit(compiler, OP_Goto, 0, 0, 0, NULL)  // Skip false path\n3. // Patch false label\n   patch_jump(compiler, false_label, compiler->code_length)\n   emit(compiler, OP_Integer, 0, result_reg, 0, NULL)\n4. // Patch end label\n   patch_jump(compiler, end_label, compiler->code_length)\n5. // Now emit the actual comparison that jumps to true_label or falls through\n   // This is complex; typically comparisons in WHERE use inverted form\n   // For simplicity, we emit the comparison at the start and rearrange\n   // Alternative: use a helper that emits compare + conditional move\n6. RETURN result_reg\n```\n### INSERT Compilation\n```\nALGORITHM: compile_insert\nINPUT: Compiler* compiler, const ASTNode* insert_ast\nOUTPUT: int (0 = success, error code on failure)\nPROCEDURE:\n1. // Lookup table\n   Table* table = database_lookup_table(compiler->db, insert_ast->data.insert_stmt.table_name)\n   IF table == NULL THEN\n     compiler_set_error(compiler, \"no such table: %s\",\n                        insert_ast->line, insert_ast->column,\n                        insert_ast->data.insert_stmt.table_name)\n     RETURN ERROR_UNKNOWN_TABLE\n2. compiler->current_table = table\n3. // Open cursor for writing\n   int cursor = allocate_cursor(compiler)\n   emit(compiler, OP_OpenWrite, cursor, table->root_page, 0, (char*)table)\n4. // Compile value expressions into consecutive registers\n   ASTNode* values = insert_ast->data.insert_stmt.values\n   int row_count = values->data.values_clause.count\n   FOR each row IN values->data.values_clause.rows:\n     a. int start_reg = allocate_register(compiler)\n     b. FOR i = 0 TO row->data.column_list.count - 1:\n         int reg = compile_expression(compiler, row->data.column_list.items[i])\n         IF reg < 0 THEN RETURN reg\n         // Ensure consecutive registers (may need copy)\n         IF reg != start_reg + i THEN\n           emit(compiler, OP_Copy, reg, start_reg + i, 0, NULL)\n     c. // Build record from values\n        int record_reg = allocate_register(compiler)\n        emit(compiler, OP_MakeRecord, start_reg, row->data.column_list.count, \n             record_reg, NULL)\n     d. // Generate new rowid\n        int rowid_reg = allocate_register(compiler)\n        emit(compiler, OP_NewRowid, cursor, rowid_reg, 0, NULL)\n     e. // Insert record\n        emit(compiler, OP_Insert, cursor, record_reg, rowid_reg, NULL)\n5. // Close cursor\n   emit(compiler, OP_Close, cursor, 0, 0, NULL)\n6. // Halt\n   emit(compiler, OP_Halt, 0, 0, 0, NULL)\n7. RETURN 0\n```\n\n![SELECT Compilation Flow](./diagrams/tdd-diag-m3-3.svg)\n\n### VM Execution Loop\n```\nALGORITHM: vdbe_execute\nINPUT: VDBE* vm (with program loaded)\nOUTPUT: int (0 = success, error code on failure)\nINVARIANTS:\n  - vm->pc is always a valid instruction index (0 <= pc < code_length) or error\n  - vm->halted is true only after OP_Halt executes\n  - All register accesses are within bounds (0 <= reg < register_count)\n  - Cursor operations validate cursor_id before access\nPROCEDURE:\n1. vm->halted = false\n   vm->pc = 0\n   vm->error_code = 0\n2. WHILE NOT vm->halted AND vm->pc < vm->code_length:\n   a. Instruction* instr = &vm->code[vm->pc]\n   b. int result = execute_instruction(vm, instr)\n   c. IF result != 0 THEN\n        vm->error_code = result\n        RETURN result\n   d. // Most instructions increment pc; jumps set it directly\n3. IF vm->pc >= vm->code_length AND NOT vm->halted THEN\n     // Ran off end of program without Halt\n     vm->error_code = ERROR_NO_HALT\n     RETURN ERROR_NO_HALT\n4. RETURN 0\nALGORITHM: execute_instruction\nINPUT: VDBE* vm, Instruction* instr\nOUTPUT: int (0 = continue, non-zero = error)\nPROCEDURE:\n1. SWITCH on instr->opcode:\n   // --- Control Flow ---\n   CASE OP_Halt:\n     vm->halted = true\n     vm->pc++\n     RETURN 0\n   CASE OP_Goto:\n     vm->pc = instr->p2\n     RETURN 0\n   CASE OP_Gosub:\n     // Push return address (simplified: store in a designated register)\n     vm->registers[0].type = VALUE_INTEGER\n     vm->registers[0].data.integer_val = vm->pc + 1\n     vm->pc = instr->p2\n     RETURN 0\n   CASE OP_Return:\n     vm->pc = (int)vm->registers[0].data.integer_val\n     RETURN 0\n   CASE OP_Noop:\n     vm->pc++\n     RETURN 0\n   // --- Value Loading ---\n   CASE OP_Integer:\n     validate_register(vm, instr->p2)\n     vm->registers[instr->p2].type = VALUE_INTEGER\n     vm->registers[instr->p2].data.integer_val = instr->p1\n     vm->pc++\n     RETURN 0\n   CASE OP_Real:\n     validate_register(vm, instr->p2)\n     vm->registers[instr->p2].type = VALUE_FLOAT\n     vm->registers[instr->p2].data.float_val = atof(instr->p4)\n     vm->pc++\n     RETURN 0\n   CASE OP_String8:\n     validate_register(vm, instr->p2)\n     value_set_string_owned(&vm->registers[instr->p2], strdup(instr->p4), \n                             strlen(instr->p4))\n     vm->pc++\n     RETURN 0\n   CASE OP_Null:\n     validate_register(vm, instr->p2)\n     value_set_null(&vm->registers[instr->p2])\n     vm->pc++\n     RETURN 0\n   CASE OP_Copy:\n     validate_register(vm, instr->p1)\n     validate_register(vm, instr->p2)\n     value_copy(&vm->registers[instr->p2], &vm->registers[instr->p1])\n     vm->pc++\n     RETURN 0\n   // --- Cursor Operations ---\n   CASE OP_OpenRead:\n     validate_cursor_id(vm, instr->p1)\n     Cursor* cur = &vm->cursors[instr->p1]\n     cur->cursor_id = instr->p1\n     cur->root_page = instr->p2\n     cur->current_page = instr->p2\n     cur->current_cell = 0\n     cur->eof = false\n     cur->is_table = true\n     cur->table = (Table*)instr->p4\n     vm->pc++\n     RETURN 0\n   CASE OP_OpenWrite:\n     // Similar to OpenRead but marks cursor as writable\n     // ...\n   CASE OP_Close:\n     validate_cursor_id(vm, instr->p1)\n     // Release any resources held by cursor\n     vm->cursors[instr->p1].cursor_id = -1  // Mark as closed\n     vm->pc++\n     RETURN 0\n   CASE OP_Rewind:\n     validate_cursor_id(vm, instr->p1)\n     Cursor* cur = &vm->cursors[instr->p1]\n     // Navigate to leftmost leaf\n     result = cursor_rewind(vm->buffer_pool, cur)\n     IF result == CURSOR_EMPTY THEN\n       vm->pc = instr->p2  // Jump if empty\n     ELSE\n       vm->pc++\n     RETURN 0\n   CASE OP_Next:\n     validate_cursor_id(vm, instr->p1)\n     Cursor* cur = &vm->cursors[instr->p1]\n     result = cursor_next(vm->buffer_pool, cur)\n     IF result == CURSOR_EOF THEN\n       vm->pc++  // End of table\n     ELSE\n       vm->pc = instr->p2  // Jump back to loop body\n     RETURN 0\n   CASE OP_Column:\n     validate_cursor_id(vm, instr->p1)\n     validate_register(vm, instr->p3)\n     Cursor* cur = &vm->cursors[instr->p1]\n     // Fetch current row and extract column\n     Row* row = cursor_get_row(vm->buffer_pool, cur)\n     IF row == NULL THEN\n       value_set_null(&vm->registers[instr->p3])\n     ELSE\n       value_copy(&vm->registers[instr->p3], &row->columns[instr->p2])\n       free_row(row)\n     vm->pc++\n     RETURN 0\n   // --- Comparisons ---\n   CASE OP_Eq:\n   CASE OP_Ne:\n   CASE OP_Lt:\n   CASE OP_Le:\n   CASE OP_Gt:\n   CASE OP_Ge:\n     validate_register(vm, instr->p1)\n     validate_register(vm, instr->p2)\n     Value* left = &vm->registers[instr->p1]\n     Value* right = &vm->registers[instr->p2]\n     // Three-valued logic: NULL comparisons never trigger jump\n     IF left->type == VALUE_NULL OR right->type == VALUE_NULL THEN\n       vm->pc++  // Don't jump, treat NULL as \"condition not met\"\n       RETURN 0\n     int cmp = value_compare(left, right)\n     bool should_jump = false\n     SWITCH on instr->opcode:\n       CASE OP_Eq: should_jump = (cmp == 0)\n       CASE OP_Ne: should_jump = (cmp != 0)\n       CASE OP_Lt: should_jump = (cmp < 0)\n       CASE OP_Le: should_jump = (cmp <= 0)\n       CASE OP_Gt: should_jump = (cmp > 0)\n       CASE OP_Ge: should_jump = (cmp >= 0)\n     IF should_jump THEN\n       vm->pc = instr->p3\n     ELSE\n       vm->pc++\n     RETURN 0\n   // --- Arithmetic ---\n   CASE OP_Add:\n     validate_register(vm, instr->p1)\n     validate_register(vm, instr->p2)\n     validate_register(vm, instr->p3)\n     Value* a = &vm->registers[instr->p1]\n     Value* b = &vm->registers[instr->p2]\n     Value* result = &vm->registers[instr->p3]\n     // NULL propagation\n     IF a->type == VALUE_NULL OR b->type == VALUE_NULL THEN\n       value_set_null(result)\n     ELSE IF a->type == VALUE_INTEGER AND b->type == VALUE_INTEGER THEN\n       result->type = VALUE_INTEGER\n       result->data.integer_val = a->data.integer_val + b->data.integer_val\n     ELSE\n       result->type = VALUE_FLOAT\n       result->data.float_val = value_to_float(a) + value_to_float(b)\n     vm->pc++\n     RETURN 0\n   // Similar for Subtract, Multiply, Divide, Remainder...\n   // --- Output ---\n   CASE OP_ResultRow:\n     // Extract values from registers and invoke callback\n     int start = instr->p1\n     int count = instr->p2\n     IF vm->result_callback != NULL THEN\n       vm->result_callback(&vm->registers[start], count, vm->callback_user_data)\n     vm->pc++\n     RETURN 0\n   DEFAULT:\n     snprintf(vm->error_message, sizeof(vm->error_message),\n              \"unknown opcode: %d\", instr->opcode)\n     RETURN ERROR_UNKNOWN_OPCODE\n```\n\n![WHERE Clause Bytecode Pattern](./diagrams/tdd-diag-m3-4.svg)\n\n---\n## State Machine\nThe compiler has implicit states based on the statement being compiled:\n```\nSTATES:\n  IDLE              -- No compilation in progress\n  COMPILING_SELECT  -- Inside SELECT statement compilation\n  COMPILING_INSERT  -- Inside INSERT statement compilation\n  COMPILING_EXPR    -- Inside expression compilation\n  ERROR             -- Compilation failed\nTRANSITIONS:\n  IDLE --compile(AST_SELECT)--> COMPILING_SELECT\n  IDLE --compile(AST_INSERT)--> COMPILING_INSERT\n  COMPILING_SELECT --WHERE clause--> COMPILING_EXPR\n  COMPILING_SELECT --error--> ERROR\n  COMPILING_EXPR --return--> COMPILING_SELECT (or INSERT)\n  ERROR --reset--> IDLE\n  ANY --compiler_free--> IDLE\n```\nThe VM has explicit execution states:\n```\nSTATES:\n  VM_READY           -- Program loaded, ready to execute\n  VM_RUNNING         -- Executing instructions\n  VM_HALTED          -- Reached Halt opcode\n  VM_ERROR           -- Execution error occurred\nTRANSITIONS:\n  VM_READY --vdbe_execute()--> VM_RUNNING\n  VM_RUNNING --OP_Halt--> VM_HALTED\n  VM_RUNNING --error--> VM_ERROR\n  VM_HALTED --vdbe_reset()--> VM_READY\n  VM_ERROR --vdbe_reset()--> VM_READY\n```\n---\n## Error Handling Matrix\n| Error | Detected By | Recovery | User-Visible? |\n|-------|-------------|----------|---------------|\n| `UNKNOWN_TABLE` | `compile_select`, `compile_insert` | Set error, return error code | Yes: \"no such table: users at line 1, column 15\" |\n| `UNKNOWN_COLUMN` | `compile_identifier` | Set error, return error code | Yes: \"no such column: email at line 2, column 5\" |\n| `TYPE_MISMATCH` | `compile_binary_expr` | Set error, return error code | Yes: \"cannot add TEXT and INTEGER at line 3, column 10\" |\n| `UNKNOWN_OPERATOR` | `compile_binary_expr` | Set error, return error code | Yes: \"unknown operator: ^^ at line 1, column 20\" |\n| `REGISTER_OVERFLOW` | `allocate_register` | Set error, return error code | Yes: \"too many registers required (limit: 1000)\" |\n| `INVALID_JUMP_TARGET` | `patch_jump` | Set error (internal), return error code | Yes: \"invalid jump target: -1\" |\n| `CURSOR_NOT_OPEN` | VM opcode handlers | Set error, halt VM | Yes: \"cursor 0 is not open\" |\n| `UNKNOWN_OPCODE` | `execute_instruction` | Set error, halt VM | Yes: \"unknown opcode: 999\" |\n| `NO_HALT` | `vdbe_execute` | Set error | Yes: \"program does not end with Halt\" |\n| `ALLOCATION_FAILURE` | Various | Set error, return error code | Yes: \"memory allocation failed\" |\n---\n## Implementation Sequence with Checkpoints\n### Phase 1: Instruction Set Definition (1 hour)\n**Files to create**: `opcode.h`, `instruction.h`, `value.h`\n**Implementation**:\n1. Define `OpCode` enum with all opcodes.\n2. Define `Instruction` struct with p1-p5 operands.\n3. Define `ValueType` enum and `Value` union.\n4. Document memory layout and constraints for each.\n**Checkpoint**: Headers compile, struct sizes verified.\n```bash\ngcc -c opcode.h instruction.h value.h -o /dev/null\n# Verify sizeof(Instruction) == 32, sizeof(Value) == 32\n```\n### Phase 2: Compiler Framework with emit() (1 hour)\n**Files to create**: `compiler.h`, `compiler.c`, `jump_resolver.h`, `jump_resolver.c`\n**Implementation**:\n1. Define `Compiler` struct with code array and allocation counters.\n2. Implement `compiler_create`, `compiler_free`.\n3. Implement `emit` function with array growth.\n4. Implement `allocate_register`, `allocate_cursor`.\n5. Implement `emit_jump_placeholder` and `patch_jump`.\n**Checkpoint**: Can emit instructions and patch jumps.\n```bash\n./test_compiler_emit\n# Emit 10 instructions, patch 3 jumps, verify targets\n```\n### Phase 3: SELECT Compilation (Table Scan) (2 hours)\n**Files to create**: `compile_select.c`\n**Implementation**:\n1. Implement `compile_select` main function.\n2. Implement `compile_column_list` for * and named columns.\n3. Integrate expression compilation for column values.\n4. Handle OpenRead, Rewind, Column, ResultRow, Next, Close sequence.\n**Checkpoint**: SELECT * FROM table compiles to valid bytecode.\n```bash\n./test_compiler_select\n# \"SELECT * FROM users\" \u2192 bytecode with OpenRead, Rewind, Column, ResultRow, Next, Close, Halt\n```\n### Phase 4: WHERE Clause to Conditional Jumps (2 hours)\n**Files to create**: `compile_expression.c` (extend)\n**Implementation**:\n1. Implement `compile_where_clause` with inverted comparisons.\n2. Implement comparison opcodes (Eq, Ne, Lt, Le, Gt, Ge).\n3. Handle AND/OR chaining with multiple conditional jumps.\n4. Handle NULL comparisons (never trigger jump).\n**Checkpoint**: WHERE clauses compile to conditional jumps.\n```bash\n./test_compiler_where\n# \"SELECT * FROM users WHERE age > 18\" \u2192 bytecode with Gt jump\n# \"SELECT * FROM users WHERE a = 1 AND b = 2\" \u2192 two jumps chained\n```\n### Phase 5: INSERT Compilation (1.5 hours)\n**Files to create**: `compile_insert.c`\n**Implementation**:\n1. Implement `compile_insert` main function.\n2. Implement value expression compilation.\n3. Handle MakeRecord, NewRowid, Insert sequence.\n4. Support multi-row INSERT.\n**Checkpoint**: INSERT statements compile correctly.\n```bash\n./test_compiler_insert\n# \"INSERT INTO users VALUES (1, 'Alice')\" \u2192 bytecode with OpenWrite, Integer, String8, MakeRecord, Insert\n```\n### Phase 6: VM Fetch-Decode-Execute Loop (1.5 hours)\n**Files to create**: `vm.c`, `cursor.c`, `value.c`\n**Implementation**:\n1. Implement `vdbe_create`, `vdbe_free`, `vdbe_load_program`.\n2. Implement main execution loop with switch statement.\n3. Implement all opcode handlers (start with Halt, Goto, Integer, String8, Null).\n4. Implement cursor operations (OpenRead, Rewind, Column, Next, Close).\n5. Implement comparison opcodes with three-valued logic.\n6. Implement arithmetic opcodes with NULL propagation.\n7. Implement ResultRow with callback invocation.\n**Checkpoint**: VM executes simple programs.\n```bash\n./test_vm_execute\n# Execute \"SELECT 42\" \u2192 ResultRow callback receives 42\n# Execute \"SELECT * FROM users\" on 10-row table \u2192 10 callback invocations\n```\n### Phase 7: EXPLAIN Command Output (1 hour)\n**Files to create**: `explain.c`\n**Implementation**:\n1. Implement `opcode_name` lookup function.\n2. Implement `explain_program` formatting.\n3. Handle all operand formats (integers, strings, pointers).\n**Checkpoint**: EXPLAIN produces readable output.\n```bash\n./test_explain\n# EXPLAIN SELECT * FROM users \u2192 formatted bytecode listing\n```\n---\n## Test Specification\n### Compiler Emission Tests\n```c\nvoid test_emit_single_instruction() {\n    Compiler* c = compiler_create(NULL);\n    int addr = emit(c, OP_Integer, 42, 0, 0, NULL);\n    assert(addr == 0);\n    assert(c->code_length == 1);\n    assert(c->code[0].opcode == OP_Integer);\n    assert(c->code[0].p1 == 42);\n    compiler_free(c);\n}\nvoid test_emit_multiple_instructions() {\n    Compiler* c = compiler_create(NULL);\n    emit(c, OP_Integer, 1, 0, 0, NULL);\n    emit(c, OP_Integer, 2, 1, 0, NULL);\n    emit(c, OP_Add, 0, 1, 2, NULL);\n    assert(c->code_length == 3);\n    compiler_free(c);\n}\nvoid test_jump_patching() {\n    Compiler* c = compiler_create(NULL);\n    int jump_addr = emit(c, OP_Goto, 0, -1, 0, NULL);  // Placeholder\n    emit(c, OP_Integer, 42, 0, 0, NULL);\n    int target = c->code_length;\n    emit(c, OP_Halt, 0, 0, 0, NULL);\n    patch_jump(c, jump_addr, target);\n    assert(c->code[jump_addr].p2 == target);\n    compiler_free(c);\n}\n```\n### SELECT Compilation Tests\n```c\nvoid test_compile_select_star() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE users (id INTEGER, name TEXT)\");\n    Compiler* c = compiler_create(db);\n    TokenStream* tokens = tokenize(\"SELECT * FROM users\");\n    ASTNode* ast = parse(tokens);\n    int result = compiler_compile(c, ast);\n    assert(result == 0);\n    assert(c->code_length > 0);\n    // Verify opcode sequence\n    assert(has_opcode(c, OP_OpenRead));\n    assert(has_opcode(c, OP_Rewind));\n    assert(has_opcode(c, OP_Column));\n    assert(has_opcode(c, OP_ResultRow));\n    assert(has_opcode(c, OP_Next));\n    assert(has_opcode(c, OP_Close));\n    assert(c->code[c->code_length - 1].opcode == OP_Halt);\n    compiler_free(c);\n    ast_free(ast);\n    free_token_stream(tokens);\n    free_database(db);\n}\nvoid test_compile_select_with_where() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, value INTEGER)\");\n    Compiler* c = compiler_create(db);\n    ASTNode* ast = parse_sql(\"SELECT * FROM test WHERE value > 10\");\n    int result = compiler_compile(c, ast);\n    assert(result == 0);\n    // Verify comparison opcode\n    assert(has_opcode(c, OP_Gt));\n    // Verify conditional jump\n    int gt_addr = find_opcode(c, OP_Gt);\n    assert(c->code[gt_addr].p3 > gt_addr);  // Jump forward\n    compiler_free(c);\n    ast_free(ast);\n    free_database(db);\n}\n```\n### INSERT Compilation Tests\n```c\nvoid test_compile_insert() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, name TEXT)\");\n    Compiler* c = compiler_create(db);\n    ASTNode* ast = parse_sql(\"INSERT INTO test VALUES (1, 'Alice')\");\n    int result = compiler_compile(c, ast);\n    assert(result == 0);\n    assert(has_opcode(c, OP_OpenWrite));\n    assert(has_opcode(c, OP_Integer));\n    assert(has_opcode(c, OP_String8));\n    assert(has_opcode(c, OP_MakeRecord));\n    assert(has_opcode(c, OP_NewRowid));\n    assert(has_opcode(c, OP_Insert));\n    assert(has_opcode(c, OP_Close));\n    assert(last_opcode(c) == OP_Halt);\n    compiler_free(c);\n    ast_free(ast);\n    free_database(db);\n}\n```\n### VM Execution Tests\n```c\nvoid test_vm_integer_load() {\n    VDBE* vm = vdbe_create(NULL, NULL);\n    Instruction code[] = {\n        {OP_Integer, 42, 0, 0, NULL, 0},\n        {OP_Halt, 0, 0, 0, NULL, 0}\n    };\n    vdbe_load_program(vm, code, 2, 10);\n    int result = vdbe_execute(vm);\n    assert(result == 0);\n    assert(vm->registers[0].type == VALUE_INTEGER);\n    assert(vm->registers[0].data.integer_val == 42);\n    vdbe_free(vm);\n}\nvoid test_vm_addition() {\n    VDBE* vm = vdbe_create(NULL, NULL);\n    Instruction code[] = {\n        {OP_Integer, 5, 0, 0, NULL, 0},\n        {OP_Integer, 3, 1, 0, NULL, 0},\n        {OP_Add, 0, 1, 2, NULL, 0},\n        {OP_Halt, 0, 0, 0, NULL, 0}\n    };\n    vdbe_load_program(vm, code, 4, 10);\n    int result = vdbe_execute(vm);\n    assert(result == 0);\n    assert(vm->registers[2].type == VALUE_INTEGER);\n    assert(vm->registers[2].data.integer_val == 8);\n    vdbe_free(vm);\n}\nvoid test_vm_comparison_jump() {\n    VDBE* vm = vdbe_create(NULL, NULL);\n    // if (5 > 3) goto 4 else halt\n    Instruction code[] = {\n        {OP_Integer, 5, 0, 0, NULL, 0},\n        {OP_Integer, 3, 1, 0, NULL, 0},\n        {OP_Gt, 0, 1, 4, NULL, 0},  // Jump to instruction 4 if 5 > 3\n        {OP_Integer, 0, 2, 0, NULL, 0},  // Should skip\n        {OP_Integer, 1, 2, 0, NULL, 0},  // Target: set r2 = 1\n        {OP_Halt, 0, 0, 0, NULL, 0}\n    };\n    vdbe_load_program(vm, code, 6, 10);\n    int result = vdbe_execute(vm);\n    assert(result == 0);\n    assert(vm->registers[2].data.integer_val == 1);  // Jumped\n    vdbe_free(vm);\n}\nvoid test_vm_null_comparison() {\n    VDBE* vm = vdbe_create(NULL, NULL);\n    // NULL > 3 should NOT trigger jump (three-valued logic)\n    Instruction code[] = {\n        {OP_Null, 0, 0, 0, NULL, 0},\n        {OP_Integer, 3, 1, 0, NULL, 0},\n        {OP_Gt, 0, 1, 5, NULL, 0},  // Should NOT jump\n        {OP_Integer, 0, 2, 0, NULL, 0},  // r2 = 0 (reached)\n        {OP_Halt, 0, 0, 0, NULL, 0},\n        {OP_Integer, 1, 2, 0, NULL, 0},  // r2 = 1 (should not reach)\n        {OP_Halt, 0, 0, 0, NULL, 0}\n    };\n    vdbe_load_program(vm, code, 7, 10);\n    int result = vdbe_execute(vm);\n    assert(result == 0);\n    assert(vm->registers[2].data.integer_val == 0);  // Did not jump\n    vdbe_free(vm);\n}\n```\n### EXPLAIN Output Tests\n```c\nvoid test_explain_format() {\n    Instruction code[] = {\n        {OP_Integer, 42, 0, 0, NULL, 0},\n        {OP_String8, 0, 1, 0, \"hello\", 0},\n        {OP_Halt, 0, 0, 0, NULL, 0}\n    };\n    char* output = capture_explain_output(code, 3);\n    assert(strstr(output, \"Integer\") != NULL);\n    assert(strstr(output, \"String8\") != NULL);\n    assert(strstr(output, \"hello\") != NULL);\n    assert(strstr(output, \"Halt\") != NULL);\n    free(output);\n}\n```\n\n![Register Allocation During Compilation](./diagrams/tdd-diag-m3-5.svg)\n\n---\n## Performance Targets\n| Operation | Target | How to Measure |\n|-----------|--------|----------------|\n| Compile 100-statement batch | < 10ms | Benchmark with diverse statement types |\n| Execute SELECT * FROM 10K-row table | < 100ms | In-memory table, no disk I/O |\n| Register allocation | O(1) per expression | Counter increment |\n| Jump resolution | O(1) per patch | Direct array access |\n| VM instruction dispatch | < 100ns per instruction | Profile hot loop |\n**Benchmark methodology**:\n```c\nvoid benchmark_select_10k_rows() {\n    Database* db = create_test_database();\n    execute_sql(db, \"CREATE TABLE test (id INTEGER, value TEXT)\");\n    for (int i = 0; i < 10000; i++) {\n        char sql[100];\n        snprintf(sql, sizeof(sql), \"INSERT INTO test VALUES (%d, 'value%d')\", i, i);\n        execute_sql(db, sql);\n    }\n    // Compile\n    Compiler* c = compiler_create(db);\n    ASTNode* ast = parse_sql(\"SELECT * FROM test\");\n    clock_t compile_start = clock();\n    compiler_compile(c, ast);\n    clock_t compile_end = clock();\n    // Execute\n    Instruction* code = compiler_get_code(c, &code_length);\n    VDBE* vm = vdbe_create(db, db->buffer_pool);\n    vdbe_load_program(vm, code, code_length, 100);\n    int row_count = 0;\n    vdbe_set_callback(vm, count_rows_callback, &row_count);\n    clock_t exec_start = clock();\n    vdbe_execute(vm);\n    clock_t exec_end = clock();\n    double compile_ms = (compile_end - compile_start) * 1000.0 / CLOCKS_PER_SEC;\n    double exec_ms = (exec_end - exec_start) * 1000.0 / CLOCKS_PER_SEC;\n    printf(\"Compile: %.2f ms, Execute: %.2f ms, Rows: %d\\n\", \n           compile_ms, exec_ms, row_count);\n    assert(exec_ms < 100.0);\n    assert(row_count == 10000);\n    vdbe_free(vm);\n    free(code);\n    compiler_free(c);\n    ast_free(ast);\n    free_database(db);\n}\n```\n\n![VM Execution Loop State Machine](./diagrams/tdd-diag-m3-6.svg)\n\n---\n[[CRITERIA_JSON: {\"module_id\": \"build-sqlite-m3\", \"criteria\": [\"Compiler defines complete instruction set with opcodes for table operations (OpenRead, OpenWrite, Close), cursor movement (Rewind, Next, Prev), data access (Column, MakeRecord, Insert, Delete), arithmetic (Add, Subtract, Multiply, Divide), comparisons (Eq, Ne, Lt, Le, Gt, Ge), control flow (Goto, Halt, Gosub, Return), and value loading (Integer, String8, Null, Copy, SCopy)\", \"Instruction struct contains opcode (enum), p1-p3 (int operands), p4 (char* for strings/pointers), p5 (int flags)\", \"Value union supports types: VALUE_NULL, VALUE_INTEGER, VALUE_FLOAT, VALUE_STRING, VALUE_BLOB with owned flag for memory management\", \"Cursor struct tracks cursor_id, root_page, current_page, current_cell, eof flag, is_table flag, and table/index pointers\", \"VDBE struct contains code array, register file, cursor array, program counter, halted flag, error state, database reference, and result callback\", \"Compiler struct contains code array, register/cursor allocation counters, pending jump list, symbol table reference, and error state\", \"Compiler implements emit() function that appends instructions to growable code array and returns instruction address\", \"Compiler implements allocate_register() returning monotonically increasing register numbers\", \"Compiler implements allocate_cursor() returning unique cursor identifiers\", \"Compiler implements emit_jump_placeholder() and patch_jump() for forward jump resolution\", \"SELECT compilation produces bytecode sequence: OpenRead \u2192 Rewind \u2192 Column opcodes \u2192 ResultRow \u2192 Next (looping) \u2192 Close \u2192 Halt\", \"INSERT compilation produces bytecode sequence: OpenWrite \u2192 value load instructions \u2192 MakeRecord \u2192 NewRowid \u2192 Insert \u2192 Close \u2192 Halt\", \"WHERE clause expressions compile to conditional jump opcodes with inverted comparisons (e.g., age > 18 compiles to Le jump-if-age<=18)\", \"AND expressions in WHERE compile to chained conditional jumps (if first condition false, skip to Next)\", \"OR expressions in WHERE compile to short-circuit logic with appropriate jump targets\", \"Expression compilation returns register number containing result value\", \"Literal expressions compile to Integer, Real, String8, Null opcodes loading values into registers\", \"Identifier expressions compile to Column opcode fetching from cursor\", \"Binary arithmetic expressions compile to Add, Subtract, Multiply, Divide opcodes with result in specified register\", \"VM implements fetch-decode-execute loop: while (!halted && pc < code_length) { execute_instruction(code[pc]); }\", \"VM comparison opcodes implement three-valued logic: NULL compared to any value does NOT trigger the jump\", \"VM arithmetic opcodes handle NULL propagation: NULL + value = NULL, and type coercion (INTEGER + INTEGER \u2192 INTEGER, mixed \u2192 FLOAT)\", \"EXPLAIN command outputs human-readable bytecode with columns: addr, opcode, p1, p2, p3, p4\", \"EXPLAIN output shows jump targets as valid instruction addresses\", \"Bytecode execution of SELECT * FROM table on 10,000-row table completes in under 100ms\", \"VM correctly handles empty tables: Rewind detects EOF and jumps past loop body without executing ResultRow\", \"ResultRow instruction extracts values from consecutive registers and invokes callback\", \"Test suite validates opcode execution in isolation (Integer sets register, Add produces correct sum, comparisons jump correctly)\", \"Test suite validates compiler output for SELECT, INSERT, and WHERE constructs (expected opcodes and jump targets)\", \"Test suite validates NULL handling in comparisons (NULL comparisons never trigger jumps)\", \"Test suite validates jump target patching (forward jumps resolve to correct addresses)\", \"Compiler reports unknown table with descriptive error including table name and position\", \"Compiler reports unknown column with descriptive error including column name and position\"]}] ]\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m4 -->\n# Buffer Pool Manager: Technical Design Specification\n## Module Charter\nThe Buffer Pool Manager is the memory-disk mediation layer that caches fixed-size database pages in memory, providing transparent page access to upper layers while managing eviction, dirty page tracking, and durability guarantees. It implements an LRU (Least Recently Used) eviction policy with pin counting to prevent eviction of pages actively in use by B-tree operations. The buffer pool does NOT perform disk I/O directly\u2014it delegates to the FileManager abstraction\u2014nor does it understand page contents (that's the B-tree layer's job). It only knows page IDs, frame occupancy, and access recency.\n**Upstream dependencies**: B-tree layer requests pages; FileManager provides disk I/O.\n**Downstream dependencies**: None\u2014the buffer pool is a leaf dependency consumed by storage layers.\n**Invariants**:\n1. A page is resident in at most one frame at any time (no duplicates).\n2. A frame with `pin_count > 0` is never selected for eviction.\n3. A dirty page is written to disk before eviction (write-back ordering).\n4. The page_table hash map is always consistent with frame occupancy.\n5. LRU list contains exactly the unpinned pages, in access recency order.\n6. All pointer dereferences are bounds-checked against frame array capacity.\n---\n## File Structure\nCreate files in this order:\n```\n1. src/buffer/frame.h           -- Frame struct definition with all fields\n2. src/buffer/frame.c           -- Frame initialization and cleanup utilities\n3. src/buffer/page_table.h      -- Hash table interface for PageId \u2192 Frame* lookup\n4. src/buffer/page_table.c      -- Hash table implementation with chaining\n5. src/buffer/lru_list.h        -- Doubly-linked LRU list interface\n6. src/buffer/lru_list.c        -- LRU list manipulation (move_to_head, remove, etc.)\n7. src/buffer/buffer_pool.h     -- BufferPool struct and public API\n8. src/buffer/buffer_pool.c     -- Core buffer pool implementation\n9. src/buffer/fetch_page.c      -- FetchPage hit/miss logic\n10. src/buffer/eviction.c       -- LRU eviction with dirty write-back\n11. src/buffer/pin_unpin.c      -- Pin/Unpin operations with validation\n12. src/buffer/flush.c          -- FlushAll and selective flush operations\n13. src/buffer/statistics.c     -- Hit rate and performance metrics\n14. tests/test_buffer_pool.c    -- Comprehensive test suite\n15. tests/test_lru_list.c       -- LRU list unit tests\n16. tests/test_page_table.c     -- Hash table unit tests\n```\n---\n## Complete Data Model\n### Frame Structure\n```c\n// File: src/buffer/frame.h\ntypedef struct Frame {\n    PageId page_id;           // Which disk page is in this frame; -1 if empty\n    char data[PAGE_SIZE];     // The actual page data (4096 bytes)\n    int pin_count;            // Number of active references (\u2265 0)\n    bool is_dirty;            // Has this page been modified?\n    // LRU list pointers (intrusive doubly-linked list)\n    struct Frame* lru_prev;   // Previous in LRU order (NULL if head or not in list)\n    struct Frame* lru_next;   // Next in LRU order (NULL if tail or not in list)\n    // Index in frame array (for reverse lookup)\n    int frame_index;\n} Frame;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       4     page_id (int32_t)\n// 4       4     padding\n// 8       4096  data[PAGE_SIZE]\n// 4104    4     pin_count\n// 4108    1     is_dirty\n// 4109    3     padding\n// 4112    8     lru_prev (pointer)\n// 4120    8     lru_next (pointer)\n// 4128    4     frame_index\n// 4132    4     padding\n// Total: 4136 bytes per frame (aligned to 8)\n```\n**Field justification**:\n- `page_id`: Required to identify which page is cached. -1 (or INVALID_PAGE_ID) indicates empty frame.\n- `data[PAGE_SIZE]`: The cached page content. Fixed 4096 bytes matches disk block size and OS page size.\n- `pin_count`: Critical for safety\u2014prevents eviction during active use. Must be \u2265 0 at all times.\n- `is_dirty`: Required for write-back caching. Dirty pages must be written before eviction.\n- `lru_prev/lru_next`: Intrusive list pointers for O(1) LRU manipulation. Frame is always in LRU list when `pin_count == 0` and `page_id != INVALID_PAGE_ID`.\n- `frame_index`: Enables O(1) removal from page_table when we have Frame pointer but need to update table.\n### PageId Type\n```c\n// File: src/buffer/types.h\ntypedef int32_t PageId;\n#define INVALID_PAGE_ID (-1)\n#define FIRST_VALID_PAGE_ID (1)  // Page 0 reserved for database header\n```\n### Page Table Entry\n```c\n// File: src/buffer/page_table.h\ntypedef struct PageTableEntry {\n    PageId page_id;\n    Frame* frame;\n    struct PageTableEntry* next;  // Chaining for collision resolution\n} PageTableEntry;\n// Memory layout (64-bit system):\n// Offset  Size  Field\n// 0       4     page_id\n// 4       4     padding\n// 8       8     frame (pointer)\n// 16      8     next (pointer)\n// Total: 24 bytes per entry\n```\n### Page Table (Hash Table)\n```c\n// File: src/buffer/page_table.h\ntypedef struct {\n    PageTableEntry** buckets;   // Array of bucket heads\n    int bucket_count;           // Number of buckets (power of 2)\n    int entry_count;            // Total entries (for load factor calculation)\n} PageTable;\n```\n### LRU List\n```c\n// File: src/buffer/lru_list.h\ntypedef struct {\n    Frame* head;                // Most recently used (MRU)\n    Frame* tail;                // Least recently used (LRU) \u2014 eviction candidate\n    int count;                  // Number of frames in list\n} LRUList;\n```\n### Buffer Pool Structure\n```c\n// File: src/buffer/buffer_pool.h\ntypedef struct {\n    // Frame storage\n    Frame* frames;              // Array of frames (contiguous allocation)\n    int frame_count;            // Number of frames (capacity)\n    int page_size;              // Page size (default 4096)\n    // Page ID \u2192 Frame lookup\n    PageTable* page_table;      // Hash table for O(1) lookup\n    // LRU eviction\n    LRUList* lru_list;          // List of unpinned pages\n    // Statistics\n    int64_t hits;               // Cache hits\n    int64_t misses;             // Cache misses (disk reads)\n    int64_t evictions;          // Pages evicted\n    int64_t dirty_writes;       // Dirty pages written to disk\n    // Disk I/O (delegated)\n    FileManager* file_manager;  // Disk I/O abstraction\n    // Configuration\n    int max_pin_count;          // Maximum pin count per frame (sanity limit)\n    // Error state\n    int last_error;\n    char error_message[256];\n} BufferPool;\n// Memory layout (64-bit system):\n// Offset   Size    Field\n// 0        8       frames (pointer)\n// 8        4       frame_count\n// 12       4       page_size\n// 16       8       page_table (pointer)\n// 24       8       lru_list (pointer)\n// 32       8       hits\n// 40       8       misses\n// 48       8       evictions\n// 56       8       dirty_writes\n// 64       8       file_manager (pointer)\n// 72       4       max_pin_count\n// 76       4       last_error\n// 80       256     error_message\n// Total: 336 bytes (base structure, excluding variable-size arrays)\n```\n\n![Buffer Pool Internal Structure](./diagrams/tdd-diag-m4-1.svg)\n\n### Statistics Structure\n```c\n// File: src/buffer/statistics.h\ntypedef struct {\n    int64_t hits;\n    int64_t misses;\n    int64_t evictions;\n    int64_t dirty_writes;\n    double hit_rate;            // Computed: hits / (hits + misses)\n    int pinned_count;           // Current pinned frames\n    int dirty_count;            // Current dirty frames\n    int resident_count;         // Frames with valid page_id\n} BufferPoolStats;\n```\n---\n## Interface Contracts\n### Frame Lifecycle\n```c\n// File: src/buffer/frame.c\n// Initialize frame to empty state\nvoid frame_init(Frame* frame, int frame_index);\n// Constraints: frame must not be NULL, frame_index >= 0\n// Postconditions: page_id = INVALID_PAGE_ID, pin_count = 0, is_dirty = false, lru_prev = lru_next = NULL\n// Clear frame (prepare for reuse after eviction)\nvoid frame_clear(Frame* frame);\n// Constraints: frame must not be NULL, pin_count must be 0\n// Postconditions: page_id = INVALID_PAGE_ID, is_dirty = false, removed from LRU list\n// Does NOT free frame->data (it's inline in struct)\n// Validate frame state\nbool frame_is_valid(const Frame* frame);\n// Returns: true if frame contains valid page data (page_id != INVALID_PAGE_ID)\nbool frame_is_pinned(const Frame* frame);\n// Returns: true if pin_count > 0\nbool frame_is_dirty(const Frame* frame);\n// Returns: true if is_dirty is true\n```\n### Page Table Operations\n```c\n// File: src/buffer/page_table.c\n// Create page table with specified bucket count\nPageTable* page_table_create(int bucket_count);\n// Constraints: bucket_count > 0, must be power of 2 for efficient modulo\n// Returns: newly allocated page table with NULL buckets\n// Free page table and all entries\nvoid page_table_free(PageTable* table);\n// Constraints: table may be NULL (no-op)\n// Side effects: frees all PageTableEntry objects, does NOT free frames\n// Look up frame by page ID\nFrame* page_table_lookup(const PageTable* table, PageId page_id);\n// Constraints: table must not be NULL\n// Returns: Frame* if found, NULL if not found\n// Time complexity: O(1) average case\n// Insert page ID \u2192 frame mapping\nint page_table_insert(PageTable* table, PageId page_id, Frame* frame);\n// Constraints: table and frame must not be NULL, page_id must be valid\n// Returns: 0 on success, ERROR_DUPLICATE if page_id already exists\n// Side effects: allocates new PageTableEntry\n// Time complexity: O(1) average case\n// Remove page ID mapping\nint page_table_remove(PageTable* table, PageId page_id);\n// Constraints: table must not be NULL\n// Returns: 0 on success, ERROR_NOT_FOUND if page_id not in table\n// Side effects: frees PageTableEntry\n// Time complexity: O(1) average case\n// Get entry count\nint page_table_count(const PageTable* table);\n// Returns: number of entries in table\n```\n### LRU List Operations\n```c\n// File: src/buffer/lru_list.c\n// Create empty LRU list\nLRUList* lru_list_create(void);\n// Returns: newly allocated LRU list with head = tail = NULL, count = 0\n// Free LRU list (does NOT free frames)\nvoid lru_list_free(LRUList* list);\n// Constraints: list may be NULL (no-op)\n// Add frame to head (most recently used)\nvoid lru_list_push_head(LRUList* list, Frame* frame);\n// Constraints: list and frame must not be NULL, frame must not already be in a list\n// Postconditions: frame->lru_prev = NULL, frame->lru_next = old head\n// Remove frame from list\nvoid lru_list_remove(LRUList* list, Frame* frame);\n// Constraints: list and frame must not be NULL, frame must be in this list\n// Postconditions: frame->lru_prev = frame->lru_next = NULL\n// Move frame to head (for access)\nvoid lru_list_move_to_head(LRUList* list, Frame* frame);\n// Constraints: list and frame must not be NULL, frame must be in this list\n// Equivalent to: remove(list, frame); push_head(list, frame)\n// Get tail (least recently used)\nFrame* lru_list_peek_tail(LRUList* list);\n// Constraints: list must not be NULL\n// Returns: least recently used frame, or NULL if list empty\n// Remove and return tail\nFrame* lru_list_pop_tail(LRUList* list);\n// Constraints: list must not be NULL\n// Returns: least recently used frame (removed from list), or NULL if empty\n```\n### Buffer Pool Lifecycle\n```c\n// File: src/buffer/buffer_pool.h\n// Create buffer pool with specified configuration\nBufferPool* buffer_pool_create(int frame_count, int page_size, FileManager* fm);\n// Constraints: frame_count > 0, page_size > 0 (typically 4096), fm must not be NULL\n// Returns: newly allocated buffer pool with empty frames\n// Side effects: allocates frame array, page table, LRU list\n// Free buffer pool and all resources\nvoid buffer_pool_free(BufferPool* pool);\n// Constraints: pool may be NULL (no-op)\n// Side effects: flushes all dirty pages, frees frames, page table, LRU list\n// Does NOT free FileManager (not owned)\n// Get buffer pool statistics\nBufferPoolStats buffer_pool_get_stats(const BufferPool* pool);\n// Constraints: pool must not be NULL\n// Returns: snapshot of current statistics\n```\n### Core Operations\n```c\n// File: src/buffer/buffer_pool.h\n// Fetch a page (load from disk if not cached)\nFrame* buffer_pool_fetch_page(BufferPool* pool, PageId page_id);\n// Constraints: pool must not be NULL, page_id must be valid (> 0)\n// Returns: Frame* containing page data, or NULL on error\n// Postconditions: frame->pin_count incremented by 1, frame moved to LRU head\n// On cache miss: may trigger eviction if no free frames\n// On error: pool->last_error is set, pool->error_message contains details\n// Thread safety: NOT thread-safe in basic implementation\n// Release a page (decrement pin count)\nvoid buffer_pool_unpin_page(BufferPool* pool, Frame* frame);\n// Constraints: pool and frame must not be NULL, frame->pin_count must be > 0\n// Postconditions: frame->pin_count decremented by 1\n// If pin_count becomes 0: frame added to LRU list (if not already there)\n// On error: logs warning (pin_count underflow)\n// Pin a page (increment pin count)\nvoid buffer_pool_pin_page(BufferPool* pool, Frame* frame);\n// Constraints: pool and frame must not be NULL\n// Postconditions: frame->pin_count incremented by 1\n// If pin_count was 0: frame removed from LRU list (pinned pages not evictable)\n// Mark a page as dirty\nvoid buffer_pool_mark_dirty(BufferPool* pool, Frame* frame);\n// Constraints: pool and frame must not be NULL\n// Postconditions: frame->is_dirty = true\n// Flush all dirty pages to disk\nint buffer_pool_flush_all(BufferPool* pool);\n// Constraints: pool must not be NULL\n// Returns: 0 on success, error code on failure\n// Side effects: writes all dirty pages, calls fsync, clears is_dirty flags\n// Does NOT unpin pages or evict\n// Flush a specific page\nint buffer_pool_flush_page(BufferPool* pool, Frame* frame);\n// Constraints: pool and frame must not be NULL\n// Returns: 0 on success, error code on failure\n// Side effects: if dirty, writes page to disk, clears is_dirty\n```\n### Error Codes\n```c\n// File: src/buffer/errors.h\ntypedef enum {\n    BUFFER_POOL_OK = 0,\n    BUFFER_POOL_ERROR_NULL_ARGUMENT,\n    BUFFER_POOL_ERROR_INVALID_PAGE_ID,\n    BUFFER_POOL_ERROR_EXHAUSTED,          // All frames pinned\n    BUFFER_POOL_ERROR_PIN_COUNT_UNDERFLOW,\n    BUFFER_POOL_ERROR_PIN_COUNT_OVERFLOW,\n    BUFFER_POOL_ERROR_DISK_READ_FAILED,\n    BUFFER_POOL_ERROR_DISK_WRITE_FAILED,\n    BUFFER_POOL_ERROR_FSYNC_FAILED,\n    BUFFER_POOL_ERROR_OUT_OF_MEMORY,\n    BUFFER_POOL_ERROR_DUPLICATE_PAGE,     // Page already resident\n} BufferPoolError;\n```\n---\n## Algorithm Specification\n### FetchPage: Hit Path\n```\nALGORITHM: buffer_pool_fetch_page (cache hit)\nINPUT: BufferPool* pool, PageId page_id\nOUTPUT: Frame* (pinned frame containing page data) or NULL on error\nINVARIANTS:\n  - page_table accurately reflects resident pages\n  - LRU list contains all unpinned resident pages\nPROCEDURE:\n1. // Validate inputs\n   IF pool == NULL THEN\n     RETURN NULL  // Cannot set error without pool\n   IF page_id <= 0 THEN\n     pool->last_error = BUFFER_POOL_ERROR_INVALID_PAGE_ID\n     snprintf(pool->error_message, ..., \"invalid page id: %d\", page_id)\n     RETURN NULL\n2. // Look up page in page table\n   Frame* frame = page_table_lookup(pool->page_table, page_id)\n3. // Cache miss: delegate to miss handler\n   IF frame == NULL THEN\n     RETURN buffer_pool_fetch_page_miss(pool, page_id)\n4. // Cache hit: update statistics and state\n   pool->hits++\n5. // Move to LRU head (even if pinned, for consistency)\n   IF frame->pin_count == 0 THEN\n     lru_list_move_to_head(pool->lru_list, frame)\n   // Note: If pinned, frame is not in LRU list, so no move needed\n6. // Increment pin count\n   frame->pin_count++\n   IF frame->pin_count > pool->max_pin_count THEN\n     pool->last_error = BUFFER_POOL_ERROR_PIN_COUNT_OVERFLOW\n     snprintf(pool->error_message, ..., \"pin count overflow for page %d\", page_id)\n     frame->pin_count--  // Rollback\n     RETURN NULL\n7. RETURN frame\n```\n### FetchPage: Miss Path\n```\nALGORITHM: buffer_pool_fetch_page_miss\nINPUT: BufferPool* pool, PageId page_id\nOUTPUT: Frame* (pinned frame with loaded page) or NULL on error\nPROCEDURE:\n1. // Update statistics\n   pool->misses++\n2. // Find a free frame or evict\n   Frame* frame = find_free_frame(pool)\n   IF frame == NULL THEN\n     frame = evict_frame(pool)\n     IF frame == NULL THEN\n       pool->last_error = BUFFER_POOL_ERROR_EXHAUSTED\n       snprintf(pool->error_message, ..., \"buffer pool exhausted: all %d frames pinned\", \n                pool->frame_count)\n       RETURN NULL\n3. // At this point, frame is either:\n   // - Empty (page_id == INVALID_PAGE_ID)\n   // - Just evicted (page_id still has old value, removed from page_table)\n   // If frame was evicted, it's already removed from page_table\n   // Clear frame state\n   frame->page_id = INVALID_PAGE_ID\n   frame->is_dirty = false\n   frame->lru_prev = NULL\n   frame->lru_next = NULL\n4. // Load page from disk\n   int result = file_manager_read_page(pool->file_manager, page_id, frame->data)\n   IF result != 0 THEN\n     pool->last_error = BUFFER_POOL_ERROR_DISK_READ_FAILED\n     snprintf(pool->error_message, ..., \"failed to read page %d from disk\", page_id)\n     RETURN NULL\n5. // Update frame metadata\n   frame->page_id = page_id\n   frame->pin_count = 1\n6. // Add to page table\n   result = page_table_insert(pool->page_table, page_id, frame)\n   IF result != 0 THEN\n     // This should not happen (we just evicted/emptied the frame)\n     pool->last_error = BUFFER_POOL_ERROR_DUPLICATE_PAGE\n     frame->page_id = INVALID_PAGE_ID\n     RETURN NULL\n7. RETURN frame\nALGORITHM: find_free_frame\nINPUT: BufferPool* pool\nOUTPUT: Frame* (empty frame) or NULL if no free frames\nPROCEDURE:\n1. FOR i = 0 TO pool->frame_count - 1:\n     IF pool->frames[i].page_id == INVALID_PAGE_ID THEN\n       RETURN &pool->frames[i]\n2. RETURN NULL  // No free frames\n```\n\n![Frame Memory Layout](./diagrams/tdd-diag-m4-2.svg)\n\n### LRU Eviction\n```\nALGORITHM: evict_frame\nINPUT: BufferPool* pool\nOUTPUT: Frame* (evicted frame, now empty) or NULL if all frames pinned\nINVARIANTS:\n  - LRU tail is the least recently used unpinned frame\n  - Pinned frames are never in the LRU list\nPROCEDURE:\n1. // Find eviction candidate\n   Frame* victim = lru_list_peek_tail(pool->lru_list)\n2. // No unpinned frames available\n   IF victim == NULL THEN\n     RETURN NULL\n3. // Write dirty page to disk BEFORE eviction\n   IF victim->is_dirty THEN\n     int result = file_manager_write_page(pool->file_manager, victim->page_id, victim->data)\n     IF result != 0 THEN\n       pool->last_error = BUFFER_POOL_ERROR_DISK_WRITE_FAILED\n       snprintf(pool->error_message, ..., \"failed to write dirty page %d during eviction\",\n                victim->page_id)\n       RETURN NULL  // Eviction failed, page not lost\n     pool->dirty_writes++\n     victim->is_dirty = false\n4. // Remove from page table\n   int result = page_table_remove(pool->page_table, victim->page_id)\n   // Should always succeed since we maintain consistency\n5. // Remove from LRU list\n   lru_list_remove(pool->lru_list, victim)\n6. // Update statistics\n   pool->evictions++\n7. // Clear frame (prepare for reuse)\n   victim->page_id = INVALID_PAGE_ID\n   victim->pin_count = 0\n   victim->lru_prev = NULL\n   victim->lru_next = NULL\n8. RETURN victim\n```\n### Pin/Unpin Operations\n```\nALGORITHM: buffer_pool_pin_page\nINPUT: BufferPool* pool, Frame* frame\nOUTPUT: void\nPRECONDITIONS: pool != NULL, frame != NULL\nPOSTCONDITIONS: frame->pin_count incremented, frame removed from LRU if now pinned\nPROCEDURE:\n1. IF frame->pin_count == 0 THEN\n     // Frame is in LRU list; remove it\n     lru_list_remove(pool->lru_list, frame)\n2. frame->pin_count++\n3. IF frame->pin_count > pool->max_pin_count THEN\n     // Log warning but don't fail (pin count still valid)\n     log_warning(\"pin count exceeded max for frame %d\", frame->frame_index)\nALGORITHM: buffer_pool_unpin_page\nINPUT: BufferPool* pool, Frame* frame\nOUTPUT: void\nPRECONDITIONS: pool != NULL, frame != NULL, frame->pin_count > 0\nPOSTCONDITIONS: frame->pin_count decremented, frame added to LRU if now unpinned\nPROCEDURE:\n1. IF frame->pin_count <= 0 THEN\n     pool->last_error = BUFFER_POOL_ERROR_PIN_COUNT_UNDERFLOW\n     log_error(\"unpin called on frame %d with pin_count = %d\", \n               frame->frame_index, frame->pin_count)\n     RETURN  // Don't decrement\n2. frame->pin_count--\n3. IF frame->pin_count == 0 THEN\n     // Frame is now unpinned; add to LRU list at head\n     lru_list_push_head(pool->lru_list, frame)\n```\n### FlushAll\n```\nALGORITHM: buffer_pool_flush_all\nINPUT: BufferPool* pool\nOUTPUT: int (0 on success, error code on failure)\nPROCEDURE:\n1. IF pool == NULL THEN\n     RETURN BUFFER_POOL_ERROR_NULL_ARGUMENT\n2. int error_count = 0\n3. FOR i = 0 TO pool->frame_count - 1:\n     Frame* frame = &pool->frames[i]\n     IF frame->page_id != INVALID_PAGE_ID AND frame->is_dirty THEN\n       int result = file_manager_write_page(pool->file_manager, frame->page_id, frame->data)\n       IF result != 0 THEN\n         error_count++\n         log_error(\"failed to flush page %d\", frame->page_id)\n       ELSE\n         frame->is_dirty = false\n         pool->dirty_writes++\n4. // Sync to ensure durability\n   IF error_count == 0 THEN\n     int result = file_manager_sync(pool->file_manager)\n     IF result != 0 THEN\n       pool->last_error = BUFFER_POOL_ERROR_FSYNC_FAILED\n       RETURN BUFFER_POOL_ERROR_FSYNC_FAILED\n5. IF error_count > 0 THEN\n     pool->last_error = BUFFER_POOL_ERROR_DISK_WRITE_FAILED\n     RETURN BUFFER_POOL_ERROR_DISK_WRITE_FAILED\n6. RETURN BUFFER_POOL_OK\n```\n\n![FetchPage Flow: Hit vs Miss](./diagrams/tdd-diag-m4-3.svg)\n\n### Hash Function for Page Table\n```\nALGORITHM: page_table_hash\nINPUT: PageId page_id, int bucket_count\nOUTPUT: int (bucket index)\nPROCEDURE:\n1. // Use multiplicative hash for good distribution\n   uint32_t hash = (uint32_t)page_id\n   hash = hash * 2654435761u  // Golden ratio constant for 32-bit\n   hash = hash ^ (hash >> 16)\n2. // Modulo for bucket count (assumes power of 2)\n   RETURN (int)(hash & (bucket_count - 1))\n```\n---\n## State Machine\nThe buffer pool has implicit states for each frame:\n```\nFRAME STATES:\n  EMPTY          -- page_id == INVALID_PAGE_ID, not in page_table or LRU\n  CLEAN_UNPINNED -- valid page_id, pin_count == 0, is_dirty == false, in LRU list\n  CLEAN_PINNED   -- valid page_id, pin_count > 0, is_dirty == false, NOT in LRU list\n  DIRTY_UNPINNED -- valid page_id, pin_count == 0, is_dirty == true, in LRU list\n  DIRTY_PINNED   -- valid page_id, pin_count > 0, is_dirty == true, NOT in LRU list\nTRANSITIONS:\n  EMPTY --FetchPage(miss)--> CLEAN_PINNED\n  CLEAN_PINNED --Unpin--> CLEAN_UNPINNED\n  CLEAN_PINNED --MarkDirty--> DIRTY_PINNED\n  CLEAN_UNPINNED --Pin--> CLEAN_PINNED\n  CLEAN_UNPINNED --MarkDirty--> DIRTY_UNPINNED\n  CLEAN_UNPINNED --Evict--> EMPTY\n  DIRTY_PINNED --Unpin--> DIRTY_UNPINNED\n  DIRTY_PINNED --Flush--> CLEAN_PINNED (if still pinned)\n  DIRTY_UNPINNED --Pin--> DIRTY_PINNED\n  DIRTY_UNPINNED --Evict--> EMPTY (after write)\n  DIRTY_UNPINNED --Flush--> CLEAN_UNPINNED\n  ANY_PINNED --FetchPage(miss, evict this)--> BLOCKED (cannot evict pinned frame)\nILLEGAL TRANSITIONS:\n  - Unpin when pin_count == 0 (underflow)\n  - Pin when pin_count > max_pin_count (overflow)\n  - Evict when pin_count > 0 (pinned)\n  - FlushAll skips frames with I/O errors (logs, continues)\n```\n---\n## Error Handling Matrix\n| Error | Detected By | Recovery | User-Visible? |\n|-------|-------------|----------|---------------|\n| `BUFFER_POOL_ERROR_NULL_ARGUMENT` | All public functions | Return NULL or error code immediately | Yes: \"null argument to buffer_pool_fetch_page\" |\n| `BUFFER_POOL_ERROR_INVALID_PAGE_ID` | `buffer_pool_fetch_page` | Return NULL, set error | Yes: \"invalid page id: -1\" |\n| `BUFFER_POOL_ERROR_EXHAUSTED` | `evict_frame` when all pinned | Return NULL, set error | Yes: \"buffer pool exhausted: all 100 frames pinned\" |\n| `BUFFER_POOL_ERROR_PIN_COUNT_UNDERFLOW` | `buffer_pool_unpin_page` | Log error, don't decrement | Yes (log): \"unpin on frame with pin_count=0\" |\n| `BUFFER_POOL_ERROR_PIN_COUNT_OVERFLOW` | `buffer_pool_fetch_page` | Rollback pin increment, return NULL | Yes: \"pin count overflow for page 42\" |\n| `BUFFER_POOL_ERROR_DISK_READ_FAILED` | `buffer_pool_fetch_page_miss` | Return NULL, frame remains empty | Yes: \"failed to read page 42 from disk\" |\n| `BUFFER_POOL_ERROR_DISK_WRITE_FAILED` | `evict_frame`, `flush_all` | Abort eviction or continue flush | Yes: \"failed to write dirty page 42\" |\n| `BUFFER_POOL_ERROR_FSYNC_FAILED` | `buffer_pool_flush_all` | Return error, data may not be durable | Yes: \"fsync failed\" |\n| `BUFFER_POOL_ERROR_OUT_OF_MEMORY` | `buffer_pool_create`, page table operations | Return NULL, cleanup partial allocation | Yes: \"failed to allocate frame array\" |\n| `BUFFER_POOL_ERROR_DUPLICATE_PAGE` | `page_table_insert` | Should not occur; indicates bug | Yes: \"page 42 already resident (internal error)\" |\n**Recovery guarantees**:\n- After `BUFFER_POOL_ERROR_EXHAUSTED`: All existing pins remain valid, no data lost.\n- After `BUFFER_POOL_ERROR_DISK_READ_FAILED`: Frame remains empty, no stale data.\n- After `BUFFER_POOL_ERROR_DISK_WRITE_FAILED`: Page remains dirty, retry possible.\n- After `BUFFER_POOL_ERROR_FSYNC_FAILED`: Data written but may not survive crash.\n---\n## Implementation Sequence with Checkpoints\n### Phase 1: Frame and BufferPool Structures (1 hour)\n**Files to create**: `frame.h`, `frame.c`, `types.h`, `buffer_pool.h`\n**Implementation**:\n1. Define `PageId` type and constants.\n2. Define `Frame` struct with all fields.\n3. Implement `frame_init`, `frame_clear`, `frame_is_valid`, `frame_is_pinned`, `frame_is_dirty`.\n4. Define `BufferPool` struct with all fields.\n5. Define `BufferPoolStats` struct.\n6. Define error codes enum.\n**Checkpoint**: Frame operations work correctly.\n```bash\ngcc -c frame.c -o frame.o\n./test_frame\n# frame_init sets page_id=-1, pin_count=0, is_dirty=false\n# frame_is_valid returns false for empty frame\n# All tests pass\n```\n### Phase 2: Page Table Hash Map (1 hour)\n**Files to create**: `page_table.h`, `page_table.c`\n**Implementation**:\n1. Define `PageTableEntry` and `PageTable` structs.\n2. Implement `page_table_create` with configurable bucket count.\n3. Implement hash function using multiplicative method.\n4. Implement `page_table_lookup` with chaining traversal.\n5. Implement `page_table_insert` with duplicate detection.\n6. Implement `page_table_remove` with proper chain maintenance.\n7. Implement `page_table_free` with entry deallocation.\n**Checkpoint**: Page table operations work correctly.\n```bash\n./test_page_table\n# Insert 100 pages, lookup all (100% hit)\n# Remove 50 pages, verify lookups fail\n# Insert duplicate returns error\n# All tests pass\n```\n### Phase 3: FetchPage with Hit/Miss Paths (2 hours)\n**Files to create**: `buffer_pool.c`, `fetch_page.c`\n**Implementation**:\n1. Implement `buffer_pool_create` with frame array allocation.\n2. Implement `buffer_pool_free` with resource cleanup.\n3. Implement `buffer_pool_fetch_page` with hit detection.\n4. Implement miss path with disk read.\n5. Implement `find_free_frame` scanning for empty frames.\n6. Integrate with page table for lookup and insertion.\n**Checkpoint**: Basic FetchPage works for single page.\n```bash\n./test_fetch_page_basic\n# FetchPage(page_id=1) loads from disk (miss)\n# FetchPage(page_id=1) returns cached frame (hit)\n# Statistics show 1 miss, 1 hit\n# All tests pass\n```\n### Phase 4: LRU List Management (1.5 hours)\n**Files to create**: `lru_list.h`, `lru_list.c`\n**Implementation**:\n1. Define `LRUList` struct.\n2. Implement `lru_list_create` and `lru_list_free`.\n3. Implement `lru_list_push_head` with proper pointer updates.\n4. Implement `lru_list_remove` handling head/tail cases.\n5. Implement `lru_list_move_to_head` as remove + push_head.\n6. Implement `lru_list_peek_tail` and `lru_list_pop_tail`.\n**Checkpoint**: LRU list maintains correct ordering.\n```bash\n./test_lru_list\n# Push A, B, C \u2192 order: C, B, A (MRU to LRU)\n# Move A to head \u2192 order: A, C, B\n# Pop tail \u2192 returns B, order: A, C\n# All tests pass\n```\n### Phase 5: Eviction with Dirty Write-Back (1 hour)\n**Files to create**: `eviction.c`\n**Implementation**:\n1. Implement `evict_frame` calling LRU tail selection.\n2. Check dirty flag and write to disk before eviction.\n3. Remove evicted page from page table.\n4. Remove evicted frame from LRU list.\n5. Clear frame metadata for reuse.\n6. Update eviction statistics.\n7. Handle disk write failure gracefully.\n**Checkpoint**: Eviction works with dirty pages.\n```bash\n./test_eviction\n# Fill pool with N frames\n# Fetch N+1th page triggers eviction\n# Dirty page written to disk before eviction\n# Statistics show 1 eviction, 1 dirty write\n# All tests pass\n```\n### Phase 6: Pin/Unpin and FlushAll (1.5 hours)\n**Files to create**: `pin_unpin.c`, `flush.c`, `statistics.c`\n**Implementation**:\n1. Implement `buffer_pool_pin_page` with LRU removal.\n2. Implement `buffer_pool_unpin_page` with LRU addition.\n3. Implement `buffer_pool_mark_dirty`.\n4. Implement `buffer_pool_flush_all` iterating all frames.\n5. Implement `buffer_pool_flush_page` for selective flush.\n6. Implement `buffer_pool_get_stats` computing hit rate.\n**Checkpoint**: Full buffer pool functionality.\n```bash\n./test_buffer_pool_full\n# Pin prevents eviction (verify error when all pinned)\n# FlushAll writes dirty pages to disk\n# Hit rate calculation correct\n# All tests pass\n```\n---\n## Test Specification\n### Frame Tests\n```c\nvoid test_frame_init() {\n    Frame frame;\n    frame_init(&frame, 5);\n    assert(frame.page_id == INVALID_PAGE_ID);\n    assert(frame.pin_count == 0);\n    assert(frame.is_dirty == false);\n    assert(frame.lru_prev == NULL);\n    assert(frame.lru_next == NULL);\n    assert(frame.frame_index == 5);\n}\nvoid test_frame_clear() {\n    Frame frame;\n    frame.page_id = 42;\n    frame.is_dirty = true;\n    frame.pin_count = 0;\n    frame_clear(&frame);\n    assert(frame.page_id == INVALID_PAGE_ID);\n    assert(frame.is_dirty == false);\n}\n```\n### Page Table Tests\n```c\nvoid test_page_table_insert_lookup() {\n    PageTable* table = page_table_create(16);\n    Frame frame1, frame2;\n    frame_init(&frame1, 0);\n    frame_init(&frame2, 1);\n    page_table_insert(table, 1, &frame1);\n    page_table_insert(table, 2, &frame2);\n    assert(page_table_lookup(table, 1) == &frame1);\n    assert(page_table_lookup(table, 2) == &frame2);\n    assert(page_table_lookup(table, 3) == NULL);\n    page_table_free(table);\n}\nvoid test_page_table_remove() {\n    PageTable* table = page_table_create(16);\n    Frame frame;\n    frame_init(&frame, 0);\n    page_table_insert(table, 42, &frame);\n    assert(page_table_lookup(table, 42) == &frame);\n    page_table_remove(table, 42);\n    assert(page_table_lookup(table, 42) == NULL);\n    page_table_free(table);\n}\nvoid test_page_table_collision() {\n    // Create small table to force collisions\n    PageTable* table = page_table_create(4);\n    Frame frames[10];\n    for (int i = 0; i < 10; i++) {\n        frame_init(&frames[i], i);\n        page_table_insert(table, i + 1, &frames[i]);\n    }\n    // All lookups should succeed despite collisions\n    for (int i = 0; i < 10; i++) {\n        assert(page_table_lookup(table, i + 1) == &frames[i]);\n    }\n    page_table_free(table);\n}\n```\n### LRU List Tests\n```c\nvoid test_lru_list_ordering() {\n    LRUList* list = lru_list_create();\n    Frame a, b, c;\n    frame_init(&a, 0); frame_init(&b, 1); frame_init(&c, 2);\n    lru_list_push_head(list, &a);  // List: [a]\n    lru_list_push_head(list, &b);  // List: [b, a]\n    lru_list_push_head(list, &c);  // List: [c, b, a]\n    assert(list->head == &c);\n    assert(list->tail == &a);\n    assert(list->count == 3);\n    lru_list_free(list);\n}\nvoid test_lru_list_move_to_head() {\n    LRUList* list = lru_list_create();\n    Frame a, b, c;\n    frame_init(&a, 0); frame_init(&b, 1); frame_init(&c, 2);\n    lru_list_push_head(list, &a);\n    lru_list_push_head(list, &b);\n    lru_list_push_head(list, &c);  // [c, b, a]\n    lru_list_move_to_head(list, &a);  // [a, c, b]\n    assert(list->head == &a);\n    assert(list->tail == &b);\n    lru_list_free(list);\n}\nvoid test_lru_list_remove_middle() {\n    LRUList* list = lru_list_create();\n    Frame a, b, c;\n    frame_init(&a, 0); frame_init(&b, 1); frame_init(&c, 2);\n    lru_list_push_head(list, &a);\n    lru_list_push_head(list, &b);\n    lru_list_push_head(list, &c);  // [c, b, a]\n    lru_list_remove(list, &b);  // [c, a]\n    assert(list->head == &c);\n    assert(list->tail == &a);\n    assert(c.lru_next == &a);\n    assert(a.lru_prev == &c);\n    assert(b.lru_prev == NULL);\n    assert(b.lru_next == NULL);\n    lru_list_free(list);\n}\n```\n### Buffer Pool Hit/Miss Tests\n```c\nvoid test_fetch_page_hit() {\n    BufferPool* pool = buffer_pool_create(10, 4096, create_mock_file_manager());\n    // First fetch: miss\n    Frame* frame1 = buffer_pool_fetch_page(pool, 1);\n    assert(frame1 != NULL);\n    assert(pool->misses == 1);\n    assert(pool->hits == 0);\n    // Second fetch: hit\n    Frame* frame2 = buffer_pool_fetch_page(pool, 1);\n    assert(frame2 == frame1);  // Same frame\n    assert(pool->hits == 1);\n    buffer_pool_unpin_page(pool, frame1);\n    buffer_pool_unpin_page(pool, frame2);\n    buffer_pool_free(pool);\n}\nvoid test_fetch_page_miss_eviction() {\n    BufferPool* pool = buffer_pool_create(3, 4096, create_mock_file_manager());\n    // Fill pool\n    Frame* f1 = buffer_pool_fetch_page(pool, 1);\n    Frame* f2 = buffer_pool_fetch_page(pool, 2);\n    Frame* f3 = buffer_pool_fetch_page(pool, 3);\n    buffer_pool_unpin_page(pool, f1);\n    buffer_pool_unpin_page(pool, f2);\n    buffer_pool_unpin_page(pool, f3);\n    // Fetch new page: triggers eviction of f1 (LRU)\n    Frame* f4 = buffer_pool_fetch_page(pool, 4);\n    assert(f4 != NULL);\n    assert(pool->evictions == 1);\n    // Page 1 should no longer be resident\n    assert(page_table_lookup(pool->page_table, 1) == NULL);\n    buffer_pool_unpin_page(pool, f4);\n    buffer_pool_free(pool);\n}\n```\n### Pin/Unpin Tests\n```c\nvoid test_pin_prevents_eviction() {\n    BufferPool* pool = buffer_pool_create(2, 4096, create_mock_file_manager());\n    Frame* f1 = buffer_pool_fetch_page(pool, 1);\n    Frame* f2 = buffer_pool_fetch_page(pool, 2);\n    // Don't unpin f1 - it stays pinned\n    buffer_pool_unpin_page(pool, f2);\n    // Try to fetch third page - should fail (all frames either pinned or just loaded)\n    Frame* f3 = buffer_pool_fetch_page(pool, 3);\n    // f3 might succeed if it evicts f2, but let's pin f2 too\n    buffer_pool_pin_page(pool, f2);\n    f3 = buffer_pool_fetch_page(pool, 3);\n    assert(f3 == NULL);  // Cannot evict, all pinned\n    assert(pool->last_error == BUFFER_POOL_ERROR_EXHAUSTED);\n    buffer_pool_unpin_page(pool, f1);\n    buffer_pool_unpin_page(pool, f2);\n    buffer_pool_free(pool);\n}\nvoid test_unpin_adds_to_lru() {\n    BufferPool* pool = buffer_pool_create(10, 4096, create_mock_file_manager());\n    Frame* frame = buffer_pool_fetch_page(pool, 1);\n    assert(frame->pin_count == 1);\n    assert(pool->lru_list->count == 0);  // Not in LRU while pinned\n    buffer_pool_unpin_page(pool, frame);\n    assert(frame->pin_count == 0);\n    assert(pool->lru_list->count == 1);  // Now in LRU\n    assert(pool->lru_list->head == frame);\n    buffer_pool_free(pool);\n}\n```\n### Dirty Page and Flush Tests\n```c\nvoid test_dirty_page_write_on_eviction() {\n    FileManager* fm = create_mock_file_manager();\n    BufferPool* pool = buffer_pool_create(2, 4096, fm);\n    Frame* f1 = buffer_pool_fetch_page(pool, 1);\n    // Modify the page\n    f1->data[0] = 'X';\n    buffer_pool_mark_dirty(pool, f1);\n    assert(f1->is_dirty == true);\n    buffer_pool_unpin_page(pool, f1);\n    // Fetch another page to trigger eviction\n    Frame* f2 = buffer_pool_fetch_page(pool, 2);\n    buffer_pool_unpin_page(pool, f2);\n    Frame* f3 = buffer_pool_fetch_page(pool, 3);\n    // f1 should have been written before eviction\n    assert(pool->dirty_writes == 1);\n    // Verify disk has the modification\n    char disk_data[4096];\n    file_manager_read_page(fm, 1, disk_data);\n    assert(disk_data[0] == 'X');\n    buffer_pool_unpin_page(pool, f3);\n    buffer_pool_free(pool);\n}\nvoid test_flush_all() {\n    BufferPool* pool = buffer_pool_create(10, 4096, create_mock_file_manager());\n    // Load and dirty multiple pages\n    for (int i = 1; i <= 5; i++) {\n        Frame* frame = buffer_pool_fetch_page(pool, i);\n        frame->data[0] = 'A' + i;\n        buffer_pool_mark_dirty(pool, frame);\n        buffer_pool_unpin_page(pool, frame);\n    }\n    assert(pool->dirty_writes == 0);  // Not yet written\n    int result = buffer_pool_flush_all(pool);\n    assert(result == BUFFER_POOL_OK);\n    assert(pool->dirty_writes == 5);  // All dirty pages written\n    // All pages should now be clean\n    for (int i = 0; i < pool->frame_count; i++) {\n        if (pool->frames[i].page_id != INVALID_PAGE_ID) {\n            assert(pool->frames[i].is_dirty == false);\n        }\n    }\n    buffer_pool_free(pool);\n}\n```\n### Statistics Tests\n```c\nvoid test_hit_rate_calculation() {\n    BufferPool* pool = buffer_pool_create(10, 4096, create_mock_file_manager());\n    // 3 misses\n    buffer_pool_fetch_page(pool, 1);\n    buffer_pool_fetch_page(pool, 2);\n    buffer_pool_fetch_page(pool, 3);\n    // 7 hits (re-access)\n    for (int i = 0; i < 7; i++) {\n        Frame* f = buffer_pool_fetch_page(pool, 1);\n        buffer_pool_unpin_page(pool, f);\n    }\n    BufferPoolStats stats = buffer_pool_get_stats(pool);\n    assert(stats.hits == 7);\n    assert(stats.misses == 3);\n    assert(fabs(stats.hit_rate - 0.7) < 0.001);  // 7 / 10 = 70%\n    // Unpin all\n    for (int i = 0; i < pool->frame_count; i++) {\n        while (pool->frames[i].pin_count > 0) {\n            buffer_pool_unpin_page(pool, &pool->frames[i]);\n        }\n    }\n    buffer_pool_free(pool);\n}\n```\n\n![LRU List Operations](./diagrams/tdd-diag-m4-4.svg)\n\n---\n## Performance Targets\n| Operation | Target | How to Measure |\n|-----------|--------|----------------|\n| FetchPage (cache hit) | < 100 ns | Microbenchmark with pre-warmed cache, exclude disk I/O |\n| FetchPage (cache miss) | Dominated by disk I/O | Measure with mock FileManager returning fixed latency |\n| Page table lookup | O(1) average | Benchmark 1M lookups, verify linear scaling |\n| LRU move to head | O(1) | Microbenchmark on linked list operations |\n| Eviction (clean page) | < 500 ns | Exclude disk I/O, measure metadata operations |\n| Eviction (dirty page) | Disk write latency + < 1 \u03bcs | Measure with mock FileManager |\n| FlushAll (1000 dirty pages) | 1000 \u00d7 disk write + fsync | Benchmark with real disk |\n| Hit rate on typical workload | > 95% | Run standard benchmark (e.g., TPC-C) with statistics |\n| Memory overhead per frame | ~4.2 KB | `sizeof(Frame)` on 64-bit system |\n**Benchmark methodology**:\n```c\nvoid benchmark_fetch_page_hit() {\n    BufferPool* pool = buffer_pool_create(1000, 4096, create_mock_file_manager());\n    // Warm up: load 500 pages\n    for (int i = 1; i <= 500; i++) {\n        Frame* f = buffer_pool_fetch_page(pool, i);\n        buffer_pool_unpin_page(pool, f);\n    }\n    // Benchmark: 1M hit fetches\n    clock_t start = clock();\n    for (int iter = 0; iter < 1000; iter++) {\n        for (int i = 1; i <= 1000; i++) {\n            Frame* f = buffer_pool_fetch_page(pool, ((i + iter) % 500) + 1);\n            buffer_pool_unpin_page(pool, f);\n        }\n    }\n    clock_t end = clock();\n    double elapsed_ns = (end - start) * 1e9 / CLOCKS_PER_SEC / 1000000;\n    printf(\"FetchPage (hit): %.1f ns\\n\", elapsed_ns);\n    assert(elapsed_ns < 100.0);\n    buffer_pool_free(pool);\n}\n```\n\n![Pin Count Lifecycle](./diagrams/tdd-diag-m4-5.svg)\n\n---\n[[CRITERIA_JSON: {\"module_id\": \"build-sqlite-m4\", \"criteria\": [\"BufferPool struct contains: frames array (Frame*), frame_count (int), page_size (int, default 4096), page_table (PageTable*), lru_list (LRUList*), statistics (hits, misses, evictions, dirty_writes), file_manager (FileManager*), error state (last_error, error_message)\", \"Frame struct contains: page_id (PageId, -1 if empty), data[PAGE_SIZE] (char array, 4096 bytes), pin_count (int, >= 0), is_dirty (bool), lru_prev/lru_next (Frame* pointers), frame_index (int)\", \"PageTable implements hash table with chaining: PageTableEntry contains page_id, frame pointer, next pointer for chain\", \"PageTable provides O(1) average-case lookup, insert, and remove operations\", \"PageTable hash function uses multiplicative hashing with golden ratio constant\", \"LRUList implements intrusive doubly-linked list with head (MRU), tail (LRU), and count\", \"LRUList provides O(1) push_head, remove, move_to_head, peek_tail, pop_tail operations\", \"buffer_pool_create allocates frame_count frames, creates page table with 2x frame_count buckets, creates empty LRU list\", \"buffer_pool_free flushes dirty pages, frees frame array, page table, and LRU list (does NOT free FileManager)\", \"buffer_pool_fetch_page validates page_id > 0, returns NULL with error for invalid page_id\", \"buffer_pool_fetch_page checks page_table first: if resident, increments pin_count, moves to LRU head, increments hit counter, returns frame\", \"buffer_pool_fetch_page on miss finds free frame or evicts, reads page from disk via FileManager, initializes frame metadata (pin_count=1, is_dirty=false), inserts into page_table, increments miss counter, returns frame\", \"find_free_frame scans frame array for page_id == INVALID_PAGE_ID, returns first empty frame or NULL\", \"evict_frame selects LRU tail frame, writes dirty page to disk before eviction, removes from page_table, removes from LRU list, clears frame metadata, increments eviction counter, returns frame\", \"evict_frame returns NULL if all frames are pinned (LRU list empty)\", \"evict_frame writes dirty page BEFORE removing from page_table (write-back ordering guarantee)\", \"buffer_pool_pin_page increments pin_count, removes frame from LRU list if pin_count becomes 1\", \"buffer_pool_unpin_page decrements pin_count, adds frame to LRU list head if pin_count becomes 0\", \"buffer_pool_unpin_page reports error (pin_count underflow) if pin_count is already 0, does not decrement\", \"buffer_pool_mark_dirty sets frame->is_dirty = true\", \"buffer_pool_flush_all iterates all frames, writes dirty pages to disk via FileManager, calls file_manager_sync (fsync), clears is_dirty flags, returns error code on failure\", \"buffer_pool_flush_all increments dirty_writes counter for each page written\", \"buffer_pool_get_stats returns BufferPoolStats with hits, misses, evictions, dirty_writes, hit_rate (computed as hits/(hits+misses)), pinned_count, dirty_count, resident_count\", \"Hit rate is calculated as hits / (hits + misses), returns 0.0 if no operations\", \"All-pinned scenario (buffer_pool_exhausted) returns NULL from buffer_pool_fetch_page with appropriate error message\", \"Test suite validates cache hit returns same frame on repeated buffer_pool_fetch_page calls\", \"Test suite validates cache miss loads from disk and increments miss counter\", \"Test suite validates LRU ordering: least recently accessed unpinned frame is evicted first\", \"Test suite validates pinned pages are never evicted even when at LRU tail\", \"Test suite validates dirty pages are written to disk before eviction\", \"Test suite validates buffer_pool_flush_all writes all dirty pages and clears dirty flags\", \"Test suite validates buffer_pool_flush_all calls fsync after writing pages\", \"Test suite validates pin/unpin correctly manages LRU list membership\", \"Test suite validates error handling for invalid page_id\", \"Test suite validates error handling for buffer pool exhausted (all frames pinned)\"]}]\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m5 -->\n# Technical Design Specification: B-tree Page Format & Table Storage\n**Module ID:** `build-sqlite-m5`  \n**Version:** 1.0.0  \n**Last Updated:** 2026-02-26\n---\n## 1. Module Charter\n### 1.1 Purpose\nThis module implements the B-tree page format and table storage layer for the SQLite clone. It provides persistent storage structures that organize data into balanced trees, enabling efficient O(log n) lookups, insertions, and deletions. The module bridges the buffer pool's raw page access with higher-level table and index operations.\n### 1.2 Scope\n| In Scope | Out of Scope |\n|----------|--------------|\n| Page header parsing and serialization | Buffer pool eviction policies |\n| Cell pointer array management | Query execution |\n| Variable-length integer (varint) encoding | SQL parsing |\n| Row serialization with serial types | Transaction management |\n| Table B-tree leaf and internal nodes | Write-ahead logging |\n| Index B+tree leaf and internal nodes | Database file header (separate module) |\n| Node splitting algorithm | |\n| System catalog (sqlite_master) | |\n### 1.3 Dependencies\n```\nbuild-sqlite-m4 (Buffer Pool)\n\u251c\u2500\u2500 Page allocation/deallocation\n\u251c\u2500\u2500 Pin/unpin page interface\n\u251c\u2500\u2500 Dirty page marking\n\u2514\u2500\u2500 Page read/write to disk\nbuild-sqlite-m3 (Bytecode Compiler)\n\u251c\u2500\u2500 Schema information for row format\n\u2514\u2500\u2500 Column type metadata\n```\n### 1.4 Success Metrics\n| Metric | Target |\n|--------|--------|\n| Row serialization throughput | > 500K rows/sec |\n| Varint encode/decode | < 50ns per operation |\n| Page split operation | < 100\u00b5s for 100 cells |\n| Memory overhead per page | < 64 bytes beyond page buffer |\n| Catalog lookup | < 1\u00b5s cached, < 100\u00b5s uncached |\n---\n## 2. File Structure\n```\nsrc/\n\u251c\u2500\u2500 btree/\n\u2502   \u251c\u2500\u2500 mod.rs                    # Module exports and public API\n\u2502   \u251c\u2500\u2500 page.rs                   # Page header and format constants\n\u2502   \u251c\u2500\u2500 cell.rs                   # Cell parsing, serialization, pointer array\n\u2502   \u251c\u2500\u2500 varint.rs                 # Variable-length integer encoding\n\u2502   \u251c\u2500\u2500 serial_type.rs            # Serial type computation and parsing\n\u2502   \u251c\u2500\u2500 row.rs                    # Row record serialization/deserialization\n\u2502   \u251c\u2500\u2500 table_btree.rs            # Table B-tree operations (rowid-organized)\n\u2502   \u251c\u2500\u2500 index_btree.rs            # Index B+tree operations (key-organized)\n\u2502   \u251c\u2500\u2500 split.rs                  # Node splitting algorithms\n\u2502   \u2514\u2500\u2500 catalog.rs                # System catalog (sqlite_master)\n\u2514\u2500\u2500 constants.rs                  # Page type constants, magic numbers\ntests/\n\u251c\u2500\u2500 btree/\n\u2502   \u251c\u2500\u2500 test_varint.rs            # Varint edge cases\n\u2502   \u251c\u2500\u2500 test_serial_type.rs       # Serial type computation\n\u2502   \u251c\u2500\u2500 test_row.rs               # Row serialization roundtrip\n\u2502   \u251c\u2500\u2500 test_cell.rs              # Cell operations\n\u2502   \u251c\u2500\u2500 test_page.rs              # Page header and slot management\n\u2502   \u251c\u2500\u2500 test_split.rs             # Split algorithm verification\n\u2502   \u2514\u2500\u2500 test_catalog.rs           # Catalog persistence\n```\n**Creation Order:**\n1. `varint.rs` - Foundation for all variable-length encoding\n2. `serial_type.rs` - Type system for row values\n3. `row.rs` - Depends on varint and serial_type\n4. `page.rs` - Page structure constants and header\n5. `cell.rs` - Depends on page and row\n6. `table_btree.rs` - Depends on cell\n7. `index_btree.rs` - Depends on cell\n8. `split.rs` - Depends on table_btree and index_btree\n9. `catalog.rs` - Depends on table_btree\n10. `mod.rs` - Finalize public API\n---\n## 3. Data Model\n### 3.1 Page Types\n```rust\n/// Page type identifiers stored in page header byte 0\n#[repr(u8)]\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum PageType {\n    /// Interior table B-tree page (0x05)\n    TableInternal = 0x05,\n    /// Leaf table B-tree page (0x0D)\n    TableLeaf = 0x0D,\n    /// Interior index B-tree page (0x02)\n    IndexInternal = 0x02,\n    /// Leaf index B-tree page (0x0A)\n    IndexLeaf = 0x0A,\n}\nimpl PageType {\n    pub fn from_byte(byte: u8) -> Result<Self, BtreeError> {\n        match byte {\n            0x05 => Ok(Self::TableInternal),\n            0x0D => Ok(Self::TableLeaf),\n            0x02 => Ok(Self::IndexInternal),\n            0x0A => Ok(Self::IndexLeaf),\n            _ => Err(BtreeError::CorruptPage(format!(\n                \"Invalid page type: 0x{:02X}\", byte\n            ))),\n        }\n    }\n    pub fn is_leaf(&self) -> bool {\n        matches!(self, Self::TableLeaf | Self::IndexLeaf)\n    }\n    pub fn is_table(&self) -> bool {\n        matches!(self, Self::TableInternal | Self::TableLeaf)\n    }\n}\n```\n### 3.2 Page Header Layout\n```\n{{DIAGRAM:page_header}}\nOffset  Size  Field                  Description\n------  ----  ---------------------  -------------------------------------------\n0       1     page_type              PageType enum value (0x05, 0x0D, 0x02, 0x0A)\n1       2     first_freeblock        Offset to first freeblock, or 0\n3       2     cell_count             Number of cells on this page\n5       2     cell_content_offset    Start of cell content area (0 = 65536)\n7       1     fragmented_free_bytes  Number of fragmented free bytes\n8       4     right_child_page       Right-most child pointer (internal only)\n              = 12 bytes total for leaf pages\n              = 8 bytes for leaf pages (no right_child)\n```\n```rust\n#[derive(Debug, Clone)]\npub struct PageHeader {\n    pub page_type: PageType,\n    pub first_freeblock: u16,\n    pub cell_count: u16,\n    pub cell_content_offset: u16,  // 0 means 65536\n    pub fragmented_free_bytes: u8,\n    pub right_child: Option<u32>,  // Only for internal pages\n}\nimpl PageHeader {\n    pub const LEAF_HEADER_SIZE: usize = 8;\n    pub const INTERNAL_HEADER_SIZE: usize = 12;\n    pub fn header_size(&self) -> usize {\n        if self.page_type.is_leaf() {\n            Self::LEAF_HEADER_SIZE\n        } else {\n            Self::INTERNAL_HEADER_SIZE\n        }\n    }\n    pub fn usable_content_offset(&self) -> u16 {\n        if self.cell_content_offset == 0 {\n            PAGE_SIZE as u16\n        } else {\n            self.cell_content_offset\n        }\n    }\n}\n```\n### 3.3 Slotted Page Layout\n```\n{{DIAGRAM:slotted_page}}\n+------------------+  Offset 0\n| Page Header      |  (8 or 12 bytes depending on page type)\n+------------------+\n| Cell Pointer     |  2 bytes per cell, grows downward\n| Array            |  pointers sorted by key order\n|                  |\n+------------------+  <-- free space start\n|                  |\n|    FREE SPACE    |  Unallocated area\n|                  |\n+------------------+  <-- cell content end (cell_content_offset)\n|                  |\n| Cell Content     |  Grows upward from end of page\n| Area             |  Cells stored in arbitrary order\n|                  |\n+------------------+  Offset 4095 (PAGE_SIZE - 1)\nCell Pointer Format (2 bytes each):\n+----------------------------------------+\n| Cell Content Offset (big-endian u16)   |\n+----------------------------------------+\nPoints to the start of a cell in the cell content area.\n```\n### 3.4 Varint Encoding\nVariable-length integers use 1-9 bytes with high-bit continuation:\n```\n{{DIAGRAM:varint_encoding}}\n1-byte varint (0-127):\n  0xxxxxxx                          (7 bits of data)\n2-byte varint (128-16383):\n  1xxxxxxx 0xxxxxxx                 (14 bits of data)\n3-byte varint (16384-2097151):\n  1xxxxxxx 1xxxxxxx 0xxxxxxx        (21 bits of data)\n4-byte varint (2097152-268435455):\n  1xxxxxxx 1xxxxxxx 1xxxxxxx 0xxxxxxx   (28 bits of data)\n5-8 byte varint: Continue pattern, each byte adds 7 bits\n9-byte varint (for full u64 range):\n  1xxxxxxx 1xxxxxxx 1xxxxxxx 1xxxxxxx 1xxxxxxx \n  1xxxxxxx 1xxxxxxx 1xxxxxxx xxxxxxxx      (8 bits in final byte)\nSpecial case: The 9th byte uses all 8 bits (no continuation bit)\nMaximum value: 2^64 - 1\n```\n```rust\npub fn encode_varint(mut value: u64, buf: &mut [u8; 9]) -> usize {\n    if value <= 0x7F {\n        buf[0] = value as u8;\n        return 1;\n    }\n    let mut len = 0;\n    // Bytes 1-8: 7 bits each with continuation bit\n    for i in 0..8 {\n        if value > 0x7F {\n            buf[i] = ((value & 0x7F) as u8) | 0x80;\n            value >>= 7;\n            len += 1;\n        } else {\n            buf[i] = (value & 0x7F) as u8;\n            return len + 1;\n        }\n    }\n    // 9th byte: full 8 bits (for values that need all 64 bits)\n    buf[8] = value as u8;\n    9\n}\npub fn decode_varint(buf: &[u8]) -> Result<(u64, usize), BtreeError> {\n    if buf.is_empty() {\n        return Err(BtreeError::InvalidVarint(\"Empty buffer\".into()));\n    }\n    let mut result: u64 = 0;\n    for i in 0..9 {\n        if i >= buf.len() {\n            return Err(BtreeError::InvalidVarint(\"Truncated varint\".into()));\n        }\n        if i < 8 {\n            result |= ((buf[i] & 0x7F) as u64) << (7 * i);\n            if buf[i] & 0x80 == 0 {\n                return Ok((result, i + 1));\n            }\n        } else {\n            // 9th byte uses all 8 bits\n            result |= (buf[8] as u64) << 56;\n            return Ok((result, 9));\n        }\n    }\n    Ok((result, 9))\n}\n```\n### 3.5 Serial Types\nSerial types encode the type and size of values in row records:\n```rust\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum SerialType {\n    Null,                    // Type 0: 0 bytes\n    Int8,                    // Type 1: 1 byte signed integer\n    Int16,                   // Type 2: 2 byte big-endian signed integer\n    Int24,                   // Type 3: 3 byte big-endian signed integer\n    Int32,                   // Type 4: 4 byte big-endian signed integer\n    Int48,                   // Type 5: 6 byte big-endian signed integer\n    Int64,                   // Type 6: 8 byte big-endian signed integer\n    Float64,                 // Type 7: 8 byte IEEE 754 float\n    Zero,                    // Type 8: Integer constant 0 (0 bytes)\n    One,                     // Type 9: Integer constant 1 (0 bytes)\n    Reserved(u8),            // Types 10-11: Reserved for internal use\n    Blob(u32),               // Types N>=12 even: (N-12)/2 bytes of blob\n    Text(u32),               // Types N>=13 odd: (N-13)/2 bytes of text\n}\nimpl SerialType {\n    pub fn from_u8(type_byte: u8) -> Result<Self, BtreeError> {\n        match type_byte {\n            0 => Ok(Self::Null),\n            1 => Ok(Self::Int8),\n            2 => Ok(Self::Int16),\n            3 => Ok(Self::Int24),\n            4 => Ok(Self::Int32),\n            5 => Ok(Self::Int48),\n            6 => Ok(Self::Int64),\n            7 => Ok(Self::Float64),\n            8 => Ok(Self::Zero),\n            9 => Ok(Self::One),\n            10 | 11 => Ok(Self::Reserved(type_byte)),\n            n if n >= 12 => {\n                if n % 2 == 0 {\n                    Ok(Self::Blob((n as u32 - 12) / 2))\n                } else {\n                    Ok(Self::Text((n as u32 - 13) / 2))\n                }\n            }\n            _ => unreachable!(),\n        }\n    }\n    pub fn to_u8(&self) -> u8 {\n        match self {\n            Self::Null => 0,\n            Self::Int8 => 1,\n            Self::Int16 => 2,\n            Self::Int24 => 3,\n            Self::Int32 => 4,\n            Self::Int48 => 5,\n            Self::Int64 => 6,\n            Self::Float64 => 7,\n            Self::Zero => 8,\n            Self::One => 9,\n            Self::Reserved(n) => *n,\n            Self::Blob(len) => 12 + (len * 2) as u8,\n            Self::Text(len) => 13 + (len * 2) as u8,\n        }\n    }\n    pub fn content_size(&self) -> usize {\n        match self {\n            Self::Null | Self::Zero | Self::One => 0,\n            Self::Int8 => 1,\n            Self::Int16 => 2,\n            Self::Int24 => 3,\n            Self::Int32 => 4,\n            Self::Int48 => 6,\n            Self::Int64 => 8,\n            Self::Float64 => 8,\n            Self::Reserved(_) => 0,\n            Self::Blob(len) => *len as usize,\n            Self::Text(len) => *len as usize,\n        }\n    }\n    /// Compute optimal serial type for a given value\n    pub fn for_integer(value: i64) -> Self {\n        if value == 0 {\n            Self::Zero\n        } else if value == 1 {\n            Self::One\n        } else if value >= -128 && value <= 127 {\n            Self::Int8\n        } else if value >= -32768 && value <= 32767 {\n            Self::Int16\n        } else if value >= -8388608 && value <= 8388607 {\n            Self::Int24\n        } else if value >= -2147483648 && value <= 2147483647 {\n            Self::Int32\n        } else if value >= -140737488355328 && value <= 140737488355327 {\n            Self::Int48\n        } else {\n            Self::Int64\n        }\n    }\n    pub fn for_blob(len: u32) -> Self {\n        Self::Blob(len)\n    }\n    pub fn for_text(len: u32) -> Self {\n        Self::Text(len)\n    }\n}\n```\n### 3.6 Row Record Format\n```\n{{DIAGRAM:row_record}}\nRow Record Format:\n+----------------+----------------+------------------+------------------+\n| Header Size    | Serial Type 1  | Serial Type 2    | ... Serial Type N|\n| (varint)       | (varint)       | (varint)         |                  |\n+----------------+----------------+------------------+------------------+\n| Value 1        | Value 2        | ... Value N                         |\n| (varies)       | (varies)       |                                     |\n+----------------+----------------+-------------------------------------+\nHeader Size: Total bytes in header (including this size field)\nSerial Types: One per column, encoded as varints\nValues: Contiguous bytes for each column's value\nExample Row: (42, \"hello\", NULL, 3.14)\nHeader: [header_size=8, int8=1, text_len=5->23, null=0, float=7]\n         0x08 0x01 0x17 0x00 0x07\nContent: [42] ['h','e','l','l','o'] [] [IEEE754 bytes]\n         0x2A 0x68656C6C6F -- 0x40091EB851EB851F\n```\n```rust\n#[derive(Debug, Clone)]\npub struct RowRecord {\n    pub columns: Vec<ColumnValue>,\n}\n#[derive(Debug, Clone)]\npub enum ColumnValue {\n    Null,\n    Integer(i64),\n    Float(f64),\n    Blob(Vec<u8>),\n    Text(String),\n}\nimpl RowRecord {\n    pub fn serialize(&self) -> Result<Vec<u8>, BtreeError> {\n        let mut result = Vec::new();\n        let mut header = Vec::new();\n        let mut content = Vec::new();\n        // Build serial types and content\n        for col in &self.columns {\n            let (serial, data) = Self::encode_column(col)?;\n            header.push(serial.to_u8());\n            content.extend_from_slice(&data);\n        }\n        // Header size includes itself (varint of total header bytes)\n        let header_size = header.len() + 1; // +1 for header size varint (assumes < 128 columns)\n        let header_size_varint = encode_varint(header_size as u64, &mut [0; 9]);\n        // Write header size\n        result.extend_from_slice(&header_size_varint[0].to_be_bytes());\n        // Write serial types (as single bytes for common cases)\n        for &t in &header {\n            result.push(t);\n        }\n        // Write content\n        result.extend_from_slice(&content);\n        Ok(result)\n    }\n    pub fn deserialize(buf: &[u8]) -> Result<(Self, usize), BtreeError> {\n        let (header_size, consumed) = decode_varint(buf)?;\n        let header_size = header_size as usize;\n        let mut offset = consumed;\n        let header_end = header_size;\n        let mut serial_types = Vec::new();\n        while offset < header_end {\n            let (st, c) = decode_varint(&buf[offset..])?;\n            serial_types.push(SerialType::from_u8(st as u8)?);\n            offset += c;\n        }\n        let mut columns = Vec::new();\n        for st in &serial_types {\n            let size = st.content_size();\n            let value = Self::decode_value(st, &buf[offset..offset+size])?;\n            columns.push(value);\n            offset += size;\n        }\n        Ok((Self { columns }, offset))\n    }\n}\n```\n### 3.7 Cell Formats\n```\n{{DIAGRAM:cell_formats}}\nTABLE B-TREE LEAF CELL:\n+----------------+----------------+------------------+\n| Payload Size   | Row ID         | Payload          |\n| (varint)       | (varint)       | (bytes)          |\n+----------------+----------------+------------------+\nUsed for: Table data storage\nKey: Row ID (implicit, not stored in payload)\nTABLE B-TREE INTERNAL CELL:\n+----------------+----------------+------------------+\n| Left Child     | Row ID         | Payload (unused) |\n| (4 bytes BE)   | (varint)       |                  |\n+----------------+----------------+------------------+\nUsed for: Table index nodes\nKey: Row ID for routing\nINDEX B+TREE LEAF CELL:\n+----------------+------------------+\n| Payload Size   | Key + Row ID     |\n| (varint)       | (serialized)     |\n+----------------+------------------+\nUsed for: Index data storage\nKey: Explicit in payload (includes row ID for uniqueness)\nINDEX B+TREE INTERNAL CELL:\n+----------------+----------------+------------------+\n| Left Child     | Payload Size   | Key + Row ID     |\n| (4 bytes BE)   | (varint)       | (serialized)     |\n+----------------+----------------+------------------+\nUsed for: Index routing nodes\n```\n```rust\n#[derive(Debug, Clone)]\npub struct TableLeafCell {\n    pub payload_size: u64,\n    pub rowid: u64,\n    pub payload: Vec<u8>,\n}\n#[derive(Debug, Clone)]\npub struct TableInternalCell {\n    pub left_child: u32,\n    pub rowid: u64,\n}\n#[derive(Debug, Clone)]\npub struct IndexLeafCell {\n    pub payload_size: u64,\n    pub key: Vec<u8>,  // Includes rowid for uniqueness\n}\n#[derive(Debug, Clone)]\npub struct IndexInternalCell {\n    pub left_child: u32,\n    pub payload_size: u64,\n    pub key: Vec<u8>,\n}\npub enum Cell {\n    TableLeaf(TableLeafCell),\n    TableInternal(TableInternalCell),\n    IndexLeaf(IndexLeafCell),\n    IndexInternal(IndexInternalCell),\n}\nimpl Cell {\n    pub fn key_compare(&self, other: &Cell) -> std::cmp::Ordering {\n        match (self, other) {\n            (Cell::TableLeaf(a), Cell::TableLeaf(b)) => {\n                a.rowid.cmp(&b.rowid)\n            }\n            (Cell::TableInternal(a), Cell::TableInternal(b)) => {\n                a.rowid.cmp(&b.rowid)\n            }\n            (Cell::IndexLeaf(a), Cell::IndexLeaf(b)) => {\n                a.key.cmp(&b.key)\n            }\n            (Cell::IndexInternal(a), Cell::IndexInternal(b)) => {\n                a.key.cmp(&b.key)\n            }\n            _ => panic!(\"Cannot compare cells of different types\"),\n        }\n    }\n}\n```\n### 3.8 System Catalog Schema\n```rust\n/// sqlite_master table structure\npub struct SqliteMasterRow {\n    pub object_type: String,    // 'table', 'index', 'view', 'trigger'\n    pub name: String,           // Object name\n    pub tbl_name: String,       // Table name (for indexes/triggers)\n    pub rootpage: u32,          // Root page number (0 for views/triggers)\n    pub sql: Option<String>,    // CREATE statement\n}\nimpl SqliteMasterRow {\n    pub const TABLE_NAME: &'static str = \"sqlite_master\";\n    pub const ROOT_PAGE: u32 = 1;\n    pub fn to_row(&self) -> RowRecord {\n        RowRecord {\n            columns: vec![\n                ColumnValue::Text(self.object_type.clone()),\n                ColumnValue::Text(self.name.clone()),\n                ColumnValue::Text(self.tbl_name.clone()),\n                ColumnValue::Integer(self.rootpage as i64),\n                match &self.sql {\n                    Some(s) => ColumnValue::Text(s.clone()),\n                    None => ColumnValue::Null,\n                },\n            ],\n        }\n    }\n}\n```\n---\n## 4. Interface Contracts\n### 4.1 Varint Interface\n```rust\npub trait VarintCodec {\n    /// Encode value as varint, returns bytes written\n    fn encode(value: u64, buf: &mut [u8]) -> Result<usize, BtreeError>;\n    /// Decode varint from buffer, returns (value, bytes_consumed)\n    fn decode(buf: &[u8]) -> Result<(u64, usize), BtreeError>;\n    /// Calculate encoded size without writing\n    fn encoded_size(value: u64) -> usize;\n}\n```\n### 4.2 Page Interface\n```rust\npub trait PageAccess {\n    /// Read page header\n    fn header(&self) -> Result<PageHeader, BtreeError>;\n    /// Write page header\n    fn set_header(&mut self, header: &PageHeader) -> Result<(), BtreeError>;\n    /// Get cell count\n    fn cell_count(&self) -> Result<u16, BtreeError>;\n    /// Get cell pointer at index\n    fn cell_pointer(&self, index: u16) -> Result<u16, BtreeError>;\n    /// Set cell pointer at index\n    fn set_cell_pointer(&mut self, index: u16, offset: u16) -> Result<(), BtreeError>;\n    /// Calculate free space available\n    fn free_space(&self) -> Result<usize, BtreeError>;\n    /// Initialize empty page\n    fn initialize(&mut self, page_type: PageType) -> Result<(), BtreeError>;\n}\n```\n### 4.3 Cell Interface\n```rust\npub trait CellAccess {\n    /// Read cell at pointer offset\n    fn read_cell(&self, offset: u16, page_type: PageType) -> Result<Cell, BtreeError>;\n    /// Write cell, returns offset where written\n    fn write_cell(&mut self, cell: &Cell) -> Result<u16, BtreeError>;\n    /// Insert cell at sorted position\n    fn insert_cell(&mut self, cell: &Cell) -> Result<(), BtreeError>;\n    /// Delete cell at index\n    fn delete_cell(&mut self, index: u16) -> Result<(), BtreeError>;\n    /// Find cell by key (binary search)\n    fn find_cell(&self, key: &[u8]) -> Result<Option<u16>, BtreeError>;\n}\n```\n### 4.4 B-tree Interface\n```rust\npub trait BtreeOperations {\n    /// Find row by rowid in table B-tree\n    fn find_row(&self, root_page: u32, rowid: u64) -> Result<Option<RowRecord>, BtreeError>;\n    /// Insert row into table B-tree, returns assigned rowid\n    fn insert_row(&mut self, root_page: u32, row: &RowRecord) -> Result<u64, BtreeError>;\n    /// Delete row from table B-tree\n    fn delete_row(&mut self, root_page: u32, rowid: u64) -> Result<bool, BtreeError>;\n    /// Create new B-tree, returns root page number\n    fn create_btree(&mut self, is_table: bool) -> Result<u32, BtreeError>;\n    /// Find entry in index B-tree\n    fn find_index_entry(&self, root_page: u32, key: &[u8]) -> Result<Option<u64>, BtreeError>;\n    /// Insert into index B-tree\n    fn insert_index_entry(&mut self, root_page: u32, key: &[u8], rowid: u64) -> Result<(), BtreeError>;\n}\n```\n### 4.5 Catalog Interface\n```rust\npub trait CatalogOperations {\n    /// Look up table by name\n    fn find_table(&self, name: &str) -> Result<Option<SqliteMasterRow>, BtreeError>;\n    /// Look up index by name\n    fn find_index(&self, name: &str) -> Result<Option<SqliteMasterRow>, BtreeError>;\n    /// List all tables\n    fn list_tables(&self) -> Result<Vec<String>, BtreeError>;\n    /// Add new table entry\n    fn create_table_entry(&mut self, entry: &SqliteMasterRow) -> Result<(), BtreeError>;\n    /// Drop table entry\n    fn drop_table_entry(&mut self, name: &str) -> Result<bool, BtreeError>;\n    /// Initialize catalog on new database\n    fn initialize_catalog(&mut self) -> Result<(), BtreeError>;\n}\n```\n---\n## 5. Algorithm Specification\n### 5.1 Varint Encoding Algorithm\n```\nALGORITHM: encode_varint\nINPUT: value (u64), buf (mutable byte array, min 9 bytes)\nOUTPUT: length (number of bytes written)\n1. IF value <= 127 THEN\n     buf[0] = value as u8\n     RETURN 1\n   END IF\n2. len = 0\n   temp = value\n3. FOR i = 0 TO 7 DO\n     IF temp > 127 THEN\n       buf[i] = (temp & 0x7F) | 0x80   // Lower 7 bits + continuation\n       temp = temp >> 7\n       len = len + 1\n     ELSE\n       buf[i] = temp as u8              // Final byte, no continuation\n       RETURN len + 1\n     END IF\n   END FOR\n4. // Value requires all 64 bits\n   buf[8] = temp as u8                  // 9th byte uses all 8 bits\n   RETURN 9\n```\n**Complexity:** O(1) - Fixed maximum of 9 iterations\n### 5.2 Varint Decoding Algorithm\n```\nALGORITHM: decode_varint\nINPUT: buf (byte array, min 1 byte)\nOUTPUT: (value, length) OR error\n1. IF buf is empty THEN\n     RETURN Error(\"Empty buffer\")\n   END IF\n2. result = 0\n3. FOR i = 0 TO 8 DO\n     IF i >= buf.length THEN\n       RETURN Error(\"Truncated varint\")\n     END IF\n     IF i < 8 THEN\n       result = result | ((buf[i] & 0x7F) << (7 * i))\n       IF buf[i] & 0x80 == 0 THEN\n         RETURN (result, i + 1)\n       END IF\n     ELSE\n       // 9th byte: all 8 bits contribute\n       result = result | (buf[8] << 56)\n       RETURN (result, 9)\n     END IF\n   END FOR\n4. RETURN (result, 9)\n```\n**Complexity:** O(1) - Fixed maximum of 9 iterations\n### 5.3 Cell Insertion with Binary Search\n```\nALGORITHM: insert_cell_sorted\nINPUT: page, cell\nOUTPUT: success or error\n1. // Find insertion point using binary search\n   left = 0\n   right = page.cell_count - 1\n   insert_pos = page.cell_count\n2. WHILE left <= right DO\n     mid = (left + right) / 2\n     existing_cell = read_cell(page.cell_pointer(mid))\n     IF cell.key < existing_cell.key THEN\n       insert_pos = mid\n       right = mid - 1\n     ELSE IF cell.key > existing_cell.key THEN\n       left = mid + 1\n     ELSE\n       RETURN Error(\"Duplicate key\")\n     END IF\n   END WHILE\n3. // Check space\n   cell_size = calculate_cell_size(cell)\n   IF page.free_space < cell_size + 2 THEN  // +2 for pointer\n     RETURN Error(\"Page full\")\n   END IF\n4. // Shift cell pointers\n   FOR i = insert_pos TO page.cell_count - 1 DO\n     page.set_cell_pointer(i + 1, page.cell_pointer(i))\n   END FOR\n5. // Write cell content\n   write_offset = page.cell_content_offset - cell_size\n   write_cell_at(write_offset, cell)\n6. // Update header\n   page.set_cell_pointer(insert_pos, write_offset)\n   page.cell_count += 1\n   page.cell_content_offset = write_offset\n7. RETURN Success\n```\n**Complexity:** O(log n) for search + O(n) for pointer shift\n### 5.4 Node Splitting Algorithm\n```\n{{DIAGRAM:node_split}}\nALGORITHM: split_node\nINPUT: page (full), new_cell\nOUTPUT: (new_page_number, split_key)\n1. // Allocate new sibling page\n   new_page = buffer_pool.allocate_page()\n   new_page.initialize(page.page_type)\n2. // Collect all cells including new one\n   cells = []\n   FOR i = 0 TO page.cell_count - 1 DO\n     cells.push(read_cell(page.cell_pointer(i)))\n   END FOR\n   cells.push(new_cell)\n   SORT cells by key\n3. // Calculate split point (50% fill factor)\n   total_size = SUM(cell_size(c) for c in cells)\n   target_size = total_size / 2\n   split_index = 0\n   accumulated = 0\n   FOR i = 0 TO cells.length - 1 DO\n     accumulated += cell_size(cells[i])\n     IF accumulated >= target_size THEN\n       split_index = i\n       BREAK\n     END IF\n   END FOR\n4. // Clear original page and redistribute\n   page.initialize(page.page_type)\n   // Left page: cells[0..split_index]\n   FOR i = 0 TO split_index - 1 DO\n     page.insert_cell(cells[i])\n   END FOR\n   // Right page: cells[split_index..end]\n   FOR i = split_index TO cells.length - 1 DO\n     new_page.insert_cell(cells[i])\n   END FOR\n5. // Determine split key\n   IF page.is_leaf() THEN\n     split_key = cells[split_index].key\n   ELSE\n     // Internal: promote middle key to parent\n     split_key = cells[split_index].key\n     // Move cells[split_index] to parent, rest to right sibling\n   END IF\n6. RETURN (new_page.page_number, split_key)\n```\n**Complexity:** O(n) where n = number of cells\n### 5.5 B-tree Insertion (with recursive split)\n```\nALGORITHM: btree_insert\nINPUT: root_page, key, payload\nOUTPUT: success or error\n1. page = buffer_pool.get_page(root_page)\n2. IF page.is_leaf() THEN\n     // Try to insert directly\n     result = page.insert_cell(Cell{key, payload})\n     IF result == \"Page full\" THEN\n       // Need to split\n       (new_page, split_key) = split_node(page, Cell{key, payload})\n       IF page.is_root() THEN\n         // Create new root\n         new_root = buffer_pool.allocate_page()\n         new_root.initialize(Internal)\n         new_root.insert_cell(Cell{left=page, key=split_key, right=new_page})\n         update_root_pointer(new_root)\n       ELSE\n         // Propagate split to parent\n         insert_in_parent(page, split_key, new_page)\n       END IF\n     END IF\n   ELSE\n     // Internal node: find child and recurse\n     child_page = find_child_page(page, key)\n     btree_insert(child_page, key, payload)\n   END IF\n3. RETURN Success\n```\n### 5.6 Row Serialization Algorithm\n```\nALGORITHM: serialize_row\nINPUT: row (vector of column values)\nOUTPUT: byte vector\n1. // Phase 1: Compute serial types and content\n   serial_types = []\n   content_parts = []\n   total_content_size = 0\n   FOR each column IN row DO\n     st = compute_serial_type(column)\n     serial_types.push(st)\n     data = encode_value(column, st)\n     content_parts.push(data)\n     total_content_size += data.length\n   END FOR\n2. // Phase 2: Build header\n   header_bytes = []\n   FOR each st IN serial_types DO\n     header_bytes.extend(encode_varint(st.to_u8()))\n   END FOR\n   header_size = header_bytes.length + varint_size(header_bytes.length + 1)\n3. // Phase 3: Assemble record\n   result = []\n   result.extend(encode_varint(header_size))\n   result.extend(header_bytes)\n   FOR each part IN content_parts DO\n     result.extend(part)\n   END FOR\n4. RETURN result\n```\n---\n## 6. Error Handling Matrix\n| Error Code | Condition | Recovery Strategy |\n|------------|-----------|-------------------|\n| `InvalidPageType` | Page type byte not in {0x02, 0x05, 0x0A, 0x0D} | Return error, mark page corrupted |\n| `CorruptPage` | Cell pointer outside page bounds | Return error, suggest recovery |\n| `InvalidVarint` | Varint exceeds 9 bytes or truncated | Return error with position |\n| `SerialTypeReserved` | Encountered type 10 or 11 | Return error, incompatible format |\n| `PageFull` | Cannot fit cell in available space | Trigger split or overflow |\n| `DuplicateKey` | Insert with existing key | Return error (unless OR REPLACE) |\n| `CellNotFound` | Delete/update non-existent key | Return false (idempotent) |\n| `InvalidRowFormat` | Row header size exceeds content | Return error, corrupted record |\n| `CatalogCorrupt` | sqlite_master unreadable | Return error, fatal condition |\n| `OverflowRequired` | Payload > usable page space | Implement overflow pages |\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum BtreeError {\n    #[error(\"Invalid page type: 0x{0:02X}\")]\n    InvalidPageType(u8),\n    #[error(\"Corrupt page: {0}\")]\n    CorruptPage(String),\n    #[error(\"Invalid varint: {0}\")]\n    InvalidVarint(String),\n    #[error(\"Serial type reserved: {0}\")]\n    SerialTypeReserved(u8),\n    #[error(\"Page full: need {need} bytes, have {available}\")]\n    PageFull { need: usize, available: usize },\n    #[error(\"Duplicate key\")]\n    DuplicateKey,\n    #[error(\"Cell not found\")]\n    CellNotFound,\n    #[error(\"Invalid row format: {0}\")]\n    InvalidRowFormat(String),\n    #[error(\"Catalog corrupt: {0}\")]\n    CatalogCorrupt(String),\n    #[error(\"Overflow required for payload size {0}\")]\n    OverflowRequired(usize),\n    #[error(\"Buffer pool error: {0}\")]\n    BufferPool(#[from] crate::buffer_pool::BufferPoolError),\n    #[error(\"IO error: {0}\")]\n    Io(#[from] std::io::Error),\n}\n```\n---\n## 7. Implementation Sequence\n### Phase 1: Varint Codec (Est: 4 hours)\n**Files:** `src/btree/varint.rs`\n**Tasks:**\n1. Implement `encode_varint()` with 9-byte buffer\n2. Implement `decode_varint()` with validation\n3. Implement `encoded_size()` helper\n4. Write edge case tests (0, 127, 128, 2^63-1, 2^64-1)\n**Checkpoint:** All varint tests pass, including roundtrip for boundary values\n```rust\n// Test cases must include:\nassert_eq!(encode_varint(0, &mut buf), 1);\nassert_eq!(encode_varint(127, &mut buf), 1);\nassert_eq!(encode_varint(128, &mut buf), 2);\nassert_eq!(encode_varint(16383, &mut buf), 2);\nassert_eq!(encode_varint(16384, &mut buf), 3);\nassert_eq!(encode_varint(u64::MAX, &mut buf), 9);\n```\n### Phase 2: Serial Types (Est: 3 hours)\n**Files:** `src/btree/serial_type.rs`\n**Tasks:**\n1. Define `SerialType` enum\n2. Implement `from_u8()` and `to_u8()`\n3. Implement `content_size()`\n4. Implement `for_integer()` optimization\n5. Write tests for all type mappings\n**Checkpoint:** Serial type correctly encodes/decodes all SQLite value types\n### Phase 3: Row Serialization (Est: 6 hours)\n**Files:** `src/btree/row.rs`\n**Tasks:**\n1. Define `ColumnValue` enum\n2. Implement `RowRecord::serialize()`\n3. Implement `RowRecord::deserialize()`\n4. Handle NULL, integers, floats, blobs, text\n5. Test roundtrip for complex rows\n**Checkpoint:** Row records serialize/deserialize with correct byte layout\n### Phase 4: Page Format (Est: 5 hours)\n**Files:** `src/btree/page.rs`, `src/constants.rs`\n**Tasks:**\n1. Define `PageType` enum\n2. Implement `PageHeader` struct\n3. Implement header read/write at offsets 0-11\n4. Implement cell pointer array access\n5. Implement free space calculation\n6. Write page initialization tests\n**Checkpoint:** Pages correctly maintain header and pointer array\n### Phase 5: Cell Operations (Est: 8 hours)\n**Files:** `src/btree/cell.rs`\n**Tasks:**\n1. Define cell structs for all 4 page types\n2. Implement cell parsing from page buffer\n3. Implement cell serialization to buffer\n4. Implement cell pointer array insertion\n5. Implement binary search for cell lookup\n6. Test all cell types\n**Checkpoint:** Cells read/write correctly, sorted insertion works\n### Phase 6: B-tree Operations (Est: 12 hours)\n**Files:** `src/btree/table_btree.rs`, `src/btree/index_btree.rs`\n**Tasks:**\n1. Implement `find_row()` with tree traversal\n2. Implement `insert_row()` with split handling\n3. Implement `delete_row()` (mark deleted, rebalance later)\n4. Implement `create_btree()` \n5. Implement index operations\n6. Test tree operations with multiple levels\n**Checkpoint:** B-tree supports CRUD operations, maintains balance\n### Phase 7: Node Splitting (Est: 8 hours)\n**Files:** `src/btree/split.rs`\n**Tasks:**\n1. Implement cell collection and sorting\n2. Implement split point calculation\n3. Implement leaf node split\n4. Implement internal node split with key promotion\n5. Implement root split (new root creation)\n6. Test splits maintain tree invariants\n**Checkpoint:** Splits produce valid balanced trees\n### Phase 8: System Catalog (Est: 6 hours)\n**Files:** `src/btree/catalog.rs`\n**Tasks:**\n1. Define `SqliteMasterRow` structure\n2. Implement `find_table()` and `find_index()`\n3. Implement `create_table_entry()`\n4. Implement `drop_table_entry()`\n5. Implement `initialize_catalog()` for new database\n6. Test catalog persistence\n**Checkpoint:** Schema persists across database reopens\n---\n## 8. Test Specification\n### 8.1 Unit Tests\n```rust\n// tests/btree/test_varint.rs\n#[test]\nfn test_varint_boundaries() {\n    let test_cases = [\n        (0u64, 1),\n        (127, 1),\n        (128, 2),\n        (16383, 2),\n        (16384, 3),\n        (2097151, 3),\n        (2097152, 4),\n        (268435455, 4),\n        (268435456, 5),\n        (34359738367, 5),\n        (34359738368, 6),\n        (4398046511103, 6),\n        (4398046511104, 7),\n        (562949953421311, 7),\n        (562949953421312, 8),\n        (72057594037927935, 8),\n        (72057594037927936, 9),\n        (u64::MAX, 9),\n    ];\n    for (value, expected_len) in test_cases {\n        let mut buf = [0u8; 9];\n        let len = encode_varint(value, &mut buf);\n        assert_eq!(len, expected_len);\n        let (decoded, consumed) = decode_varint(&buf[..len]).unwrap();\n        assert_eq!(decoded, value);\n        assert_eq!(consumed, len);\n    }\n}\n#[test]\nfn test_serial_type_optimization() {\n    assert_eq!(SerialType::for_integer(0), SerialType::Zero);\n    assert_eq!(SerialType::for_integer(1), SerialType::One);\n    assert_eq!(SerialType::for_integer(-128), SerialType::Int8);\n    assert_eq!(SerialType::for_integer(127), SerialType::Int8);\n    assert_eq!(SerialType::for_integer(128), SerialType::Int16);\n    assert_eq!(SerialType::for_integer(i64::MAX), SerialType::Int64);\n}\n#[test]\nfn test_row_serialization_roundtrip() {\n    let row = RowRecord {\n        columns: vec![\n            ColumnValue::Integer(42),\n            ColumnValue::Text(\"hello\".to_string()),\n            ColumnValue::Null,\n            ColumnValue::Float(3.14159),\n            ColumnValue::Blob(vec![0x00, 0xFF, 0xAB]),\n        ],\n    };\n    let serialized = row.serialize().unwrap();\n    let (deserialized, _) = RowRecord::deserialize(&serialized).unwrap();\n    assert_eq!(row.columns.len(), deserialized.columns.len());\n    for (a, b) in row.columns.iter().zip(deserialized.columns.iter()) {\n        assert_column_values_equal(a, b);\n    }\n}\n```\n### 8.2 Integration Tests\n```rust\n// tests/btree/test_btree.rs\n#[test]\nfn test_btree_insert_and_find() {\n    let mut btree = Btree::new(temp_database_path());\n    let root = btree.create_btree(true).unwrap();\n    // Insert rows\n    for i in 1..=100 {\n        let row = RowRecord {\n            columns: vec![ColumnValue::Integer(i * 10)],\n        };\n        let rowid = btree.insert_row(root, &row).unwrap();\n        assert_eq!(rowid, i as u64);\n    }\n    // Find rows\n    for i in 1..=100 {\n        let found = btree.find_row(root, i as u64).unwrap();\n        assert!(found.is_some());\n        let row = found.unwrap();\n        assert_eq!(row.columns[0], ColumnValue::Integer(i * 10));\n    }\n}\n#[test]\nfn test_node_splitting() {\n    let mut btree = Btree::new(temp_database_path());\n    let root = btree.create_btree(true).unwrap();\n    // Insert enough rows to cause multiple splits\n    for i in 1..=1000 {\n        let row = RowRecord {\n            columns: vec![\n                ColumnValue::Integer(i),\n                ColumnValue::Text(\"x\".repeat(100)),  // Larger payload\n            ],\n        };\n        btree.insert_row(root, &row).unwrap();\n    }\n    // Verify all rows still accessible\n    for i in 1..=1000 {\n        let found = btree.find_row(root, i as u64).unwrap();\n        assert!(found.is_some());\n    }\n}\n#[test]\nfn test_catalog_persistence() {\n    let path = temp_database_path();\n    // Create database with table\n    {\n        let mut catalog = Catalog::new(&path);\n        catalog.initialize_catalog().unwrap();\n        catalog.create_table_entry(&SqliteMasterRow {\n            object_type: \"table\".to_string(),\n            name: \"users\".to_string(),\n            tbl_name: \"users\".to_string(),\n            rootpage: 2,\n            sql: Some(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)\".to_string()),\n        }).unwrap();\n    }\n    // Reopen and verify\n    {\n        let catalog = Catalog::new(&path);\n        let table = catalog.find_table(\"users\").unwrap();\n        assert!(table.is_some());\n        assert_eq!(table.unwrap().rootpage, 2);\n    }\n}\n```\n### 8.3 Stress Tests\n```rust\n#[test]\nfn stress_test_large_payloads() {\n    let mut btree = Btree::new(temp_database_path());\n    let root = btree.create_btree(true).unwrap();\n    // Insert rows with payloads approaching overflow threshold\n    for i in 0..100 {\n        let large_text = \"x\".repeat(3000);  // ~3KB per row\n        let row = RowRecord {\n            columns: vec![\n                ColumnValue::Integer(i),\n                ColumnValue::Text(large_text.clone()),\n            ],\n        };\n        btree.insert_row(root, &row).unwrap();\n        // Verify immediately\n        let found = btree.find_row(root, (i + 1) as u64).unwrap();\n        assert!(found.is_some());\n    }\n}\n#[test]\nfn stress_test_concurrent_tables() {\n    let mut btree = Btree::new(temp_database_path());\n    let mut roots = Vec::new();\n    for _ in 0..10 {\n        roots.push(btree.create_btree(true).unwrap());\n    }\n    // Insert into all tables\n    for i in 0..100 {\n        for &root in &roots {\n            let row = RowRecord {\n                columns: vec![ColumnValue::Integer(i)],\n            };\n            btree.insert_row(root, &row).unwrap();\n        }\n    }\n    // Verify all tables\n    for &root in &roots {\n        for i in 1..=100 {\n            assert!(btree.find_row(root, i).unwrap().is_some());\n        }\n    }\n}\n```\n---\n## 9. Performance Targets\n| Operation | Target | Measurement Method |\n|-----------|--------|-------------------|\n| Varint encode | < 50ns | Benchmark with criterion |\n| Varint decode | < 50ns | Benchmark with criterion |\n| Row serialize (5 cols) | < 500ns | Benchmark mixed types |\n| Row deserialize (5 cols) | < 500ns | Benchmark mixed types |\n| Cell insert (sorted) | < 10\u00b5s | With binary search |\n| Page split | < 100\u00b5s | 100 cells average |\n| B-tree insert | < 50\u00b5s | Single row, cached pages |\n| B-tree find | < 20\u00b5s | 3-level tree, cached |\n| Catalog lookup (cached) | < 1\u00b5s | In-memory hash lookup |\n| Catalog lookup (uncached) | < 100\u00b5s | Single page read |\n**Memory Constraints:**\n- Per-page overhead: < 64 bytes (excluding page buffer)\n- Cell cache entry: < 32 bytes\n- Catalog cache: < 1KB per table\n---\n## 10. Diagrams\n### 10.1 Page Header Layout\n{{DIAGRAM:page_header}}\n### 10.2 Slotted Page Structure\n{{DIAGRAM:slotted_page}}\n### 10.3 Varint Encoding\n{{DIAGRAM:varint_encoding}}\n### 10.4 Row Record Format\n{{DIAGRAM:row_record}}\n### 10.5 Cell Formats\n{{DIAGRAM:cell_formats}}\n### 10.6 Node Splitting\n{{DIAGRAM:node_split}}\n### 10.7 B-tree Structure\n{{DIAGRAM:btree_structure}}\n---\n## CRITERIA_JSON\n```json\n{\n  \"module_id\": \"build-sqlite-m5\",\n  \"version\": \"1.0.0\",\n  \"completion_criteria\": [\n    {\n      \"id\": \"varint-codec\",\n      \"description\": \"Varint encoding/decoding with 1-9 byte support\",\n      \"test_coverage\": [\"test_varint_boundaries\", \"test_varint_roundtrip\"],\n      \"acceptance\": \"All boundary values (0, 127, 128, 16383, 16384, u64::MAX) roundtrip correctly\"\n    },\n    {\n      \"id\": \"serial-types\",\n      \"description\": \"Complete serial type system for NULL, integers, floats, blobs, text\",\n      \"test_coverage\": [\"test_serial_type_optimization\", \"test_serial_type_all_types\"],\n      \"acceptance\": \"Serial types 0-9 and N>=12 computed and parsed correctly\"\n    },\n    {\n      \"id\": \"row-serialization\",\n      \"description\": \"Row record serialization with header and content sections\",\n      \"test_coverage\": [\"test_row_serialization_roundtrip\"],\n      \"acceptance\": \"Rows with mixed column types serialize/deserialize identically\"\n    },\n    {\n      \"id\": \"page-format\",\n      \"description\": \"Page header and cell pointer array management\",\n      \"test_coverage\": [\"test_page_header\", \"test_cell_pointers\", \"test_free_space\"],\n      \"acceptance\": \"Pages maintain correct header state and cell pointer array\"\n    },\n    {\n      \"id\": \"cell-operations\",\n      \"description\": \"Cell CRUD with binary search insertion\",\n      \"test_coverage\": [\"test_cell_insert\", \"test_cell_delete\", \"test_cell_find\"],\n      \"acceptance\": \"Cells inserted in sorted order, found by key, deleted cleanly\"\n    },\n    {\n      \"id\": \"btree-operations\",\n      \"description\": \"B-tree insert, find, delete with tree traversal\",\n      \"test_coverage\": [\"test_btree_insert_and_find\", \"test_btree_delete\"],\n      \"acceptance\": \"Trees support 1000+ rows with correct retrieval\"\n    },\n    {\n      \"id\": \"node-splitting\",\n      \"description\": \"Node splitting with 50% fill factor\",\n      \"test_coverage\": [\"test_node_splitting\", \"test_split_maintains_invariants\"],\n      \"acceptance\": \"Splits produce balanced trees, all rows remain accessible\"\n    },\n    {\n      \"id\": \"system-catalog\",\n      \"description\": \"sqlite_master persistence and lookup\",\n      \"test_coverage\": [\"test_catalog_persistence\", \"test_catalog_crud\"],\n      \"acceptance\": \"Schema entries persist across database sessions\"\n    },\n    {\n      \"id\": \"performance\",\n      \"description\": \"Meet all performance targets\",\n      \"test_coverage\": [\"bench_varint\", \"bench_row_serialize\", \"bench_btree_operations\"],\n      \"acceptance\": \"All benchmarks within specified targets\"\n    }\n  ],\n  \"dependencies\": [\"build-sqlite-m4\"],\n  \"estimated_hours\": 52,\n  \"deliverables\": [\n    \"src/btree/mod.rs\",\n    \"src/btree/varint.rs\",\n    \"src/btree/serial_type.rs\",\n    \"src/btree/row.rs\",\n    \"src/btree/page.rs\",\n    \"src/btree/cell.rs\",\n    \"src/btree/table_btree.rs\",\n    \"src/btree/index_btree.rs\",\n    \"src/btree/split.rs\",\n    \"src/btree/catalog.rs\",\n    \"tests/btree/*.rs\"\n  ]\n}\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m6 -->\n# Technical Design Specification: build-sqlite-m6\n## SELECT Execution & DML\n---\n## 1. Module Charter\n### 1.1 Purpose\nModule `build-sqlite-m6` implements the runtime execution layer for SQL data manipulation operations. This module bridges the gap between compiled VDBE bytecode programs (M3) and the physical storage layer (M4/M5), providing cursor-based table traversal, three-valued logic evaluation for WHERE clauses, column projection, and constraint-enforced INSERT/UPDATE/DELETE operations.\n### 1.2 Scope\n| In Scope | Out of Scope |\n|----------|--------------|\n| Cursor lifecycle management | Query optimization |\n| Three-valued logic (TRUE/FALSE/NULL) | Index-based lookups |\n| Sequential table scans | Join algorithms |\n| Column projection | Aggregate functions |\n| WHERE clause evaluation | ORDER BY sorting |\n| INSERT with constraint checking | GROUP BY processing |\n| UPDATE with re-insertion pattern | Subquery execution |\n| DELETE with two-pass algorithm | Transaction management |\n| NOT NULL constraint enforcement | WAL recovery |\n| UNIQUE constraint enforcement | Foreign key constraints |\n### 1.3 Dependencies\n```\nbuild-sqlite-m6\n\u251c\u2500\u2500 build-sqlite-m3 (Bytecode Compiler)\n\u2502   \u251c\u2500\u2500 VDBE opcode definitions\n\u2502   \u251c\u2500\u2500 Register file structure\n\u2502   \u2514\u2500\u2500 Program counter semantics\n\u251c\u2500\u2500 build-sqlite-m4 (Buffer Pool)\n\u2502   \u251c\u2500\u2500 Page pin/unpin operations\n\u2502   \u251c\u2500\u2500 Frame allocation\n\u2502   \u2514\u2500\u2500 Dirty page tracking\n\u2514\u2500\u2500 build-sqlite-m5 (B-tree Storage)\n    \u251c\u2500\u2500 Page header format\n    \u251c\u2500\u2500 Cell iteration APIs\n    \u251c\u2500\u2500 Row serialization\n    \u2514\u2500\u2500 Varint encoding/decoding\n```\n### 1.4 Success Metrics\n| Metric | Target | Measurement Method |\n|--------|--------|-------------------|\n| Full table scan (10K rows) | < 50ms | Benchmark with sequential data |\n| WHERE with NULL evaluation | < 10% overhead vs non-NULL | Comparative benchmark |\n| Constraint check per row | < 1\u03bcs | Micro-benchmark |\n| Memory per cursor | < 512 bytes | Static analysis + runtime check |\n| Test coverage | > 90% lines | gcov/lcov report |\n---\n## 2. File Structure\n```\nsrc/\n\u251c\u2500\u2500 execution/\n\u2502   \u251c\u2500\u2500 cursor.c              # Cursor creation, positioning, movement\n\u2502   \u251c\u2500\u2500 cursor.h              # Cursor struct and API declarations\n\u2502   \u251c\u2500\u2500 executor.c            # Main execution loop, opcode dispatch\n\u2502   \u251c\u2500\u2500 executor.h            # Executor context and public API\n\u2502   \u251c\u2500\u2500 projection.c          # Column extraction and type coercion\n\u2502   \u251c\u2500\u2500 projection.h          # Projection function signatures\n\u2502   \u251c\u2500\u2500 three_valued.c        # 3VL logic operations (AND, OR, NOT, IS)\n\u2502   \u251c\u2500\u2500 three_valued.h        # TriBool enum and operations\n\u2502   \u251c\u2500\u2500 where_eval.c          # WHERE clause expression evaluation\n\u2502   \u251c\u2500\u2500 where_eval.h          # Expression tree evaluation API\n\u2502   \u251c\u2500\u2500 dml_insert.c          # INSERT execution with constraints\n\u2502   \u251c\u2500\u2500 dml_insert.h          # Insert operation signatures\n\u2502   \u251c\u2500\u2500 dml_update.c          # UPDATE execution (delete + re-insert)\n\u2502   \u251c\u2500\u2500 dml_update.h          # Update operation signatures\n\u2502   \u251c\u2500\u2500 dml_delete.c          # DELETE execution (two-pass algorithm)\n\u2502   \u251c\u2500\u2500 dml_delete.h          # Delete operation signatures\n\u2502   \u2514\u2500\u2500 constraints.c         # Constraint checking utilities\n\u2502   \u2514\u2500\u2500 constraints.h         # Constraint error reporting\n\u251c\u2500\u2500 vdbe/\n\u2502   \u2514\u2500\u2500 opcodes.h             # Opcode definitions (extend from M3)\n\u2514\u2500\u2500 include/\n    \u2514\u2500\u2500 sqlite_clone/\n        \u251c\u2500\u2500 execution_types.h # Public type definitions\n        \u2514\u2500\u2500 errors.h          # Error code definitions (extend)\ntests/\n\u251c\u2500\u2500 unit/\n\u2502   \u251c\u2500\u2500 test_cursor.c\n\u2502   \u251c\u2500\u2500 test_three_valued.c\n\u2502   \u251c\u2500\u2500 test_projection.c\n\u2502   \u251c\u2500\u2500 test_where_eval.c\n\u2502   \u251c\u2500\u2500 test_constraints.c\n\u2502   \u2514\u2500\u2500 test_dml_operations.c\n\u251c\u2500\u2500 integration/\n\u2502   \u251c\u2500\u2500 test_select_execution.c\n\u2502   \u251c\u2500\u2500 test_insert_constraints.c\n\u2502   \u251c\u2500\u2500 test_update_execution.c\n\u2502   \u2514\u2500\u2500 test_delete_execution.c\n\u2514\u2500\u2500 benchmark/\n    \u251c\u2500\u2500 bench_table_scan.c\n    \u251c\u2500\u2500 bench_where_null.c\n    \u2514\u2500\u2500 bench_constraints.c\n```\n---\n## 3. Complete Data Model\n### 3.1 TriBool Enumeration\nThree-valued logic is fundamental to SQL NULL handling.\n```c\n// three_valued.h\ntypedef enum {\n    TRI_FALSE = 0,\n    TRI_TRUE  = 1,\n    TRI_NULL  = 2\n} TriBool;\n// Truth tables\n// AND: TRUE AND NULL = NULL, FALSE AND NULL = FALSE (short-circuit)\n// OR:  TRUE OR NULL = TRUE (short-circuit), FALSE OR NULL = NULL\n// NOT: NOT NULL = NULL\n// IS:  NULL IS NULL = TRUE (special case)\n```\n### 3.2 Cursor Structure\n```c\n// cursor.h\ntypedef struct Cursor {\n    // Identity\n    uint32_t cursor_id;         // Unique cursor identifier\n    uint32_t table_id;          // Root page number of table B-tree\n    // Position state\n    uint32_t current_page;      // Current B-tree page number\n    uint16_t cell_index;        // Index within page's cell array\n    bool    is_eof;             // End-of-table flag\n    bool    is_before_first;    // Positioned before first row\n    // Buffer pool integration\n    BufferPool* pool;           // Reference to buffer pool\n    Frame*      pinned_frame;   // Currently pinned frame (may be NULL)\n    // Row cache\n    Row current_row;            // Deserialized current row\n    bool row_valid;             // Whether current_row is populated\n    // Write tracking (for DML)\n    bool is_writable;           // Cursor allows modifications\n    uint32_t last_rowid;        // Last rowid seen (for iteration stability)\n} Cursor;\n// Size: ~64 bytes (target: < 512 bytes with slack)\n```\n**Memory Layout (64 bytes target):**\n| Offset | Size | Field | Description |\n|--------|------|-------|-------------|\n| 0x00 | 4 | cursor_id | Unique identifier |\n| 0x04 | 4 | table_id | Root page number |\n| 0x08 | 4 | current_page | Active page |\n| 0x0C | 2 | cell_index | Cell position |\n| 0x0E | 1 | is_eof | EOF flag |\n| 0x0F | 1 | is_before_first | BOF flag |\n| 0x10 | 8 | pool | Pointer to buffer pool |\n| 0x18 | 8 | pinned_frame | Pinned frame pointer |\n| 0x20 | 24 | current_row | Embedded row struct |\n| 0x38 | 1 | row_valid | Cache validity |\n| 0x39 | 1 | is_writable | Write permission |\n| 0x3A | 4 | last_rowid | Last seen rowid |\n| 0x3E | 2 | padding | Alignment |\n### 3.3 Row Structure\n```c\n// execution_types.h\ntypedef struct Row {\n    uint32_t rowid;             // Row identifier\n    uint16_t column_count;      // Number of columns\n    uint16_t data_size;         // Size of serialized data\n    uint8_t* data;              // Serialized column data (varint format)\n    // Type cache (optional, for performance)\n    ColumnType* column_types;   // Type of each column (cached)\n    uint16_t* column_offsets;   // Offset of each column in data\n} Row;\n// Serialized row format (from M5):\n// [payload_size:varint] [rowid:varint] [header_size:varint] [types...] [values...]\n```\n### 3.4 Executor Context\n```c\n// executor.h\ntypedef struct Executor {\n    // VDBE state\n    VdbeProgram* program;       // Bytecode program to execute\n    uint32_t pc;                // Program counter\n    Register* registers;        // Register file\n    uint16_t register_count;    // Number of registers\n    // Cursor management\n    Cursor** cursors;           // Array of open cursors\n    uint16_t cursor_count;      // Number of open cursors\n    uint16_t cursor_capacity;   // Allocated capacity\n    // Result handling\n    ResultRow* result_rows;     // Accumulated result rows\n    uint32_t result_count;      // Number of result rows\n    uint32_t result_capacity;   // Allocated capacity\n    // Error state\n    ErrorCode last_error;       // Last error code\n    char error_message[256];    // Human-readable error\n    // Execution flags\n    bool halted;                // Execution complete\n    bool aborted;               // Execution aborted (error)\n} Executor;\n```\n### 3.5 Constraint Descriptor\n```c\n// constraints.h\ntypedef enum {\n    CONSTRAINT_NOT_NULL,\n    CONSTRAINT_UNIQUE,\n    CONSTRAINT_PRIMARY_KEY\n} ConstraintType;\ntypedef struct Constraint {\n    ConstraintType type;\n    uint16_t column_index;      // Column this constraint applies to\n    const char* column_name;    // Column name for error messages\n    const char* constraint_name;// Optional constraint name\n} Constraint;\ntypedef struct ConstraintViolation {\n    ConstraintType type;\n    uint16_t column_index;\n    char message[128];          // Formatted error message\n} ConstraintViolation;\n```\n### 3.6 Expression Node (for WHERE evaluation)\n```c\n// where_eval.h\ntypedef enum {\n    EXPR_LITERAL,               // Literal value (int, text, null)\n    EXPR_COLUMN,                // Column reference\n    EXPR_BINARY_OP,             // Binary operator (=, <, AND, OR)\n    EXPR_UNARY_OP,              // Unary operator (NOT, IS NULL)\n    EXPR_COMPARE                // Comparison with NULL handling\n} ExprType;\ntypedef struct ExprNode {\n    ExprType type;\n    union {\n        // EXPR_LITERAL\n        struct {\n            DataType data_type;\n            union {\n                int64_t int_val;\n                double float_val;\n                const char* text_val;\n            };\n        } literal;\n        // EXPR_COLUMN\n        struct {\n            uint16_t column_index;\n            const char* column_name;\n        } column;\n        // EXPR_BINARY_OP, EXPR_UNARY_OP\n        struct {\n            OpCode op;              // Operator\n            struct ExprNode* left;  // Left operand\n            struct ExprNode* right; // Right operand (NULL for unary)\n        } op;\n    };\n} ExprNode;\n```\n---\n## 4. Interface Contracts\n### 4.1 Cursor API\n```c\n// cursor.h\n/**\n * Create a new cursor for table traversal.\n * \n * @param pool       Buffer pool for page access\n * @param table_id   Root page number of the table\n * @param writable   true if cursor will be used for DML\n * @return           New cursor or NULL on allocation failure\n */\nCursor* cursor_create(BufferPool* pool, uint32_t table_id, bool writable);\n/**\n * Destroy a cursor and release resources.\n * Unpins any held page frame.\n * \n * @param cursor     Cursor to destroy\n */\nvoid cursor_destroy(Cursor* cursor);\n/**\n * Position cursor before the first row.\n * Sets is_before_first=true, is_eof=false.\n * \n * @param cursor     Target cursor\n * @return           EXEC_OK or error code\n */\nErrorCode cursor_rewind(Cursor* cursor);\n/**\n * Advance cursor to next row.\n * Updates current_row cache.\n * \n * @param cursor     Target cursor\n * @return           EXEC_OK if row available, EXEC_EOF if end of table\n */\nErrorCode cursor_next(Cursor* cursor);\n/**\n * Get current row from cursor.\n * Returns pointer to internally cached row (do not free).\n * \n * @param cursor     Target cursor\n * @param out_row    Output pointer to current row\n * @return           EXEC_OK or EXEC_NO_CURRENT_ROW\n */\nErrorCode cursor_get_row(Cursor* cursor, const Row** out_row);\n/**\n * Get column value from current row.\n * Handles NULL values and type coercion.\n * \n * @param cursor     Target cursor\n * @param col_index  Column index (0-based)\n * @param out_value  Output value structure\n * @return           EXEC_OK or EXEC_COLUMN_OUT_OF_RANGE\n */\nErrorCode cursor_get_column(Cursor* cursor, uint16_t col_index, Value* out_value);\n```\n### 4.2 Three-Valued Logic API\n```c\n// three_valued.h\n/**\n * Three-valued AND operation.\n * Truth table:\n *   AND | F | T | N\n *   ----+---+---+---\n *   F   | F | F | F\n *   T   | F | T | N\n *   N   | F | N | N\n */\nTriBool tribool_and(TriBool a, TriBool b);\n/**\n * Three-valued OR operation.\n * Truth table:\n *   OR  | F | T | N\n *   ----+---+---+---\n *   F   | F | T | N\n *   T   | T | T | T\n *   N   | N | T | N\n */\nTriBool tribool_or(TriBool a, TriBool b);\n/**\n * Three-valued NOT operation.\n * Truth table: NOT(F)=T, NOT(T)=F, NOT(N)=N\n */\nTriBool tribool_not(TriBool a);\n/**\n * IS comparison (NULL-aware equality).\n * NULL IS NULL = TRUE\n * x IS NULL = FALSE (for non-NULL x)\n * x IS y = (x == y) for non-NULL x, y\n */\nTriBool tribool_is(TriBool a_is_null, TriBool b_is_null, bool values_equal);\n/**\n * Convert comparison result to TriBool.\n * Handles NULL propagation in comparisons.\n */\nTriBool comparison_to_tribool(int cmp_result, bool a_null, bool b_null);\n```\n### 4.3 WHERE Evaluation API\n```c\n// where_eval.h\n/**\n * Evaluate WHERE expression against current row.\n * \n * @param expr       Expression tree root\n * @param row        Current row data\n * @param schema     Table schema for column lookup\n * @return           TRI_TRUE if row passes, TRI_FALSE if rejected,\n *                   TRI_NULL if result is indeterminate\n */\nTriBool where_evaluate(ExprNode* expr, const Row* row, const TableSchema* schema);\n/**\n * Evaluate expression to extract a value.\n * Used for computed columns and expressions.\n * \n * @param expr       Expression to evaluate\n * @param row        Current row data\n * @param schema     Table schema\n * @param out_value  Output value (may be NULL)\n * @return           true if value is non-NULL, false if NULL\n */\nbool expr_evaluate_value(ExprNode* expr, const Row* row, \n                         const TableSchema* schema, Value* out_value);\n```\n### 4.4 Projection API\n```c\n// projection.h\n/**\n * Project selected columns from a row.\n * Creates a new row containing only specified columns.\n * \n * @param source        Source row\n * @param col_indices   Array of column indices to project\n * @param col_count     Number of columns to project\n * @param out_row       Output projected row (caller frees)\n * @return              EXEC_OK or error code\n */\nErrorCode project_columns(const Row* source, const uint16_t* col_indices,\n                          uint16_t col_count, Row** out_row);\n/**\n * Extract single column value from row.\n * \n * @param row         Source row\n * @param col_index   Column index\n * @param schema      Table schema for type info\n * @param out_value   Output value\n * @return            EXEC_OK, EXEC_NULL_VALUE, or error\n */\nErrorCode extract_column_value(const Row* row, uint16_t col_index,\n                               const TableSchema* schema, Value* out_value);\n/**\n * Format row values as strings for output.\n * \n * @param row         Row to format\n * @param schema      Table schema\n * @param delimiter   Column delimiter\n * @param out_buffer  Output buffer\n * @param buffer_size Buffer capacity\n * @return            Bytes written or negative error\n */\nint format_row_output(const Row* row, const TableSchema* schema,\n                      const char* delimiter, char* out_buffer, size_t buffer_size);\n```\n### 4.5 DML API\n```c\n// dml_insert.h\n/**\n * Execute INSERT operation.\n * Validates constraints, allocates rowid, inserts into B-tree.\n * \n * @param executor    Executor context\n * @param table_id    Target table root page\n * @param values      Column values to insert\n * @param col_count   Number of columns\n * @param constraints Table constraints (may be NULL)\n * @return            EXEC_OK or constraint error\n */\nErrorCode execute_insert(Executor* executor, uint32_t table_id,\n                         const Value* values, uint16_t col_count,\n                         const Constraint* constraints);\n// dml_update.h\n/**\n * Execute UPDATE operation.\n * Uses delete-then-reinsert pattern.\n * \n * @param executor    Executor context\n * @param cursor      Cursor positioned on target row\n * @param new_values  New column values (NULL for unchanged)\n * @param col_count   Number of columns\n * @param constraints Table constraints\n * @return            EXEC_OK or error\n */\nErrorCode execute_update(Executor* executor, Cursor* cursor,\n                         const Value* new_values, uint16_t col_count,\n                         const Constraint* constraints);\n// dml_delete.h\n/**\n * Execute single-row DELETE.\n * \n * @param executor    Executor context\n * @param cursor      Cursor positioned on row to delete\n * @return            EXEC_OK or error\n */\nErrorCode execute_delete_single(Executor* executor, Cursor* cursor);\n/**\n * Execute multi-row DELETE (two-pass algorithm).\n * First pass collects rowids, second pass deletes.\n * \n * @param executor    Executor context\n * @param table_id    Target table\n * @param where_expr  WHERE clause (NULL = delete all)\n * @return            Number of rows deleted or negative error\n */\nint64_t execute_delete_with_where(Executor* executor, uint32_t table_id,\n                                   ExprNode* where_expr);\n```\n### 4.6 Constraint Checking API\n```c\n// constraints.h\n/**\n * Check NOT NULL constraint.\n * \n * @param value       Value to check\n * @param constraint  Constraint descriptor\n * @param violation   Output violation details (if any)\n * @return            true if valid, false if violated\n */\nbool check_not_null(const Value* value, const Constraint* constraint,\n                    ConstraintViolation* violation);\n/**\n * Check UNIQUE constraint.\n * Requires index lookup (future: use B-tree for uniqueness).\n * \n * @param executor    Executor context\n * @param table_id    Table to check\n * @param col_index   Column index\n * @param value       Value to check for uniqueness\n * @param constraint  Constraint descriptor\n * @param violation   Output violation details\n * @return            true if unique, false if duplicate\n */\nbool check_unique(Executor* executor, uint32_t table_id, uint16_t col_index,\n                  const Value* value, const Constraint* constraint,\n                  ConstraintViolation* violation);\n/**\n * Format constraint violation as error message.\n * \n * @param violation   Violation details\n * @param buffer      Output buffer\n * @param buffer_size Buffer capacity\n * @return            Bytes written\n */\nint format_constraint_error(const ConstraintViolation* violation,\n                            char* buffer, size_t buffer_size);\n```\n---\n## 5. Algorithm Specification\n### 5.1 Main Execution Loop\n```\nALGORITHM: execute_program\nINPUT: VdbeProgram* program, BufferPool* pool\nOUTPUT: ExecutionResult\n1. INIT executor context:\n   - Allocate register file (program->max_reg registers)\n   - Initialize cursor array (empty)\n   - Set pc = 0, halted = false\n2. WHILE NOT halted:\n   a. FETCH instruction at program->instructions[pc]\n   b. DECODE opcode and operands\n   c. DISPATCH to opcode handler:\n      - OP_OpenRead    -> handle_open_read\n      - OP_OpenWrite   -> handle_open_write\n      - OP_Rewind      -> handle_rewind\n      - OP_Next        -> handle_next\n      - OP_Column      -> handle_column\n      - OP_ResultRow   -> handle_result_row\n      - OP_MakeRecord  -> handle_make_record\n      - OP_Insert      -> handle_insert\n      - OP_Update      -> handle_update\n      - OP_Delete      -> handle_delete\n      - OP_Halt        -> handle_halt\n      - ... (other opcodes from M3)\n   d. IF error: SET aborted = true; BREAK\n   e. INCREMENT pc (unless handler modified it)\n3. CLEANUP:\n   - Destroy all cursors\n   - Free registers\n   - RETURN result\n```\n### 5.2 Cursor Traversal Algorithm\n```\nALGORITHM: cursor_next\nINPUT: Cursor* cursor\nOUTPUT: ErrorCode\n1. IF cursor->is_eof:\n   RETURN EXEC_EOF\n2. IF cursor->is_before_first:\n   // Move to first row\n   cursor->current_page = cursor->table_id\n   cursor->cell_index = 0\n   cursor->is_before_first = false\n   GOTO load_row\n3. // Advance to next cell in current page\n   cursor->cell_index++\n4. GET page at cursor->current_page\n   GET cell_count from page header\n5. IF cursor->cell_index >= cell_count:\n   // Need to move to next page (or child)\n   CALL advance_to_next_page(cursor)\n   IF cursor->is_eof:\n     RETURN EXEC_EOF\n6. load_row:\n   CALL deserialize_row_at_cursor(cursor)\n   cursor->row_valid = true\n   cursor->last_rowid = cursor->current_row.rowid\n7. RETURN EXEC_OK\nALGORITHM: advance_to_next_page\nINPUT: Cursor* cursor\nOUTPUT: void (sets is_eof if done)\n1. GET current page header\n2. IF page is leaf:\n   // Check for rightmost pointer\n   IF page->right_child == 0:\n     cursor->is_eof = true\n     RETURN\n   ELSE:\n     cursor->current_page = page->right_child\n     cursor->cell_index = 0\n     // Descend to leftmost leaf of right child\n     CALL descend_to_leftmost_leaf(cursor)\n3. ELSE (page is interior):\n   // This shouldn't happen in simple sequential scan\n   // (we should be iterating leaf pages via sibling pointers)\n   cursor->is_eof = true\n```\n### 5.3 WHERE Clause Evaluation (Three-Valued Logic)\n```\nALGORITHM: where_evaluate\nINPUT: ExprNode* expr, Row* row, TableSchema* schema\nOUTPUT: TriBool\n1. SWITCH ON expr->type:\n   CASE EXPR_LITERAL:\n     IF expr->literal.data_type == TYPE_NULL:\n       RETURN TRI_NULL\n     ELSE:\n       RETURN TRI_TRUE  // Literal exists\n   CASE EXPR_COLUMN:\n     // Column reference doesn't evaluate to boolean directly\n     // Used in comparisons\n     RETURN TRI_TRUE\n   CASE EXPR_BINARY_OP:\n     SWITCH ON expr->op.op:\n       CASE OP_And:\n         left = where_evaluate(expr->op.left, row, schema)\n         right = where_evaluate(expr->op.right, row, schema)\n         RETURN tribool_and(left, right)\n       CASE OP_Or:\n         left = where_evaluate(expr->op.left, row, schema)\n         right = where_evaluate(expr->op.right, row, schema)\n         RETURN tribool_or(left, right)\n       CASE OP_Eq, OP_Ne, OP_Lt, OP_Le, OP_Gt, OP_Ge:\n         // Get values for comparison\n         left_null = get_expr_value(expr->op.left, row, &left_val)\n         right_null = get_expr_value(expr->op.right, row, &right_val)\n         IF left_null OR right_null:\n           // NULL comparison: result is NULL (unknown)\n           // EXCEPT for IS operator handled separately\n           RETURN TRI_NULL\n         cmp = compare_values(left_val, right_val)\n         RETURN comparison_to_tribool(cmp, expr->op.op)\n       CASE OP_Is:\n         // NULL-aware comparison\n         left_null = get_expr_value(expr->op.left, row, &left_val)\n         right_null = get_expr_value(expr->op.right, row, &right_val)\n         IF left_null AND right_null:\n           RETURN TRI_TRUE  // NULL IS NULL = TRUE\n         IF left_null OR right_null:\n           RETURN TRI_FALSE  // x IS NULL = FALSE for non-NULL x\n         RETURN (compare_values(left_val, right_val) == 0) ? TRI_TRUE : TRI_FALSE\n   CASE EXPR_UNARY_OP:\n     SWITCH ON expr->op.op:\n       CASE OP_Not:\n         inner = where_evaluate(expr->op.left, row, schema)\n         RETURN tribool_not(inner)\n       CASE OP_IsNull:\n         is_null = get_expr_value(expr->op.left, row, &val)\n         RETURN is_null ? TRI_TRUE : TRI_FALSE\n       CASE OP_NotNull:\n         is_null = get_expr_value(expr->op.left, row, &val)\n         RETURN is_null ? TRI_FALSE : TRI_TRUE\n2. RETURN TRI_NULL  // Unknown expression type\n```\n**Three-Valued Truth Tables:**\n```\nAND Operation:\n     | F   T   N\n-----+-----------\n  F  | F   F   F\n  T  | F   T   N\n  N  | F   N   N\nOR Operation:\n     | F   T   N\n-----+-----------\n  F  | F   T   N\n  T  | T   T   T\n  N  | N   T   N\nNOT Operation:\n  F -> T\n  T -> F\n  N -> N\nComparison with NULL:\n  NULL = 5     -> NULL (not FALSE!)\n  NULL = NULL  -> NULL (not TRUE!)\n  NULL < 5     -> NULL\n  NULL IS NULL -> TRUE (special case)\n```\n### 5.4 INSERT Execution\n```\nALGORITHM: execute_insert\nINPUT: Executor* exec, uint32_t table_id, Value[] values, Constraint[] constraints\nOUTPUT: ErrorCode\n1. VALIDATE constraints:\n   FOR each constraint IN constraints:\n     IF constraint.type == NOT_NULL:\n       IF values[constraint.column_index].is_null:\n         RETURN CONSTRAINT_NOT_NULL error with column name\n     IF constraint.type == UNIQUE OR constraint.type == PRIMARY_KEY:\n       IF NOT values[constraint.column_index].is_null:\n         // Check for existing value\n         found = btree_search(table_id, values[constraint.column_index])\n         IF found:\n           RETURN CONSTRAINT_UNIQUE error\n2. ALLOCATE rowid:\n   // For auto-increment or explicit rowid\n   IF values[0].is_null OR auto_increment:\n     rowid = get_next_rowid(table_id)\n   ELSE:\n     rowid = values[0].int_value\n3. SERIALIZE row:\n   record = serialize_row(values, col_count)\n   record_size = calculate_record_size(values)\n4. INSERT into B-tree:\n   error = btree_insert(table_id, rowid, record, record_size)\n   IF error:\n     RETURN error\n5. UPDATE indexes (if any):\n   // Future: maintain index B-trees\n6. RETURN EXEC_OK\n```\n### 5.5 UPDATE Execution (Delete + Re-insert Pattern)\n```\nALGORITHM: execute_update\nINPUT: Executor* exec, Cursor* cursor, Value[] new_values, Constraint[] constraints\nOUTPUT: ErrorCode\n1. VERIFY cursor is writable:\n   IF NOT cursor->is_writable:\n     RETURN EXEC_CURSOR_NOT_WRITABLE\n2. VERIFY cursor is positioned:\n   IF cursor->is_eof OR cursor->is_before_first:\n     RETURN EXEC_NO_CURRENT_ROW\n3. GET current row:\n   old_row = cursor->current_row\n   old_rowid = old_row->rowid\n4. MERGE values:\n   FOR i IN 0..col_count:\n     IF new_values[i].is_explicit:\n       merged[i] = new_values[i]\n     ELSE:\n       merged[i] = old_row->columns[i]\n5. VALIDATE constraints on merged values:\n   // Same as INSERT validation\n6. DELETE old row:\n   error = btree_delete_at_cursor(cursor)\n   IF error:\n     RETURN error\n7. INSERT new row (same rowid):\n   record = serialize_row(merged, col_count)\n   error = btree_insert_at_rowid(cursor->table_id, old_rowid, record)\n   IF error:\n     // CRITICAL: Old row is gone, new row failed\n     // In production: would need transaction rollback\n     RETURN error\n8. RETURN EXEC_OK\n```\n### 5.6 DELETE Execution (Two-Pass Algorithm)\n```\nALGORITHM: execute_delete_with_where\nINPUT: Executor* exec, uint32_t table_id, ExprNode* where_expr\nOUTPUT: int64_t rows_deleted (or negative error)\n1. FIRST PASS: Collect rowids to delete\n   CREATE empty array rowid_list\n   CREATE cursor = cursor_create(pool, table_id, false)\n   cursor_rewind(cursor)\n   WHILE NOT cursor->is_eof:\n     cursor_next(cursor)\n     IF cursor->is_eof: BREAK\n     // Evaluate WHERE clause\n     result = where_evaluate(where_expr, cursor->current_row, schema)\n     IF result == TRI_TRUE:\n       // Mark for deletion\n       APPEND cursor->current_row.rowid TO rowid_list\n     // NOTE: TRI_FALSE and TRI_NULL both skip the row\n     // (NULL result means \"unknown\" -> row is NOT deleted)\n2. SECOND PASS: Delete collected rows\n   deleted_count = 0\n   SORT rowid_list DESCENDING  // Delete from end first for stability\n   FOR each rowid IN rowid_list:\n     error = btree_delete_by_rowid(table_id, rowid)\n     IF error:\n       // Partial delete - return count so far\n       RETURN -(deleted_count + 1)  // Negative indicates error\n     deleted_count++\n3. CLEANUP:\n   cursor_destroy(cursor)\n   FREE rowid_list\n4. RETURN deleted_count\n```\n**Why Two-Pass Delete?**\n- Deleting while iterating corrupts cursor state\n- Page splits/merges change cell positions\n- Collecting rowids first ensures we visit all target rows\n- Deleting in reverse rowid order minimizes cursor disruption\n---\n## 6. Error Handling Matrix\n| Error Code | Constant | Condition | Recovery |\n|------------|----------|-----------|----------|\n| 0x1001 | `CONSTRAINT_NOT_NULL` | INSERT/UPDATE with NULL in NOT NULL column | Rollback statement, report column name |\n| 0x1002 | `CONSTRAINT_UNIQUE` | INSERT/UPDATE creates duplicate in UNIQUE column | Rollback statement, report column/value |\n| 0x1003 | `CONSTRAINT_PRIMARY_KEY` | INSERT with duplicate or NULL primary key | Rollback statement |\n| 0x1004 | `TABLE_NOT_FOUND` | OpenRead/OpenWrite on non-existent table | Abort execution |\n| 0x1005 | `COLUMN_NOT_FOUND` | Column reference to non-existent column | Abort execution |\n| 0x1006 | `CURSOR_NOT_OPEN` | Operation on closed/invalid cursor | Abort execution |\n| 0x1007 | `EXEC_NO_CURRENT_ROW` | Column access when cursor not positioned | Return NULL, continue |\n| 0x1008 | `EXEC_EOF` | Cursor past end of table | Normal condition, not error |\n| 0x1009 | `EXEC_OUT_OF_MEMORY` | Allocation failure during execution | Abort, cleanup |\n| 0x100A | `BTREE_CORRUPT` | B-tree invariant violation detected | Abort, report for repair |\n**Error Message Format:**\n```c\n// Constraint error messages\nstatic const char* ERROR_FMT_NOT_NULL = \n    \"NOT NULL constraint failed: %s.%s\";\nstatic const char* ERROR_FMT_UNIQUE = \n    \"UNIQUE constraint failed: %s.%s\";\nstatic const char* ERROR_FMT_PRIMARY_KEY = \n    \"PRIMARY KEY constraint failed: %s.%s\";\n```\n---\n## 7. Implementation Sequence\n### Phase 1: Three-Valued Logic Foundation (1.5 hours)\n**Files:** `src/execution/three_valued.c`, `src/execution/three_valued.h`\n**Checkpoints:**\n- [ ] Define `TriBool` enum with TRUE/FALSE/NULL values\n- [ ] Implement `tribool_and()` with correct truth table\n- [ ] Implement `tribool_or()` with short-circuit optimization\n- [ ] Implement `tribool_not()` \n- [ ] Implement `tribool_is()` for NULL-aware equality\n- [ ] Implement `comparison_to_tribool()` wrapper\n- [ ] Unit tests pass: `test_three_valued.c`\n**Test Cases:**\n```c\n// Expected behaviors\nassert(tribool_and(TRI_FALSE, TRI_NULL) == TRI_FALSE);  // Short-circuit\nassert(tribool_and(TRI_TRUE, TRI_NULL) == TRI_NULL);\nassert(tribool_or(TRI_TRUE, TRI_NULL) == TRI_TRUE);     // Short-circuit\nassert(tribool_or(TRI_FALSE, TRI_NULL) == TRI_NULL);\nassert(tribool_not(TRI_NULL) == TRI_NULL);\n```\n### Phase 2: Cursor Implementation (2 hours)\n**Files:** `src/execution/cursor.c`, `src/execution/cursor.h`\n**Checkpoints:**\n- [ ] Define `Cursor` struct with all fields\n- [ ] Implement `cursor_create()` with buffer pool binding\n- [ ] Implement `cursor_destroy()` with frame unpin\n- [ ] Implement `cursor_rewind()` to position before first\n- [ ] Implement `cursor_next()` with page traversal\n- [ ] Implement `cursor_get_row()` with row cache\n- [ ] Implement `cursor_get_column()` with NULL handling\n- [ ] Unit tests pass: `test_cursor.c`\n**Integration:** Link with M4 buffer pool and M5 B-tree APIs\n### Phase 3: WHERE Evaluation Engine (2 hours)\n**Files:** `src/execution/where_eval.c`, `src/execution/where_eval.h`\n**Checkpoints:**\n- [ ] Define `ExprNode` struct for expression trees\n- [ ] Implement `where_evaluate()` recursive evaluator\n- [ ] Handle `EXPR_BINARY_OP` with three-valued logic\n- [ ] Handle `EXPR_UNARY_OP` (NOT, IS NULL, NOT NULL)\n- [ ] Handle `EXPR_LITERAL` and `EXPR_COLUMN`\n- [ ] Implement `expr_evaluate_value()` for value extraction\n- [ ] Unit tests pass: `test_where_eval.c`\n**Key Test Cases:**\n```c\n// NULL = 5 should be NULL (not FALSE)\nassert(eval(\"NULL = 5\") == TRI_NULL);\n// NULL IS NULL should be TRUE\nassert(eval(\"NULL IS NULL\") == TRI_TRUE);\n// 5 IS NULL should be FALSE\nassert(eval(\"5 IS NULL\") == TRI_FALSE);\n// TRUE AND NULL should be NULL\nassert(eval(\"TRUE AND NULL\") == TRI_NULL);\n// FALSE AND NULL should be FALSE (short-circuit)\nassert(eval(\"FALSE AND NULL\") == TRI_FALSE);\n```\n### Phase 4: Column Projection (1 hour)\n**Files:** `src/execution/projection.c`, `src/execution/projection.h`\n**Checkpoints:**\n- [ ] Implement `project_columns()` for SELECT column list\n- [ ] Implement `extract_column_value()` with type handling\n- [ ] Implement `format_row_output()` for result display\n- [ ] Handle NULL column values in output\n- [ ] Unit tests pass: `test_projection.c`\n### Phase 5: Constraint Checking (1.5 hours)\n**Files:** `src/execution/constraints.c`, `src/execution/constraints.h`\n**Checkpoints:**\n- [ ] Define `Constraint` and `ConstraintViolation` structs\n- [ ] Implement `check_not_null()` validation\n- [ ] Implement `check_unique()` with B-tree lookup\n- [ ] Implement `format_constraint_error()` messages\n- [ ] Unit tests pass: `test_constraints.c`\n### Phase 6: INSERT Execution (1 hour)\n**Files:** `src/execution/dml_insert.c`, `src/execution/dml_insert.h`\n**Checkpoints:**\n- [ ] Implement `execute_insert()` with constraint checking\n- [ ] Integrate rowid allocation\n- [ ] Integrate B-tree insertion from M5\n- [ ] Return proper constraint errors\n- [ ] Unit tests pass: `test_dml_operations.c` (INSERT section)\n### Phase 7: UPDATE Execution (0.5 hours)\n**Files:** `src/execution/dml_update.c`, `src/execution/dml_update.h`\n**Checkpoints:**\n- [ ] Implement `execute_update()` with delete-reinsert pattern\n- [ ] Merge old and new column values\n- [ ] Validate constraints before committing\n- [ ] Unit tests pass: `test_dml_operations.c` (UPDATE section)\n### Phase 8: DELETE Execution (0.5 hours)\n**Files:** `src/execution/dml_delete.c`, `src/execution/dml_delete.h`\n**Checkpoints:**\n- [ ] Implement `execute_delete_single()` for one row\n- [ ] Implement `execute_delete_with_where()` two-pass algorithm\n- [ ] Collect rowids in first pass\n- [ ] Delete in reverse order in second pass\n- [ ] Return count of deleted rows\n- [ ] Unit tests pass: `test_dml_operations.c` (DELETE section)\n---\n## 8. Test Specification\n### 8.1 Unit Tests\n**File:** `tests/unit/test_three_valued.c`\n| Test Name | Description | Expected Result |\n|-----------|-------------|-----------------|\n| `test_and_truth_table` | Verify all 9 AND combinations | Matches SQL three-valued AND |\n| `test_or_truth_table` | Verify all 9 OR combinations | Matches SQL three-valued OR |\n| `test_not_truth_table` | Verify NOT for all 3 values | F->T, T->F, N->N |\n| `test_is_null` | NULL IS NULL | TRI_TRUE |\n| `test_is_not_null` | 5 IS NULL | TRI_FALSE |\n| `test_comparison_null` | NULL = 5, NULL < 5 | TRI_NULL for both |\n**File:** `tests/unit/test_cursor.c`\n| Test Name | Description | Expected Result |\n|-----------|-------------|-----------------|\n| `test_cursor_create_destroy` | Basic lifecycle | No memory leaks |\n| `test_cursor_rewind_empty` | Rewind on empty table | is_before_first=true |\n| `test_cursor_next_empty` | Next on empty table | EXEC_EOF |\n| `test_cursor_traverse_single` | Traverse one row | Gets row, then EOF |\n| `test_cursor_traverse_multiple` | Traverse 100 rows | All rows visited in order |\n| `test_cursor_get_column` | Extract column values | Correct values, handles NULL |\n**File:** `tests/unit/test_where_eval.c`\n| Test Name | Description | Expected Result |\n|-----------|-------------|-----------------|\n| `test_literal_true` | WHERE TRUE | TRI_TRUE |\n| `test_literal_false` | WHERE FALSE | TRI_FALSE |\n| `test_literal_null` | WHERE NULL | TRI_NULL |\n| `test_and_null_propagation` | TRUE AND NULL | TRI_NULL |\n| `test_and_short_circuit` | FALSE AND NULL | TRI_FALSE |\n| `test_or_short_circuit` | TRUE OR NULL | TRI_TRUE |\n| `test_comparison_with_null` | col = NULL (col is 5) | TRI_NULL |\n| `test_is_null_operator` | col IS NULL (col is NULL) | TRI_TRUE |\n| `test_is_not_null_operator` | col IS NOT NULL (col is 5) | TRI_TRUE |\n| `test_nested_expression` | (a > 5) AND (b IS NOT NULL) | Correct evaluation |\n**File:** `tests/unit/test_constraints.c`\n| Test Name | Description | Expected Result |\n|-----------|-------------|-----------------|\n| `test_not_null_pass` | Non-NULL value | Passes |\n| `test_not_null_fail` | NULL value | Returns violation |\n| `test_unique_pass` | Unique value | Passes |\n| `test_unique_fail` | Duplicate value | Returns violation |\n| `test_error_message_format` | Format violation | Correct message text |\n### 8.2 Integration Tests\n**File:** `tests/integration/test_select_execution.c`\n| Test Name | Description | Expected Result |\n|-----------|-------------|-----------------|\n| `test_select_star` | SELECT * FROM table | All columns returned |\n| `test_select_columns` | SELECT a, c FROM table | Only specified columns |\n| `test_select_where_eq` | SELECT WHERE col = 5 | Matching rows only |\n| `test_select_where_null` | SELECT WHERE col IS NULL | NULL column rows |\n| `test_select_where_and` | SELECT WHERE a > 5 AND b < 10 | Combined filter |\n| `test_select_empty_result` | SELECT WHERE FALSE | Empty result set |\n**File:** `tests/integration/test_insert_constraints.c`\n| Test Name | Description | Expected Result |\n|-----------|-------------|-----------------|\n| `test_insert_valid` | Insert valid row | Success |\n| `test_insert_not_null_violation` | Insert NULL in NOT NULL column | Error with column name |\n| `test_insert_unique_violation` | Insert duplicate in UNIQUE column | Error with column name |\n| `test_insert_multiple` | Insert 100 rows | All succeed |\n| `test_insert_rollback_on_error` | Batch insert with one bad row | None inserted (future) |\n**File:** `tests/integration/test_delete_execution.c`\n| Test Name | Description | Expected Result |\n|-----------|-------------|-----------------|\n| `test_delete_all` | DELETE FROM table | All rows removed |\n| `test_delete_where` | DELETE WHERE col = 5 | Only matching rows removed |\n| `test_delete_where_null` | DELETE WHERE col IS NULL | NULL rows removed |\n| `test_delete_none` | DELETE WHERE FALSE | Zero rows deleted |\n| `test_delete_returns_count` | Check return value | Correct count |\n### 8.3 Benchmark Tests\n**File:** `tests/benchmark/bench_table_scan.c`\n```c\n// Benchmark: Full table scan of 10,000 rows\n// Target: < 50ms\n// Setup: Create table with 10K rows, 5 columns each\n// Measure: Time to scan all rows with cursor_next()\nvoid benchmark_table_scan() {\n    // ... setup code ...\n    start_timer();\n    cursor_rewind(cursor);\n    while (cursor_next(cursor) != EXEC_EOF) {\n        // Access each row (simulate projection)\n        cursor_get_row(cursor, &row);\n    }\n    elapsed = stop_timer();\n    assert(elapsed < 50);  // milliseconds\n}\n```\n**File:** `tests/benchmark/bench_where_null.c`\n```c\n// Benchmark: WHERE evaluation with NULL values\n// Target: < 10% overhead vs non-NULL evaluation\n// Setup: Table with 10K rows, 50% NULL in filter column\nvoid benchmark_where_null() {\n    // ... setup code ...\n    // Measure baseline (no NULLs)\n    baseline_time = measure_where_eval(non_null_table);\n    // Measure with NULLs\n    null_time = measure_where_eval(mixed_null_table);\n    overhead = (null_time - baseline_time) / baseline_time * 100;\n    assert(overhead < 10);  // percent\n}\n```\n**File:** `tests/benchmark/bench_constraints.c`\n```c\n// Benchmark: Constraint checking per row\n// Target: < 1\u03bcs per row\nvoid benchmark_constraints() {\n    // Setup: 10,000 inserts with NOT NULL + UNIQUE constraints\n    start_timer();\n    for (int i = 0; i < 10000; i++) {\n        execute_insert(exec, table_id, values, constraints);\n    }\n    elapsed = stop_timer();\n    per_row_us = (elapsed * 1000) / 10000;  // microseconds\n    assert(per_row_us < 1);\n}\n```\n---\n## 9. Performance Targets\n| Operation | Target | Measurement | Priority |\n|-----------|--------|-------------|----------|\n| Full table scan (10K rows) | < 50ms | Wall clock | Critical |\n| WHERE evaluation with NULL | < 10% overhead | Comparative | High |\n| Constraint check per row | < 1\u03bcs | Micro-benchmark | High |\n| Cursor next() operation | < 5\u03bcs | Micro-benchmark | Medium |\n| Column extraction | < 1\u03bcs | Micro-benchmark | Medium |\n| Row serialization (INSERT) | < 10\u03bcs | Micro-benchmark | Medium |\n**Memory Budgets:**\n| Component | Budget | Justification |\n|-----------|--------|---------------|\n| Cursor struct | 512 bytes | Fixed per-cursor overhead |\n| Executor context | 4 KB | Registers, cursors, results |\n| Row cache per cursor | 256 bytes | Deserialized row data |\n| WHERE expression tree | 1 KB per expression | Recursive structure |\n| Rowid collection (DELETE) | 8 bytes \u00d7 row count | Two-pass algorithm |\n---\n## 10. Diagram Specifications\n### Diagram 1: Cursor State Machine\n**Placeholder:** `[[tdd-diag-m6-1]]`\n**Specification:** State diagram showing cursor states (BEFORE_FIRST, ON_ROW, AFTER_LAST/EOF) and transitions (rewind, next, delete). Include buffer pool pin/unpin actions on transitions.\n### Diagram 2: Three-Valued Logic Flow\n**Placeholder:** `[[tdd-diag-m6-2]]`\n**Specification:** Flowchart for WHERE clause evaluation with NULL handling. Show decision points for each TriBool result and how TRI_NULL flows through AND/OR operations.\n### Diagram 3: SELECT Execution Pipeline\n**Placeholder:** `[[tdd-diag-m6-3]]`\n**Specification:** Data flow diagram from VDBE program \u2192 Executor \u2192 Cursor \u2192 Row \u2192 Projection \u2192 Result. Show buffer pool and B-tree integration points.\n### Diagram 4: INSERT with Constraints\n**Placeholder:** `[[tdd-diag-m6-4]]`\n**Specification:** Sequence diagram for INSERT: receive values \u2192 check NOT NULL \u2192 check UNIQUE \u2192 allocate rowid \u2192 serialize \u2192 B-tree insert \u2192 return result. Show error paths.\n### Diagram 5: Two-Pass DELETE Algorithm\n**Placeholder:** `[[tdd-diag-m6-5]]`\n**Specification:** Two-phase diagram. Phase 1: Cursor scan \u2192 WHERE eval \u2192 collect rowids. Phase 2: Sort rowids DESC \u2192 iterate \u2192 B-tree delete each. Show why reverse order matters.\n### Diagram 6: UPDATE Delete-Reinsert Pattern\n**Placeholder:** `[[tdd-diag-m6-6]]`\n**Specification:** Sequence showing: positioned cursor \u2192 read old row \u2192 merge values \u2192 validate constraints \u2192 delete at cursor \u2192 insert new row at same rowid. Show rollback concern if insert fails.\n---\n## 11. Synced Criteria\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m7 -->\n# Technical Design Document: M7 - Secondary Indexes\n## Module Charter\nModule M7 implements B+tree secondary indexes that map column values to rowids, enabling efficient data retrieval without full table scans. Secondary indexes exist as separate B+tree structures distinct from table B-trees, storing (key, rowid) pairs in sorted order within leaf pages linked via right_sibling pointers for range scans. The module provides automatic index maintenance during INSERT, UPDATE, and DELETE operations, ensuring index-table consistency through transactional updates. UNIQUE constraints are enforced at the index level by detecting duplicate keys before modification commits. The query optimizer selects appropriate indexes based on WHERE clause predicates, employing the double lookup pattern where an index seek yields a rowid followed by a table seek to retrieve remaining columns. Covering indexes allow query resolution entirely from index data when all requested columns exist in the index key, eliminating the table lookup step. This module integrates with the existing VDBE execution engine, Buffer Pool manager, and B-tree storage layer to provide transparent index acceleration for SELECT, UPDATE, and DELETE statements.\n---\n## File Structure\n```\nsrc/\n\u251c\u2500\u2500 index/\n\u2502   \u251c\u2500\u2500 index.h              // [1] Index structure definitions and constants\n\u2502   \u251c\u2500\u2500 index.c              // [2] Index creation and drop implementation\n\u2502   \u251c\u2500\u2500 index_btree.h        // [3] Index B+tree page format definitions\n\u2502   \u251c\u2500\u2500 index_btree.c        // [4] Index B+tree operations (insert/delete/search)\n\u2502   \u251c\u2500\u2500 index_maintenance.h  // [5] Index maintenance API for DML hooks\n\u2502   \u2514\u2500\u2500 index_maintenance.c  // [6] Automatic index update on table mutation\n\u251c\u2500\u2500 vdbe/\n\u2502   \u251c\u2500\u2500 opcodes_index.h      // [7] Index-specific VDBE opcode definitions\n\u2502   \u2514\u2500\u2500 opcodes_index.c      // [8] Index opcode implementations\n\u251c\u2500\u2500 optimizer/\n\u2502   \u251c\u2500\u2500 index_selector.h     // [9] Index selection strategy definitions\n\u2502   \u2514\u2500\u2500 index_selector.c     // [10] Query optimizer index choice logic\n\u2514\u2500\u2500 parser/\n    \u2514\u2500\u2500 create_index.c       // [11] CREATE INDEX / DROP INDEX parsing\n```\n---\n## Data Model\n### Index Schema Entry\n```c\n// Stored in sqlite_master type='index'\ntypedef struct IndexSchema {\n    char*       name;           // Index name (owned pointer)\n    char*       table_name;     // Parent table name (owned pointer)\n    bool        is_unique;      // UNIQUE constraint flag\n    bool        is_primary;     // True if backing PRIMARY KEY\n    uint8_t     column_count;   // Number of indexed columns\n    int16_t*    columns;        // Column indices in table schema (owned)\n    uint8_t*    collations;     // Collation sequence per column (owned)\n    bool*       desc_order;     // DESC ordering per column (owned)\n    Pgno        root_page;      // B+tree root page number\n    char*       sql;            // Original CREATE INDEX statement\n} IndexSchema;\n// Memory management\nvoid index_schema_init(IndexSchema* idx);\nvoid index_schema_free(IndexSchema* idx);\nint  index_schema_parse(IndexSchema* idx, const char* sql);\n```\n### Index Entry Structure\n```c\n// In-memory representation of index key + rowid\ntypedef struct IndexEntry {\n    uint8_t*    key_data;       // Serialized column values (owned)\n    uint32_t    key_size;       // Key serialization size in bytes\n    Rowid       rowid;          // Target row identifier\n} IndexEntry;\n// Key serialization format:\n// [col1_type|col1_value][col2_type|col2_value]...[rowid_varint]\n// Types: 0=NULL, 1=I8, 2=I16, 3=I24, 4=I32, 5=I48, 6=I64, 7=F64, 8=0, 9=1, \u226512=BLOB/TEXT\n```\n### Index Cursor\n```c\ntypedef struct IndexCursor {\n    // Core state\n    Pgno            root_page;      // Index root page number\n    BufferPool*     pool;           // Buffer pool reference\n    BTreeCursor     btree_cur;      // Underlying B-tree cursor\n    // Positioning state\n    bool            positioned;     // Cursor is on valid entry\n    bool            eof;            // End of index reached\n    IndexEntry      current;        // Current entry buffer\n    // Range scan state\n    IndexEntry*     lower_bound;    // Inclusive start key (or NULL)\n    IndexEntry*     upper_bound;    // Exclusive end key (or NULL)\n    bool            upper_inclusive;// Upper bound inclusive flag\n    // Ownership flags\n    bool            owns_lower;     // Free lower_bound on close\n    bool            owns_upper;     // Free upper_bound on close\n} IndexCursor;\n// Lifecycle\nint  index_cursor_open(IndexCursor* cur, BufferPool* pool, Pgno root);\nint  index_cursor_close(IndexCursor* cur);\n```\n### Index Key Builder\n```c\ntypedef struct IndexKeyBuilder {\n    uint8_t*    buffer;         // Serialization buffer\n    uint32_t    capacity;       // Buffer capacity\n    uint32_t    size;           // Current write position\n    uint8_t     temp[256];      // Small static buffer for common cases\n} IndexKeyBuilder;\n// Building operations\nint  index_key_init(IndexKeyBuilder* kb, uint8_t column_count);\nint  index_key_append_null(IndexKeyBuilder* kb);\nint  index_key_append_int(IndexKeyBuilder* kb, int64_t value);\nint  index_key_append_float(IndexKeyBuilder* kb, double value);\nint  index_key_append_blob(IndexKeyBuilder* kb, const void* data, uint32_t len);\nint  index_key_append_text(IndexKeyBuilder* kb, const char* str, uint32_t len);\nint  index_key_append_rowid(IndexKeyBuilder* kb, Rowid rowid);\nint  index_key_finalize(IndexKeyBuilder* kb, IndexEntry* out);\nvoid index_key_reset(IndexKeyBuilder* kb);\n```\n### Table Index Registry\n```c\ntypedef struct TableIndexList {\n    IndexSchema**   indexes;        // Array of index pointers\n    uint32_t        count;          // Active index count\n    uint32_t        capacity;       // Array capacity\n} TableIndexList;\n// Registry operations\nint  table_index_list_init(TableIndexList* list);\nvoid table_index_list_free(TableIndexList* list);\nint  table_index_list_add(TableIndexList* list, IndexSchema* idx);\nint  table_index_list_remove(TableIndexList* list, const char* name);\nIndexSchema* table_index_list_find(TableIndexList* list, const char* name);\n```\n### Index Statistics\n```c\ntypedef struct IndexStats {\n    uint32_t    entry_count;        // Approximate row count\n    uint32_t    leaf_pages;         // Leaf page count\n    uint32_t    interior_pages;     // Interior page count\n    uint32_t    depth;              // Tree depth\n    uint8_t     avg_key_len;        // Average key length\n} IndexStats;\nint index_get_stats(BufferPool* pool, Pgno root, IndexStats* stats);\n```\n---\n## Interface Contracts\n### Index Creation\n```c\n/**\n * Create a secondary index on specified table columns.\n * \n * @param db        Database connection\n * @param idx_name  Index name (must be unique)\n * @param tbl_name  Table name (must exist)\n * @param columns   Column index array\n * @param col_count Number of columns\n * @param unique    Enforce UNIQUE constraint\n * @return SQLITE_OK, SQLITE_ERROR, SQLITE_CONSTRAINT\n */\nint index_create(Database* db, const char* idx_name, const char* tbl_name,\n                 const int16_t* columns, uint8_t col_count, bool unique);\n/**\n * Drop an existing index by name.\n * \n * @param db        Database connection\n * @param idx_name  Index name to drop\n * @param if_exists Suppress error if not found\n * @return SQLITE_OK, SQLITE_ERROR\n */\nint index_drop(Database* db, const char* idx_name, bool if_exists);\n```\n### Index Lookup Operations\n```c\n/**\n * Seek to first index entry matching the key prefix.\n * \n * @param cur       Index cursor (opened)\n * @param key       Search key (prefix match)\n * @return SQLITE_OK, SQLITE_NOTFOUND\n */\nint index_seek(IndexCursor* cur, const IndexEntry* key);\n/**\n * Seek to exact key match including rowid.\n * \n * @param cur       Index cursor\n * @param key       Full key with rowid\n * @return SQLITE_OK, SQLITE_NOTFOUND\n */\nint index_seek_exact(IndexCursor* cur, const IndexEntry* key);\n/**\n * Position cursor at first entry >= key.\n * \n * @param cur       Index cursor\n * @param key       Lower bound key (without rowid)\n * @return SQLITE_OK\n */\nint index_seek_ge(IndexCursor* cur, const IndexEntry* key);\n/**\n * Advance cursor to next index entry.\n * Uses right_sibling linkage through leaf pages.\n * \n * @param cur       Index cursor\n * @return SQLITE_OK, SQLITE_DONE (at EOF)\n */\nint index_next(IndexCursor* cur);\n/**\n * Move cursor to previous index entry.\n * \n * @param cur       Index cursor\n * @return SQLITE_OK, SQLITE_DONE (at BOF)\n */\nint index_prev(IndexCursor* cur);\n/**\n * Get rowid at current cursor position.\n * \n * @param cur       Index cursor\n * @param rowid_out Output rowid\n * @return SQLITE_OK, SQLITE_ERROR (not positioned)\n */\nint index_get_rowid(IndexCursor* cur, Rowid* rowid_out);\n/**\n * Get complete key at current position.\n * \n * @param cur       Index cursor\n * @param entry_out Output entry (caller frees)\n * @return SQLITE_OK\n */\nint index_get_entry(IndexCursor* cur, IndexEntry* entry_out);\n```\n### Index Maintenance Hooks\n```c\n/**\n * Insert entry into all table indexes after row insert.\n * Called by INSERT execution after successful table insert.\n * \n * @param db        Database connection\n * @param tbl_name  Table name\n * @param rowid     Newly inserted rowid\n * @param record    Inserted record values\n * @return SQLITE_OK, SQLITE_CONSTRAINT (UNIQUE violation)\n */\nint index_maintain_insert(Database* db, const char* tbl_name,\n                          Rowid rowid, const Record* record);\n/**\n * Delete entries from all table indexes before row delete.\n * Called by DELETE execution before table delete.\n * \n * @param db        Database connection\n * @param tbl_name  Table name\n * @param rowid     Rowid being deleted\n * @param record    Record values before deletion\n * @return SQLITE_OK\n */\nint index_maintain_delete(Database* db, const char* tbl_name,\n                          Rowid rowid, const Record* record);\n/**\n * Update index entries for modified row.\n * Called by UPDATE execution for affected rows.\n * \n * @param db        Database connection\n * @param tbl_name  Table name\n * @param rowid     Unchanged rowid\n * @param old_rec   Record before update\n * @param new_rec   Record after update\n * @param changed   Bitmap of changed columns\n * @return SQLITE_OK, SQLITE_CONSTRAINT\n */\nint index_maintain_update(Database* db, const char* tbl_name,\n                          Rowid rowid, const Record* old_rec,\n                          const Record* new_rec, const uint8_t* changed);\n```\n### Index Selection\n```c\n/**\n * Select optimal index for WHERE clause evaluation.\n * \n * @param table     Table schema\n * @param where     WHERE clause expression tree\n * @param order_by  ORDER BY columns (or NULL)\n * @param idx_out   Selected index (or NULL for table scan)\n * @param cost_out  Estimated query cost\n * @return SQLITE_OK\n */\nint index_select_for_query(const TableSchema* table, const ExprNode* where,\n                           const ColumnList* order_by, \n                           IndexSchema** idx_out, double* cost_out);\n/**\n * Estimate index scan cost.\n * \n * @param stats     Index statistics\n * @param selectivity  Estimated row fraction matching\n * @return Estimated I/O cost\n */\ndouble index_estimate_cost(const IndexStats* stats, double selectivity);\n/**\n * Check if index covers all requested columns.\n * \n * @param idx       Index schema\n * @param columns   Required column indices\n * @param col_count Number of required columns\n * @return true if covering\n */\nbool index_is_covering(const IndexSchema* idx, const int16_t* columns,\n                       uint8_t col_count);\n```\n---\n## Algorithm Specifications\n### Algorithm 1: Index Key Serialization\n```\nFUNCTION serialize_index_key(schema, record, rowid, out_entry)\n    INIT key_builder with column_count + 1\n    FOR i = 0 TO schema.column_count - 1:\n        col_idx = schema.columns[i]\n        value = record.columns[col_idx]\n        CASE value.type OF\n            NULL:    key_builder.append_null()\n            INTEGER: key_builder.append_int(value.int_val)\n            FLOAT:   key_builder.append_float(value.float_val)\n            TEXT:    key_builder.append_text(value.text_val, value.len)\n            BLOB:    key_builder.append_blob(value.blob_val, value.len)\n        END CASE\n    END FOR\n    // Rowid appended last for uniqueness\n    key_builder.append_rowid(rowid)\n    key_builder.finalize(out_entry)\n    RETURN SQLITE_OK\nEND FUNCTION\n```\n### Algorithm 2: Index Entry Insertion\n```\nFUNCTION index_insert_entry(pool, root, entry, unique)\n    cursor = OPEN btree_cursor ON pool, root\n    // Seek to insertion position\n    result = btree_seek(cursor, entry.key_data, entry.key_size)\n    IF unique AND result == SQLITE_FOUND THEN\n        // Duplicate key in unique index\n        CLOSE cursor\n        RETURN SQLITE_CONSTRAINT\n    END IF\n    // Insert at positioned location\n    result = btree_insert(cursor, entry.key_data, entry.key_size, NULL, 0)\n    CLOSE cursor\n    RETURN result\nEND FUNCTION\n```\n### Algorithm 3: Double Lookup Pattern\n```\nFUNCTION double_lookup(db, table, index, where_clause, result_set)\n    table_cur = OPEN table_cursor ON db, table.root\n    index_cur = OPEN index_cursor ON db, index.root\n    // Build search key from equality predicates\n    search_key = BUILD key FROM where_clause.equality_columns\n    result = index_seek_ge(index_cur, search_key)\n    IF result != SQLITE_OK THEN\n        RETURN empty_result\n    END IF\n    results = EMPTY list\n    WHILE NOT index_cur.eof:\n        // Get rowid from index\n        rowid = index_get_rowid(index_cur)\n        // Seek table to full row\n        table_seek_rowid(table_cur, rowid)\n        // Evaluate remaining WHERE predicates\n        IF evaluate_where(table_cur, where_clause) THEN\n            row = extract_columns(table_cur, result_set)\n            APPEND row TO results\n        END IF\n        index_next(index_cur)\n    END WHILE\n    CLOSE index_cur\n    CLOSE table_cur\n    RETURN results\nEND FUNCTION\n```\n### Algorithm 4: Covering Index Scan\n```\nFUNCTION covering_index_scan(db, index, where_clause, result_set)\n    // Verify all columns exist in index\n    IF NOT index_is_covering(index, result_set.columns) THEN\n        RETURN ERROR // Fall back to double lookup\n    END IF\n    index_cur = OPEN index_cursor ON db, index.root\n    // Build range bounds from predicates\n    lower = BUILD lower_bound FROM where_clause\n    upper = BUILD upper_bound FROM where_clause\n    index_set_bounds(index_cur, lower, upper)\n    results = EMPTY list\n    WHILE NOT index_cur.eof:\n        entry = index_get_entry(index_cur)\n        // Extract columns directly from index key\n        row = extract_from_index_key(entry, result_set.columns)\n        APPEND row TO results\n        index_next(index_cur)\n    END WHILE\n    CLOSE index_cur\n    RETURN results\nEND FUNCTION\n```\n### Algorithm 5: Index Maintenance on UPDATE\n```\nFUNCTION maintain_update(db, table, rowid, old_record, new_record, changed_cols)\n    FOR EACH index IN table.indexes:\n        // Check if index affected by update\n        affected = FALSE\n        FOR i = 0 TO index.column_count - 1:\n            IF changed_cols[index.columns[i]] THEN\n                affected = TRUE\n                BREAK\n            END IF\n        END FOR\n        IF NOT affected THEN\n            CONTINUE\n        END IF\n        // Delete old entry\n        old_key = serialize_index_key(index, old_record, rowid)\n        index_delete_entry(db.pool, index.root, old_key)\n        // Insert new entry\n        new_key = serialize_index_key(index, new_record, rowid)\n        result = index_insert_entry(db.pool, index.root, new_key, index.is_unique)\n        IF result == SQLITE_CONSTRAINT THEN\n            // Rollback: re-insert old entry\n            index_insert_entry(db.pool, index.root, old_key, FALSE)\n            RETURN SQLITE_CONSTRAINT\n        END IF\n    END FOR\n    RETURN SQLITE_OK\nEND FUNCTION\n```\n### Algorithm 6: UNIQUE Constraint Check\n```\nFUNCTION check_unique_constraint(pool, root, key_data, key_size, exclude_rowid)\n    cursor = OPEN btree_cursor ON pool, root\n    // Seek to key position\n    result = btree_seek_ge(cursor, key_data, key_size)\n    IF result == SQLITE_NOTFOUND THEN\n        CLOSE cursor\n        RETURN SQLITE_OK // No conflict\n    END IF\n    // Check if found key matches prefix (ignoring rowid)\n    found_key = btree_get_key(cursor)\n    // Compare key prefix without rowid suffix\n    prefix_size = key_size - varint_size(exclude_rowid)\n    found_prefix_size = found_key.size - varint_size(found_rowid)\n    IF prefix_size != found_prefix_size THEN\n        CLOSE cursor\n        RETURN SQLITE_OK // Different key lengths, no conflict\n    END IF\n    IF memcmp(key_data, found_key.data, prefix_size) == 0 THEN\n        // Same prefix: check if same rowid\n        found_rowid = extract_rowid_from_key(found_key)\n        IF found_rowid != exclude_rowid THEN\n            CLOSE cursor\n            RETURN SQLITE_CONSTRAINT // Duplicate!\n        END IF\n    END IF\n    CLOSE cursor\n    RETURN SQLITE_OK\nEND FUNCTION\n```\n---\n## Diagram: tdd-diag-m7-1 - Index B+tree Structure\n```\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                    INDEX B+TREE OVERVIEW                      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502   Interior Page 1     \u2502  (Root)\n                        \u2502   Pgno: 101           \u2502\n                        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                        \u2502 Cell: [50 \u2192 Pg 102]   \u2502\n                        \u2502 Cell: [100 \u2192 Pg 103]  \u2502\n                        \u2502 Cell: [150 \u2192 Pg 104]  \u2502\n                        \u2502 Right: \u2192 Pg 105       \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502                         \u2502                         \u2502\n          \u25bc                         \u25bc                         \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Leaf Page 1  \u2502\u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 Leaf Page 2  \u2502\u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 Leaf Page 3  \u2502\n   \u2502 Pgno: 102    \u2502  sibling\u2502 Pgno: 103   \u2502 sibling\u2502 Pgno: 104    \u2502\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502(10,rid=1)    \u2502        \u2502(55,rid=42)   \u2502        \u2502(101,rid=7)   \u2502\n   \u2502(25,rid=5)    \u2502        \u2502(72,rid=18)   \u2502        \u2502(125,rid=3)   \u2502\n   \u2502(40,rid=9)    \u2502        \u2502(90,rid=31)   \u2502        \u2502(150,rid=12)  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   INDEX ENTRY FORMAT (in cell):\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Payload Length (varint) \u2502 Key Data \u2502 Rowid (varint)\u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   Example: Index on (age) for table employees\n   Key: age=25 \u2192 Rowid=5 \u2192 Seek to row 5 in table B-tree\n```\n---\n## Diagram: tdd-diag-m7-2 - Double Lookup Pattern\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         DOUBLE LOOKUP EXECUTION FLOW                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nQUERY: SELECT name, salary FROM employees WHERE age = 35\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502      INDEX B+TREE (age)         \u2502\n                    \u2502         Root: 101               \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  SEEK age=35 in index           \u2502\n                    \u2502  Result: (35, rowid=42)         \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u2502 rowid = 42\n                                    \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     TABLE B-TREE                \u2502\n                    \u2502     Root: 1                     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  SEEK rowid=42 in table         \u2502\n                    \u2502  Result: full record            \u2502\n                    \u2502  {id:42,name:\"Alice\",           \u2502\n                    \u2502   age:35,salary:85000}          \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  EXTRACT name, salary           \u2502\n                    \u2502  Output: (\"Alice\", 85000)       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nPERFORMANCE COMPARISON:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Approach       \u2502   Pages Read      \u2502      Complexity     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Full Table Scan    \u2502 All leaf pages    \u2502 O(N) rows           \u2502\n\u2502 Double Lookup      \u2502 Index depth + 1   \u2502 O(log N) + O(1)     \u2502\n\u2502 Covering Index     \u2502 Index depth only  \u2502 O(log N)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n---\n## Diagram: tdd-diag-m7-3 - Covering Index\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         COVERING INDEX OPTIMIZATION                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nQUERY: SELECT age FROM employees WHERE age BETWEEN 30 AND 40\nSTANDARD INDEX (age):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Index stores: (age, rowid)                                                \u2502\n\u2502  Query needs: age                                                          \u2502\n\u2502  Result: COVERING \u2713                                                        \u2502\n\u2502                                                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n\u2502  \u2502     INDEX LEAF PAGE                 \u2502                                   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502                                   \u2502\n\u2502  \u2502  \u2502 Key: (30, rowid=5)          \u2502    \u2502 \u2190 Extract age=30 directly          \u2502\n\u2502  \u2502  \u2502 Key: (32, rowid=12)         \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2502 Key: (35, rowid=42)         \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2502 Key: (38, rowid=7)          \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2502 Key: (40, rowid=19)         \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502                                   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n\u2502                                                                            \u2502\n\u2502  NO TABLE LOOKUP REQUIRED!                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nCOMPOSITE INDEX (department, age):\nQUERY: SELECT department, age FROM employees WHERE department = 'Engineering'\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Index stores: (department, age, rowid)                                    \u2502\n\u2502  Query needs: department, age                                              \u2502\n\u2502  Result: COVERING \u2713                                                        \u2502\n\u2502                                                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n\u2502  \u2502     COMPOSITE INDEX LEAF            \u2502                                   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502                                   \u2502\n\u2502  \u2502  \u2502 (\"Eng\", 25, 1)              \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2502 (\"Eng\", 30, 5)              \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2502 (\"Eng\", 35, 42)             \u2502    \u2502 \u2190 All columns in index!            \u2502\n\u2502  \u2502  \u2502 (\"Eng\", 40, 7)              \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2502 (\"Sales\", 28, 3)            \u2502    \u2502                                   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502                                   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNON-COVERING CASE:\nQUERY: SELECT name FROM employees WHERE age = 35\nINDEX: (age) only\nNEEDS: name (not in index) \u2192 MUST do table lookup\n```\n---\n## Diagram: tdd-diag-m7-4 - Index Maintenance Flow\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    INDEX MAINTENANCE ON DML OPERATIONS                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nINSERT OPERATION\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 INSERT INTO emp   \u2502\n  \u2502 VALUES (...)      \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     SUCCESS\n  \u2502 Table B-tree      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Insert record     \u2502             \u2502\n  \u2502 Get new rowid     \u2502             \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                                    \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502        FOR EACH INDEX ON TABLE            \u2502\n            \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n            \u2502  \u2502 1. Build key from record + rowid    \u2502  \u2502\n            \u2502  \u2502 2. Insert into index B-tree         \u2502  \u2502\n            \u2502  \u2502 3. IF UNIQUE violation:             \u2502  \u2502\n            \u2502  \u2502    - Rollback table insert          \u2502  \u2502\n            \u2502  \u2502    - Return SQLITE_CONSTRAINT       \u2502  \u2502\n            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nUPDATE OPERATION  \n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 UPDATE emp SET    \u2502\n  \u2502 age=40 WHERE id=5 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502               FOR EACH AFFECTED ROW                        \u2502\n  \u2502                                                            \u2502\n  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n  \u2502  \u2502 1. Read OLD record (before update)                  \u2502  \u2502\n  \u2502  \u2502 2. Apply changes \u2192 NEW record                       \u2502  \u2502\n  \u2502  \u2502 3. FOR EACH index:                                  \u2502  \u2502\n  \u2502  \u2502    IF index columns changed:                        \u2502  \u2502\n  \u2502  \u2502      a. Delete OLD key from index                   \u2502  \u2502\n  \u2502  \u2502      b. Insert NEW key into index                   \u2502  \u2502\n  \u2502  \u2502      c. IF UNIQUE violation:                        \u2502  \u2502\n  \u2502  \u2502         - Re-insert OLD key                         \u2502  \u2502\n  \u2502  \u2502         - Return SQLITE_CONSTRAINT                  \u2502  \u2502\n  \u2502  \u2502 4. Update table B-tree record                       \u2502  \u2502\n  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nDELETE OPERATION\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 DELETE FROM emp   \u2502\n  \u2502 WHERE id=5        \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502               FOR EACH AFFECTED ROW                        \u2502\n  \u2502                                                            \u2502\n  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n  \u2502  \u2502 1. Read record (before delete)                      \u2502  \u2502\n  \u2502  \u2502 2. FOR EACH index:                                  \u2502  \u2502\n  \u2502  \u2502    - Delete (key, rowid) from index                 \u2502  \u2502\n  \u2502  \u2502 3. Delete record from table B-tree                  \u2502  \u2502\n  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n---\n## Diagram: tdd-diag-m7-5 - UNIQUE Constraint Enforcement\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    UNIQUE CONSTRAINT ENFORCEMENT FLOW                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nCREATE UNIQUE INDEX idx_email ON users(email);\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502         INDEX STRUCTURE             \u2502\n                    \u2502    Key: (email, rowid)              \u2502\n                    \u2502    UNIQUE: TRUE                     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nINSERT ATTEMPT: email = \"alice@example.com\"\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 STEP 1: Build index key             \u2502\n                    \u2502 key = (\"alice@example.com\", rowid)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 STEP 2: Seek in index B-tree        \u2502\n                    \u2502 Search for key prefix               \u2502\n                    \u2502 (email only, ignore rowid)          \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                                     \u2502\n                    \u25bc                                     \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 NOT FOUND           \u2502             \u2502 FOUND               \u2502\n        \u2502 (no duplicate)      \u2502             \u2502 (\"alice@...\", rid=5)\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502                                   \u2502\n                   \u25bc                                   \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Insert new entry    \u2502             \u2502 Compare rowids:     \u2502\n        \u2502 Return SQLITE_OK    \u2502             \u2502 found.rid != new.rid\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                          \u2502 DUPLICATE DETECTED! \u2502\n                                          \u2502 Return              \u2502\n                                          \u2502 SQLITE_CONSTRAINT   \u2502\n                                          \u2502 Error: UNIQUE       \u2502\n                                          \u2502 constraint failed   \u2502\n                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nUPDATE ATTEMPT: Changing email from \"bob@...\" to \"alice@...\"\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 STEP 1: Delete old entry            \u2502\n                    \u2502 Delete (\"bob@...\", rowid=10)        \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 STEP 2: Check UNIQUE for new key    \u2502\n                    \u2502 Seek \"alice@...\"                    \u2502\n                    \u2502 If found with diff rowid \u2192 FAIL     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                                     \u2502\n                    \u25bc                                     \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 No conflict         \u2502             \u2502 Conflict exists     \u2502\n        \u2502 Insert new entry    \u2502             \u2502 Re-insert old entry \u2502\n        \u2502 Return SQLITE_OK    \u2502             \u2502 Return CONSTRAINT   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n---\n## Diagram: tdd-diag-m7-6 - Index Selection Decision Tree\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      QUERY OPTIMIZER INDEX SELECTION                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nQUERY: SELECT col1, col2 FROM table WHERE condition ORDER BY cols\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   Parse WHERE &     \u2502\n                              \u2502   ORDER BY clauses  \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                                         \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 FOR EACH available index on table:     \u2502\n                    \u2502                                        \u2502\n                    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                    \u2502 \u2502 1. Extract indexed columns         \u2502 \u2502\n                    \u2502 \u2502 2. Match against WHERE predicates  \u2502 \u2502\n                    \u2502 \u2502 3. Match against ORDER BY columns  \u2502 \u2502\n                    \u2502 \u2502 4. Calculate selectivity estimate  \u2502 \u2502\n                    \u2502 \u2502 5. Compute estimated cost          \u2502 \u2502\n                    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                                         \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502       INDEX CANDIDATE SCORING          \u2502\n                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                    \u2502 Factors:                               \u2502\n                    \u2502 \u2022 Equality matches: HIGH priority      \u2502\n                    \u2502 \u2022 Range matches: MEDIUM priority       \u2502\n                    \u2502 \u2022 ORDER BY match: BONUS (avoids sort)  \u2502\n                    \u2502 \u2022 Covering: BONUS (skips table seek)   \u2502\n                    \u2502 \u2022 Index size: Smaller = faster         \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                                         \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Compare costs:      \u2502\n                              \u2502 \u2022 Best index cost   \u2502\n                              \u2502 \u2022 Full table scan   \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                                         \u2502\n                    \u25bc                                         \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Index cost <        \u2502                 \u2502 Table scan cheaper  \u2502\n        \u2502 Table scan cost     \u2502                 \u2502 (few rows, no       \u2502\n        \u2502                     \u2502                 \u2502  suitable index)    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502                                       \u2502\n                   \u25bc                                       \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 USE INDEX           \u2502                 \u2502 USE TABLE SCAN      \u2502\n        \u2502                     \u2502                 \u2502                     \u2502\n        \u2502 Check covering:     \u2502                 \u2502 Plan:               \u2502\n        \u2502 \u2022 YES: Index-only   \u2502                 \u2502 \u2022 Open table cursor \u2502\n        \u2502   scan              \u2502                 \u2502 \u2022 Full traversal    \u2502\n        \u2502 \u2022 NO: Double lookup \u2502                 \u2502 \u2022 Filter WHERE      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502 \u2022 Sort if needed    \u2502\n                                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nCOST ESTIMATION FORMULA:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 index_cost = index_depth + (selectivity \u00d7 table_rows \u00d7 table_seek_cost)   \u2502\n\u2502 covering_cost = index_depth + (selectivity \u00d7 index_rows)                  \u2502\n\u2502 table_scan_cost = table_leaf_pages                                        \u2502\n\u2502                                                                            \u2502\n\u2502 selectivity = 1 / (distinct_values \u00d7 range_factor)                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n---\n## Error Handling Matrix\n| Error Code | Condition | Recovery Action | User Message |\n|------------|-----------|-----------------|--------------|\n| `SQLITE_CONSTRAINT` | UNIQUE violation on index insert | Rollback transaction | \"UNIQUE constraint failed: index {name}\" |\n| `SQLITE_ERROR` | Index not found on drop | Return error (unless IF EXISTS) | \"no such index: {name}\" |\n| `SQLITE_ERROR` | Table has no indexes | No-op, return OK | N/A |\n| `SQLITE_CORRUPT` | Index page checksum invalid | Mark database read-only | \"database disk image is malformed\" |\n| `SQLITE_LOCKED` | Index page locked by writer | Retry with timeout | N/A (internal) |\n| `SQLITE_NOMEM` | Key buffer allocation failed | Abort operation | \"out of memory\" |\n| `SQLITE_FULL` | Index page split fails (disk full) | Rollback transaction | \"database or disk is full\" |\n| `SQLITE_IOERR` | Page read/write failure | Abort, mark connection bad | \"disk I/O error\" |\n| `SQLITE_SCHEMA` | Schema changed during operation | Reprepare statement | \"database schema has changed\" |\n### Error Propagation\n```c\ntypedef enum {\n    INDEX_ERR_NONE = 0,\n    INDEX_ERR_KEY_TOO_LARGE,      // Key exceeds page capacity\n    INDEX_ERR_INVALID_COLLATION,  // Unknown collation sequence\n    INDEX_ERR_CYCLE,              // Circular index reference\n    INDEX_ERR_TYPE_MISMATCH       // Key type incompatible with index\n} IndexInternalError;\n```\n---\n## Implementation Sequence\n### Phase 1: Core Index Data Structures (6 hours)\n**Files:** `index/index.h`, `index/index.c`\n**Tasks:**\n- [ ] Define IndexSchema structure with all fields\n- [ ] Implement index_schema_init/free functions\n- [ ] Implement index_schema_parse for CREATE INDEX\n- [ ] Add IndexEntry and IndexKeyBuilder types\n- [ ] Implement key serialization functions\n**Checkpoint 1:** Unit tests pass for schema parsing and key serialization\n### Phase 2: Index B+tree Operations (10 hours)\n**Files:** `index/index_btree.h`, `index/index_btree.c`\n**Tasks:**\n- [ ] Define index cell format (differs from table cells)\n- [ ] Implement index page cell insertion\n- [ ] Implement index page cell deletion\n- [ ] Implement index key comparison with collation\n- [ ] Implement page split for index pages\n- [ ] Implement range scan via right_sibling\n**Checkpoint 2:** Index B+tree passes all CRUD tests independently\n### Phase 3: Index Cursor and Lookup (8 hours)\n**Files:** `index/index_btree.c` (continued)\n**Tasks:**\n- [ ] Implement IndexCursor structure and lifecycle\n- [ ] Implement index_seek, index_seek_ge, index_seek_exact\n- [ ] Implement index_next, index_prev with page traversal\n- [ ] Implement index_get_rowid and index_get_entry\n- [ ] Implement range bounds for BETWEEN/AND queries\n**Checkpoint 3:** Cursor can traverse entire index and locate entries\n### Phase 4: CREATE/DROP INDEX Execution (8 hours)\n**Files:** `parser/create_index.c`, `index/index.c`\n**Tasks:**\n- [ ] Parse CREATE INDEX statement to AST\n- [ ] Parse DROP INDEX statement\n- [ ] Implement index_create with schema registration\n- [ ] Implement index_drop with page deallocation\n- [ ] Add IF EXISTS / IF NOT EXISTS handling\n- [ ] Populate index with existing table data\n**Checkpoint 4:** CREATE INDEX builds populated index; DROP cleans up\n### Phase 5: VDBE Index Opcodes (8 hours)\n**Files:** `vdbe/opcodes_index.h`, `vdbe/opcodes_index.c`\n**Tasks:**\n- [ ] Implement OpenIndex opcode\n- [ ] Implement SeekIndex opcode\n- [ ] Implement IndexNext/IndexPrev opcodes\n- [ ] Implement IndexRowid opcode\n- [ ] Implement IndexColumn for covering scans\n- [ ] Implement MakeIndexKey opcode\n- [ ] Implement IndexInsert/IndexDelete opcodes\n**Checkpoint 5:** VDBE can execute index operations via bytecode\n### Phase 6: Index Maintenance Hooks (10 hours)\n**Files:** `index/index_maintenance.h`, `index/index_maintenance.c`\n**Tasks:**\n- [ ] Implement index_maintain_insert\n- [ ] Implement index_maintain_delete\n- [ ] Implement index_maintain_update\n- [ ] Hook into INSERT execution flow\n- [ ] Hook into UPDATE execution flow\n- [ ] Hook into DELETE execution flow\n- [ ] Implement UNIQUE constraint checking\n**Checkpoint 6:** All DML operations maintain index consistency\n### Phase 7: Index Selection Optimizer (8 hours)\n**Files:** `optimizer/index_selector.h`, `optimizer/index_selector.c`\n**Tasks:**\n- [ ] Implement index_select_for_query\n- [ ] Implement index_estimate_cost\n- [ ] Implement index_is_covering check\n- [ ] Add equality predicate detection\n- [ ] Add range predicate detection\n- [ ] Add ORDER BY matching for index selection\n- [ ] Integrate with query planner\n**Checkpoint 7:** Query optimizer selects optimal index automatically\n---\n## Test Specification\n### Unit Tests\n```c\n// test_index_key.c\nvoid test_key_serialize_null(void);\nvoid test_key_serialize_integer(void);\nvoid test_key_serialize_text(void);\nvoid test_key_serialize_composite(void);\nvoid test_key_comparison_ordering(void);\nvoid test_key_rowid_suffix(void);\n// test_index_btree.c\nvoid test_index_insert_single(void);\nvoid test_index_insert_multiple(void);\nvoid test_index_insert_duplicate_unique(void);\nvoid test_index_delete_entry(void);\nvoid test_index_seek_exact(void);\nvoid test_index_seek_range(void);\nvoid test_index_traversal_forward(void);\nvoid test_index_traversal_backward(void);\nvoid test_index_page_split(void);\n// test_index_cursor.c\nvoid test_cursor_open_close(void);\nvoid test_cursor_positioning(void);\nvoid test_cursor_eof_detection(void);\nvoid test_cursor_range_bounds(void);\nvoid test_cursor_rowid_extraction(void);\n// test_index_maintenance.c\nvoid test_maintain_insert_simple(void);\nvoid test_maintain_insert_unique_violation(void);\nvoid test_maintain_delete_simple(void);\nvoid test_maintain_update_affected_column(void);\nvoid test_maintain_update_unaffected_column(void);\nvoid test_maintain_update_unique_violation(void);\nvoid test_maintain_multiple_indexes(void);\n// test_index_selector.c\nvoid test_select_equality_match(void);\nvoid test_select_range_match(void);\nvoid test_select_orderby_match(void);\nvoid test_select_covering_detection(void);\nvoid test_select_table_scan_fallback(void);\nvoid test_select_cost_estimation(void);\n```\n### Integration Tests\n```c\n// test_index_integration.c\nvoid test_create_index_populates(void);\nvoid test_drop_index_cleans_up(void);\nvoid test_unique_constraint_enforced(void);\nvoid test_select_uses_index(void);\nvoid test_covering_index_skips_table(void);\nvoid test_index_survives_checkpoint(void);\nvoid test_index_consistent_after_rollback(void);\nvoid test_concurrent_index_access(void);\n```\n### Performance Tests\n```c\n// test_index_performance.c\nvoid benchmark_index_insert_batch(int count);\nvoid benchmark_index_lookup(int count);\nvoid benchmark_index_range_scan(int count);\nvoid benchmark_covering_vs_double_lookup(int count);\nvoid benchmark_index_maintenance_overhead(int count);\n```\n---\n## Performance Targets\n| Operation | Target | Measurement |\n|-----------|--------|-------------|\n| Index seek (exact) | < 10\u03bcs | 100K+ rows index |\n| Index range scan (100 rows) | < 100\u03bcs | Sequential leaf reads |\n| Index insert | < 15\u03bcs | Including key serialization |\n| Index delete | < 15\u03bcs | Including key comparison |\n| UNIQUE constraint check | < 5\u03bcs | Additional to insert |\n| CREATE INDEX (1M rows) | < 2s | Bulk load optimization |\n| Covering scan vs double lookup | 30-50% faster | Measured I/O reduction |\n### Memory Targets\n| Resource | Limit |\n|----------|-------|\n| Index cursor size | 256 bytes |\n| Key builder buffer | 4KB default, growable |\n| Index schema overhead | 64 bytes per index |\n| Maintenance context | 128 bytes per DML |\n---\n## Synced Criteria\n```json\n{\n  \"module_id\": \"build-sqlite-m7\",\n  \"version\": \"1.0.0\",\n  \"milestone\": \"M7\",\n  \"title\": \"Secondary Indexes\",\n  \"criteria\": [\n    {\n      \"id\": \"m7-structure\",\n      \"description\": \"Index schema stored in sqlite_master with type='index'\",\n      \"test\": \"Verify IndexSchema serialization/deserialization\",\n      \"weight\": 1\n    },\n    {\n      \"id\": \"m7-btree\",\n      \"description\": \"Secondary indexes use B+tree with (key, rowid) cell format\",\n      \"test\": \"Insert and retrieve entries, verify page linkage\",\n      \"weight\": 2\n    },\n    {\n      \"id\": \"m7-cursor\",\n      \"description\": \"IndexCursor supports seek, next, prev, and range scans\",\n      \"test\": \"Traverse index in order with bounds\",\n      \"weight\": 2\n    },\n    {\n      \"id\": \"m7-double-lookup\",\n      \"description\": \"Double lookup pattern: index seek yields rowid for table seek\",\n      \"test\": \"Execute SELECT with index, verify rowid extraction\",\n      \"weight\": 2\n    },\n    {\n      \"id\": \"m7-covering\",\n      \"description\": \"Covering indexes skip table lookup when all columns present\",\n      \"test\": \"Compare I/O count for covering vs non-covering query\",\n      \"weight\": 1\n    },\n    {\n      \"id\": \"m7-unique\",\n      \"description\": \"UNIQUE indexes enforce constraint on insert/update\",\n      \"test\": \"Attempt duplicate insert, expect SQLITE_CONSTRAINT\",\n      \"weight\": 2\n    },\n    {\n      \"id\": \"m7-maintenance\",\n      \"description\": \"DML operations automatically maintain index entries\",\n      \"test\": \"INSERT/UPDATE/DELETE on indexed table, verify index consistency\",\n      \"weight\": 3\n    },\n    {\n      \"id\": \"m7-create-drop\",\n      \"description\": \"CREATE INDEX populates from existing data; DROP deallocates\",\n      \"test\": \"Create index on populated table, verify all entries present\",\n      \"weight\": 2\n    },\n    {\n      \"id\": \"m7-opcodes\",\n      \"description\": \"VDBE opcodes for index operations functional\",\n      \"test\": \"Execute bytecode program using index opcodes\",\n      \"weight\": 2\n    },\n    {\n      \"id\": \"m7-optimizer\",\n      \"description\": \"Query optimizer selects appropriate index based on predicates\",\n      \"test\": \"EXPLAIN shows index usage for indexed column in WHERE\",\n      \"weight\": 2\n    },\n    {\n      \"id\": \"m7-range-scan\",\n      \"description\": \"Range scans use right_sibling linkage through leaf pages\",\n      \"test\": \"Scan 1000+ entry index spanning multiple pages\",\n      \"weight\": 1\n    },\n    {\n      \"id\": \"m7-performance\",\n      \"description\": \"Index operations meet performance targets\",\n      \"test\": \"Benchmark suite passes timing thresholds\",\n      \"weight\": 1\n    }\n  ],\n  \"dependencies\": [\"build-sqlite-m4\", \"build-sqlite-m5\", \"build-sqlite-m6\"],\n  \"diagrams\": [\n    \"tdd-diag-m7-1\",\n    \"tdd-diag-m7-2\", \n    \"tdd-diag-m7-3\",\n    \"tdd-diag-m7-4\",\n    \"tdd-diag-m7-5\",\n    \"tdd-diag-m7-6\"\n  ],\n  \"estimated_hours\": 58,\n  \"files_created\": 11,\n  \"min_characters\": 8000\n}\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m8 -->\n# Technical Design Specification: Query Planner & Statistics\n**Module ID:** build-sqlite-m8  \n**Version:** 1.0  \n**Status:** Code-Ready  \n---\n## 1. Module Charter\n### Purpose\nImplement a cost-based query optimizer that transforms parsed SQL ASTs into optimal execution plans using statistical analysis of table data distributions. This module bridges the gap between the parser output and the execution engine, ensuring queries execute with minimal I/O and CPU cost.\n### Scope\n- Statistics collection via `ANALYZE` command\n- Selectivity estimation for predicate filtering\n- Cost model for access path selection (table scan vs index scan)\n- Join ordering optimization using dynamic programming\n- Execution plan generation for the execution engine\n### Dependencies\n- **build-sqlite-m5**: B-Tree module for table/index traversal\n- **build-sqlite-m6**: Pager module for I/O cost estimation\n- **build-sqlite-m7**: SQL Parser for Query AST input\n---\n## 2. File Structure\n```\nsrc/\n\u251c\u2500\u2500 planner/\n\u2502   \u251c\u2500\u2500 mod.rs              # Module exports and QueryPlanner facade\n\u2502   \u251c\u2500\u2500 statistics.rs       # Statistics collection and storage\n\u2502   \u251c\u2500\u2500 selectivity.rs      # Predicate selectivity estimation\n\u2502   \u251c\u2500\u2500 cost_model.rs       # I/O and CPU cost calculations\n\u2502   \u251c\u2500\u2500 access_path.rs      # Table scan vs index scan selection\n\u2502   \u251c\u2500\u2500 join_planner.rs     # Join ordering with dynamic programming\n\u2502   \u251c\u2500\u2500 plan.rs             # ExecutionPlan data structures\n\u2502   \u2514\u2500\u2500 explain.rs          # EXPLAIN output formatting\n\u251c\u2500\u2500 storage/\n\u2502   \u2514\u2500\u2500 stat_table.rs       # sqlite_stat1 system table operations\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 planner_tests.rs    # Integration tests\n```\n---\n## 3. Complete Data Model\n### 3.1 Statistics Structures\n```rust\n/// Column-level statistics stored in sqlite_stat1\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ColumnStatistics {\n    /// Column name in the table\n    pub column_name: String,\n    /// Number of distinct values (approximate for large tables)\n    pub distinct_count: u64,\n    /// Minimum value (as string representation)\n    pub min_value: Option<String>,\n    /// Maximum value (as string representation)\n    pub max_value: Option<String>,\n    /// Number of NULL values\n    pub null_count: u64,\n    /// Histogram buckets for skewed distributions (optional)\n    pub histogram: Option<Vec<HistogramBucket>>,\n}\n/// Table-level aggregated statistics\n#[derive(Debug, Clone)]\npub struct TableStatistics {\n    /// Table name\n    pub table_name: String,\n    /// Total row count\n    pub row_count: u64,\n    /// Number of leaf pages in table B-Tree\n    pub page_count: u64,\n    /// Average rows per page\n    pub rows_per_page: f64,\n    /// Per-column statistics\n    pub columns: HashMap<String, ColumnStatistics>,\n    /// Last ANALYZE timestamp\n    pub analyzed_at: u64,\n    /// Statistics version for cache invalidation\n    pub version: u32,\n}\n/// Histogram bucket for value distribution\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct HistogramBucket {\n    pub lower_bound: String,\n    pub upper_bound: String,\n    pub frequency: u64,\n    pub distinct_values: u64,\n}\n/// Index statistics for cost estimation\n#[derive(Debug, Clone)]\npub struct IndexStatistics {\n    /// Index name\n    pub index_name: String,\n    /// Table the index belongs to\n    pub table_name: String,\n    /// Number of entries in index\n    pub entry_count: u64,\n    /// B-Tree depth (levels)\n    pub tree_depth: u32,\n    /// Average key size in bytes\n    pub avg_key_size: u32,\n    /// Clustering factor (how ordered index is vs table)\n    pub clustering_factor: u64,\n    /// Indexed columns selectivity\n    pub column_selectivities: Vec<f64>,\n}\n```\n### 3.2 Selectivity Model\n```rust\n/// Estimated selectivity for a predicate (0.0 to 1.0)\n#[derive(Debug, Clone)]\npub struct SelectivityEstimate {\n    /// Fraction of rows expected to match\n    pub selectivity: f64,\n    /// Confidence level (0.0 to 1.0)\n    pub confidence: f64,\n    /// Source of estimate\n    pub source: SelectivitySource,\n}\n#[derive(Debug, Clone, PartialEq)]\npub enum SelectivitySource {\n    /// Exact count from statistics\n    Exact,\n    /// Interpolated from histogram\n    HistogramInterpolation,\n    /// Heuristic estimate (no stats available)\n    Heuristic,\n    /// Default fallback (e.g., 0.1 for equality)\n    Default,\n}\n/// Predicate selectivity context\npub struct SelectivityContext<'a> {\n    pub table_stats: &'a TableStatistics,\n    pub index_stats: Vec<&'a IndexStatistics>,\n    pub correlated_columns: HashSet<String>,\n}\n```\n### 3.3 Cost Model Structures\n```rust\n/// Complete cost estimate for an operation\n#[derive(Debug, Clone, Default)]\npub struct CostEstimate {\n    /// I/O cost (page reads)\n    pub io_cost: f64,\n    /// CPU cost (tuple processing)\n    pub cpu_cost: f64,\n    /// Total cost (weighted combination)\n    pub total_cost: f64,\n    /// Estimated result cardinality\n    pub cardinality: u64,\n}\n/// Cost model configuration\npub struct CostModelConfig {\n    /// Weight for sequential I/O operations\n    pub seq_page_cost: f64,        // default: 1.0\n    /// Weight for random I/O operations\n    pub random_page_cost: f64,     // default: 4.0\n    /// Weight for CPU tuple processing\n    pub cpu_tuple_cost: f64,       // default: 0.01\n    /// Weight for index traversal\n    pub cpu_index_tuple_cost: f64, // default: 0.005\n    /// Weight for operator evaluation\n    pub cpu_operator_cost: f64,    // default: 0.0025\n    /// Effective cache size (pages assumed in buffer)\n    pub effective_cache_size: u64,\n}\n```\n### 3.4 Execution Plan Structures\n```rust\n/// Complete execution plan for a query\n#[derive(Debug, Clone)]\npub struct ExecutionPlan {\n    /// Root plan node\n    pub root: PlanNode,\n    /// Total estimated cost\n    pub estimated_cost: CostEstimate,\n    /// Tables accessed\n    pub tables: Vec<String>,\n    /// Indexes used\n    pub indexes_used: Vec<String>,\n    /// Plan generation timestamp\n    pub created_at: u64,\n}\n/// Single node in the execution plan tree\n#[derive(Debug, Clone)]\npub enum PlanNode {\n    /// Full table scan\n    TableScan {\n        table_name: String,\n        alias: Option<String>,\n        estimated_rows: u64,\n        cost: CostEstimate,\n    },\n    /// Index scan with optional lookup\n    IndexScan {\n        index_name: String,\n        table_name: String,\n        alias: Option<String>,\n        start_key: Option<ScanKey>,\n        end_key: Option<ScanKey>,\n        is_covering: bool,\n        estimated_rows: u64,\n        cost: CostEstimate,\n    },\n    /// Filter operation (WHERE clause)\n    Filter {\n        input: Box<PlanNode>,\n        predicate: Predicate,\n        selectivity: f64,\n    },\n    /// Projection (SELECT columns)\n    Project {\n        input: Box<PlanNode>,\n        columns: Vec<ProjectColumn>,\n    },\n    /// Join operation\n    Join {\n        join_type: JoinType,\n        left: Box<PlanNode>,\n        right: Box<PlanNode>,\n        condition: Option<JoinCondition>,\n        estimated_rows: u64,\n        cost: CostEstimate,\n    },\n    /// Aggregation\n    Aggregate {\n        input: Box<PlanNode>,\n        group_by: Vec<String>,\n        aggregates: Vec<AggregateFunction>,\n        estimated_groups: u64,\n    },\n    /// Sort operation\n    Sort {\n        input: Box<PlanNode>,\n        order_by: Vec<SortKey>,\n        estimated_rows: u64,\n    },\n    /// Limit/Offset\n    Limit {\n        input: Box<PlanNode>,\n        limit: u64,\n        offset: u64,\n    },\n    /// Nested loop join implementation\n    NestedLoopJoin {\n        outer: Box<PlanNode>,\n        inner: Box<PlanNode>,\n        condition: JoinCondition,\n        inner_unique: bool,\n    },\n    /// Hash join implementation\n    HashJoin {\n        build: Box<PlanNode>,\n        probe: Box<PlanNode>,\n        build_key: String,\n        probe_key: String,\n        hash_buckets: u64,\n    },\n    /// Merge join implementation\n    MergeJoin {\n        left: Box<PlanNode>,\n        right: Box<PlanNode>,\n        left_key: String,\n        right_key: String,\n    },\n}\n#[derive(Debug, Clone)]\npub struct ScanKey {\n    pub columns: Vec<String>,\n    pub bounds: Vec<KeyBound>,\n}\n#[derive(Debug, Clone)]\npub enum KeyBound {\n    Inclusive(Value),\n    Exclusive(Value),\n    Unbounded,\n}\n#[derive(Debug, Clone, Copy, PartialEq)]\npub enum JoinType {\n    Inner,\n    LeftOuter,\n    RightOuter,\n    FullOuter,\n    Cross,\n}\n```\n---\n## 4. Interface Contracts\n### 4.1 QueryPlanner Trait\n```rust\npub trait QueryPlanner: Send + Sync {\n    /// Generate optimal execution plan for a query\n    fn plan(&self, query: &QueryAst) -> Result<ExecutionPlan, PlannerError>;\n    /// Update statistics for a table\n    fn analyze(&mut self, table_name: &str) -> Result<TableStatistics, PlannerError>;\n    /// Invalidate cached statistics\n    fn invalidate_stats(&mut self, table_name: &str);\n    /// Get current statistics for a table\n    fn get_statistics(&self, table_name: &str) -> Option<&TableStatistics>;\n    /// Explain a plan in human-readable format\n    fn explain(&self, plan: &ExecutionPlan, format: ExplainFormat) -> String;\n}\n/// Query AST from parser module\npub struct QueryAst {\n    pub select: SelectStatement,\n    pub from: FromClause,\n    pub where_clause: Option<WhereClause>,\n    pub group_by: Option<GroupByClause>,\n    pub having: Option<HavingClause>,\n    pub order_by: Option<OrderByClause>,\n    pub limit: Option<LimitClause>,\n}\n```\n### 4.2 Statistics Manager Trait\n```rust\npub trait StatisticsManager: Send + Sync {\n    /// Collect statistics for a table\n    fn collect(&self, table_name: &str) -> Result<TableStatistics, PlannerError>;\n    /// Load statistics from sqlite_stat1\n    fn load(&self, table_name: &str) -> Result<Option<TableStatistics>, PlannerError>;\n    /// Persist statistics to sqlite_stat1\n    fn persist(&self, stats: &TableStatistics) -> Result<(), PlannerError>;\n    /// Load all statistics into memory cache\n    fn load_all(&self) -> Result<HashMap<String, TableStatistics>, PlannerError>;\n}\n```\n### 4.3 Cost Estimator Trait\n```rust\npub trait CostEstimator: Send + Sync {\n    /// Estimate cost of table scan\n    fn estimate_table_scan(&self, stats: &TableStatistics) -> CostEstimate;\n    /// Estimate cost of index scan\n    fn estimate_index_scan(\n        &self, \n        index_stats: &IndexStatistics,\n        table_stats: &TableStatistics,\n        selectivity: f64,\n        is_covering: bool\n    ) -> CostEstimate;\n    /// Estimate cost of join operation\n    fn estimate_join(\n        &self,\n        left_cost: &CostEstimate,\n        right_cost: &CostEstimate,\n        join_type: JoinType,\n        selectivity: f64\n    ) -> CostEstimate;\n    /// Estimate cost of sort operation\n    fn estimate_sort(&self, rows: u64, width: u32) -> CostEstimate;\n}\n```\n---\n## 5. Algorithm Specification\n### 5.1 Statistics Collection Algorithm\n```\nALGORITHM: collect_statistics(table_name)\nINPUT: table_name - name of table to analyze\nOUTPUT: TableStatistics\n1. Initialize TableStatistics struct\n2. Open table B-Tree cursor\n3. Traverse all leaf pages:\n   a. row_count += page.row_count\n   b. page_count += 1\n   c. For each row:\n      - For each column:\n        - Track min/max values\n        - Count NULLs\n        - Add to value set for distinct counting\n      - Sample every Nth row for histogram\n4. Compute distinct_count per column (use HyperLogLog for large tables)\n5. Calculate rows_per_page = row_count / page_count\n6. Build histograms for columns with high distinct count\n7. Persist to sqlite_stat1 table\n8. Return TableStatistics\nCOMPLEXITY: O(N) where N = total rows in table\n```\n### 5.2 Selectivity Estimation Algorithm\n```\nALGORITHM: estimate_selectivity(predicate, stats)\nINPUT: predicate - WHERE clause predicate\n       stats - table statistics\nOUTPUT: SelectivityEstimate\n1. MATCH predicate type:\n   CASE equality (col = value):\n     IF column has histogram:\n       selectivity = histogram.lookup(value) / stats.row_count\n       source = HistogramInterpolation\n     ELSE IF stats.distinct_count > 0:\n       selectivity = 1.0 / stats.distinct_count\n       source = Exact\n     ELSE:\n       selectivity = 0.1  -- default heuristic\n       source = Default\n   CASE range (col < value OR col > value OR col BETWEEN):\n     IF histogram available:\n       selectivity = histogram.range_fraction(bounds)\n       source = HistogramInterpolation\n     ELSE:\n       selectivity = (value - min) / (max - min)  -- linear assumption\n       source = Heuristic\n   CASE LIKE (col LIKE pattern):\n     IF pattern starts with constant prefix:\n       selectivity = estimate_prefix_selectivity(prefix, stats)\n     ELSE:\n       selectivity = 0.05  -- conservative default\n   CASE IN (col IN (values...)):\n     selectivity = MIN(len(values) / distinct_count, 1.0)\n   CASE AND (pred1 AND pred2):\n     s1 = estimate_selectivity(pred1, stats)\n     s2 = estimate_selectivity(pred2, stats)\n     selectivity = s1 * s2  -- assume independence\n   CASE OR (pred1 OR pred2):\n     s1 = estimate_selectivity(pred1, stats)\n     s2 = estimate_selectivity(pred2, stats)\n     selectivity = s1 + s2 - (s1 * s2)\n   CASE NOT (NOT pred):\n     selectivity = 1.0 - estimate_selectivity(pred, stats)\n2. confidence = calculate_confidence(source, stats.completeness)\n3. RETURN SelectivityEstimate { selectivity, confidence, source }\n```\n### 5.3 Access Path Selection Algorithm\n```\nALGORITHM: select_access_path(table, predicates, stats, indexes)\nINPUT: table - table metadata\n       predicates - applicable WHERE predicates\n       stats - table statistics\n       indexes - available indexes on table\nOUTPUT: PlanNode with minimum cost\n1. Compute table scan cost:\n   table_scan_cost = cost_estimator.estimate_table_scan(stats)\n   table_scan_cardinality = stats.row_count * combined_selectivity\n2. Initialize best_plan = TableScan { cost: table_scan_cost }\n   best_cost = table_scan_cost.total_cost\n3. For each applicable index:\n   a. Check if index can satisfy predicates:\n      index_predicates = extract_index_predicates(predicates, index)\n      IF index_predicates.is_empty(): CONTINUE\n   b. Calculate selectivity for index predicates:\n      index_selectivity = estimate_selectivity(index_predicates, stats)\n   c. Check if index is covering (all SELECT columns in index):\n      is_covering = check_covering(index, query_columns)\n   d. Estimate index scan cost:\n      index_cost = cost_estimator.estimate_index_scan(\n          index_stats, stats, index_selectivity, is_covering)\n   e. IF is_covering:\n         total_cost = index_cost\n      ELSE:\n         -- Need table lookup for each match\n         lookup_cost = index_cardinality * random_page_cost\n         total_cost = index_cost + lookup_cost\n   f. IF total_cost < best_cost:\n         best_cost = total_cost\n         best_plan = IndexScan { index_name, selectivity, ... }\n4. RETURN best_plan\n```\n### 5.4 Join Ordering Algorithm (Dynamic Programming)\n```\nALGORITHM: optimize_join_order(tables, join_conditions, stats_map)\nINPUT: tables - list of table references\n       join_conditions - join predicates between tables\n       stats_map - statistics per table\nOUTPUT: Optimal join plan tree\n1. n = len(tables)\n2. IF n <= 1: RETURN tables[0] as single table plan\n3. Initialize memo table: dp[subset] = best plan for subset\n   For each single table t:\n     dp[{t}] = create_base_plan(t, stats_map[t])\n4. For subset_size = 2 to n:\n     For each subset S of size subset_size:\n       best_plan[S] = NULL\n       best_cost[S] = INFINITY\n       For each partition (S1, S2) where S1 \u222a S2 = S and S1 \u2229 S2 = \u2205:\n         IF dp[S1] exists AND dp[S2] exists:\n           join_cond = find_join_condition(S1, S2, join_conditions)\n           IF join_cond exists OR subset_size == n:\n             -- Consider all join implementations\n             FOR join_impl IN [NestedLoop, HashJoin, MergeJoin]:\n               plan = create_join_plan(\n                   join_impl, dp[S1], dp[S2], join_cond)\n               IF plan.cost < best_cost[S]:\n                 best_cost[S] = plan.cost\n                 best_plan[S] = plan\n5. RETURN dp[{all tables}]\nCOMPLEXITY: O(3^n) subsets, O(n * 2^n) for practical pruning\nNOTE: For n > 10, fall back to greedy join ordering\n```\n### 5.5 Greedy Join Ordering (Fallback for Large Queries)\n```\nALGORITHM: greedy_join_order(tables, join_conditions, stats_map)\nINPUT: Same as DP algorithm\nOUTPUT: Near-optimal join plan\n1. remaining = set of all tables\n2. Select starting table with smallest estimated cardinality\n3. current_plan = base_plan(start_table)\n4. remaining.remove(start_table)\n5. WHILE remaining not empty:\n     best_next = NULL\n     best_cost = INFINITY\n     FOR each table t in remaining:\n       join_cond = find_join_condition(current_plan.tables, {t})\n       candidate = create_join_plan(current_plan, base_plan(t), join_cond)\n       IF candidate.cost < best_cost:\n         best_cost = candidate.cost\n         best_next = t\n     current_plan = join(current_plan, base_plan(best_next))\n     remaining.remove(best_next)\n6. RETURN current_plan\nCOMPLEXITY: O(n^2) where n = number of tables\n```\n---\n## 6. Error Handling Matrix\n| Error Code | Category | Condition | Recovery Action |\n|------------|----------|-----------|-----------------|\n| `STATS_NOT_FOUND` | Statistics | Table statistics missing | Fall back to default estimates |\n| `STATS_STALE` | Statistics | Stats older than threshold | Log warning, use cached stats |\n| `STATS_COLLECTION_FAILED` | Statistics | ANALYZE fails mid-collection | Partial stats persisted with warning |\n| `SELECTIVITY_UNKNOWN` | Estimation | Cannot estimate predicate | Use conservative 0.5 selectivity |\n| `NO_VALID_PLAN` | Planning | Cannot find valid execution plan | Return error with diagnostic info |\n| `JOIN_TOO_COMPLEX` | Planning | More than 10 tables in join | Use greedy ordering with warning |\n| `INDEX_CORRUPT` | Validation | Index statistics inconsistent | Invalidate index stats, rescan |\n| `COST_OVERFLOW` | Calculation | Cost exceeds max value | Cap at MAX_COST, continue planning |\n| `RECURSION_LIMIT` | Planning | Nested subquery depth > 50 | Return error, query too complex |\n| `MEMORY_LIMIT` | Resource | Plan memory exceeds limit | Trigger plan pruning |\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum PlannerError {\n    #[error(\"Statistics not found for table '{0}'\")]\n    StatsNotFound(String),\n    #[error(\"Statistics are stale for table '{0}' (age: {1}s)\")]\n    StatsStale(String, u64),\n    #[error(\"Statistics collection failed: {0}\")]\n    StatsCollectionFailed(String),\n    #[error(\"Cannot estimate selectivity for predicate: {0}\")]\n    SelectivityUnknown(String),\n    #[error(\"No valid execution plan found for query\")]\n    NoValidPlan,\n    #[error(\"Join too complex: {0} tables exceeds limit of 10\")]\n    JoinTooComplex(usize),\n    #[error(\"Index '{0}' statistics are corrupt\")]\n    IndexCorrupt(String),\n    #[error(\"Cost calculation overflow\")]\n    CostOverflow,\n    #[error(\"Query recursion limit exceeded\")]\n    RecursionLimitExceeded,\n    #[error(\"Memory limit exceeded during planning\")]\n    MemoryLimitExceeded,\n}\n```\n---\n## 7. Implementation Sequence\n### Phase 1: Statistics Foundation (2 hours)\n**Checkpoint 1.1:** `ColumnStatistics` and `TableStatistics` structs\n- Define all statistics data structures\n- Implement serialization for sqlite_stat1 storage\n- Unit tests for statistics structure validation\n**Checkpoint 1.2:** Statistics collection implementation\n- Implement table traversal for statistics\n- Column-level distinct counting with HyperLogLog\n- Histogram construction for skewed columns\n- Integration with B-Tree module for traversal\n### Phase 2: Selectivity Engine (1.5 hours)\n**Checkpoint 2.1:** Predicate selectivity estimation\n- Equality predicate estimation\n- Range predicate with histogram interpolation\n- LIKE pattern estimation\n- Compound predicate (AND/OR/NOT) handling\n**Checkpoint 2.2:** Selectivity caching\n- Cache frequently estimated predicates\n- Invalidation on statistics update\n- Confidence scoring for estimates\n### Phase 3: Cost Model (1.5 hours)\n**Checkpoint 3.1:** I/O cost estimation\n- Sequential page cost calculation\n- Random page cost for index lookups\n- Effective cache size consideration\n**Checkpoint 3.2:** CPU cost estimation\n- Tuple processing cost\n- Operator evaluation cost\n- Index traversal cost\n### Phase 4: Access Path Selection (1.5 hours)\n**Checkpoint 4.1:** Table scan cost calculation\n- Full table scan estimation\n- Filter push-down consideration\n**Checkpoint 4.2:** Index scan cost calculation\n- Index-only scan detection\n- Index + table lookup cost\n- Multi-column index handling\n**Checkpoint 4.3:** Path comparison and selection\n- Cost comparison logic\n- Tie-breaking heuristics\n### Phase 5: Join Planning (2 hours)\n**Checkpoint 5.1:** Base join implementations\n- Nested loop join planning\n- Hash join planning\n- Merge join planning\n**Checkpoint 5.2:** Dynamic programming join ordering\n- Subset enumeration\n- Memoization table\n- Plan pruning\n**Checkpoint 5.3:** Greedy fallback\n- Greedy selection algorithm\n- Threshold for DP vs greedy\n### Phase 6: Plan Generation (1 hour)\n**Checkpoint 6.1:** Complete plan assembly\n- Combine access paths and joins\n- Add projection, sort, limit nodes\n- Plan tree validation\n**Checkpoint 6.2:** EXPLAIN output\n- Text format EXPLAIN\n- JSON format for tooling\n- Cost annotation display\n### Phase 7: Integration & Testing (0.5 hours)\n**Checkpoint 7.1:** End-to-end integration\n- Connect with parser AST\n- Connect with execution engine\n- sqlite_stat1 table management\n---\n## 8. Test Specification\n### 8.1 Statistics Collection Tests\n```rust\n#[test]\nfn test_collect_statistics_empty_table() {\n    // Verify stats for table with 0 rows\n}\n#[test]\nfn test_collect_statistics_single_column() {\n    // Verify distinct_count, min, max, null_count\n}\n#[test]\nfn test_collect_statistics_histogram() {\n    // Verify histogram bucket distribution\n}\n#[test]\nfn test_statistics_persistence() {\n    // Verify round-trip to sqlite_stat1\n}\n#[test]\nfn test_statistics_incremental_update() {\n    // Verify stats update after INSERT/DELETE\n}\n```\n### 8.2 Selectivity Estimation Tests\n```rust\n#[test]\nfn test_selectivity_equality_exact() {\n    // col = value with known distinct_count\n    assert_approx!(selectivity, 1.0 / distinct_count, 0.01);\n}\n#[test]\nfn test_selectivity_range_histogram() {\n    // col BETWEEN a AND b with histogram\n}\n#[test]\nfn test_selectivity_and_combination() {\n    // pred1 AND pred2 = s1 * s2 (independent)\n}\n#[test]\nfn test_selectivity_or_combination() {\n    // pred1 OR pred2 = s1 + s2 - s1*s2\n}\n#[test]\nfn test_selectivity_default_fallback() {\n    // Unknown predicate type returns 0.5\n}\n#[test]\nfn test_selectivity_correlated_columns() {\n    // Handle correlated column predicates\n}\n```\n### 8.3 Cost Model Tests\n```rust\n#[test]\nfn test_table_scan_cost_calculation() {\n    // seq_page_cost * page_count + cpu_tuple_cost * row_count\n}\n#[test]\nfn test_index_scan_covering_cost() {\n    // Index-only scan, no table lookup\n}\n#[test]\nfn test_index_scan_with_lookup_cost() {\n    // Index scan + random_page_cost * matches\n}\n#[test]\nfn test_nested_loop_join_cost() {\n    // outer_cost + outer_rows * inner_cost_per_row\n}\n#[test]\nfn test_hash_join_cost() {\n    // build_cost + probe_cost + hash_overhead\n}\n```\n### 8.4 Join Planning Tests\n```rust\n#[test]\nfn test_join_order_two_tables() {\n    // Verify optimal join order for 2 tables\n}\n#[test]\nfn test_join_order_many_to_many() {\n    // Verify handling of M:N relationships\n}\n#[test]\nfn test_join_order_dp_optimal() {\n    // Verify DP finds optimal for 5+ tables\n}\n#[test]\nfn test_join_order_greedy_fallback() {\n    // Verify greedy used for > 10 tables\n}\n#[test]\nfn test_join_with_no_conditions() {\n    // Cross join handling\n}\n#[test]\nfn test_left_join_optimization() {\n    // Verify outer join ordering constraints\n}\n```\n### 8.5 Integration Tests\n```rust\n#[test]\nfn test_end_to_end_simple_select() {\n    // SELECT * FROM table WHERE id = 1\n    // Verify index scan selected\n}\n#[test]\nfn test_end_to_end_complex_join() {\n    // 4-table join with various conditions\n    // Verify optimal join order\n}\n#[test]\nfn test_explain_output_format() {\n    // Verify EXPLAIN matches expected format\n}\n#[test]\nfn test_analyze_command() {\n    // Execute ANALYZE, verify stats populated\n}\n#[test]\nfn test_stats_invalidation() {\n    // Modify table, verify stats invalidated\n}\n```\n### 8.6 Performance Benchmarks\n```rust\n#[bench]\nfn bench_plan_generation_simple(b: &mut Bencher) {\n    // Single table, single predicate\n    // Target: < 0.1ms\n}\n#[bench]\nfn bench_plan_generation_10_table_join(b: &mut Bencher) {\n    // 10 table join\n    // Target: < 1ms\n}\n#[bench]\nfn bench_statistics_collection(b: &mut Bencher) {\n    // 1M row table\n    // Target: < 5s\n}\n#[bench]\nfn bench_selectivity_estimation(b: &mut Bencher) {\n    // Complex predicate estimation\n    // Target: < 10\u03bcs\n}\n```\n---\n## 9. Performance Targets\n| Metric | Target | Measurement Method |\n|--------|--------|-------------------|\n| Plan generation (simple query) | < 0.1ms | Benchmark timing |\n| Plan generation (10-table join) | < 1.0ms | Benchmark timing |\n| Statistics collection (1M rows) | < 5s | ANALYZE timing |\n| Selectivity estimation | < 10\u03bcs | Per-predicate timing |\n| Memory usage (planning) | < 10MB | Peak memory profiling |\n| Selectivity accuracy | Within 10x | Actual vs estimated rows |\n| DP join planning | \u226410 tables | Algorithm limit |\n| Statistics storage | < 1KB per column | sqlite_stat1 size |\n---\n## 10. Diagrams\n### Diagram tdd-diag-m8-1: Query Planner Architecture\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     QUERY PLANNER MODULE                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Query AST   \u2502\u2500\u2500\u2500\u25b6\u2502  Query      \u2502\u2500\u2500\u2500\u25b6\u2502  Execution Plan     \u2502 \u2502\n\u2502  \u2502 (from       \u2502    \u2502  Planner    \u2502    \u2502  (to Executor)      \u2502 \u2502\n\u2502  \u2502  Parser)    \u2502    \u2502  (Facade)   \u2502    \u2502                     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                            \u2502                                    \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502         \u25bc                  \u25bc                  \u25bc                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502 Statistics  \u2502    \u2502 Selectivity \u2502    \u2502 Cost Model  \u2502        \u2502\n\u2502  \u2502 Manager     \u2502    \u2502 Estimator   \u2502    \u2502             \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502         \u2502                                      \u2502               \u2502\n\u2502         \u25bc                                      \u25bc               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502sqlite_stat1 \u2502                       \u2502 Access Path \u2502        \u2502\n\u2502  \u2502 (storage)   \u2502                       \u2502 Selector    \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                                               \u2502                \u2502\n\u2502                                               \u25bc                \u2502\n\u2502                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502                                        \u2502 Join        \u2502        \u2502\n\u2502                                        \u2502 Planner     \u2502        \u2502\n\u2502                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n### Diagram tdd-diag-m8-2: Statistics Collection Flow\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  ANALYZE       \u2502     \u2502  Table B-Tree  \u2502     \u2502  Column        \u2502\n\u2502  Command       \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Traversal     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Aggregators   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                       \u25bc                              \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  Distinct      \u2502             \u2502  Min/Max       \u2502\n              \u2502  Counter       \u2502             \u2502  Tracker       \u2502\n              \u2502  (HyperLogLog) \u2502             \u2502                \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502                              \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u25bc\n                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                            \u2502  Histogram     \u2502\n                            \u2502  Builder       \u2502\n                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                            \u2502 TableStatistics\u2502\n                            \u2502 Structure      \u2502\n                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                            \u2502 sqlite_stat1   \u2502\n                            \u2502 Persistence    \u2502\n                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n### Diagram tdd-diag-m8-3: Selectivity Estimation Decision Tree\n```\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502   Predicate     \u2502\n                         \u2502   Type?         \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u25bc                        \u25bc                        \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Equality  \u2502           \u2502  Range    \u2502           \u2502 Compound  \u2502\n   \u2502 col=val   \u2502           \u2502 col</>/   \u2502           \u2502 AND/OR/   \u2502\n   \u2502           \u2502           \u2502 BETWEEN   \u2502           \u2502 NOT       \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u25bc                       \u25bc                       \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Histogram \u2502     Yes   \u2502 Histogram \u2502     Yes   \u2502 Recurse   \u2502\n   \u2502 Available?\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Available?\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 into      \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518           \u2502 sub-exprs \u2502\n   No   \u2502                       \u2502   No             \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc                       \u25bc                       \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n   \u2502 1/distinct\u2502           \u2502 Linear    \u2502                 \u2502\n   \u2502 count     \u2502           \u2502 interp.   \u2502                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502 min-max   \u2502                 \u2502\n                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n                                                        \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Combine:  \u2502\n   \u2502 AND: s1*s2\u2502\n   \u2502 OR: s1+s2 \u2502\n   \u2502   -s1*s2  \u2502\n   \u2502 NOT: 1-s  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n### Diagram tdd-diag-m8-4: Access Path Selection\n```\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502  Query Predicates   \u2502\n                         \u2502  + Table Info       \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502  Calculate Table    \u2502\n                         \u2502  Scan Cost          \u2502\n                         \u2502  (seq_page * pages) \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502  For Each Index:    \u2502\n                         \u2502  Can it help?       \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                     \u25bc                     \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Index 1    \u2502        \u2502 Index 2    \u2502        \u2502 Index N    \u2502\n       \u2502 Applicable?\u2502        \u2502 Applicable?\u2502        \u2502 Applicable?\u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                     \u2502                     \u2502\n             \u25bc                     \u25bc                     \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Estimate   \u2502        \u2502 Estimate   \u2502        \u2502 Estimate   \u2502\n       \u2502 Selectivity\u2502        \u2502 Selectivity\u2502        \u2502 Selectivity\u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                     \u2502                     \u2502\n             \u25bc                     \u25bc                     \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Index Cost \u2502        \u2502 Index Cost \u2502        \u2502 Index Cost \u2502\n       \u2502 + Lookup?  \u2502        \u2502 + Lookup?  \u2502        \u2502 + Lookup?  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                     \u2502                     \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u25bc\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502  Select Minimum     \u2502\n                         \u2502  Cost Path          \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502  Return Best        \u2502\n                         \u2502  PlanNode           \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n### Diagram tdd-diag-m8-5: Join Ordering DP Algorithm\n```\nTables: [A, B, C, D]\nDP Table (memoization):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Subset        \u2502 Best Plan              \u2502 Cost   \u2502 Cardinality \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 {A}           \u2502 TableScan(A)           \u2502 100    \u2502 1000        \u2502\n\u2502 {B}           \u2502 IndexScan(B)           \u2502 50     \u2502 500         \u2502\n\u2502 {C}           \u2502 TableScan(C)           \u2502 200    \u2502 2000        \u2502\n\u2502 {D}           \u2502 TableScan(D)           \u2502 75     \u2502 750         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 {A,B}         \u2502 Join(A, B)             \u2502 180    \u2502 100         \u2502\n\u2502 {A,C}         \u2502 Join(A, C)             \u2502 400    \u2502 500         \u2502\n\u2502 {A,D}         \u2502 Join(A, D)             \u2502 250    \u2502 200         \u2502\n\u2502 {B,C}         \u2502 Join(B, C)             \u2502 350    \u2502 400         \u2502\n\u2502 {B,D}         \u2502 Join(B, D)             \u2502 200    \u2502 150         \u2502\n\u2502 {C,D}         \u2502 Join(C, D)             \u2502 450    \u2502 600         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 {A,B,C}       \u2502 Join({A,B}, C)         \u2502 450    \u2502 50          \u2502\n\u2502 {A,B,D}       \u2502 Join({A,B}, D)         \u2502 300    \u2502 25          \u2502\n\u2502 {A,C,D}       \u2502 Join({A,D}, C)         \u2502 550    \u2502 100         \u2502\n\u2502 {B,C,D}       \u2502 Join({B,D}, C)         \u2502 480    \u2502 80          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 {A,B,C,D}     \u2502 Join({A,B,D}, C)       \u2502 420    \u2502 10          \u2502\n\u2502               \u2502 \u2605 OPTIMAL              \u2502        \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nEnumeration Order:\n1. Single tables (size 1)\n2. Pairs (size 2) - consider all partitions\n3. Triples (size 3) - consider all 2+1 partitions\n4. Quadruples (size 4) - consider all 3+1 and 2+2 partitions\n```\n### Diagram tdd-diag-m8-6: Execution Plan Tree Structure\n```\nExample Query:\nSELECT o.id, c.name, SUM(oi.quantity)\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN order_items oi ON o.id = oi.order_id\nWHERE o.status = 'completed' AND c.country = 'US'\nGROUP BY o.id, c.name\nORDER BY SUM(oi.quantity) DESC\nLIMIT 10;\nExecution Plan Tree:\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502   LIMIT     \u2502\n                           \u2502   limit=10  \u2502\n                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502    SORT     \u2502\n                           \u2502 ORDER BY    \u2502\n                           \u2502 SUM DESC    \u2502\n                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502  AGGREGATE  \u2502\n                           \u2502 GROUP BY    \u2502\n                           \u2502 o.id,c.name \u2502\n                           \u2502 SUM(qty)    \u2502\n                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                         \u2502                         \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n  \u2502  FILTER   \u2502             \u2502  INDEX    \u2502                  \u2502\n  \u2502 status=   \u2502             \u2502  SCAN     \u2502                  \u2502\n  \u2502 completed \u2502             \u2502 order_    \u2502                  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518             \u2502 items_idx \u2502                  \u2502\n        \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510                                            \u2502\n  \u2502 HASH JOIN \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2502 build:    \u2502\n  \u2502 probe:    \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 INDEX SCAN\u2502\n  \u2502 customers \u2502\n  \u2502 country_idx\u2502\n  \u2502 country=US\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nCost Annotations (shown in EXPLAIN):\n- LIMIT: cost=0, rows=10\n- SORT: cost=15, rows=100\n- AGGREGATE: cost=50, rows=100\n- HASH JOIN: cost=200, rows=500\n- FILTER: cost=100, rows=1000\n- INDEX SCAN (items): cost=80, rows=5000\n- INDEX SCAN (customers): cost=20, rows=50\n```\n---\n## CRITERIA_JSON\n```json\n{\n  \"module_id\": \"build-sqlite-m8\",\n  \"module_name\": \"Query Planner & Statistics\",\n  \"version\": \"1.0\",\n  \"status\": \"code-ready\",\n  \"character_count\": 18500,\n  \"sections_complete\": [\n    \"module_charter\",\n    \"file_structure\",\n    \"data_model\",\n    \"interface_contracts\",\n    \"algorithm_specification\",\n    \"error_handling_matrix\",\n    \"implementation_sequence\",\n    \"test_specification\",\n    \"performance_targets\",\n    \"diagrams\"\n  ],\n  \"diagrams_included\": [\n    \"tdd-diag-m8-1\",\n    \"tdd-diag-m8-2\",\n    \"tdd-diag-m8-3\",\n    \"tdd-diag-m8-4\",\n    \"tdd-diag-m8-5\",\n    \"tdd-diag-m8-6\"\n  ],\n  \"dependencies\": [\n    \"build-sqlite-m5\",\n    \"build-sqlite-m6\",\n    \"build-sqlite-m7\"\n  ],\n  \"estimated_implementation_hours\": 10,\n  \"key_abstractions\": [\n    \"TableStatistics\",\n    \"SelectivityEstimate\",\n    \"CostEstimate\",\n    \"ExecutionPlan\",\n    \"PlanNode\"\n  ],\n  \"synced_at\": \"2026-02-26T00:00:00Z\"\n}\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m9 -->\n# Technical Design Specification: M9 - Transactions (Rollback Journal)\n## Module Charter\n| Attribute | Value |\n|-----------|-------|\n| **Module ID** | build-sqlite-m9 |\n| **Domain** | Systems/Storage |\n| **Priority** | Critical |\n| **Dependencies** | M1 (Schema Parser), M2 (Storage Engine), M3 (B-Tree), M4 (Pager), M5 (VM), M6 (SQL Parser), M7 (Query Planner), M8 (Table Access) |\n| **Dependents** | M10 (WAL Mode), M11 (Concurrency Control) |\n| **Estimated Effort** | 10 hours |\n| **Risk Level** | High |\n### Mission Statement\nImplement ACID-compliant transactions using a rollback journal pattern. This module provides atomicity and durability guarantees through write-before-modify journaling, strict write ordering, and crash recovery via hot journal detection.\n### Success Criteria\n- All ACID properties verified through crash simulation tests\n- Zero data loss after simulated crashes at any point in transaction lifecycle\n- Performance within specified targets under normal operation\n- Complete recovery from hot journals after process termination\n---\n## File Structure\n```\nsrc/\n\u251c\u2500\u2500 transaction/\n\u2502   \u251c\u2500\u2500 mod.rs              # Module exports\n\u2502   \u251c\u2500\u2500 manager.rs          # TransactionManager, TransactionState\n\u2502   \u251c\u2500\u2500 journal.rs          # Journal file operations, JournalHeader\n\u2502   \u251c\u2500\u2500 recovery.rs         # Hot journal detection, recovery logic\n\u2502   \u2514\u2500\u2500 state_machine.rs    # Transaction state machine transitions\n\u251c\u2500\u2500 pager/\n\u2502   \u2514\u2500\u2500 journal_integration.rs  # Pager hooks for write-before-modify\ntests/\n\u251c\u2500\u2500 transaction_test.rs     # Unit tests for transaction logic\n\u251c\u2500\u2500 journal_test.rs         # Journal format tests\n\u251c\u2500\u2500 crash_recovery_test.rs  # Crash simulation tests\n\u2514\u2500\u2500 concurrency_test.rs     # Isolation verification tests\n```\n---\n## Complete Data Model\n### Transaction State Enumeration\n```rust\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum TransactionState {\n    /// No active transaction; each statement auto-commits\n    AUTOCOMMIT,\n    /// BEGIN executed; no writes yet; journal not created\n    STARTED,\n    /// First write occurred; journal created and active\n    DIRTY,\n    /// COMMIT in progress; writing database file\n    COMMITTING,\n    /// ROLLBACK in progress; restoring pages from journal\n    ROLLING_BACK,\n}\n```\n### Transaction Mode Enumeration\n```rust\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum TransactionMode {\n    /// Default: BEGIN (deferred transaction)\n    DEFERRED,\n    /// BEGIN IMMEDIATE: acquire write lock immediately\n    IMMEDIATE,\n    /// BEGIN EXCLUSIVE: acquire exclusive lock\n    EXCLUSIVE,\n}\n```\n### Journal Header Structure (Byte-Level Layout)\n**Total Size: 32 bytes**\n| Offset | Size | Field | Type | Description |\n|--------|------|-------|------|-------------|\n| 0 | 4 | magic | u32 LE | Magic number: 0xD9D50589 |\n| 4 | 4 | format_version | u32 LE | Journal format version: 1 |\n| 8 | 4 | page_count | u32 LE | Number of pages in journal |\n| 12 | 4 | nonce_a | u32 LE | Random salt for checksum A |\n| 16 | 4 | nonce_b | u32 LE | Random salt for checksum B |\n| 20 | 4 | checksum_initial_a | u32 LE | Initial checksum A (before any pages) |\n| 24 | 4 | checksum_initial_b | u32 LE | Initial checksum B (before any pages) |\n| 28 | 4 | db_size_pages | u32 LE | Database size in pages at transaction start |\n```\nOffset  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n      |     magic      |   version    |  page_count   |\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\nOffset 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n      |    nonce_a     |    nonce_b    | checksum_A[0] |\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\nOffset 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n      | checksum_B[0]  | db_size_pages |   RESERVED    |\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n```\n### Journal Page Record Structure (Byte-Level Layout)\n**Total Size: 8 + page_size bytes (typically 4104 bytes for 4096-byte pages)**\n| Offset | Size | Field | Type | Description |\n|--------|------|-------|------|-------------|\n| 0 | 4 | page_number | u32 LE | 1-indexed page number in database |\n| 4 | 4 | record_checksum_a | u32 LE | Rolling checksum A after this record |\n| 8 | 4 | record_checksum_b | u32 LE | Rolling checksum B after this record |\n| 12 | page_size | page_data | [u8] | Original page content before modification |\n**Note:** For compatibility, checksums are stored before page data in the record header.\n```\nRecord Layout (for page_size = 4096):\nOffset  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n      |  page_number   | checksum_A[n] | checksum_B[n] |\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\nOffset 16 ... 4111\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n      |              page_data (4096 bytes)           |\n      |                                               |\n      +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n```\n### Journal File Complete Layout\n```\n+---------------------------+\n|     Journal Header        |  32 bytes\n|  (magic, version, salts)  |\n+---------------------------+\n|      Page Record 1        |  12 + page_size bytes\n|  (pgno, checksums, data)  |\n+---------------------------+\n|      Page Record 2        |\n+---------------------------+\n|          ...              |\n+---------------------------+\n|      Page Record N        |\n+---------------------------+\n```\n### Transaction Control Block\n```rust\n#[derive(Debug)]\npub struct TransactionContext {\n    /// Current state of the transaction\n    pub state: TransactionState,\n    /// Transaction mode (deferred, immediate, exclusive)\n    pub mode: TransactionMode,\n    /// Journal file path\n    pub journal_path: PathBuf,\n    /// Journal file handle (None if not created)\n    pub journal_handle: Option<File>,\n    /// Journal header (cached)\n    pub journal_header: Option<JournalHeader>,\n    /// Set of page numbers written to journal\n    pub journaled_pages: HashSet<u32>,\n    /// Database size at transaction start (for rollback)\n    pub initial_db_size: u32,\n    /// Checksum state for rolling checksums\n    pub checksum_state: ChecksumState,\n    /// Lock held on the database\n    pub lock_level: LockLevel,\n}\n```\n### Checksum Algorithm\n```rust\npub struct ChecksumState {\n    pub sum_a: u32,\n    pub sum_b: u32,\n    pub nonce_a: u32,\n    pub nonce_b: u32,\n}\nimpl ChecksumState {\n    /// Update checksum with page data\n    /// Algorithm: iterated sum with rotation\n    pub fn update(&mut self, data: &[u8]) {\n        let words = data.chunks_exact(4);\n        for chunk in words {\n            let word = u32::from_le_bytes(chunk.try_into().unwrap());\n            self.sum_a = self.sum_a.wrapping_add(word).wrapping_add(self.nonce_a);\n            self.sum_b = self.sum_a.wrapping_add(self.sum_b).wrapping_add(self.nonce_b);\n        }\n    }\n}\n```\n---\n## Interface Contracts\n### ITransactionManager\n```rust\npub trait ITransactionManager: Send + Sync {\n    /// Begin a new transaction with specified mode\n    /// Returns error if transaction already active\n    fn begin(&mut self, mode: TransactionMode) -> Result<(), TransactionError>;\n    /// Commit the current transaction\n    /// Implements full write ordering protocol\n    fn commit(&mut self) -> Result<(), TransactionError>;\n    /// Rollback the current transaction\n    /// Restores all journaled pages\n    fn rollback(&mut self) -> Result<(), TransactionError>;\n    /// Get current transaction state\n    fn state(&self) -> TransactionState;\n    /// Check if a page needs to be journaled before write\n    /// Called by pager before any page modification\n    fn should_journal_page(&self, page_num: u32) -> bool;\n    /// Record a page in the journal (write-before-modify)\n    /// Must be called before modifying the page\n    fn journal_page(&mut self, page_num: u32, page_data: &[u8]) -> Result<(), TransactionError>;\n    /// Run auto-commit if in AUTOCOMMIT mode\n    /// Called after statement execution\n    fn maybe_autocommit(&mut self) -> Result<(), TransactionError>;\n}\n```\n### IJournalManager\n```rust\npub trait IJournalManager: Send + Sync {\n    /// Create a new journal file at the specified path\n    /// Writes header with random nonces\n    fn create(&mut self, path: &Path, db_size: u32) -> Result<(), JournalError>;\n    /// Open existing journal for reading (recovery)\n    fn open_for_recovery(&mut self, path: &Path) -> Result<JournalHeader, JournalError>;\n    /// Write a page record to the journal\n    /// Updates rolling checksum\n    fn write_page_record(&mut self, page_num: u32, page_data: &[u8]) -> Result<(), JournalError>;\n    /// Sync journal file to disk\n    fn sync(&mut self) -> Result<(), JournalError>;\n    /// Read all page records from journal (for recovery)\n    fn read_all_pages(&mut self) -> Result<Vec<(u32, Vec<u8>)>, JournalError>;\n    /// Delete the journal file after successful commit\n    fn delete(&mut self) -> Result<(), JournalError>;\n    /// Check if a journal file exists (hot journal detection)\n    fn exists(&self, path: &Path) -> bool;\n    /// Validate journal integrity via checksum\n    fn validate(&mut self) -> Result<bool, JournalError>;\n}\n```\n### IRecoveryManager\n```rust\npub trait IRecoveryManager: Send + Sync {\n    /// Check for hot journal and perform recovery if needed\n    /// Called at database open\n    fn check_and_recover(&mut self, db_path: &Path) -> Result<RecoveryResult, RecoveryError>;\n    /// Perform rollback from hot journal\n    fn recover_from_journal(&mut self, journal_path: &Path) -> Result<(), RecoveryError>;\n}\n```\n### Pager Integration Interface\n```rust\npub trait IJournalAwarePager {\n    /// Called by pager before modifying a page\n    /// Returns error if journaling fails\n    fn on_before_page_modify(&mut self, page_num: u32) -> Result<(), PagerError>;\n    /// Get current database size in pages\n    fn database_size(&self) -> u32;\n    /// Write a page directly to database file\n    fn write_page_direct(&mut self, page_num: u32, data: &[u8]) -> Result<(), PagerError>;\n    /// Read a page from database file\n    fn read_page(&self, page_num: u32) -> Result<Vec<u8>, PagerError>;\n    /// Sync database file to disk\n    fn sync_database(&mut self) -> Result<(), PagerError>;\n    /// Truncate database to specified size\n    fn truncate(&mut self, page_count: u32) -> Result<(), PagerError>;\n}\n```\n---\n## Algorithm Specification\n### ALG-001: Transaction State Machine\n```\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                                                     \u2502\n                    \u25bc                                                     \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                               \u2502\n              \u2502 AUTOCOMMIT\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n              \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                                                \u2502\n                    \u2502 BEGIN                                                \u2502 COMMIT success\n                    \u25bc                         ROLLBACK                     \u2502 (journal deleted)\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502                          \u2502\n              \u2502  STARTED  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                          \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502                          \u2502\n                    \u2502 First write               \u2502                          \u2502\n                    \u25bc                           \u2502                          \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502                          \u2502\n              \u2502   DIRTY   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                          \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                     \u25bc                          \u2502\n                    \u2502 COMMIT              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n                    \u25bc                     \u2502 ROLLING_BACK\u2502                 \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n              \u2502 COMMITTING\u2502                                               \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                                               \u2502\n                    \u2502 Write complete                                      \u2502\n                    \u2502 journal synced                                      \u2502\n                    \u2502 db synced                                            \u2502\n                    \u2502 journal deleted                                      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n**State Transitions:**\n| From | To | Trigger | Guard Condition |\n|------|-----|---------|-----------------|\n| AUTOCOMMIT | STARTED | BEGIN | No active transaction |\n| STARTED | DIRTY | First page write | Journal created successfully |\n| STARTED | AUTOCOMMIT | ROLLBACK | No changes made |\n| DIRTY | COMMITTING | COMMIT | Journal synced |\n| DIRTY | ROLLING_BACK | ROLLBACK | Journal exists and valid |\n| COMMITTING | AUTOCOMMIT | Journal deleted | Database synced |\n| ROLLING_BACK | AUTOCOMMIT | Rollback complete | All pages restored |\n### ALG-002: Write-Before-Modify Journaling\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     WRITE-BEFORE-MODIFY FLOW                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                          \u2502\n\u2502  Pager.request_write(page_num)                                           \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 Is page already    \u2502\u2500\u2500YES\u2500\u2500\u25ba\u2502 Allow modification                      \u2502\n\u2502  \u2502 in journal?        \u2502        \u2502 (no action needed)                      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n\u2502           \u2502 NO                                                             \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 Read current page  \u2502                                                  \u2502\n\u2502  \u2502 from database      \u2502                                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                  \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 Create journal if  \u2502                                                  \u2502\n\u2502  \u2502 not exists (lazy)  \u2502                                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                  \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 Write page record  \u2502                                                  \u2502\n\u2502  \u2502 to journal         \u2502                                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                  \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 Mark page as       \u2502                                                  \u2502\n\u2502  \u2502 journaled          \u2502                                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                  \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u2502 Allow modification \u2502                                                  \u2502\n\u2502                                                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n**Pseudocode:**\n```\nPROCEDURE journal_page_before_modify(page_num, pager, txn_manager):\n    IF txn_manager.is_page_journaled(page_num):\n        RETURN  // Already saved, can modify freely\n    // Lazy journal creation on first write\n    IF txn_manager.state == STARTED:\n        txn_manager.create_journal(pager.database_size())\n        txn_manager.state = DIRTY\n    // Read original page content\n    original_data = pager.read_page(page_num)\n    // Write to journal\n    txn_manager.journal.write_page_record(page_num, original_data)\n    // Mark as journaled\n    txn_manager.mark_page_journaled(page_num)\nEND PROCEDURE\n```\n### ALG-003: COMMIT with Write Ordering\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     COMMIT WRITE ORDERING PROTOCOL                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                          \u2502\n\u2502  COMMIT invoked                                                          \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 State == STARTED?  \u2502\u2500\u2500YES\u2500\u2500\u25ba\u2502 Transition to AUTOCOMMIT                \u2502\n\u2502  \u2502 (no changes made)  \u2502        \u2502 RETURN success                          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n\u2502           \u2502 NO                                                             \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 Transition to      \u2502                                                  \u2502\n\u2502  \u2502 COMMITTING         \u2502                                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                  \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   FAIL   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 STEP 1: Sync       \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 RETURN error (transaction still    \u2502\u2502\n\u2502  \u2502 journal file       \u2502         \u2502 active, can retry)                  \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502           \u2502 SUCCESS                                                       \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   FAIL   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 STEP 2: Write all  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 Journal valid, may have partial     \u2502\u2502\n\u2502  \u2502 dirty pages to DB  \u2502         \u2502 writes. ROLLBACK still possible.    \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502           \u2502 SUCCESS                                                       \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   FAIL   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 STEP 3: Sync       \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 DB may be inconsistent.             \u2502\u2502\n\u2502  \u2502 database file      \u2502         \u2502 Hot journal will enable recovery.   \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502           \u2502 SUCCESS                                                       \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   FAIL   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 STEP 4: Delete     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 Transaction committed, but journal  \u2502\u2502\n\u2502  \u2502 journal file       \u2502         \u2502 remains. Will be cleaned on next    \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 open or treated as stale.           \u2502\u2502\n\u2502           \u2502 SUCCESS             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502\n\u2502  \u2502 Transition to      \u2502                                                  \u2502\n\u2502  \u2502 AUTOCOMMIT         \u2502                                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                  \u2502\n\u2502           \u2502                                                              \u2502\n\u2502           \u25bc                                                              \u2502\n\u2502  RETURN success                                                          \u2502\n\u2502                                                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n**Critical Write Order:**\n1. **Journal Sync** - Ensures all undo data is durable\n2. **Database Write** - Write new page versions\n3. **Database Sync** - Ensure new data is durable\n4. **Journal Delete** - Mark transaction complete (commit point)\n### ALG-004: ROLLBACK Implementation\n```\nPROCEDURE rollback(txn_manager, pager):\n    IF txn_manager.state == STARTED:\n        // No changes made, just cleanup\n        txn_manager.state = AUTOCOMMIT\n        IF txn_manager.journal_exists():\n            txn_manager.journal.delete()\n        RETURN success\n    IF txn_manager.state != DIRTY:\n        RETURN error(\"Invalid state for rollback\")\n    txn_manager.state = ROLLING_BACK\n    // Open journal for reading\n    journal = txn_manager.journal\n    journal.open_for_recovery()\n    // Validate journal integrity\n    IF NOT journal.validate():\n        RETURN error(\"Journal corrupted, cannot rollback\")\n    // Read all journaled pages\n    pages = journal.read_all_pages()\n    // Restore each page in reverse order (optional, but matches SQLite)\n    FOR (page_num, page_data) IN pages:\n        pager.write_page_direct(page_num, page_data)\n    // Sync database to ensure rollback is durable\n    pager.sync_database()\n    // Restore database size if it was extended\n    IF txn_manager.initial_db_size < pager.database_size():\n        pager.truncate(txn_manager.initial_db_size)\n    // Delete journal\n    journal.delete()\n    // Reset state\n    txn_manager.state = AUTOCOMMIT\n    txn_manager.journaled_pages.clear()\n    RETURN success\nEND PROCEDURE\n```\n### ALG-005: Hot Journal Detection and Recovery\n```\nPROCEDURE check_and_recover(db_path):\n    journal_path = derive_journal_path(db_path)\n    IF NOT file_exists(journal_path):\n        RETURN RecoveryResult::NoRecoveryNeeded\n    // Hot journal exists - crash occurred during transaction\n    log_warning(\"Hot journal detected: {}\", journal_path)\n    // Validate journal integrity\n    journal = open_journal(journal_path)\n    header = journal.read_header()\n    // Verify magic number\n    IF header.magic != JOURNAL_MAGIC:\n        log_warning(\"Invalid journal magic, deleting stale journal\")\n        delete_file(journal_path)\n        RETURN RecoveryResult::StaleJournalDeleted\n    // Validate checksums\n    IF NOT journal.validate_checksums():\n        log_error(\"Journal checksum mismatch, cannot recover safely\")\n        // This is a critical error - manual intervention may be needed\n        RETURN RecoveryResult::CorruptedJournal\n    // Perform rollback recovery\n    pages = journal.read_all_pages()\n    FOR (page_num, page_data) IN pages:\n        write_page_to_database(db_path, page_num, page_data)\n    sync_database(db_path)\n    // Truncate if needed\n    IF header.db_size_pages < current_database_size(db_path):\n        truncate_database(db_path, header.db_size_pages)\n    // Delete recovered journal\n    delete_file(journal_path)\n    log_info(\"Recovery complete, restored {} pages\", pages.len())\n    RETURN RecoveryResult::Recovered(pages.len())\nEND PROCEDURE\n```\n---\n## Diagrams\n### Diagram 1: Transaction State Machine\n`![tdd-diag-m9-1: Transaction State Machine](tdd-diag-m9-1.svg)`\nFull state machine with all transitions, guards, and actions. Shows AUTOCOMMIT as initial/final state with all paths through STARTED, DIRTY, COMMITTING, and ROLLING_BACK states.\n### Diagram 2: Journal File Format\n`![tdd-diag-m9-2: Journal File Format](tdd-diag-m9-2.svg)`\nByte-level diagram of journal file showing 32-byte header layout followed by variable-length page records. Includes field sizes, offsets, and data types.\n### Diagram 3: Write Ordering Protocol\n`![tdd-diag-m9-3: Write Ordering Protocol](tdd-diag-m9-3.svg)`\nSequence diagram showing the four-phase write ordering: Journal Sync \u2192 Database Write \u2192 Database Sync \u2192 Journal Delete. Includes failure points and recovery implications at each stage.\n### Diagram 4: Write-Before-Modify Flow\n`![tdd-diag-m9-4: Write-Before-Modify Flow](tdd-diag-m9-4.svg)`\nFlowchart of the write-before-modify process showing lazy journal creation, page reading, journal writing, and the relationship between pager and transaction manager.\n### Diagram 5: Crash Recovery Process\n`![tdd-diag-m9-5: Crash Recovery Process](tdd-diag-m9-5.svg)`\nProcess flow for hot journal detection and recovery at database open. Shows decision tree for valid/invalid journals and the page restoration process.\n### Diagram 6: Transaction Lifecycle Timeline\n`![tdd-diag-m9-6: Transaction Lifecycle Timeline](tdd-diag-m9-6.svg)`\nTimeline showing typical transaction from BEGIN through multiple writes to COMMIT, with journal operations at each phase. Includes fsync points and atomic commit point marker.\n---\n## Error Handling Matrix\n| Error Code | Condition | Recovery Action | User Message |\n|------------|-----------|-----------------|--------------|\n| TXN_ALREADY_ACTIVE | BEGIN called with active transaction | Return error, no state change | \"database is locked\" |\n| TXN_NO_ACTIVE | COMMIT/ROLLBACK without transaction | Return error | \"no transaction is active\" |\n| JOURNAL_CREATE_FAILED | Cannot create journal file | Return error, remain in STARTED | \"disk I/O error\" |\n| JOURNAL_WRITE_FAILED | Write to journal failed | Return error, transaction continues | \"disk I/O error\" |\n| JOURNAL_SYNC_FAILED | fsync of journal failed | Return error, may retry commit | \"disk I/O error\" |\n| DB_WRITE_FAILED | Write to database failed | Transaction still recoverable | \"disk I/O error\" |\n| DB_SYNC_FAILED | fsync of database failed | Hot journal enables recovery | \"disk I/O error\" |\n| JOURNAL_DELETE_FAILED | Cannot delete journal | Log warning, commit succeeded | None (silent) |\n| ROLLBACK_DURING_ROLLBACK | Nested rollback attempt | Return error | \"rollback in progress\" |\n| JOURNAL_CORRUPTED | Checksum mismatch in journal | Mark database read-only | \"database disk image is malformed\" |\n| RECOVERY_FAILED | Cannot recover from hot journal | Return error, preserve journal | \"recovery failed\" |\n| LOCK_CONTENTION | Cannot acquire required lock | Return error | \"database is locked\" |\n| DISK_FULL | No space for journal | Return error | \"database or disk is full\" |\n### Error Propagation Strategy\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    ERROR PROPAGATION HIERARCHY                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                      \u2502\n\u2502  Layer 1: Journal Operations                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 JOURNAL_CREATE_FAILED, JOURNAL_WRITE_FAILED, JOURNAL_SYNC_   \u2502   \u2502\n\u2502  \u2502 FAILED \u2192 Wrap as TransactionError::JournalError              \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                           \u2502                                          \u2502\n\u2502                           \u25bc                                          \u2502\n\u2502  Layer 2: Transaction Operations                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 COMMIT_FAILED, ROLLBACK_FAILED \u2192 TransactionError::Commit    \u2502   \u2502\n\u2502  \u2502 or TransactionError::Rollback with cause                     \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                           \u2502                                          \u2502\n\u2502                           \u25bc                                          \u2502\n\u2502  Layer 3: User-Facing Errors                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 All errors mapped to SQLite-compatible error codes and       \u2502   \u2502\n\u2502  \u2502 messages for application compatibility                       \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n---\n## Implementation Sequence with Checkpoints\n### Phase 1: Transaction State Machine (1 hour)\n**Files:** `src/transaction/state_machine.rs`, `src/transaction/mod.rs`\n**Tasks:**\n1. Define `TransactionState` enum with all states\n2. Define `TransactionMode` enum\n3. Implement state transition validation\n4. Create `TransactionStateMachine` struct with transition guards\n5. Write unit tests for all valid and invalid transitions\n**Checkpoint C1:**\n- [ ] State machine tests pass for all valid transitions\n- [ ] Invalid transitions return appropriate errors\n- [ ] State query methods work correctly\n### Phase 2: Journal File Format and Header (1 hour)\n**Files:** `src/transaction/journal.rs`\n**Tasks:**\n1. Define `JournalHeader` struct with exact byte layout\n2. Implement serialization/deserialization (LE format)\n3. Implement checksum algorithm\n4. Create random nonce generation\n5. Write unit tests for header round-trip\n6. Write unit tests for checksum calculation\n**Checkpoint C2:**\n- [ ] Header serializes to exactly 32 bytes\n- [ ] Magic number and version correct\n- [ ] Checksums verify correctly\n### Phase 3: BEGIN and Lazy Journal Creation (1 hour)\n**Files:** `src/transaction/manager.rs`\n**Tasks:**\n1. Implement `TransactionManager::begin()`\n2. Implement lazy journal creation on first write\n3. Integrate with pager for database size query\n4. Generate journal path from database path\n5. Write tests for BEGIN with each mode\n6. Write tests for lazy journal creation\n**Checkpoint C3:**\n- [ ] BEGIN transitions to STARTED state\n- [ ] Journal not created until first write\n- [ ] Journal path correctly derived\n### Phase 4: Write-Before-Modify Journaling (1.5 hours)\n**Files:** `src/transaction/journal.rs`, `src/pager/journal_integration.rs`\n**Tasks:**\n1. Implement `JournalManager::write_page_record()`\n2. Implement page tracking in `TransactionManager`\n3. Create pager hook for pre-modify notification\n4. Implement duplicate page detection (skip if already journaled)\n5. Write tests for single page journaling\n6. Write tests for duplicate page handling\n**Checkpoint C4:**\n- [ ] Page written to journal before modification\n- [ ] Duplicate writes do not create duplicate journal entries\n- [ ] State transitions to DIRTY on first write\n### Phase 5: COMMIT with Write Ordering (2 hours)\n**Files:** `src/transaction/manager.rs`, `src/transaction/journal.rs`\n**Tasks:**\n1. Implement journal sync operation\n2. Implement database write phase\n3. Implement database sync operation\n4. Implement journal deletion\n5. Implement state transition to COMMITTING then AUTOCOMMIT\n6. Handle empty transaction (STARTED \u2192 AUTOCOMMIT directly)\n7. Write tests for successful commit\n8. Write tests for failure at each phase\n**Checkpoint C5:**\n- [ ] Write ordering preserved (journal\u2192db\u2192sync\u2192delete)\n- [ ] Empty transactions commit without journal\n- [ ] Journal deleted after successful commit\n### Phase 6: ROLLBACK Implementation (1.5 hours)\n**Files:** `src/transaction/manager.rs`, `src/transaction/journal.rs`\n**Tasks:**\n1. Implement journal reading for recovery\n2. Implement page restoration from journal\n3. Implement database truncation if needed\n4. Implement state transition through ROLLING_BACK\n5. Write tests for partial rollback\n6. Write tests for rollback after multiple page writes\n7. Write tests for rollback of database extension\n**Checkpoint C6:**\n- [ ] All journaled pages restored\n- [ ] Database size restored if extended\n- [ ] State returns to AUTOCOMMIT\n### Phase 7: Hot Journal Detection and Recovery (2 hours)\n**Files:** `src/transaction/recovery.rs`\n**Tasks:**\n1. Implement journal path derivation\n2. Implement hot journal existence check\n3. Implement journal validation (magic, checksums)\n4. Implement full recovery procedure\n5. Integrate recovery into database open sequence\n6. Write tests for crash simulation and recovery\n7. Write tests for corrupted journal handling\n8. Write tests for stale journal cleanup\n**Checkpoint C7:**\n- [ ] Hot journal detected at open\n- [ ] Valid journals recovered successfully\n- [ ] Corrupted journals handled safely\n---\n## Test Specification\n### Unit Tests\n| Test ID | Description | Assertions |\n|---------|-------------|------------|\n| UT-TXN-001 | State machine initial state | State is AUTOCOMMIT |\n| UT-TXN-002 | BEGIN transitions to STARTED | State is STARTED after begin() |\n| UT-TXN-003 | BEGIN when active fails | Error returned, state unchanged |\n| UT-TXN-004 | COMMIT without transaction fails | Error returned |\n| UT-TXN-005 | ROLLBACK without transaction fails | Error returned |\n| UT-TXN-006 | First write transitions to DIRTY | State is DIRTY after journal_page() |\n| UT-JRN-001 | Header serialization size | Exactly 32 bytes |\n| UT-JRN-002 | Magic number correct | 0xD9D50589 |\n| UT-JRN-003 | Checksum calculation | Known input produces expected output |\n| UT-JRN-004 | Page record size | 12 + page_size bytes |\n| UT-JRN-005 | Duplicate page not re-journaled | Page count unchanged |\n### Integration Tests\n| Test ID | Description | Setup | Assertions |\n|---------|-------------|-------|------------|\n| IT-TXN-001 | Full transaction cycle | Empty DB | BEGIN \u2192 Write \u2192 COMMIT succeeds |\n| IT-TXN-002 | Rollback restores data | DB with data | Data unchanged after rollback |\n| IT-TXN-003 | Multiple page journaling | Multi-page DB | All pages journaled and restored |\n| IT-TXN-004 | Empty transaction commit | Empty DB | STARTED \u2192 COMMIT succeeds |\n| IT-TXN-005 | Concurrent transaction prevention | None | Second BEGIN fails |\n| IT-RCV-001 | Hot journal recovery | Simulated crash | Data restored to pre-crash state |\n| IT-RCV-002 | Corrupted journal handling | Corrupted journal | Error returned, DB untouched |\n| IT-RCV-003 | Stale journal cleanup | Invalid magic | Journal deleted, open succeeds |\n### Crash Simulation Tests\n| Test ID | Crash Point | Expected Result |\n|---------|-------------|-----------------|\n| CT-001 | Before journal sync | Original data intact |\n| CT-002 | After journal sync, before DB write | Original data intact (recovery) |\n| CT-003 | After DB write, before DB sync | Original data intact (recovery) |\n| CT-004 | After DB sync, before journal delete | New data persisted |\n| CT-005 | During rollback | Partial rollback recovered |\n### Performance Tests\n| Test ID | Operation | Target | Measurement |\n|---------|-----------|--------|-------------|\n| PT-001 | BEGIN (deferred) | < 0.1ms | Time to state transition |\n| PT-002 | COMMIT (read-only) | < 1ms | Time for empty transaction |\n| PT-003 | COMMIT (1 page) | < 5ms | Full commit cycle |\n| PT-004 | Journal write per page | < 1ms | Write + checksum |\n| PT-005 | Recovery (100 pages) | < 100ms | Full recovery time |\n---\n## Performance Targets\n| Metric | Target | Measurement Method |\n|--------|--------|-------------------|\n| BEGIN latency (deferred) | < 0.1ms | Microbenchmark |\n| COMMIT latency (read-only) | < 1ms | Microbenchmark |\n| COMMIT latency (1 page) | < 5ms | Including fsync |\n| Journal write per page | < 1ms | Write + checksum |\n| Rollback per page | < 0.5ms | Read + write |\n| Crash recovery (1000 pages) | < 100ms | Full recovery |\n| Memory overhead | < 1KB + journaled page tracking | Per transaction |\n### fsync Assumptions\n- Modern SSD: fsync ~1-5ms\n- HDD: fsync ~10-30ms\n- Targets assume SSD-class storage\n---\n## Synced Criteria\n```json\n{\n  \"module_id\": \"build-sqlite-m9\",\n  \"version\": \"1.0.0\",\n  \"generated_at\": \"2026-02-26T00:00:00Z\",\n  \"criteria\": [\n    {\n      \"id\": \"CRIT-001\",\n      \"category\": \"functionality\",\n      \"description\": \"BEGIN statement creates transaction in STARTED state\",\n      \"validation\": \"Unit test verifies state after begin() call\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-002\",\n      \"category\": \"functionality\",\n      \"description\": \"COMMIT persists all changes durably with correct write ordering\",\n      \"validation\": \"Integration test with crash simulation at each phase\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-003\",\n      \"category\": \"functionality\",\n      \"description\": \"ROLLBACK restores database to pre-transaction state\",\n      \"validation\": \"Integration test comparing data before and after rollback\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-004\",\n      \"category\": \"durability\",\n      \"description\": \"Crash before journal sync leaves original data intact\",\n      \"validation\": \"Crash simulation test CT-001\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-005\",\n      \"category\": \"durability\",\n      \"description\": \"Crash after journal sync enables full recovery\",\n      \"validation\": \"Crash simulation tests CT-002 through CT-004\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-006\",\n      \"category\": \"functionality\",\n      \"description\": \"Hot journal detection at database open\",\n      \"validation\": \"Recovery test with simulated hot journal\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-007\",\n      \"category\": \"integrity\",\n      \"description\": \"Journal checksums detect corruption\",\n      \"validation\": \"Test with intentionally corrupted journal\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-008\",\n      \"category\": \"functionality\",\n      \"description\": \"Write-before-modify saves original page before any change\",\n      \"validation\": \"Verify journal contains pre-modification data\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-009\",\n      \"category\": \"performance\",\n      \"description\": \"BEGIN (deferred) completes in < 0.1ms\",\n      \"validation\": \"Microbenchmark PT-001\",\n      \"priority\": \"should\"\n    },\n    {\n      \"id\": \"CRIT-010\",\n      \"category\": \"performance\",\n      \"description\": \"COMMIT (read-only) completes in < 1ms\",\n      \"validation\": \"Microbenchmark PT-002\",\n      \"priority\": \"should\"\n    },\n    {\n      \"id\": \"CRIT-011\",\n      \"category\": \"performance\",\n      \"description\": \"Journal write per page < 1ms\",\n      \"validation\": \"Microbenchmark PT-004\",\n      \"priority\": \"should\"\n    },\n    {\n      \"id\": \"CRIT-012\",\n      \"category\": \"performance\",\n      \"description\": \"Crash recovery (1000 pages) < 100ms\",\n      \"validation\": \"Recovery performance test PT-005\",\n      \"priority\": \"should\"\n    },\n    {\n      \"id\": \"CRIT-013\",\n      \"category\": \"correctness\",\n      \"description\": \"Lazy journal creation - journal not created until first write\",\n      \"validation\": \"Verify journal file does not exist after BEGIN\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-014\",\n      \"category\": \"correctness\",\n      \"description\": \"Duplicate page journaling prevented\",\n      \"validation\": \"Journal contains each page at most once\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-015\",\n      \"category\": \"isolation\",\n      \"description\": \"Only one transaction active at a time\",\n      \"validation\": \"Second BEGIN returns error\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-016\",\n      \"category\": \"atomicity\",\n      \"description\": \"Transaction is all-or-nothing\",\n      \"validation\": \"After rollback, no partial changes visible\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-017\",\n      \"category\": \"format\",\n      \"description\": \"Journal header is exactly 32 bytes\",\n      \"validation\": \"Serialize and verify byte count\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-018\",\n      \"category\": \"format\",\n      \"description\": \"Journal magic number is 0xD9D50589\",\n      \"validation\": \"Byte-level verification of header\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-019\",\n      \"category\": \"recovery\",\n      \"description\": \"Invalid magic number treated as stale journal\",\n      \"validation\": \"Test with wrong magic, verify cleanup\",\n      \"priority\": \"must\"\n    },\n    {\n      \"id\": \"CRIT-020\",\n      \"category\": \"recovery\",\n      \"description\": \"Database truncation on rollback of extensions\",\n      \"validation\": \"Test transaction that adds pages, verify size restored\",\n      \"priority\": \"must\"\n    }\n  ],\n  \"diagrams\": [\n    \"tdd-diag-m9-1\",\n    \"tdd-diag-m9-2\",\n    \"tdd-diag-m9-3\",\n    \"tdd-diag-m9-4\",\n    \"tdd-diag-m9-5\",\n    \"tdd-diag-m9-6\"\n  ],\n  \"checkpoints\": [\n    \"C1: State machine tests pass\",\n    \"C2: Journal header format verified\",\n    \"C3: BEGIN and lazy journal creation working\",\n    \"C4: Write-before-modify implemented\",\n    \"C5: COMMIT with write ordering complete\",\n    \"C6: ROLLBACK fully implemented\",\n    \"C7: Hot journal recovery working\"\n  ],\n  \"acceptance_criteria\": [\n    \"All 20 criteria validated\",\n    \"All 7 checkpoints passed\",\n    \"All crash simulation tests pass\",\n    \"Performance within targets on SSD storage\"\n  ]\n}\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m10 -->\n# Technical Design Specification: build-sqlite-m10\n## Module Charter\n**Module ID:** build-sqlite-m10  \n**Title:** WAL Mode (Write-Ahead Logging)  \n**Version:** 1.0.0  \n**Status:** Draft  \n### Purpose\nImplements Write-Ahead Logging to enable concurrent readers during writes. Writers append modified pages to a WAL file rather than modifying the database directly. Readers access consistent snapshots without blocking writers. This inverts the rollback journal model (M9) from undo-based to redo-based recovery.\n### Scope\n- WAL file format with header, frames, and cumulative checksums\n- WAL index for O(1) page lookup by page_id\n- Snapshot isolation allowing multiple concurrent readers\n- Checkpoint process to merge WAL frames back to database\n- Crash recovery using WAL replay\n- Hot WAL detection and automatic recovery\n### Dependencies\n| Module | Dependency Type |\n|--------|-----------------|\n| build-sqlite-m04 | Buffer Pool Manager (page allocation) |\n| build-sqlite-m05 | B-tree Page Format (page serialization) |\n| build-sqlite-m09 | Transaction Manager (commit protocol) |\n---\n## File Structure\n```\nsrc/\n\u251c\u2500\u2500 wal/\n\u2502   \u251c\u2500\u2500 wal.h              [1] WAL manager interface\n\u2502   \u251c\u2500\u2500 wal.c              [2] WAL manager implementation\n\u2502   \u251c\u2500\u2500 wal_format.h       [3] WAL file format definitions\n\u2502   \u251c\u2500\u2500 wal_format.c       [4] WAL serialization/deserialization\n\u2502   \u251c\u2500\u2500 wal_index.h        [5] WAL index interface\n\u2502   \u251c\u2500\u2500 wal_index.c        [6] WAL index implementation\n\u2502   \u251c\u2500\u2500 wal_checkpoint.h   [7] Checkpoint interface\n\u2502   \u251c\u2500\u2500 wal_checkpoint.c   [8] Checkpoint implementation\n\u2502   \u2514\u2500\u2500 wal_recovery.h     [9] Recovery interface\n\u2502   \u2514\u2500\u2500 wal_recovery.c     [10] Recovery implementation\ntest/\n\u251c\u2500\u2500 test_wal_format.c      [11] Format serialization tests\n\u251c\u2500\u2500 test_wal_index.c       [12] Index lookup tests\n\u251c\u2500\u2500 test_wal_checkpoint.c  [13] Checkpoint tests\n\u251c\u2500\u2500 test_wal_concurrent.c  [14] Concurrency tests\n\u2514\u2500\u2500 test_wal_recovery.c    [15] Crash recovery tests\n```\n---\n## Data Model\n### WAL File Format\n#### WAL Header (32 bytes)\n```\nOffset  Size  Field           Description\n------  ----  -----           -----------\n0       4     magic           Magic number: 0x377F0682\n4       4     format_version  Format version: 3007000\n8       4     page_size       Database page size\n12      4     checkpoint_seq  Checkpoint sequence number\n16      4     salt1           Salt value 1 (random)\n20      4     salt2           Salt value 2 (random)\n24      4     checksum1       Header checksum part 1\n28      4     checksum2       Header checksum part 2\n```\n**Byte Order:** Big-endian for all multi-byte fields.\n**Magic Number Validation:**\n```c\n#define WAL_MAGIC        0x377F0682\n#define WAL_MAGIC_LE     0x82067F37  // Little-endian read of big-endian\n#define WAL_FORMAT_VERSION 3007000\n```\n**Salt Generation:**\n```c\nvoid wal_generate_salts(uint32_t *salt1, uint32_t *salt2) {\n    // Combine multiple entropy sources\n    struct timespec ts;\n    clock_gettime(CLOCK_REALTIME, &ts);\n    *salt1 = (uint32_t)ts.tv_sec ^ (uint32_t)ts.tv_nsec ^ (uint32_t)getpid();\n    *salt2 = (uint32_t)((ts.tv_sec >> 32) | (ts.tv_nsec << 1)) ^ (uint32_t)getppid();\n}\n```\n#### WAL Frame Header (24 bytes)\n```\nOffset  Size  Field           Description\n------  ----  -----           -----------\n0       4     page_number     Database page number (1-indexed)\n4       4     commit_size     Commit marker: db size in pages, 0 if not commit\n8       4     salt1           Must match WAL header salt1\n12      4     salt2           Must match WAL header salt2\n16      4     checksum1       Cumulative checksum part 1\n20      4     checksum2       Cumulative checksum part 2\n```\n**Frame Layout:**\n```\n+------------------+\n| Frame Header     |  24 bytes\n+------------------+\n| Page Data        |  page_size bytes\n+------------------+\n```\n**Total Frame Size:** `24 + page_size` bytes\n#### Checksum Algorithm\nCumulative checksum using SQLite's WAL checksum algorithm:\n```c\ntypedef struct wal_checksum {\n    uint32_t s0;\n    uint32_t s1;\n} wal_checksum_t;\nvoid wal_compute_checksum(\n    const uint8_t *data,      // Input data (must be 8-byte aligned)\n    size_t len,               // Length in bytes (must be multiple of 8)\n    wal_checksum_t *prev,     // Previous checksum values (IN/OUT)\n    bool native_byte_order    // true = native, false = big-endian\n) {\n    uint32_t s0 = prev->s0;\n    uint32_t s1 = prev->s1;\n    const uint32_t *ptr = (const uint32_t *)data;\n    size_t n_words = len / 4;\n    for (size_t i = 0; i < n_words; i += 2) {\n        uint32_t w0 = ptr[i];\n        uint32_t w1 = ptr[i + 1];\n        if (!native_byte_order) {\n            w0 = __builtin_bswap32(w0);\n            w1 = __builtin_bswap32(w1);\n        }\n        s0 += w0 + s1;\n        s1 += w1 + s0;\n    }\n    prev->s0 = s0;\n    prev->s1 = s1;\n}\n```\n**Initial Checksum Values:** s0 = 0, s1 = 0 (for header)  \n**Frame Checksum:** Continues from previous frame's final values\n### WAL Index Format\nThe WAL index resides in shared memory for process coordination.\n#### WAL Index Header (136 bytes)\n```\nOffset  Size  Field              Description\n------  ----  -----              -----------\n0       4     version            Index format version: 3007000\n4       4     unused             Reserved (must be 0)\n8       4     change_counter     Incremented on every modification\n12      4     is_init            Non-zero when initialized\n16      4     big_endian_checksum Checksum byte order flag\n20      4     salt1              Copy of WAL header salt1\n24      4     salt2              Copy of WAL header salt2\n28      4     max_frame          Highest valid frame number\n32      4     n_backfill         Frames already checkpointed\n36      4     read_lock[5]       Reader slots (max frame per reader)\n56      4     write_lock         Write lock (0=unlocked, 1=locked)\n60      4     ckpt_lock          Checkpoint lock\n64      4     recover_lock       Recovery lock\n68      4     read_mark[5]       Reader snapshot markers\n88      48    reserved           Future expansion\n```\n#### Hash Table Entry (8 bytes)\n```\nOffset  Size  Field              Description\n------  ----  -----              -----------\n0       4     page_number        Database page number (0 = empty)\n4       4     frame_number       WAL frame containing this page\n```\n**Hash Function:**\n```c\n#define WAL_INDEX_HASH_SIZE 4096\nuint32_t wal_hash_page(uint32_t page_number) {\n    // Simple multiplicative hash\n    return (page_number * 2654435761u) % WAL_INDEX_HASH_SIZE;\n}\n```\n**Collision Resolution:** Linear probing with wrap-around\n#### WAL Index Page Structure\n```\n+------------------------+\n| Index Header (136)     |\n+------------------------+\n| Hash Table (32768)     |  4096 entries \u00d7 8 bytes\n+------------------------+\n| Unused / Future        |\n+------------------------+\nTotal: 32904 bytes (rounded to system page size)\n```\n### Snapshot Structure\n```c\ntypedef struct wal_snapshot {\n    uint32_t max_frame;         // Highest frame visible to this snapshot\n    uint32_t salt1;             // Salt values for validation\n    uint32_t salt2;\n    uint32_t read_mark_index;   // Index in read_mark[] array\n    uint32_t change_counter;    // Snapshot of change counter\n} wal_snapshot_t;\n```\n### Checkpoint State Machine\n```\nenum checkpoint_state {\n    CKPT_NONE = 0,              // No checkpoint in progress\n    CKPT_PASSIVE = 1,           // Passive checkpoint (non-blocking)\n    CKPT_FULL = 2,              // Full checkpoint (wait for readers)\n    CKPT_RESTART = 3,           // Restart checkpoint (exclusive)\n    CKPT_TRUNCATE = 4           // Truncate checkpoint (exclusive + delete)\n};\ntypedef struct checkpoint_result {\n    uint32_t frames_moved;      // Number of frames checkpointed\n    uint32_t pages_written;     // Unique pages written to DB\n    int error_code;             // 0 on success\n    bool blocked;               // True if blocked by readers\n} checkpoint_result_t;\n```\n---\n## Interface Contracts\n### WAL Manager Interface\n```c\n// wal/wal.h\ntypedef struct wal_manager wal_manager_t;\n// Lifecycle\nwal_manager_t* wal_create(const char *db_path, uint32_t page_size);\nvoid wal_destroy(wal_manager_t *wal);\nint wal_open(wal_manager_t *wal, bool create_if_missing);\nint wal_close(wal_manager_t *wal);\n// Write Operations\nint wal_begin_write(wal_manager_t *wal);\nint wal_write_frame(wal_manager_t *wal, uint32_t page_num, \n                    const void *page_data);\nint wal_commit(wal_manager_t *wal, uint32_t db_size);\nint wal_rollback(wal_manager_t *wal);\n// Read Operations\nint wal_read_page(wal_manager_t *wal, uint32_t page_num, \n                  wal_snapshot_t *snap, void *page_data);\n// Snapshot Management\nint wal_begin_snapshot(wal_manager_t *wal, wal_snapshot_t *snap);\nint wal_end_snapshot(wal_manager_t *wal, wal_snapshot_t *snap);\n// Checkpoint\nint wal_checkpoint(wal_manager_t *wal, enum checkpoint_state mode,\n                   checkpoint_result_t *result);\n// Recovery\nint wal_recover(wal_manager_t *wal, const char *db_path);\nbool wal_exists(const char *db_path);\nbool wal_is_hot(const char *db_path);\n```\n### WAL Index Interface\n```c\n// wal/wal_index.h\ntypedef struct wal_index wal_index_t;\n// Lifecycle\nwal_index_t* wal_index_create(const char *db_path);\nvoid wal_index_destroy(wal_index_t *idx);\nint wal_index_open(wal_index_t *idx);\nint wal_index_close(wal_index_t *idx);\n// Mapping Operations\nint wal_index_add(wal_index_t *idx, uint32_t page_num, uint32_t frame_num);\nint wal_index_lookup(wal_index_t *idx, uint32_t page_num, \n                     uint32_t *frame_num);\nint wal_index_remove(wal_index_t *idx, uint32_t page_num);\n// Frame Range\nuint32_t wal_index_max_frame(wal_index_t *idx);\nint wal_index_set_max_frame(wal_index_t *idx, uint32_t frame);\nuint32_t wal_index_backfill_count(wal_index_t *idx);\n// Reader Coordination\nint wal_index_acquire_read_lock(wal_index_t *idx, uint32_t *slot, \n                                 uint32_t max_frame);\nint wal_index_release_read_lock(wal_index_t *idx, uint32_t slot);\nuint32_t wal_index_min_read_frame(wal_index_t *idx);\n// Checkpoint Coordination\nint wal_index_acquire_ckpt_lock(wal_index_t *idx);\nint wal_index_release_ckpt_lock(wal_index_t *idx);\nint wal_index_update_backfill(wal_index_t *idx, uint32_t count);\n```\n### Checkpoint Interface\n```c\n// wal/wal_checkpoint.h\ntypedef struct checkpoint_ctx checkpoint_ctx_t;\n// Checkpoint Modes\ntypedef enum {\n    CKPT_MODE_PASSIVE = 0,    // Best-effort, non-blocking\n    CKPT_MODE_FULL = 1,       // Wait for readers, checkpoint all\n    CKPT_MODE_RESTART = 2,    // Exclusive access, restart WAL\n    CKPT_MODE_TRUNCATE = 3    // Exclusive access, truncate WAL file\n} ckpt_mode_t;\n// Execution\nint checkpoint_execute(\n    wal_manager_t *wal,\n    ckpt_mode_t mode,\n    uint32_t min_frame,       // Minimum frame to checkpoint\n    checkpoint_result_t *result\n);\n// Callbacks\ntypedef void (*checkpoint_progress_cb)(\n    uint32_t frames_done,\n    uint32_t frames_total,\n    void *user_data\n);\nint checkpoint_set_progress_callback(\n    checkpoint_ctx_t *ctx,\n    checkpoint_progress_cb cb,\n    void *user_data\n);\n```\n### Recovery Interface\n```c\n// wal/wal_recovery.h\ntypedef struct recovery_ctx recovery_ctx_t;\ntypedef enum {\n    RECOVER_CONTINUE = 0,     // Continue processing frames\n    RECOVER_COMMIT = 1,       // Found commit marker, db is consistent\n    RECOVER_ABORT = 2         // Corrupt frame, stop recovery\n} recover_action_t;\ntypedef recover_action_t (*recover_frame_cb)(\n    uint32_t frame_num,\n    uint32_t page_num,\n    const void *page_data,\n    void *user_data\n);\n// Recovery Process\nint wal_recover_db(\n    const char *db_path,\n    const char *wal_path,\n    recover_frame_cb callback,\n    void *user_data\n);\n// Hot WAL Detection\nbool wal_detect_hot(const char *db_path);\nint wal_validate_header(const char *wal_path, uint32_t *page_size);\n```\n---\n## Algorithm Specifications\n### Algorithm 1: WAL Write Protocol\n```\nINPUT: page_num, page_data\nOUTPUT: frame_number on success, error code on failure\n1. ACQUIRE write_lock\n2. IF write_lock acquisition fails:\n   RETURN WAL_LOCK_FAILED\n3. frame_num = next_frame_number\n4. prev_checksum = last_frame_checksum\n5. \n6. // Build frame header\n7. frame_header.page_number = page_num\n8. frame_header.commit_size = 0  // Set during commit\n9. frame_header.salt1 = wal_header.salt1\n10. frame_header.salt2 = wal_header.salt2\n11.\n12. // Compute cumulative checksum\n13. checksum = prev_checksum\n14. wal_compute_checksum(frame_header, 24, &checksum, BIG_ENDIAN)\n15. wal_compute_checksum(page_data, page_size, &checksum, BIG_ENDIAN)\n16. frame_header.checksum1 = checksum.s0\n17. frame_header.checksum2 = checksum.s1\n18.\n19. // Write frame atomically\n20. SEEK wal_file TO frame_offset(frame_num)\n21. WRITE frame_header\n22. WRITE page_data\n23. SYNC wal_file  // fsync for durability\n24.\n25. // Update index\n26. wal_index_add(page_num, frame_num)\n27. next_frame_number++\n28. last_frame_checksum = checksum\n29.\n30. RELEASE write_lock\n31. RETURN frame_num\n```\n### Algorithm 2: WAL Read with Snapshot\n```\nINPUT: page_num, snapshot\nOUTPUT: page_data or NOT_FOUND\n1. // Validate snapshot\n2. IF snapshot.change_counter != index.change_counter:\n   RETURN SNAPSHOT_STALE\n3.\n4. // Check WAL index for recent version\n5. frame_num = wal_index_lookup(page_num)\n6. IF frame_num == 0 OR frame_num > snapshot.max_frame:\n   // Page not in WAL or too recent for snapshot\n7.   RETURN read_from_database(page_num)\n8.\n9. // Read from WAL frame\n10. frame_offset = WAL_HEADER_SIZE + (frame_num - 1) * frame_size\n11. SEEK wal_file TO frame_offset + FRAME_HEADER_SIZE\n12. READ page_data (page_size bytes)\n13.\n14. RETURN page_data\n```\n### Algorithm 3: Checkpoint Execution\n```\nINPUT: checkpoint_mode\nOUTPUT: checkpoint_result\n1. // Acquire checkpoint lock\n2. IF NOT acquire_ckpt_lock():\n   RETURN CHECKPOINT_BUSY\n3.\n4. // Determine safe frame range\n5. max_frame = wal_index_max_frame()\n6. min_reader_frame = wal_index_min_read_frame()\n7. safe_frame = min(max_frame, min_reader_frame - 1)\n8. backfill = wal_index_backfill_count()\n9.\n10. IF safe_frame <= backfill:\n11.   release_ckpt_lock()\n12.   RETURN {frames_moved: 0, blocked: false}  // Nothing to do\n13.\n14. // Handle blocking modes\n15. IF checkpoint_mode == CKPT_MODE_PASSIVE AND min_reader_frame <= backfill + 1:\n16.   release_ckpt_lock()\n17.   RETURN {frames_moved: 0, blocked: true}\n18.\n19. IF checkpoint_mode >= CKPT_MODE_FULL:\n20.   WAIT for all readers to advance past safe_frame\n21.\n22. // Build page map: latest frame for each page\n23. page_map = empty hash map\n24. FOR frame_num = backfill + 1 TO safe_frame:\n25.   page_num = read_frame_page_number(frame_num)\n26.   page_map[page_num] = frame_num  // Overwrites older versions\n27.\n28. // Write pages to database\n29. pages_written = 0\n30. FOR each (page_num, frame_num) IN page_map:\n31.   page_data = read_frame_data(frame_num)\n32.   db_write_page(db_file, page_num, page_data)\n33.   pages_written++\n34.\n35. // Sync database\n36. SYNC db_file\n37.\n38. // Update backfill counter\n39. wal_index_update_backfill(safe_frame)\n40.\n41. // Handle WAL reset for RESTART/TRUNCATE modes\n42. IF checkpoint_mode >= CKPT_MODE_RESTART AND safe_frame == max_frame:\n43.   reset_wal_file()\n44.   IF checkpoint_mode == CKPT_MODE_TRUNCATE:\n45.     TRUNCATE wal_file TO WAL_HEADER_SIZE\n46.\n47. release_ckpt_lock()\n48. RETURN {frames_moved: safe_frame - backfill, pages_written, blocked: false}\n```\n### Algorithm 4: Crash Recovery\n```\nINPUT: db_path, wal_path\nOUTPUT: recovery status\n1. // Check for hot WAL\n2. IF NOT file_exists(wal_path):\n3.   RETURN RECOVERY_NOT_NEEDED\n4.\n5. // Validate WAL header\n6. header = read_wal_header(wal_path)\n7. IF header.magic != WAL_MAGIC:\n8.   RETURN WAL_CORRUPTION\n9.\n10. // Verify salts match\n11. IF header.salt1 == 0 AND header.salt2 == 0:\n12.   // Uninitialized WAL, safe to delete\n13.   DELETE wal_path\n14.   RETURN RECOVERY_NOT_NEEDED\n15.\n16. // Scan frames to find last valid commit\n17. valid_commit_frame = 0\n18. prev_checksum = header_checksum(header)\n19. frame_num = 1\n20.\n21. WHILE TRUE:\n22.   frame_header = read_frame_header(frame_num)\n23.   \n24.   // Check for end of WAL\n25.   IF frame_header == READ_ERROR:\n26.     BREAK\n27.   \n28.   // Validate salts\n29.   IF frame_header.salt1 != header.salt1 OR \n30.      frame_header.salt2 != header.salt2:\n31.     BREAK  // End of valid frames\n32.   \n33.   // Verify cumulative checksum\n34.   expected_checksum = compute_frame_checksum(frame_num, prev_checksum)\n35.   IF frame_header.checksum1 != expected_checksum.s0 OR\n36.      frame_header.checksum2 != expected_checksum.s1:\n37.     RETURN WAL_CORRUPTION\n38.   \n39.   prev_checksum = expected_checksum\n40.   \n41.   // Track commit markers\n42.   IF frame_header.commit_size > 0:\n43.     valid_commit_frame = frame_num\n44.   \n45.   frame_num++\n46.\n47. // Replay committed frames\n48. IF valid_commit_frame > 0:\n49.   FOR i = 1 TO valid_commit_frame:\n50.     page_num = read_frame_page_number(i)\n51.     page_data = read_frame_data(i)\n52.     db_write_page(db_file, page_num, page_data)\n53.   \n54.   SYNC db_file\n55.\n56. // Truncate WAL\n57. TRUNCATE wal_path TO header_size\n58.\n59. RETURN RECOVERY_SUCCESS\n```\n### Algorithm 5: Snapshot Acquisition\n```\nINPUT: wal_manager\nOUTPUT: snapshot structure\n1. // Find available reader slot\n2. FOR slot = 0 TO 4:\n3.   IF read_lock[slot] == 0:\n4.     BREAK\n5.\n6. IF slot >= 5:\n7.   RETURN TOO_MANY_READERS\n8.\n9. // Capture consistent snapshot\n10. ATOMIC:\n11.   snapshot.max_frame = wal_index_max_frame()\n12.   snapshot.salt1 = wal_header.salt1\n13.   snapshot.salt2 = wal_header.salt2\n14.   snapshot.change_counter = index.change_counter\n15.   snapshot.read_mark_index = slot\n16.\n17. // Register reader\n18. read_lock[slot] = snapshot.max_frame\n19.\n20. RETURN snapshot\n```\n---\n## Diagram Specifications\n### Diagram 1: WAL File Layout\n**ID:** tdd-diag-m10-1\n```\nWAL File Physical Layout\n========================\n+------------------------+\n|     WAL Header         |  Offset: 0, Size: 32 bytes\n|   +----------------+   |\n|   | Magic 0x377... |   |  Bytes 0-3\n|   +----------------+   |\n|   | Format Version |   |  Bytes 4-7\n|   +----------------+   |\n|   | Page Size      |   |  Bytes 8-11\n|   +----------------+   |\n|   | Checkpoint Seq |   |  Bytes 12-15\n|   +----------------+   |\n|   | Salt1          |   |  Bytes 16-19\n|   +----------------+   |\n|   | Salt2          |   |  Bytes 20-23\n|   +----------------+   |\n|   | Checksum1      |   |  Bytes 24-27\n|   +----------------+   |\n|   | Checksum2      |   |  Bytes 28-31\n|   +----------------+   |\n+------------------------+\n|     Frame 1            |  Offset: 32\n|   +----------------+   |\n|   | Frame Header   |   |  24 bytes\n|   | - Page Num     |   |\n|   | - Commit Size  |   |\n|   | - Salts        |   |\n|   | - Checksums    |   |\n|   +----------------+   |\n|   | Page Data      |   |  page_size bytes\n|   +----------------+   |\n+------------------------+\n|     Frame 2            |  Offset: 32 + frame_size\n|        ...             |\n+------------------------+\n|     Frame N            |\n+------------------------+\nFrame Size = 24 + page_size\nFrame N Offset = 32 + (N-1) * Frame Size\n```\n### Diagram 2: WAL Index Structure\n**ID:** tdd-diag-m10-2\n```\nWAL Index Shared Memory Layout\n==============================\n+------------------------------------------+\n|          WAL Index Header (136 bytes)     |\n| +--------------------------------------+ |\n| | Version (4)                          | |\n| +--------------------------------------+ |\n| | Change Counter (4)                   | |\n| +--------------------------------------+ |\n| | Max Frame (4)                        | |\n| +--------------------------------------+ |\n| | Backfill Count (4)                   | |\n| +--------------------------------------+ |\n| | Salt1, Salt2 (8)                     | |\n| +--------------------------------------+ |\n| | Read Locks [5] (20)                  | |\n| +--------------------------------------+ |\n| | Read Marks [5] (20)                  | |\n| +--------------------------------------+ |\n| | Write Lock (4)                       | |\n| +--------------------------------------+ |\n| | Checkpoint Lock (4)                  | |\n| +--------------------------------------+ |\n| | Reserved (64)                        | |\n| +--------------------------------------+ |\n+------------------------------------------+\n|          Hash Table (32768 bytes)         |\n| +--------------------------------------+ |\n| | Entry 0: page=0, frame=0 (empty)     | |\n| +--------------------------------------+ |\n| | Entry 1: page=5, frame=3             | |\n| +--------------------------------------+ |\n| | Entry 2: page=0, frame=0 (empty)     | |\n| +--------------------------------------+ |\n| | Entry K: page=P, frame=F             | |\n| +--------------------------------------+ |\n| | ... 4096 entries total               | |\n| +--------------------------------------+ |\n+------------------------------------------+\nLookup: hash(page_num) -> probe until match or empty\n```\n### Diagram 3: Snapshot Isolation Flow\n**ID:** tdd-diag-m10-3\n```\nSnapshot Isolation for Concurrent Reads\n========================================\nTimeline:\n---------\nFrame:  1     2     3     4     5     6     7     8\n        |     |     |     |     |     |     |     |\nWriter: [Write Pg1] [Write Pg2] [Commit] [Write Pg3] [Commit]\n        |           |                |                |\n        v           v                v                v\n        +-----------+----------------+----------------+\n             |            |                |\nReader A:    |  snapshot  |                |\n             |  max=2     |                |\n             |  sees:     |                |\n             |  - Pg1@F1  |                |\n             |  - Pg2@F2  |                |\n             +------------+----------------+\n                               |\nReader B:                      |  snapshot\n                               |  max=5\n                               |  sees:\n                               |  - Pg1@F1\n                               |  - Pg2@F2\n                               |  - Pg3@F4\n                               +------------+\nRules:\n- Reader can only see frames <= max_frame at snapshot time\n- Multiple versions of same page may exist in WAL\n- Reader always gets latest version visible to its snapshot\n- Writer never blocks readers (append-only)\n```\n### Diagram 4: Checkpoint Process\n**ID:** tdd-diag-m10-4\n```\nCheckpoint State Machine\n========================\n                    +----------------+\n                    |   CKPT_NONE    |\n                    +-------+--------+\n                            |\n           +----------------+----------------+\n           |                                 |\n           v                                 v\n    +-------------+                  +---------------+\n    |  PASSIVE    |                  |    FULL       |\n    | (non-block) |                  | (wait readers)|\n    +------+------+                  +-------+-------+\n           |                                 |\n           | blocked by reader               | all readers done\n           v                                 v\n    +-------------+                  +---------------+\n    |   ABORT     |                  |   RESTART     |\n    | (try later) |                  | (exclusive)   |\n    +-------------+                  +-------+-------+\n                                            |\n                                            | reset WAL\n                                            v\n                                     +---------------+\n                                     |   TRUNCATE    |\n                                     | (delete WAL)  |\n                                     +-------+-------+\n                                             |\n                                             v\n                                     +---------------+\n                                     |   COMPLETE    |\n                                     +---------------+\nCheckpoint Sequence:\n1. Acquire ckpt_lock\n2. Find min_reader_frame\n3. Determine safe_frame = min(max_frame, min_reader_frame - 1)\n4. Copy frames [backfill+1, safe_frame] to DB\n5. Update backfill = safe_frame\n6. Release ckpt_lock\n```\n### Diagram 5: Checksum Chain\n**ID:** tdd-diag-m10-5\n```\nCumulative Checksum Chain\n=========================\nWAL Header:\n+------------------+\n| checksum1 = C0   |  <-- Initial: s0=0, s1=0\n| checksum2 = C1   |      compute(header bytes 0-23)\n+------------------+\n         |\n         | feeds into\n         v\nFrame 1:\n+------------------+\n| checksum1 = D0   |  <-- prev = {C0, C1}\n| checksum2 = D1   |      compute(frame1 header + page)\n+------------------+\n         |\n         | feeds into\n         v\nFrame 2:\n+------------------+\n| checksum1 = E0   |  <-- prev = {D0, D1}\n| checksum2 = E1   |      compute(frame2 header + page)\n+------------------+\n         |\n         | feeds into\n         v\nFrame N:\n+------------------+\n| checksum1 = Z0   |  <-- Each frame's checksum\n| checksum2 = Z1   |      depends on ALL previous data\n+------------------+\nValidation:\n- Break in chain = corruption detected\n- Cannot modify any frame without invalidating all subsequent frames\n- Provides end-to-end integrity\n```\n### Diagram 6: Recovery Flow\n**ID:** tdd-diag-m10-6\n```\nCrash Recovery Process\n======================\n+-------------------+\n| Database Open     |\n+--------+----------+\n         |\n         v\n+--------+----------+\n| WAL file exists?  |\n+--------+----------+\n         |\n    +----+----+\n    |         |\n   No        Yes\n    |         |\n    v         v\n+--------+ +--------+\n| Normal | | Hot WAL|\n| Start  | | Detect |\n+--------+ +----+---+\n                |\n                v\n         +------+------+\n         | Validate    |\n         | WAL Header  |\n         +------+------+\n                |\n         +------+------+------+\n         |             |      |\n      Valid       Invalid   Empty\n         |             |      |\n         v             v      v\n   +-----+----+  +-----+----+ +--+\n   | Scan     |  | WAL      | |Del|\n   | Frames   |  | CORRUPT  | |ete|\n   +-----+----+  +----------+ +--+\n         |\n         v\n   +-----+----+\n   | Find Last|\n   | Commit   |\n   +-----+----+\n         |\n         v\n   +-----+----+\n   | Replay   |\n   | Committed|\n   | Frames   |\n   +-----+----+\n         |\n         v\n   +-----+----+\n   | Sync DB  |\n   +-----+----+\n         |\n         v\n   +-----+----+\n   | Truncate |\n   | WAL      |\n   +-----+----+\n         |\n         v\n   +-----+----+\n   | Normal   |\n   | Operation|\n   +----------+\n```\n### Diagram 7: Concurrency Model\n**ID:** tdd-diag-m10-7\n```\nWAL Concurrency Model\n=====================\nProcess View:\n-------------\n        +-------------+\n        |   Writer    |\n        +------+------+       Append-only\n               |              to WAL\n               v\n        +-------------+\n        |  WAL File   |<-----------+\n        +------+------+            |\n               ^                   |\n               |                   |\n        +------+------+     +------+------+\n        |  WAL Index  |---->|  Readers    |\n        | (Shared Mem)|     | (up to 5)   |\n        +------+------+     +-------------+\n               |                   |\n               v                   v\n        +-------------+     +-------------+\n        |   Database  |---->| Snapshots   |\n        +-------------+     +-------------+\nLock Hierarchy:\n--------------\n    +------------------+\n    | recover_lock     |  Recovery exclusive\n    +--------+---------+\n             |\n    +--------v---------+\n    | write_lock       |  Single writer\n    +--------+---------+\n             |\n    +--------v---------+\n    | ckpt_lock        |  Checkpoint exclusive\n    +--------+---------+\n             |\n    +--------v---------+\n    | read_lock[0-4]   |  5 concurrent readers\n    +------------------+\nBlocking Matrix:\n---------------\n             Writer  Reader  Checkpoint  Recovery\nWriter        X        -         -          -\nReader        -        -         -          -\nCheckpoint    -        X*        X          -\nRecovery      X        X         X          X\nX = blocks   - = does not block\n* = PASSIVE mode blocked, FULL/RESTART/TRUNCATE wait\nReaders never block writers!\n```\n---\n## Error Handling Matrix\n| Error Code | Value | Category | Description | Recovery Action |\n|------------|-------|----------|-------------|-----------------|\n| WAL_OK | 0 | Success | Operation completed | N/A |\n| WAL_CORRUPTION | 1001 | Critical | Checksum mismatch, invalid magic | Delete WAL, recover from backup |\n| WAL_WRITE_FAILED | 1002 | I/O | Cannot write to WAL file | Retry with backoff, check disk |\n| WAL_READ_FAILED | 1003 | I/O | Cannot read WAL file | Retry once, then fail |\n| CHECKPOINT_FAILED | 1004 | I/O | Checkpoint write failed | Retry checkpoint |\n| CHECKPOINT_BUSY | 1005 | Concurrency | Checkpoint blocked by readers | Retry with FULL mode |\n| SNAPSHOT_TOO_OLD | 1006 | Concurrency | Snapshot no longer valid | Acquire new snapshot |\n| WAL_LOCK_FAILED | 1007 | Concurrency | Cannot acquire lock | Wait and retry |\n| WAL_NOT_OPEN | 1008 | State | WAL not initialized | Open WAL first |\n| WAL_ALREADY_OPEN | 1009 | State | WAL already open | Close first |\n| WAL_RECOVERY_NEEDED | 1010 | State | Hot WAL detected | Run recovery |\n| TOO_MANY_READERS | 1011 | Concurrency | All reader slots full | Wait for slot |\n| WAL_INDEX_CORRUPT | 1012 | Critical | Index invariant violated | Rebuild index |\n### Error Propagation\n```c\ntypedef struct wal_error {\n    int code;\n    const char *message;\n    const char *file;\n    int line;\n    uint64_t timestamp;\n} wal_error_t;\n#define WAL_SET_ERROR(ctx, code, msg) \\\n    do { \\\n        (ctx)->error.code = (code); \\\n        (ctx)->error.message = (msg); \\\n        (ctx)->error.file = __FILE__; \\\n        (ctx)->error.line = __LINE__; \\\n        (ctx)->error.timestamp = get_timestamp_ms(); \\\n    } while(0)\n```\n---\n## Implementation Sequence\n### Phase 1: WAL File Format (8 hours)\n**Checkpoint:** Parse existing WAL files, verify checksums\n**Tasks:**\n1. Define `wal_format.h` structures with exact byte layouts\n2. Implement `wal_read_header()` and `wal_write_header()`\n3. Implement `wal_read_frame_header()` and `wal_write_frame_header()`\n4. Implement `wal_compute_checksum()` with cumulative chaining\n5. Add byte-order handling for cross-platform compatibility\n6. Write unit tests for checksum algorithm\n7. Write unit tests for header/frame serialization\n**Validation:**\n```c\n// Test: Can read SQLite-generated WAL\nassert(wal_validate_header(\"test_sqlite.wal\", &page_size) == WAL_OK);\n// Test: Checksum chain is correct\nwal_checksum_t c = {0, 0};\nwal_compute_checksum(header_bytes, 24, &c, false);\nassert(c.s0 == expected_s0 && c.s1 == expected_s1);\n```\n### Phase 2: WAL Index (10 hours)\n**Checkpoint:** Hash table lookups for page\u2192frame mapping\n**Tasks:**\n1. Define `wal_index.h` structures for shared memory\n2. Implement `wal_index_create()` with shared memory allocation\n3. Implement hash function and linear probing\n4. Implement `wal_index_add()` with collision handling\n5. Implement `wal_index_lookup()` with O(1) average case\n6. Implement reader slot management\n7. Write concurrency tests for index operations\n**Validation:**\n```c\n// Test: Add and lookup\nwal_index_add(idx, 5, 3);\nwal_index_add(idx, 5, 7);  // Update to newer frame\nuint32_t frame;\nassert(wal_index_lookup(idx, 5, &frame) == WAL_OK);\nassert(frame == 7);\n```\n### Phase 3: Snapshot Management (6 hours)\n**Checkpoint:** Multiple concurrent snapshots, isolation verification\n**Tasks:**\n1. Implement `wal_begin_snapshot()` with slot allocation\n2. Implement `wal_end_snapshot()` with slot release\n3. Implement snapshot validation against change counter\n4. Add min_read_frame tracking for checkpoint coordination\n5. Write tests for snapshot isolation\n6. Write tests for snapshot expiration\n**Validation:**\n```c\n// Test: Snapshot isolation\nwal_snapshot_t snap1, snap2;\nwal_begin_snapshot(wal, &snap1);\n// Writer commits more frames...\nwal_begin_snapshot(wal, &snap2);\nassert(snap2.max_frame > snap1.max_frame);\n// Reader with snap1 should NOT see new frames\n```\n### Phase 4: WAL Write Path (8 hours)\n**Checkpoint:** Durable writes, correct checksum chain\n**Tasks:**\n1. Implement `wal_begin_write()` with lock acquisition\n2. Implement `wal_write_frame()` with frame construction\n3. Implement checksum computation for frames\n4. Implement `wal_commit()` with commit marker\n5. Implement `wal_rollback()` with WAL truncation\n6. Add fsync for durability\n7. Write tests for write/commit/rollback\n**Validation:**\n```c\n// Test: Commit creates valid commit marker\nwal_begin_write(wal);\nwal_write_frame(wal, 1, page1);\nwal_write_frame(wal, 2, page2);\nwal_commit(wal, 2);\n// Verify frame 2 has commit_size = 2\n```\n### Phase 5: WAL Read Path (6 hours)\n**Checkpoint:** Read from WAL with snapshot isolation\n**Tasks:**\n1. Implement `wal_read_page()` with index lookup\n2. Add fallback to database file when page not in WAL\n3. Implement snapshot filtering (max_frame check)\n4. Add checksum validation during read\n5. Write tests for read path\n6. Write tests for concurrent reads\n**Validation:**\n```c\n// Test: Read correct version\nwal_write_frame(wal, 1, page_v1);\nwal_commit(wal, 1);\nsnap1 = get_snapshot();\nwal_write_frame(wal, 1, page_v2);\nwal_commit(wal, 1);\n// Read with snap1 should get page_v1\nwal_read_page(wal, 1, &snap1, buffer);\nassert(memcmp(buffer, page_v1, page_size) == 0);\n```\n### Phase 6: Checkpoint Implementation (10 hours)\n**Checkpoint:** Checkpoint all modes, verify DB consistency\n**Tasks:**\n1. Implement `checkpoint_execute()` with mode selection\n2. Implement PASSIVE mode with non-blocking semantics\n3. Implement FULL mode with reader waiting\n4. Implement RESTART mode with WAL reset\n5. Implement TRUNCATE mode with file truncation\n6. Add progress callback support\n7. Write tests for each checkpoint mode\n**Validation:**\n```c\n// Test: Checkpoint moves frames to DB\nwrite_multiple_frames(wal, 10);\ncheckpoint_result_t result;\nwal_checkpoint(wal, CKPT_MODE_FULL, &result);\nassert(result.frames_moved == 10);\nassert(result.blocked == false);\n// Verify pages in DB file match WAL\n```\n### Phase 7: Crash Recovery (8 hours)\n**Checkpoint:** Recover from simulated crashes, verify consistency\n**Tasks:**\n1. Implement `wal_detect_hot()` for hot WAL detection\n2. Implement `wal_validate_header()` with magic/salt check\n3. Implement frame scanning to find last commit\n4. Implement frame replay to database\n5. Add database sync after recovery\n6. Add WAL truncation after recovery\n7. Write crash simulation tests\n**Validation:**\n```c\n// Test: Recover from crash mid-commit\nwrite_frames_with_partial_commit(wal);\ncrash_simulation();  // Close without proper shutdown\nwal_recover_db(db_path, wal_path, NULL, NULL);\n// Verify only committed frames are in DB\n```\n### Phase 8: Integration Testing (6 hours)\n**Checkpoint:** End-to-end WAL mode with concurrent operations\n**Tasks:**\n1. Integrate WAL manager with buffer pool (M04)\n2. Integrate with transaction manager (M09)\n3. Write concurrent read/write stress tests\n4. Write crash recovery stress tests\n5. Write checkpoint while active tests\n6. Performance benchmarking\n7. Document API usage\n**Validation:**\n```c\n// Test: Concurrent operations\nspawn_reader_threads(5);\nspawn_writer_thread(1000);  // 1000 transactions\nwait_all_threads();\nverify_database_consistency();\n// Test: Checkpoint during reads\nacquire_snapshots(5);\ncheckpoint(CKPT_MODE_PASSIVE);  // Should not block\nverify_snapshots_still_valid();\n```\n---\n## Test Specification\n### Unit Tests\n| Test ID | Description | Pass Criteria |\n|---------|-------------|---------------|\n| UT-WAL-001 | Header serialization | Round-trip identical |\n| UT-WAL-002 | Frame header serialization | Round-trip identical |\n| UT-WAL-003 | Checksum computation | Match SQLite values |\n| UT-WAL-004 | Cumulative checksum chain | Detect mid-chain modification |\n| UT-WAL-005 | Index hash function | Uniform distribution |\n| UT-WAL-006 | Index collision handling | Linear probing works |\n| UT-WAL-007 | Snapshot creation | Correct max_frame captured |\n| UT-WAL-008 | Snapshot expiration | SNAPSHOT_TOO_OLD raised |\n### Integration Tests\n| Test ID | Description | Pass Criteria |\n|---------|-------------|---------------|\n| IT-WAL-001 | Write single frame | Frame readable, checksum valid |\n| IT-WAL-002 | Write multiple frames | All frames readable in order |\n| IT-WAL-003 | Commit marker | commit_size set correctly |\n| IT-WAL-004 | Rollback | WAL truncated to pre-transaction |\n| IT-WAL-005 | Concurrent readers | 5 readers, no blocking |\n| IT-WAL-006 | Reader during write | Reader sees old snapshot |\n| IT-WAL-007 | PASSIVE checkpoint | Non-blocking when possible |\n| IT-WAL-008 | FULL checkpoint | Waits for readers |\n| IT-WAL-009 | TRUNCATE checkpoint | WAL file reset to header |\n| IT-WAL-010 | Recovery from crash | Committed data preserved |\n### Stress Tests\n| Test ID | Description | Pass Criteria |\n|---------|-------------|---------------|\n| ST-WAL-001 | 10000 transactions | All committed data readable |\n| ST-WAL-002 | 100 concurrent readers | No deadlocks, all complete |\n| ST-WAL-003 | Continuous checkpoint | No data loss |\n| ST-WAL-004 | Random crash injection | Recovery succeeds |\n### Performance Tests\n| Test ID | Description | Target |\n|---------|-------------|--------|\n| PT-WAL-001 | WAL write latency | P99 < 5ms |\n| PT-WAL-002 | WAL read latency | P99 < 1ms |\n| PT-WAL-003 | Checkpoint 1000 frames | < 100ms |\n| PT-WAL-004 | Concurrent read throughput | 50k+ reads/sec |\n---\n## Performance Targets\n| Metric | Target | Measurement Method |\n|--------|--------|-------------------|\n| WAL write latency (P99) | < 5ms | Time from write_frame call to return |\n| WAL read latency (P99) | < 1ms | Time from read_page call to data available |\n| Checkpoint 1000 frames | < 100ms | Time to complete checkpoint |\n| Concurrent readers | 5 simultaneous | Supported without blocking |\n| Read throughput | 50k+ reads/sec | With 5 concurrent readers |\n| Write throughput | 10k+ writes/sec | Single writer |\n| Recovery time (1000 frames) | < 500ms | Time to replay and sync |\n---\n## Appendix A: SQLite Compatibility Notes\n### WAL File Compatibility\nThis implementation produces WAL files compatible with SQLite 3.7.0+:\n- Same magic number: 0x377F0682\n- Same checksum algorithm\n- Same frame layout\n- Can read SQLite-generated WAL files\n- SQLite can read our WAL files\n### Index Compatibility\nThe WAL index uses platform-specific shared memory:\n- Unix: mmap'd file in /tmp or /dev/shm\n- Windows: Named shared memory object\n- Index format is internal; not required to match SQLite\n### Checkpoint Modes\nImplements SQLite's four checkpoint modes:\n1. `PASSIVE` - Best effort, don't block\n2. `FULL` - Wait for readers, complete checkpoint\n3. `RESTART` - Like FULL, but restart WAL\n4. `TRUNCATE` - Like RESTART, but truncate file\n---\n## Appendix B: Migration from Rollback Journal\nWhen switching from rollback journal (M09) to WAL mode:\n1. Close all connections\n2. Run `PRAGMA journal_mode=WAL`\n3. This triggers:\n   - Checkpoint any existing journal\n   - Create empty WAL file with header\n   - Create WAL index in shared memory\n4. Subsequent transactions use WAL\nRollback journal can be re-enabled with `PRAGMA journal_mode=DELETE`.\n---\n```json\n{\n  \"module_id\": \"build-sqlite-m10\",\n  \"version\": \"1.0.0\",\n  \"status\": \"ready_for_implementation\",\n  \"estimated_hours\": 62,\n  \"dependencies\": [\"build-sqlite-m04\", \"build-sqlite-m05\", \"build-sqlite-m09\"],\n  \"artifacts\": [\n    \"src/wal/wal.h\",\n    \"src/wal/wal.c\",\n    \"src/wal/wal_format.h\",\n    \"src/wal/wal_format.c\",\n    \"src/wal/wal_index.h\",\n    \"src/wal/wal_index.c\",\n    \"src/wal/wal_checkpoint.h\",\n    \"src/wal/wal_checkpoint.c\",\n    \"src/wal/wal_recovery.h\",\n    \"src/wal/wal_recovery.c\"\n  ],\n  \"test_artifacts\": [\n    \"test/test_wal_format.c\",\n    \"test/test_wal_index.c\",\n    \"test/test_wal_checkpoint.c\",\n    \"test/test_wal_concurrent.c\",\n    \"test/test_wal_recovery.c\"\n  ],\n  \"diagrams\": [\n    \"tdd-diag-m10-1\",\n    \"tdd-diag-m10-2\",\n    \"tdd-diag-m10-3\",\n    \"tdd-diag-m10-4\",\n    \"tdd-diag-m10-5\",\n    \"tdd-diag-m10-6\",\n    \"tdd-diag-m10-7\"\n  ],\n  \"error_codes\": {\n    \"WAL_OK\": 0,\n    \"WAL_CORRUPTION\": 1001,\n    \"WAL_WRITE_FAILED\": 1002,\n    \"WAL_READ_FAILED\": 1003,\n    \"CHECKPOINT_FAILED\": 1004,\n    \"CHECKPOINT_BUSY\": 1005,\n    \"SNAPSHOT_TOO_OLD\": 1006,\n    \"WAL_LOCK_FAILED\": 1007,\n    \"WAL_NOT_OPEN\": 1008,\n    \"WAL_ALREADY_OPEN\": 1009,\n    \"WAL_RECOVERY_NEEDED\": 1010,\n    \"TOO_MANY_READERS\": 1011,\n    \"WAL_INDEX_CORRUPT\": 1012\n  },\n  \"performance_targets\": {\n    \"write_latency_p99_ms\": 5,\n    \"read_latency_p99_ms\": 1,\n    \"checkpoint_1000_frames_ms\": 100,\n    \"concurrent_readers\": 5,\n    \"read_throughput_per_sec\": 50000,\n    \"write_throughput_per_sec\": 10000,\n    \"recovery_1000_frames_ms\": 500\n  },\n  \"phases\": [\n    {\"id\": 1, \"name\": \"WAL File Format\", \"hours\": 8},\n    {\"id\": 2, \"name\": \"WAL Index\", \"hours\": 10},\n    {\"id\": 3, \"name\": \"Snapshot Management\", \"hours\": 6},\n    {\"id\": 4, \"name\": \"WAL Write Path\", \"hours\": 8},\n    {\"id\": 5, \"name\": \"WAL Read Path\", \"hours\": 6},\n    {\"id\": 6, \"name\": \"Checkpoint Implementation\", \"hours\": 10},\n    {\"id\": 7, \"name\": \"Crash Recovery\", \"hours\": 8},\n    {\"id\": 8, \"name\": \"Integration Testing\", \"hours\": 6}\n  ]\n}\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m11 -->\n# Technical Design Specification: build-sqlite-m11\n## Aggregate Functions & JOIN\n---\n## 1. Module Charter\nThis module implements aggregate functions (COUNT, SUM, AVG, MIN, MAX) with correct SQL NULL handling semantics, GROUP BY clause processing using hash-based aggregation, HAVING clause for post-aggregation filtering, and INNER JOIN using nested loop algorithm with optional index optimization. Aggregate functions maintain running state accumulators that process rows one-at-a-time, supporting both scalar aggregates (no GROUP BY) and grouped aggregates. GROUP BY uses a hash map from group keys to aggregate states, enabling O(N) aggregation complexity. INNER JOIN implements a nested loop join where the outer loop iterates over the left table and the inner loop searches for matching rows in the right table, with optional B-tree index acceleration when join columns are indexed. Predicate pushdown allows WHERE conditions to be applied before joining when semantically valid. The module handles three-valued logic for NULL comparisons in joins and ensures NULL values are excluded from aggregate computations per SQL standard.\n---\n## 2. File Structure\n```\nsrc/execution/\n\u251c\u2500\u2500 aggregate.h              # 1 - Aggregate function definitions\n\u251c\u2500\u2500 aggregate.c              # 2 - Aggregate state management\n\u251c\u2500\u2500 group_by.h               # 3 - GROUP BY hash aggregation\n\u251c\u2500\u2500 group_by.c               # 4 - GROUP BY implementation\n\u251c\u2500\u2500 join.h                   # 5 - JOIN definitions\n\u251c\u2500\u2500 join.c                   # 6 - Nested loop join implementation\n\u2514\u2500\u2500 having.h                 # 7 - HAVING clause evaluation\n\u2514\u2500\u2500 having.c                 # 8 - HAVING implementation\ntests/\n\u251c\u2500\u2500 test_aggregate.c         # 9 - Aggregate function tests\n\u251c\u2500\u2500 test_group_by.c          # 10 - GROUP BY tests\n\u251c\u2500\u2500 test_join.c              # 11 - JOIN tests\n\u2514\u2500\u2500 test_integration_m11.c   # 12 - Integration tests\n```\n---\n## 3. Data Model\n### 3.1 Aggregate Function Types\n```c\n/* Aggregate function identifiers - 1 byte each */\ntypedef enum {\n    AGG_COUNT      = 0x01,   /* COUNT(*) or COUNT(expr) */\n    AGG_COUNT_STAR = 0x02,   /* COUNT(*) - counts all rows */\n    AGG_SUM        = 0x03,   /* SUM(expr) - numeric only */\n    AGG_AVG        = 0x04,   /* AVG(expr) - numeric only */\n    AGG_MIN        = 0x05,   /* MIN(expr) - any comparable */\n    AGG_MAX        = 0x06,   /* MAX(expr) - any comparable */\n} AggFunc;\n```\n### 3.2 Aggregate State Structure\n```c\n/*\n * Aggregate State - 64 bytes per aggregate\n * Maintains running accumulator for each aggregate function\n */\ntypedef struct {\n    /* Offset 0: Function type (1 byte) */\n    AggFunc func_type;\n    /* Offset 1: Result datatype (1 byte) */\n    DataType result_type;\n    /* Offset 2: Flags (1 byte) */\n    uint8_t flags;\n    #define AGG_FLAG_HAS_VALUE  0x01  /* At least one non-NULL seen */\n    #define AGG_FLAG_IS_STAR    0x02  /* COUNT(*) variant */\n    /* Offset 3: Padding (1 byte) */\n    uint8_t _pad1;\n    /* Offset 4: Count value (8 bytes) */\n    int64_t count;\n    /* Offset 12: Sum value (16 bytes - doubles for precision) */\n    double sum_value;\n    /* Offset 24: Min/Max value storage (32 bytes) */\n    /* Stores the actual value for MIN/MAX comparison */\n    struct {\n        DataType type;       /* Offset 24: 1 byte */\n        uint8_t is_null;     /* Offset 25: 1 byte */\n        uint8_t _pad2[6];    /* Offset 26-31: padding */\n        union {\n            int64_t i64;     /* Offset 32: 8 bytes */\n            double f64;      /* Offset 32: 8 bytes */\n            struct {\n                uint32_t len;  /* Offset 32: 4 bytes */\n                char data[24]; /* Offset 36: 24 bytes max inline */\n            } str;\n        } v;\n    } minmax;\n    /* Total: 64 bytes */\n} AggregateState;\n```\n### 3.3 Group Key Structure\n```c\n/*\n * Group Key - Variable length, header + data\n * Used as hash map key for GROUP BY\n */\ntypedef struct {\n    /* Offset 0: Number of columns in key (2 bytes) */\n    uint16_t column_count;\n    /* Offset 2: Total key length including header (4 bytes) */\n    uint32_t key_length;\n    /* Offset 6: Hash value for fast comparison (8 bytes) */\n    uint64_t hash_value;\n    /* Offset 14: Column type array (column_count bytes) */\n    /* DataType types[column_count]; */\n    /* Variable: Column data, each prefixed with 2-byte length */\n    /* For NULL: length = 0xFFFF */\n} GroupKey;\n/*\n * Group Key Entry Layout:\n * | header (14B) | types (N bytes) | data (variable) |\n * \n * Each column value in data section:\n * | len (2B) | value (len bytes) |\n * \n * NULL encoding: len = 0xFFFF, no value bytes\n */\n```\n### 3.4 Group Hash Table Entry\n```c\n/*\n * Hash Table Entry for GROUP BY - 40 bytes header + variable\n */\ntypedef struct GroupEntry {\n    /* Offset 0: Hash value for bucket lookup (8 bytes) */\n    uint64_t hash;\n    /* Offset 8: Pointer to group key (8 bytes) */\n    GroupKey *key;\n    /* Offset 16: Array of aggregate states (8 bytes) */\n    AggregateState *agg_states;\n    /* Offset 24: Number of aggregates (2 bytes) */\n    uint16_t agg_count;\n    /* Offset 26: First row flag (1 byte) */\n    uint8_t is_first_row;\n    /* Offset 27: Padding (5 bytes) */\n    uint8_t _pad[5];\n    /* Offset 32: Next entry in bucket chain (8 bytes) */\n    struct GroupEntry *next;\n    /* Total header: 40 bytes */\n} GroupEntry;\n/*\n * Group Hash Table:\n * | bucket_count (4B) | entry_count (4B) | buckets[] (8B each) |\n */\ntypedef struct {\n    uint32_t bucket_count;\n    uint32_t entry_count;\n    GroupEntry **buckets;  /* Array of bucket heads */\n} GroupHashTable;\n```\n### 3.5 Join State Structure\n```c\n/*\n * Join State - 96 bytes\n * Maintains state for nested loop join execution\n */\ntypedef struct {\n    /* Offset 0: Join type (1 byte) */\n    uint8_t join_type;\n    #define JOIN_TYPE_INNER    0x01\n    #define JOIN_TYPE_LEFT     0x02  /* Future: LEFT JOIN */\n    #define JOIN_TYPE_RIGHT    0x03  /* Future: RIGHT JOIN */\n    /* Offset 1: Flags (1 byte) */\n    uint8_t flags;\n    #define JOIN_FLAG_USE_INDEX    0x01  /* Index available for inner */\n    #define JOIN_FLAG_PUSHDOWN     0x02  /* Predicates pushed to inner */\n    /* Offset 2: Padding (2 bytes) */\n    uint8_t _pad1[2];\n    /* Offset 4: Outer table cursor (8 bytes) */\n    Cursor *outer_cursor;\n    /* Offset 12: Inner table cursor (8 bytes) */\n    Cursor *inner_cursor;\n    /* Offset 20: Outer row cache (8 bytes) */\n    Row *outer_row_cache;\n    /* Offset 28: Inner row cache (8 bytes) */\n    Row *inner_row_cache;\n    /* Offset 36: Join condition expression (8 bytes) */\n    Expr *join_condition;\n    /* Offset 44: Pushed-down predicates (8 bytes) */\n    Expr *pushed_predicates;\n    /* Offset 52: Index for inner table lookup (8 bytes) */\n    BTreeIndex *inner_index;\n    /* Offset 60: Index column number (4 bytes) */\n    uint32_t index_column;\n    /* Offset 64: Current phase (1 byte) */\n    uint8_t phase;\n    #define JOIN_PHASE_INIT        0\n    #define JOIN_PHASE_SCAN_OUTER  1\n    #define JOIN_PHASE_SCAN_INNER  2\n    #define JOIN_PHASE_MATCHED     3\n    #define JOIN_PHASE_EOF         4\n    /* Offset 65: Match found flag (1 byte) */\n    uint8_t match_found;\n    /* Offset 66: Padding (2 bytes) */\n    uint8_t _pad2[2];\n    /* Offset 68: Rows processed counter (8 bytes) */\n    uint64_t rows_processed;\n    /* Offset 76: Matches found counter (8 bytes) */\n    uint64_t matches_found;\n    /* Offset 84: Current outer rowid (8 bytes) */\n    int64_t current_outer_rowid;\n    /* Offset 92: Padding (4 bytes) */\n    uint8_t _pad3[4];\n    /* Total: 96 bytes */\n} JoinState;\n```\n### 3.6 Aggregate Result Row\n```c\n/*\n * Aggregate Result Row - Variable length\n * Output row from aggregation containing final values\n */\ntypedef struct {\n    /* Offset 0: Number of columns (2 bytes) */\n    uint16_t column_count;\n    /* Offset 2: Group key present flag (1 byte) */\n    uint8_t has_group_key;\n    /* Offset 3: Padding (1 byte) */\n    uint8_t _pad;\n    /* Offset 4: Group key data (variable, if has_group_key) */\n    /* Copy of GroupKey structure */\n    /* Variable: Column values */\n    /* Each: | is_null (1B) | type (1B) | len (2B) | data (len B) | */\n} AggResultRow;\n```\n---\n## 4. Interface Contracts\n### 4.1 Aggregate Functions\n```c\n/*\n * agg_init_state - Initialize aggregate state\n * \n * Parameters:\n *   state: Pointer to AggregateState to initialize (64 bytes allocated)\n *   func: Aggregate function type\n *   result_type: Expected result datatype\n * \n * Returns: 0 on success, error code on failure\n * \n * Contract: state must be zeroed before call\n * Post-condition: state is ready for agg_update calls\n */\nint agg_init_state(AggregateState *state, AggFunc func, DataType result_type);\n/*\n * agg_update - Update aggregate with new value\n * \n * Parameters:\n *   state: Initialized aggregate state\n *   value: Value to incorporate (may be NULL)\n * \n * Returns: 0 on success, error code on failure\n * \n * Contract: NULL values are handled per SQL semantics:\n *   - COUNT(*): Always increment count\n *   - COUNT(col): Increment only if value non-NULL\n *   - SUM/AVG: Add to sum only if non-NULL\n *   - MIN/MAX: Compare only if non-NULL\n */\nint agg_update(AggregateState *state, const Value *value);\n/*\n * agg_finalize - Compute final aggregate value\n * \n * Parameters:\n *   state: Aggregate state after all updates\n *   result: Output value (caller-allocated)\n * \n * Returns: 0 on success, error code on failure\n * \n * Post-condition: result contains final value\n *   - COUNT: Integer count\n *   - SUM: Sum of values (or NULL if no non-NULL values)\n *   - AVG: Sum/count (or NULL if count=0)\n *   - MIN/MAX: Extreme value (or NULL if no non-NULL values)\n */\nint agg_finalize(AggregateState *state, Value *result);\n```\n### 4.2 GROUP BY Functions\n```c\n/*\n * group_hash_init - Initialize group hash table\n * \n * Parameters:\n *   table: Hash table to initialize\n *   initial_buckets: Starting bucket count (power of 2)\n *   agg_count: Number of aggregates per group\n * \n * Returns: 0 on success, error code on failure\n */\nint group_hash_init(GroupHashTable *table, uint32_t initial_buckets, \n                    uint16_t agg_count);\n/*\n * group_hash_find_or_create - Find or create group entry\n * \n * Parameters:\n *   table: Hash table to search\n *   key: Group key to find\n *   agg_defs: Array of aggregate function definitions\n *   agg_count: Number of aggregates\n *   created: Output flag, true if new entry created\n * \n * Returns: Pointer to GroupEntry, NULL on error\n */\nGroupEntry *group_hash_find_or_create(GroupHashTable *table, \n                                       GroupKey *key,\n                                       AggFunc *agg_defs,\n                                       uint16_t agg_count,\n                                       bool *created);\n/*\n * group_key_build - Build group key from row\n * \n * Parameters:\n *   row: Source row\n *   group_cols: Array of column indices for GROUP BY\n *   col_count: Number of group columns\n *   key_out: Output key (caller-allocated buffer)\n * \n * Returns: 0 on success, error code on failure\n */\nint group_key_build(const Row *row, const uint16_t *group_cols,\n                    uint16_t col_count, GroupKey *key_out);\n/*\n * group_hash_iterate - Iterate over all groups\n * \n * Parameters:\n *   table: Hash table to iterate\n *   callback: Function called for each entry\n *   user_data: Passed to callback\n * \n * Returns: Number of entries iterated\n */\nint group_hash_iterate(GroupHashTable *table,\n                       void (*callback)(GroupEntry *, void *),\n                       void *user_data);\n```\n### 4.3 JOIN Functions\n```c\n/*\n * join_init - Initialize join state\n * \n * Parameters:\n *   state: Join state to initialize\n *   outer_cursor: Cursor for outer (left) table\n *   inner_cursor: Cursor for inner (right) table\n *   condition: Join condition expression\n * \n * Returns: 0 on success, error code on failure\n */\nint join_init(JoinState *state, Cursor *outer_cursor, Cursor *inner_cursor,\n              Expr *condition);\n/*\n * join_set_index - Enable index-based join optimization\n * \n * Parameters:\n *   state: Join state\n *   index: B-tree index on inner table\n *   column: Column number in inner table for lookup\n * \n * Returns: 0 on success, error code on failure\n * \n * Contract: Index must be on the join column of inner table\n */\nint join_set_index(JoinState *state, BTreeIndex *index, uint32_t column);\n/*\n * join_step - Execute one step of nested loop join\n * \n * Parameters:\n *   state: Join state\n *   result_row: Output row (joined row)\n * \n * Returns:\n *   > 0: Row produced, return code\n *   0: No row yet, need more steps\n *   < 0: Error or EOF (SQLITE_DONE)\n */\nint join_step(JoinState *state, Row *result_row);\n/*\n * join_pushdown_predicate - Push WHERE predicate to inner scan\n * \n * Parameters:\n *   state: Join state\n *   predicate: Predicate referencing only inner table\n * \n * Returns: 0 on success, error code on failure\n * \n * Contract: Predicate must not reference outer table columns\n */\nint join_pushdown_predicate(JoinState *state, Expr *predicate);\n/*\n * join_build_result_row - Combine outer and inner rows\n * \n * Parameters:\n *   outer: Outer table row\n *   inner: Inner table row\n *   result: Output combined row\n * \n * Returns: 0 on success, error code on failure\n * \n * Layout: outer columns || inner columns\n */\nint join_build_result_row(const Row *outer, const Row *inner, Row *result);\n```\n### 4.4 HAVING Functions\n```c\n/*\n * having_evaluate - Evaluate HAVING condition for group\n * \n * Parameters:\n *   entry: Group entry with finalized aggregates\n *   condition: HAVING condition expression\n *   result: Output boolean result\n * \n * Returns: 0 on success, error code on failure\n * \n * Contract: Aggregates in entry must be finalized\n */\nint having_evaluate(GroupEntry *entry, Expr *condition, bool *result);\n```\n---\n## 5. Algorithm Specification\n### 5.1 Aggregate Update Algorithm\n```\nAGG_UPDATE(state, value):\n    IF state IS NULL:\n        RETURN ERR_NULL_STATE\n    // Handle NULL values per SQL semantics\n    IF value IS NULL:\n        IF state.func == AGG_COUNT_STAR:\n            state.count += 1\n            state.flags |= AGG_FLAG_HAS_VALUE\n        RETURN 0  // Other aggregates ignore NULL\n    state.flags |= AGG_FLAG_HAS_VALUE\n    SWITCH state.func:\n        CASE AGG_COUNT:\n        CASE AGG_COUNT_STAR:\n            state.count += 1\n        CASE AGG_SUM:\n            IF value.type NOT IN (INT, FLOAT):\n                RETURN ERR_AGGREGATE_TYPE_ERROR\n            state.count += 1\n            IF value.type == INT:\n                state.sum_value += CAST(value.i64 AS double)\n            ELSE:\n                state.sum_value += value.f64\n        CASE AGG_AVG:\n            IF value.type NOT IN (INT, FLOAT):\n                RETURN ERR_AGGREGATE_TYPE_ERROR\n            state.count += 1\n            IF value.type == INT:\n                state.sum_value += CAST(value.i64 AS double)\n            ELSE:\n                state.sum_value += value.f64\n        CASE AGG_MIN:\n            IF state.minmax.is_null OR value < state.minmax:\n                COPY(value, state.minmax)\n        CASE AGG_MAX:\n            IF state.minmax.is_null OR value > state.minmax:\n                COPY(value, state.minmax)\n    RETURN 0\n```\n### 5.2 Aggregate Finalize Algorithm\n```\nAGG_FINALIZE(state, result):\n    IF state IS NULL:\n        RETURN ERR_NULL_STATE\n    SWITCH state.func:\n        CASE AGG_COUNT:\n        CASE AGG_COUNT_STAR:\n            result.type = INT\n            result.i64 = state.count\n            result.is_null = FALSE\n        CASE AGG_SUM:\n            IF NOT (state.flags & AGG_FLAG_HAS_VALUE):\n                result.is_null = TRUE\n            ELSE:\n                result.type = state.result_type\n                IF state.result_type == INT:\n                    result.i64 = CAST(state.sum_value AS int64)\n                ELSE:\n                    result.f64 = state.sum_value\n                result.is_null = FALSE\n        CASE AGG_AVG:\n            IF state.count == 0:\n                result.is_null = TRUE\n            ELSE:\n                result.type = FLOAT\n                result.f64 = state.sum_value / state.count\n                result.is_null = FALSE\n        CASE AGG_MIN:\n        CASE AGG_MAX:\n            IF state.minmax.is_null:\n                result.is_null = TRUE\n            ELSE:\n                COPY(state.minmax, result)\n    RETURN 0\n```\n### 5.3 GROUP BY Hash Aggregation Algorithm\n```\nGROUP_BY_AGGREGATE(rows, group_cols, agg_funcs):\n    // Initialize hash table\n    hash_table = CREATE_HASH_TABLE(initial_buckets=1024)\n    // Process each row\n    FOR EACH row IN rows:\n        // Build group key\n        key = BUILD_GROUP_KEY(row, group_cols)\n        // Find or create group entry\n        entry = hash_table.FIND_OR_CREATE(key)\n        IF entry IS NEW:\n            INIT_AGGREGATES(entry.agg_states, agg_funcs)\n        // Update aggregates for this group\n        FOR i = 0 TO agg_funcs.count:\n            value = EXTRACT_VALUE(row, agg_funcs[i].column)\n            AGG_UPDATE(entry.agg_states[i], value)\n    // Return groups (no particular order)\n    RETURN hash_table.ENTRIES()\n```\n### 5.4 Nested Loop Join Algorithm\n```\nNESTED_LOOP_JOIN(state):\n    result_rows = EMPTY_LIST\n    // Phase: Scan outer table\n    WHILE state.outer_cursor HAS MORE ROWS:\n        outer_row = FETCH_NEXT(state.outer_cursor)\n        state.current_outer_row = outer_row\n        // Reset inner cursor\n        IF state.inner_index EXISTS:\n            // Index-based lookup\n            key = EXTRACT(outer_row, state.join_condition.outer_column)\n            inner_rows = INDEX_SEARCH(state.inner_index, key)\n        ELSE:\n            // Full table scan\n            RESET_CURSOR(state.inner_cursor)\n            inner_rows = state.inner_cursor\n        state.match_found = FALSE\n        // Phase: Scan inner for matches\n        FOR EACH inner_row IN inner_rows:\n            // Apply pushed-down predicates first\n            IF state.pushed_predicates EXISTS:\n                IF NOT EVALUATE(state.pushed_predicates, inner_row):\n                    CONTINUE\n            // Evaluate join condition\n            IF EVALUATE_JOIN_CONDITION(state.join_condition, \n                                       outer_row, inner_row):\n                result_row = BUILD_RESULT_ROW(outer_row, inner_row)\n                result_rows.APPEND(result_row)\n                state.match_found = TRUE\n        // INNER JOIN: No action needed if no match\n        // (LEFT JOIN would emit null-extended row here)\n    RETURN result_rows\n```\n### 5.5 Predicate Pushdown Analysis\n```\nANALYZE_PREDICATE_PUSHDOWN(where_clause, join_condition, tables):\n    pushdown = EMPTY_LIST\n    remaining = EMPTY_LIST\n    FOR EACH predicate IN SPLIT_CONJUNCTS(where_clause):\n        referenced_tables = GET_REFERENCED_TABLES(predicate)\n        IF referenced_tables CONTAINS ONLY ONE TABLE:\n            // Can potentially push down\n            IF referenced_tables == inner_table:\n                pushdown.APPEND(predicate)\n                CONTINUE\n        remaining.APPEND(predicate)\n    RETURN (pushdown, remaining)\n```\n---\n## 6. Error Handling Matrix\n| Error Code | Constant | Description | Recovery |\n|------------|----------|-------------|----------|\n| 0x1101 | AGGREGATE_TYPE_ERROR | Non-numeric value for SUM/AVG | Return NULL result |\n| 0x1102 | AGGREGATE_OVERFLOW | Integer overflow in SUM | Convert to float |\n| 0x1103 | GROUP_BY_COLUMN_INVALID | Column not in source tables | Query error |\n| 0x1104 | GROUP_BY_MEMORY_ERROR | Hash table allocation failed | Use disk spill |\n| 0x1105 | JOIN_CONDITION_INVALID | Invalid join condition | Query error |\n| 0x1106 | JOIN_TABLE_NOT_FOUND | Referenced table missing | Schema error |\n| 0x1107 | JOIN_CURSOR_ERROR | Cursor operation failed | Retry/abort |\n| 0x1108 | HAVING_TYPE_ERROR | Non-boolean HAVING result | Query error |\n| 0x1109 | GROUP_KEY_TOO_LARGE | Group key exceeds 4KB | Truncate error |\n| 0x110A | JOIN_RESULT_TOO_WIDE | Joined row exceeds page size | Limit columns |\n---\n## 7. Implementation Sequence\n### Phase 1: Aggregate State Management (1.5 hours)\n**Files:** `src/execution/aggregate.h`, `src/execution/aggregate.c`\n**Tasks:**\n1. Define AggregateState structure (64 bytes)\n2. Implement agg_init_state for each function type\n3. Implement agg_update with NULL handling\n4. Implement agg_finalize with NULL result semantics\n**Checkpoint:** COUNT and SUM aggregates pass unit tests\n### Phase 2: MIN/MAX Aggregates (1 hour)\n**Files:** `src/execution/aggregate.c`\n**Tasks:**\n1. Implement MIN/MAX value comparison for all types\n2. Handle string comparison with length awareness\n3. Handle NULL in minmax storage\n**Checkpoint:** All 5 aggregate functions pass unit tests\n### Phase 3: Group Key Building (1.5 hours)\n**Files:** `src/execution/group_by.h`, `src/execution/group_by.c`\n**Tasks:**\n1. Implement GroupKey structure layout\n2. Implement group_key_build from row\n3. Implement group_key_hash for consistent hashing\n4. Implement group_key_compare for equality\n**Checkpoint:** Group keys build and compare correctly\n### Phase 4: Hash Aggregation (2 hours)\n**Files:** `src/execution/group_by.c`\n**Tasks:**\n1. Implement GroupHashTable with dynamic resizing\n2. Implement group_hash_find_or_create\n3. Integrate with aggregate state management\n4. Implement group_hash_iterate for results\n**Checkpoint:** GROUP BY with single aggregate works\n### Phase 5: HAVING Clause (1 hour)\n**Files:** `src/execution/having.h`, `src/execution/having.c`\n**Tasks:**\n1. Implement having_evaluate\n2. Support aggregate references in HAVING\n3. Support group column references in HAVING\n**Checkpoint:** HAVING filters grouped results correctly\n### Phase 6: Nested Loop Join (2.5 hours)\n**Files:** `src/execution/join.h`, `src/execution/join.c`\n**Tasks:**\n1. Implement JoinState structure\n2. Implement join_init with cursor setup\n3. Implement join_step for nested loop\n4. Implement join_build_result_row\n**Checkpoint:** Simple INNER JOIN produces correct results\n### Phase 7: Index Join Optimization (1.5 hours)\n**Files:** `src/execution/join.c`\n**Tasks:**\n1. Implement join_set_index\n2. Modify join_step to use index when available\n3. Extract key from outer row for inner lookup\n**Checkpoint:** Index-accelerated join outperforms full scan\n### Phase 8: Predicate Pushdown & Integration (1 hour)\n**Files:** `src/execution/join.c`, `tests/test_integration_m11.c`\n**Tasks:**\n1. Implement predicate pushdown analysis\n2. Integrate all components\n3. Write integration tests\n4. Performance validation\n**Checkpoint:** All integration tests pass, performance targets met\n---\n## 8. Test Specification\n### 8.1 Aggregate Function Tests\n```c\n/* test_aggregate.c */\nTEST(agg_count_star_counts_all_rows) {\n    AggregateState state;\n    agg_init_state(&state, AGG_COUNT_STAR, TYPE_INT);\n    Value v = { .type = TYPE_NULL, .is_null = true };\n    agg_update(&state, &v);  // NULL should still count\n    agg_update(&state, &v);\n    Value result;\n    agg_finalize(&state, &result);\n    ASSERT_EQ(result.i64, 2);\n}\nTEST(agg_count_col_excludes_nulls) {\n    AggregateState state;\n    agg_init_state(&state, AGG_COUNT, TYPE_INT);\n    Value null_val = { .is_null = true };\n    Value int_val = { .type = TYPE_INT, .i64 = 42, .is_null = false };\n    agg_update(&state, &null_val);\n    agg_update(&state, &int_val);\n    agg_update(&state, &null_val);\n    Value result;\n    agg_finalize(&state, &result);\n    ASSERT_EQ(result.i64, 1);  // Only non-NULL counted\n}\nTEST(agg_sum_returns_null_when_no_values) {\n    AggregateState state;\n    agg_init_state(&state, AGG_SUM, TYPE_INT);\n    Value null_val = { .is_null = true };\n    agg_update(&state, &null_val);\n    agg_update(&state, &null_val);\n    Value result;\n    agg_finalize(&state, &result);\n    ASSERT_TRUE(result.is_null);\n}\nTEST(agg_avg_computes_correctly) {\n    AggregateState state;\n    agg_init_state(&state, AGG_AVG, TYPE_FLOAT);\n    Value v1 = { .type = TYPE_INT, .i64 = 10, .is_null = false };\n    Value v2 = { .type = TYPE_INT, .i64 = 20, .is_null = false };\n    Value v3 = { .type = TYPE_INT, .i64 = 30, .is_null = false };\n    agg_update(&state, &v1);\n    agg_update(&state, &v2);\n    agg_update(&state, &v3);\n    Value result;\n    agg_finalize(&state, &result);\n    ASSERT_EQ(result.type, TYPE_FLOAT);\n    ASSERT_DOUBLE_EQ(result.f64, 20.0);\n}\nTEST(agg_min_max_with_strings) {\n    AggregateState min_state, max_state;\n    agg_init_state(&min_state, AGG_MIN, TYPE_TEXT);\n    agg_init_state(&max_state, AGG_MAX, TYPE_TEXT);\n    Value v1 = make_string_value(\"apple\");\n    Value v2 = make_string_value(\"banana\");\n    Value v3 = make_string_value(\"cherry\");\n    agg_update(&min_state, &v2);\n    agg_update(&min_state, &v1);\n    agg_update(&min_state, &v3);\n    agg_update(&max_state, &v2);\n    agg_update(&max_state, &v1);\n    agg_update(&max_state, &v3);\n    Value min_result, max_result;\n    agg_finalize(&min_state, &min_result);\n    agg_finalize(&max_state, &max_result);\n    ASSERT_STR_EQ(min_result.str.data, \"apple\");\n    ASSERT_STR_EQ(max_result.str.data, \"cherry\");\n}\n```\n### 8.2 GROUP BY Tests\n```c\n/* test_group_by.c */\nTEST(group_by_single_column) {\n    // Table: (dept, salary)\n    // Rows: (\"eng\", 100), (\"sales\", 80), (\"eng\", 120)\n    GroupHashTable table;\n    group_hash_init(&table, 16, 1);\n    // Process rows...\n    // Verify 2 groups created: \"eng\" and \"sales\"\n    ASSERT_EQ(table.entry_count, 2);\n}\nTEST(group_by_multiple_columns) {\n    // Group by (dept, level)\n    // Verify composite keys work correctly\n}\nTEST(group_by_with_null_key) {\n    // Rows with NULL in group column form separate group\n    // NULLs are equal for grouping purposes\n}\nTEST(group_hash_resize) {\n    // Insert many groups, verify hash table grows\n    // Verify no data loss during resize\n}\n```\n### 8.3 JOIN Tests\n```c\n/* test_join.c */\nTEST(inner_join_basic) {\n    // Table A: (id, name)\n    // Table B: (a_id, value)\n    // Join on A.id = B.a_id\n    // Verify correct matched rows returned\n}\nTEST(inner_join_no_matches) {\n    // Tables with no matching rows\n    // Result should be empty\n}\nTEST(inner_join_with_index) {\n    // Same as basic but with index on inner table\n    // Verify index is used, results identical\n}\nTEST(join_with_nulls) {\n    // NULL values in join columns\n    // NULL != NULL, no match expected\n}\nTEST(join_predicate_pushdown) {\n    // WHERE clause with predicates on inner table only\n    // Verify pushdown occurs and reduces inner scans\n}\nTEST(join_cartesian_product) {\n    // Join without condition (cross join)\n    // Verify M*N result rows\n}\n```\n### 8.4 Integration Tests\n```c\n/* test_integration_m11.c */\nTEST(full_query_with_group_by_and_having) {\n    // SELECT dept, COUNT(*), AVG(salary)\n    // FROM employees\n    // GROUP BY dept\n    // HAVING COUNT(*) > 2\n}\nTEST(join_with_aggregation) {\n    // SELECT d.name, COUNT(e.id)\n    // FROM departments d\n    // JOIN employees e ON d.id = e.dept_id\n    // GROUP BY d.id, d.name\n}\nTEST(complex_having_with_aggregates) {\n    // HAVING AVG(salary) > 50000 AND COUNT(*) >= 3\n}\nTEST(null_handling_end_to_end) {\n    // Insert rows with NULLs\n    // Verify aggregates handle correctly\n    // Verify joins handle NULL comparisons\n}\n```\n---\n## 9. Performance Targets\n| Operation | Target | Measurement |\n|-----------|--------|-------------|\n| COUNT(*) on 100K rows | < 50ms | Sequential scan |\n| SUM on 100K rows | < 50ms | Sequential scan |\n| GROUP BY 1000 groups | < 100ms | 100K input rows |\n| GROUP BY 10000 groups | < 500ms | 100K input rows |\n| INNER JOIN (no index) | O(N*M) | Nested loop baseline |\n| INNER JOIN (with index) | O(N log M) | Index lookup |\n| JOIN 10K x 10K with index | < 200ms | With B-tree index |\n| Hash table resize | < 10ms | Per resize operation |\n| Aggregate finalize | < 1\u03bcs | Per aggregate |\n---\n## 10. Diagrams\n### Diagram 1: Aggregate State Lifecycle\n`[tdd-diag-m11-1]`\nFlowchart showing aggregate state from initialization through updates to finalization.\n### Diagram 2: GROUP BY Hash Table Structure\n`[tdd-diag-m11-2]`\nDiagram of hash table with buckets, entries, and aggregate state pointers.\n### Diagram 3: Group Key Encoding\n`[tdd-diag-m11-3]`\nByte-level diagram showing how multi-column group keys are encoded.\n### Diagram 4: Nested Loop Join Algorithm\n`[tdd-diag-m11-4]`\nFlowchart showing outer scan, inner scan/reset, and result building phases.\n### Diagram 5: Index-Accelerated Join\n`[tdd-diag-m11-5]`\nComparison of full table scan vs. index lookup for inner table.\n### Diagram 6: Predicate Pushdown Analysis\n`[tdd-diag-m11-6]`\nDecision tree for which predicates can be pushed to inner table.\n### Diagram 7: JOIN Result Row Construction\n`[tdd-diag-m11-7]`\nMemory layout showing concatenation of outer and inner row columns.\n### Diagram 8: Full Query Execution Pipeline\n`[tdd-diag-m11-8]`\nEnd-to-end flow from table scan \u2192 join \u2192 aggregate \u2192 having \u2192 result.\n---\n## 11. SQL Semantics Reference\n### NULL Handling in Aggregates\n| Function | All NULLs | Mixed | No NULLs |\n|----------|-----------|-------|----------|\n| COUNT(*) | Returns 0 | Returns count | Returns count |\n| COUNT(col) | Returns 0 | Returns non-NULL count | Returns count |\n| SUM(col) | Returns NULL | Returns sum of non-NULL | Returns sum |\n| AVG(col) | Returns NULL | Returns avg of non-NULL | Returns avg |\n| MIN(col) | Returns NULL | Returns min of non-NULL | Returns min |\n| MAX(col) | Returns NULL | Returns max of non-NULL | Returns max |\n### Three-Valued Logic in JOINs\n```\nNULL = NULL  \u2192  UNKNOWN (no match)\nNULL = 5     \u2192  UNKNOWN (no match)\n5 = NULL     \u2192  UNKNOWN (no match)\n5 = 5        \u2192  TRUE (match)\n5 = 3        \u2192  FALSE (no match)\n```\n---\n## 12. Dependencies\n- **build-sqlite-m4**: Cursor interface for table scanning\n- **build-sqlite-m5**: Row structure and value types\n- **build-sqlite-m6**: SELECT execution framework\n- **build-sqlite-m7**: B-tree index interface for index joins\n- **build-sqlite-m3**: Expression evaluation for HAVING and join conditions\n---\n```json\n{\n  \"module_id\": \"build-sqlite-m11\",\n  \"title\": \"Aggregate Functions & JOIN\",\n  \"phase\": 11,\n  \"total_phases\": 12,\n  \"estimated_hours\": 12,\n  \"files_created\": 12,\n  \"key_structures\": [\n    \"AggregateState (64 bytes)\",\n    \"GroupKey (variable)\",\n    \"GroupEntry (40 bytes header)\",\n    \"GroupHashTable\",\n    \"JoinState (96 bytes)\"\n  ],\n  \"functions_implemented\": [\n    \"agg_init_state\",\n    \"agg_update\",\n    \"agg_finalize\",\n    \"group_hash_init\",\n    \"group_hash_find_or_create\",\n    \"group_key_build\",\n    \"join_init\",\n    \"join_step\",\n    \"join_set_index\",\n    \"join_pushdown_predicate\",\n    \"having_evaluate\"\n  ],\n  \"performance_targets\": {\n    \"aggregate_100k_rows\": \"<50ms\",\n    \"group_by_1000_groups\": \"<100ms\",\n    \"index_join_complexity\": \"O(N log M)\"\n  ],\n  \"diagrams\": [\n    \"tdd-diag-m11-1\",\n    \"tdd-diag-m11-2\",\n    \"tdd-diag-m11-3\",\n    \"tdd-diag-m11-4\",\n    \"tdd-diag-m11-5\",\n    \"tdd-diag-m11-6\",\n    \"tdd-diag-m11-7\",\n    \"tdd-diag-m11-8\"\n  ],\n  \"dependencies\": [\n    \"build-sqlite-m3\",\n    \"build-sqlite-m4\",\n    \"build-sqlite-m5\",\n    \"build-sqlite-m6\",\n    \"build-sqlite-m7\"\n  ]\n}\n```\n<!-- END_TDD_MOD -->\n",
  "tdd_current_mod_index": 11,
  "tdd_diagrams_to_generate": [
    {
      "id": "tdd-diag-m9-4",
      "title": "ROLLBACK Page Restoration",
      "description": "Steps: open journal \u2192 skip header \u2192 for each record: read page_id \u2192 read original data \u2192 write to database at page_id \u2192 sync database \u2192 delete journal.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m9"
    },
    {
      "id": "tdd-diag-m9-5",
      "title": "Hot Journal Recovery Flow",
      "description": "Startup: check for .db-journal file \u2192 (exists?) validate header \u2192 restore pages \u2192 sync database \u2192 delete journal \u2192 proceed. Shows automatic recovery.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m9"
    },
    {
      "id": "tdd-diag-m9-6",
      "title": "fsync Ordering Guarantee",
      "description": "Timeline: T1 write journal \u2192 T2 fsync (durable) \u2192 T3 write database \u2192 T4 fsync (durable) \u2192 T5 delete journal. Crash at any point: before T2 (no recovery needed), after T2 (recoverable), after T4 (committed).",
      "type": "sequence",
      "anchor_target": "build-sqlite-m9"
    },
    {
      "id": "tdd-diag-m10-1",
      "title": "WAL vs Rollback Journal Architecture",
      "description": "Side-by-side: Rollback (modify database, journal originals, undo on crash) vs WAL (append to log, checkpoint later, redo on crash). Shows fundamental inversion.",
      "type": "architecture",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m10-2",
      "title": "WAL File Structure",
      "description": "Header (32 bytes): magic, version, page_size, checkpoint_seq, salt1, salt2, checksums. Frames: [header (24 bytes) + page_data] repeated. Commit markers indicated.",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m10-3",
      "title": "Snapshot Isolation Mechanism",
      "description": "Reader R1 starts at frame 100, sees frames 1-100. Writer W appends frames 101-150. Reader R2 starts at frame 150, sees frames 1-150. R1 still sees only 1-100. Shows temporal views.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m10-4",
      "title": "WAL Read Path",
      "description": "Page read: check WAL index for page_id \u2192 (found and frame \u2264 snapshot?) read from WAL \u2192 (not found or too new?) read from database. Shows fallback logic.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m10-5",
      "title": "Checkpoint Process",
      "description": "Steps: identify committed frames \u2192 wait for readers (FULL mode) \u2192 copy frames to database \u2192 sync database \u2192 update checkpoint marker \u2192 (TRUNCATE?) reset WAL. Shows coordination.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m10-6",
      "title": "Concurrent Reader/Writer Coordination",
      "description": "Writer: append frame \u2192 update index \u2192 release. Reader: capture snapshot \u2192 read pages (no locks). Checkpoint: wait for readers with old snapshots \u2192 proceed. Shows non-blocking reads.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m10-7",
      "title": "WAL Recovery Replay",
      "description": "Startup: open WAL \u2192 validate header \u2192 scan frames \u2192 find last commit marker \u2192 replay committed frames to database \u2192 sync \u2192 build index for remaining. Shows partial transaction handling.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m11-1",
      "title": "Aggregate State Accumulation",
      "description": "For each row: update count, check NULL for col-based aggregates, update sum (if not NULL), update min/max (if not NULL and better). Shows incremental state changes.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-2",
      "title": "NULL Handling in Aggregates",
      "description": "Table with NULL values showing: COUNT(*)=5, COUNT(col)=3, SUM= sum of 3 non-NULL, AVG= sum/3, MIN/MAX from non-NULL only. Visualizes exclusion.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-3",
      "title": "Hash-Based GROUP BY",
      "description": "Input rows \u2192 extract group key \u2192 hash \u2192 lookup in hash table \u2192 (found?) update existing aggregate state : create new entry. Shows hash table structure.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-4",
      "title": "GROUP BY Execution Flow",
      "description": "Two-phase: Phase 1 scan table, build hash table of groups. Phase 2 iterate hash table, finalize aggregates, output results. Shows separation.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-5",
      "title": "HAVING vs WHERE Execution",
      "description": "WHERE filters rows before grouping (row-level). HAVING filters groups after aggregation (group-level). Shows execution order: scan \u2192 WHERE \u2192 GROUP BY \u2192 HAVING \u2192 output.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-6",
      "title": "Nested Loop Join Algorithm",
      "description": "For each outer row: for each inner row: evaluate condition \u2192 (match?) output combined row. Shows O(N\u00d7M) complexity and result row construction.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-7",
      "title": "Join with Index Optimization",
      "description": "For each outer row: extract join key \u2192 seek in inner index \u2192 (found?) fetch inner row by rowid \u2192 output combined. Shows O(N \u00d7 log M) with index.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-8",
      "title": "JOIN Bytecode Structure",
      "description": "OpenRead outer \u2192 Rewind outer \u2192 OUTER_LOOP: OpenRead inner \u2192 Rewind inner \u2192 INNER_LOOP: [compare keys] \u2192 (match?) ResultRow \u2192 Next inner \u2192 INNER_LOOP \u2192 Next outer \u2192 OUTER_LOOP \u2192 Halt.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m11"
    }
  ],
  "external_reading": "",
  "running_criteria": [
    {
      "module_id": "build-sqlite-m6",
      "criteria": [
        {
          "id": "M6-001",
          "description": "Implement TriBool enum and three-valued logic operations (AND, OR, NOT, IS)",
          "type": "implementation",
          "priority": "critical",
          "validation": "Unit tests for all truth table combinations pass"
        },
        {
          "id": "M6-002",
          "description": "Implement Cursor struct with create, destroy, rewind, next operations",
          "type": "implementation",
          "priority": "critical",
          "validation": "Cursor traverses test table correctly"
        },
        {
          "id": "M6-003",
          "description": "Implement cursor_get_row and cursor_get_column with NULL handling",
          "type": "implementation",
          "priority": "critical",
          "validation": "NULL column values returned correctly"
        },
        {
          "id": "M6-004",
          "description": "Implement WHERE clause evaluation with three-valued logic",
          "type": "implementation",
          "priority": "critical",
          "validation": "NULL comparisons return TRI_NULL, IS NULL returns correct boolean"
        },
        {
          "id": "M6-005",
          "description": "Implement column projection for SELECT statements",
          "type": "implementation",
          "priority": "high",
          "validation": "SELECT a, c returns only specified columns"
        },
        {
          "id": "M6-006",
          "description": "Implement NOT NULL constraint checking",
          "type": "implementation",
          "priority": "critical",
          "validation": "INSERT with NULL in NOT NULL column returns error"
        },
        {
          "id": "M6-007",
          "description": "Implement UNIQUE constraint checking",
          "type": "implementation",
          "priority": "critical",
          "validation": "INSERT with duplicate UNIQUE value returns error"
        },
        {
          "id": "M6-008",
          "description": "Implement execute_insert with rowid allocation and constraint validation",
          "type": "implementation",
          "priority": "critical",
          "validation": "Valid INSERT succeeds, constraint violations return descriptive errors"
        },
        {
          "id": "M6-009",
          "description": "Implement execute_update with delete-reinsert pattern",
          "type": "implementation",
          "priority": "high",
          "validation": "UPDATE modifies row in place, constraints enforced"
        },
        {
          "id": "M6-010",
          "description": "Implement two-pass DELETE algorithm for WHERE clauses",
          "type": "implementation",
          "priority": "high",
          "validation": "DELETE WHERE deletes correct rows, returns count"
        },
        {
          "id": "M6-011",
          "description": "Table scan performance: 10K rows in < 50ms",
          "type": "performance",
          "priority": "critical",
          "validation": "Benchmark bench_table_scan passes"
        },
        {
          "id": "M6-012",
          "description": "WHERE with NULL overhead: < 10% vs non-NULL",
          "type": "performance",
          "priority": "high",
          "validation": "Benchmark bench_where_null passes"
        },
        {
          "id": "M6-013",
          "description": "Constraint checking: < 1\u03bcs per row",
          "type": "performance",
          "priority": "high",
          "validation": "Benchmark bench_constraints passes"
        },
        {
          "id": "M6-014",
          "description": "Error messages include column and table names",
          "type": "quality",
          "priority": "medium",
          "validation": "Constraint errors formatted as 'NOT NULL constraint failed: table.column'"
        },
        {
          "id": "M6-015",
          "description": "Integration with M4 buffer pool (pin/unpin on cursor movement)",
          "type": "integration",
          "priority": "critical",
          "validation": "No buffer pool leaks detected"
        },
        {
          "id": "M6-016",
          "description": "Integration with M5 B-tree (page traversal, cell access)",
          "type": "integration",
          "priority": "critical",
          "validation": "Cursor correctly navigates B-tree leaf pages"
        }
      ]
    }
  ],
  "explained_concepts": [],
  "blueprint_warnings": [],
  "system_diagram_d2": null,
  "system_diagram_iteration": 0,
  "system_diagram_done": false
}