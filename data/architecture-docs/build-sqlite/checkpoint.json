{
  "project_id": "build-sqlite",
  "meta": {
    "id": "build-sqlite",
    "name": "Build Your Own SQLite",
    "description": "Embedded SQL database with tokenizer, parser, bytecode compiler (VDBE), page-based B-tree/B+tree storage, buffer pool, query planner, and ACID transactions via rollback journal and WAL.",
    "difficulty": "expert",
    "estimated_hours": 105,
    "essence": "SQL tokenization and recursive-descent parsing producing ASTs, compiled into bytecode instructions executed by a virtual machine (VDBE), operating over a page-based storage engine with B-trees for clustered table storage and B+trees for secondary indexes, managed through a buffer pool with LRU eviction, and ACID guarantees via either rollback journal or write-ahead logging.",
    "why_important": "Building a database from scratch teaches the fundamental data structures and algorithms underlying all modern databases. These skills\u2014disk I/O management, B-tree indexing, bytecode execution, query optimization, and crash recovery\u2014 are directly applicable to backend engineering, distributed systems, and performance optimization roles.",
    "learning_outcomes": [
      "Build a lexer and recursive-descent parser converting SQL text into an AST",
      "Compile ASTs into bytecode instructions for a virtual machine (VDBE)",
      "Implement a buffer pool manager with LRU eviction and dirty page tracking",
      "Design page-based B-tree storage for tables (clustered) and B+tree for indexes",
      "Implement row storage with variable-length record encoding",
      "Build a cost-based query planner with statistics-driven cardinality estimation",
      "Implement ACID transactions with rollback journal for crash recovery",
      "Design WAL mode for improved concurrent read/write performance"
    ],
    "skills": [
      "SQL Parsing",
      "Bytecode Compilation",
      "Virtual Machine Execution",
      "Buffer Pool Management",
      "B-tree/B+tree Indexing",
      "Query Optimization",
      "Page-Based Storage",
      "Write-Ahead Logging",
      "Transaction Management",
      "Binary File Formats"
    ],
    "tags": [
      "acid",
      "btree",
      "build-from-scratch",
      "databases",
      "expert",
      "persistence",
      "query-engine",
      "sql",
      "virtual-machine"
    ],
    "architecture_doc": "architecture-docs/build-sqlite/index.md",
    "languages": {
      "recommended": [
        "C",
        "Rust",
        "Go"
      ],
      "also_possible": [
        "Java"
      ]
    },
    "resources": [
      {
        "name": "Let's Build a Simple Database",
        "url": "https://cstack.github.io/db_tutorial/",
        "type": "tutorial"
      },
      {
        "name": "SQLite Architecture",
        "url": "https://www.sqlite.org/arch.html",
        "type": "documentation"
      },
      {
        "name": "SQLite File Format",
        "url": "https://www.sqlite.org/fileformat2.html",
        "type": "documentation"
      },
      {
        "name": "SQLite VDBE Documentation",
        "url": "https://www.sqlite.org/vdbe.html",
        "type": "documentation"
      },
      {
        "name": "CMU 15-445 Database Systems",
        "url": "https://15445.courses.cs.cmu.edu",
        "type": "course"
      },
      {
        "name": "CodeCrafters SQLite Challenge",
        "url": "https://app.codecrafters.io/courses/sqlite/overview",
        "type": "tool"
      }
    ],
    "prerequisites": [
      {
        "type": "skill",
        "name": "B-tree data structure"
      },
      {
        "type": "skill",
        "name": "SQL basics"
      },
      {
        "type": "skill",
        "name": "File I/O and binary formats"
      },
      {
        "type": "skill",
        "name": "Basic compiler concepts (lexer, parser, AST)"
      }
    ],
    "milestones": [
      {
        "id": "build-sqlite-m1",
        "name": "SQL Tokenizer",
        "description": "Build a lexer that converts SQL text into a stream of typed tokens.",
        "acceptance_criteria": [
          "Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.) case-insensitively",
          "String literals enclosed in single quotes are parsed including escaped quotes ('it''s' \u2192 it's)",
          "Numeric literals including integers and floating-point values (42, 3.14, -7) are recognized as distinct token types",
          "Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon) are tokenized as distinct tokens",
          "Identifiers (table names, column names) support quoted identifiers with double quotes (\"column name\")",
          "Tokenizer reports error position (line and column) for unrecognized characters",
          "Token stream correctly tokenizes at least 20 diverse SQL statements in a test suite",
          "Tokenizer recognizes keywords like SELECT and INSERT case-insensitively",
          "String literals correctly handle escaped single quotes ('') and preserve internal content",
          "Numeric literals distinguish between integers (42) and floats (3.14)",
          "Double-quoted identifiers ('\"Table Name\"') are correctly tokenized as identifiers",
          "The tokenizer returns a list or stream of objects containing type, value, line, and column",
          "A test suite of 20+ SQL statements produces the expected token sequence"
        ],
        "pitfalls": [
          "Not handling escaped quotes in string literals ('it''s') causes premature string termination",
          "Keywords must be case-insensitive but identifiers may be case-sensitive depending on quoting\u2014handle both",
          "Unicode identifiers require careful handling; start simple with ASCII and document limitations",
          "Negative numbers may be ambiguous with subtraction operator; handle at parser level, not tokenizer"
        ],
        "concepts": [
          "Lexical analysis converts character stream to token stream",
          "Finite state machine drives character-by-character token recognition",
          "Token types classify each lexeme (keyword, identifier, literal, operator)",
          "Error reporting with source location enables useful diagnostics"
        ],
        "skills": [
          "String parsing",
          "State machine implementation",
          "Token classification",
          "Error reporting"
        ],
        "deliverables": [
          "Lexer producing token stream from SQL input string",
          "Keyword recognition for all supported SQL words",
          "String and numeric literal parsing with escape handling",
          "Operator and punctuation tokenization",
          "Error reporting with line and column position",
          "Test suite with at least 20 SQL statements"
        ],
        "estimated_hours": 4
      },
      {
        "id": "build-sqlite-m2",
        "name": "SQL Parser (AST)",
        "description": "Build a recursive-descent parser that converts the token stream into an Abstract Syntax Tree (AST).",
        "acceptance_criteria": [
          "SELECT parser produces AST with column list (including *), FROM clause, optional WHERE, ORDER BY, and LIMIT",
          "INSERT parser produces AST with target table, optional column names, and VALUES clause",
          "CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT, REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison (=,<,>) > AND > OR",
          "Parenthesized expressions override default precedence",
          "Parser produces clear error messages with token position for syntax errors",
          "Test suite covers at least 15 valid and 10 invalid SQL statements",
          "SELECT parser produces AST with column list, FROM, and optional WHERE/LIMIT",
          "INSERT parser handles target table and VALUES mapping",
          "CREATE TABLE parser extracts column names, types, and constraints (PRIMARY KEY, NOT NULL)",
          "Expression parser correctly handles NOT > AND > OR precedence",
          "Parenthesized expressions correctly override default precedence levels",
          "Parser provides error position (line/column) for syntax errors",
          "Test suite passes for 15+ valid and 10+ invalid SQL edge cases",
          "Tokenizer output is correctly consumed as parser input (token stream integration)",
          "SELECT parser produces AST with column list including * wildcard support",
          "SELECT parser produces AST with FROM clause containing table name",
          "SELECT parser produces optional WHERE clause as Expression node",
          "SELECT parser produces optional ORDER BY clause with column names and ASC/DESC",
          "SELECT parser produces optional LIMIT clause with numeric value",
          "INSERT parser produces AST with target table name",
          "INSERT parser produces optional column name list before VALUES",
          "INSERT parser produces VALUES clause with one or more tuples of expressions",
          "CREATE TABLE parser extracts table name from statement",
          "CREATE TABLE parser extracts column definitions with names and data types (INTEGER, TEXT, REAL, BLOB)",
          "CREATE TABLE parser extracts column constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison > AND > OR",
          "Binary expressions (AND, OR, comparison operators) produce BinaryExpression AST nodes",
          "Unary NOT expression produces UnaryExpression AST node",
          "Parser provides error position (line and column) for syntax errors",
          "Parser reports meaningful error messages for unexpected tokens",
          "NULL keyword is parsed as LiteralExpression not IdentifierExpression",
          "String and numeric literals are parsed as LiteralExpression nodes",
          "Column references are parsed as IdentifierExpression nodes",
          "Test suite passes for 15+ valid SQL statements across SELECT, INSERT, and CREATE TABLE",
          "Test suite correctly rejects 10+ invalid SQL statements with position information"
        ],
        "pitfalls": [
          "Left recursion in expression grammar causes infinite recursion in recursive-descent parsers\u2014use precedence climbing or Pratt parsing",
          "AND binds tighter than OR in SQL (unlike some programming languages)\u2014get this wrong and WHERE clauses evaluate incorrectly",
          "Not handling parenthesized sub-expressions breaks complex WHERE clauses",
          "NULL is a keyword, not an identifier\u2014treat it specially in expression parsing"
        ],
        "concepts": [
          "Recursive descent parsing uses mutually recursive functions for grammar rules",
          "AST represents the syntactic structure of a SQL statement as a tree",
          "Operator precedence determines evaluation order of expressions",
          "Pratt parsing or precedence climbing handles binary operators elegantly"
        ],
        "skills": [
          "Recursive function design",
          "Tree data structures",
          "Grammar rule encoding",
          "Precedence handling"
        ],
        "deliverables": [
          "SELECT statement parser producing AST",
          "INSERT statement parser producing AST",
          "CREATE TABLE parser producing AST with column definitions and constraints",
          "Expression parser with correct operator precedence",
          "Error reporting with token position for parse errors",
          "Test suite for valid and invalid SQL inputs"
        ],
        "estimated_hours": 7
      },
      {
        "id": "build-sqlite-m3",
        "name": "Bytecode Compiler (VDBE)",
        "description": "Compile parsed AST into bytecode instructions executed by a virtual machine. This is the execution engine of the database.",
        "acceptance_criteria": [
          "Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable, Rewind, Column, ResultRow, Next, Halt",
          "Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord, Insert, Halt",
          "Virtual machine executes bytecode programs step-by-step, processing one opcode per cycle",
          "VM maintains a register file (array of typed values) for intermediate computation",
          "EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable format",
          "WHERE clause compiles to conditional jump opcodes that skip non-matching rows",
          "Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under 100ms",
          "Compiler translates SELECT AST into opcodes including OpenTable, Rewind, Column, ResultRow, Next, and Halt",
          "Compiler translates INSERT AST into opcodes including OpenTable, MakeRecord, and Insert",
          "VM executes bytecode in a fetch-decode-execute loop, processing one opcode per cycle",
          "VM manages a register file of typed values for intermediate calculations",
          "WHERE clauses are correctly compiled into conditional jump opcodes (e.g., Gt, Le, Ne)",
          "The EXPLAIN command displays the human-readable opcode sequence for any valid SQL statement",
          "The VM executes a full table scan of 10,000 rows in under 100ms",
          "Bytecode execution of SELECT * FROM t on a 10,000-row table completes in under 100ms"
        ],
        "pitfalls": [
          "Directly interpreting AST nodes (tree-walking interpreter) is simpler but significantly slower than bytecode\u2014commit to bytecode",
          "Register allocation must handle nested expressions without clobbering intermediate values",
          "Opcodes for cursor management (open, rewind, next, close) must match the storage engine's iterator interface",
          "Missing a Halt opcode causes the VM to run past the end of the program into garbage memory"
        ],
        "concepts": [
          "Bytecode compilation translates high-level AST into low-level instruction sequence",
          "Virtual machine executes bytecode using a fetch-decode-execute loop",
          "Register-based VM uses a register file for operand storage (vs stack-based)",
          "Cursor opcodes abstract B-tree traversal for the VM"
        ],
        "skills": [
          "Bytecode generation",
          "Virtual machine implementation",
          "Register allocation",
          "Instruction set design"
        ],
        "deliverables": [
          "Bytecode instruction set with opcodes for table operations, comparisons, jumps, and output",
          "Compiler translating SELECT, INSERT, CREATE TABLE ASTs into bytecode",
          "Virtual machine executing bytecode with register file and program counter",
          "EXPLAIN command displaying bytecode for any SQL statement",
          "WHERE clause compilation to conditional jumps"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m4",
        "name": "Buffer Pool Manager",
        "description": "Implement a page cache that sits between the B-tree layer and disk, managing fixed-size pages with LRU eviction and dirty page tracking.",
        "acceptance_criteria": [
          "Buffer pool manages a configurable number of in-memory page frames (default 1000 pages)",
          "Pages are fixed-size (4096 bytes by default, configurable)",
          "FetchPage loads a page from disk into a free frame, or returns the cached frame if already resident",
          "LRU eviction selects the least recently used unpinned page for replacement when no free frames exist",
          "Dirty page tracking marks pages modified in memory; eviction writes dirty pages to disk before replacement",
          "Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations",
          "FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)",
          "Buffer pool hit rate is measurable and logged for performance tuning",
          "Buffer pool initializes with a fixed number of 4096-byte frames",
          "FetchPage returns the correct page from memory if already loaded (hit)",
          "FetchPage loads page from disk if not in memory (miss)",
          "LRU algorithm correctly identifies the least recently used page for eviction",
          "Pinned pages (count > 0) are never selected for eviction",
          "Dirty pages are written back to disk only when evicted or on FlushAll",
          "Buffer pool hit rate is tracked and accessible for performance metrics"
        ],
        "pitfalls": [
          "Evicting a pinned page causes data corruption or use-after-free\u2014enforce pin counting",
          "Not flushing dirty pages before eviction loses committed data",
          "Page ID collisions if page numbering is not globally unique across the database file",
          "Buffer pool deadlocks when B-tree operations pin too many pages simultaneously\u2014set pin limits"
        ],
        "concepts": [
          "Buffer pool caches disk pages in memory for fast access",
          "LRU (Least Recently Used) eviction approximates optimal page replacement",
          "Pin counting prevents eviction of actively-used pages",
          "Dirty page tracking enables write-back caching"
        ],
        "skills": [
          "Page cache implementation",
          "LRU eviction algorithm",
          "Pin/unpin lifecycle management",
          "Disk I/O management"
        ],
        "deliverables": [
          "Buffer pool with configurable frame count and page size",
          "FetchPage loading pages from disk or returning cached frames",
          "LRU eviction selecting least recently used unpinned page",
          "Dirty page tracking and write-back on eviction",
          "Pin/Unpin API for B-tree layer",
          "FlushAll for shutdown and checkpoint"
        ],
        "estimated_hours": 8
      },
      {
        "id": "build-sqlite-m5",
        "name": "B-tree Page Format & Table Storage",
        "description": "Implement the on-disk page structure for B-trees (tables) and B+trees (indexes), with row serialization and node splitting.",
        "acceptance_criteria": [
          "Page header contains page type (leaf/internal, table/index), cell count, free space pointer, and right-child pointer (internal only)",
          "Table B-tree leaf pages store rows keyed by rowid with variable-length record encoding",
          "Table B-tree internal pages store rowid separator keys and child page numbers",
          "Index B+tree leaf pages store (indexed column value, rowid) pairs with data only in leaves",
          "Index B+tree internal pages store only separator keys and child pointers (no row data)",
          "CREATE TABLE creates a B-tree root page and records the schema in a system catalog (sqlite_master equivalent)",
          "INSERT serializes a row and inserts into the correct B-tree leaf; node splitting creates a new page and promotes a separator key",
          "Full table scan traverses all leaf pages in rowid order, returning all rows",
          "Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer pool",
          "Page header correctly identifies Leaf vs Internal and Table vs Index types",
          "Slotted page format implements bidirectional growth (pointers vs cells)",
          "Table B-tree stores full records in leaf nodes keyed by rowid",
          "Index B+tree stores key/rowid pairs only in leaf nodes",
          "Node split algorithm correctly rebalances the tree and promotes keys to parents",
          "Varint implementation handles 1-9 byte encoding for 64-bit integers",
          "System catalog (sqlite_master) persists table root page numbers",
          "Full table scan successfully iterates through all leaf pages in order"
        ],
        "pitfalls": [
          "Conflating B-tree (data in all nodes) with B+tree (data only in leaves)\u2014tables use B-tree, indexes use B+tree in SQLite",
          "Cell overflow when a row exceeds page capacity requires overflow pages\u2014handle or document the size limit",
          "Page fragmentation after deletions wastes space\u2014track free space within pages",
          "Endianness must be consistent between write and read (SQLite uses big-endian for portability)",
          "Variable-length integer encoding (varint) must handle the full range of 64-bit integers"
        ],
        "concepts": [
          "B-tree stores key-value pairs in all nodes (used for rowid-keyed tables)",
          "B+tree stores data only in leaf nodes with linked leaf pages (used for indexes)",
          "Slotted page format uses cell pointers for variable-length records",
          "Node splitting maintains B-tree balance on insert overflow",
          "System catalog stores table and index schema metadata"
        ],
        "skills": [
          "Binary page format design",
          "B-tree/B+tree implementation",
          "Variable-length record encoding",
          "Node splitting algorithms"
        ],
        "deliverables": [
          "Page format with header, cell pointer array, and cell content area",
          "Table B-tree with leaf (row storage) and internal (separator + child pointer) pages",
          "Index B+tree with leaf (key + rowid) and internal (separator + child pointer) pages",
          "Row serialization with variable-length encoding for column values",
          "Node splitting on insert overflow with separator key promotion",
          "System catalog table storing schema metadata",
          "Full table scan via leaf page traversal"
        ],
        "estimated_hours": 12
      },
      {
        "id": "build-sqlite-m6",
        "name": "SELECT Execution & DML",
        "description": "Execute SELECT, INSERT, UPDATE, and DELETE via the bytecode VM, with row deserialization, projection, and filtering.",
        "acceptance_criteria": [
          "SELECT * FROM table returns all rows in rowid order via B-tree leaf scan",
          "SELECT col1, col2 returns only specified columns (projection)",
          "WHERE clause filters rows during scan, evaluating boolean expressions on deserialized column values",
          "INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data",
          "UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes",
          "DELETE removes rows matching WHERE; subsequent SELECT no longer returns them",
          "NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null",
          "Operations on non-existent tables return an error with the table name",
          "SELECT * returns all rows by iterating through the B-tree leaf sequence",
          "SELECT with column names correctly projects only the requested fields",
          "WHERE clause correctly filters rows using Three-Valued Logic (handling NULLs)",
          "INSERT adds a new row and updates the B-tree structure correctly",
          "UPDATE and DELETE modify/remove rows while maintaining B-tree integrity",
          "NOT NULL constraints reject invalid writes with a descriptive error",
          "Attempting to query a table not in the System Catalog returns an 'undefined table' error",
          "WHERE clause correctly filters rows using Three-Valued Logic (NULL = NULL evaluates to NULL, not TRUE)",
          "Column projection correctly deserializes variable-length records to extract requested fields",
          "INSERT with NULL for INTEGER PRIMARY KEY triggers auto-increment behavior",
          "UPDATE cannot change the rowid (primary key) - must reject or handle as delete+insert",
          "DELETE during iteration uses two-pass approach to avoid cursor corruption"
        ],
        "pitfalls": [
          "Column name case sensitivity: SQL standard is case-insensitive for identifiers; be consistent",
          "NULL handling in WHERE: NULL = NULL evaluates to NULL (falsy), not TRUE\u2014use IS NULL for null checks",
          "B-tree rebalancing after DELETE is complex; initially, mark rows as deleted and reclaim space during compaction",
          "Updating the primary key (rowid) requires delete + re-insert at the new position",
          "Memory management for large result sets\u2014stream results row-by-row, don't buffer all in memory"
        ],
        "concepts": [
          "Cursor pattern abstracts B-tree traversal for the VM",
          "Projection selects a subset of columns from each row",
          "Predicate evaluation filters rows during scan",
          "Three-valued logic (TRUE, FALSE, NULL) for SQL boolean expressions"
        ],
        "skills": [
          "B-tree cursor implementation",
          "Row deserialization",
          "Expression evaluation",
          "DML execution"
        ],
        "deliverables": [
          "Table scan operator iterating all rows via B-tree cursor",
          "Row deserialization from binary page format to typed column values",
          "Column projection selecting specified fields",
          "WHERE clause evaluation with three-valued logic",
          "INSERT, UPDATE, DELETE execution via bytecode VM",
          "NOT NULL and UNIQUE constraint enforcement during writes"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m7",
        "name": "Secondary Indexes",
        "description": "Implement secondary indexes using B+trees and integrate index lookups into query execution.",
        "acceptance_criteria": [
          "CREATE INDEX builds a B+tree index mapping (indexed column value \u2192 rowid) from existing table data",
          "Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries added/removed/updated)",
          "Index lookup retrieves rows matching an equality predicate without full table scan, verified by counting pages read",
          "Range scan on index returns rows within a value range using B+tree leaf traversal",
          "Query execution uses index scan when an indexed column appears in WHERE with equality or range predicate",
          "UNIQUE index rejects INSERT or UPDATE that would create duplicate values",
          "CREATE INDEX builds a B+tree mapping column values to rowids",
          "INSERT/UPDATE/DELETE operations maintain all associated indexes synchronously",
          "Index lookup (equality) avoids full table scan and visits significantly fewer pages",
          "Index range scan (BETWEEN or < >) traverses linked leaf pages",
          "UNIQUE index correctly rejects duplicate value insertions",
          "Bytecode VM can perform a 'Double Lookup' from index cursor to table cursor"
        ],
        "pitfalls": [
          "Forgetting to update indexes on INSERT/UPDATE/DELETE causes stale or missing index entries",
          "Index on a column with many NULL values\u2014NULLs must be handled consistently (SQLite allows multiple NULLs in UNIQUE index)",
          "Composite indexes (multi-column) require careful key comparison\u2014leftmost prefix must match for index to be useful",
          "Index maintenance overhead can make writes slower; only create indexes that benefit read patterns"
        ],
        "concepts": [
          "Secondary index maps indexed column values to primary keys (rowids)",
          "B+tree leaf traversal enables efficient range scans",
          "Index maintenance ensures indexes stay consistent with table data",
          "Covering index can satisfy a query without table lookup if all needed columns are in the index"
        ],
        "skills": [
          "B+tree index implementation",
          "Index maintenance on DML",
          "Index scan execution",
          "Unique constraint enforcement"
        ],
        "deliverables": [
          "CREATE INDEX building B+tree from existing table data",
          "Index maintenance on INSERT, UPDATE, DELETE",
          "Index equality lookup avoiding full table scan",
          "Index range scan using B+tree leaf traversal",
          "UNIQUE index constraint enforcement"
        ],
        "estimated_hours": 8
      },
      {
        "id": "build-sqlite-m8",
        "name": "Query Planner & Statistics",
        "description": "Implement a cost-based query planner that chooses between table scan and index scan based on collected statistics.",
        "acceptance_criteria": [
          "ANALYZE command collects statistics: row count per table, distinct value count per indexed column",
          "Cost model estimates pages read for full table scan (total_pages) and index scan (estimated_rows / rows_per_page)",
          "Planner selects index scan when estimated selectivity (matching_rows / total_rows) is below a threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too low",
          "EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated row count",
          "For multi-table queries (JOIN), planner estimates join cardinality and selects join order to minimize intermediate result size",
          "ANALYZE command collects row count per table and distinct value count per indexed column",
          "Cost model estimates pages read for full table scan based on total pages",
          "Cost model estimates I/O cost for index scan based on selectivity and random I/O factor",
          "Planner selects index scan when estimated selectivity is below threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too high",
          "EXPLAIN displays the chosen plan including scan type, index name if used, and estimated row count",
          "For multi-table queries, planner estimates join cardinality and considers join order to minimize intermediate result size"
        ],
        "pitfalls": [
          "Without statistics (before ANALYZE), the planner has no data for cost estimation\u2014use default assumptions (e.g., assume 1M rows)",
          "Stale statistics after many inserts/deletes cause the planner to choose suboptimal plans\u2014recommend periodic ANALYZE",
          "Cardinality estimation errors compound through joins\u2014a 10x error in one table becomes 100x after a two-table join",
          "Plan search space explodes exponentially with number of tables in JOIN\u2014limit to dynamic programming for \u226410 tables"
        ],
        "concepts": [
          "Cost-based optimization compares estimated cost of alternative plans",
          "Cardinality estimation predicts the number of rows each operator produces",
          "Selectivity is the fraction of rows matching a predicate",
          "Statistics collection (ANALYZE) provides data for cost estimation",
          "Dynamic programming-based join ordering for multi-table queries"
        ],
        "skills": [
          "Statistics collection",
          "Cost model design",
          "Cardinality estimation",
          "Plan enumeration"
        ],
        "deliverables": [
          "ANALYZE command collecting table and index statistics",
          "Cost model estimating I/O for table scan vs index scan",
          "Plan selection choosing cheapest access path per table",
          "EXPLAIN command displaying chosen plan with cost estimates",
          "Join order optimization for multi-table queries"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m9",
        "name": "Transactions (Rollback Journal)",
        "description": "Implement ACID transactions using a rollback journal for crash recovery.",
        "acceptance_criteria": [
          "BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or ROLLBACK",
          "COMMIT makes all changes permanent by flushing dirty pages and removing the rollback journal",
          "ROLLBACK undoes all changes by restoring original pages from the rollback journal",
          "Rollback journal records original page contents BEFORE modification (for undo on crash)",
          "Changes are not visible to other connections until COMMIT (basic read isolation)",
          "Crash recovery on startup detects an existing rollback journal and automatically rolls back the incomplete transaction",
          "Journal file is fsync'd before modified pages are written to the database file (write ordering guarantee)",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the engine state",
          "A .db-journal file is created and contains original page data before any write to the main .db file",
          "The journal file is physically flushed to disk (fsync) before the main database is modified",
          "A manual ROLLBACK restores the state from the journal and clears the journal file",
          "Startup logic detects a 'Hot Journal' and automatically restores the database to a consistent state",
          "Writes are not visible to other database connections until the COMMIT is complete",
          "BEGIN starts a transaction and buffers all subsequent writes until COMMIT or ROLLBACK",
          "Rollback journal records original page contents BEFORE any modification to the database file",
          "Changes are not visible to other database connections until COMMIT completes",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the transaction manager state",
          "A manual ROLLBACK restores the database state from the journal and deletes the journal file"
        ],
        "pitfalls": [
          "Writing modified pages to database before journal is fsync'd causes unrecoverable corruption on crash",
          "Partial page writes (torn pages) on crash can corrupt the database\u2014journal must contain complete original pages",
          "Lock ordering between multiple connections must be consistent to prevent deadlocks",
          "Long-running transactions holding locks block all other writers\u2014document the locking behavior"
        ],
        "concepts": [
          {
            "ACID": "Atomicity (rollback journal), Consistency (constraints), Isolation (locking), Durability (fsync)"
          },
          "Rollback journal records undo information (original page images) before modification",
          {
            "Write ordering": "journal fsync \u2192 database write \u2192 journal delete"
          },
          {
            "Crash recovery": "hot journal detected \u2192 restore original pages \u2192 delete journal"
          }
        ],
        "skills": [
          "Rollback journal implementation",
          "Write ordering enforcement",
          "Crash recovery logic",
          "Lock management"
        ],
        "deliverables": [
          "BEGIN/COMMIT/ROLLBACK command implementation",
          "Rollback journal recording original page contents before modification",
          "Write ordering ensuring journal is durable before database pages are modified",
          "Crash recovery detecting hot journal and restoring database to pre-transaction state",
          "ACID guarantee verification test suite"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m10",
        "name": "WAL Mode",
        "description": "Implement Write-Ahead Logging as an alternative to rollback journal, enabling concurrent readers during writes.",
        "acceptance_criteria": [
          "WAL mode appends modified pages to a separate WAL file instead of modifying the main database file",
          "Writers append to WAL; readers check WAL for the most recent version of a page before reading from the main database",
          "Multiple readers can execute queries concurrently while a single writer appends to the WAL",
          "Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database file",
          "WAL checkpoint is required to prevent unbounded WAL growth\u2014auto-checkpoint triggers after configurable page count (default 1000)",
          "Readers see a consistent snapshot: a reader that starts before a commit does not see that commit's changes (snapshot isolation for reads)",
          "WAL file corruption is detected via page checksums",
          "Writers append to a separate WAL file instead of modifying the main .db file",
          "Readers search the WAL for the most recent page version before falling back to the main file",
          "Writers and multiple readers can operate simultaneously without blocking",
          "Checkpointing copies WAL pages to the main database and truncates the WAL",
          "Automatic checkpoint triggers after 1000 pages (configurable)",
          "Readers use a consistent snapshot based on the WAL state at their start time",
          "Checksums are used to detect and reject corrupted WAL frames"
        ],
        "pitfalls": [
          "WAL grows unbounded without checkpointing\u2014auto-checkpoint is not optional, it's required",
          "Readers pinning old WAL frames prevent checkpoint from truncating\u2014long-running reads block WAL cleanup",
          "WAL and rollback journal are mutually exclusive modes\u2014switching requires careful state management",
          "Checkpoint must not run while readers are using WAL frames that would be overwritten",
          "WAL file corruption must be detected (checksums) to prevent propagating bad data to the main database"
        ],
        "concepts": [
          "WAL appends redo information (new page images) to a log file",
          "Readers use WAL index (wal-index) to find most recent page version",
          "Checkpoint merges WAL changes back into the main database file",
          {
            "Snapshot isolation": "each reader sees the database as of its start time"
          },
          {
            "WAL vs rollback journal": "WAL enables concurrent readers, rollback journal does not"
          }
        ],
        "skills": [
          "Write-ahead log implementation",
          "Snapshot isolation for readers",
          "Checkpoint algorithm",
          "Concurrent read/write coordination"
        ],
        "deliverables": [
          "WAL file format appending modified pages with checksums",
          "WAL reader looking up most recent page version before main database",
          "Checkpoint process copying WAL pages into main database",
          "Auto-checkpoint triggered by WAL page count threshold",
          "Concurrent reader support during active write transactions",
          "WAL mode toggle (PRAGMA journal_mode=WAL)"
        ],
        "estimated_hours": 12
      },
      {
        "id": "build-sqlite-m11",
        "name": "Aggregate Functions & JOIN",
        "description": "Implement aggregate functions (COUNT, SUM, AVG, MIN, MAX), GROUP BY, and basic JOIN execution.",
        "acceptance_criteria": [
          "COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values",
          "SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY groups rows by specified columns before applying aggregate functions",
          "HAVING filters groups after aggregation",
          "INNER JOIN combines rows from two tables matching a join condition",
          "Nested loop join is implemented as the baseline join algorithm",
          "JOIN with WHERE clause filters correctly after join",
          "COUNT(*) accurately counts rows including NULLs",
          "COUNT(col) ignores NULL values in the target column",
          "AVG returns a REAL/float even if input column is INTEGER",
          "GROUP BY correctly partitions aggregate states into buckets",
          "HAVING filters out aggregated groups based on result values",
          "INNER JOIN correctly combines rows from two tables using a nested loop",
          "JOIN with WHERE correctly filters rows before or during the join process",
          "COUNT(*) returns the number of rows including NULLs",
          "COUNT(col) returns count of non-NULL values only",
          "SUM produces correct results over grouped and ungrouped queries",
          "AVG returns REAL/float even if input column is INTEGER",
          "AVG ignores NULL values in computation",
          "MIN and MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY partitions rows into groups before applying aggregate functions",
          "GROUP BY without ORDER BY can return groups in any order",
          "HAVING filters groups after aggregation based on aggregate values",
          "JOIN with WHERE clause filters correctly after or during join",
          "Empty table with aggregates returns appropriate default values (0 for COUNT, NULL for SUM/AVG)",
          "Multiple aggregates can be computed in a single query"
        ],
        "pitfalls": [
          "AVG must handle integer division correctly (return REAL, not INTEGER)",
          "NULL handling in aggregates: COUNT(*) counts NULLs, COUNT(col) does not; SUM/AVG ignore NULLs",
          "GROUP BY without aggregate function is valid SQL but confusing\u2014handle correctly",
          "Nested loop join is O(n*m)\u2014acceptable for small tables but document the limitation"
        ],
        "concepts": [
          "Aggregate functions accumulate values across row groups",
          "GROUP BY partitions rows into groups for aggregation",
          "Nested loop join iterates all combinations of rows from two tables",
          "HAVING filters groups after aggregation (vs WHERE which filters before)"
        ],
        "skills": [
          "Aggregate computation",
          "Group-by execution",
          "Join algorithms",
          "NULL handling in aggregates"
        ],
        "deliverables": [
          "COUNT, SUM, AVG, MIN, MAX aggregate functions",
          "GROUP BY execution with hash-based or sort-based grouping",
          "HAVING clause filtering groups after aggregation",
          "Nested loop INNER JOIN execution",
          "Test suite covering aggregates with NULLs, empty tables, and multi-table joins"
        ],
        "estimated_hours": 14
      }
    ],
    "domain": "data-storage"
  },
  "blueprint": {
    "title": "Architecting a Relational Engine: The SQLite Blueprint",
    "overview": "This project is a deep-dive into the internals of a relational database management system (RDBMS). Instead of treating SQL as a black box, you will build the entire vertical stack: from a string-consuming lexer to a bytecode-executing virtual machine, down to a page-managed B-tree storage engine. The journey mimics the evolution of data storage\u2014moving from simple file appending to complex, ACID-compliant transactional systems.",
    "design_philosophy": "SQLite's genius lies in its modularity through the Virtual Database Engine (VDBE). By compiling SQL into bytecode, the 'Frontend' (Parser/Compiler) is decoupled from the 'Backend' (B-tree/Pager). This project teaches students that a database is not one monolithic algorithm, but a pipeline where high-level intent is translated into low-level page manipulations. We prioritize 'The Page' as the fundamental unit of reality.",
    "is_build_your_own": true,
    "prerequisites": {
      "assumed_known": [
        "B-tree data structure logic",
        "Binary file manipulation",
        "Basic SQL syntax",
        "Pointer arithmetic / Memory management"
      ],
      "must_teach_first": [
        {
          "concept": "Virtual Machine Architecture (Registers vs Stack)",
          "depth": "intermediate",
          "when": "Milestone 3"
        },
        {
          "concept": "The Pager Abstraction",
          "depth": "foundational",
          "when": "Milestone 4"
        },
        {
          "concept": "Write-Ahead Logging (WAL) theory",
          "depth": "intermediate",
          "when": "Milestone 10"
        }
      ]
    },
    "milestones": [
      {
        "id": "build-sqlite-m1",
        "title": "SQL Tokenizer",
        "anchor_id": "m1-tokenizer",
        "summary": "Build a lexer that converts raw SQL text into a stream of typed tokens, handling literals, identifiers, and escaped strings.",
        "misconception": "Developers often think tokenizing is just splitting a string by spaces.",
        "reveal": "A tokenizer is a State Machine where context determines the meaning of a character (e.g., a single quote starts a literal, but two consecutive single quotes represent one escaped quote).",
        "cascade": [
          "Finite State Machines (FSM) \u2014 The core logic for all pattern recognition.",
          "Lexical Scoping \u2014 How symbols are identified before they are understood.",
          "Error Localization \u2014 Why your compiler knows exactly which column your typo is in."
        ],
        "yaml_acceptance_criteria": [
          "Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.) case-insensitively",
          "String literals enclosed in single quotes are parsed including escaped quotes ('it''s' \u2192 it's)",
          "Numeric literals including integers and floating-point values (42, 3.14, -7) are recognized as distinct token types",
          "Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon) are tokenized as distinct tokens",
          "Identifiers (table names, column names) support quoted identifiers with double quotes (\"column name\")",
          "Tokenizer reports error position (line and column) for unrecognized characters",
          "Token stream correctly tokenizes at least 20 diverse SQL statements in a test suite",
          "Tokenizer recognizes keywords like SELECT and INSERT case-insensitively",
          "String literals correctly handle escaped single quotes ('') and preserve internal content",
          "Numeric literals distinguish between integers (42) and floats (3.14)",
          "Double-quoted identifiers ('\"Table Name\"') are correctly tokenized as identifiers",
          "The tokenizer returns a list or stream of objects containing type, value, line, and column",
          "A test suite of 20+ SQL statements produces the expected token sequence"
        ]
      },
      {
        "id": "build-sqlite-m2",
        "title": "SQL Parser (AST)",
        "anchor_id": "m2-parser",
        "summary": "Implement a recursive-descent parser to transform the token stream into an Abstract Syntax Tree, strictly enforcing SQL grammar and operator precedence.",
        "misconception": "Nested expressions (like AND/OR in WHERE clauses) can be parsed linearly.",
        "reveal": "SQL grammar is recursive; the parser must use the call stack (or a Pratt parser) to 'climb' the precedence levels, ensuring 'NOT' binds tighter than 'AND'.",
        "cascade": [
          "Recursive Descent \u2014 How code represents complex hierarchical rules.",
          "Operator Precedence \u2014 The mathematical foundation of logic evaluation.",
          "AST (Abstract Syntax Trees) \u2014 The universal bridge between human code and machine execution."
        ],
        "yaml_acceptance_criteria": [
          "SELECT parser produces AST with column list (including *), FROM clause, optional WHERE, ORDER BY, and LIMIT",
          "INSERT parser produces AST with target table, optional column names, and VALUES clause",
          "CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT, REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison (=,<,>) > AND > OR",
          "Parenthesized expressions override default precedence",
          "Parser produces clear error messages with token position for syntax errors",
          "Test suite covers at least 15 valid and 10 invalid SQL statements",
          "SELECT parser produces AST with column list, FROM, and optional WHERE/LIMIT",
          "INSERT parser handles target table and VALUES mapping",
          "CREATE TABLE parser extracts column names, types, and constraints (PRIMARY KEY, NOT NULL)",
          "Expression parser correctly handles NOT > AND > OR precedence",
          "Parenthesized expressions correctly override default precedence levels",
          "Parser provides error position (line/column) for syntax errors",
          "Test suite passes for 15+ valid and 10+ invalid SQL edge cases",
          "Tokenizer output is correctly consumed as parser input (token stream integration)",
          "SELECT parser produces AST with column list including * wildcard support",
          "SELECT parser produces AST with FROM clause containing table name",
          "SELECT parser produces optional WHERE clause as Expression node",
          "SELECT parser produces optional ORDER BY clause with column names and ASC/DESC",
          "SELECT parser produces optional LIMIT clause with numeric value",
          "INSERT parser produces AST with target table name",
          "INSERT parser produces optional column name list before VALUES",
          "INSERT parser produces VALUES clause with one or more tuples of expressions",
          "CREATE TABLE parser extracts table name from statement",
          "CREATE TABLE parser extracts column definitions with names and data types (INTEGER, TEXT, REAL, BLOB)",
          "CREATE TABLE parser extracts column constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison > AND > OR",
          "Binary expressions (AND, OR, comparison operators) produce BinaryExpression AST nodes",
          "Unary NOT expression produces UnaryExpression AST node",
          "Parser provides error position (line and column) for syntax errors",
          "Parser reports meaningful error messages for unexpected tokens",
          "NULL keyword is parsed as LiteralExpression not IdentifierExpression",
          "String and numeric literals are parsed as LiteralExpression nodes",
          "Column references are parsed as IdentifierExpression nodes",
          "Test suite passes for 15+ valid SQL statements across SELECT, INSERT, and CREATE TABLE",
          "Test suite correctly rejects 10+ invalid SQL statements with position information"
        ]
      },
      {
        "id": "build-sqlite-m3",
        "title": "Bytecode Compiler (VDBE)",
        "anchor_id": "m3-vdbe",
        "summary": "Compile the AST into a sequence of low-level bytecode instructions (Opcodes) that run on a register-based Virtual Machine.",
        "misconception": "Databases execute SQL by 'walking the tree' of the AST during the query.",
        "reveal": "Production databases (like SQLite) compile SQL into a program for a custom VM; this turns a complex search into a tight loop of primitive instructions like 'Rewind', 'Column', and 'Next'.",
        "cascade": [
          "Virtual Machine Design \u2014 Learning how to build custom execution environments.",
          "Instruction Sets \u2014 How high-level logic decomposes into basic math and jumps.",
          "Register Allocation \u2014 Managing finite resources (registers) for infinite logic."
        ],
        "yaml_acceptance_criteria": [
          "Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable, Rewind, Column, ResultRow, Next, Halt",
          "Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord, Insert, Halt",
          "Virtual machine executes bytecode programs step-by-step, processing one opcode per cycle",
          "VM maintains a register file (array of typed values) for intermediate computation",
          "EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable format",
          "WHERE clause compiles to conditional jump opcodes that skip non-matching rows",
          "Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under 100ms",
          "Compiler translates SELECT AST into opcodes including OpenTable, Rewind, Column, ResultRow, Next, and Halt",
          "Compiler translates INSERT AST into opcodes including OpenTable, MakeRecord, and Insert",
          "VM executes bytecode in a fetch-decode-execute loop, processing one opcode per cycle",
          "VM manages a register file of typed values for intermediate calculations",
          "WHERE clauses are correctly compiled into conditional jump opcodes (e.g., Gt, Le, Ne)",
          "The EXPLAIN command displays the human-readable opcode sequence for any valid SQL statement",
          "The VM executes a full table scan of 10,000 rows in under 100ms"
        ]
      },
      {
        "id": "build-sqlite-m4",
        "title": "Buffer Pool Manager",
        "anchor_id": "m4-buffer-pool",
        "summary": "Implement a page cache that mediates between the database file and memory, using LRU eviction and pin-counts to ensure efficiency and safety.",
        "misconception": "Writing to a file is enough to ensure data is 'on disk'.",
        "reveal": "The OS page cache is unpredictable. A database must manage its own 'Buffer Pool' to guarantee specific pages stay in memory during complex B-tree rebalances.",
        "cascade": [
          "LRU Caching \u2014 The universal pattern for high-performance systems.",
          "Page Management \u2014 Understanding the 'fixed-size block' reality of hardware.",
          "Dirty Page Management \u2014 Why 'Saving' and 'Writing' are two different stages."
        ],
        "yaml_acceptance_criteria": [
          "Buffer pool manages a configurable number of in-memory page frames (default 1000 pages)",
          "Pages are fixed-size (4096 bytes by default, configurable)",
          "FetchPage loads a page from disk into a free frame, or returns the cached frame if already resident",
          "LRU eviction selects the least recently used unpinned page for replacement when no free frames exist",
          "Dirty page tracking marks pages modified in memory; eviction writes dirty pages to disk before replacement",
          "Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations",
          "FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)",
          "Buffer pool hit rate is measurable and logged for performance tuning",
          "Buffer pool initializes with a fixed number of 4096-byte frames",
          "FetchPage returns the correct page from memory if already loaded (hit)",
          "FetchPage loads page from disk if not in memory (miss)",
          "LRU algorithm correctly identifies the least recently used page for eviction",
          "Pinned pages (count > 0) are never selected for eviction",
          "Dirty pages are written back to disk only when evicted or on FlushAll",
          "Buffer pool hit rate is tracked and accessible for performance metrics"
        ]
      },
      {
        "id": "build-sqlite-m5",
        "title": "B-tree Page Format & Table Storage",
        "anchor_id": "m5-btree",
        "summary": "Design the binary layout of database pages. Implement B-trees for row storage and B+trees for indexes, including node splitting logic.",
        "misconception": "A B-tree node is just an object with pointers to other objects.",
        "reveal": "In a database, a B-tree node is a 4096-byte array. Pointers are 4-byte 'Page IDs', and data is packed into a 'slotted' format to prevent fragmentation.",
        "cascade": [
          "Slotted Page Layout \u2014 How to store variable-length data in fixed-size blocks.",
          "Endianness \u2014 Why cross-platform binary formats require strict byte ordering.",
          "Tree Balancing \u2014 The algorithmic art of maintaining O(log N) depth."
        ],
        "yaml_acceptance_criteria": [
          "Page header contains page type (leaf/internal, table/index), cell count, free space pointer, and right-child pointer (internal only)",
          "Table B-tree leaf pages store rows keyed by rowid with variable-length record encoding",
          "Table B-tree internal pages store rowid separator keys and child page numbers",
          "Index B+tree leaf pages store (indexed column value, rowid) pairs with data only in leaves",
          "Index B+tree internal pages store only separator keys and child pointers (no row data)",
          "CREATE TABLE creates a B-tree root page and records the schema in a system catalog (sqlite_master equivalent)",
          "INSERT serializes a row and inserts into the correct B-tree leaf; node splitting creates a new page and promotes a separator key",
          "Full table scan traverses all leaf pages in rowid order, returning all rows",
          "Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer pool",
          "Page header correctly identifies Leaf vs Internal and Table vs Index types",
          "Slotted page format implements bidirectional growth (pointers vs cells)",
          "Table B-tree stores full records in leaf nodes keyed by rowid",
          "Index B+tree stores key/rowid pairs only in leaf nodes",
          "Node split algorithm correctly rebalances the tree and promotes keys to parents",
          "Varint implementation handles 1-9 byte encoding for 64-bit integers",
          "System catalog (sqlite_master) persists table root page numbers",
          "Full table scan successfully iterates through all leaf pages in order"
        ]
      },
      {
        "id": "build-sqlite-m6",
        "title": "SELECT Execution & DML",
        "anchor_id": "m6-dml",
        "summary": "Integrate the VM with the B-tree layer to execute full SELECT, INSERT, UPDATE, and DELETE queries.",
        "misconception": "Updating a row is as simple as writing new bytes over the old ones.",
        "reveal": "Rows are variable-length. Updating a row often means it no longer fits in its original slot, requiring a delete-then-insert and potentially a page split.",
        "cascade": [
          "The Cursor Pattern \u2014 Decoupling logical iteration from physical tree traversal.",
          "Three-Valued Logic \u2014 Why NULL behaves differently in comparisons.",
          "Write Amplification \u2014 Understanding how a 1-row update can touch 5+ pages."
        ],
        "yaml_acceptance_criteria": [
          "SELECT * FROM table returns all rows in rowid order via B-tree leaf scan",
          "SELECT col1, col2 returns only specified columns (projection)",
          "WHERE clause filters rows during scan, evaluating boolean expressions on deserialized column values",
          "INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data",
          "UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes",
          "DELETE removes rows matching WHERE; subsequent SELECT no longer returns them",
          "NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null",
          "Operations on non-existent tables return an error with the table name",
          "SELECT * returns all rows by iterating through the B-tree leaf sequence",
          "SELECT with column names correctly projects only the requested fields",
          "WHERE clause correctly filters rows using Three-Valued Logic (handling NULLs)",
          "INSERT adds a new row and updates the B-tree structure correctly",
          "UPDATE and DELETE modify/remove rows while maintaining B-tree integrity",
          "NOT NULL constraints reject invalid writes with a descriptive error",
          "Attempting to query a table not in the System Catalog returns an 'undefined table' error",
          "WHERE clause correctly filters rows using Three-Valued Logic (NULL = NULL evaluates to NULL, not TRUE)",
          "Column projection correctly deserializes variable-length records to extract requested fields",
          "INSERT with NULL for INTEGER PRIMARY KEY triggers auto-increment behavior",
          "UPDATE cannot change the rowid (primary key) - must reject or handle as delete+insert",
          "DELETE during iteration uses two-pass approach to avoid cursor corruption"
        ]
      },
      {
        "id": "build-sqlite-m7",
        "title": "Secondary Indexes",
        "anchor_id": "m7-indexes",
        "summary": "Implement B+trees that map column values back to RowIDs, enabling fast lookups without full table scans.",
        "misconception": "Indexes contain the full row data for faster access.",
        "reveal": "Indexes are 'thin' trees. They only store the key and a 'pointer' (RowID). Finding a row via index requires two lookups: one in the Index B+tree, and one in the Table B-tree.",
        "cascade": [
          "Index Maintenance \u2014 The cost of 'Free' read speed is slower writes.",
          "Range Scans \u2014 Why B+trees are superior to Hash Maps for databases.",
          "Covering Indexes \u2014 When an index is so thick you don't need the table at all."
        ],
        "yaml_acceptance_criteria": [
          "CREATE INDEX builds a B+tree index mapping (indexed column value \u2192 rowid) from existing table data",
          "Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries added/removed/updated)",
          "Index lookup retrieves rows matching an equality predicate without full table scan, verified by counting pages read",
          "Range scan on index returns rows within a value range using B+tree leaf traversal",
          "Query execution uses index scan when an indexed column appears in WHERE with equality or range predicate",
          "UNIQUE index rejects INSERT or UPDATE that would create duplicate values",
          "CREATE INDEX builds a B+tree mapping column values to rowids",
          "INSERT/UPDATE/DELETE operations maintain all associated indexes synchronously",
          "Index lookup (equality) avoids full table scan and visits significantly fewer pages",
          "Index range scan (BETWEEN or < >) traverses linked leaf pages",
          "UNIQUE index correctly rejects duplicate value insertions",
          "Bytecode VM can perform a 'Double Lookup' from index cursor to table cursor"
        ]
      },
      {
        "id": "build-sqlite-m8",
        "title": "Query Planner & Statistics",
        "anchor_id": "m8-planner",
        "summary": "Implement a cost-based optimizer that uses table statistics to decide between a Full Table Scan and an Index Scan.",
        "misconception": "The database always uses an index if it's available.",
        "reveal": "If a query returns 90% of a table, using an index is actually SLOWER than a full scan due to random I/O. The planner must estimate 'Selectivity' to choose.",
        "cascade": [
          "Cost Modeling \u2014 Predicting performance before executing code.",
          "Heuristics vs Statistics \u2014 How databases handle missing information.",
          "The ANALYZE command \u2014 Why databases need to 'study' their own data."
        ],
        "yaml_acceptance_criteria": [
          "ANALYZE command collects statistics: row count per table, distinct value count per indexed column",
          "Cost model estimates pages read for full table scan (total_pages) and index scan (estimated_rows / rows_per_page)",
          "Planner selects index scan when estimated selectivity (matching_rows / total_rows) is below a threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too low",
          "EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated row count",
          "For multi-table queries (JOIN), planner estimates join cardinality and selects join order to minimize intermediate result size",
          "ANALYZE command collects row count per table and distinct value count per indexed column",
          "Cost model estimates pages read for full table scan based on total pages",
          "Cost model estimates I/O cost for index scan based on selectivity and random I/O factor",
          "Planner selects index scan when estimated selectivity is below threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too high",
          "EXPLAIN displays the chosen plan including scan type, index name if used, and estimated row count",
          "For multi-table queries, planner estimates join cardinality and considers join order to minimize intermediate result size"
        ]
      },
      {
        "id": "build-sqlite-m9",
        "title": "Transactions (Rollback Journal)",
        "anchor_id": "m9-transactions",
        "summary": "Achieve ACID compliance by implementing a rollback journal that saves 'original' pages before they are modified.",
        "misconception": "Computers crash between operations; databases crash during a single byte write.",
        "reveal": "Durability is about 'Write Ordering'. You must fsync the journal TO DISK before you touch the main database, or a crash will leave you with a corrupt hybrid state.",
        "cascade": [
          "ACID Properties \u2014 The gold standard for data reliability.",
          "fsync() and I/O barriers \u2014 Why 'closed' doesn't mean 'saved'.",
          "Crash Recovery \u2014 Building systems that heal themselves on reboot."
        ],
        "yaml_acceptance_criteria": [
          "BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or ROLLBACK",
          "COMMIT makes all changes permanent by flushing dirty pages and removing the rollback journal",
          "ROLLBACK undoes all changes by restoring original pages from the rollback journal",
          "Rollback journal records original page contents BEFORE modification (for undo on crash)",
          "Changes are not visible to other connections until COMMIT (basic read isolation)",
          "Crash recovery on startup detects an existing rollback journal and automatically rolls back the incomplete transaction",
          "Journal file is fsync'd before modified pages are written to the database file (write ordering guarantee)",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the engine state",
          "A .db-journal file is created and contains original page data before any write to the main .db file",
          "The journal file is physically flushed to disk (fsync) before the main database is modified",
          "A manual ROLLBACK restores the state from the journal and clears the journal file",
          "Startup logic detects a 'Hot Journal' and automatically restores the database to a consistent state",
          "Writes are not visible to other database connections until the COMMIT is complete"
        ]
      },
      {
        "id": "build-sqlite-m10",
        "title": "WAL Mode",
        "anchor_id": "m10-wal",
        "summary": "Implement Write-Ahead Logging to allow concurrent readers and writers, moving the engine toward production-grade concurrency.",
        "misconception": "To read a consistent version of data, you must block writers from changing it.",
        "reveal": "With WAL, writers append NEW versions to a log, while readers look at OLD versions in the main file. They never cross paths, enabling high-performance concurrency.",
        "cascade": [
          "Redo Logs \u2014 The heart of modern high-availability databases.",
          "Snapshot Isolation \u2014 Seeing the past to ensure a consistent present.",
          "Checkpointing \u2014 The process of 'merging' reality back into the main store."
        ],
        "yaml_acceptance_criteria": [
          "WAL mode appends modified pages to a separate WAL file instead of modifying the main database file",
          "Writers append to WAL; readers check WAL for the most recent version of a page before reading from the main database",
          "Multiple readers can execute queries concurrently while a single writer appends to the WAL",
          "Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database file",
          "WAL checkpoint is required to prevent unbounded WAL growth\u2014auto-checkpoint triggers after configurable page count (default 1000)",
          "Readers see a consistent snapshot: a reader that starts before a commit does not see that commit's changes",
          "WAL file corruption is detected via page checksums",
          "Writers append to a separate WAL file instead of modifying the main .db file",
          "Readers search the WAL for the most recent page version before falling back to the main file",
          "Writers and multiple readers can operate simultaneously without blocking",
          "Checkpointing copies WAL pages to the main database and truncates the WAL",
          "Automatic checkpoint triggers after 1000 pages (configurable)",
          "Readers use a consistent snapshot based on the WAL state at their start time",
          "Checksums are used to detect and reject corrupted WAL frames"
        ]
      },
      {
        "id": "build-sqlite-m11",
        "title": "Aggregate Functions & JOIN",
        "anchor_id": "m11-joins",
        "summary": "Expand the engine to support cross-table relations and data summaries via COUNT, SUM, and INNER JOIN.",
        "misconception": "A JOIN is a complex mathematical operation.",
        "reveal": "A JOIN is fundamentally a nested loop: For every row in Table A, open a cursor and find matching rows in Table B. Performance depends entirely on if Table B has an index.",
        "cascade": [
          "Nested Loop Joins \u2014 The basic building block of relational algebra.",
          "Aggregation State \u2014 How to compute averages without storing all rows in memory.",
          "Set Theory \u2014 Visualizing SQL through the lens of Venn diagrams."
        ],
        "yaml_acceptance_criteria": [
          "COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values",
          "SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY groups rows by specified columns before applying aggregate functions",
          "HAVING filters groups after aggregation",
          "INNER JOIN combines rows from two tables matching a join condition",
          "Nested loop join is implemented as the baseline join algorithm",
          "JOIN with WHERE clause filters correctly after join",
          "COUNT(*) accurately counts rows including NULLs",
          "COUNT(col) ignores NULL values in the target column",
          "AVG returns a REAL/float even if input column is INTEGER",
          "GROUP BY correctly partitions aggregate states into buckets",
          "HAVING filters out aggregated groups based on result values",
          "INNER JOIN correctly combines rows from two tables using a nested loop",
          "JOIN with WHERE correctly filters rows before or during the join process",
          "Multiple aggregates can be computed in a single query"
        ]
      }
    ],
    "diagrams": [
      {
        "id": "diag-l0-map",
        "title": "The SQLite Satellite Map",
        "description": "A global view showing the journey from SQL string down to physical bytes on disk.",
        "anchor_target": "m1-tokenizer",
        "level": "satellite"
      },
      {
        "id": "diag-vdbe-step",
        "title": "VDBE Execution Trace",
        "description": "Visualizing how a SELECT AST is flattened into OpCodes and how the Program Counter moves.",
        "anchor_target": "m3-vdbe",
        "level": "street"
      },
      {
        "id": "diag-slotted-page",
        "title": "The Slotted Page Architecture",
        "description": "Microscopic view of a 4KB page showing Cell Pointers growing down and Cells growing up.",
        "anchor_target": "m5-btree",
        "level": "microscopic"
      },
      {
        "id": "diag-btree-split",
        "title": "B-tree Node Split Sequence",
        "description": "Step-by-step evolution of a page overflowing and a new median key being promoted to the parent.",
        "anchor_target": "m5-btree",
        "level": "microscopic"
      },
      {
        "id": "diag-index-lookup",
        "title": "The Double-Lookup Walk",
        "description": "Trace of an index seek: Trait 1 (Index B+tree) -> RowID -> Trait 2 (Table B-tree) -> Full Data.",
        "anchor_target": "m7-indexes",
        "level": "street"
      },
      {
        "id": "diag-journal-flow",
        "title": "Atomic Write Choreography",
        "description": "The 'Safe Sequence': 1. Write Journal, 2. fsync(), 3. Write Main DB, 4. Delete Journal.",
        "anchor_target": "m9-transactions",
        "level": "street"
      },
      {
        "id": "diag-wal-snapshots",
        "title": "WAL Concurrency: Reader vs Writer",
        "description": "A reader looking at the main file (Version 1) while a writer appends to the WAL (Version 2).",
        "anchor_target": "m10-wal",
        "level": "street"
      },
      {
        "id": "diag-nested-loop-join",
        "title": "Nested Loop Join Trace",
        "description": "Visualizing two cursors iterating: the outer loop row by row, the inner loop seeking via index.",
        "anchor_target": "m11-joins",
        "level": "microscopic"
      }
    ]
  },
  "accumulated_md": "# Architecting a Relational Engine: The SQLite Blueprint\n\nThis project is a deep-dive into the internals of a relational database management system (RDBMS). Instead of treating SQL as a black box, you will build the entire vertical stack: from a string-consuming lexer to a bytecode-executing virtual machine, down to a page-managed B-tree storage engine. The journey mimics the evolution of data storage\u2014moving from simple file appending to complex, ACID-compliant transactional systems.\n\n\n\n<!-- MS_ID: build-sqlite-m1 -->\n# Milestone 1: The SQL Tokenizer (Lexer)\n\nIn the architecture of a database, the **Tokenizer** (or Lexer) is the gatekeeper. It is the first component to touch a raw SQL string, transforming a chaotic stream of characters into a structured sequence of **tokens**. Before your engine can understand the *meaning* of a query, it must identify the *parts* of the query.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n### The Tension: Structure vs. Ambiguity\n\nIf you were building a simple CLI tool, you might be tempted to use `split(' ')` to break a command into words. In a database engine, this approach fails immediately. Consider the SQL statement:\n\n```sql\nSELECT 'Value with spaces', \"Table Name\", 123.45 FROM users;\n```\n\nA naive split by spaces would break `'Value with spaces'` into three separate chunks, destroying the string literal. It would fail to distinguish the comma from the identifier preceding it. \n\nThe fundamental tension in tokenization is **Speed vs. Contextual Precision**. The tokenizer must be incredibly fast\u2014it processes every single byte of every query\u2014but it must also understand that a character's meaning changes based on what came before it. A single quote `'` is just a character until it starts a string; once it starts a string, every character following it (including spaces and semicolons) is treated as data, not syntax, until the closing quote appears.\n\n---\n\n### The Three-Level View: From Bytes to Meaning\n\nTo understand how the tokenizer fits into the \"Build Your Own SQLite\" journey, look at the transition layers:\n\n1.  **Level 1 \u2014 The Raw Buffer (Source)**: A contiguous array of UTF-8 bytes in memory. At this level, `SELECT` is just the byte sequence `[83, 69, 76, 69, 67, 84]`.\n2.  **Level 2 \u2014 The Token Stream (Lexer)**: The output of this milestone. A list of objects where `SELECT` is identified as `TOKEN_SELECT`, and `123.45` is identified as `TOKEN_NUMERIC_LITERAL`.\n3.  **Level 3 \u2014 The Abstract Syntax Tree (Parser)**: The next milestone. Here, the tokens are arranged into a tree structure that represents the logic of the command (e.g., \"This is a SELECT operation targeting these columns\").\n\n---\n\n### The Soul of the Lexer: Finite State Machines (FSM)\n\nThe most robust way to build a tokenizer is through a **Finite State Machine**. \n\n> \n> **\ud83d\udd11 Foundation: A Finite State Machine**\n> \n> ### 1. What it IS\nA Finite State Machine (FSM) is a system that organizes logic into a fixed set of \"modes\" (States). At any given moment, the system exists in exactly one state\u2014never two at once, and never in between. To move from one state to another, a specific event must occur (a Transition). \n\nThink of a simple lamp: it has two states (**Off** and **On**). The transition is the act of \"flipping the switch.\" You cannot be in the \"On\" state without that transition occurring, and the lamp cannot be both \"On\" and \"Off\" simultaneously.\n\n### 2. WHY you need it right now\nIn intermediate development, we often fall into the trap of \"Boolean Soup\"\u2014using multiple true/false flags to manage complex logic (e.g., `isLoading`, `isError`, `isAuthenticated`). As the project grows, these flags inevitably conflict, leading to \"impossible\" bugs where a loading spinner and an error message show up at the same time.\n\nAn FSM replaces those messy variables with a single source of truth. By defining exactly which states are possible and how they connect, you make it impossible for the application to enter an invalid configuration. If your app is in the `SUCCESS` state, the FSM rules ensure it cannot accidentally trigger a `LOADING` logic block unless a specific \"retry\" or \"refresh\" event is fired.\n\n### 3. Key Insight: Make Impossible States Unrepresentable\nThe most powerful mental model for an FSM is **the guardrail**. Instead of writing code to \"check\" if something is wrong, you design the system so that the \"wrong\" thing literally cannot exist. If there is no transition defined between `LOGGED_OUT` and `PURCHASE_CONFIRMED`, the user simply cannot bypass the checkout flow, no matter what buttons they click or APIs they hit. The state itself dictates the available logic.\n\n\nIn your lexer, the \"State\" tells you how to interpret the current character. \n- In the **START** state, a digit `5` transitions you to the **NUMBER** state. \n- In the **START** state, a single quote `'` transitions you to the **STRING_LITERAL** state.\n- In the **STRING_LITERAL** state, a space is just another character to append to the value, not a separator.\n\n#### The \"Peek and Consume\" Pattern\nTo implement this FSM, you will use two primary pointers:\n1.  **Start Pointer**: Marks the beginning of the current lexeme (the raw text of the token).\n2.  **Current Pointer**: Scans forward to find the end of the token.\n\nYou will often need to **Peek** at the next character without moving the `Current` pointer. This is essential for operators like `>=` or `!=`. When you see `>`, you must peek at the next byte. If it's `=`, you consume both and return `GREATER_EQUAL`. If it's not, you return `GREATER` and leave the next character for the next iteration.\n\n---\n\n### Anatomy of a Token\n\nIn your implementation, a token is more than just a string. It is a structure (or class) containing metadata that the parser and error-reporter will need later.\n\n```rust\n// Conceptual structure (Adapt to your language)\nstruct Token {\n    type: TokenType,    // e.g., SELECT, IDENTIFIER, COMMA\n    lexeme: String,     // The exact text from the query (e.g., \"users\")\n    literal: Object,    // The parsed value (e.g., the number 123.45)\n    line: int,          // Line number for error reporting\n    column: int,        // Column number for error reporting\n}\n```\n\n**Why include line and column?** \nImagine a user submits a 1,000-line script with a typo on line 452. If your database just says \"Syntax Error,\" the user will be frustrated. By tracking the position during tokenization, you enable **Error Localization**, allowing you to point exactly at the offending character.\n\n---\n\n### The Revelation: The Escaped Quote Paradox\n\nMany developers assume that a string literal is simply \"everything between two quotes.\" SQLite (and standard SQL) introduces a wrinkle: **the double-single-quote escape**.\n\n```sql\n'It''s a beautiful day'\n```\n\nIn SQL, to include a single quote inside a string literal, you use two single quotes. If your tokenizer stops at the second `'` it sees, it will incorrectly truncate the string to `'It'` and then fail when it encounters `s a beautiful...`.\n\n**The Solution:** When your FSM is in the `STRING_LITERAL` state and encounters a `'`, it must **peek** at the next character. \n1. If the next character is *also* a `'`, it's an escaped quote. You consume both, append a single `'` to your literal value, and stay in the `STRING_LITERAL` state.\n2. If the next character is *not* a `'`, the string is finished.\n\n---\n\n### Handling Numbers: Integers vs. Floats\n\nYour tokenizer needs to distinguish between `42` (Integer) and `3.14` (Float/Real). This is important because the storage engine (Level 3) treats them differently to save space and maintain precision.\n\nThe logic follows a specific path:\n1. Consume all digits.\n2. If the next character is a dot `.` and the character after *that* is a digit, you have a float.\n3. Consume the dot and the remaining digits.\n\n*Note on Negative Numbers:* In many SQL implementations, the `-` in `-7` is actually a **Unary Operator** applied to the literal `7` during the parsing phase. However, for this project, you may choose to handle negative literals directly in the tokenizer if it simplifies your initial parser.\n\n---\n\n### Keywords and Identifiers: The Triage\n\nIn SQL, `SELECT` is a keyword, but `select_count` might be a column name (an identifier). To a tokenizer, they both look the same: a sequence of alphanumeric characters starting with a letter.\n\nThe standard pattern is **Triage**:\n1.  Scan the sequence as an **Identifier**.\n2.  Once the sequence is finished (e.g., you hit a space or comma), check if the text matches a known **Keyword** list (SELECT, FROM, WHERE, etc.).\n3.  **Case Insensitivity**: SQL keywords are case-insensitive. Your check should compare `select`, `Select`, and `SELECT` against the same keyword type.\n4.  If it's not a keyword, it's an identifier.\n\n#### Quoted Identifiers\nSometimes users want table names with spaces or keywords as names (e.g., `CREATE TABLE \"Order\" ...`). In SQL, double quotes `\"` are used for identifiers, while single quotes `'` are used for string literals. Your tokenizer must handle both states separately.\n\n---\n\n### Design Decisions: Manual FSM vs. Regex\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **Manual FSM (Chosen \u2713)** | Maximum performance, no external dependencies, perfect error reporting. | More \"boilerplate\" code. | SQLite, Postgres, V8 (JavaScript) |\n| Regular Expressions | Quick to write for simple patterns. | Can be slow (backtracking), difficult to handle complex nested escapes like `''`. | Simple scripting tools |\n\n**Why we choose Manual FSM:** \nIn the **Data Storage** domain, performance is paramount. A manual state machine allows us to iterate through the string exactly once (O(n) complexity) with zero allocations until we are sure we need to create a Token object.\n\n---\n\n### Implementation Roadmap\n\n1.  **Define your Token Types**: Create an enum/list of all keywords (SELECT, INSERT, etc.), operators (=, <, >, etc.), and literals (STRING, NUMBER).\n2.  **The Loop**: Create a `scan_token` function that reads the next character and decides which state to enter.\n3.  **Whitespace**: Skip spaces, tabs, and newlines, but increment your `line` counter on newlines.\n4.  **The Switch/Match**:\n    *   `(` -> `LEFT_PAREN`\n    *   `,` -> `COMMA`\n    *   `-` -> Check if it's the start of a comment `--` or an operator.\n    *   `'` -> Enter `string()` state.\n    *   `\"` -> Enter `identifier()` state.\n    *   `0-9` -> Enter `number()` state.\n    *   `a-z/A-Z` -> Enter `keyword_or_identifier()` state.\n5.  **Error Handling**: If a character doesn't match any state (e.g., a random `@` symbol), generate an error including the current line and column.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\nBy completing this tokenizer, you have unlocked concepts used in almost every area of software engineering:\n\n*   **Compiler Theory**: You've just built the \"Frontend\" of a compiler. This same logic allows languages like Rust, Python, or Java to understand the text you write.\n*   **Protocol Parsing**: When a web server reads an HTTP request (e.g., `GET /index.html HTTP/1.1`), it uses a tokenizer very similar to yours to identify the method, path, and version.\n*   **Security (Lexical Scoping)**: Many SQL Injection attacks rely on \"breaking out\" of a string literal. By building the lexer yourself, you see exactly how an attacker might use an unescaped `'` to end a string prematurely and start injecting new commands.\n\n### System Awareness\nIn the broader system, your Tokenizer sits between the **User Interface (REPL/API)** and the **Parser**. If the tokenizer returns a bad stream, the parser will attempt to build an invalid AST, eventually causing the Virtual Machine (VDBE) to execute the wrong operations. \n\n> \ud83d\udd2d **Deep Dive**: The SQLite tokenizer is actually generated using a tool called `RE2C`, which compiles a high-level description of tokens into a highly optimized C state machine. While we are writing ours by hand for learning, production databases often \"compile\" their tokenizers for maximum speed.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m2 -->\n<!-- MS_ID: build-sqlite-m2 -->\n# Milestone 2: The SQL Parser (AST)\n\nIf the Tokenizer was the gatekeeper, the **Parser** is the Architect. Its job is to take the flat, linear stream of tokens you generated in the previous milestone and arrange them into a hierarchical, logical structure known as an **Abstract Syntax Tree (AST)**. \n\nIn the lifecycle of a query, the Parser is where \"strings\" become \"intent.\" The machine doesn't care about the word `SELECT`; it cares about the fact that you are initiating a **Read Operation** targeting specific **Data Sources** filtered by a set of **Logical Predicates**.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n### The Fundamental Tension: Ambiguity vs. Precedence\n\nThe greatest challenge in parsing SQL is not just identifying keywords, but resolving the **recursive nature of human logic**. Consider the following `WHERE` clause:\n\n```sql\nWHERE status = 'active' OR priority = 1 AND category = 'urgent'\n```\n\nDoes this mean \"find active items, OR find items that are both priority 1 and urgent\"? Or does it mean \"find items that are either active or priority 1, and also must be urgent\"? \n\nIn mathematics and logic, we solve this with **Operator Precedence**. Just as multiplication happens before addition ($1 + 2 \\times 3 = 7$, not $9$), the logical `AND` operator in SQL \"binds tighter\" than `OR`. A naive, linear parser that simply reads tokens from left to right would get this wrong every time. \n\nThe tension here is **Computational Simplicity vs. Mathematical Correctness**. To build a parser that respects these rules, you cannot simply loop through tokens; you must use a strategy that allows the parser to \"look ahead\" and \"climb\" levels of importance.\n\n---\n\n### The Three-Level View: Constructing the Logic\n\nTo visualize the Parser's role, look at how a single query transforms across the stack:\n\n1.  **Level 1 \u2014 Token Stream (Input)**: `[SELECT, IDENTIFIER(name), FROM, IDENTIFIER(users), WHERE, IDENTIFIER(id), EQUAL, NUMBER(5)]`\n2.  **Level 2 \u2014 Abstract Syntax Tree (Process)**: A tree where the root is a `SelectStatement`. One branch points to the \"Result Columns\" (`name`), another to the \"Source Table\" (`users`), and a third to a binary expression node (`id == 5`).\n3.  **Level 3 \u2014 Virtual Machine Bytecode (Next Milestone)**: The tree is \"flattened\" into instructions like `OpenTable`, `SeekRowid`, and `ResultRow`.\n\n---\n\n### The Blueprint: What is an AST?\n\nBefore you write a single line of parsing logic, you must define the **Nodes** of your tree. In a language like Rust or C++, this usually involves a series of nested structures or enums.\n\n> \n> **\n> **\ud83d\udd11 Foundation: The Abstract Syntax Tree**\n> \n> **What it IS**\nAn Abstract Syntax Tree (AST) is a tree-based data structure that represents the logical structure of source code. After a lexer turns your code into a flat list of tokens (like `[LET, X, EQUALS, 5]`), the parser organizes those tokens into a hierarchy. It is called \"abstract\" because it ignores \"syntactic sugar\" that doesn't affect the program's meaning\u2014such as semicolons, parentheses, and whitespace\u2014focusing entirely on the relationships between operations and data.\n\n**Why you need it right now**\nComputers cannot \"understand\" a flat string of text. To evaluate an expression like `5 + 2 * 10`, the computer needs to know that the multiplication happens first, despite the addition appearing earlier in the string. The AST encodes this order of operations into its shape: the multiplication node will be a child of the addition node (or vice versa, depending on your evaluation strategy), making it trivial for an interpreter or compiler to traverse the tree and execute the code in the correct logical order.\n\n**Key Insight**\n**The AST is the \"Skeleton\" of your code.** While the source code is the \"skin\" (full of visual details for humans), the AST is the underlying structure that defines how the program actually functions. Once you have a valid AST, the original text format of the code no longer matters.\n**\n> An Abstract Syntax Tree is a tree representation of the abstract syntactic structure of source code. \"Abstract\" means it doesn't represent every detail of the syntax (like parentheses or semicolons), but rather the structural relationship between components. For example, in the expression `(1 + 2)`, the AST node is a `BinaryExpression(lhs: 1, op: +, rhs: 2)`. The parentheses are \"abstracted\" away because their meaning\u2014grouping\u2014is already captured by the tree's structure itself.\n\nFor this milestone, your AST needs three primary \"families\" of nodes:\n\n1.  **Statements**: The top-level actions (`CreateStatement`, `InsertStatement`, `SelectStatement`).\n2.  **Expressions**: Logic that evaluates to a value (`LiteralExpression`, `BinaryExpression`, `UnaryExpression`).\n3.  **Definitions**: Metadata for creating structures (`ColumnDefinition`, `Constraint`).\n\n---\n\n### The Soul of the Parser: Recursive Descent\n\nHow do you turn a list into a tree? You use **Recursive Descent**.\n\nRecursive Descent is a top-down parsing technique where you write one function for every \"rule\" in your grammar. These functions call each other recursively to match the structure of the SQL.\n\nImagine your parser is like a foreman at a construction site.\n- The `parse_statement()` function looks at the first token. \n- If it sees `CREATE`, it delegates the job to `parse_create_table()`.\n- If it sees `SELECT`, it delegates to `parse_select()`.\n- `parse_select()` itself delegates parts of its job: \"Hey, `parse_expression()`, go figure out what's in this `WHERE` clause and give me back a tree node.\"\n\n#### The \"Peek and Match\" Pattern\nYour parser will maintain a \"current\" index in the token stream. You will use two helper methods constantly:\n- **Match(type)**: If the current token is of the expected type, consume it and return true.\n- **Consume(type, error_message)**: If the current token is the expected type, move forward. If not, throw a syntax error.\n\n---\n\n### The Revelation: Expression Precedence (Pratt Parsing)\n\nThe \"Recursive Descent\" model works beautifully for high-level statements like `SELECT * FROM table`, but it struggles with expressions like `5 + 3 * 2`. If `parse_expression` is too simple, it will treat it as `(5 + 3) * 2`.\n\nThe solution used by professional engines (and the one you should implement) is **Precedence Climbing** (a simplified form of Pratt Parsing).\n\n> \n> **\n> **\ud83d\udd11 Foundation: Pratt Parsing / Precedence Climbing**\n> \n> **What it IS**\nPratt Parsing (also known as Top-Down Operator Precedence parsing) is an elegant algorithm used to parse expressions. Unlike standard recursive descent parsers, which require a different function for every level of operator precedence (e.g., `parseAddition`, `parseMultiplication`), a Pratt parser uses a single loop and a table of \"binding powers\" (numerical weights) to determine how tokens should be grouped.\n\n**Why you need it right now**\nParsing mathematical expressions or complex logic is the \"messy\" part of writing a parser. If you use standard recursive descent, your code becomes deeply nested and difficult to maintain as you add more operators (like `==`, `&&`, or `^`). Pratt parsing allows you to add new operators by simply assigning them a precedence number and a function, keeping your parser flat, readable, and highly extensible.\n\n**Key Insight**\n**Think of operators as magnets with different \"Pull Strengths.\"** In the expression `1 + 2 * 3`, the `*` operator has a stronger \"binding power\" (magnetic pull) than the `+`. When the parser looks at the `2`, it sees both operators and allows the `*` to \"win\" the `2` because its pull is stronger. The Pratt parser is simply a system that lets tokens compete for their operands based on these weights.\n**\n> Pratt Parsing associates a **precedence level** (an integer) with every operator. When parsing an expression, the parser \"climbs\" up the precedence levels. If it encounters an operator with a higher priority than the current one, it recursively calls itself to handle that \"tighter\" bond before finishing the current operation. \n> \n> **SQL Precedence Hierarchy (Low to High):**\n> 1. `OR`\n> 2. `AND`\n> 3. `NOT`\n> 4. `IS`, `MATCH`, `LIKE`, `IN`\n> 5. `=`, `!=`, `<`, `<=`, `>`, `>=`\n> 6. `+`, `-`\n> 7. `*`, `/`, `%`\n> 8. Unary minus (`-`), Bitwise NOT (`~`)\n\n**The Algorithm in Action:**\nWhen parsing `A OR B AND C`:\n1. The parser starts at level 0 (`OR` level).\n2. It parses `A`.\n3. It sees `OR`. Since `OR` is at level 1, it prepares to parse the right-hand side.\n4. Before it finishes `OR`, it looks at `B`. \n5. It sees `AND`. Since `AND` has a **higher** precedence (level 2) than `OR`, the parser recursively calls `parse_expression(level: 2)`.\n6. This ensures `B AND C` is grouped together as a single node *before* the `OR` node is finalized.\n\n---\n\n### Implementing the Statement Parsers\n\n#### 1. The `CREATE TABLE` Parser\nThis is a \"structural\" parser. It doesn't involve much logic but requires strict adherence to the schema.\n- **Table Name**: Usually an identifier.\n- **Columns**: A loop that consumes an identifier (name), a keyword (type like `INTEGER`, `TEXT`), and optional constraints.\n- **Constraints**: You must handle `PRIMARY KEY`, `NOT NULL`, and `UNIQUE`. \n\n*Design Note*: In SQLite, `INTEGER PRIMARY KEY` is special\u2014it makes the column an alias for the internal `rowid`. Your AST should preserve this information.\n\n#### 2. The `INSERT` Parser\nThe `INSERT` statement follows a rigid pattern: `INSERT INTO [table] ([columns...]) VALUES ([expressions...])`.\n- **Ambiguity Check**: The column list is optional. If you don't see `(`, you must assume the values map to the table's columns in the order they were defined.\n- **Values**: Each row in `VALUES` is a list of expressions. This is where your expression parser is first put to the test.\n\n#### 3. The `SELECT` Parser\nThis is the most complex component. A `SELECT` statement has many optional parts that must appear in a specific order:\n1.  **Columns**: Can be `*` (All) or a comma-separated list of expressions.\n2.  **FROM**: The source table.\n3.  **WHERE**: An optional expression node.\n4.  **ORDER BY**: An optional list of columns and directions (`ASC`/`DESC`).\n5.  **LIMIT**: An optional integer.\n\n**The \"Projection\" Concept:** In database theory, selecting specific columns is called a **Projection**. Your AST node for `SelectStatement` should store a list of `ResultColumn` objects, which handle both the expression (e.g., `price * 1.05`) and the alias (e.g., `AS adjusted_price`).\n\n---\n\n### Handling the NULL Mystery\n\nIn SQL, `NULL` is not just \"nothing\"\u2014it is a distinct state representing \"Unknown.\" \n\nWhen parsing expressions, you must ensure that `NULL` is treated as a **Literal Expression**, similar to the number `5` or the string `'hello'`. A common pitfall is treating `NULL` as an identifier (like a column name). If your tokenizer didn't distinguish them, your parser must.\n\n**Three-Valued Logic (3VL):** \nWhile the parser just builds the tree, you must ensure your binary expression nodes can support SQL's unique logic:\n- `TRUE AND NULL` is `NULL`\n- `FALSE AND NULL` is `FALSE`\n- `NULL = NULL` is `NULL` (not TRUE!)\n\nThis logic will be executed in the VDBE milestone, but your AST nodes must be ready to carry these values.\n\n---\n\n### Error Handling: The Panic Mode\n\nA parser that crashes on the first error is useless for debugging. You need a strategy for **Syntax Error Recovery**.\n\nWhen the parser encounters a token it doesn't expect (e.g., `SELECT FROM WHERE`), it should:\n1.  Report the error, using the `line` and `column` from the offending token.\n2.  Enter **Panic Mode**: Discard tokens until it finds a \"synchronization point\"\u2014usually a semicolon `;`.\n3.  Resume parsing after the semicolon to find more errors in subsequent statements.\n\n> \ud83d\udd2d **Deep Dive**: SQLite uses a parser generator called **Lemon**. Unlike the more famous `Yacc` or `Bison`, Lemon is designed to be thread-safe and highly efficient for embedded systems. It generates a \"Push Down Automaton\" parser in C. For this project, writing your own Recursive Descent parser gives you much more insight into the \"mechanics of thought\" inside a database.\n\n---\n\n### Design Decisions: To AST or not to AST?\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **AST (Chosen \u2713)** | Decouples syntax from execution. Allows for query optimization and re-writing. | More memory overhead to store the tree. | PostgreSQL, MySQL, Modern SQLite |\n| Single-Pass Compilation | Extremely fast; generates bytecode while reading tokens. | Very difficult to optimize queries or handle complex JOIN re-ordering. | Early versions of SQLite/BASIC |\n\nBy building an AST, you are future-proofing your database. It allows you to implement a **Query Planner** (Milestone 8) that can look at the tree and say, \"Wait, I can use an index for this WHERE clause instead of a full scan.\"\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **Recursive Descent & Hierarchical Logic**: This pattern is the foundation of every programming language. If you can parse SQL, you can parse JSON, HTML, or your own custom scripting language.\n2.  **Precedence Climbing & Math Engines**: This logic is exactly how scientific calculators work. Every time you type an equation, a \"Pratt Parser\" is building a tree in the background.\n3.  **ASTs & Static Analysis**: Tools like ESLint, Prettier, or the Rust compiler's borrow checker all operate on ASTs. Understanding how to build and traverse a tree is the \"master key\" to advanced tooling.\n\n### System Awareness\nYour Parser is the bridge between the **Frontend** (UI/Lexer) and the **Execution Engine** (VDBE). If your parser is too strict, users will find the database \"fussy.\" If it's too loose, you will pass \"garbage\" trees to the Virtual Machine, leading to crashes at runtime. The Parser's job is to ensure that by the time a query reaches the VM, it is **syntactically perfect**.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m3 -->\n<!-- MS_ID: build-sqlite-m3 -->\n# Milestone 3: The Bytecode Compiler (VDBE)\n\nYou have successfully transformed raw SQL strings into a logical Abstract Syntax Tree (AST). Now, you face the most critical architectural transition in the engine: turning that \"intent\" into \"execution.\" \n\nIn this milestone, you will build the **Virtual Database Engine (VDBE)**. This is the heart of the database\u2014a custom-built, register-based virtual machine designed specifically to manipulate B-trees and records. You will write a **Compiler** that flattens your recursive AST into a linear sequence of **Opcodes**, and an **Executor** that runs those instructions in a high-speed loop.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n### The Tension: The Cost of Flexibility\n\nThe fundamental tension at this stage is **High-Level Abstraction vs. Low-Level Efficiency**. \n\nSQL is a declarative language; the user says *what* they want, not *how* to get it. Your AST reflects this. However, a CPU cannot execute an AST. If you chose to execute queries by \"walking the tree\" (visiting nodes and evaluating them recursively), you would encounter a massive performance wall:\n\n1.  **Pointer Chasing**: Every node in an AST is likely a separate heap allocation. Traversing them causes frequent cache misses.\n2.  **Branch Misprediction**: A tree-walker is filled with `if/else` or `match` statements to handle different node types. Modern CPUs hate unpredictable branches; they thrive on tight, linear loops.\n3.  **Redundant Evaluation**: In a 1,000,000-row scan, a tree-walker evaluates the same \"shape\" of the `WHERE` clause 1,000,000 times, wasting cycles on structural overhead rather than data processing.\n\nTo solve this, we compile the query into a specialized **Instruction Set Architecture (ISA)**. We trade the flexibility of the tree for the raw speed of a linear bytecode program.\n\n---\n\n### The Revelation: Databases Don't \"Read\" SQL, They Run Programs\n\nMany developers assume that when they run `SELECT * FROM users`, the database engine is \"interpreting\" the SQL as it goes. \n\n**The Reveal:** In professional engines like SQLite, the SQL is actually a **programming language for a hidden computer.** When you submit a query, the engine compiles it into a binary program. The \"Virtual Machine\" (VDBE) then runs this program. The \"database\" is less like a calculator and more like a Just-In-Time (JIT) compiler for a very specific, data-centric assembly language.\n\nIf you run `EXPLAIN SELECT * FROM users;` in a real SQLite shell, you won't see a tree\u2014you will see a list of assembly-like instructions. This is exactly what you are about to build.\n\n---\n\n### The Architecture: Register-Based VM\n\nYour VDBE will be a **Register-Based Virtual Machine**.\n\n> \n> **\ud83d\udd11 Foundation: Registers vs Stack**\n> \n> ### 1. What it IS\nIn Virtual Machine design, **Registers** and **Stacks** are the two primary ways a VM manages data during computation.\n\n*   **Stack-based Architecture:** Operations rely on a Last-In, First-Out (LIFO) data structure. To add two numbers, you \"push\" them onto the stack. The `ADD` instruction implicitly knows to pop the top two values, add them, and push the result back.\n    *   *Example:* `PUSH 5`, `PUSH 10`, `ADD` (Result `15` is now on top).\n*   **Register-based Architecture:** Operations use a set of named, fast-access \"slots\" (registers). Instructions must explicitly state which registers hold the inputs and where to store the output.\n    *   *Example:* `ADD R1, R2, R3` (Take the value in `R1`, add it to `R2`, and store it in `R3`).\n\n### 2. WHY you need it right now\nIf you are building or analyzing a VM, this choice dictates your entire **Instruction Set Architecture (ISA)**.\n\n*   **Implementation Ease:** Stack machines (like the JVM or CPython) are significantly easier to write compilers for. You don't have to worry about \"register allocation\"\u2014deciding which variable goes in which slot. You just push everything onto the stack as you see it.\n*   **Performance:** Register machines (like Lua or Dalvik) generally require fewer instructions to perform the same task. In a stack VM, a simple addition takes three instructions (`push`, `push`, `add`); in a register VM, it takes one. Since the \"dispatch loop\" (the overhead of fetching the next instruction) is the slowest part of a VM, fewer instructions often lead to better performance.\n\n### 3. ONE key insight to remember\n**Stack machines have implicit operands; Register machines have explicit operands.** \n\nThink of a **Stack machine** like a **shared notepad**: you only ever look at the very bottom line to do work. It\u2019s simple, but you spend a lot of time moving data up and down. Think of a **Register machine** like a **professional workbench**: you have specific tools in specific spots. It's more complex to organize, but you can grab exactly what you need without shuffling everything else out of the way.\n\n> ### Virtual Machine Architecture: Registers vs. Stack\n> There are two primary ways to design a VM: **Stack-based** (like the JVM or Python) and **Register-based** (like Lua or the VDBE).\n>\n> In a **Stack-based VM**, operations happen at the \"top of the stack.\" To add 5 + 2, you `PUSH 5`, `PUSH 2`, then call `ADD`. The `ADD` instruction pops both, adds them, and pushes `7`. It\u2019s simple to implement but requires many instructions.\n>\n> In a **Register-based VM**, you have a \"Register File\" (an array of memory slots). To add 5 + 2, you might have a single instruction: `ADD R1, R2, R3` (Add contents of R1 and R2, store in R3). This maps much more naturally to how database columns work (Column 1 -> Register 1) and significantly reduces the total number of instructions the VM must execute, leading to higher performance.\n\n#### The Register File\nYour VM needs a \"Register File\"\u2014a fixed-size array where each slot can hold a value of any supported type (Integer, Real, Text, Blob, or NULL). When the compiler processes an expression, it assigns a register to hold the intermediate result.\n\n#### The Cursor\nA unique feature of a Database VM is the **Cursor**. A cursor is a special pointer to a B-tree. It maintains state: which table it's looking at, which page it's on, and which row it's currently pointing to. Your bytecode will have instructions to `Open`, `Seek`, `Read`, and `Advance` these cursors.\n\n---\n\n### The Instruction Set (ISAs)\n\nYou need to define a set of Opcodes. For this milestone, you will implement at least these core categories:\n\n| Category | Opcode | Description |\n|----------|--------|-------------|\n| **Cursor** | `OpenRead` / `OpenWrite` | Open a cursor for a specific table B-tree. |\n| **Traversal** | `Rewind` | Move a cursor to the first entry in the table. |\n| **Traversal** | `Next` | Move to the next entry. Jump to a target if not at the end. |\n| **Data** | `Column` | Extract data from a column of the current row into a register. |\n| **Logic** | `Eq` / `Ne` / `Lt` / `Gt` | Compare two registers; jump if the condition is (not) met. |\n| **Output** | `ResultRow` | Take a range of registers and emit them as a result row. |\n| **Flow** | `Halt` | Stop execution and return. |\n\n\n![VDBE Execution Trace](./diagrams/diag-vdbe-step.svg)\n\n\n---\n\n### The Compiler: Translating AST to Opcodes\n\nThe Compiler's job is to traverse your AST and \"emit\" (write out) the corresponding instructions. This is where you implement the \"How\" of the query.\n\n#### Case Study: Compiling a Simple Scan\nConsider: `SELECT name FROM users;`\n\n1.  **Emit `OpenRead(cursor=0, table=\"users\")`**: Prepares the engine to read the \"users\" B-tree.\n2.  **Emit `Rewind(cursor=0, jump_to_halt)`**: Moves to the start. If the table is empty, jump to the end.\n3.  **Capture Label `LoopStart`**: (A symbolic marker for the top of the loop).\n4.  **Emit `Column(cursor=0, col=1, reg=1)`**: Assuming `name` is column 1.\n5.  **Emit `ResultRow(reg=1, count=1)`**: Hands the value in Register 1 to the user.\n6.  **Emit `Next(cursor=0, jump_to=LoopStart)`**: Move to the next row. If a row exists, go back to the top.\n7.  **Emit `Halt`**: Clean up.\n\n#### Case Study: Compiling a WHERE Clause\nConsider: `... WHERE age > 21`\n\nThe `WHERE` clause introduces a **Conditional Jump**. \n1.  Inside the loop, you first emit `Column(cursor=0, col=age_idx, reg=2)`.\n2.  Then emit `Le(reg=2, constant=21, jump_to=NextRow)`. \n    *   *Note the logic reversal:* If the age is Less than or Equal to 21, we \"jump over\" the `ResultRow` instruction directly to the `Next` instruction. This \"skips\" the row.\n\n---\n\n### Implementation Detail: The Fetch-Decode-Execute Loop\n\nThe \"Virtual Machine\" is essentially a large `while` loop that iterates over your instruction array.\n\n```rust\nfn execute(program: Vec<Instruction>) {\n    let mut pc = 0; // Program Counter\n    let mut registers = [Value::Null; 256];\n    let mut cursors = [Cursor::default(); 16];\n\n    while pc < program.len() {\n        let ins = &program[pc];\n        match ins.opcode {\n            Opcode::OpenRead => { /* Open table logic */ },\n            Opcode::Column => { \n                let row = cursors[ins.p1].current_row();\n                registers[ins.p3] = row.get_column(ins.p2);\n            },\n            Opcode::Next => {\n                if cursors[ins.p1].advance() {\n                    pc = ins.p2; // Jump back to loop start\n                    continue;\n                }\n            },\n            Opcode::Halt => return,\n            // ... other opcodes\n        }\n        pc += 1;\n    }\n}\n```\n\n**Optimization Tip: Dispatching**\nIn an **Expert** implementation, a `switch` or `match` inside a loop can be a bottleneck. The CPU's branch predictor struggles with a single \"mega-branch.\" Advanced engines use **Computed Gotos** (labels-as-values) to jump directly to the handler for the next instruction, effectively giving each opcode its own branch prediction slot.\n\n---\n\n### Register Allocation: Managing the File\n\nWhen compiling an expression like `(price * tax) + shipping`, your compiler needs three temporary registers to hold the results of `price * tax`, the constant `shipping`, and the final `+`.\n\nYou must implement a simple **Register Allocator**. \n- A simple strategy: Maintain a `next_available_register` counter. \n- As you go deeper into an expression tree, increment the counter. \n- Once you finish a branch of the tree, you can often \"reuse\" that register for the next branch.\n\n*Danger*: Be careful not to \"clobber\" (overwrite) a register that is still needed for a parent calculation!\n\n---\n\n### The `EXPLAIN` Command\n\nTo verify your work, you must implement the `EXPLAIN` keyword. When a query starts with `EXPLAIN`, your engine should NOT execute the bytecode. Instead, it should return the list of instructions as a result set.\n\nExample output for `EXPLAIN SELECT * FROM t`:\n```text\naddr  opcode      p1    p2    p3    comment\n----  ----------  ----  ----  ----  -------\n0     Init        0     1     0     \n1     OpenRead    0     2     0     root=2\n2     Rewind      0     5     0     \n3     Column      0     0     1     \n4     ResultRow   1     1     0     \n5     Next        0     3     0     \n6     Halt        0     0     0     \n```\n\nThis is your most powerful debugging tool. If your `SELECT` isn't returning data, `EXPLAIN` will tell you if your logic jumps are skipping every row by mistake.\n\n---\n\n### Performance Requirement: The 100ms Target\n\nYou are tasked with scanning 10,000 rows in under 100ms. On a modern machine, this is actually quite generous, but it forces you to ensure your VM loop is tight. \n\n- **Avoid allocations in the loop**: Do not create new `String` or `Vec` objects inside the `Column` or `Next` handlers. Reuse buffers.\n- **Minimize Dispatch Overhead**: Ensure your instruction structure is small (e.g., using a packed struct or bitfields) to keep it in the L1 cache.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **Virtual Machine Design**: The principles you use here\u2014Program Counters, Opcodes, and Register Files\u2014are the exact same principles used by the **Ethereum Virtual Machine (EVM)** for smart contracts and the **Lua** interpreter.\n2.  **Compiler Backend**: You are effectively building a \"Backend\" that targets a custom \"Chip\" (the VDBE). This mirrors how `LLVM` translates high-level code into machine instructions for x86 or ARM.\n3.  **Instruction Set Architecture (ISA)**: By deciding what Opcodes to include, you are performing \"Hardware-Software Co-design.\" If you add a `CountTableRows` opcode, you make `SELECT COUNT(*)` faster but make the VM more complex. This is the same trade-off between **RISC** (Reduced Instruction Set) and **CISC** (Complex Instruction Set) architectures.\n\n### System Awareness\nThe VDBE is the orchestrator. Upstream, it receives a plan from the Parser. Downstream, it calls methods on the **Buffer Pool** and **B-tree** layers. It bridges the gap between \"Logical SQL\" and \"Physical Bytes.\" Without a high-performance VDBE, even the fastest B-tree in the world would be bogged down by the slow interpretation of the query.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m4 -->\n# Milestone 4: The Buffer Pool Manager\n\nIn the previous milestones, you built the \"Brain\" of the database\u2014the Tokenizer, Parser, and Virtual Machine. Now, we descend into the \"Engine Room.\" This is where the abstract logic of SQL meets the cold, hard reality of spinning platters and NAND flash.\n\nIn this milestone, you will implement the **Buffer Pool Manager** (often called the **Pager** in SQLite nomenclature). This component is the bridge between the Virtual Machine (which wants to treat data as objects in memory) and the Disk (which only understands blocks of bytes).\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n### The Fundamental Tension: Persistence vs. Performance\n\nThe central conflict of data storage is a physical one: **RAM is fast but volatile; Disk is slow but permanent.** \n\nIf your database read from the disk every time the VDBE executed a `Column` instruction, your performance would crater. A single mechanical disk seek takes ~10 milliseconds. In that same time, a modern CPU can execute over 30 million instructions. To a CPU, waiting for a disk read is like a human waiting 10 years for a pizza delivery.\n\nTo solve this, we use a **Buffer Pool**\u2014a reserved chunk of memory that mirrors \"pages\" of the database file. \n\nBut here is the catch: You cannot fit a 1-terabyte database into 8 gigabytes of RAM. You must decide which data to keep and which to throw away. If you throw away the wrong data, you trigger a \"Cache Miss,\" forcing the CPU to wait for the disk. If you throw away data that hasn't been saved yet, you cause **Data Corruption**.\n\n---\n\n### The Revelation: The OS is a Liar\n\n> #### The \"Double Buffering\" Trap\n> \n> **The Misconception**: Many developers believe that calling `write()` to a file is enough to ensure data is \"on disk.\" They also believe the Operating System's built-in file cache is sufficient for a database.\n> \n> **The Reality**: The OS \"Page Cache\" is a general-purpose beast. It manages memory for word processors, web browsers, and background updates simultaneously. It has no idea that Page 42 and Page 108 of your database are logically linked in a B-Tree. \n> \n> If the OS decides to evict Page 42 while you are in the middle of a B-Tree split, and then the power goes out, your database is now a collection of random bytes. To guarantee **ACID (Atomicity, Consistency, Isolation, Durability)**, the database must take manual control of its memory. It must say to the OS: *\"Don't touch this. I will decide when these bytes hit the physical disk.\"*\n\n---\n\n### The Three-Level View: The Pager's Perspective\n\nTo understand how the Buffer Pool sits in your engine, look at the layers of data movement:\n\n1.  **Level 1 \u2014 The Cursor API (Internal)**: The B-Tree layer asks: \"Give me Page #7.\" It doesn't care if Page #7 is in RAM or on a disk in another country. It expects a pointer to memory it can read.\n2.  **Level 2 \u2014 The Buffer Pool (The Manager)**: This is what you are building. It checks a **Hash Map** to see if Page #7 is already in a \"Frame.\" If not, it finds an empty Frame, calls the Disk I/O layer, and loads the data. It also tracks if the page is \"Dirty\" (modified).\n3.  **Level 3 \u2014 Disk/OS I/O (The Metal)**: This layer performs the raw `pwrite()` and `pread()` calls. It uses `fsync` to force the hardware to actually commit bits to the physical medium.\n\n\n> **\ud83d\udd11 Foundation: The Pager is the specific submodule in SQLite that handles reading/writing fixed-size pages. It abstracts the file system into an array of pages.**\n> \n> 1. **What it IS**\nThe Pager is the layer of SQLite that sits between the B-Tree (which handles data logic) and the raw OS file system. Its primary job is to present the database file not as a stream of bytes, but as a zero-indexed array of fixed-size blocks called \"pages\" (typically 4KB). When the database needs to read data, it asks the Pager for \"Page #5\"; the Pager calculates the byte offset, reads it from the disk, and returns a pointer to a memory buffer containing that data.\n\n2. **WHY you need it right now**\nIn a database engine, you cannot simply read and write directly to the disk every time a value changes\u2014it is too slow and risks data corruption. The Pager solves three critical problems simultaneously:\n*   **Caching:** It maintains a \"page cache\" in RAM so that frequently accessed data doesn't require slow disk I/O.\n*   **Concurrency:** It manages locks to ensure that multiple processes don't write to the same page at the same time.\n*   **Atomicity:** It handles the \"Journal\" or \"Write-Ahead Log\" (WAL). By managing how pages are flushed to disk, the Pager ensures that if the power cuts out mid-transaction, the database remains in a consistent state.\n\n3. **ONE key insight or mental model**\n**The Virtual Array:** Think of the Pager as a **Virtual Array of RAM**. To the layers above it, the entire database looks like it's already loaded into a giant array in memory. The Pager's \"magic\" is swapping these array elements (pages) in and out of actual physical RAM behind the scenes, ensuring that what you \"write\" to the array is safely and atomically persisted to the disk.\n\n\n---\n\n### The Anatomy of the Buffer Pool\n\nYour Buffer Pool is essentially a collection of **Frames**. A Frame is a slot in memory exactly the size of a database **Page** (usually 4096 bytes).\n\n#### 1. The Page Frame\nA byte array (e.g., `uint8_t[4096]`). This holds the actual data.\n\n#### 2. The Page Descriptor (Metadata)\nFor every frame, you must track:\n- **Page ID**: Which part of the file does this frame currently represent?\n- **Pin Count**: How many active threads/cursors are currently looking at this page?\n- **Dirty Flag**: Has someone modified this page since it was loaded?\n- **Usage Metadata**: Information used by the eviction algorithm (e.g., a timestamp or a position in a list).\n\n---\n\n### The Soul of the Manager: The LRU Eviction Algorithm\n\nWhen the Buffer Pool is full and you need to load a new page, you must pick a victim to \"evict\" (remove from memory). The gold standard for this is **Least Recently Used (LRU)**.\n\n> \n> **\ud83d\udd11 Foundation: LRU (Least Recently Used) Caching**\n> \n> ### 1. What it IS\nLRU is a strategy for managing a finite amount of memory. It operates on a simple heuristic: data that was accessed recently is likely to be accessed again soon (Temporal Locality). When the cache is full, the item that hasn't been touched for the longest amount of time is removed to make room for new data.\n> \n> ### 2. WHY you need it right now\nIn a database, \"Hot\" pages (like the Root of a B-Tree) are accessed constantly. \"Cold\" pages (like a row in a massive table scan) might only be seen once. An LRU policy ensures that the Root stays in RAM while the scanned rows flow through the buffer pool without pushing out the most important data.\n> \n> ### 3. Key Insight: The O(1) Implementation\nTo make LRU efficient, you cannot simply search a list for the oldest item (that's $O(n)$). Instead, you use two data structures in tandem:\n1.  **A Doubly Linked List**: Stores the pages. When a page is accessed, you \"splat\" it to the front of the list. The tail of the list is always the \"Least Recently Used\" item.\n2.  **A Hash Map**: Maps `PageID` to the corresponding node in the Linked List. This allows you to find and move a page in $O(1)$ time.\n\n---\n\n### Mechanism: The Pinning Safety Valve\n\nThis is the most frequent source of bugs in custom databases. Imagine the following sequence:\n1.  The B-Tree layer asks for Page #10. The Buffer Pool loads it.\n2.  The B-Tree layer starts reading Page #10.\n3.  Suddenly, another thread triggers a massive load that fills the Buffer Pool.\n4.  The Buffer Pool sees that Page #10 was the \"Least Recently Used\" and decides to evict it to make room.\n5.  **CRASH**: The B-Tree layer is now holding a pointer to memory that has been overwritten with different data.\n\nTo prevent this, you must implement **Pinning**.\n- When a layer fetches a page, the `PinCount` increments.\n- An LRU algorithm **must never** evict a page whose `PinCount > 0`.\n- When the caller is done with the page, they must call `Unpin()`.\n\n**The Trade-off**: If your code forgets to `Unpin()`, you get a \"Buffer Pool Leak.\" Eventually, every page is pinned, and the database can no longer load new data, effectively locking up.\n\n---\n\n### Dirty Pages and the Write-Back Strategy\n\nWhen the VDBE executes an `INSERT`, it modifies a page in the Buffer Pool. We mark this page as **Dirty**. \n\nWe do **not** write it to disk immediately. Writing to disk is expensive. If a user performs 100 inserts on the same page, we'd rather write to disk once than 100 times.\n\n**The Lifecycle of a Dirty Page:**\n1.  Page is modified in the Frame. `is_dirty` set to `true`.\n2.  The page remains in the Buffer Pool as long as it's \"Hot.\"\n3.  The LRU algorithm eventually selects the dirty page for eviction.\n4.  **The Intercept**: Before the Frame can be reused, the Buffer Pool Manager sees the `is_dirty` flag. It triggers a `write()` to the disk.\n5.  Once the write is confirmed, the page is no longer dirty, and the Frame is cleared for new data.\n\n---\n\n### Design Decisions: Page Size\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **4096 Bytes (Chosen \u2713)** | Matches modern OS page sizes and SSD block sizes. Minimal \"Internal Fragmentation.\" | Higher overhead for very large rows. | SQLite (Default), Postgres |\n| 16KB / 64KB | Better throughput for large sequential scans. Fewer B-tree levels. | More \"Wasted\" I/O if you only need a 10-byte row. | MySQL (InnoDB), SQL Server |\n\n**Why we choose 4k**: Your operating system moves data in 4KB chunks (Pages). If your database uses a 4KB page size, a single database read maps perfectly to a single OS read. If you used 5KB, every database read would force the OS to perform two reads, doubling your I/O overhead.\n\n---\n\n### Implementation Roadmap\n\n#### 1. The Frame and Descriptor Structures\nDefine a `Page` object that holds a fixed-size buffer and the metadata (ID, Dirty, PinCount). Initialize a fixed array of these (the \"Pool\").\n\n#### 2. The FetchPage(page_id) Logic\nThis is your primary entry point:\n1.  Check `page_map` (Hash Map) for `page_id`.\n2.  **If Hit**: Move page to the \"front\" of LRU. Increment `PinCount`. Return pointer.\n3.  **If Miss**: \n    - Find a \"Victim\" using LRU (must have `PinCount == 0`).\n    - If Victim is `dirty`, `Flush(victim)`.\n    - Remove Victim from `page_map`.\n    - Read new data from disk into the Victim's Frame.\n    - Update `page_map` and metadata.\n    - Return pointer.\n\n#### 3. Tracking Hit Rate\nTo measure performance, keep two counters: `num_lookups` and `num_misses`.\n$$\\text{Hit Rate} = 1 - \\frac{\\text{misses}}{\\text{lookups}}$$\nIf your hit rate is low (e.g., < 80%), your Buffer Pool is too small for your workload, or your access patterns are highly random.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\nBy mastering the Buffer Pool, you have unlocked the architecture of almost all high-performance computing:\n\n1.  **CPU Caches (L1/L2/L3)**: Your Buffer Pool is a software implementation of exactly what your CPU hardware does. The CPU cache uses a \"Least Recently Used\" variant to decide which memory addresses to keep close to the cores.\n2.  **Virtual Memory**: Your Operating System treats your Hard Drive as \"Slow RAM\" using a Page Table. When you access memory that isn't in RAM, it triggers a \"Page Fault,\" which is functionally identical to a \"Buffer Pool Miss.\"\n3.  **Content Delivery Networks (CDNs)**: Systems like Cloudflare or Akamai act as a \"Buffer Pool for the Internet.\" They cache \"Pages\" (images/HTML) close to users and evict them based on popularity (LRU).\n4.  **Write-Back vs. Write-Through**: You implemented a \"Write-Back\" cache (write on eviction). \"Write-Through\" caches write to disk immediately. Write-through is safer; write-back is significantly faster.\n\n### System Awareness\nThe Buffer Pool is the foundation of **Durability**. In Milestone 9 (Transactions), you will learn how the Buffer Pool coordinates with the **Write-Ahead Log (WAL)**. The Buffer Pool must ensure that the WAL is written to disk *before* it evicts a dirty page. This is known as **Write-Ahead Logging protocol**, and without the manual control you are building now, it would be impossible to implement.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m5 -->\n# Milestone 5: B-tree Page Format & Table Storage\n\nYou have built the Virtual Machine (the logic) and the Buffer Pool (the memory manager). Now, you are about to implement the most iconic structure in database history: the **B-tree**. \n\nIn this milestone, you will design the physical layout of the data itself. You will transform a 4096-byte \"frame\" of raw memory into a structured **Slotted Page**, implement the serialization of SQL rows into binary payloads, and build the logic that keeps your B-trees balanced as they grow from a few rows to millions.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n### The Fundamental Tension: Flexibility vs. Fragmentation\n\nThe data storage layer faces a brutal physical reality: **SQL rows are variable-length, but disk pages are fixed-size.**\n\nA row containing the string `\"Hi\"` is much smaller than a row containing a 1,000-character bio. If you simply appended rows one after another in a page, you would eventually run out of space. If you then deleted a row from the middle, you would leave a \"hole.\" If the next row you try to insert is larger than that hole, you can't use the space. This is **External Fragmentation**, and in a database, it is a slow death by a thousand cuts.\n\nTo solve this, we cannot treat a page as a simple array. We must treat it as a **managed heap**. We need a format that allows rows to move around inside the page without breaking the \"pointers\" that the B-tree logic uses to find them.\n\n---\n\n### The Revelation: A Node is not an Object\n\nIf you were building a B-tree in an in-memory algorithms class, you would define a `Node` class with an array of `Key` objects and a list of `Child` pointers. \n\n**The Revelation:** In a database, a \"Node\" does not exist as an object in the heap. A Node **is** a Page. It is a 4096-byte array of `uint8_t`. \n- There are no \"pointers\" to memory addresses; there are only **Page IDs** (integers). \n- There are no \"objects\"; there are only **Byte Offsets** within the page.\n\nIf you want to find the 5th key in a node, you don't access `node.keys[4]`. You read the 2-byte integer at `page_buffer[offset + 10]` to find the location of the key's data elsewhere in the buffer. This shift from \"Object-Oriented Thinking\" to \"Buffer-Oriented Thinking\" is the hallmark of a systems engineer.\n\n---\n\n### The Slotted Page Architecture\n\nTo solve the fragmentation problem, SQLite (and almost every major RDBMS) uses the **Slotted Page** (or \"Offset Array\") layout. \n\n\n![The Slotted Page Architecture](./diagrams/diag-slotted-page.svg)\n\n\nIn this layout, the page is divided into four zones:\n1.  **The Header**: Fixed-size metadata at the very beginning of the page.\n2.  **The Cell Pointer Array**: A list of 2-byte offsets that grow **downward** from the top.\n3.  **The Unallocated Space**: The \"empty\" gap in the middle.\n4.  **The Cell Content Area**: The actual row data (cells), which grows **upward** from the bottom.\n\n#### Why this works:\nWhen you need to insert a new row, you put the data at the end of the \"Unallocated Space\" (moving the bottom boundary up) and add a 2-byte pointer to the \"Cell Pointer Array\" (moving the top boundary down). \n\nBecause the B-tree logic refers to rows by their **Index** in the pointer array (e.g., \"Give me the 3rd cell\"), you can rearrange the actual data in the Content Area (to defragment it) as long as you update the 2-byte pointer. The \"outside world\" never knows the data moved.\n\n---\n\n### The Anatomy of the Page Header\n\nEvery page in your database file must identify itself. For this milestone, your header (starting at byte 0 of every page) should contain:\n\n| Field | Size | Description |\n|-------|------|-------------|\n| **Page Type** | 1 byte | Identifies the page: `0x05` (Table Internal), `0x0D` (Table Leaf), `0x02` (Index Internal), `0x0A` (Index Leaf). |\n| **Free Block Offset**| 2 bytes | Pointer to the first \"hole\" in the page (for space reclamation). |\n| **Cell Count** | 2 bytes | How many rows/keys are stored in this page. |\n| **Cell Start** | 2 bytes | The offset to the start of the cell content area (the \"bottom\" boundary). |\n| **Right Child** | 4 bytes | (Internal Pages Only) The Page ID of the rightmost child. |\n\n> \ud83d\udd2d **Deep Dive**: **Endianness**. Your header must use **Big-Endian** (Network Byte Order) for all multi-byte integers. Why? Because if you save your database on an x86 machine (Little-Endian) and open it on an ARM machine (Big-Endian), the numbers must remain the same. Big-endian is the standard for binary file formats because it's easier for humans to read in a hex editor (the most significant bits come first).\n\n---\n\n### Row Serialization: The \"Cell\"\n\nA **Cell** is the unit of storage in a B-tree. In a **Table B-tree Leaf**, a cell contains:\n1.  **Payload Size**: (Varint) The size of the row data.\n2.  **RowID**: (Varint) The unique 64-bit integer key for this row.\n3.  **Payload**: The actual serialized column values.\n\n#### The Varint (Variable-Length Integer)\nTo save space, databases don't use a full 8 bytes for every integer. If a row's ID is `5`, using 64 bits is wasteful. We use a **Varint**.\n\n> **\ud83d\udd11 Foundation: Variable-Length Integers (Varints)**\n>\n> ### 1. What it IS\n> A Varint is a way of encoding integers using only as many bytes as necessary. Most implementations use the \"MSB (Most Significant Bit) Flag\" method. Each byte uses 7 bits for the number and 1 bit (the 8th bit) to signal if another byte follows. \n> - If the 8th bit is `1`, keep reading.\n> - If the 8th bit is `0`, this is the last byte.\n>\n> ### 2. WHY you need it right now\n> Databases are dominated by small numbers: row counts, lengths, and small IDs. Using a fixed 8-byte `int64` for a row length of `20` wastes 7 bytes. In a billion-row table, that's 7GB of wasted disk space and I/O.\n>\n> ### 3. Key Insight\n> **Varints trade CPU cycles for I/O bandwidth.** It takes a few extra CPU instructions to \"unpack\" a varint, but because disk I/O is the bottleneck, reducing the number of bytes we read from disk results in a massive net speed gain.\n\n---\n\n### Table B-trees vs. Index B+trees\n\nIn SQLite, tables and indexes use slightly different structures.\n\n#### 1. Table B-tree (Clustered Index)\nThe table itself is a B-tree keyed by a hidden (or explicit) `rowid`. \n- **Leaf Nodes**: Store the actual row data (the values for every column).\n- **Internal Nodes**: Store only the `rowid` and the `Page ID` of the child. They act as a \"map\" to find which leaf contains the row you want.\n\n#### 2. Index B+tree (Secondary Index)\nA secondary index (like `CREATE INDEX idx_name ON users(name)`) is a separate B+tree.\n- **Leaf Nodes**: Store the **Indexed Value** (e.g., \"Alice\") and the **rowid** (e.g., 5). It does *not* store the other columns.\n- **Internal Nodes**: Store only keys for navigation.\n\n**Why the difference?**\nBy storing the full row data in the Table B-tree leaves, we ensure that once we find the `rowid`, we have the data. This is called a **Clustered Index**. It makes `SELECT * WHERE rowid = ?` extremely fast.\n\n---\n\n### The Algorithm: Node Splitting\n\nThe most complex part of this milestone is the **Split**. When you attempt to insert a cell into a page that is full (i.e., the \"Unallocated Space\" is smaller than the cell size), you must split the node.\n\n\n![B-tree Node Split Sequence](./diagrams/diag-btree-split.svg)\n\n\n**The Split Sequence:**\n1.  **Create a New Page**: Ask the Pager for a new, empty page.\n2.  **Find the Median**: Identify the \"middle\" cell in the full page.\n3.  **Move the Content**: Move all cells *after* the median to the new page.\n4.  **Promote the Key**: \n    - Take the key of the median cell.\n    - Insert it into the **Parent Node**.\n    - Set the Left Child of that key to the original page.\n    - Set the Right Child of that key to the new page.\n5.  **Root Split**: If the Root is full, you create two new children, move the root's content into them, and the Root becomes a simple internal node with one key and two children. This is the only way a B-tree grows in height!\n\n---\n\n### The System Catalog (`sqlite_master`)\n\nHow does the database know where the `users` table starts? It needs a \"Root of all Roots.\"\n\nYou must reserve **Page 1** of your database file for the **System Catalog**. This is a special Table B-tree that stores the schema. Every time you run `CREATE TABLE`, you insert a row into Page 1:\n- `type`: \"table\" or \"index\"\n- `name`: \"users\"\n- `root_page`: The Page ID where the B-tree starts.\n- `sql`: The original SQL text (for recreation).\n\nWhen your database starts, it reads Page 1, builds an in-memory map of table names to Root Page IDs, and uses this to bootstrap the VDBE.\n\n---\n\n### Implementation Roadmap\n\n1.  **Binary Utilities**: Write functions to read/write Big-Endian integers and Varints from a byte buffer.\n2.  **Page Wrapper**: Create a class/struct that takes a 4096-byte buffer and provides methods like `get_cell_count()`, `insert_cell(data)`, and `delete_cell(index)`.\n3.  **The B-tree Logic**:\n    - `find_leaf(key)`: Traverse from the root to find the leaf page that *should* contain the key.\n    - `insert(key, payload)`: Find the leaf, insert the data. If full, trigger `split()`.\n4.  **Serialization**: Write a \"Record Encoder\" that takes a list of SQL values (from the VDBE) and produces the binary payload for a cell.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **Slotted Pages in Modern Hardware**: The slotted page layout is used by **PostgreSQL, MySQL (InnoDB), and SQL Server**. Even modern \"In-Memory\" databases often use a variation of this to manage memory fragmentation.\n2.  **Tree Balancing (O(log N))**: By keeping the B-tree balanced through splits, you guarantee that searching for one row in a 1,000,000-row table only takes about $\\log_{100}(1,000,000) \\approx 3$ page reads. This is the \"Magic of Databases.\"\n3.  **Zero-Copy Networking**: The way you build a \"Cell\" by packing bytes into a buffer is exactly how **Network Protocols** (like TCP/IP or Protobuf) work. You are learning to speak the language of the wire.\n4.  **Clustered vs. Non-Clustered**: Now you know why primary keys are \"special.\" They define the physical order of the data on the disk.\n\n### System Awareness\nThe B-tree layer is the heart of the engine. Upstream, the **VDBE** (Milestone 3) uses **Cursors** to ask the B-tree for the \"Next\" or \"Previous\" row. Downstream, the B-tree asks the **Buffer Pool** (Milestone 4) for specific Page IDs. Without this layer, your database is just a flat file; with it, it becomes a high-speed search engine.\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m6 -->\n# Milestone 6: SELECT Execution & DML\n\nYou have built the brain (the Parser and VM) and the skeletal structure (the B-tree and Pager). Now, you are about to connect the nervous system. In this milestone, you will integrate the Virtual Machine with the Storage Engine to perform **Data Manipulation Language (DML)** operations: `SELECT`, `INSERT`, `UPDATE`, and `DELETE`.\n\nThis is the moment your database becomes \"alive.\" Until now, your B-tree was a static library of bytes; now, it becomes a dynamic, queryable engine. You will implement the logic that allows the VM to navigate the B-tree using **Cursors**, deserialize variable-length records into registers, and handle the \"Update Paradox.\"\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n---\n\n### The Fundamental Tension: The Impedance Mismatch\n\nThe core tension in this milestone is **Logical Sets vs. Physical Records**. \n\nSQL is a **Declarative, Set-Based** language. When you write `DELETE FROM users WHERE age > 30`, you are describing a *set* of rows to be removed. However, a B-tree is a **Procedural, Entry-Based** structure. It only understands operations like \"Go to this specific RowID\" or \"Move to the next record.\"\n\nYour task is to bridge this gap. You must ensure that the set-based logic of the SQL parser is correctly translated into a series of pointer movements and page modifications. If you get the mapping wrong, you might delete the wrong row, or worse, leave the B-tree in an inconsistent state where a child page exists but its parent no longer points to it.\n\n---\n\n### The Three-Level View: The Execution Stack\n\nTo see how a query like `SELECT name FROM users WHERE id = 5` moves through the system:\n\n1.  **Level 1 \u2014 The Logical Plan (AST)**: The parser identifies that we need a specific row from the `users` table where the primary key is `5`.\n2.  **Level 2 \u2014 The Bytecode Execution (VM)**: The VM opens a **Cursor** on the `users` B-tree, executes a `Seek` to find RowID `5`, and extracts the `name` column into a register.\n3.  **Level 3 \u2014 The Physical Storage (B-tree/Pager)**: The B-tree layer traverses internal pages, uses the Buffer Pool to load Page #12, finds the record in the slotted page's cell content area, and returns the raw bytes.\n\n---\n\n### The Soul of Iteration: The Cursor Pattern\n\nIn database internals, a **Cursor** is the primary abstraction for record navigation. \n\n> **\ud83d\udd11 Foundation: The Cursor Pattern**\n>\n> ### 1. What it IS\n> A Cursor is a stateful object that represents a \"position\" within a database structure (like a Table B-tree or an Index). It keeps track of which page it is currently on and which \"slot\" (index in the cell pointer array) it is pointing to. \n>\n> ### 2. WHY you need it right now\n> The Virtual Machine (VM) should not need to know the complexities of B-tree page splitting or depth. By using a Cursor, the VM can simply say `Next()` or `Prev()`. The Cursor handles the heavy lifting: if it reaches the end of Page #5, it knows to look at the \"Right Child\" or \"Parent\" to find Page #6.\n>\n> ### 3. Key Insight: Decoupling Logical and Physical\n> **The Cursor is a \"Bookmark\" for the VM.** Without it, every opcode would have to re-calculate the path from the root of the B-tree to the data. With a Cursor, the database maintains a \"hot\" path to the data, significantly reducing the CPU cost of sequential scans.\n\nWhen you implement the `OpenRead` or `OpenWrite` opcodes, you are instantiating a Cursor. This cursor must be stored in the VM's `cursors[]` array, allowing subsequent opcodes (`Column`, `Next`, `Insert`) to reference it by index.\n\n---\n\n### Mechanism 1: SELECT and Projection\n\nA `SELECT` statement involves two main phases: **Scanning** and **Projection**.\n\n#### 1. The Scan (`Rewind` & `Next`)\nTo execute `SELECT * FROM users`, the compiler generates a `Rewind` opcode followed by a loop containing `Next`.\n- `Rewind`: Tells the cursor to find the \"leftmost\" leaf in the B-tree. If the table is empty, it jumps to the end of the program.\n- `Next`: Tells the cursor to move to the next slot in the current page. If it's at the last slot, it uses the B-tree's sibling pointers (or parent traversal) to move to the next leaf page.\n\n#### 2. The Projection (`Column` & Record Deserialization)\nThe `Column` opcode is responsible for **Projection**\u2014extracting specific data from a raw B-tree cell. \n\nRecall that a B-tree cell contains a **Record**. This record is a serialized sequence of values. To extract \"Column 2\", you cannot just jump to a fixed offset because columns like `TEXT` or `BLOB` are variable-length. \n\n**The Record Format Header:**\nSQLite-style records start with a **Header Size** (varint) followed by a series of **Serial Types** (one varint per column).\n- If Serial Type is `1`, the column is an 8-bit integer.\n- If Serial Type is `13`, the column is a string of length `(13-13)/2 = 0`? No, the formula is usually `(N-12)/2` for strings.\n- You must parse this header to find the byte offset of the specific column the VM is asking for.\n\n---\n\n### The Revelation Arc: The Update Paradox\n\nMost developers approaching database design have a simple mental model for `UPDATE`: \"Just find the bytes on the disk and overwrite them with the new values.\"\n\n**The Scenario:**\nYou have a row: `(ID: 1, Name: \"Bob\", Bio: \"Short bio\")`.\nThe user executes: `UPDATE users SET Bio = \"[A 500-page novel...]\" WHERE ID = 1;`\n\n**The Problem:**\nThe original row occupied 50 bytes. The new row occupies 50,000 bytes. It **physically cannot fit** in the same slot in the slotted page. It might not even fit on the same **page**. \n\n**The Reveal:**\nIn a B-tree storage engine, an `UPDATE` is almost never a simple overwrite. It is actually a **DELETE** followed by an **INSERT**. \n1. The engine deletes the old, small record.\n2. The engine attempts to insert the new, large record.\n3. This insertion triggers a **Node Split** (Milestone 5) because the page is now over-capacity.\n4. The B-tree's structure changes entirely.\n\n**Key Insight:** Because updates change row sizes, they can cause \"Write Amplification.\" A single-row update might force the database to re-balance three levels of the B-tree and write 5+ new pages to disk. This is why \"Primary Keys\" (RowIDs) are usually immutable\u2014changing the key would require moving the row to a completely different part of the tree.\n\n---\n\n### Mechanism 2: WHERE and Three-Valued Logic (3VL)\n\nWhen the VM evaluates a `WHERE` clause, it uses comparison opcodes (`Eq`, `Gt`, `Ne`). These opcodes must respect SQL's **Three-Valued Logic**.\n\nIn most programming languages, a boolean is `TRUE` or `FALSE`. In SQL, it is `TRUE`, `FALSE`, or `NULL` (Unknown). \n\n> \ud83d\udd2d **Deep Dive**: **3VL Comparison Rules**. \n> - `5 = 5` is `TRUE`.\n> - `5 = 6` is `FALSE`.\n> - `5 = NULL` is `NULL`.\n> - `NULL = NULL` is `NULL`.\n> \n> For a full matrix of logical operations (AND/OR/NOT) in 3VL, refer to **ISO/IEC 9075 (SQL Standard)**. The critical takeaway for your VM: a `WHERE` clause only includes a row if the expression evaluates to **exactly TRUE**. If it evaluates to `FALSE` or `NULL`, the row is skipped.\n\nYour comparison opcodes must check the \"Serial Type\" of the values in the registers. Comparing an `INTEGER` to a `TEXT` string requires **Type Affinity** rules (e.g., converting the string \"123\" to the number 123 before comparing).\n\n---\n\n### Mechanism 3: INSERT and Auto-increment\n\nWhen executing an `INSERT`, your VM must handle the `rowid`. \n- If the user provides an ID (`INSERT INTO t(id, val) VALUES(10, 'hi')`), the B-tree attempts to insert that specific key. If it exists, it must return a `UNIQUE CONSTRAINT` error.\n- If the user provides `NULL` for an `INTEGER PRIMARY KEY`, or omits it, you must implement **Auto-increment**.\n\n**The Max-RowID Optimization:**\nTo find the next ID, don't scan the whole table. Since the Table B-tree is keyed by `rowid`, the largest ID is always in the **rightmost** leaf. Your engine should maintain a `last_inserted_rowid` in the table's metadata or perform a quick \"Seek to End\" to find the current maximum.\n\n---\n\n### The \"Sawing Off the Branch\" Problem (DELETE)\n\nThere is a subtle but deadly bug waiting in your `DELETE` implementation. Consider:\n`DELETE FROM users WHERE age > 20;`\n\nThe VM loop looks like this:\n1. `Rewind` cursor.\n2. Check `age`. If > 20, call `Delete`.\n3. Call `Next` on the cursor.\n\n**The Trap:** When you call `Delete` on a B-tree, the page might be re-balanced or merged with a neighbor. The cursor, which was pointing to \"Slot 4 of Page #10,\" is now pointing at garbage or the wrong row. Calling `Next` after a `Delete` often leads to skipped rows or crashes.\n\n**The Solution: Two-Pass Delete.**\n1. **Pass 1 (Collect)**: Scan the table and collect the `rowid` of every matching row into a temporary list (or a VM register array).\n2. **Pass 2 (Execute)**: Iterate through the collected IDs and perform a point-delete for each one. This ensures that B-tree mutations don't corrupt the \"Scanning Cursor.\"\n\n---\n\n### Constraint Enforcement: NOT NULL\n\nBefore the `Insert` or `Update` opcodes commit data to the B-tree, they must validate **Constraints**. \n\nThe `NOT NULL` constraint is checked during the **MakeRecord** phase. As the VM assembles the binary payload from registers, it checks the table's schema (stored in your System Catalog). If a column is marked `NOT NULL` but the register contains a `NULL` value, the VM must abort the transaction and return a descriptive error: `Error: NOT NULL constraint failed: users.name`.\n\n---\n\n### Design Decisions: Write Amplification\n\n| Feature | Impact | Trade-off |\n|--------|------|---------|\n| **In-Place Update** | High Performance | Only works if the new data is exactly the same size as the old data. |\n| **Delete + Insert (Chosen \u2713)** | High Reliability | Causes **Write Amplification**. Updating 1 byte might cause a 4KB page write. |\n\n**Why we choose Delete + Insert**: It is the only way to support variable-length columns (`VARCHAR`, `TEXT`) and maintain B-tree balance. While it increases disk I/O, it simplifies the storage engine logic immensely and prevents \"shattered\" records that span across non-contiguous fragments.\n\n---\n\n### Implementation Roadmap\n\n1.  **Table Validation**: In the `OpenRead/Write` opcodes, check the System Catalog. If the table name doesn't exist, throw an error.\n2.  **The Record Decoder**: Implement a utility that takes a raw byte buffer and a column index, and returns a `Value` object (Integer, String, etc.) by parsing the Record Header.\n3.  **The Cursor Wrapper**: Enhance your B-tree `find()` and `insert()` methods to work with a `Cursor` object that tracks `currentPage` and `currentSlot`.\n4.  **The Delete Opcode**: Implement the two-pass logic or a \"Cursor Revalidation\" step to prevent corruption during scans.\n5.  **3VL Logic**: Update your comparison opcodes (`Eq`, `Lt`, etc.) to handle `NULL` correctly according to SQL standards.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **The Cursor Pattern**: This is identical to **Iterators** in C++, **Generators** in Python, or **Streams** in Java. You are learning how to process data that is too large to fit in memory by using a \"window\" (the cursor).\n2.  **Write Amplification**: This concept is vital for **SSD Engineering**. SSDs cannot overwrite individual bytes; they must erase entire blocks (e.g., 256KB) to change one bit. Your B-tree's page-level updates mirror the physical reality of modern flash memory.\n3.  **Three-Valued Logic (3VL)**: This is a form of **Kleene Logic**. Understanding how \"Unknown\" propagates through expressions is the foundation of modern data science and missing-data handling.\n4.  **The Record Format**: The way you pack metadata (headers) before data is the basis of **Protobuf**, **Avro**, and even **TCP Packet Headers**. You are learning to design efficient binary protocols.\n\n### System Awareness\nIn the context of the whole engine, Milestone 6 is the **Integration Point**. You are finally using the `Pager` (M4), the `B-tree` (M5), and the `VM` (M3) together. If the Pager doesn't flush dirty pages, your `INSERT`s will vanish. If your `B-tree` split logic is buggy, your `SELECT` will find only half the rows. This is where you prove that your architectural layers are truly solid.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m7 -->\n# Milestone 7: Secondary Indexes\n\nYou have a functioning database. You can create tables, insert rows, and retrieve them. But if you have a million rows and you want to find the user with the email `dev@null.com`, your engine currently has to perform a **Full Table Scan**. It must load every single page from disk, deserialize every row, and check the email column. Even with an efficient Buffer Pool, this is $O(N)$\u2014a linear crawl that kills performance as your data grows.\n\nIn this milestone, you will implement **Secondary Indexes** using **B+trees**. You will build the \"Search Engine\" of your database, allowing the Virtual Machine to jump directly to the data it needs in $O(\\log N)$ time. You will implement index maintenance (the \"write tax\"), range scans, and the critical \"Double Lookup\" pattern.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n---\n\n### The Fundamental Tension: The \"Index Tax\"\n\nThe tension in indexing is **Read Speed vs. Write Latency & Storage Space.**\n\nEvery index you add to a table is a secondary, physical structure that must be stored on disk and kept in perfect synchronization with the main table. There is no such thing as a \"free\" index:\n1.  **Storage Cost**: An index on a `name` column requires storing every name a second time, plus the overhead of B-tree internal pages.\n2.  **Write Penalty**: When you `INSERT` a row, you no longer just write to the Table B-tree. You must also navigate and insert into *every* associated Index B+tree. This turns a single write into multiple writes, increasing the risk of fragmentation and I/O wait.\n3.  **The \"Stale Data\" Risk**: If a bug in your engine updates the Table but fails to update the Index, your queries will return \"ghost\" rows or miss existing ones.\n\nThe engineering challenge is to make the retrieval so much faster ($O(\\log N)$ vs $O(N)$) that the user is willing to pay the \"tax\" on every write.\n\n---\n\n### The Revelation Arc: The \"Thin\" Tree vs. The \"Thick\" Tree\n\n#### The Misconception\nMany developers beginning their journey into database internals assume that an index is just a \"fast copy\" of the table. They imagine that if they create an index on `email`, the index contains the full row data sorted by email so the engine can just read it and be done.\n\n#### The Scenario\nImagine a table `Users` with columns: `ID, Email, Bio, ProfilePicture (BLOB)`. \nThe `ProfilePicture` is 100KB. If you have 10,000 users, and you create an index on `Email`, should the index also store the 100KB `ProfilePicture`? \n\n#### The Reveal\nIf the index stored the full row, it would be as large as the table itself. Your Buffer Pool would be exhausted by duplicate data. Instead, indexes are **\"Thin.\"** \n\nAn index entry (a \"Cell\" in the Index B+tree) contains only two things:\n1.  **The Key**: The value being indexed (e.g., `dev@null.com`).\n2.  **The Pointer**: The `RowID` of the corresponding row in the Table B-tree.\n\n**Key Insight**: An index does not usually give you the answer; it gives you the **Address** of the answer. Finding a row via an index is a two-stage journey called the **Double Lookup**. \n\n---\n\n### Mechanism: The Double-Lookup Walk\n\nWhen the VDBE executes a query like `SELECT * FROM users WHERE email = 'dev@null.com'`, and an index exists on `email`, it follows this path:\n\n1.  **Search the Index B+tree**: Use the B+tree search algorithm to find the leaf page containing `'dev@null.com'`.\n2.  **Extract the RowID**: The leaf cell for `'dev@null.com'` contains the value `42`.\n3.  **Search the Table B-tree**: The VM then takes that `RowID: 42` and performs a second, high-speed lookup in the main Table B-tree. \n4.  **Retrieve the Row**: The Table B-tree provides the actual row containing the `Bio` and `ProfilePicture`.\n\n\n![The Double-Lookup Walk](./diagrams/diag-index-lookup.svg)\n\n\n---\n\n### B-tree vs. B+tree: Why the \"+\" Matters\n\nIn Milestone 5, you built a B-tree for table storage. For indexes, we specifically use the **B+tree** variant. \n\n> **\ud83d\udd11 Foundation: The B+tree Variant**\n>\n> ### 1. What it IS\n> A B+tree is a variation of the B-tree where:\n> 1.  **Data only lives in Leaves**: Internal nodes *only* store keys for navigation.\n> 2.  **Linked Leaves**: Every leaf page contains a pointer to the \"Next\" leaf page in sorted order.\n>\n> ### 2. WHY you need it right now\n> Databases love **Range Scans** (e.g., `WHERE price BETWEEN 10 AND 50`). In a standard B-tree, performing a range scan requires \"backtracking\" up to the parent and down to the next child repeatedly. In a B+tree, you find the start of the range (`10`) and then simply follow the \"Next Page\" pointers horizontally across the leaves until you hit `50`.\n>\n> ### 3. Key Insight\n> **B+trees turn Random I/O into Sequential I/O.** By linking the leaves, the B+tree allows the engine to treat the bottom layer of the tree as a sorted, linked list, which is significantly faster for modern disk controllers to read.\n\n---\n\n### Implementation: The Index Cell Format\n\nAn Index B+tree page uses the same **Slotted Page Architecture** as your tables, but the **Cells** (the data units) are formatted differently. \n\nAn Index Cell must store a composite value. Even if you are only indexing one column, the cell actually stores a tuple: `(IndexValue, RowID)`.\n\n**Why include the RowID in the key?**\nIn SQL, you can have multiple rows with the same value (e.g., ten users named \"Smith\"). If the index only stored the value \"Smith,\" it wouldn't know which `RowID` to return. By storing `(Smith, 10)` and `(Smith, 12)`, every entry in the index becomes unique and sorted.\n\n**Handling NULLs**: \nIn SQL, `NULL` is usually treated as the \"smallest\" possible value for sorting purposes. Your index comparison logic must handle `NULL` values consistently so that `WHERE col IS NULL` can also use the index.\n\n---\n\n### Index Maintenance: The Synchronous Tax\n\nYou must modify your `INSERT`, `UPDATE`, and `DELETE` execution logic to maintain all associated indexes. This logic must be **Synchronous**\u2014the user must not see the `INSERT` as \"complete\" until the indexes are updated.\n\n#### 1. INSERT Hook\n1.  Insert the row into the Table B-tree to get the `RowID`.\n2.  For every index on that table:\n    *   Extract the value of the indexed column(s) from the new row.\n    *   Construct an Index Cell: `(Value, RowID)`.\n    *   Insert into the Index B+tree.\n\n#### 2. DELETE Hook\n1.  Read the row from the Table B-tree *before* deleting it to get the current column values.\n2.  For every index:\n    *   Construct the Index Cell `(OldValue, RowID)`.\n    *   Perform a point-delete in the Index B+tree.\n3.  Delete the row from the Table B-tree.\n\n#### 3. UPDATE Hook\nAn update is the most expensive operation. If the indexed column changes, you must perform an **Index Delete** of the old value and an **Index Insert** of the new value. \n\n---\n\n### The Unique Constraint: Free Enforcement\n\nA `UNIQUE` index is a special type of index that performs a \"Double Duty.\" In addition to speeding up reads, it enforces business logic.\n\nWhen you attempt to insert `(Value: 'Smith', RowID: 15)` into a `UNIQUE` index:\n1.  The B+tree performs a search for `Value: 'Smith'`.\n2.  If it finds *any* entry with the value `'Smith'`, the insertion fails with a `UNIQUE CONSTRAINT VIOLATION`.\n3.  Because the search happens in $O(\\log N)$, enforcing uniqueness is extremely cheap. Without an index, checking for uniqueness would require a full table scan on every single insert!\n\n---\n\n### VDBE Integration: New Opcodes\n\nTo use the index, your Bytecode Compiler needs new instructions that understand how to navigate \"Thin\" trees.\n\n| Opcode | Parameters | Logic |\n|--------|------------|-------|\n| **`IdxGE`** | `Cursor, Label, Reg` | **Index Greater-Equal**: Seek the index cursor to the first entry $\\ge$ value in `Reg`. If not found, jump to `Label`. |\n| **`IdxRowid`** | `IdxCursor, OutReg` | Extract the `RowID` from the current index cell and put it in `OutReg`. |\n| **`SeekRowid`** | `TableCursor, RowidReg`| Move the table cursor to the row identified by the ID in `RowidReg`. |\n\n**The VDBE Loop for an Indexed Query:**\n1.  `IdxGE(idx_cursor, end_label, target_value)`\n2.  `LoopStart:`\n3.  `IdxRowid(idx_cursor, r1)`\n4.  `SeekRowid(table_cursor, r1)`\n5.  `Column(table_cursor, ...)`\n6.  `ResultRow`\n7.  `Next(idx_cursor, LoopStart)`\n8.  `Halt`\n\n---\n\n### Optimization: The \"Covering Index\"\n\nThere is one exception to the \"Double Lookup\" rule. If your query only asks for columns that are already present inside the index, the VDBE can skip the second lookup entirely.\n\n**The Scenario**:\n`CREATE INDEX idx_name ON users(name);`\n`SELECT name FROM users WHERE name = 'Alice';`\n\nSince the index already contains the `name` \"Alice\", the engine has all the information it needs. It can emit the `ResultRow` using the data inside the index cell. This is a **Covering Index**, and it is the fastest possible way to retrieve data in a relational engine.\n\n---\n\n### Design Decisions: B+Tree vs. Hash Index\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **B+Tree (Chosen \u2713)** | Supports range scans (`<`, `>`, `BETWEEN`). Stays balanced naturally. | Slightly slower equality lookups ($O(\\log N)$). | SQLite, Postgres, MySQL |\n| Hash Index | Perfect $O(1)$ equality lookups. | No range scans. Hash collisions can degrade to $O(N)$. | Memory engines, Redis |\n\n**Why we choose B+Tree**: In SQL, users constantly perform range queries and `ORDER BY` operations. A B+tree provides sorted results \"for free\" because of its internal structure. A Hash Index would require a full sort of the results every time.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **Index Maintenance and LSM-Trees**: You've seen that updating B-tree indexes is expensive because of random writes. **Log-Structured Merge-Trees (LSM)** (used in RocksDB or Cassandra) solve this by buffering index updates in memory and writing them sequentially.\n2.  **Search Engines**: The \"Inverted Index\" used by systems like **Elasticsearch** is essentially a \"Thin\" index where the Key is a word and the \"RowID\" is a list of Document IDs. The fundamental concept is the same: mapping a value to its location.\n3.  **Composite Indexing**: Now that you understand the tuple `(Value, RowID)`, you can see how multi-column indexes work: `(Col1, Col2, RowID)`. This explains the \"Leftmost Prefix\" rule: an index on `(FirstName, LastName)` can help you find \"John\", but it cannot help you find everyone with the last name \"Smith\" without a full scan.\n4.  **Covering Indexes**: This is a core trick in **Database Tuning**. By adding an extra column to an index (even if you don't search by it), you can \"cover\" the query and eliminate the expensive disk seek of the Double Lookup.\n\n### System Awareness\nIn the System Map, the Secondary Index is a \"Satellite\" structure. Upstream, the **Query Planner** (Milestone 8) must now decide: \"Is it cheaper to do a Full Table Scan or an Index Scan?\" Downstream, the Index uses the same **Buffer Pool** and **Pager** as the table. If your Buffer Pool is too small to hold both the Index and the Table's \"hot\" pages, you will experience **Cache Thrashing**, where the system spends all its time swapping pages in and out.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m8 -->\n# Milestone 8: The Query Planner & Statistics\n\nIn the previous milestones, you built a powerful engine capable of navigating B-trees and executing complex bytecode. But your engine is currently \"blind.\" When it receives a query like `SELECT * FROM users WHERE age > 25 AND status = 'active'`, it has no idea whether it should scan the whole table or use an index on `age`. If you have an index on `status` as well, it doesn't know which one will filter out more rows.\n\nIn this milestone, you will build the **Query Planner**. This is the \"brain\" of the database that performs **Cost-Based Optimization (CBO)**. You will implement the `ANALYZE` command to gather metadata about your data, build a mathematical model to predict I/O costs, and write the logic that chooses the most efficient execution path.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n---\n\n### The Fundamental Tension: The Random I/O Penalty\n\nThe tension at the heart of query planning is **Sequential I/O vs. Random I/O.**\n\nIn Milestone 7, we celebrated the B+tree index for allowing us to jump directly to a row. However, \"jumping\" has a hidden cost. When you perform a **Full Table Scan**, the Pager reads pages sequentially. Modern OS kernels and disk controllers are optimized for this; they \"read ahead,\" pulling the next 10 pages into memory before you even ask for them. Sequential reading is like a high-speed train on a straight track.\n\nWhen you use an **Index Scan** (the \"Double Lookup\"), you are performing Random I/O.\n1. Read an Index Page.\n2. Jump to a Table Page (possibly far away on disk).\n3. Jump back to the next Index Page.\n4. Jump to a different Table Page.\n\nEvery \"jump\" risks a **Buffer Pool Miss**. If the table is large, these jumps force the disk head (in HDDs) to move or the flash controller (in SSDs) to open new blocks. On mechanical disks, a random seek is **100x to 1000x slower** than a sequential read. Even on NVMe SSDs, the overhead of random access is significant due to the loss of read-ahead optimizations.\n\nThe Planner\u2019s job is to answer one question: **\"Is the number of rows I'm skipping worth the cost of the random jumps I'm adding?\"**\n\n---\n\n### The Revelation Arc: The Index Trap\n\n#### The Misconception\nAs a developer, your instinct is likely: *\"If an index exists on a column in the WHERE clause, I should always use it. Why would I ever scan the whole table if I have a shortcut?\"*\n\n#### The Scenario\nImagine a table `Votes` with 1,000,000 rows. You have an index on the column `voted_at`. \nYou run the query: `SELECT * FROM Votes WHERE voted_at > '2000-01-01'`.\nIt turns out that 950,000 people have voted since the year 2000. \n\n#### The Reveal\nIf you use the index, you will perform **950,000 random lookups** into the main table. You will visit almost every page in the database, but you will do it in a fragmented, chaotic order, jumping back and forth. \n\nIf you just perform a **Full Table Scan**, you read every page exactly once, in order, using high-speed sequential I/O. \n\n**The Reveal:** For queries with **low selectivity** (queries that return a large percentage of the table), a Full Table Scan is significantly faster than an Index Scan. The Query Planner's primary duty is to detect these \"low selectivity\" traps and force the engine to stay on the \"straight track\" of a sequential scan.\n\n---\n\n### The Three-Level View: The Planner's Eye\n\nThe Query Planner operates as a translation layer between the user's intent and the VM's execution.\n\n1.  **Level 1 \u2014 The Logical Request (AST)**: \"I want all users where age is 30.\"\n2.  **Level 2 \u2014 The Candidate Plans (Optimization)**: The Planner looks at the schema. \n    *   Plan A: Full Table Scan + Filter by age. \n    *   Plan B: Index Scan on `idx_age` + Double Lookup.\n3.  **Level 3 \u2014 The Physical Execution (Bytecode)**: The Planner calculates that Plan B is 50x cheaper. It tells the Compiler to emit `IdxGE` and `SeekRowid` opcodes instead of a `Rewind` loop.\n\n---\n\n### Phase 1: The `ANALYZE` Command (Statistics Collection)\n\nTo make a \"Cost-Based\" decision, the Planner needs data about the data. It needs **Statistics**. \n\nYou must implement the `ANALYZE` command. When run, it performs a full scan of the database and populates a system table (e.g., `sqlite_stat1`). \n\n> **\ud83d\udd11 Foundation: Selectivity and Cardinality**\n>\n> ### 1. What it IS\n> **Cardinality** is the number of *distinct* values in a column. If a `gender` column has values \"Male\", \"Female\", and \"Other\", its cardinality is 3, regardless of whether the table has 10 rows or 10 million.\n> **Selectivity** is the \"filtering power\" of a value. It is calculated as $\\frac{1}{\\text{Cardinality}}$. A high-cardinality column (like `email`) has high selectivity (it filters out almost everything). A low-cardinality column (like `is_active`) has low selectivity.\n>\n> ### 2. WHY you need it right now\n> If you query `WHERE email = 'a@b.com'`, the Planner knows (via cardinality stats) that this will likely return 1 row. Using an index is a no-brainer. If you query `WHERE is_active = 1`, the Planner knows this might return 50% of the table. It might choose to ignore the index entirely.\n>\n> ### 3. Key Insight\n> **Statistics are a \"Snapshot in Time.\"** The database does not update these stats on every `INSERT` because it would be too slow. This is why databases sometimes \"get slow\" over time until you manually run `ANALYZE` to refresh the Planner's view of the world.\n\n**What `ANALYZE` should collect:**\n- **Table Level**: Total number of rows (`nRow`).\n- **Index Level**: For each column in the index, the average number of rows that share the same value. (e.g., \"In the `last_name` index, there are an average of 15 rows per name\").\n\n---\n\n### Phase 2: The Cost Model\n\nNow you must define the \"Math of Performance.\" A cost model assigns a numerical value (usually representing I/O operations) to a plan.\n\n$$ \\text{Total Cost} = (\\text{CPU Cost}) + (\\text{I/O Cost}) $$\n\nIn our engine, I/O is the dominant factor. Let's build a simplified model:\n\n#### 1. Full Table Scan Cost\n$$ \\text{Cost}_{scan} = \\text{TotalPagesInTable} $$\n*Intuition: We have to touch every page once.*\n\n#### 2. Index Scan Cost\n$$ \\text{Cost}_{index} = \\text{PagesInIndex} + (\\text{EstimatedMatchingRows} \\times \\text{RandomIOFactor}) $$\n\n- **EstimatedMatchingRows**: Calculated using your stats. If you have 1,000 rows and the column has a cardinality of 10, you estimate 100 matching rows for an equality predicate (`col = ?`).\n- **RandomIOFactor**: This is a \"weight\" you assign to the penalty of a random seek. In your implementation, start with a value of **4.0**. This means one random jump is considered 4x more expensive than one sequential page read.\n\n**The Threshold**:\nYour Planner should compare the costs. If $\\text{Cost}_{index} < \\text{Cost}_{scan}$, use the index. \n*Note: Most engines find that the \"tipping point\" is around 20-30%. If a query matches more than 20% of the table, the index is usually slower than a scan.*\n\n---\n\n### Phase 3: Choosing the \"Access Path\"\n\nWhen the Compiler receives an AST for a `SELECT` statement, it now calls the Planner before emitting opcodes.\n\n**The Planning Logic:**\n1.  Identify all columns in the `WHERE` clause.\n2.  Find all indexes that involve those columns.\n3.  For each candidate index:\n    *   Estimate how many rows will be returned based on the operator.\n    *   `=` (Equality): $\\text{TotalRows} / \\text{Cardinality}$.\n    *   `<` or `>` (Range): Usually estimated at **33%** of the table if no better stats exist (this is a **Heuristic**).\n    *   `BETWEEN`: Usually estimated at **10%**.\n4.  Calculate the cost of using that index vs. the cost of a full scan.\n5.  Select the plan with the lowest cost.\n\n---\n\n### Phase 4: Join Optimization (The N-Body Problem)\n\nThis is the most complex part of query planning. When a user joins three tables (`A JOIN B JOIN C`), there are $3! = 6$ possible orders to join them.\n1. $(A \\bowtie B) \\bowtie C$\n2. $(B \\bowtie A) \\bowtie C$\n3. $(C \\bowtie B) \\bowtie A$\n...and so on.\n\n\n> **\ud83d\udd11 Foundation: Why Join Order Matters: Joining a 10-row table to a 1-million-row table is much faster if you start with the 10-row table and use an index to find the matching millionth-row entries**\n> \n> **1. What it IS**\nJoin order complexity refers to the mathematical explosion of possible sequences in which a database can combine multiple tables to satisfy a query. While the final result of a join is the same regardless of the order (e.g., Joining Table A to B is logically the same as B to A), the physical path the database takes to get there\u2014the \"execution plan\"\u2014determines the performance. In a query with $N$ tables, the number of possible join orders grows factorially ($N!$), meaning for a 10-table join, there are over 3.6 million possible ways to execute the query.\n\n**2. WHY you need it right now**\nIn this project, you are dealing with asymmetrical data sizes (e.g., a small `Users` table and a massive `Logs` table). If the database engine chooses an inefficient join order, it might attempt to load the massive table into memory or perform a full table scan before applying filters. Understanding join order complexity allows you to diagnose \"slow queries\" that look fine on paper but are failing because the Query Optimizer is overwhelmed by the search space or lacks the statistics to pick the cheapest path.\n\n**3. Key Insight: The \"Funnel\" Principle**\nThink of a join sequence as a funnel. Your goal is to **discard data as early as possible.** If you join two small tables first to create a tiny intermediate result, the subsequent join against a million-row table becomes a series of quick, indexed lookups. If you join the large tables first, you are forcing the database to carry a massive \"heavy\" dataset through every subsequent step of the process.\n\n\n#### The Nested Loop Join Strategy\nFor this milestone, we use the **Nested Loop Join** (the only join algorithm you've implemented so far). \n\n\n![Nested Loop Join Trace](./diagrams/diag-nested-loop-join.svg)\n\n\n**The Planner's Goal for Joins:**\nFind the \"Outer Loop\" that is the smallest, and an \"Inner Loop\" that has an index on the join column.\n\n**Estimation (Cardinality of a Join):**\nIf you join `Users` and `Orders` on `Users.id = Orders.user_id`:\n$$ \\text{EstimatedRows} = \\frac{\\text{Rows(Users)} \\times \\text{Rows(Orders)}}{\\max(\\text{Cardinality(Users.id)}, \\text{Cardinality(Orders.user_id)})} $$\n\nYour Planner should iterate through possible join orders (for small numbers of tables, you can just try them all) and pick the one with the lowest total estimated cost.\n\n---\n\n### Phase 5: Integrating with `EXPLAIN`\n\nThe `EXPLAIN` command you built in Milestone 3 must now be upgraded. Instead of just showing opcodes, it should first show the **Plan Description**. \n\nWhen a user runs `EXPLAIN QUERY PLAN SELECT...`, your engine should output a human-readable summary:\n- `SCAN TABLE users` (If it chose a full scan)\n- `SEARCH TABLE users USING INDEX idx_age (age>?)` (If it chose an index)\n- `NESTED LOOP JOIN` (For joins)\n\nThis allows you to verify that your Planner is actually working. If you add 1,000,000 rows and the `EXPLAIN` still says `SCAN TABLE`, you know your cost model or your statistics are broken.\n\n---\n\n### Design Decisions: Heuristics vs. Statistics\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **Heuristics (Rules of Thumb)** | Instant. No need for `ANALYZE`. Works even on empty tables. | Often wrong. Doesn't know if a column is \"skewed\" (e.g., 99% of users are from one country). | MySQL (Early versions), SQLite (Simple queries) |\n| **Statistics (Chosen \u2713)** | High precision. Adapts to the actual data distribution. | Requires the `ANALYZE` tax. Stats can become stale. | SQLite, Postgres, SQL Server |\n\n**Why we choose Statistics**: An Expert-level database must handle data \"skew.\" If your `status` column has two values, \"active\" (99%) and \"banned\" (1%), a heuristic would say selectivity is 50% for both. Statistics tell the Planner: \"Use the index for 'banned', but scan the table for 'active'.\"\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **Cost Modeling in Distributed Systems**: The same logic is used in **Apache Spark** or **Presto**. The \"Cost\" there isn't just disk I/O; it's **Network Shuffles**. The Planner tries to minimize moving data between servers.\n2.  **Machine Learning for Query Planning**: Modern research (like **PostgreSQL's Learned Optimizer**) uses Neural Networks to predict query costs instead of manual formulas. They \"learn\" the Random I/O penalty of the specific hardware they are running on.\n3.  **The N-Body Problem**: Join ordering is a **Combinatorial Optimization** problem. As the number of tables grows, databases switch from \"Try every order\" to **Genetic Algorithms** or **Dynamic Programming** to find a \"good enough\" plan without taking forever to plan it.\n4.  **Static Analysis in Compilers**: Your Query Planner is a form of **Profile-Guided Optimization (PGO)**. Just as C++ compilers can use execution traces to optimize code, your DB uses `ANALYZE` to optimize the data path.\n\n### System Awareness\nThe Query Planner is the **Navigator** of your engine. It sits between the **Parser** (M2) and the **Bytecode Compiler** (M3). It is the only part of the system that \"understands\" the data distribution. Without it, your B-trees (M5/M7) are like libraries with no index cards\u2014you know the information is there, but you have no efficient way to find it.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m9 -->\n<!-- MS_ID: build-sqlite-m9 -->\n# Milestone 9: Transactions (Rollback Journal)\n\nUp to this point, your database has been a \"fair-weather\" friend. If the power stays on and your code never crashes, it works perfectly. But in the world of data storage, we must build for the \"storm.\" A database that cannot survive a sudden power loss or a kernel panic is not a database\u2014it is a temporary buffer with a file extension.\n\nIn this milestone, you will implement **ACID Transactions** using a **Rollback Journal**. This is the classic SQLite mechanism for ensuring that either a query happens completely, or it doesn't happen at all. You will move from simple file writes to a choreographed sequence of operations that use the disk's physical properties to guarantee safety.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n### The Fundamental Tension: The \"Torn Page\" and the Lie of the Cache\n\nThe core tension in this milestone is **Speed vs. Absolute Durability.**\n\nWhen you call `write()` in your code, the Operating System (OS) doesn't actually write to the disk. It copies your data into a kernel buffer (the Page Cache) and tells your program, \"Done!\" This makes your app feel fast. However, if the power fails five seconds later, that data\u2014which the OS promised was \"written\"\u2014is gone forever.\n\nEven worse is the **Torn Page** problem. A database page is typically 4096 bytes. Most hardware (especially older HDDs or cheap SSDs) can only guarantee that a 512-byte \"sector\" is written atomically. If a crash occurs while the drive is halfway through writing your 4KB page, the file will contain 2KB of new data and 2KB of old data. This \"Frankenstein Page\" is physically corrupt; your B-tree pointers will point to garbage, and your database is effectively destroyed.\n\nThe only way to solve this is to accept a performance penalty: we must force the hardware to wait until the data is physically on the platter (or NAND flash) before we proceed. This is the **fsync()** tax.\n\n---\n\n### The Revelation Arc: The Crash Paradox\n\n#### The Misconception\nMost developers think about crashes as something that happens *between* operations. You think: \"I'll write the data, and if I crash *after* that, it\u2019s fine.\" \n\n#### The Scenario\nYou are executing an `UPDATE` that modifies three different pages in your B-tree: a Leaf page, an Internal node, and the Root. You call `write()` for Page A, then Page B, then Page C. \n\n#### The Reveal\n**Computers don't just crash between operations; they crash *during* them.** \n\nIf the system fails while writing Page B, you now have:\n- Page A: New version.\n- Page B: Half-new, half-old (Torn).\n- Page C: Old version.\n\nYour B-tree is now logically inconsistent. Page A might point to a record that Page B doesn't know about. There is no \"Undo\" button on a raw file. \n\n**The Reveal:** Durability is not about writing data; it's about **Write Ordering**. To survive a crash, you must follow a protocol where you save the \"Original State\" to a safe place **before** you touch the main file. If you crash during the write, you can use that \"Original State\" to repair the damage.\n\n---\n\n### The Three-Level View: The Transaction Stack\n\nTo understand how a transaction works, look at how the `COMMIT` command traverses the stack:\n\n1.  **Level 1 \u2014 The API (SQL)**: The user executes `BEGIN TRANSACTION` and then `COMMIT`. This sets the logical boundaries of the work.\n2.  **Level 2 \u2014 The Transaction Manager (VDBE/Pager)**: This layer creates the `.db-journal` file. It intercepts all \"Dirty Page\" requests from the Buffer Pool and copies the *original* bytes into the journal before allowing the Buffer Pool to modify them in RAM.\n3.  **Level 3 \u2014 The OS/Disk (I/O)**: This layer uses `fsync()` to create **I/O Barriers**. It ensures the journal is \"Hardened\" to the disk before the main database file is touched.\n\n---\n\n### Mechanism: The Atomic Commit Choreography\n\nSQLite\u2019s Rollback Journal uses a specific \"dance\" to ensure atomicity. You will implement this sequence in your Pager.\n\n\n![Atomic Write Choreography](./diagrams/diag-journal-flow.svg)\n\n\n#### Step 1: Preparation (The Journal)\nWhen the first write occurs after a `BEGIN`:\n1.  Open a new file named `[your-db-name]-journal`.\n2.  **The Header**: Write a header to the journal containing the original size of the database file.\n3.  **The Undo Log**: For every page the VDBE wants to change, read the *original* page from the `.db` file and write it to the journal.\n\n#### Step 2: The First Barrier (fsync)\nBefore you write a single modified byte to the main `.db` file, you must call `fsync()` on the journal file. \n- **Why?** If you crash during Step 3, the journal *must* be complete and valid on the disk so the recovery logic can find it. If the journal is only in the OS cache and not on disk, it's useless.\n\n#### Step 3: The Update\nNow, and only now, write the \"Dirty\" (modified) pages from your Buffer Pool into the main `.db` file. \n\n#### Step 4: The Second Barrier (fsync)\nCall `fsync()` on the main `.db` file. This ensures all the new data is physically safe.\n\n#### Step 5: The Commit\nDelete the journal file (or truncate it to zero bytes). \n- **The Magic Moment**: The exact microsecond the journal is deleted is the \"Commit Point.\" If you crash a microsecond before this, the database will roll back. If you crash a microsecond after, the changes are permanent.\n\n---\n\n### The Concept: ACID Properties\n\nBy implementing this choreography, you are fulfilling the **ACID** contract.\n\n> **\ud83d\udd11 Foundation: ACID**\n> \n> ### 1. What it IS\nACID is an acronym representing the four primary requirements of a reliable database transaction:\n- **Atomicity**: The \"All or Nothing\" rule. If a transaction has 10 steps and fails on step 9, the first 8 steps are undone.\n- **Consistency**: The database moves from one valid state to another, never violating constraints (like `NOT NULL`).\n- **Isolation**: Concurrent transactions don't interfere with each other. One person's \"half-finished\" work isn't visible to another.\n- **Durability**: Once a transaction is committed, it remains committed even if the power fails or the OS crashes.\n\n### 2. WHY you need it right now\nWithout ACID, you cannot build financial systems, inventory trackers, or even a simple blog. If two users buy the last item in a store at the same time, or if the server reboots during a checkout, ACID is what prevents the store from losing money or corrupting its inventory.\n\n### 3. Key Insight: Durability is the most expensive\nOf the four, **Durability** is the one that slows your database down the most. It requires `fsync()`, which forces the CPU to stop and wait for the physical disk (thousands of times slower than RAM). Most \"High-Performance\" databases that claim to be 100x faster than SQLite are simply lying about Durability\u2014they skip the `fsync()` and hope the power doesn't go out.\n\n\n---\n\n### Mechanism: The \"Hot Journal\" and Crash Recovery\n\nHow does the database \"heal\" itself? You must implement **Crash Recovery logic** that runs every time your database opens.\n\n**The Recovery Logic:**\n1.  Look for a file named `[db-name]-journal`.\n2.  If it doesn't exist, the previous session closed cleanly. Continue as normal.\n3.  If it *does* exist, it might be a **Hot Journal**. A journal is \"Hot\" if:\n    - It exists on disk.\n    - It is not empty.\n    - The process that created it is no longer running (or doesn't hold an exclusive lock).\n4.  **The Rollback**:\n    - Read the original page images from the journal.\n    - Write them back into the main `.db` file at their original offsets.\n    - Truncate the `.db` file to the original size stored in the journal header (this removes any pages that were appended during the failed transaction).\n    - `fsync()` the `.db` file.\n    - Delete the journal.\n\n**Result**: The database is now exactly as it was before the failed transaction started. No \"Torn Pages,\" no partial data.\n\n---\n\n### The Locking Protocol: Managing Concurrency\n\nTo prevent one user from reading Page A while another user is halfway through writing the journal, you need a **Locking State Machine**.\n\nSQLite uses five locking states for the database file:\n\n1.  **UNLOCKED**: No one is touching the file.\n2.  **SHARED**: Multiple users can read, but no one can write.\n3.  **RESERVED**: One user *intends* to write soon. Others can still read, but no other user can start a write.\n4.  **PENDING**: A writer is waiting for all current readers to finish so it can take an Exclusive lock. No new readers are allowed.\n5.  **EXCLUSIVE**: One writer is modifying the file. No one else can read or write.\n\n**The \"Read Isolation\" Rule**: In Rollback Journal mode, readers see the \"Old\" data on the disk because the writer hasn't overwritten it yet. Once the writer starts writing to the `.db` file (Step 3 of the choreography), it must hold an **EXCLUSIVE** lock so that no reader sees a \"partial\" or \"torn\" page.\n\n---\n\n### Implementation Detail: The Journal File Format\n\nDon't just dump bytes into the journal. It needs structure so the recovery logic can validate it.\n\n| Field | Size | Description |\n|-------|------|-------------|\n| **Magic Number** | 8 bytes | A constant (e.g., `0xd9d505f9`) to identify this as a valid journal. |\n| **Page Count** | 4 bytes | Number of pages stored in this journal. |\n| **Random Nonce** | 4 bytes | Used for calculating checksums. |\n| **Original DB Size**| 4 bytes | Size of the DB in pages before the transaction. |\n| **Page Record** | Variable | `[Page Number (4b)] + [Original Page Data (4KB)] + [Checksum (4b)]` |\n\n**The Checksum**: For every page in the journal, calculate a simple checksum. During recovery, if a page's checksum doesn't match, it means the crash happened *while* the journal was being written (Step 1). In this case, the journal is not \"Hot\" and should be ignored, because the main `.db` file hasn't been touched yet!\n\n---\n\n### Design Decisions: Rollback vs. WAL\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **Rollback Journal (Chosen \u2713)** | Simpler to implement. Single-file database when idle. Better for \"Read-Heavy\" workloads with rare writes. | Writers block all readers. `fsync()` happens twice per transaction. | SQLite (Legacy default) |\n| Write-Ahead Log (WAL) | Readers don't block writers. Faster writes (only one `fsync`). | More complex. Requires a shared-memory file (`-shm`). Database is spread across multiple files. | SQLite (Modern default), Postgres |\n\n**Why we choose Rollback Journal**: It is the best way to learn the fundamentals of **Atomic Commit**. WAL (which you will build in the next milestone) is an optimization, but the Rollback Journal is the \"Soul\" of durability.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **I/O Barriers and Fences**: The `fsync()` call is a software-level **I/O Barrier**. In multi-core programming, you use **Memory Barriers** (like `std::atomic_thread_fence` in C++ or `atomic::fence` in Rust) to ensure that one CPU core sees writes from another core in the correct order.\n2.  **Self-Healing Systems**: The \"Hot Journal\" recovery pattern is the basis for **Journaling File Systems** like **ext4** or **NTFS**. When your computer reboots after a crash and says \"Fixing C: drive,\" it is performing a rollback exactly like the one you are building.\n3.  **Distributed Consensus**: In distributed systems (like **Raft** or **Paxos**), nodes use a \"Log\" to agree on state. If a leader crashes, the followers use the log to determine which \"Transaction\" was actually committed to the majority of the cluster.\n4.  **Version Control (Git)**: Git's internal object store is effectively a giant, immutable append-only log. It avoids \"Torn Pages\" by writing a file to a temporary name and then using an **Atomic Rename** (an OS-level atomic operation) to commit it.\n\n---\n\n### System Awareness\n\nIn the architecture of your engine, the Transaction Manager sits inside the **Pager**. It coordinates between the **Buffer Pool** (which holds the \"Dirty\" pages) and the **VFS (Virtual File System)** (which talks to the disk). \n\nWithout this milestone, your database is a toy. With it, you have built a system that can be trusted with real-world data. You have conquered the \"Crash Paradox\" and tamed the entropy of the physical disk.\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m10 -->\nIn Milestone 9, you conquered the \"Crash Paradox\" using the Rollback Journal. It gave your database a \"Soul of Durability.\" But that durability came with a heavy cost: **Strict Seriality.** In Rollback mode, if a writer is updating the database, no one else can read. If a reader is scanning a table, the writer must wait. This \"Stop the World\" approach is the death of performance in high-concurrency environments like web servers.\n\nIn this milestone, you will implement **Write-Ahead Logging (WAL)**. This is the modern engine of SQLite (and PostgreSQL, and MySQL). You will move from an \"Undo\" architecture to a \"Redo\" architecture, enabling **Snapshot Isolation** where readers and writers can operate simultaneously without ever stepping on each other's toes.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n### The Fundamental Tension: The Concurrency Bottlecap\n\nThe tension here is **Data Consistency vs. Concurrent Access.**\n\nIn a Rollback Journal system, the main database file always represents the \"Current Committed Truth.\" To change it, a writer must overwrite pages in place. To prevent a reader from seeing a \"half-written\" (torn) page, the writer must kick all readers out of the building (Exclusive Lock). This creates a **Concurrency Bottlecap**: your database's throughput is limited by the fact that only one \"type\" of operation can happen at a time.\n\nIf your database is powering a website with 1,000 users reading posts and 1 user writing a new post, that 1 writer will periodically \"freeze\" the experience for all 1,000 readers. In modern systems, we demand **Non-blocking Reads**. We want readers to see a consistent version of the world *as it existed when they started*, even if a writer is currently busy carving out a new reality in the background.\n\n---\n\n### The Revelation Arc: The Ghost in the Machine\n\n#### The Misconception\nYou likely believe that for a reader to get a consistent view of the data, they must look at the actual database file, and therefore, that file cannot be changing while they look at it.\n\n#### The Scenario\nImagine a reader starts a long-running query: `SELECT SUM(balance) FROM accounts`. \nHalfway through the scan, a writer performs a transfer: \n`UPDATE accounts SET balance = balance - 100 WHERE id = 1;` \n`UPDATE accounts SET balance = balance + 100 WHERE id = 999;`\n\n#### The Reveal\nIn a Rollback system, if the reader processed account #1 *before* the update and account #999 *after* the update, the total sum would be off by $100. The reader saw a \"ghost\" state that never actually existed. To prevent this, Rollback mode blocks the writer entirely.\n\n**The Reveal:** We don't need to block the writer. Instead of the writer modifying the main file, the writer can just \"shout\" their changes into a separate log file (the WAL). \n- The **Writer** appends new versions of pages to the end of the WAL file. \n- The **Reader** ignores those new versions for now. They continue reading the \"Old Truth\" from the main database file. \n\nBecause the main file never changes during the transaction, the reader's view remains perfectly consistent. The writer and reader literally operate on different files, moving the bottleneck from \"Locking\" to \"I/O Management.\"\n\n---\n\n### The Three-Level View: The WAL Perspective\n\nTo understand how the WAL shifts the database's behavior, look at the layers of execution:\n\n1.  **Level 1 \u2014 The Logical View (SQL)**: The user sets `PRAGMA journal_mode=WAL;`. Transactions look identical to the user, but `COMMIT` suddenly becomes significantly faster because it no longer requires multiple `fsync()` calls to the main database file.\n2.  **Level 2 \u2014 The Log Manager (Pager/WAL)**: This layer redirects all writes to a `.db-wal` file. It maintains a **WAL Index** (usually a shared-memory file) to track which page versions are in the log versus the main file. It acts as a router for the Buffer Pool.\n3.  **Level 3 \u2014 Disk/OS I/O**: Instead of random-writing pages across the database file (which is slow), the system performs a single, high-speed **Sequential Append** to the WAL file.\n\n---\n\n### Mechanism 1: The Redo Log (The WAL File)\n\nIn Rollback mode, we recorded \"Undo\" data (how to go back). In WAL mode, we record **\"Redo\" data** (how to go forward).\n\nThe WAL file is an append-only sequence of **WAL Frames**. Each frame contains:\n- **Page Number**: Which page is this?\n- **Page Data**: The full 4096 bytes of the new version.\n- **Checksums**: Two 32-bit integers to detect corruption.\n\n**The Append Advantage**: Appending to a file is the fastest operation a disk can perform. There is no \"seeking\" involved. The disk head stays in one place, and the bits flow. This is why WAL mode often results in a 10x-100x improvement in write throughput compared to the Rollback Journal.\n\n---\n\n### Mechanism 2: The WAL Index (`-shm`)\n\nIf the writer is putting all the new data into a separate file, how does a reader find it? \n\nIf a reader needs Page #5, they first have to check: \"Is there a newer version of Page #5 in the WAL file?\" Scanning a 100MB WAL file for every page read would be $O(N)$ and would destroy your performance. \n\nTo solve this, you must implement the **WAL Index** (the `wal-index`). \n- This is a hash table or a compact array stored in a separate file (ending in `-shm`). \n- It maps `Page Number` $\\rightarrow$ `Last Frame Index in WAL`.\n\nWhen the Pager asks for a page:\n1.  Check the WAL Index. \n2.  **If Hit**: Page #5 is found at Frame #102 in the WAL. Read it from the WAL file.\n3.  **If Miss**: Page #5 is NOT in the WAL Index. Read it from the main `.db` file.\n\n> \ud83d\udd2d **Deep Dive**: **Shared Memory (`mmap`)**. In production SQLite, the `-shm` file is mapped into the process's memory using `mmap()`. This allows multiple processes to see the same index instantly without performing disk I/O. For your intermediate implementation, you can simulate this with a global data structure or a simple file-backed cache if you are not working in a multi-process environment.\n\n---\n\n### Mechanism 3: Snapshot Isolation\n\nThis is the \"Brain\" of the WAL. How do we ensure a reader doesn't see a write that finished *after* the reader started?\n\nEvery time a writer finishes a transaction, it records a **Commit Mark** (a special bit or flag in the WAL frame). The WAL Index tracks these commit marks as \"End-of-Transaction\" points.\n\nWhen a **Reader** starts:\n1.  It notes the current **WAL Read Mark** (the index of the last committed frame in the WAL at that exact moment).\n2.  During its entire lifetime, the reader **only** looks at frames in the WAL that are $\\le$ its Read Mark.\n3.  Even if a writer appends 1,000 new frames while the reader is working, the reader ignores them. To that reader, the database is \"frozen\" in time at the moment they started.\n\n\n![WAL Concurrency: Reader vs Writer](./diagrams/diag-wal-snapshots.svg)\n\n\n---\n\n### Mechanism 4: The Checkpoint (Merging Reality)\n\nIf we keep appending to the WAL, it will eventually fill up the entire disk. We need to move those changes back into the main database file so we can truncate the WAL. This process is called **Checkpointing**.\n\n> \n> **\ud83d\udd11 Foundation: Checkpointing Depth**\n> \n> ### What it IS\n**Checkpointing depth** is a configuration parameter that determines the granularity at which a system saves its intermediate state during a complex computation. In the context of deep learning, it specifically refers to \"gradient checkpointing\"\u2014a technique where you decide how many layers of a neural network to skip before saving the mathematical outputs (activations) to memory. \n\nInstead of storing the activations for every single layer during the forward pass (which is the default), a specific \"depth\" tells the system to only store a few \"anchor\" points. If the system needs a missing piece of data later, it simply re-calculates it on the fly starting from the nearest saved anchor.\n\n### WHY you need it right now\nYou need to manage checkpointing depth when you hit the **Memory Wall**. As models grow larger (e.g., LLMs or high-resolution vision transformers), the activations alone can easily exceed the available VRAM on a GPU, leading to \"Out of Memory\" (OOM) errors.\n\nBy increasing the checkpointing depth (saving less frequently), you drastically reduce the memory footprint of your model. This allows you to:\n1. Train larger models on consumer-grade hardware.\n2. Use larger batch sizes, which can lead to more stable training.\n3. Fit longer sequences (context lengths) into memory.\n\nThe trade-off is a roughly 20\u201330% increase in computation time, as you are trading \"compute\" to save \"space.\"\n\n### Key Insight: The \"Save Game\" Analogy\nThink of checkpointing depth like **save points in a difficult video game**. \n* **Low Depth (Save often):** You save after every room. If you die, you restart exactly where you were, but your memory card fills up instantly with save files.\n* **High Depth (Save rarely):** You only save at the end of each level. You save massive amounts of space on your memory card, but if you fail, you have to replay the entire level to get back to where you were. \n\n**Checkpointing depth is your strategy for balancing \"how much I have to replay\" vs. \"how much space I have on the card.\"**\n\n> **Checkpointing** is the process of synchronizing the \"Log\" (the temporary list of changes) with the \"Main Store\" (the actual database file). It involves reading the latest version of every page from the WAL and writing it to its proper location in the `.db` file. Without a checkpoint, the WAL grows infinitely. Furthermore, reading from a massive WAL index becomes slower and slower. Checkpointing \"cleans the slate,\" allowing the database to stay compact and fast.\n\n**The Checkpoint Algorithm:**\n1.  **Identify the Barrier**: Find the oldest \"Read Mark\" among all active readers. You can only checkpoint up to this mark.\n2.  **The Transfer**: For every page in the WAL (up to the barrier):\n    *   Read the latest version from the WAL.\n    *   Write it to the corresponding offset in the `.db` file.\n3.  **The Flush**: Call `fsync()` on the `.db` file.\n4.  **The Reset**: Once all readers have finished using the old WAL frames, you can \"Restart\" the WAL from byte 0.\n\n*Key Insight*: A checkpoint can only safely move pages that are no longer being looked at by *any* active reader. If a \"Zombie Reader\" holds onto a snapshot from 10 minutes ago, the WAL cannot be fully checkpointed. This is the primary cause of WAL file bloat.\n\n---\n\n### The Revelation: Redo Logs are the Heart of High Availability\n\nNow that you understand WAL, you understand how modern cloud databases like **Amazon Aurora** or **Google Spanner** work. \n\nIn a traditional database, you replicate the *entire database file* to a backup server. In a modern WAL-based system, you don't send the files; you only stream the **WAL Frames** over the network. The backup server (the \"Replica\") just keeps \"Redoing\" the frames it receives from the primary. This is called **Log Shipping**, and it's how we achieve sub-second failover in the cloud. You are building the \"Seed\" of a distributed system.\n\n---\n\n### Design Decisions: The WAL Frame Checksum\n\nIn Milestone 9, we worried about **Torn Pages**. In WAL mode, we handle this using **Cumulative Checksums**.\n\nEvery WAL frame contains a checksum that is a function of:\n1.  The Page Data.\n2.  The Checksum of the **Previous** frame.\n\nThis creates a \"Chain of Integrity.\" If a crash occurs and the last frame in the WAL is only partially written (torn), the checksum will fail. But more importantly, because the checksum is cumulative, we can detect if a frame in the middle of the log was corrupted. If a checksum fails, the WAL reader stops immediately and treats the rest of the file as \"junk,\" ensuring that partial or corrupted transactions are never \"Redone\" into the main database.\n\n---\n\n### Implementation Roadmap\n\n#### 1. The WAL File Structure\nImplement a `WalManager` that opens `[db-name]-wal`. Define a `write_frame(page_no, data)` method that appends the header, data, and calculates the cumulative checksum using a robust algorithm like CRC-32 or Fletcher's checksum.\n\n#### 2. The WAL-Index (Search Logic)\nCreate a lookup table (PageID $\\rightarrow$ FrameIndex). \n- *Intermediate Hack*: Rebuild this index from the WAL file every time the database starts by scanning the frame headers. In-memory, store it as a `HashMap<PageID, u32>`.\n\n#### 3. Integrated Pager (The Search)\nUpdate your Pager's `fetch_page(id)` logic to check the WAL before the main file:\n```rust\n// Conceptual Logic\nfn fetch_page(id: PageID) -> Page {\n    if let Some(frame_idx) = wal_index.lookup(id, current_read_mark) {\n        return read_from_wal(frame_idx); // Redo source\n    }\n    return read_from_main_db(id); // Main source\n}\n```\n\n#### 4. The Auto-Checkpoint\nIn your `COMMIT` logic, check the size of the WAL file. If it exceeds 1,000 pages (approx 4MB), trigger a `wal_checkpoint(PASSIVE)`.\n- **PASSIVE**: Checkpoint as many frames as possible without blocking any readers.\n- **RESTART**: Block until all readers are gone, then wipe the WAL.\n\n---\n\n### Design Decisions \u2014 WAL vs. Rollback Journal\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **Rollback Journal** | Simpler. Single-file database when idle. Better for purely read-heavy workloads with very rare writes. | Writers block all readers. `fsync()` happens twice per transaction. | SQLite (Legacy), Simple Embedded DBs |\n| **WAL Mode \u2713** | **Concurrent Readers & Writers**. High write throughput (Sequential I/O). | Higher complexity. Requires managing the WAL Index. DB is spread across multiple files. | SQLite (Modern), PostgreSQL, MySQL |\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **Redo Logs and Replay**: This \"Redo\" logic is the foundation of **Event Sourcing** in distributed systems. Instead of storing the \"Current State\" of an object, you store a log of every change and \"Replay\" it.\n2.  **Snapshot Isolation**: You've now implemented a form of **MVCC (Multi-Version Concurrency Control)**. This is exactly how **Git** handles branches\u2014multiple versions of a file coexist, and you \"check out\" a specific snapshot.\n3.  **LSM-Trees (Log-Structured Merge-Trees)**: WAL mode is essentially a tiny, one-level LSM-tree. Systems like **BigTable** and **Cassandra** take this to the extreme, where the *entire database* is just a series of sorted logs.\n4.  **CPU Write Buffering**: Your CPU has a \"Store Buffer.\" When you write to memory, the CPU appends the write to a buffer (the WAL) and continues executing. It only \"Checkpoints\" that buffer into the cache when the bus is free.\n\n### System Awareness\nThe WAL is the \"Concurrency Booster\" of your engine. It sits inside the **Pager** layer. When WAL is active, the Pager's role changes from \"Disk Coordinator\" to \"Version Switcher.\" Without the WAL, your database is robust but serial. With it, your database becomes a concurrent powerhouse, ready to handle the high-traffic demands of a real-world application.\n\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: build-sqlite-m11 -->\n<!-- MS_ID: build-sqlite-m11 -->\n# Milestone 11: Aggregate Functions & JOIN\n\nYou have built a database that can store, index, and retrieve individual rows with surgical precision. But data is rarely useful in isolation. Real-world questions\u2014\"What is the total revenue per region?\" or \"Show me the names of users who bought this specific item\"\u2014require the engine to perform **Relational Algebra**.\n\nIn this milestone, you will implement the two most powerful tools in the SQL arsenal: **Aggregate Functions** (COUNT, SUM, AVG, etc.) and **INNER JOINs**. You will move from a single-table execution model to a multi-cursor orchestration model. You will learn how to compute summaries over millions of rows without exhausting your RAM and how to connect disparate tables using the foundational **Nested Loop Join** algorithm.\n\n\n![The SQLite Satellite Map](./diagrams/diag-l0-map.svg)\n\n\n---\n\n### The Fundamental Tension: The Multi-Cursor Explosion\n\nThe tension in this milestone is **Declarative Simplicity vs. Combinatorial Complexity.**\n\nIn a single-table query, the cost of the query is linear: $O(N)$ for a scan or $O(\\log N)$ for an index lookup. When you introduce a **JOIN**, you are no longer looking at a list; you are looking at a **Cartesian Product**. If Table A has 1,000 rows and Table B has 1,000 rows, there are 1,000,000 possible combinations of those rows.\n\nAs the developer, you must ensure that your Virtual Machine (VDBE) can manage multiple cursors simultaneously without losing its place. If your join logic is inefficient, a simple three-table join can turn a sub-second query into a multi-minute \"hang\" that saturates the CPU. You are shifting from being a \"Librarian\" who finds one book to a \"Matchmaker\" who must find every valid pair in a crowded room.\n\n---\n\n### The Revelation Arc: The Join Misconception\n\n#### The Misconception\nIf you look at most SQL tutorials online, they explain `JOIN` using **Venn Diagrams**. They show two overlapping circles and say, \"The INNER JOIN is the intersection where the circles meet.\"\n\n#### The Scenario\nYou are building the execution logic for your database. You have an AST node representing `users JOIN orders ON users.id = orders.user_id`. You look at the Venn diagram. How do you turn that \"overlapping circle\" into a series of byte-level operations on a B-tree? \n\n#### The Reveal\n**A JOIN is not a mathematical intersection; it is a Nested Loop.**\n\nComputers do not \"intersect\" tables. They iterate. To a database engine, an `INNER JOIN` is physically a `for` loop inside another `for` loop.\n1.  **Outer Loop**: Open a cursor on Table A and move to the first row.\n2.  **Inner Loop**: Open a cursor on Table B. For the current row of Table A, scan Table B to find rows where the condition (the \"Predicate\") is true.\n3.  **The Match**: When a match is found, \"stitch\" the columns of both rows together and emit them as a single `ResultRow`.\n4.  **Repeat**: Move Table A to the next row and reset the Table B cursor to the beginning.\n\n**The Reveal**: The Venn diagram is a logical abstraction for the user. The **Nested Loop** is the physical reality of the machine. Once you realize this, you understand why Join Order (Milestone 8) and Secondary Indexes (Milestone 7) are so critical: if the Inner Loop has to do a full scan for every row of the Outer Loop, your database will crawl. If the Inner Loop can use an index to \"jump\" to the matching row, the $O(N \\times M)$ cost collapses back toward $O(N \\times \\log M)$.\n\n---\n\n### The Three-Level View: Joining the Dots\n\nTo understand how a Join executes, look at the layers of the engine:\n\n1.  **Level 1 \u2014 The API (SQL)**: `SELECT users.name, orders.amount FROM users JOIN orders ON users.id = orders.user_id`. The user expresses a relationship between two entities.\n2.  **Level 2 \u2014 The Execution Engine (VDBE)**: The VM manages two independent B-tree cursors. It uses a \"Rewind\" on the outer cursor and a \"Seek\" or \"Rewind\" on the inner cursor. It tracks the \"Nested Loop\" state using program counter jumps.\n3.  **Level 3 \u2014 The Storage Engine (B-tree)**: The cursors perform independent page-loads. One cursor might be at Page 50 of `users.db`, while the other is bouncing between Page 2 and Page 10 of `orders.db`.\n\n---\n\n### Mechanism 1: The Nested Loop Join (NLJ)\n\nYou will implement the **Nested Loop Join** as your baseline join algorithm. \n\n\n![Nested Loop Join Trace](./diagrams/diag-nested-loop-join.svg)\n\n\n#### The VDBE Opcodes for Join\nTo support joins, your compiler must emit a structure like this:\n\n| Addr | Opcode | Parameters | Logic |\n|------|--------|------------|-------|\n| 0 | `OpenRead` | `0 (users)` | Open outer cursor. |\n| 1 | `OpenRead` | `1 (orders)` | Open inner cursor. |\n| 2 | `Rewind` | `0, 10` | Start outer loop. Jump to 10 if empty. |\n| 3 | `Column` | `0, 0, r1` | Get `users.id` into Register 1. |\n| 4 | `SeekGE` | `1, 9, r1` | **Inner Loop Optimization**: Jump to `orders` row matching `r1`. |\n| 5 | `Column` | `0, 1, r2` | Get `users.name`. |\n| 6 | `Column` | `1, 1, r3` | Get `orders.amount`. |\n| 7 | `ResultRow` | `r2, 2` | Output the joined row. |\n| 8 | `Next` | `1, 5` | Next row in inner loop (if multiple orders per user). |\n| 9 | `Next` | `0, 3` | Next row in outer loop. |\n| 10| `Halt` | | Done. |\n\n**Key Insight**: Notice how the `Next` instruction at Address 8 loops back to the *inner* logic, while the `Next` at Address 9 loops back to the *outer* logic. This is the exact structure of a nested `for` loop in bytecode.\n\n---\n\n### Mechanism 2: Aggregate State Management\n\nAggregate functions like `SUM`, `COUNT`, and `AVG` pose a different challenge: they are **Stateful**. \n\nUnlike a simple `SELECT name`, which looks at one row at a time, `SUM(price)` needs to remember the total it has seen so far. You must implement the concept of an **Accumulator Register**.\n\n> \n> **\ud83d\udd11 Foundation: Aggregation State**\n> \n> ### 1. What it IS\nAggregation state is the temporary memory held by a database to track progress during a \"Folding\" operation (where many rows are reduced to one result). \n- For `COUNT`, the state is an integer (starts at 0).\n- For `SUM`, the state is a number (starts at 0 or NULL).\n- For `AVG`, the state is a **tuple**: `(running_sum, running_count)`.\n- For `MIN`/`MAX`, the state is the \"best value seen so far.\"\n\n### 2. WHY you need it right now\nIf you try to compute an average by loading all 1,000,000 rows into an array and then dividing, you will crash your engine with an `OutOfMemory` error. Streaming aggregation allows you to process 1,000,000 rows while only ever holding **two numbers** in memory: the sum and the count.\n\n### 3. Key Insight: The Initialize-Step-Finalize Lifecycle\nAggregates in SQL follow a three-step lifecycle:\n1.  **Init**: Reset the accumulator register to a default (0 or NULL).\n2.  **Step**: For every row found by the scan, update the accumulator (e.g., `acc = acc + new_value`).\n3.  **Finalize**: Perform the final math (e.g., for `AVG`, divide `sum` by `count`).\n\n\n#### Handling NULLs in Aggregates\nThe SQL standard defines specific (and sometimes counter-intuitive) rules for NULLs in aggregates:\n- `COUNT(*)`: Counts every row, even if all columns are NULL.\n- `COUNT(col)`: Only counts rows where `col` is **NOT NULL**.\n- `SUM(col)` / `AVG(col)`: Ignores NULL values completely. If all values in a group are NULL, the result of `SUM` is **NULL**, not 0.\n- `AVG` must return a **REAL** (floating point), even if the input column is an integer. \n\n---\n\n### Mechanism 3: GROUP BY (The Bucket Strategy)\n\n`GROUP BY` takes aggregation to the next level. Instead of one global accumulator, you need **one accumulator per group**.\n\nIf you run `SELECT category, SUM(price) FROM products GROUP BY category`, and you have 50 categories, you need 50 separate `SUM` counters. \n\n**There are two primary ways to implement this:**\n\n#### 1. Sort-Based Grouping (The Streaming Way)\nIf the input data is **already sorted** by the grouping column (e.g., via an index), you only need one accumulator.\n- As you scan, if the `category` is the same as the last row, update the current accumulator.\n- If the `category` changes, \"Finalize\" the current accumulator, emit the result row, and reset the accumulator for the new category.\n\n#### 2. Hash-Based Grouping (The Memory Way)\nIf the data is unsorted, you must use a **Hash Map** in memory.\n- The Key of the map is the grouping value (`category`).\n- The Value of the map is the **Aggregation State** object.\n- For every row, look up the category in the map and update its state.\n- Once the scan is finished, iterate through the Hash Map and emit one row per entry.\n\n**Implementation Tip**: For this milestone, if you haven't built a robust internal Hash Map yet, you can implement the **Sort-Based** approach by first performing a \"Sort\" operation on the input data (or requiring an index).\n\n---\n\n### Mechanism 4: The HAVING Clause\n\nThe `HAVING` clause is a filter that applies **after** aggregation. This is the \"Aha!\" moment of SQL query order. \n\nConsider: `SELECT category FROM products GROUP BY category HAVING SUM(price) > 1000`.\n- The `WHERE` clause filters rows **before** they are grouped (Physical rows).\n- The `HAVING` clause filters rows **after** they are grouped (Logical groups).\n\nIn your VDBE, the `HAVING` logic must be placed *after* the `Finalize` step of the aggregate but *before* the `ResultRow` opcode. It acts exactly like a `WHERE` clause, but its inputs are the values in the accumulator registers rather than the column values from the B-tree cursors.\n\n---\n\n### The Revelation Arc: Joins and Three-Valued Logic\n\nWhen joining tables, you will encounter the \"Missing Data\" problem. \n`SELECT * FROM users JOIN orders ON users.id = orders.user_id`\n\nIf a user exists but has **zero orders**, the `INNER JOIN` logic will result in the inner loop (orders) finding 0 matches. The `ResultRow` is never reached. Consequently, the user \"disappears\" from the results. \n\n**The Reveal**: This is why `NULL` handling is so critical in joins. If you were implementing an `OUTER JOIN`, you would need logic to say: \"If the inner loop finishes without finding a single match, emit a row anyway, but fill the 'orders' columns with `NULL`.\" While this milestone focuses on `INNER JOIN`, ensuring your `Column` opcode can return a `Value::Null` for non-existent matches is the first step toward supporting all join types.\n\n---\n\n### Design Decisions: Join Algorithms\n\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **Nested Loop Join (Chosen \u2713)** | Simple to implement. Works with any join condition (`>`, `<`, `!=`). Uses very little memory. | $O(N \\times M)$ performance. Deadly without indexes. | SQLite, All RDBMS (as fallback) |\n| **Hash Join** | Extremely fast ($O(N+M)$) for equality joins. | Only works for `=` joins. Requires building a large hash table in memory. | PostgreSQL, MySQL 8.0+, SQL Server |\n| **Merge Join** | Very fast if both sides are already sorted. | Requires sorting both tables first, which can be $O(N \\log N)$. | Oracle, PostgreSQL |\n\n**Why we choose NLJ**: In the context of an embedded database like SQLite, memory is often scarce. A Nested Loop Join (especially an **Indexed Nested Loop Join**) provides a great balance of code simplicity and performance. It allows us to leverage the B-trees we've already spent so much time perfecting.\n\n---\n\n### Knowledge Cascade: Learn One, Unlock Ten\n\n1.  **Nested Loop Joins & Relational Algebra**: This is the \"Multiplication\" of the database world. By mastering the nested loop, you've understood how databases implement the **Cross Product** ($\\times$) and **Join** ($\\bowtie$) operators that form the basis of relational theory.\n2.  **Aggregation State & Streaming Analytics**: The \"Initialize-Step-Finalize\" pattern is the exact same logic used in **MapReduce** (Google) and **Apache Flink**. \"Map\" is the scan, \"Shuffle\" is the grouping, and \"Reduce\" is the aggregation step.\n3.  **Venn Diagrams vs. Loops**: This insight\u2014that mathematical sets are physically loops\u2014applies to **Computer Graphics** (clipping pixels), **Computational Geometry** (intersecting shapes), and **Search Engines** (intersecting keyword lists).\n4.  **NULL Propagation**: Your work with aggregates and NULLs is a primer for **Functional Programming**. The way NULL \"poisons\" an `AVG` or `SUM` is identical to how the `Option` or `Maybe` monad works in languages like Rust or Haskell.\n\n---\n\n### System Awareness\n\nIn the System Map, we are now at the **highest level of the Execution Engine**. \n- Upstream: The **Query Planner** (Milestone 8) is deciding which table should be the \"Outer\" one.\n- Downstream: The **VDBE** is managing multiple **Cursors** (Milestone 6) and pulling pages through the **Buffer Pool** (Milestone 4).\n\nIf your Buffer Pool is too small to hold the \"hot\" pages of both tables being joined, the system will start **Thrashing**. Each iteration of the outer loop will push the inner table's pages out of the cache, and each iteration of the inner loop will push the outer table's pages out. This is why tuning your Buffer Pool size is often the first step in fixing a \"slow join.\"\n\n---\n<!-- END_MS -->\n",
  "current_ms_index": 11,
  "diagrams_to_generate": [],
  "diagram_attempt": 0,
  "current_diagram_code": null,
  "current_diagram_meta": null,
  "last_error": null,
  "status": "tdd_writing",
  "phase": "tdd",
  "tdd_blueprint": {
    "project_title": "Build Your Own SQLite",
    "design_vision": "A high-performance, embedded relational engine prioritizing ACID durability via Write-Ahead Logging and efficient data retrieval through a page-based B-tree storage engine and a register-based virtual machine.",
    "modules": [
      {
        "id": "build-sqlite-m1",
        "name": "SQL Tokenizer",
        "description": "Lexical analyzer that converts raw SQL strings into a stream of typed tokens using a Finite State Machine (FSM). Handles keywords, identifiers, numeric/string literals, and operators.",
        "specs": {
          "inputs": "Raw UTF-8 SQL string buffer.",
          "outputs": "List or Stream of Token structs {type, lexeme, literal_value, line, col}.",
          "abstractions": "Scanner state, TokenType enum, Character classification.",
          "error_categories": [
            "UnterminatedString",
            "InvalidCharacter",
            "NumericFormatError"
          ],
          "concurrency_model": "Single-threaded during query preparation.",
          "performance_targets": [
            "Scan 1MB of SQL in < 5ms",
            "O(N) single-pass complexity"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Character Classification",
            "estimated_hours": "1-2"
          },
          {
            "phase": 2,
            "name": "FSM State Implementation",
            "estimated_hours": "2-3"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m1-arch",
            "title": "Lexer Architecture",
            "description": "Scanner struct containing pointers and state machine dispatch logic.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m1"
          },
          {
            "id": "tdd-diag-m1-fsm",
            "title": "Tokenizer State Machine",
            "description": "State transitions for identifiers, numbers, and quoted strings.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m1"
          },
          {
            "id": "tdd-diag-m1-esc",
            "title": "SQL String Escape Logic",
            "description": "Lookahead logic for handling doubled-single-quotes ('') in literals.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m1"
          }
        ]
      },
      {
        "id": "build-sqlite-m2",
        "name": "SQL Parser (AST)",
        "description": "Recursive-descent parser that consumes tokens to build an Abstract Syntax Tree representing the logical query plan.",
        "specs": {
          "inputs": "Stream of Tokens.",
          "outputs": "Root AST Node (Statement).",
          "abstractions": "Statement, Expression, ColumnDef, BinaryOp.",
          "error_categories": [
            "UnexpectedToken",
            "InvalidSyntax",
            "IncompleteExpression"
          ],
          "concurrency_model": "None (Pure function).",
          "performance_targets": [
            "Parse 1000 nodes in < 1ms"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Expression Pratt Parsing",
            "estimated_hours": "3-4"
          },
          {
            "phase": 2,
            "name": "Statement Recursive Descent",
            "estimated_hours": "3-4"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m2-ast-node",
            "title": "AST Node Memory Layout",
            "description": "Struct definition for variant types (Select, Insert, BinaryExpr).",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m2"
          },
          {
            "id": "tdd-diag-m2-pratt-flow",
            "title": "Pratt Parsing Precedence Climbing",
            "description": "Recursive flow showing how binding power groups tokens like 5 + 3 * 2.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m2"
          },
          {
            "id": "tdd-diag-m2-select-flow",
            "title": "SELECT Statement Data Flow",
            "description": "Transformation of tokens into a SelectStatement tree structure.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m2"
          }
        ]
      },
      {
        "id": "build-sqlite-m3",
        "name": "Bytecode Compiler (VDBE)",
        "description": "Translates AST into low-level instructions for a register-based Virtual Machine.",
        "specs": {
          "inputs": "AST Root.",
          "outputs": "Instruction Array (Opcodes).",
          "abstractions": "Register, Opcode, Cursor, ProgramCounter.",
          "error_categories": [
            "RegisterOverflow",
            "InvalidOpcode",
            "TableNotFound"
          ],
          "concurrency_model": "VM runs in single-threaded context per connection.",
          "performance_targets": [
            "Fetch-Decode loop < 10ns overhead per instruction"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Instruction Set Design",
            "estimated_hours": "3-4"
          },
          {
            "phase": 2,
            "name": "Register Allocation Logic",
            "estimated_hours": "3-4"
          },
          {
            "phase": 3,
            "name": "VM Dispatch Loop",
            "estimated_hours": "3-4"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m3-vm-arch",
            "title": "VDBE Runtime Architecture",
            "description": "Interactions between PC, Registers, Cursors, and Pager.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m3"
          },
          {
            "id": "tdd-diag-m3-reg-layout",
            "title": "Register File Memory Layout",
            "description": "Dynamic typed Value array (Integer, Real, Text, Blob, Null).",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m3"
          },
          {
            "id": "tdd-diag-m3-compile-flow",
            "title": "AST to Bytecode Mapping",
            "description": "How a WHERE clause compiles into conditional jump instructions.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m3"
          }
        ]
      },
      {
        "id": "build-sqlite-m4",
        "name": "Buffer Pool Manager",
        "description": "Manages fixed-size memory frames, caching disk pages using LRU eviction and dirty tracking.",
        "specs": {
          "inputs": "Page ID requests from B-tree.",
          "outputs": "Memory pointers to 4096-byte frames.",
          "abstractions": "Pager, Frame, EvictionPolicy, FileHandle.",
          "error_categories": [
            "PoolFull (All pinned)",
            "DiskIOError",
            "CorruptionDetected"
          ],
          "concurrency_model": "Synchronized access to frame metadata (latches).",
          "performance_targets": [
            "Cache hit rate > 90% for standard workloads",
            "LRU lookup O(1)"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "LRU with Hashmap-Linked-List",
            "estimated_hours": "3-4"
          },
          {
            "phase": 2,
            "name": "Pager Fetch/Evict Logic",
            "estimated_hours": "4-5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m4-pager-arch",
            "title": "Pager Architecture",
            "description": "Relationship between Page Cache, LRU List, and Disk Interface.",
            "type": "architecture",
            "anchor_target": "build-sqlite-m4"
          },
          {
            "id": "tdd-diag-m4-frame-layout",
            "title": "Page Frame Memory Layout",
            "description": "4KB buffer + metadata (PageID, PinCount, DirtyBit, LRUPointers).",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m4"
          },
          {
            "id": "tdd-diag-m4-lru-algo",
            "title": "LRU Eviction Algorithm",
            "description": "Visualizing Hash Map O(1) lookups and List Splice operations.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m4"
          },
          {
            "id": "tdd-diag-m4-pin-seq",
            "title": "Page Pinning Sequence",
            "description": "Lifecycle of a page from fetch to unpin, preventing premature eviction.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m4"
          }
        ]
      },
      {
        "id": "build-sqlite-m5",
        "name": "B-tree Page Format & Table Storage",
        "description": "Slotted page implementation for clustered table storage. Handles rowid-keyed B-trees.",
        "specs": {
          "inputs": "Raw row data and RowID.",
          "outputs": "Serialized pages on disk.",
          "abstractions": "SlottedPage, Cell, Varint, ClusteredIndex.",
          "error_categories": [
            "PageOverflow",
            "DuplicateRowID",
            "InvalidPageType"
          ],
          "concurrency_model": "Page-level locks; single-writer/multi-reader.",
          "performance_targets": [
            "Serialization overhead < 5% of I/O time",
            "Log(N) search depth"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Slotted Page Header/Cell Logic",
            "estimated_hours": "4-5"
          },
          {
            "phase": 2,
            "name": "Varint Encoding/Decoding",
            "estimated_hours": "2-3"
          },
          {
            "phase": 3,
            "name": "B-tree Split Algorithm",
            "estimated_hours": "5-6"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m5-slotted-layout",
            "title": "Slotted Page Binary Format",
            "description": "Visualizing bidirectional growth: Header at top, Cells at bottom, Free space in middle.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-split-steps",
            "title": "B-tree Node Split Sequence",
            "description": "Creating new sibling, moving median key to parent, and re-linking.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m5"
          },
          {
            "id": "tdd-diag-m5-varint-layout",
            "title": "SQLite Varint Bit Layout",
            "description": "7-bit payload bytes with MSB continuation flags.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m5"
          }
        ]
      },
      {
        "id": "build-sqlite-m6",
        "name": "SELECT Execution & DML",
        "description": "Execution of SQL operations via cursors. Handles projection, filtering, and table mutations.",
        "specs": {
          "inputs": "VDBE Opcodes.",
          "outputs": "Result rows or B-tree modifications.",
          "abstractions": "Cursor, RowDeserializer, PredicateEvaluator.",
          "error_categories": [
            "ConstraintViolation",
            "NoSuchTable",
            "TypeMismatch"
          ],
          "concurrency_model": "Lock upgrade (Shared to Reserved).",
          "performance_targets": [
            "Scan 10k rows in < 100ms"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Cursor Navigation API",
            "estimated_hours": "4-5"
          },
          {
            "phase": 2,
            "name": "Record Deserialization",
            "estimated_hours": "3-4"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m6-cursor-sm",
            "title": "Cursor State Machine",
            "description": "States: Uninitialized, ValidRow, EOF, InvalidatedByWrite.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m6"
          },
          {
            "id": "tdd-diag-m6-dml-flow",
            "title": "DML Write Pipeline",
            "description": "AST -> VDBE -> Record Packer -> B-tree Insert.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m6"
          }
        ]
      },
      {
        "id": "build-sqlite-m7",
        "name": "Secondary Indexes",
        "description": "Maintains non-clustered B+trees for column-value lookups mapping to RowIDs.",
        "specs": {
          "inputs": "Indexed column values.",
          "outputs": "RowIDs (for Double Lookup).",
          "abstractions": "SecondaryIndex, B+Tree, KeySerialization.",
          "error_categories": [
            "UniqueConstraintFailed",
            "StaleIndexEntry"
          ],
          "concurrency_model": "Synchronous index updates during table writes.",
          "performance_targets": [
            "Index lookup O(log N) vs O(N) scan"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "B+Tree Leaf Linking",
            "estimated_hours": "4-5"
          },
          {
            "phase": 2,
            "name": "Unique Constraint Enforcement",
            "estimated_hours": "2-3"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m7-index-lookup",
            "title": "Double-Lookup Sequence",
            "description": "Search Index B+tree -> Retrieve RowID -> Search Table B-tree -> Return Row.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m7"
          },
          {
            "id": "tdd-diag-m7-index-cell",
            "title": "Index Cell Layout",
            "description": "Key content followed by RowID payload.",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m7"
          }
        ]
      },
      {
        "id": "build-sqlite-m8",
        "name": "Query Planner & Statistics",
        "description": "Cost-based optimizer choosing access paths based on selectivity and cardinality statistics.",
        "specs": {
          "inputs": "SQL Query + Table Statistics.",
          "outputs": "Optimized Bytecode Plan.",
          "abstractions": "CostModel, SelectivityEstimator, PlanCandidate.",
          "error_categories": [
            "StatisticsStale",
            "PlannerTimeout"
          ],
          "concurrency_model": "Read-only access to system stats catalog.",
          "performance_targets": [
            "Plan generation < 1ms for standard queries"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "ANALYZE (Stats Collection)",
            "estimated_hours": "5-6"
          },
          {
            "phase": 2,
            "name": "Cost Model Implementation",
            "estimated_hours": "4-5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m8-planner-flow",
            "title": "Planner Decision Tree",
            "description": "Evaluating Full Scan vs Index Scan based on selectivity threshold.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m8"
          },
          {
            "id": "tdd-diag-m8-stats-model",
            "title": "Cost Calculation Data Flow",
            "description": "Inputs: PageCount, RowCount, Selectivity -> Output: Estimated I/O Units.",
            "type": "data_flow",
            "anchor_target": "build-sqlite-m8"
          }
        ]
      },
      {
        "id": "build-sqlite-m9",
        "name": "Transactions (Rollback Journal)",
        "description": "Atomicity and Durability manager using a journal file to record original pages before modification.",
        "specs": {
          "inputs": "Dirty pages from Pager.",
          "outputs": "fsync'd journal and data files.",
          "abstractions": "JournalHeader, UndoLog, LockManager.",
          "error_categories": [
            "JournalCorrupt",
            "LockDeadlock",
            "PartialWrite"
          ],
          "concurrency_model": "Multi-state locking (Shared, Reserved, Pending, Exclusive).",
          "performance_targets": [
            "Zero data loss on power failure",
            "fsync count < 3 per transaction"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Locking State Machine",
            "estimated_hours": "3-4"
          },
          {
            "phase": 2,
            "name": "Journaling & fsync Ordering",
            "estimated_hours": "4-5"
          },
          {
            "phase": 3,
            "name": "Crash Recovery Logic",
            "estimated_hours": "3-4"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m9-write-seq",
            "title": "Atomic Commit Sequence (Rollback)",
            "description": "Journal write -> Journal fsync -> Data write -> Data fsync -> Journal Delete.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m9"
          },
          {
            "id": "tdd-diag-m9-locks",
            "title": "Database Locking State Machine",
            "description": "Transitions between Shared, Reserved, and Exclusive locks.",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m9"
          },
          {
            "id": "tdd-diag-m9-recovery-flow",
            "title": "Crash Recovery Algorithm",
            "description": "Detection of hot journal and bit-for-bit restoration of main DB.",
            "type": "algorithm_steps",
            "anchor_target": "build-sqlite-m9"
          }
        ]
      },
      {
        "id": "build-sqlite-m10",
        "name": "WAL Mode",
        "description": "Alternative durability mode allowing concurrent readers and writers by appending changes to a log file.",
        "specs": {
          "inputs": "Dirty pages.",
          "outputs": "WAL frames.",
          "abstractions": "WALFrame, WALIndex, Checkpointer.",
          "error_categories": [
            "WALChecksumMismatch",
            "CheckpointBlocked"
          ],
          "concurrency_model": "Snapshot Isolation; concurrent readers + single writer.",
          "performance_targets": [
            "Write throughput 2-5x higher than Rollback Mode"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "WAL File Format",
            "estimated_hours": "4-5"
          },
          {
            "phase": 2,
            "name": "WAL Index (mmap/shared-memory)",
            "estimated_hours": "4-5"
          },
          {
            "phase": 3,
            "name": "Checkpointing Logic",
            "estimated_hours": "4-5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m10-wal-layout",
            "title": "WAL File Binary Layout",
            "description": "WAL Header + repeated Frames [FrameHeader, PageData].",
            "type": "memory_layout",
            "anchor_target": "build-sqlite-m10"
          },
          {
            "id": "tdd-diag-m10-isolation",
            "title": "Snapshot Isolation (Read Marks)",
            "description": "Reader pinning a WAL frame index to maintain consistent view during writes.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m10"
          }
        ]
      },
      {
        "id": "build-sqlite-m11",
        "name": "Aggregate Functions & JOIN",
        "description": "Multi-table query execution using Nested Loop Joins and streaming aggregation logic.",
        "specs": {
          "inputs": "Multi-cursor query plan.",
          "outputs": "Aggregated result set.",
          "abstractions": "Accumulator, HashJoin, CrossProduct.",
          "error_categories": [
            "AmbiguousColumn",
            "AggregateOnNonNumeric"
          ],
          "concurrency_model": "Multiple cursors in one transaction context.",
          "performance_targets": [
            "Join O(N*log M) with indexing"
          ]
        },
        "implementation_phases": [
          {
            "phase": 1,
            "name": "Aggregate Accumulator Logic",
            "estimated_hours": "4-5"
          },
          {
            "phase": 2,
            "name": "Nested Loop Join Implementation",
            "estimated_hours": "6-7"
          },
          {
            "phase": 3,
            "name": "GROUP BY Bucket Management",
            "estimated_hours": "4-5"
          }
        ],
        "diagrams": [
          {
            "id": "tdd-diag-m11-nlj-flow",
            "title": "Nested Loop Join Flow",
            "description": "Outer cursor scan -> Inner cursor seek/match -> Result emit.",
            "type": "sequence",
            "anchor_target": "build-sqlite-m11"
          },
          {
            "id": "tdd-diag-m11-agg-sm",
            "title": "Aggregate State Machine",
            "description": "States: Initialize -> Step (Process Row) -> Finalize (Output).",
            "type": "state_machine",
            "anchor_target": "build-sqlite-m11"
          }
        ]
      }
    ]
  },
  "tdd_accumulated_md": "\n\n# TDD\n\nA high-performance, embedded relational engine prioritizing ACID durability via Write-Ahead Logging and efficient data retrieval through a page-based B-tree storage engine and a register-based virtual machine.\n\n\n\n<!-- TDD_MOD_ID: build-sqlite-m1 -->\n# Module Design: SQL Tokenizer (build-sqlite-m1)\n\n## 1. Module Charter\n\nThe **SQL Tokenizer** (Lexer) is the entry point of the SQL execution pipeline. Its primary responsibility is to transform a raw UTF-8 string buffer into a linear stream of discrete, typed **Tokens**. This module performs lexical analysis only; it does not validate SQL grammar or schema existence. \n\n**Core Responsibilities:**\n- Scan the input buffer in a single O(N) pass.\n- Categorize character sequences into keywords, identifiers, literals (string/numeric), and operators.\n- Track source locations (line and column) for precise error reporting in downstream modules.\n- Normalize case for keywords while preserving case for quoted identifiers and string literals.\n\n**Non-Goals:**\n- Validating if a table exists.\n- Checking if a `SELECT` statement has a `FROM` clause.\n- Handling multi-statement scripts (this module processes one \"Statement Block\" at a time).\n\n**Invariants:**\n- The output token stream must preserve the logical order of the input.\n- White space and comments are discarded after serving as token delimiters.\n- Every byte of input must either contribute to a token or trigger a lexical error.\n\n---\n\n## 2. File Structure\n\nImplementation should follow this numbered sequence to establish dependencies:\n\n1. `src/include/tokenizer/token_types.h`: Definition of the `TokenType` enumeration.\n2. `src/include/tokenizer/token.h`: The `Token` structure and literal value unions.\n3. `src/include/tokenizer/scanner.h`: The state machine container definition.\n4. `src/tokenizer/scanner.c`: Core logic for character classification and state transitions.\n5. `src/tokenizer/keywords.c`: Lookup table for SQL reserved words.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 TokenType Enum\nThe `TokenType` distinguishes between syntax, data, and metadata.\n\n| Category | Type | Example |\n| :--- | :--- | :--- |\n| **Keywords** | `TK_SELECT`, `TK_INSERT`, `TK_CREATE`, `TK_TABLE`, `TK_FROM`, `TK_WHERE`, `TK_JOIN`, `TK_ORDER`, `TK_BY`, `TK_LIMIT`, `TK_AND`, `TK_OR`, `TK_NOT`, `TK_NULL`, `TK_VALUES`, `TK_PRIMARY`, `TK_KEY` | `SELECT`, `FROM` |\n| **Literals** | `TK_STRING`, `TK_INTEGER`, `TK_FLOAT`, `TK_IDENTIFIER` | `'it''s'`, `42`, `3.14`, `users` |\n| **Operators** | `TK_EQUAL`, `TK_BANG_EQUAL`, `TK_LESS`, `TK_LESS_EQUAL`, `TK_GREATER`, `TK_GREATER_EQUAL` | `=`, `!=`, `<=`, `>=` |\n| **Symbols** | `TK_LEFT_PAREN`, `TK_RIGHT_PAREN`, `TK_COMMA`, `TK_DOT`, `TK_SEMICOLON`, `TK_STAR` | `(`, `)`, `,`, `.`, `;`, `*` |\n| **Special** | `TK_EOF`, `TK_ERROR` | (End of input / Lexical error) |\n\n### 3.2 Token Structure\nFor expert-level performance, use a \"String View\" approach to avoid excessive allocations.\n\n```c\ntypedef enum {\n    VAL_INT,\n    VAL_FLOAT,\n    VAL_STR,\n    VAL_NONE\n} LiteralType;\n\ntypedef struct {\n    TokenType type;\n    const char* start;   // Pointer to start of lexeme in source buffer\n    size_t length;       // Length of lexeme\n    int line;\n    int col;\n    \n    // Literal interpretation (optional at this stage, but helpful)\n    struct {\n        LiteralType type;\n        union {\n            int64_t i_val;\n            double f_val;\n        } as;\n    } literal;\n} Token;\n```\n\n### 3.3 Scanner State\n```c\ntypedef struct {\n    const char* source;  // The full SQL string\n    const char* start;   // Start of current token being scanned\n    const char* current; // Current character pointer\n    int line;\n    int col;\n} Scanner;\n```\n\n{{DIAGRAM:tdd-diag-m1-arch}}\n*(Visual: Pipeline showing `Buffer -> Scanner -> Token Stream -> ResultSet`)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `void scanner_init(Scanner* scanner, const char* source)`\n- **Purpose**: Initialize the FSM state.\n- **Pre-condition**: `source` must be a null-terminated UTF-8 string.\n- **Post-condition**: Scanner pointers are set to the first byte; line and col set to 1.\n\n### 4.2 `Token scanner_next_token(Scanner* scanner)`\n- **Purpose**: The core FSM execution. Scans until the next valid token is found.\n- **Returns**: A `Token` struct. If an error is found, `type` is `TK_ERROR` and `lexeme` contains the error message.\n- **Constraints**: \n    - Single pass (O(1) amortized).\n    - Must skip all whitespace (space, tab, newline, carriage return).\n\n### 4.3 `bool scanner_is_at_end(Scanner* scanner)`\n- **Returns**: True if `*current == '\\0'`.\n\n---\n\n## 5. Algorithm Specification: The FSM\n\nThe tokenizer is a **Deterministic Finite Automaton (DFA)**. The state is implicitly determined by the current character under the `current` pointer.\n\n### 5.1 Basic Character Classification\nUse a lookup table or macros for fast classification:\n- `IS_DIGIT(c)`: `(c >= '0' && c <= '9')`\n- `IS_ALPHA(c)`: `(c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || (c == '_')`\n- `IS_WHITESPACE(c)`: `(c == ' ' || c == '\\r' || c == '\\t' || c == '\\n')`\n\n### 5.2 The Main Loop (Algorithm: `scan_token`)\n1. **Skip Whitespace**: \n    - While `IS_WHITESPACE(*current)`:\n        - If `\\n`, increment `line`, reset `col`.\n        - Else, increment `col`.\n        - Advance `current`.\n2. **Mark Start**: Set `scanner->start = scanner->current`.\n3. **Switch on `*current`**:\n    - **Single-byte symbols**: `(`, `)`, `,`, `.`, `;`, `*`, `+`, `-`. Return corresponding type.\n    - **Operators with lookahead**: \n        - If `!`, peek next. If `=`, return `TK_BANG_EQUAL`. Else return `TK_ERROR`.\n        - If `=`, return `TK_EQUAL`.\n        - If `<`, peek next. If `=`, return `TK_LESS_EQUAL`. If `>`, return `TK_BANG_EQUAL` (SQL style). Else return `TK_LESS`.\n    - **String Literals**: If `'`, call `scan_string()`.\n    - **Identifiers/Keywords**: If `IS_ALPHA`, call `scan_identifier()`.\n    - **Numbers**: If `IS_DIGIT`, call `scan_number()`.\n\n{{DIAGRAM:tdd-diag-m1-fsm}}\n*(Visual: State machine diagram with edges for String, Number, and Identifier branching)*\n\n### 5.3 `scan_string()` (The Escaped Quote Logic)\n1. Advance past initial `'`.\n2. Loop:\n    - If at end: return `TK_ERROR` (\"Unterminated string\").\n    - If `\\n`: increment `line`.\n    - If `'`:\n        - **Peek** next. If it is also `'`:\n            - This is an escaped quote. Consume both. Stay in loop.\n        - Else:\n            - The string is closed. Advance past `'` and break.\n    - Advance `current`.\n3. Return `TK_STRING`.\n\n{{DIAGRAM:tdd-diag-m1-esc}}\n*(Visual: Detail of the string escape state: ' -> [' -> ' -> stay] vs [non-' -> stay] vs [end -> exit])*\n\n### 5.4 `scan_number()` (Int vs Float)\n1. Consume all digits.\n2. If `.` is found AND next is a digit:\n    - Consume `.`.\n    - Consume all subsequent digits.\n    - Return `TK_FLOAT`.\n3. Else: return `TK_INTEGER`.\n\n### 5.5 `scan_identifier()` (Keyword Triage)\n1. Consume all `IS_ALPHA` or `IS_DIGIT` characters.\n2. Extract the lexeme.\n3. Perform a case-insensitive comparison against the keyword table.\n4. If match: return `TK_[KEYWORD]`.\n5. Else: return `TK_IDENTIFIER`.\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| Unterminated String | `scan_string` | Return `TK_ERROR`, advance to EOF | Yes: \"Unterminated string literal\" |\n| Invalid Character | `scan_token` default case | Return `TK_ERROR`, advance one byte | Yes: \"Unexpected character '@'\" |\n| Malformed Number | `scan_number` | Return `TK_ERROR`, consume until whitespace | Yes: \"Invalid numeric format\" |\n| Unterminated Quoted Ident | `scan_quoted_ident` | Return `TK_ERROR`, advance to EOF | Yes: \"Unterminated identifier\" |\n\n---\n\n## 7. Implementation Sequence\n\n### Phase 1: Skeleton & Symbols (2 Hours)\n- Define `TokenType` and `Token` structs.\n- Implement `scanner_init` and `scanner_next_token` for single-byte symbols only.\n- **Checkpoint**: Scanning `( , ; )` should return 4 tokens followed by `TK_EOF`.\n\n### Phase 2: Literals & Lookahead (3 Hours)\n- Implement `scan_string` with escape logic.\n- Implement `scan_number`.\n- Implement multi-byte operators (`>=`, `!=`).\n- **Checkpoint**: `SELECT 'it''s' FROM t WHERE x >= 10.5;` should correctly identify the string `'it''s'` as one token and `10.5` as a float.\n\n### Phase 3: Keywords & Identifiers (2 Hours)\n- Implement `scan_identifier` and the keyword lookup table.\n- Ensure case-insensitivity (e.g., `sElEcT` matches `TK_SELECT`).\n- **Checkpoint**: Full query `SELECT * FROM users` returns `[TK_SELECT, TK_STAR, TK_FROM, TK_IDENTIFIER]`.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Complex SELECT\n- **Input**: `SELECT id, name FROM \"Users\" WHERE salary > 50000.00;`\n- **Expected Output**:\n    1. `TK_SELECT`, \"SELECT\"\n    2. `TK_IDENTIFIER`, \"id\"\n    3. `TK_COMMA`, \",\"\n    4. `TK_IDENTIFIER`, \"name\"\n    5. `TK_FROM`, \"FROM\"\n    6. `TK_IDENTIFIER`, \"Users\" (Length 5, skip quotes)\n    7. `TK_WHERE`, \"WHERE\"\n    8. `TK_IDENTIFIER`, \"salary\"\n    9. `TK_GREATER`, \">\"\n    10. `TK_FLOAT`, \"50000.00\"\n    11. `TK_SEMICOLON`, \";\"\n    12. `TK_EOF`\n\n### 8.2 Edge Case: Escaped Quotes\n- **Input**: `'O''Reilly'`\n- **Expected**: `TK_STRING`, lexeme `'O''Reilly'`. (Note: The literal value interpretation would be `O'Reilly`, but the lexeme in the token should match the source).\n\n### 8.3 Edge Case: Whitespace and Lines\n- **Input**: \n  ```sql\n  SELECT \n  * \n  FROM \n  t;\n  ```\n- **Expected**: `TK_STAR` should report `line: 2`, `TK_FROM` should report `line: 3`.\n\n### 8.4 Failure Case: Unterminated String\n- **Input**: `SELECT 'unclosed string`\n- **Expected**: `TK_SELECT`, followed by `TK_ERROR` with message containing \"Unterminated\".\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Throughput** | > 200 MB/s | Use `clock_gettime` to measure time to tokenize a repeating 1MB SQL string 100 times. |\n| **Pass Count** | Exactly 1 | Verify that the `current` pointer never decrements (no backtracking). |\n| **Allocation** | 0 per Token | Ensure `Token.lexeme` is a pointer/length into the existing buffer, not a `strdup`. |\n\n---\n\n## 10. Memory Layout of Hot Structures\n\nSince the Tokenizer is often the bottleneck in high-volume query ingestion (e.g., bulk inserts), the `Scanner` state should fit within a single CPU cache line (64 bytes).\n\n**Scanner Layout (Offsets):**\n- `0x00`: `source` pointer (8 bytes)\n- `0x08`: `start` pointer (8 bytes)\n- `0x10`: `current` pointer (8 bytes)\n- `0x18`: `line` (4 bytes)\n- `0x1C`: `col` (4 bytes)\n- `0x20`: Padding/Reserved (32 bytes)\n- **Total**: 64 bytes.\n\nThe `TokenType` enum should be 1 byte if possible (packed) to keep `Token` structs small during large stream operations.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m2 -->\n# Module Design: SQL Parser (AST) (build-sqlite-m2)\n\n## 1. Module Charter\n\nThe **SQL Parser** is the structural architect of the database engine. It consumes the linear stream of tokens produced by the Tokenizer and constructs a hierarchical **Abstract Syntax Tree (AST)** that represents the logical intent of the query. Its primary goal is to enforce the SQL grammar and resolve operator precedence, ensuring that the subsequent Bytecode Compiler (VDBE) receives a syntactically perfect and unambiguous tree.\n\n**Core Responsibilities:**\n- Implement a **Recursive Descent** parser for high-level SQL statements (`SELECT`, `INSERT`, `CREATE TABLE`).\n- Implement a **Pratt Parser** (Precedence Climbing) for complex expressions to correctly handle mathematical and logical operator precedence (e.g., `AND` vs `OR`).\n- Transform raw token strings (like escaped quotes `'it''s'`) into normalized literal values within AST nodes.\n- Provide \"Panic Mode\" error recovery to detect multiple syntax errors in a single pass.\n- Utilize an **Arena Allocator** for AST nodes to guarantee $O(1)$ allocation and sub-millisecond cleanup.\n\n**Non-Goals:**\n- **Semantic Validation**: This module does not check if a table or column exists in the database schema.\n- **Type Checking**: It does not validate if you are adding a string to an integer (this happens in the VDBE/Binder).\n- **Optimization**: No query rewriting occurs here; the AST is a \"raw\" syntactic representation.\n\n**Invariants:**\n- Every node in the AST must maintain a reference to its source token for error localization.\n- The parser must be stateless regarding the database file; it only operates on the token stream.\n- An empty token stream must result in a `NULL` or `EMPTY_STMT` node, never a crash.\n\n---\n\n## 2. File Structure\n\nImplementation follows this numbered sequence:\n\n1. `src/include/parser/ast_nodes.h`: Definition of AST node types and the Tagged Union structure.\n2. `src/include/parser/arena.h`: Interface for the Arena Allocator.\n3. `src/parser/arena.c`: Implementation of the contiguous memory block allocator.\n4. `src/parser/expr_parser.c`: The Pratt Parsing logic for expressions.\n5. `src/parser/stmt_parser.c`: Recursive descent logic for `SELECT`, `INSERT`, and `CREATE`.\n6. `src/parser/parser.c`: Top-level entry point and error recovery logic.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 AST Node Memory Layout (Expert Level)\nTo achieve the performance target of 1000 nodes in < 1ms, we avoid individual `malloc` calls. We use a **Contiguous Tagged Union** pattern. Each node is exactly 64 bytes to align with cache lines.\n\n| Offset | Type | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | `uint32_t` | `type` | `AST_SELECT`, `AST_EXPR_BINARY`, etc. |\n| 0x04 | `uint32_t` | `line` | Source line for error reporting. |\n| 0x08 | `uint32_t` | `col` | Source column for error reporting. |\n| 0x0C | `uint32_t` | `_pad` | Alignment padding. |\n| 0x10 | `union` | `data` | Variant-specific data (48 bytes). |\n\n### 3.2 AST Node Variants\n\n```c\ntypedef enum {\n    NODE_SELECT, NODE_INSERT, NODE_CREATE_TABLE,\n    NODE_EXPR_BINARY, NODE_EXPR_UNARY, NODE_EXPR_LITERAL, NODE_EXPR_COLUMN,\n    NODE_COLUMN_DEF, NODE_LIMIT_OFFSET\n} ASTNodeType;\n\n// Expression Data\ntypedef struct {\n    TokenType op;\n    struct ASTNode* left;\n    struct ASTNode* right;\n} BinaryExpr;\n\n// Select Statement Data\ntypedef struct {\n    struct ASTNode* result_columns; // Linked list or Array of expressions\n    const char* table_name;\n    struct ASTNode* where_clause;\n    struct ASTNode* limit_clause;\n    bool is_distinct;\n} SelectStmt;\n\n// Literal Data\ntypedef struct {\n    LiteralType type;\n    union {\n        int64_t i_val;\n        double f_val;\n        char* s_val; // Unescaped string\n    } value;\n} LiteralExpr;\n```\n\n{{DIAGRAM:tdd-diag-m2-ast-node}}\n*(Visual: A memory block diagram showing the 64-byte node with a pointer pointing to children nodes in the same Arena block)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `ASTNode* parser_parse(TokenStream* stream)`\n- **Purpose**: Main entry point for the parser.\n- **Input**: A stream/iterator of tokens from Milestone 1.\n- **Output**: Root of the AST.\n- **Error**: Returns `NULL` and populates the `ParserError` context if syntax is invalid.\n\n### 4.2 `ASTNode* parser_parse_expression(ParserContext* ctx, int min_precedence)`\n- **Purpose**: The Pratt implementation.\n- **Precedence Logic**: Only consumes tokens with binding power greater than `min_precedence`.\n- **Edge Case**: Handles parenthesized expressions by resetting `min_precedence` to 0.\n\n### 4.3 `void parser_arena_free(Arena* arena)`\n- **Purpose**: Wipes all AST nodes in a single $O(1)$ memory release.\n- **Constraint**: Must be called after the Bytecode Compiler has finished processing the tree.\n\n---\n\n## 5. Algorithm Specification: Pratt Expression Parsing\n\nTraditional recursive descent for expressions results in deep, inefficient stacks (e.g., `parse_or` -> `parse_and` -> `parse_equality`...). Pratt parsing replaces this with a simple loop and a precedence table.\n\n### 5.1 Precedence Table (Binding Power)\n\n| Token Type | Precedence | Associativity |\n| :--- | :--- | :--- |\n| `TK_OR` | 10 | Left |\n| `TK_AND` | 20 | Left |\n| `TK_NOT` | 30 | Right (Unary) |\n| `TK_EQUAL`, `TK_BANG_EQUAL` | 40 | Left |\n| `TK_LESS`, `TK_GREATER` | 50 | Left |\n| `TK_PLUS`, `TK_MINUS` | 60 | Left |\n| `TK_STAR`, `TK_SLASH` | 70 | Left |\n\n### 5.2 Procedure: `parse_expression(precedence)`\n1. **Prefix Step**: \n    - Peek at the current token.\n    - If `TK_INTEGER`, `TK_STRING`, `TK_NULL`: Return a `NODE_EXPR_LITERAL`.\n    - If `TK_IDENTIFIER`: Return a `NODE_EXPR_COLUMN`.\n    - If `TK_MINUS` or `TK_NOT`: Call `parse_expression(UNARY_PRECEDENCE)` and return `NODE_EXPR_UNARY`.\n    - If `TK_LEFT_PAREN`: Call `parse_expression(0)`, then consume `TK_RIGHT_PAREN`.\n2. **Infix Loop**:\n    - While `get_precedence(peek_token()) > precedence`:\n        - `token = consume()`\n        - `left_node = create_binary_node(op: token, left: left_node, right: parse_expression(token.precedence))`\n3. **Return `left_node`**.\n\n{{DIAGRAM:tdd-diag-m2-pratt-flow}}\n*(Visual: Flowchart showing the \"Climbing\" loop: Parse Prefix -> Peek Next Op -> If Precedence > Current -> Recurse)*\n\n---\n\n## 6. Algorithm Specification: SELECT Statement\n\nThe `SELECT` statement is parsed using traditional Recursive Descent rules.\n\n### 6.1 Procedure: `parse_select()`\n1. Consume `TK_SELECT`.\n2. **Result Columns**: \n    - Loop: \n        - If `TK_STAR`, create a special \"All\" node.\n        - Else, call `parse_expression(0)`.\n        - Check for optional `TK_AS` + `TK_IDENTIFIER` (Alias).\n        - If `TK_COMMA`, continue loop; else break.\n3. **FROM Clause**:\n    - Consume `TK_FROM`.\n    - Consume `TK_IDENTIFIER` (Table Name).\n4. **WHERE Clause (Optional)**:\n    - If `TK_WHERE`:\n        - `where_node = parse_expression(0)`.\n5. **LIMIT Clause (Optional)**:\n    - If `TK_LIMIT`:\n        - `limit_node = parse_expression(0)`.\n6. Consume `TK_SEMICOLON` or `TK_EOF`.\n\n{{DIAGRAM:tdd-diag-m2-select-flow}}\n*(Visual: Sequential flowchart showing the checkpoints: [SELECT] -> [COLS] -> [FROM] -> [WHERE?] -> [LIMIT?])*\n\n---\n\n## 7. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `UnexpectedToken` | `consume()` | Skip to next `TK_SEMICOLON` | Yes: \"Expected 'FROM', found 'WHERE' at line 1:12\" |\n| `MismatchedParen` | `parse_expression` | Panic to next statement | Yes: \"Unclosed parenthesis\" |\n| `MissingColumnDef`| `parse_create` | Abort `CREATE TABLE` node | Yes: \"Table must have at least one column\" |\n| `IncompleteExpr`  | `parse_expression` | Return error node | Yes: \"Trailing operator in expression\" |\n\n---\n\n## 8. Implementation Sequence\n\n### Phase 1: Expression Pratt Parsing (Estimated: 4 Hours)\n- Implement `arena_alloc`.\n- Implement `parse_expression` for literals, identifiers, and binary operators.\n- **Checkpoint**: Running `parser_parse_expression` on `1 + 2 * 3` should produce a tree where `+` is the root and `*` is the right child.\n\n### Phase 2: Statement Recursive Descent (Estimated: 4 Hours)\n- Implement `parse_create_table` with column constraints.\n- Implement `parse_insert`.\n- Implement `parse_select` with `WHERE` and `LIMIT`.\n- **Checkpoint**: `SELECT * FROM users WHERE id = 5;` should produce a `SelectStmt` node with a `BinaryExpr` node in the `where_clause` field.\n\n---\n\n## 9. Test Specification\n\n### 9.1 Happy Path: Operator Precedence\n- **Input**: `SELECT * FROM t WHERE a = 1 OR b = 2 AND c = 3`\n- **Logic**: `AND` (20) > `OR` (10).\n- **Expected AST Structure**:\n    - Root: `SelectStmt`\n    - Where: `BinaryExpr(OR)`\n        - Left: `BinaryExpr(=, a, 1)`\n        - Right: `BinaryExpr(AND)`\n            - Left: `BinaryExpr(=, b, 2)`\n            - Right: `BinaryExpr(=, c, 3)`\n\n### 9.2 Edge Case: String Unescaping\n- **Input**: `INSERT INTO t VALUES ('It''s logic');`\n- **Expected**: The `LiteralExpr` node for the string should contain the value `It's logic` (one single quote), not `It''s logic`.\n\n### 9.3 Failure Case: Invalid Syntax\n- **Input**: `SELECT FROM table;` (Missing column list)\n- **Expected**: `parser_parse` returns `NULL`. `ParserError` contains `line: 1, col: 8, message: \"Expected expression or '*' before 'FROM'\"`.\n\n---\n\n## 10. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Parsing Latency** | < 1ms for 1k nodes | Time `parser_parse` on a query with a 500-clause `WHERE` (e.g., `x=1 AND x=2 AND ...`). |\n| **Memory Efficiency** | 0 Small Mallocs | Use a memory profiler (Valgrind/Instruments) to ensure only large Arena blocks are allocated. |\n| **Recursion Depth** | > 500 Levels | Ensure stack safety by testing deeply nested parentheses `((((...1...))))`. |\n\n---\n\n## 11. Column Constraints Representation\n\nWhen parsing `CREATE TABLE`, the parser must capture column attributes. These are stored in a `ColumnDef` node:\n\n```c\ntypedef struct {\n    const char* name;\n    DataType type; // INT, TEXT, etc.\n    bool is_primary_key;\n    bool is_not_null;\n    bool is_unique;\n} ColumnDef;\n```\nIf multiple constraints appear (e.g., `PRIMARY KEY NOT NULL`), the parser simply flips multiple booleans on the same `ColumnDef` node.\n\n{{DIAGRAM:m2-alt-reality}}\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m3 -->\n# Module Design: Bytecode Compiler & VDBE (build-sqlite-m3)\n\n## 1. Module Charter\n\nThe **Virtual Database Engine (VDBE)** is the execution core of the database. It acts as a specialized, register-based virtual machine designed to perform high-speed data manipulation by abstracting B-tree operations into a linear instruction set. The **Bytecode Compiler** acts as the \"backend\" of the SQL engine, translating the high-level, recursive Abstract Syntax Tree (AST) into a flat array of VDBE instructions (Opcodes).\n\n**Core Responsibilities:**\n- **Code Generation**: Traverse the AST and emit a sequence of opcodes that implement the query logic.\n- **Register Management**: Allocate and manage a virtual register file for intermediate expression results and column data.\n- **Control Flow**: Implement jumps and loops required for table scans, `WHERE` clause filtering, and `JOIN` logic.\n- **Abstraction**: Provide a \"Cursor\" interface that hides the complexity of B-tree traversal from the high-level execution logic.\n- **Execution**: Run the bytecode in a highly optimized fetch-decode-execute loop.\n\n**Non-Goals:**\n- **Parsing**: This module assumes a valid AST is provided by Milestone 2.\n- **Physical I/O**: The VDBE calls B-tree/Pager methods; it does not handle file descriptors or buffer eviction directly.\n- **Optimization**: This milestone focuses on a \"straight-line\" compiler; cost-based optimization is deferred to Milestone 8.\n\n**Invariants:**\n- Every bytecode program must end with a `Halt` instruction to prevent program counter overflow.\n- Registers are \"typed\" at runtime; an operation on incompatible types (e.g., `Add` on a String and Integer) must trigger a VM-level error.\n- Cursors must be explicitly opened before access and are automatically closed when the VM halts.\n\n---\n\n## 2. File Structure\n\nThe implementation follows this sequence:\n\n1. `src/include/vdbe/opcodes.h`: Definition of the Instruction Set Architecture (ISA).\n2. `src/include/vdbe/value.h`: The `Value` tagged union for register storage.\n3. `src/include/vdbe/vdbe.h`: The `Vdbe` machine state and `Instruction` struct.\n4. `src/vdbe/compiler.c`: Recursive visitor that emits bytecode from AST nodes.\n5. `src/vdbe/vm.c`: The core execution loop (Fetch-Decode-Execute).\n6. `src/vdbe/explain.c`: Human-readable formatting for bytecode programs.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 The Instruction Word (Byte-Level Layout)\nTo ensure high performance and cache alignment, each instruction is exactly 16 bytes.\n\n| Offset | Type | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | `uint8_t` | `opcode` | The operation to perform (e.g., `OP_Column`). |\n| 0x01 | `int8_t` | `p1` | Usually a cursor index or register index. |\n| 0x02 | `int16_t` | `p2` | Usually a jump target (PC) or column index. |\n| 0x04 | `int32_t` | `p3` | Usually a destination register or constant value. |\n| 0x08 | `void*` | `p4` | Pointer to complex data (strings, blobs) or metadata. |\n\n{{DIAGRAM:tdd-diag-m3-vm-arch}}\n*(Visual: Pipeline showing AST -> Compiler -> Opcode Array -> VM Loop -> Register/Cursor State)*\n\n### 3.2 Register Value (`Value` Struct)\nRegisters are dynamic. They store the \"Live\" data during a query.\n\n```c\ntypedef enum {\n    VAL_NULL,\n    VAL_INT,\n    VAL_FLOAT,\n    VAL_TEXT,\n    VAL_BLOB\n} ValueType;\n\ntypedef struct {\n    ValueType type;\n    union {\n        int64_t i;\n        double r;\n        struct {\n            char* z;\n            int n;\n        } s; // String/Blob\n    } u;\n} Mem; // In SQLite, registers are called \"Mem\" units\n```\n\n### 3.3 The VDBE State\n```c\ntypedef struct {\n    Instruction* aOp;    // Array of instructions\n    int nOp;            // Number of instructions\n    int pc;             // Program Counter\n    Mem* aReg;          // Register File (e.g., 256 registers)\n    VdbeCursor** apCsr; // Array of Cursors (e.g., 16 cursors)\n    int nReg;\n    int nCsr;\n    char* zErrMsg;      // Runtime error message\n} Vdbe;\n```\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `Vdbe* vdbe_compile(ASTNode* root)`\n- **Purpose**: Translates AST to Bytecode.\n- **Input**: AST root.\n- **Output**: Allocated `Vdbe` object ready for execution.\n- **Error**: Returns `NULL` on `TableNotFound` or `RegisterOverflow` (if an expression is too deep).\n\n### 4.2 `VdbeResult vdbe_step(Vdbe* p)`\n- **Purpose**: Executes the program until a row is produced or the program ends.\n- **Return Values**:\n    - `VDBE_ROW`: A result row is available in the designated registers.\n    - `VDBE_DONE`: Execution finished successfully.\n    - `VDBE_ERROR`: A runtime error occurred (e.g., Type Mismatch).\n\n### 4.3 `void vdbe_explain(Vdbe* p)`\n- **Purpose**: Prints the instruction array to `stdout` in a tabular format for debugging.\n\n---\n\n## 5. Algorithm Specification: Instruction Set (ISA)\n\nThe VDBE implements a \"Dataflow Assembly.\" Below are the critical opcodes required for Milestone 3.\n\n| Opcode | P1 | P2 | P3 | Action |\n| :--- | :--- | :--- | :--- | :--- |\n| `Integer` | Value | Reg | - | `aReg[P2] = (int64_t)P1` |\n| `String` | - | Reg | P4 | `aReg[P2] = (string)P4` |\n| `OpenRead` | Csr | Root | - | Open cursor P1 on B-tree starting at P2. |\n| `Rewind` | Csr | Jump | - | Move Csr P1 to first row. If empty, `pc = P2`. |\n| `Next` | Csr | Jump | - | Move Csr P1 to next row. If successful, `pc = P2`. |\n| `Column` | Csr | Col | Reg | `aReg[P3] = aCsr[P1].data[P2]` |\n| `ResultRow` | Reg | Count | - | Output `Count` registers starting at `Reg`. |\n| `Halt` | - | - | - | Terminate execution. |\n| `Add` | RegL | RegR | RegOut| `aReg[P3] = aReg[P1] + aReg[P2]` |\n| `Lt` | RegL | Jump | RegR | If `aReg[P1] < aReg[P3]`, `pc = P2`. |\n\n---\n\n## 6. Algorithm Specification: Code Generation\n\nThe compiler uses a recursive `code_gen(node)` function. It maintains a `next_reg` counter that resets for each statement.\n\n### 6.1 Compiling `SELECT * FROM table`\n1. **Emit `OpenRead(0, root_page)`**: P1=0 (Cursor 0), P2=Root of table.\n2. **Emit `Rewind(0, halt_label)`**: P1=0. P2 will be patched later.\n3. **Loop Start Label**: `L1`\n4. **For each column `i` in table**:\n   - **Emit `Column(0, i, i+1)`**: Extract column `i` into Register `i+1`.\n5. **Emit `ResultRow(1, num_cols)`**: Output registers.\n6. **Emit `Next(0, L1)`**: Jump back to `L1` if more rows exist.\n7. **Label `halt_label`**: Emit `Halt`.\n\n### 6.2 Backpatching Jumps\nSince the compiler emits code linearly, the jump target for `Rewind` (the end of the loop) isn't known until the loop is finished.\n1. Store the index of the `Rewind` instruction.\n2. Compile the loop body.\n3. After `Next` is emitted, set `aOp[rewind_idx].p2 = current_pc`.\n\n{{DIAGRAM:tdd-diag-m3-compile-flow}}\n*(Visual: Sequential steps of compiling a SELECT: [Emit Open] -> [Mark L1] -> [Emit Column] -> [Emit Next L1] -> [Backpatch Rewind])*\n\n---\n\n## 7. Algorithm Specification: Register Allocation\n\nTo prevent register collisions in complex expressions like `(a + b) * (c + d)`, the compiler uses a **Stack-Based Allocation** approach during expression traversal.\n\n**Procedure: `allocate_expr(node)`**\n1. If `node` is a Literal:\n   - `r = next_reg++`\n   - Emit `Integer` or `String` into `r`.\n   - Return `r`.\n2. If `node` is Binary (`+`):\n   - `reg_left = allocate_expr(node->left)`\n   - `reg_right = allocate_expr(node->right)`\n   - `reg_out = next_reg++`\n   - Emit `Add(reg_left, reg_right, reg_out)`\n   - **Important**: We do *not* decrement `next_reg` yet to ensure parent nodes can safely use `reg_out` without child nodes overwriting it. Registers are recycled at the end of the statement.\n\n---\n\n## 8. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `RegisterOverflow` | Compiler | Abort compilation, free `Vdbe` | Yes: \"Expression too complex\" |\n| `TypeMismatch` | VM (`Add` op) | `Halt` with error code | Yes: \"Cannot add TEXT and INT\" |\n| `InvalidCursor` | VM (`Column` op) | `Halt` with error code | No (Internal bug) |\n| `TableNotFound` | Compiler | Abort compilation | Yes: \"Table 'x' does not exist\" |\n\n---\n\n## 9. Implementation Sequence\n\n### Phase 1: ISA & VM Skeleton (4 Hours)\n- Define `Opcode` enum and `Instruction` struct.\n- Implement the `vdbe_step` while loop with a `switch(opcode)` block.\n- Implement `Halt`, `Integer`, and `String` opcodes.\n- **Checkpoint**: Manually construct an instruction array `[Integer 42 R1, Halt]` and verify `aReg[1].i == 42` after execution.\n\n### Phase 2: Simple Scan Compiler (4 Hours)\n- Implement `vdbe_compile` for `SELECT * FROM table`.\n- Implement `OpenRead`, `Rewind`, `Column`, `Next`, and `ResultRow` in the VM.\n- Integrate with the (stubbed) B-tree layer to \"scan\" a hardcoded array of records.\n- **Checkpoint**: Compiling `SELECT * FROM users` produces the correct opcode sequence (verified via `vdbe_explain`).\n\n### Phase 3: Expression & Logic (3 Hours)\n- Implement `code_gen` for Binary Expressions (`+`, `-`, `*`).\n- Implement comparison opcodes (`Eq`, `Lt`, `Gt`) and conditional jumps.\n- Implement `WHERE` clause compilation (conditional jump to `Next`).\n- **Checkpoint**: `SELECT name FROM t WHERE age > 21` correctly emits a `Lt` or `Le` instruction that jumps over the `ResultRow`.\n\n---\n\n## 10. Test Specification\n\n### 10.1 Happy Path: Constant Arithmetic\n- **Input AST**: `SELECT 1 + 2;`\n- **Expected Bytecode**:\n    1. `Integer 1, R1`\n    2. `Integer 2, R2`\n    3. `Add R1, R2, R3`\n    4. `ResultRow R3, 1`\n    5. `Halt`\n- **Verification**: `vdbe_step` returns `VDBE_ROW`, `aReg[R3]` contains `VAL_INT(3)`.\n\n### 10.2 Edge Case: Empty Table\n- **Scenario**: `SELECT * FROM empty_table;`\n- **Logic**: `Rewind` should immediately jump to the instruction *after* the `Next` opcode.\n- **Verification**: `vdbe_step` returns `VDBE_DONE` immediately without ever hitting `ResultRow`.\n\n### 10.3 Failure Case: Register Exhaustion\n- **Input**: A query with 300 nested additions.\n- **Expected**: Compiler returns `NULL` and sets error to \"Register limit exceeded.\"\n\n---\n\n## 11. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Instruction Dispatch**| < 10ns / op | Run a loop of 10M `Add` instructions; measure total time / 10M. |\n| **Full Table Scan** | 1M rows/sec | Scan a table with 1M rows (cached in RAM) and count `VDBE_ROW` returns. |\n| **Memory Overhead** | < 64KB / Vdbe | Measure `sizeof(Vdbe) + (nOp * 16) + (nReg * sizeof(Mem))`. |\n\n---\n\n## 12. State Machine: VM Execution\n\nThe VM follows a strict fetch-decode-execute cycle.\n\n1. **FETCH**: `instr = aOp[pc]`\n2. **DECODE**: Extract P1, P2, P3, P4.\n3. **EXECUTE**: `switch(instr.opcode)`\n4. **INCREMENT**: `pc++` (unless a jump occurred).\n5. **HALT**: If `opcode == OP_Halt` or `pc >= nOp`.\n\n**Illegal Transitions**:\n- Jumping to `pc < 0` or `pc >= nOp`.\n- Accessing `apCsr[i]` where `i >= nCsr`.\n- Accessing `aReg[i]` where `i >= nReg`.\n\n{{DIAGRAM:m3-reg-use}}\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m4 -->\n# Module Design: Buffer Pool Manager (build-sqlite-m4)\n\n## 1. Module Charter\n\nThe **Buffer Pool Manager** (Pager) is the central memory management authority of the storage engine. It abstracts the physical database file into a logical array of fixed-size **Pages** (4096 bytes), providing a \"virtual memory\" layer for the B-tree and Virtual Machine. Its primary duty is to cache frequently accessed disk blocks in a finite set of memory **Frames**, utilizing an **LRU (Least Recently Used)** eviction policy to maximize cache hit rates. This module is responsible for the \"Durability Soul\" of the database, ensuring that modified (dirty) pages are safely flushed to disk and that active pages are \"pinned\" to prevent use-after-free corruption. It does not understand the content of the pages (B-trees, records); it treats them as opaque byte buffers.\n\n**Invariants:**\n- A page with a `pin_count > 0` must **never** be selected for eviction.\n- A \"dirty\" page must be written to disk (and `fsync`'d if in synchronous mode) before its frame can be repurposed for a different page ID.\n- Every `fetch_page` call must eventually be balanced by an `unpin_page` call to prevent pool exhaustion.\n\n---\n\n## 2. File Structure\n\nImplementation should follow this sequence to ensure structural integrity:\n\n1. `src/include/storage/pager_types.h`: Definition of constants (PAGE_SIZE), Frame descriptors, and Error codes.\n2. `src/include/storage/lru_cache.h`: Interface for the O(1) LRU eviction policy.\n3. `src/storage/lru_cache.c`: Implementation of the Doubly Linked List + Hash Map for LRU.\n4. `src/include/storage/pager.h`: Main Pager interface for fetching, pinning, and flushing pages.\n5. `src/storage/pager.c`: Core logic for disk I/O coordination, frame management, and dirty tracking.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 Frame Descriptor (Memory Layout)\nEach frame in the buffer pool is associated with a metadata descriptor. These descriptors are stored in a contiguous array to minimize cache misses during lookup.\n\n| Offset | Type | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | `uint32_t` | `page_id` | Physical ID of the page currently in this frame. |\n| 0x04 | `uint32_t` | `pin_count` | Number of active references (prevents eviction). |\n| 0x08 | `bool` | `is_dirty` | True if the frame has been modified and requires flush. |\n| 0x09 | `bool` | `is_valid` | True if the frame contains actual disk data. |\n| 0x10 | `void*` | `data` | Pointer to the 4096-byte memory block. |\n| 0x18 | `LRUNode*` | `lru_node` | Pointer to the node in the eviction list. |\n\n### 3.2 Buffer Pool Structure\nThe `Pager` object maintains the state of the entire caching system.\n\n```c\n#define PAGE_SIZE 4096\n\ntypedef struct {\n    int fd;                 // File descriptor for the .db file\n    uint32_t num_frames;    // Size of the buffer pool (e.g., 1000)\n    PageDescriptor* frames; // Array of metadata descriptors\n    void* buffer_pool_mem;  // Contiguous block of (num_frames * 4096) bytes\n    LRUCache* lru;          // Eviction policy controller\n    HashTable* page_table;  // Maps page_id -> frame_index\n    pthread_mutex_t latch;  // Global lock for metadata synchronization\n} Pager;\n```\n\n{{DIAGRAM:tdd-diag-m4-pager-arch}}\n*(Visual: Architecture showing B-tree calling Pager -> Pager checking PageTable -> If Miss, LRU selects victim -> Pager calls OS `pread`/`pwrite`)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `void* pager_fetch_page(Pager* p, uint32_t page_id)`\n- **Purpose**: Retrieves a pointer to a page's data.\n- **Constraints**: \n    - If `page_id` is in cache: Update LRU status, increment `pin_count`, return pointer.\n    - If `page_id` is NOT in cache: Execute Eviction/Load algorithm (Section 5.2).\n- **Errors**: \n    - `POOL_FULL`: If all frames are pinned (`pin_count > 0`), no eviction is possible.\n    - `IO_ERROR`: If `pread` fails.\n\n### 4.2 `void pager_unpin_page(Pager* p, uint32_t page_id, bool is_dirty)`\n- **Purpose**: Releases a reference to a page.\n- **Logic**: Decrement `pin_count`. If `is_dirty` is true, set the frame's `is_dirty` flag. \n- **Invariant**: Must be called after every `fetch_page`.\n\n### 4.3 `void pager_flush_all(Pager* p)`\n- **Purpose**: Forces all dirty pages to disk.\n- **Use Case**: Shutdown or Transaction Commit.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 O(1) LRU Strategy\nTo achieve O(1) eviction, the `LRUCache` uses a **Doubly Linked List** (for order) and a **Hash Map** (for location).\n\n1. **Accessing a Page**:\n   - Find node in Hash Map.\n   - Remove node from its current position in the Linked List.\n   - Move node to the **Head** of the Linked List (Most Recently Used).\n2. **Evicting a Page**:\n   - Start at the **Tail** of the Linked List (Least Recently Used).\n   - Check `pin_count` of the frame associated with that node.\n   - If `pin_count > 0`, move to the previous node (searching for an unpinned victim).\n   - Once an unpinned victim is found: Remove from Hash Map, remove from List, return `frame_index`.\n\n{{DIAGRAM:tdd-diag-m4-lru-algo}}\n*(Visual: Linked List showing [Head/MRU] <-> [Node] <-> [Tail/LRU] and a HashMap pointing into the nodes)*\n\n### 5.2 The Pager Fetch/Evict Logic (The Implementation Path)\nWhen `pager_fetch_page(page_id)` is called:\n\n1. **Lock** the Pager latch.\n2. **Check Page Table**:\n   - If `page_id` exists in `page_table`:\n     - `frame_idx = page_table[page_id]`\n     - Increment `frames[frame_idx].pin_count`.\n     - `lru_touch(p->lru, frame_idx)`.\n     - **Unlock** and return `frames[frame_idx].data`.\n3. **If Miss (Not in Cache)**:\n   - **Identify Victim**: `victim_idx = lru_get_victim(p->lru)`.\n   - If no victim found (all pinned): **Unlock**, return `ERR_POOL_FULL`.\n4. **Evict Victim**:\n   - `PageDescriptor* victim = &p->frames[victim_idx]`.\n   - If `victim->is_dirty`:\n     - `lseek(p->fd, victim->page_id * PAGE_SIZE, SEEK_SET)`.\n     - `write(p->fd, victim->data, PAGE_SIZE)`.\n     - `fsync(p->fd)` (depending on durability mode).\n   - Remove `victim->page_id` from `page_table`.\n5. **Load New Page**:\n   - `lseek(p->fd, page_id * PAGE_SIZE, SEEK_SET)`.\n   - `read(p->fd, victim->data, PAGE_SIZE)`.\n   - Update `victim` metadata: `page_id = page_id`, `is_dirty = false`, `pin_count = 1`, `is_valid = true`.\n   - Add `page_id -> victim_idx` to `page_table`.\n   - `lru_add(p->lru, victim_idx)`.\n6. **Unlock** and return `victim->data`.\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `POOL_FULL` | `lru_get_victim` | B-tree must wait or abort transaction. | Yes: \"Out of memory frames\" |\n| `DISK_FULL` | `write` during flush | Attempt to rollback; enter Read-Only mode. | Yes: \"Disk full, write failed\" |\n| `IO_ERROR` | `read`/`write` | Retry once, then panic (Database is unstable). | Yes: \"Physical I/O error\" |\n| `PAGE_CORRUPTION`| Checksum check | Reject page, return `ERR_CORRUPT`. | Yes: \"Database file is corrupt\" |\n\n---\n\n## 7. Implementation Sequence\n\n### Phase 1: LRU Infrastructure (Estimated: 3-4 Hours)\n- Implement Doubly Linked List with `prev/next` pointers.\n- Implement Hash Map (or simple array if `num_frames` is small) to store `frame_index -> ListNode*`.\n- **Checkpoint**: Test `lru_touch` and `lru_get_victim` with dummy indices. Verify that the oldest touched index is returned as the victim.\n\n### Phase 2: Pager Machinery (Estimated: 4-5 Hours)\n- Implement `pager_init` to `mmap` or `malloc` the buffer pool memory.\n- Implement `pager_fetch_page` with the Disk Read logic.\n- Implement `pager_unpin_page` with the Dirty Flag logic.\n- **Checkpoint**: Fetch Page 0, write \"Hello\" into the pointer, unpin with `is_dirty=true`. Close pager. Reopen pager and fetch Page 0\u2014verify \"Hello\" persists on disk.\n\n### Phase 3: Concurrency & Stress (Estimated: 2 Hours)\n- Add `pthread_mutex` around the descriptor updates.\n- Stress test: Multiple threads fetching and unpinning random page IDs.\n- **Checkpoint**: Ensure no deadlocks and that the `pin_count` logic successfully prevents the pool from being \"over-evicted.\"\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Cache Hit\n- **Setup**: Fetch Page 1, Unpin Page 1.\n- **Action**: Fetch Page 1 again.\n- **Expected**: `page_table` lookup succeeds; no `read()` syscall is issued (verify via mock I/O).\n\n### 8.2 Edge Case: Victim Pinning\n- **Setup**: Pool size = 2. Fetch Page 1 (pin=1), Fetch Page 2 (pin=1).\n- **Action**: Attempt to Fetch Page 3.\n- **Expected**: Return `ERR_POOL_FULL` because no unpinned victims exist.\n\n### 8.3 Failure Case: Write Failure on Eviction\n- **Setup**: Mark Page 1 as dirty.\n- **Action**: Fetch Page 100 (triggering Page 1 eviction). Simulate `write()` failure.\n- **Expected**: Frame 1 remains dirty and pinned; `fetch_page` returns `IO_ERROR`.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **LRU Lookup** | O(1) | Profile with 10k frames; lookup time must remain constant. |\n| **Cache Hit Latency** | < 500ns | Average time of `pager_fetch_page` when page is resident. |\n| **I/O Alignment** | 4096-byte | Use `posix_memalign` and verify addresses are 4k aligned for Direct I/O. |\n\n---\n\n## 10. Concurrency Specification\n\nThe Pager uses a **Single Global Latch (Mutex)** strategy for the metadata. While this limits parallelism, it is the standard \"Intermediate\" approach to avoid complex fine-grained locking.\n\n**Lock Ordering:**\n1. Acquire `Pager.latch`.\n2. Perform Hash Table/LRU operations.\n3. If I/O is needed: **Release** `Pager.latch` during the actual `read()`/`write()` to allow other threads to access *resident* pages, then **Re-acquire** to update metadata.\n4. Release `Pager.latch`.\n\n**Thread Safety Invariant**: A pointer returned by `fetch_page` is safe to use as long as the caller holds the \"Pin.\" No other thread can evict that frame while the `pin_count > 0`.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m5 -->\n# Module Design: B-tree Page Format & Table Storage (build-sqlite-m5)\n\n## 1. Module Charter\n\nThe **B-tree Page Format & Table Storage** module is the physical heart of the database. It defines how logical SQL rows are mapped into fixed-size 4096-byte blocks on disk. This module implements **Clustered Storage**, where the primary table data is stored directly within the leaves of a B-tree, keyed by a 64-bit `rowid`. It manages the **Slotted Page** architecture to handle variable-length records efficiently while minimizing fragmentation. \n\n**Core Responsibilities:**\n- Implement binary serialization/deserialization for page headers and cell pointer arrays.\n- Provide variable-length integer (Varint) encoding to minimize metadata overhead.\n- Manage the \"dual-growth\" slotted page layout (pointers grow down, data grows up).\n- Execute B-tree balancing operations, specifically **Node Splitting**, when a page exceeds its 4KB capacity.\n- Abstract the physical layout into a \"Cell\" interface for the Virtual Machine.\n\n**Non-Goals:**\n- This module does not handle the Buffer Pool (it requests pages from the Pager).\n- It does not handle WAL or Rollback Journals (it assumes the Pager provides a consistent view).\n- It does not implement SQL parsing or execution logic.\n\n**Invariants:**\n- Every page must be exactly 4096 bytes when flushed to the Pager.\n- In a Table B-tree, internal nodes store only `rowid` separators and child pointers; leaf nodes store actual row payloads.\n- All multi-byte integers on disk must be stored in **Big-Endian** format.\n\n---\n\n## 2. File Structure\n\nImplementation follows this sequence to build from primitive encodings to complex tree structures:\n\n1. `src/include/storage/varint.h`: Varint encoding/decoding macros and prototypes.\n2. `src/storage/varint.c`: Implementation of the 1-9 byte variable integer logic.\n3. `src/include/storage/page_format.h`: Struct definitions for Page Headers and Cell Pointers.\n4. `src/storage/page_format.c`: Logic for initializing pages and managing the slotted heap.\n5. `src/include/storage/btree.h`: The B-tree manager and Cursor definitions.\n6. `src/storage/btree.c`: Implementation of Search, Insert, and the Node Split algorithm.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 On-Disk Page Layout (Byte-Level)\n\nThe page is a 4096-byte buffer. The header occupies the first few bytes, followed by the cell pointer array.\n\n| Offset | Size | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | 1 byte | `page_type` | `0x05`: Table Internal, `0x0D`: Table Leaf. |\n| 0x01 | 2 bytes | `freeblock_ptr` | Offset to the first free block in the page heap (Big-Endian). |\n| 0x03 | 2 bytes | `cell_count` | Number of cells stored in this page (Big-Endian). |\n| 0x05 | 2 bytes | `content_start` | Offset to the start of the cell content area (Big-Endian). |\n| 0x07 | 1 byte | `fragmented_free` | Number of fragmented free bytes. |\n| 0x08 | 4 bytes | `right_child` | (Internal Only) Page ID of the right-most child. |\n\n**Cell Pointer Array**: Starts at offset `0x08` (for leaves) or `0x0C` (for internal nodes). It is an array of `uint16_t` offsets pointing to the start of cells in the content area.\n\n{{DIAGRAM:tdd-diag-m5-slotted-layout}}\n{{DIAGRAM:m5-layout}}\n\n### 3.2 Cell Formats (Binary Payload)\n\n#### Table B-tree Leaf Cell (Data Node)\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `payload_size` | Varint | Total size of the record data. |\n| `rowid` | Varint | The 64-bit primary key. |\n| `payload` | Bytes | Serialized SQL row (column values). |\n\n#### Table B-tree Internal Cell (Navigation Node)\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `left_child` | uint32_t | Page ID of the child containing keys \u2264 `rowid`. |\n| `rowid` | Varint | The separator key. |\n\n### 3.3 Memory Structures\n```c\ntypedef struct {\n    uint8_t type;\n    uint16_t cell_count;\n    uint16_t content_start; // Marks the \"top\" of the bottom heap\n    uint32_t right_child;   // Only for Internal\n    uint8_t* data;          // Pointer to the 4096-byte buffer from Pager\n} BtreePage;\n\ntypedef struct {\n    uint32_t payload_size;\n    int64_t rowid;\n    uint8_t* pPayload;      // Pointer into the page buffer\n} BtreeCell;\n```\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `int varint_encode(uint8_t* out, uint64_t value)`\n- **Purpose**: Encodes a 64-bit integer into 1-9 bytes.\n- **Constraints**: `out` must have at least 9 bytes of space.\n- **Returns**: Number of bytes written.\n\n### 4.2 `uint32_t page_free_space(BtreePage* p)`\n- **Purpose**: Calculate remaining bytes in the \"gap\" between pointers and content.\n- **Formula**: `content_start - (header_size + cell_count * 2)`.\n\n### 4.3 `int btree_insert(Pager* p, uint32_t root_id, int64_t rowid, const uint8_t* data, uint32_t len)`\n- **Purpose**: Insert a new row into the B-tree.\n- **Logic**:\n    - Traverse to leaf.\n    - If space exists: Insert cell, update pointers.\n    - If no space: Execute `btree_split()`.\n- **Errors**: `ERR_PAGE_OVERFLOW`, `ERR_DUPLICATE_KEY`.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 Varint Encoding (MSB Flag)\n1. If `value <= 0x7F`: Write 1 byte.\n2. For larger values: \n   - Use the 8th bit of each byte as a \"More\" flag (1 = continue, 0 = last byte).\n   - The 9th byte is a special case in SQLite: it uses all 8 bits for data.\n3. **Logic**:\n   ```c\n   while (value > 0x7F && count < 8) {\n       out[count++] = (value & 0x7F) | 0x80;\n       value >>= 7;\n   }\n   out[count++] = value & 0xFF;\n   ```\n   *Note: Bytes are written MSB-first.*\n\n### 5.2 B-tree Leaf Search (`find_leaf`)\n1. Start at `root_page_id`.\n2. Fetch page from Pager.\n3. If `page_type == 0x0D` (Leaf): Return this page.\n4. If `page_type == 0x05` (Internal):\n   - Perform binary search on the `cell_pointer_array`.\n   - Find the first cell where `cell.rowid >= search_rowid`.\n   - If found: `next_page = cell.left_child`.\n   - If not found: `next_page = page.right_child`.\n   - Repeat from step 2 with `next_page`.\n\n### 5.3 Node Split Algorithm (`btree_split`)\nThis is triggered when an `insert` exceeds the `page_free_space`.\n\n1. **Identify the Median**: Find the middle cell in the overflowing page.\n2. **Create Sibling**: Request a new page `P_new` from the Pager. Initialize it with the same `page_type`.\n3. **Redistribute**:\n   - Move all cells after the median from the old page `P_old` to `P_new`.\n   - Update `P_old->cell_count` and `P_new->cell_count`.\n   - Adjust `content_start` for both.\n4. **Promote**:\n   - If `P_old` was the Root:\n     - Create a new Root page `P_root_new`. Set type to Internal.\n     - Add a cell to `P_root_new` with `rowid = median.rowid` and `left_child = P_old`.\n     - Set `P_root_new->right_child = P_new`.\n   - If `P_old` was not Root:\n     - Insert `(median.rowid, P_old)` into the parent of `P_old`.\n     - Update parent's `right_child` or neighboring pointer to `P_new`.\n\n{{DIAGRAM:tdd-diag-m5-split-steps}}\n{{DIAGRAM:m5-split-visual}}\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `PageOverflow` | `btree_insert` | Trigger `btree_split()`. If split fails (no disk), abort. | No (Handled internally) |\n| `DuplicateRowID` | `btree_insert` | Return error to VM; roll back transaction. | Yes: \"UNIQUE constraint failed\" |\n| `InvalidPageType` | `fetch_page` | Pager/Btree verify header byte. Panic if unknown. | Yes: \"Database disk image is malformed\" |\n| `CellTooLarge` | `btree_insert` | If cell > 4KB, trigger Overflow Page logic (or reject in simple build). | Yes: \"Row size too large\" |\n\n---\n\n## 7. Implementation Sequence\n\n### Phase 1: Binary Primitives (Estimated: 2-3 Hours)\n- Implement `varint_encode` and `varint_decode`.\n- Implement Big-Endian read/write utilities (`get_u16`, `put_u16`, `get_u32`).\n- **Checkpoint**: Encode the number `150`. Verify it occupies 2 bytes: `0x81 0x16`. Decode it back.\n\n### Phase 2: Slotted Page Management (Estimated: 4-5 Hours)\n- Implement `page_init`, `page_add_cell`, and `page_get_cell`.\n- Logic for shifting `content_start` and updating the `cell_pointer_array`.\n- **Checkpoint**: Initialize a 4KB buffer. Add 5 cells of varying sizes. Verify `cell_count` is 5 and the pointers correctly resolve to the payloads.\n\n### Phase 3: B-tree Logic (Estimated: 6-8 Hours)\n- Implement `btree_find_leaf` (Recursive or Iterative).\n- Implement `btree_insert` with the split logic.\n- **Checkpoint**: Insert 100 rows with sequential IDs into a tree. Verify that the tree height grows from 1 to 2 once the first page fills.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Sequential Insert\n- **Input**: Insert 10 rows with IDs 1-10.\n- **Expected**: `page_get_cell(0)` returns ID 1, `page_get_cell(9)` returns ID 10. `cell_count` is 10.\n\n### 8.2 Edge Case: Random Insertion\n- **Input**: Insert IDs 10, 5, 20, 1.\n- **Expected**: The `cell_pointer_array` must be kept sorted by `rowid`. `page_get_cell(0)` must point to ID 1 even though it was inserted last.\n\n### 8.3 Failure Case: Page Full\n- **Input**: Insert 500 rows of 100 bytes each.\n- **Expected**: `btree_insert` should successfully split pages. Total pages in the file should increase. A `find(250)` must successfully navigate from root to the correct leaf.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Search Depth** | $O(\\log_{100} N)$ | For 1M rows, verify `btree_find_leaf` fetches $\\le$ 4 pages. |\n| **Serialization** | < 10\u03bcs / row | Measure time for `page_add_cell` for 1000 iterations. |\n| **Space Efficiency** | > 85% Fill | Measure total data size / (total pages * 4096) after many random inserts. |\n\n---\n\n## 10. Concurrency Specification\n\n**Model: Single-Writer, Multi-Reader (SWMR)**\n\n1. **Reader Path**:\n   - Acquires `SHARED` lock on the B-tree Root.\n   - As it descends, it unpins parent pages once the child is pinned (Crabbing).\n2. **Writer Path**:\n   - Acquires `EXCLUSIVE` lock on the B-tree.\n   - During a split, it must hold pins on the Parent, the Full Page, and the New Sibling.\n   - **Lock Ordering**: Always lock Page IDs in ascending order or Parent-then-Child to avoid deadlocks.\n\n**Torn Page Protection**: \nThe B-tree layer relies on the **Pager** to handle atomicity. However, if a split is interrupted, the \"Right Child\" pointer in the parent must be updated **last** to ensure that an old version of the tree remains reachable until the new pages are fully flushed.\n\n---\n\n## 11. Wire Format: Varint Detail\n\nThe varint layout is a sequence of 1-9 bytes.\n- Bytes 1-8: `1` in MSB means \"another byte follows\". `0` in MSB means \"this is the last byte\". Use the remaining 7 bits for data.\n- Byte 9: All 8 bits are data.\n\n**Example: The number 1000**\n- Binary: `11 11101000` (10 bits)\n- Grouped in 7s: `0000111` and `1101000`\n- Add MSB flags: `1 0000111` (0x87) and `0 1101000` (0x68)\n- Hex Output: `0x87 0x68`\n\n{{DIAGRAM:tdd-diag-m5-varint-layout}}\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m6 -->\n# Module Design: SELECT Execution & DML (build-sqlite-m6)\n\n## 1. Module Charter\n\nThe **SELECT Execution & DML** module is the functional bridge between the high-level Virtual Machine (VDBE) and the low-level B-tree storage engine. It provides the mechanisms required to navigate B-tree structures, deserialize binary records into typed registers, and modify data while maintaining structural integrity and constraints. \n\n**Core Responsibilities:**\n- Implement the **Cursor** abstraction for stateful B-tree navigation (Forward/Backward scans).\n- Provide a **Record Deserializer** capable of parsing the SQLite-style variable-length binary format into VDBE `Value` objects.\n- Execute **Data Manipulation Language (DML)** operations: `INSERT` (row creation), `UPDATE` (modification via delete-and-insert), and `DELETE` (removal with space reclamation).\n- Enforce **NOT NULL** and basic schema constraints during the write path.\n- Coordinate with the Pager to manage **Lock Upgrades** (moving from Shared to Reserved/Exclusive) during write transactions.\n\n**Non-Goals:**\n- **Secondary Index Maintenance**: Handled in Milestone 7.\n- **Complex Query Planning**: Handled in Milestone 8.\n- **Transaction Recovery**: Handled by the Pager/Journal in Milestone 9/10.\n\n**Invariants:**\n- A cursor must always point to a valid cell, the EOF marker, or be explicitly marked as \"Invalid\" after a B-tree mutation.\n- Column extraction must be zero-copy where possible (referencing the Buffer Pool frames directly for strings/blobs).\n- No write operation can proceed without first upgrading the database lock state to `RESERVED`.\n\n---\n\n## 2. File Structure\n\nImplementation follows this sequence to ensure the navigation API is solid before building complex DML logic:\n\n1. `src/include/vdbe/cursor.h`: Definition of the `VdbeCursor` state and navigation constants.\n2. `src/vdbe/cursor.c`: Implementation of `CursorNext`, `CursorRewind`, and `CursorSeek`.\n3. `src/include/vdbe/record.h`: Serialization/Deserialization logic for the SQLite Record Format.\n4. `src/vdbe/record.c`: Logic for parsing Serial Types and calculating byte offsets within cells.\n5. `src/vdbe/dml_ops.c`: Integration of VDBE opcodes (`OP_Column`, `OP_Insert`, `OP_Delete`) with the B-tree layer.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 VdbeCursor Structure\nThe cursor tracks the \"Current Position\" of a scan within a Table B-tree.\n\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `btree_root` | `uint32_t` | The Page ID of the B-tree root this cursor is exploring. |\n| `curr_page` | `uint32_t` | The Page ID of the leaf node currently being read. |\n| `cell_idx` | `uint16_t` | The index in the slotted page's cell pointer array. |\n| `is_eof` | `bool` | Set to true when `Next()` moves past the last cell in the last leaf. |\n| `is_writable` | `bool` | Flag indicating if the cursor was opened for DML operations. |\n| `cache_status` | `uint8_t` | Metadata for optimizing repeated column access on the same row. |\n\n### 3.2 The Record Format (Wire Format)\nData is stored as a \"Record\" within the B-tree cell payload. This format uses a header to define the types of all columns before the data begins.\n\n**Record Layout:**\n- **Header Size**: Varint (Total bytes in the header, including this varint).\n- **Serial Types**: A sequence of Varints (One per column).\n- **Data**: The actual column values, concatenated.\n\n**Serial Type Mapping (The SQLite Protocol):**\n| Serial Type | Content Size | Meaning |\n| :--- | :--- | :--- |\n| `0` | 0 bytes | NULL |\n| `1` | 1 byte | 8-bit signed integer |\n| `2` | 2 bytes | 16-bit signed integer |\n| `3` | 3 bytes | 24-bit signed integer |\n| `4` | 4 bytes | 32-bit signed integer |\n| `6` | 8 bytes | 64-bit signed integer |\n| `7` | 8 bytes | IEEE 754 64-bit float |\n| `8` | 0 bytes | The integer 0 (constant) |\n| `9` | 0 bytes | The integer 1 (constant) |\n| `N >= 12, even` | `(N-12)/2` | BLOB |\n| `N >= 13, odd` | `(N-13)/2` | UTF-8 String |\n\n{{DIAGRAM:tdd-diag-m6-record-layout}}\n*(Visual: [HeaderSize][ST1][ST2][ST3][Data1][Data2][Data3]. Show offsets: Data2 starts at HeaderSize + Size(Data1))*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `int cursor_move_to(VdbeCursor* pCur, int64_t rowid)`\n- **Purpose**: Perform a point-lookup in the Table B-tree.\n- **Logic**: Uses B-tree traversal to find the leaf and slot for the target `rowid`.\n- **Return**: `SQLITE_OK` if found, `SQLITE_NOTFOUND` if missing.\n\n### 4.2 `VdbeValue record_get_column(const uint8_t* pPayload, int col_idx)`\n- **Purpose**: Extract a specific column from a raw payload.\n- **Algorithm**: See Section 5.1.\n- **Constraints**: Must handle cases where the record has fewer columns than the schema (returns NULL).\n\n### 4.3 `int op_insert(Vdbe* p, int cursor_idx, int reg_payload, int reg_rowid)`\n- **Purpose**: VDBE opcode handler for inserting a row.\n- **Logic**:\n    - Retrieve binary payload from `reg_payload`.\n    - Retrieve `rowid` from `reg_rowid`.\n    - Call `btree_insert`.\n- **Errors**: `ConstraintViolation` (Duplicate rowid).\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 Record Deserialization (The Column Skip)\nSince columns are variable-length, finding Column $N$ requires scanning the header.\n\n1. **Parse Header Size**: Read first varint to find where data begins (`data_offset`).\n2. **Scan Serial Types**:\n   - Initialize `current_data_ptr = data_offset`.\n   - For `i` from 0 to `col_idx - 1`:\n     - Determine `size` of column `i` based on its Serial Type.\n     - `current_data_ptr += size`.\n3. **Extract Value**:\n   - Read Serial Type for `col_idx`.\n   - If `type == 0`: return `Value(NULL)`.\n   - If `type == 1`: return `Value((int8_t)pPayload[current_data_ptr])`.\n   - ... (Repeat for all types).\n   - If `type >= 13`: return `Value(string_view(pPayload + current_data_ptr, (type-13)/2))`.\n\n### 5.2 The Two-Pass Delete Algorithm\nTo prevent cursor corruption during a `DELETE FROM ... WHERE ...` scan:\n\n1. **Pass 1 (Identify)**:\n   - Open Scan Cursor.\n   - For each row:\n     - Evaluate `WHERE` clause logic in VM.\n     - If true: Store `rowid` in a temporary `vector<int64_t>`.\n2. **Pass 2 (Mutate)**:\n   - For each `rowid` in the vector:\n     - Perform a point-lookup (`SeekRowid`).\n     - Call `btree_delete` on that specific row.\n3. **Invalidation**: Mark all active cursors on this table as \"Needs Reset\" if they were pointing to the deleted rows.\n\n{{DIAGRAM:tdd-diag-m6-dml-flow}}\n*(Visual: Flowchart showing Evaluation Loop -> ID Collection -> Batch Deletion)*\n\n---\n\n## 6. State Machine: VdbeCursor Lifecycle\n\nA cursor's state determines which opcodes are valid.\n\n| State | Transition Event | Next State | Action |\n| :--- | :--- | :--- | :--- |\n| **UNINITIALIZED**| `OpenRead` / `OpenWrite`| **BEFORE_FIRST** | Allocate cursor, pin root page. |\n| **BEFORE_FIRST** | `Rewind` / `Seek` | **VALID_ROW** | Navigate to first/specific cell. |\n| **VALID_ROW** | `Next` (Success) | **VALID_ROW** | Advance `cell_idx`, handle page boundaries. |\n| **VALID_ROW** | `Next` (End of Table) | **EOF** | Mark `is_eof = true`. |\n| **EOF** | `Rewind` | **VALID_ROW** | Reset to beginning. |\n| **ANY** | `BtreeMutation` | **INVALID** | Clear cached page pointers. |\n\n**Illegal Transitions**:\n- Calling `Column` on a cursor in `EOF` state.\n- Calling `Insert` on a cursor opened with `OpenRead`.\n- Calling `Next` on an `UNINITIALIZED` cursor.\n\n{{DIAGRAM:tdd-diag-m6-cursor-sm}}\n\n---\n\n## 7. Locking & Concurrency: The Upgrade\n\nThis module introduces the **Lock Upgrade** logic required for DML.\n\n1. **Shared Lock**: Acquired by `OpenRead`. Multiple readers permitted.\n2. **Reserved Lock**: Acquired when the first `Insert/Update/Delete` opcode is encountered. \n   - Only one process can hold `RESERVED`. \n   - New readers can still enter `SHARED`.\n3. **Exclusive Lock**: Acquired during the final `PagerFlush` (Commit). No readers allowed.\n\n**Implementation Path**:\n- When `OP_Insert` is called:\n  - If `pager->lock_state < RESERVED`:\n    - Attempt `pager_upgrade_lock(RESERVED)`.\n    - If failed (another writer exists): Return `SQLITE_BUSY`.\n\n---\n\n## 8. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `ConstraintViolation` | `OP_Insert` / `MakeRecord` | Abort VM loop, trigger Rollback. | Yes: \"NOT NULL constraint failed\" |\n| `TypeMismatch` | `PredicateEvaluator` | Return `NULL` or Error based on strictness. | Yes: \"Cannot compare INT and BLOB\" |\n| `NoSuchTable` | `OP_OpenRead` | Abort compilation/execution. | Yes: \"Table 'x' not found\" |\n| `CorruptRecord` | `RecordDeserializer` | Return `INTERNAL_ERROR`, stop VM. | Yes: \"Database file is malformed\" |\n\n---\n\n## 9. Implementation Sequence\n\n### Phase 1: Cursor Navigation (4-5 Hours)\n- Implement `VdbeCursor` struct and `cursor_rewind`.\n- Implement B-tree leaf-to-leaf logic in `cursor_next` (handling the \"Right Child\" transition).\n- **Checkpoint**: Open a cursor on a 3-page B-tree. Manually call `Next()` and verify it visits every row in order.\n\n### Phase 2: Record Deserialization (3-4 Hours)\n- Implement `RecordHeader` parsing.\n- Implement `record_get_column` with the \"skip\" algorithm.\n- **Checkpoint**: Provide a raw byte buffer containing `[Header: 3, Types: 1, 1, Data: 0x05, 0x0A]`. Extract Col 1, verify it returns integer `10`.\n\n### Phase 3: DML Integration (3 Hours)\n- Implement `OP_Column`, `OP_Insert`, and `OP_Delete` logic.\n- Integrate Lock Upgrade logic in the Pager/VDBE boundary.\n- **Checkpoint**: Run `INSERT INTO t VALUES (1, 'test')` and then `SELECT * FROM t`. Verify the row is persistent.\n\n---\n\n## 10. Test Specification\n\n### 10.1 Happy Path: Full Table Scan\n- **Input**: Table with 10,000 rows. `SELECT * FROM table;`\n- **Expected**: `vdbe_step` returns `VDBE_ROW` exactly 10,000 times. Time taken < 100ms.\n\n### 10.2 Edge Case: Column Overflow\n- **Input**: Table has 3 columns. Query asks for `Column(idx=5)`.\n- **Expected**: `record_get_column` returns `VAL_NULL` safely without reading out of bounds.\n\n### 10.3 Failure Case: Lock Conflict\n- **Setup**: Process A holds a `RESERVED` lock.\n- **Action**: Process B attempts an `INSERT`.\n- **Expected**: `op_insert` returns `SQLITE_BUSY`.\n\n---\n\n## 11. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Row Scan** | 10k rows < 100ms | Measure `SELECT COUNT(*)` on 10k cached rows. |\n| **Column Extract** | < 200ns | Measure time to extract the 10th column of a wide row. |\n| **Record Encoding**| < 1\u03bcs | Measure time to convert 5 registers into one binary B-tree cell. |\n\n---\n\n## 12. Record Encoding Byte-Level Detail\n\nWhen the VM prepares an `INSERT`, it uses `OP_MakeRecord`.\n\n1. **Calculate Header Size**: Sum of varint sizes for each column's type.\n2. **Buffer Allocation**: `Total = HeaderSize + Sum(DataSizes)`.\n3. **Write Header**:\n   - Write `HeaderSize` as varint.\n   - Write `SerialType` for each register.\n4. **Write Data**:\n   - Append raw bytes for each register in order.\n   - *Note*: Integers must be Big-Endian.\n\n{{DIAGRAM:m6-record-example}}\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m7 -->\n# Module Design: Secondary Indexes (build-sqlite-m7)\n\n## 1. Module Charter\n\nThe **Secondary Index** module manages non-clustered B+tree structures that enable high-speed data retrieval based on column values other than the primary `rowid`. It functions as a mapping layer that translates an indexed value (or a composite tuple of values) into the corresponding `rowid` for a \"Double Lookup\" into the main Table B-tree. This module is responsible for the synchronous maintenance of index integrity during all DML operations (`INSERT`, `UPDATE`, `DELETE`), ensuring that indexes never become stale. It implements the **B+tree** variant where data resides exclusively in leaf nodes and leaves are horizontally linked to optimize range scans. Finally, it provides the primary mechanism for enforcing **UNIQUE** constraints at the storage level.\n\n**Core Responsibilities:**\n- Implement the **Index B+tree** page format, distinguishing between internal navigation nodes and leaf data nodes.\n- Manage **Index Cell** serialization, which combines indexed column values with the `rowid` to ensure unique, sortable keys.\n- Implement **Synchronous Maintenance** hooks that trigger on every table modification.\n- Provide B+tree traversal algorithms for **Equality Lookups** ($O(\\log N)$) and **Range Scans** ($O(N)$ following leaf pointers).\n- Enforce **UNIQUE** constraints by performing a \"pre-check\" search before insertion.\n\n**Non-Goals:**\n- This module does not decide *when* to use an index; that is the responsibility of the Query Planner (Milestone 8).\n- It does not handle the primary table data (clustered storage).\n- It does not manage its own memory (uses the Buffer Pool from Milestone 4).\n\n**Invariants:**\n- Every entry in a secondary index B+tree must contain a `rowid` pointing to a valid row in the associated Table B-tree.\n- Leaf nodes in the Index B+tree must be linked in ascending key order via the `next_leaf` pointer.\n- For `UNIQUE` indexes, no two cells may have the same indexed value (NULLs excepted per SQL standard).\n\n---\n\n## 2. File Structure\n\nImplementation follows this sequence to establish the B+tree variant before integrating with the VM:\n\n1. `src/include/storage/index_format.h`: Byte-level definitions for Index Page headers and Cell layouts.\n2. `src/storage/index_page.c`: Logic for managing Index B+tree pages (splitting, cell insertion).\n3. `src/include/storage/index_manager.h`: High-level API for index creation and maintenance.\n4. `src/storage/index_manager.c`: The \"Maintenance Hook\" logic and Unique Constraint checks.\n5. `src/vdbe/index_ops.c`: Implementation of index-specific VDBE opcodes (`IdxGE`, `IdxRowid`, `IdxDelete`).\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 Index B+tree Page Layout (Binary Format)\n\nIndex pages use the same 4096-byte frame as tables but utilize different `page_type` identifiers and header fields.\n\n| Offset | Size | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | 1 byte | `page_type` | `0x02`: Index Internal, `0x0A`: Index Leaf. |\n| 0x01 | 2 bytes | `freeblock_ptr` | Offset to the first free block (Big-Endian). |\n| 0x03 | 2 bytes | `cell_count` | Number of index entries in this page. |\n| 0x05 | 2 bytes | `content_start` | Offset to the start of the cell heap. |\n| 0x07 | 1 byte | `frag_free` | Fragmented free bytes count. |\n| 0x08 | 4 bytes | `next_leaf` | **(Leaf Only)** Page ID of the next leaf in sorted order. |\n| 0x08 | 4 bytes | `right_child` | **(Internal Only)** Page ID of the right-most child. |\n\n{{DIAGRAM:tdd-diag-m7-index-layout}}\n{{DIAGRAM:m7_page_layout}}\n\n### 3.2 Index Cell Layout (The \"Search Key\")\n\nIn a secondary index, the \"Key\" we search for must be unique to maintain B+tree stability. SQLite achieves this by appending the `rowid` to the user's indexed columns.\n\n#### Index Leaf Cell\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `payload_size` | Varint | Total size of the key + rowid record. |\n| `record_data` | Bytes | **The Record**: Contains the indexed columns followed by the `rowid`. |\n\n*Example*: An index on `Name` for a row with `Name='Alice', RowID=5` becomes a record: `[Header: 0x03, Type1: String(5), Type2: Int(5)][Data: 'Alice', 5]`.\n\n#### Index Internal Cell\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `left_child` | uint32_t | Page ID of the child tree. |\n| `payload_size` | Varint | Size of the separator key. |\n| `record_data` | Bytes | The separator record (IndexValue + RowID). |\n\n### 3.3 Key Comparison logic\nIndices require a strict comparison function.\n- **NULLs**: In SQL, NULL is considered \"smaller\" than all other values.\n- **Types**: Comparison must follow Type Affinity (Integer < Float < String < Blob).\n- **Tie-Breaking**: If the indexed values are identical (e.g., two people named \"Smith\"), the `rowid` (the last element in the record) acts as the tie-breaker to ensure a deterministic sort order.\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `int index_insert(IndexManager* mgr, uint32_t root_id, Value* cols, int64_t rowid, bool is_unique)`\n- **Purpose**: Inserts a new entry into the B+tree.\n- **Logic**:\n    - Serialize `cols + rowid` into a Record.\n    - If `is_unique`, perform `index_find(root_id, cols)` first.\n    - Traverse to the correct leaf and insert.\n- **Errors**: `ERR_UNIQUE_VIOLATION` if `is_unique` and key exists.\n\n### 4.2 `int index_cursor_seek(IndexCursor* pCur, Value* search_key, int op)`\n- **Purpose**: Position an index cursor for range or equality scans.\n- **Logic**:\n    - Use B+tree traversal to find the first cell $\\ge$ `search_key`.\n    - Set cursor to `is_eof` if no such key exists.\n- **Operations**: `SEEK_EQ` (Equality), `SEEK_GE` (Greater-Equal).\n\n### 4.3 `int64_t index_cursor_rowid(IndexCursor* pCur)`\n- **Purpose**: Extract the `rowid` from the current index entry.\n- **Return**: The 64-bit RowID used for the double-lookup.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 The \"Double Lookup\" Query Flow\nExecuting `SELECT * FROM table WHERE col = 'val'` using an index:\n\n1. **VDBE Instruction**: `IdxGE(cursor=index_csr, label=end, reg=target_val)`.\n2. **Implementation**:\n   - `index_cursor_seek` finds the first entry in the B+tree where `IndexValue >= 'val'`.\n3. **Loop Body**:\n   - `IdxRowid(cursor=index_csr, reg=r1)`: Extracts the `rowid` from the index record.\n   - `SeekRowid(cursor=table_csr, reg=r1)`: Uses the B-tree implementation from M5 to jump to the actual row in the main table.\n   - `Column(cursor=table_csr, ...)`: Extracts the full data.\n   - `Next(cursor=index_csr, label=loop_start)`: Advances to the next cell in the index leaf.\n4. **Range Check**: If the next index cell has a value `> 'val'`, the loop terminates (for equality).\n\n{{DIAGRAM:tdd-diag-m7-index-lookup}}\n{{DIAGRAM:m7_double_lookup}}\n\n### 5.2 B+Tree Leaf Linking & Range Scan\nUnlike the Table B-tree (M5), the Index B+tree must support fast horizontal traversal.\n\n1. **During `index_split`**:\n   - When Leaf `L1` splits into `L1` and `L2`:\n   - `L2->next_leaf = L1->next_leaf`.\n   - `L1->next_leaf = L2_PageID`.\n2. **During `index_cursor_next`**:\n   - `cursor->cell_idx++`.\n   - If `cursor->cell_idx >= page->cell_count`:\n     - `next_id = page->header.next_leaf`.\n     - If `next_id == 0`: `cursor->is_eof = true`.\n     - Else: Fetch `next_id`, set `cursor->cell_idx = 0`.\n\n### 5.3 Maintenance: The Write Tax\nEvery Table DML operation must trigger index updates.\n\n**Algorithm: `table_insert_with_indexes(Table* t, Row* r, int64_t rowid)`**\n1. Insert `r` into `t->btree` at `rowid`.\n2. For each `Index* idx` in `t->indexes`:\n   - `Value* vals = extract_columns(r, idx->col_indices)`.\n   - `index_insert(idx, vals, rowid)`.\n   - If `index_insert` returns `ERR_UNIQUE_VIOLATION`:\n     - **Rollback**: Delete `r` from `t->btree` (and any previous indexes).\n     - Return error to VM.\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `UniqueConstraintFailed` | `index_insert` | Abort DML, trigger transaction rollback. | Yes: \"UNIQUE constraint failed: table.col\" |\n| `StaleIndexEntry` | `SeekRowid` | If RowID found in Index but not in Table, panic. | Yes: \"Database malformed: orphaned index\" |\n| `IndexPageFull` | `index_page` | Trigger `index_split`. | No |\n| `TypeMismatch` | `index_compare` | Apply SQL conversion rules or return error. | Yes: \"Comparison error\" |\n\n---\n\n## 7. Implementation Sequence\n\n### Phase 1: B+Tree Leaf Linking (Estimated: 4-5 Hours)\n- Modify the `page_split` logic from Milestone 5 to support the `next_leaf` pointer at offset 0x08.\n- Implement the horizontal `cursor_next` logic that follows the `next_leaf` pointer.\n- **Checkpoint**: Insert enough index entries to cause 3 splits. Manually iterate from the first leaf using only `next_leaf` pointers and verify all entries are present in sorted order.\n\n### Phase 2: Unique Constraint Enforcement (Estimated: 2-3 Hours)\n- Implement `index_find(root, values)` which searches the B+tree.\n- Add a check in `index_insert` that fails if a match is found and `is_unique` is true.\n- **Checkpoint**: Attempt to insert the same value twice into a UNIQUE index. Verify the second call returns `ERR_UNIQUE_VIOLATION`.\n\n### Phase 3: VDBE Integration (Estimated: 4 Hours)\n- Implement `OP_IdxGE`, `OP_IdxRowid`, and `OP_IdxNext`.\n- Update the Bytecode Compiler to detect when an index can be used (Simple equality check for now).\n- **Checkpoint**: Run `EXPLAIN SELECT * FROM t WHERE indexed_col = 5`. Verify it uses `IdxGE` instead of `Rewind`.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Equality Lookup\n- **Input**: Table with index on `age`. Rows with ages `[20, 21, 22, 21]`.\n- **Query**: `SELECT rowid FROM t WHERE age = 21`\n- **Expected**: Index search finds RowID for first 21, `IdxNext` finds RowID for second 21, then terminates. Total 2 RowIDs returned.\n\n### 8.2 Edge Case: Indexing NULLs\n- **Input**: Insert `NULL` into an indexed column.\n- **Expected**: Index B+tree should store the NULL at the very beginning of the first leaf. `WHERE col IS NULL` should successfully use the index.\n\n### 8.3 Failure Case: Maintenance Failure\n- **Action**: Manually delete a row from the Table B-tree but *not* the Index.\n- **Query**: Use the index to find that row.\n- **Expected**: `SeekRowid` fails to find the row in the table. The system must report a corruption error.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Point Lookup** | $O(\\log N)$ | Measure time for 1M row table; search must take < 5 page fetches. |\n| **Range Scan** | > 500k entries/sec | Measure time to scan 10% of a 1M entry index. |\n| **Maintenance Overhd**| < 2x write time | Compare `INSERT` time with 0 indexes vs 1 index. |\n\n---\n\n## 10. Concurrency: Synchronous Updates\n\nIn this intermediate build, we enforce **Atomic Multi-Tree Updates**.\n\n1. When a transaction modifies a table, it must hold an `EXCLUSIVE` lock on the entire database file.\n2. The index update happens within the same atomic block as the table update.\n3. If the index update fails (e.g., Unique constraint), the Pager's **Rollback Journal** (M9) will be used to undo the change to the main table.\n\n**Invariant**: No reader will ever see a state where the Table B-tree is updated but the Index B+tree is not.\n\n---\n\n## 11. Wire Format: Index Cell Detail\n\nThe `record_data` in an index cell is a standard SQL Record (from M6).\n\n**Example for Index on (City, Zip):**\nRow: `City='London', Zip=10001, RowID=500`\n1. **Values**: `['London', 10001, 500]`\n2. **Serial Types**: `[13+(6*2)=25, 4 (32-bit int), 4 (32-bit int)]`\n3. **Record Header**: `[Size: 4, 25, 4, 4]`\n4. **Record Data**: `['L','o','n','d','o','n', 0x00, 0x00, 0x27, 0x11, 0x00, 0x00, 0x01, 0xF4]`\n\n{{DIAGRAM:tdd-diag-m7-index-cell}}\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m8 -->\n# Module Design: Query Planner & Statistics (build-sqlite-m8)\n\n## 1. Module Charter\n\nThe **Query Planner & Statistics** module is the intelligence layer of the RDBMS. It acts as a strategic advisor to the Bytecode Compiler, transforming a declarative SQL AST into the most efficient physical execution plan. By utilizing a **Cost-Based Optimizer (CBO)**, it evaluates multiple \"Access Paths\" (e.g., Table Scan vs. Index Scan) and selects the one that minimizes predicted Disk I/O. \n\n**Core Responsibilities:**\n- **Statistics Collection (`ANALYZE`)**: Persistently store metadata about table volume and column cardinality in a system catalog.\n- **Selectivity Estimation**: Calculate the \"filtering power\" of `WHERE` clause predicates based on available statistics or heuristics.\n- **Cost Modeling**: Quantify the predicted performance of a plan candidate using a weighted formula that penalizes random I/O.\n- **Join Optimization**: Determine the optimal nesting order for multi-table joins to minimize intermediate result sets (the \"Funnel Principle\").\n- **Plan Generation**: Emit a refined set of high-level execution directives that the VDBE Compiler uses to generate final bytecode.\n\n**Non-Goals:**\n- This module does not perform physical data access; it queries the metadata catalog.\n- It does not handle syntax validation (assumed valid from Milestone 2).\n- It does not implement low-level B-tree splits (handled in Milestone 5/7).\n\n**Invariants:**\n- The Planner must always fall back to a Full Table Scan if no suitable indexes exist or if statistics are missing.\n- Cost estimates must be reproducible and deterministic for a given set of statistics.\n- System statistics tables must be accessible via standard internal B-tree cursors.\n\n---\n\n## 2. File Structure\n\nImplementation follows this sequence to ensure the data source (Stats) exists before the logic (Planner) is built:\n\n1. `src/include/optimizer/stats.h`: Definition of the `sqlite_stat1` equivalent internal structures.\n2. `src/optimizer/analyzer.c`: Implementation of the `ANALYZE` command (B-tree sampling logic).\n3. `src/include/optimizer/cost_model.h`: Cost constants and the `PlanCandidate` structure.\n4. `src/optimizer/selectivity.c`: Logic for estimating row counts from predicates (`=`, `<`, `>`, `BETWEEN`).\n5. `src/optimizer/planner.c`: The core search algorithm for selecting access paths and join orders.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 Statistics Storage (System Catalog)\nStatistics are stored in an internal table named `sys_stats`. This table is a standard Table B-tree.\n\n| Column | Type | Description |\n| :--- | :--- | :--- |\n| `tbl` | TEXT | Name of the table. |\n| `idx` | TEXT | Name of the index (NULL if table-level stats). |\n| `stat` | TEXT | A space-separated string of integers: `nRow nDist1 nDist2 ...` |\n\n**Example**: `users | idx_age | 1000000 500`\n- `1000000`: Total rows in the table.\n- `500`: Average number of rows per distinct value in the index. (Selectivity = 500 / 1,000,000).\n\n### 3.2 In-Memory Optimizer Structures\n\n```c\ntypedef struct {\n    uint64_t row_count;      // Total rows (nRow)\n    uint32_t page_count;     // Total pages in B-tree (from Pager)\n    double avg_row_size;     // bytes\n} TableStats;\n\ntypedef struct {\n    char* index_name;\n    uint64_t distinct_values; // Cardinality\n    double selectivity;       // range [0.0 - 1.0]\n    uint32_t root_page;\n} IndexStats;\n\ntypedef enum {\n    PATH_SCAN,\n    PATH_INDEX\n} AccessPathType;\n\ntypedef struct {\n    AccessPathType type;\n    IndexStats* index;       // NULL if Table Scan\n    double est_cost;         // The calculated \"Cost\"\n    uint64_t est_rows;       // Predicted output rows\n} PlanCandidate;\n```\n\n{{DIAGRAM:tdd-diag-m8-stats-model}}\n*(Visual: Relationship diagram: Table -> has multiple IndexStats -> used by PlanCandidate to calculate Cost)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `void optimizer_analyze_table(const char* table_name)`\n- **Purpose**: Scans the table B-tree and all associated index B+trees to update `sys_stats`.\n- **Constraint**: Must hold a `SHARED` lock on the table. For massive tables, it may sample every Nth page.\n- **Output**: Updates/Inserts rows in the `sys_stats` B-tree.\n\n### 4.2 `double estimate_selectivity(Predicate* pred, IndexStats* stats)`\n- **Purpose**: Predicts what fraction of the table matches a WHERE clause.\n- **Rules**:\n    - Equality (`=`): If index exists, `1.0 / stats->distinct_values`. Else, default `0.1`.\n    - Range (`<`, `>`): Default `0.33` (Rule of thumb).\n    - In-list (`IN (...)`): `num_elements / stats->distinct_values`.\n- **Return**: A double between `0.0` and `1.0`.\n\n### 4.3 `PlanCandidate optimizer_find_best_path(TableContext* tbl, WhereClause* where)`\n- **Purpose**: Iterates through all available indexes and compares costs.\n- **Algorithm**: See Section 5.2.\n- **Error**: Returns `PATH_SCAN` as default if errors occur.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 The `ANALYZE` Procedure (Phase 1)\nTo populate statistics without blocking the DB for hours:\n\n1. **Table Scan**:\n   - Initialize `row_counter = 0`.\n   - Iterate through the Table B-tree leaf sequence.\n   - For every row, increment `row_counter`.\n2. **Index Scan**:\n   - For each index:\n     - Initialize `distinct_counter = 1`.\n     - Iterate through Index B+tree. \n     - Compare current key to `prev_key`. If different, `distinct_counter++`.\n3. **Persist**:\n   - Calculate `avg_rows_per_value = row_counter / distinct_counter`.\n   - Write `row_counter` and `avg_rows_per_value` to `sys_stats` using an internal `INSERT` (VDBE).\n\n### 5.2 The Cost Model (Phase 2)\nThe planner uses a weighted formula where Random I/O is the primary \"currency\".\n\n**Constants:**\n- `SEQ_PAGE_COST = 1.0` (Cost to read one page sequentially)\n- `RAND_PAGE_COST = 4.0` (Cost to seek to a random page - SSD/HDD penalty)\n- `CPU_ROW_COST = 0.01` (Cost to process one row in the VM)\n\n**Formula 1: Table Scan Cost**\n$$Cost_{scan} = (TablePages \\times SEQ\\_PAGE\\_COST) + (TotalRows \\times CPU\\_ROW\\_COST)$$\n\n**Formula 2: Index Scan Cost (Double Lookup)**\n1. `MatchRows = TotalRows * Selectivity(Predicate)`\n2. `IndexPagesRead = MatchRows / RowsPerPageIndex`\n3. `TablePagesRead = MatchRows` (Assuming every match is a random seek)\n4. $$Cost_{index} = (IndexPagesRead \\times SEQ\\_PAGE\\_COST) + (TablePagesRead \\times RAND\\_PAGE\\_COST) + (MatchRows \\times CPU\\_ROW\\_COST)$$\n\n{{DIAGRAM:tdd-diag-m8-planner-flow}}\n*(Visual: Decision Tree: Start -> For each Index -> Calc Selectivity -> Calc Cost -> Compare with Scan -> Choose Min)*\n\n### 5.3 Join Order Optimization (Greedy Approach)\nFor a query `A JOIN B JOIN C`:\n\n1. Start with the smallest table (lowest `nRow`) as the **Outer Loop**.\n2. For the next table, evaluate which one has an index on the join predicate (e.g., `B.id = A.b_id`).\n3. If multiple tables have indexes, pick the one where the index scan cost is lowest.\n4. If no indexes exist, pick the smallest remaining table to minimize the product of the nested loops.\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `StatisticsStale` | `optimizer_find_best_path` | If `sys_stats` is older than a threshold or missing, use **Heuristics** (default selectivity). | No (Silent fallback) |\n| `PlannerTimeout` | `join_optimizer` | If join permutations > 1000, abort exhaustive search and use **Greedy** search. | No (Silent fallback) |\n| `InternalStatsCorrupt`| `analyzer` | If `sys_stats` format is unreadable, delete the stats row and use heuristics. | Yes (Warning) |\n| `DivisionByZero` | `selectivity` | If `distinct_values == 0`, assume cardinality of 1. | No |\n\n---\n\n## 7. Implementation Sequence\n\n### Phase 1: `ANALYZE` Infrastructure (Estimated: 5-6 Hours)\n- Implement the `sys_stats` table creation in the system catalog.\n- Write the `analyzer_scan` logic that traverses B-trees and counts keys.\n- **Checkpoint**: Run `ANALYZE users;`. Verify that the `sys_stats` table contains a row for `users` with the correct row count.\n\n### Phase 2: Cost Model & Selectivity (Estimated: 4-5 Hours)\n- Implement the `CostModel` functions for Scan and Index paths.\n- Implement the `SelectivityEstimator` with default heuristics for equality and ranges.\n- **Checkpoint**: Create a table with 10,000 rows and an index. Manually calculate the cost. Verify the code's output matches your calculation.\n\n### Phase 3: Access Path Selection (Estimated: 4 Hours)\n- Integrate the planner into the Bytecode Compiler.\n- Modify `vdbe_compile_select` to call `find_best_path`.\n- **Checkpoint**: Run `EXPLAIN SELECT * FROM t WHERE age = 20`. \n    - Case A (No index): `EXPLAIN` shows `Rewind`.\n    - Case B (With index): `EXPLAIN` shows `IdxGE`.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Index Choice\n- **Setup**: Table `T` with 1M rows. Index `I` on `col`.\n- **Action**: Query `SELECT * FROM T WHERE col = 5`.\n- **Expected**: Planner chooses `PATH_INDEX` because $Cost_{index} \\approx 4.0 \\ll Cost_{scan} \\approx 10,000.0$.\n\n### 8.2 Edge Case: High Selectivity (The Scan Choice)\n- **Setup**: Table `T` with 1M rows. Index `I` on `status`. Only 2 statuses exist (Active/Inactive).\n- **Action**: Query `WHERE status = 'Active'`.\n- **Expected**: Selectivity is 0.5. $Cost_{index} = 500,000 \\times 4.0 = 2,000,000$. $Cost_{scan} \\approx 10,000$. Planner chooses `PATH_SCAN`.\n\n### 8.3 Failure Case: Missing Stats\n- **Setup**: New table `T2` created, but `ANALYZE` not yet run.\n- **Action**: Query `WHERE col = 5`.\n- **Expected**: Planner falls back to **Heuristics**. It assumes `col` is unique enough to justify an index if one exists.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Planning Latency** | < 1ms | Measure `vdbe_compile` time for a 3-table join. |\n| **ANALYZE Speed** | 100k rows/sec | Time the `ANALYZE` command on a medium table. |\n| **Join Permutations**| Cap at 1000 | Ensure complex joins (10+ tables) don't hang the compiler. |\n\n---\n\n## 10. Concurrency Specification\n\n**Model: Read-Only Metadata Access**\n\n1. The Planner acquires a `SHARED` lock on the `sys_stats` table at the start of compilation.\n2. It reads the necessary rows into a local cache (stack-allocated) to avoid repeated B-tree lookups during plan enumeration.\n3. The lock is released as soon as the bytecode is emitted.\n4. **Race Condition**: If `ANALYZE` is running while a query is being planned, the Planner sees either the old stats or the new stats (Atomicity is guaranteed by the Pager). Stale stats are acceptable; incorrect stats are not.\n\n---\n\n## 11. Heuristics Table (When Stats are Missing)\n\n| Condition | Default Selectivity |\n| :--- | :--- |\n| `column = constant` | 0.05 (Assume 20 distinct values) |\n| `column > constant` | 0.33 |\n| `column BETWEEN c1 AND c2` | 0.10 |\n| `column LIKE 'prefix%'` | 0.10 |\n| `column IS NULL` | 0.01 |\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m9 -->\n# Module Design: Transactions (Rollback Journal) (build-sqlite-m9)\n\n## 1. Module Charter\n\nThe **Transactions (Rollback Journal)** module is the guardian of the database's \"Durability Soul.\" Its primary mission is to implement **ACID Atomicity** and **Crash Recovery** using the classic Rollback Journal mechanism. This module ensures that even in the event of a power failure, OS crash, or application panic, the database remains in a consistent state\u2014either fully reflecting a completed transaction or appearing as if the transaction never began. It manages the physical choreography of writing \"Undo\" data to a separate journal file before any modifications touch the main database.\n\n**Core Responsibilities:**\n- Implement a **Multi-state Locking Machine** (Shared, Reserved, Pending, Exclusive) to coordinate concurrent access.\n- Manage the creation, population, and synchronization (`fsync`) of the **Rollback Journal** file.\n- Intercept the Pager's write requests to ensure original page data is archived before being overwritten (the **Undo Log**).\n- Execute the **Atomic Commit Choreography**, strictly ordering I/O operations and hardware flushes.\n- Perform **Hot Journal Recovery** on startup, detecting interrupted transactions and restoring the database to its last consistent state.\n\n**Non-Goals:**\n- This module does not handle Write-Ahead Logging (WAL); that is Milestone 10.\n- it does not implement row-level locking; locking is performed at the database file level.\n- It does not handle logical \"Undo\" for individual SQL statements within a transaction (savepoints), only the full transaction rollback.\n\n**Invariants:**\n- The Rollback Journal must be physically flushed to disk (`fsync`) before the first modified page is written to the database file.\n- A \"Hot Journal\" (a journal existing without an accompanying exclusive lock) must always trigger a recovery before any other operation.\n- The database file must be `fsync`'d before the journal file is deleted or truncated during a commit.\n\n---\n\n## 2. File Structure\n\nImplementation must proceed in this specific order to establish the locking safety net before implementing file mutations:\n\n1. `src/include/storage/lock_manager.h`: Definition of the 5-state locking machine and OS-specific lock types.\n2. `src/storage/lock_manager.c`: Implementation of advisory file locking (using `fcntl` on Unix or `LockFileEx` on Windows).\n3. `src/include/storage/journal_format.h`: Byte-level layout for the journal header and page records.\n4. `src/storage/journal.c`: Logic for creating the journal and appending original page images.\n5. `src/storage/recovery.c`: The startup \"Healer\" logic that detects and processes hot journals.\n6. `src/storage/pager_transaction.c`: Integration with the Pager to trigger journaling on `pager_write()`.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 The Rollback Journal Wire Format (Byte-Level)\n\nThe journal file (`.db-journal`) is a sequence of a header followed by multiple page records.\n\n**Journal Header (Offset 0x00)**\n| Offset | Size | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | 8 bytes | `magic` | Constant: `0xd9d505f920a1c3b7`. |\n| 0x08 | 4 bytes | `page_count` | Number of page records in this journal (Big-Endian). |\n| 0x0C | 4 bytes | `nonce` | Random integer used to salt the checksums. |\n| 0x10 | 4 bytes | `db_size_pages` | Total size of the DB file in pages before transaction (Big-Endian). |\n| 0x14 | 4 bytes | `sector_size` | The sector size of the underlying disk (usually 512). |\n| 0x18 | 4 bytes | `page_size` | The page size of the database (usually 4096). |\n\n**Journal Page Record (Appended)**\n| Offset | Size | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | 4 bytes | `page_no` | The physical page number in the `.db` file. |\n| 0x04 | 4096 bytes| `data` | The **original** 4KB content of the page. |\n| 0x1004 | 4 bytes | `checksum` | Cumulative checksum of `nonce + page_no + data`. |\n\n### 3.2 Locking State Machine\n\n| State | Readers? | Writers? | Description |\n| :--- | :--- | :--- | :--- |\n| **UNLOCKED** | Yes | Yes | Initial state. No file locks held. |\n| **SHARED** | Many | No | Multiple processes can read. No one can write. |\n| **RESERVED** | Many | 1 (Intended)| One process intends to write. New readers can still enter. |\n| **PENDING** | Current | 1 (Waiting) | Writer waiting for current readers to exit. No new readers allowed. |\n| **EXCLUSIVE** | 0 | 1 (Active) | Writer is modifying the file. Absolute isolation. |\n\n{{DIAGRAM:tdd-diag-m9-locks}}\n*(Visual: State diagram showing transitions: Unlocked -> Shared -> Reserved -> Pending -> Exclusive)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `int pager_begin_transaction(Pager* p)`\n- **Purpose**: Transitions from `SHARED` to `RESERVED`.\n- **Pre-condition**: Pager must already hold a `SHARED` lock.\n- **Errors**: `SQLITE_BUSY` if another process holds a `RESERVED` or `EXCLUSIVE` lock.\n\n### 4.2 `int pager_write(Pager* p, uint32_t page_no)`\n- **Purpose**: Intercepts a write attempt.\n- **Logic**: \n    1. If `page_no` is not already in the journal:\n        - Read original page from disk.\n        - Append to journal file.\n        - Increment `page_count` in journal header (in-memory).\n    2. Mark the Buffer Pool frame as `dirty`.\n- **Invariants**: Must be called *before* any byte in the memory frame is changed.\n\n### 4.3 `int pager_commit(Pager* p)`\n- **Purpose**: Executes the Atomic Commit Dance.\n- **Logic**: See Algorithm 5.1.\n- **Recovery**: If any `fsync` fails, the transaction is considered failed and must be rolled back.\n\n### 4.4 `int pager_rollback(Pager* p)`\n- **Purpose**: Manually undoes changes.\n- **Logic**: Reads journal, writes original pages back to DB, deletes journal, drops locks.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 The Atomic Commit Dance (The \"Dance of Durability\")\n\nThis procedure is the core of Milestone 9. It ensures that the database is never in a state where a crash causes permanent corruption.\n\n1. **Journal Flush**: Call `fsync()` on the journal file. This ensures the \"Undo Log\" is safe on the physical platter.\n2. **Lock Upgrade (Exclusive)**: Transition from `RESERVED` to `EXCLUSIVE`. This requires waiting for all `SHARED` readers to close their handles. (State moves to `PENDING` then `EXCLUSIVE`).\n3. **Database Write**: Write all `dirty` pages from the Buffer Pool into the main `.db` file at their original offsets.\n4. **Database Flush**: Call `fsync()` on the `.db` file. The new data is now hardened.\n5. **Cleanup (The Commit Point)**:\n    - Delete the journal file.\n    - **Note**: On some file systems, you must also `fsync()` the parent directory to ensure the deletion is persistent.\n6. **Lock Downgrade**: Move back to `SHARED` or `UNLOCKED`.\n\n{{DIAGRAM:tdd-diag-m9-write-seq}}\n*(Visual: Sequence diagram: Pager -> [fsync Journal] -> [Lock Exclusive] -> [Write DB] -> [fsync DB] -> [Delete Journal])*\n\n### 5.2 Hot Journal Recovery (Startup Procedure)\n\nThis algorithm must run whenever a database connection is opened.\n\n1. **Check for Journal**: Does `[db-name]-journal` exist?\n2. **Determine \"Hotness\"**: Attempt to acquire a `SHARED` lock on the database file.\n    - If successful, and the journal exists, the journal is **HOT** (it means the previous process died while holding an exclusive lock and left the journal behind).\n3. **The Rollback**:\n    - Acquire an `EXCLUSIVE` lock on the database (to prevent others from reading during recovery).\n    - Read the `page_size` and `db_size_pages` from the journal header.\n    - For each page record in the journal:\n        - Read `page_no`, `data`, and `checksum`.\n        - Verify `checksum(nonce, page_no, data)`. \n        - If checksum fails, the crash happened during journal writing; stop and ignore the rest of the journal.\n        - Write `data` into the `.db` file at `page_no * page_size`.\n    - Truncate the `.db` file to `db_size_pages * page_size` (removes any appended pages).\n    - `fsync()` the `.db` file.\n4. **Cleanup**: Delete the journal and release the `EXCLUSIVE` lock.\n\n{{DIAGRAM:tdd-diag-m9-recovery-flow}}\n*(Visual: Flowchart: Start -> Journal Exists? -> Can lock Shared? -> If Yes, Rollback -> Delete Journal -> Finish)*\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `JournalCorrupt` | `recovery` | If magic number or checksums fail, the transaction was never started/half-journaled; delete journal safely. | Yes (Warning) |\n| `LockDeadlock` | `lock_manager` | If `PENDING` state times out, release all locks and return `BUSY`. | Yes: \"Database is locked\" |\n| `PartialWrite` | `write()` | If disk fills during journal write, delete journal and abort transaction before touching DB. | Yes: \"Disk full\" |\n| `FsyncFailed` | `fsync()` | Hardware error. Panic and close connection. Database remains in \"Recovery Needed\" state. | Yes: \"I/O Error\" |\n\n---\n\n## 7. Implementation Sequence\n\n### Phase 1: Locking State Machine (Estimated: 3-4 Hours)\n- Implement `src/storage/lock_manager.c`.\n- Use `fcntl` (Unix) with `F_SETLK`. Map states:\n    - `SHARED`: Read lock on bytes 100-101.\n    - `RESERVED`: Write lock on byte 102.\n    - `PENDING`: Write lock on byte 103.\n    - `EXCLUSIVE`: Write lock on bytes 100-101.\n- **Checkpoint**: Run two processes. Verify Process B cannot acquire `RESERVED` if Process A holds it. Verify Process A cannot acquire `EXCLUSIVE` if Process B holds `SHARED`.\n\n### Phase 2: Journaling & fsync (Estimated: 4-5 Hours)\n- Implement `pager_write` to copy pages to the journal.\n- Implement the `fsync` ordering logic in `pager_commit`.\n- **Checkpoint**: Perform an `UPDATE`. Kill the process (using `SIGKILL`) *after* the `fsync` of the journal but *before* the deletion of the journal. Verify the `.db-journal` file exists and contains the correct \"original\" page data.\n\n### Phase 3: Crash Recovery (Estimated: 3-4 Hours)\n- Implement `pager_recover_if_needed` in the database open path.\n- Implement the checksum and truncation logic.\n- **Checkpoint**: Run the \"killed\" database from Phase 2. Verify that the `SELECT` query returns the **old** data (rollback successful) and the journal file is automatically deleted.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Successful Commit\n- **Setup**: `UPDATE users SET name = 'New' WHERE id = 1`.\n- **Action**: Call `COMMIT`.\n- **Expected**: \n    - Journal is created then deleted.\n    - Database file contains 'New'.\n    - No journal file exists on disk.\n\n### 8.2 Failure Case: Crash Mid-Transaction\n- **Setup**: Start `UPDATE`.\n- **Action**: Manually write garbage to a database page in a hex editor while a journal exists. Run `recovery`.\n- **Expected**: The \"garbage\" page is overwritten by the original page from the journal.\n\n### 8.3 Failure Case: Lock Contention\n- **Setup**: Thread A has an open `SELECT` cursor (holding `SHARED`).\n- **Action**: Thread B calls `BEGIN TRANSACTION` followed by an `INSERT`.\n- **Expected**: Thread B succeeds in `RESERVED`, but `pager_commit` returns `SQLITE_BUSY` (or waits) at the `PENDING` stage because Thread A is still reading.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Commit Latency** | < 50ms | Time from `COMMIT` command to return (includes 2-3 `fsync` calls). |\n| **Fsync Count** | Exactly 2-3 | Use `strace -e fsync` to count calls per transaction. |\n| **Recovery Time** | < 100ms | Measure time to recover from a 100-page \"Hot Journal\". |\n\n---\n\n## 10. Concurrency Specification: The Lock Protocol\n\nTo ensure no deadlocks and absolute consistency:\n\n1. **Reader Path**:\n    - `UNLOCKED` -> `SHARED`.\n    - Cannot move to `SHARED` if a `PENDING` or `EXCLUSIVE` lock is on the file.\n2. **Writer Path**:\n    - Must be in `SHARED` first.\n    - `SHARED` -> `RESERVED` (Only one writer allowed).\n    - `RESERVED` -> `PENDING` (Prevents new readers from entering, but lets current readers finish).\n    - `PENDING` -> `EXCLUSIVE` (Waits for `SHARED` count to hit 0).\n3. **Wait Policy**: If a lock cannot be acquired, the manager should sleep for 10ms and retry up to 100 times before returning `SQLITE_BUSY`.\n\n---\n\n## 11. Recovery Checksum Algorithm\n\nUse the **Fletcher-32** checksum for speed and reliability.\n\n```c\nuint32_t calculate_checksum(uint32_t nonce, uint32_t page_no, uint8_t* data) {\n    uint32_t s1 = nonce + page_no;\n    uint32_t s2 = s1;\n    for(int i=0; i < 4096; i++) {\n        s1 += data[i];\n        s2 += s1;\n    }\n    return (s1 & 0xffff) | (s2 << 16);\n}\n```\n*Note: In production, checksums are verified to ensure that a \"Torn Write\" to the journal doesn't result in restoring garbage to the database.*\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: build-sqlite-m10 -->\n# Module Design: Write-Ahead Logging (WAL) Mode (build-sqlite-m10)\n\n## 1. Module Charter\n\nThe **Write-Ahead Logging (WAL) Mode** module provides a high-concurrency durability alternative to the Rollback Journal. Its primary mission is to decouple readers from writers, allowing multiple simultaneous readers to query a consistent snapshot of the database while a single writer appends changes to a separate log. This module implements **Snapshot Isolation** by ensuring that readers only see transactions committed before their start time. It replaces the \"Undo\" logic of the Rollback Journal with a \"Redo\" logic, where the main database file remains untouched during transactions, and updates are periodically merged back via a **Checkpointer**.\n\n**Core Responsibilities:**\n- Manage the **WAL File** (`.db-wal`) as a sequential append-only redo log.\n- Implement the **WAL Index** (`.db-shm`) as a memory-mapped hash table for O(1) page lookups.\n- Coordinate **Snapshot Isolation** using \"Read Marks\" to track the visibility of WAL frames for active readers.\n- Execute the **Checkpointing** process to migrate pages from the WAL back to the main database file.\n- Provide high-performance **Cumulative Checksumming** to detect torn writes or log corruption.\n\n**Non-Goals:**\n- This module is not a replacement for the B-tree layer; it is an I/O redirection layer within the Pager.\n- It does not support multiple simultaneous writers (SQLite remains 1-writer-N-readers).\n- It does not handle logical SQL undo; it only manages physical page versions.\n\n**Invariants:**\n- A reader must never see a WAL frame with an index greater than the \"Commit Mark\" present at the reader's start time.\n- The main database file must never be modified while a reader is accessing a version of a page that has not yet been checkpointed.\n- WAL frames must be appended in strictly increasing transaction order.\n\n---\n\n## 2. File Structure\n\nImplementation must follow this sequence to build the log structure before the indexing/concurrency logic:\n\n1. `src/include/storage/wal_format.h`: Byte-level definitions for WAL headers and frame structures.\n2. `src/storage/wal_checksum.c`: Implementation of the 64-bit cumulative checksum algorithm.\n3. `src/include/storage/wal_index.h`: Memory-mapped WAL Index structure (shared-memory).\n4. `src/storage/wal_index.c`: Logic for updating and searching the WAL Index.\n5. `src/storage/wal_manager.c`: Core logic for appending frames and coordinating snapshots.\n6. `src/storage/checkpoint.c`: The checkpointer logic for merging log data into the main DB.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 The WAL File Format (Byte-Level)\n\nThe WAL file consists of a fixed 32-byte header followed by zero or more frames.\n\n**WAL Header (Offset 0x00)**\n| Offset | Size | Field | Description | Endian |\n| :--- | :--- | :--- | :--- | :--- |\n| 0x00 | 4 bytes | `magic` | `0x377f0682` (normal) or `0x377f0683` (big-endian). | Native |\n| 0x04 | 4 bytes | `version` | WAL format version (currently 3007000). | Big |\n| 0x08 | 4 bytes | `page_size` | Database page size (e.g., 4096). | Big |\n| 0x0C | 4 bytes | `checkpoint_seq`| Incremental counter for checkpoint operations. | Big |\n| 0x10 | 4 bytes | `salt_1` | Random value for checksum initialization. | Big |\n| 0x14 | 4 bytes | `salt_2` | Second random value for checksum initialization. | Big |\n| 0x18 | 4 bytes | `checksum_1` | First 32 bits of header checksum. | Big |\n| 0x1C | 4 bytes | `checksum_2` | Second 32 bits of header checksum. | Big |\n\n**WAL Frame (Repeated)**\nEach frame encapsulates one database page.\n| Offset | Size | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | 4 bytes | `page_no` | The original Page ID in the `.db` file. |\n| 0x04 | 4 bytes | `db_size` | Size of DB file in pages after this commit (0 if not a commit). |\n| 0x08 | 4 bytes | `salt_1` | Copy of `salt_1` from header. |\n| 0x0C | 4 bytes | `salt_2` | Copy of `salt_2` from header. |\n| 0x10 | 4 bytes | `checksum_1` | Cumulative checksum up to this point (Part 1). |\n| 0x14 | 4 bytes | `checksum_2` | Cumulative checksum up to this point (Part 2). |\n| 0x18 | `PAGE_SIZE` | `page_data` | The raw 4KB page content. |\n\n{{DIAGRAM:tdd-diag-m10-wal-layout}}\n\n### 3.2 The WAL Index (`-shm` file)\n\nThe WAL Index is a memory-mapped file that enables fast lookups. For this implementation, we use a simplified version of the SQLite WAL-Index.\n\n**WAL Index Header**\n| Offset | Size | Field | Description |\n| :--- | :--- | :--- | :--- |\n| 0x00 | 4 bytes | `version` | Index version. |\n| 0x04 | 4 bytes | `is_init` | Initialization flag. |\n| 0x08 | 4 bytes | `mx_frame` | Index of the last valid frame in the WAL. |\n| 0x0C | 4 bytes | `n_page` | Number of pages in the DB according to the WAL. |\n| 0x10 | 32 bytes | `read_marks` | Array of 8 `uint32_t` marks for concurrent readers. |\n\n**Hash Table Blocks**\nFollowing the header, the index contains blocks of 16-bit integers mapping `PageID % HashSize -> FrameIndex`. This allows the engine to skip scanning the WAL file.\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `int wal_append_frames(Wal* pWal, Page** apPages, int nPages, uint32_t dbSize)`\n- **Purpose**: Appends a set of modified pages to the log as a single atomic transaction.\n- **Constraints**: \n    - `nPages` must be $\\ge 1$.\n    - `dbSize` is the post-transaction file size (only set on the last frame).\n- **Errors**: `DISK_FULL`, `IO_ERROR`, `WAL_TOO_LARGE`.\n\n### 4.2 `int wal_find_page(Wal* pWal, uint32_t page_no, uint32_t read_mark, void** ppOut)`\n- **Purpose**: Search for a page version visible to a specific reader.\n- **Logic**: \n    1. Search the WAL Index for the highest `frame_idx` such that `WAL_INDEX[frame_idx].page_no == page_no` AND `frame_idx <= read_mark`.\n    2. If found, read from `.db-wal` at `32 + frame_idx * (24 + PAGE_SIZE)`.\n- **Return**: `SQLITE_OK` if found in WAL, `SQLITE_NOTFOUND` if the reader must fallback to the `.db` file.\n\n### 4.3 `int wal_checkpoint(Wal* pWal, int mode)`\n- **Purpose**: Merge WAL frames into the main database.\n- **Modes**:\n    - `PASSIVE`: Checkpoint as much as possible without blocking readers.\n    - `FULL`: Wait for all readers to finish, then checkpoint everything.\n- **Errors**: `CHECKPOINT_BLOCKED`.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 Cumulative Checksumming\nThe WAL uses a running checksum to detect torn writes. The checksum of Frame $N$ depends on the checksum of Frame $N-1$.\n\n1. **Initialize**: `s1 = salt_1`, `s2 = salt_2`.\n2. **Step 1 (Header)**: Iterate through the first 24 bytes of the header. Update `s1, s2` using the Fletcher-64 variant.\n3. **Step 2 (Frames)**: For each frame:\n    - Update `s1, s2` using the 8-byte Frame Header (excluding the checksum fields themselves).\n    - Update `s1, s2` using the 4096-byte Page Data.\n    - Store current `s1, s2` in the Frame Header's checksum fields.\n4. **Validation**: During recovery, if a frame's calculated checksum doesn't match its stored checksum, the log is truncated at the *previous* frame.\n\n### 5.2 The Snapshot Read (Read Mark Logic)\nTo support concurrent readers, the system uses a set of \"Read Marks\" in the shared memory.\n\n1. **Reader Starts**:\n    - Acquire a `SHARED` lock on the database.\n    - Read `mx_frame` (the last committed frame index) from the WAL Index.\n    - Find an available `read_mark` slot in the WAL Index and set it to `mx_frame`.\n    - This `read_mark` is the reader's \"Horizon.\"\n2. **Reading Page #X**:\n    - Call `wal_find_page(X, horizon)`.\n    - If found, use the WAL version.\n    - If not found, use the main `.db` file version.\n3. **Reader Finishes**:\n    - Set its `read_mark` slot back to `0xFFFFFFFF` (Available).\n\n{{DIAGRAM:tdd-diag-m10-isolation}}\n\n### 5.3 Passive Checkpointing\n1. **Determine the Limit**:\n    - Scan all active `read_marks` in the WAL Index.\n    - `min_mark = min(all_active_read_marks)`. \n    - This is the highest frame that *all* current readers have already moved past (or haven't reached yet).\n2. **The Redo Loop**:\n    - For `i` from `last_checkpointed_frame + 1` to `min_mark`:\n        - Read `page_no` and `data` from WAL Frame `i`.\n        - Write `data` to `.db` file at `page_no * page_size`.\n3. **Flush**: `fsync()` the `.db` file.\n4. **Update Metadata**: Update `last_checkpointed_frame` in the WAL Index.\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `WALChecksumMismatch` | `wal_open` | Truncate WAL at last valid frame. This represents a crash mid-write. | No (Automatic) |\n| `CheckpointBlocked` | `wal_checkpoint`| Return `SQLITE_BUSY`. Trigger auto-checkpoint later when readers exit. | Yes (If manual PRAGMA) |\n| `IndexCorrupt` | `wal_find_page` | Rebuild WAL Index from raw `.db-wal` file (Single scan). | No |\n| `SHM_MappingFailed` | `wal_open` | Fallback to Rollback Journal mode or return error. | Yes |\n\n---\n\n## 7. Implementation Sequence\n\n### Phase 1: WAL File Format (Estimated: 4-5 Hours)\n- Implement `wal_checksum.c` with the cumulative logic.\n- Implement `wal_append_frames` to write the header and serialized frames.\n- **Checkpoint**: Run a loop that appends 100 random pages. Verify the `.db-wal` file size is exactly `32 + 100 * (24 + 4096)` bytes and that checksums are mathematically valid.\n\n### Phase 2: WAL Index & Search (Estimated: 4-5 Hours)\n- Implement `mmap` logic for the `-shm` file.\n- Implement `wal_index_append` to update the hash table whenever a frame is written.\n- Implement `wal_find_page`.\n- **Checkpoint**: Manually append a new version of Page #1 to the WAL. Call `wal_find_page(1)`. Verify it returns the new version from the WAL, not the old one from the DB.\n\n### Phase 3: Checkpointing & Recovery (Estimated: 4-5 Hours)\n- Implement `wal_checkpoint(PASSIVE)`.\n- Implement `wal_open` recovery logic (detecting and truncating invalid frames).\n- **Checkpoint**: Fill WAL with 2000 pages. Run `wal_checkpoint`. Verify the `.db` file is updated and the WAL file is truncated/reset.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Simultaneous Read/Write\n- **Setup**: One thread starts a long `SELECT` (holds read mark at frame 10).\n- **Action**: Another thread performs 5 `INSERT`s (appends 50 frames to WAL).\n- **Expected**: The reader thread continues to see the data as it was at frame 10, even though the WAL now contains 60 frames.\n\n### 8.2 Edge Case: Log Wrapping\n- **Setup**: WAL reaches `auto_checkpoint` limit.\n- **Action**: Checkpoint runs successfully.\n- **Expected**: The next `INSERT` should overwrite the WAL from the beginning (offset 32), reset `mx_frame` to 1.\n\n### 8.3 Failure Case: Torn Write\n- **Setup**: Append a frame but intentionally corrupt its checksum footer.\n- **Action**: Re-open the database.\n- **Expected**: `wal_open` detects the mismatch, truncates the WAL at the previous valid frame, and allows the DB to function normally.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Write Throughput** | > 5,000 tx/sec | Run 10k single-row INSERTS in WAL mode. Compare to Rollback Mode. |\n| **Lookup Latency** | < 200ns | Measure `wal_find_page` time when the page is in a 1000-frame WAL. |\n| **Checkpt Overhead** | < 10% CPU | Monitor CPU usage during a background passive checkpoint. |\n\n---\n\n## 10. Concurrency Specification\n\n**Concurrency Logic: Snapshot Isolation (SI)**\n\n1. **The Writer**:\n    - Acquires `RESERVED` lock on the `.db` file (Only one writer).\n    - Appends to WAL.\n    - Updates `mx_frame` in SHM *after* `fsync` of WAL.\n2. **The Readers**:\n    - No locks on `.db` file required.\n    - Acquire `SHARED` lock on the SHM `read_mark` array to prevent a checkpointer from overwriting frames they are using.\n3. **The Checkpointer**:\n    - Acquires `EXCLUSIVE` lock on the SHM `read_mark` array to find the \"Safe Point.\"\n    - Acquires `CHECKPOINTER` lock to ensure only one checkpoint happens at a time.\n\n**Deadlock Avoidance**: Locks must always be acquired in the order: `SHM` -> `WAL` -> `DB`.\n\n---\n\n## 11. Memory Layout: WAL Index Hash Table\n\nTo achieve O(1) lookups, the `-shm` file uses a simple hash table.\n- **Bucket Count**: 1024 per block.\n- **Entry**: `uint16_t` (The index of the frame).\n- **Collision Handling**: Linear probing within the bucket block.\n\n```c\nuint32_t hash_page(uint32_t page_no) {\n    return (page_no * 383) % 1024;\n}\n```\n*Note: We multiply by a prime (383) to distribute Page IDs across the 1024 buckets.*\n<!-- END_TDD_MOD -->\n",
  "tdd_current_mod_index": 10,
  "tdd_diagrams_to_generate": [
    {
      "id": "tdd-diag-m1-arch",
      "title": "Lexer Architecture",
      "description": "Scanner struct containing pointers and state machine dispatch logic.",
      "type": "architecture",
      "anchor_target": "build-sqlite-m1"
    },
    {
      "id": "tdd-diag-m1-fsm",
      "title": "Tokenizer State Machine",
      "description": "State transitions for identifiers, numbers, and quoted strings.",
      "type": "state_machine",
      "anchor_target": "build-sqlite-m1"
    },
    {
      "id": "tdd-diag-m1-esc",
      "title": "SQL String Escape Logic",
      "description": "Lookahead logic for handling doubled-single-quotes ('') in literals.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m1"
    },
    {
      "id": "tdd-diag-m2-ast-node",
      "title": "AST Node Memory Layout",
      "description": "Struct definition for variant types (Select, Insert, BinaryExpr).",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m2"
    },
    {
      "id": "tdd-diag-m2-pratt-flow",
      "title": "Pratt Parsing Precedence Climbing",
      "description": "Recursive flow showing how binding power groups tokens like 5 + 3 * 2.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m2"
    },
    {
      "id": "tdd-diag-m2-select-flow",
      "title": "SELECT Statement Data Flow",
      "description": "Transformation of tokens into a SelectStatement tree structure.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m2"
    },
    {
      "id": "tdd-diag-m3-vm-arch",
      "title": "VDBE Runtime Architecture",
      "description": "Interactions between PC, Registers, Cursors, and Pager.",
      "type": "architecture",
      "anchor_target": "build-sqlite-m3"
    },
    {
      "id": "tdd-diag-m3-reg-layout",
      "title": "Register File Memory Layout",
      "description": "Dynamic typed Value array (Integer, Real, Text, Blob, Null).",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m3"
    },
    {
      "id": "tdd-diag-m3-compile-flow",
      "title": "AST to Bytecode Mapping",
      "description": "How a WHERE clause compiles into conditional jump instructions.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m3"
    },
    {
      "id": "tdd-diag-m4-pager-arch",
      "title": "Pager Architecture",
      "description": "Relationship between Page Cache, LRU List, and Disk Interface.",
      "type": "architecture",
      "anchor_target": "build-sqlite-m4"
    },
    {
      "id": "tdd-diag-m4-frame-layout",
      "title": "Page Frame Memory Layout",
      "description": "4KB buffer + metadata (PageID, PinCount, DirtyBit, LRUPointers).",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m4"
    },
    {
      "id": "tdd-diag-m4-lru-algo",
      "title": "LRU Eviction Algorithm",
      "description": "Visualizing Hash Map O(1) lookups and List Splice operations.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m4"
    },
    {
      "id": "tdd-diag-m4-pin-seq",
      "title": "Page Pinning Sequence",
      "description": "Lifecycle of a page from fetch to unpin, preventing premature eviction.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m4"
    },
    {
      "id": "tdd-diag-m5-slotted-layout",
      "title": "Slotted Page Binary Format",
      "description": "Visualizing bidirectional growth: Header at top, Cells at bottom, Free space in middle.",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m5"
    },
    {
      "id": "tdd-diag-m5-split-steps",
      "title": "B-tree Node Split Sequence",
      "description": "Creating new sibling, moving median key to parent, and re-linking.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m5"
    },
    {
      "id": "tdd-diag-m5-varint-layout",
      "title": "SQLite Varint Bit Layout",
      "description": "7-bit payload bytes with MSB continuation flags.",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m5"
    },
    {
      "id": "tdd-diag-m6-cursor-sm",
      "title": "Cursor State Machine",
      "description": "States: Uninitialized, ValidRow, EOF, InvalidatedByWrite.",
      "type": "state_machine",
      "anchor_target": "build-sqlite-m6"
    },
    {
      "id": "tdd-diag-m6-dml-flow",
      "title": "DML Write Pipeline",
      "description": "AST -> VDBE -> Record Packer -> B-tree Insert.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m6"
    },
    {
      "id": "tdd-diag-m7-index-lookup",
      "title": "Double-Lookup Sequence",
      "description": "Search Index B+tree -> Retrieve RowID -> Search Table B-tree -> Return Row.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m7"
    },
    {
      "id": "tdd-diag-m7-index-cell",
      "title": "Index Cell Layout",
      "description": "Key content followed by RowID payload.",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m7"
    },
    {
      "id": "tdd-diag-m8-planner-flow",
      "title": "Planner Decision Tree",
      "description": "Evaluating Full Scan vs Index Scan based on selectivity threshold.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m8"
    },
    {
      "id": "tdd-diag-m8-stats-model",
      "title": "Cost Calculation Data Flow",
      "description": "Inputs: PageCount, RowCount, Selectivity -> Output: Estimated I/O Units.",
      "type": "data_flow",
      "anchor_target": "build-sqlite-m8"
    },
    {
      "id": "tdd-diag-m9-write-seq",
      "title": "Atomic Commit Sequence (Rollback)",
      "description": "Journal write -> Journal fsync -> Data write -> Data fsync -> Journal Delete.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m9"
    },
    {
      "id": "tdd-diag-m9-locks",
      "title": "Database Locking State Machine",
      "description": "Transitions between Shared, Reserved, and Exclusive locks.",
      "type": "state_machine",
      "anchor_target": "build-sqlite-m9"
    },
    {
      "id": "tdd-diag-m9-recovery-flow",
      "title": "Crash Recovery Algorithm",
      "description": "Detection of hot journal and bit-for-bit restoration of main DB.",
      "type": "algorithm_steps",
      "anchor_target": "build-sqlite-m9"
    },
    {
      "id": "tdd-diag-m10-wal-layout",
      "title": "WAL File Binary Layout",
      "description": "WAL Header + repeated Frames [FrameHeader, PageData].",
      "type": "memory_layout",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m10-isolation",
      "title": "Snapshot Isolation (Read Marks)",
      "description": "Reader pinning a WAL frame index to maintain consistent view during writes.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m10"
    },
    {
      "id": "tdd-diag-m11-nlj-flow",
      "title": "Nested Loop Join Flow",
      "description": "Outer cursor scan -> Inner cursor seek/match -> Result emit.",
      "type": "sequence",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "tdd-diag-m11-agg-sm",
      "title": "Aggregate State Machine",
      "description": "States: Initialize -> Step (Process Row) -> Finalize (Output).",
      "type": "state_machine",
      "anchor_target": "build-sqlite-m11"
    },
    {
      "id": "m2-alt-reality",
      "title": "Grammar Ambiguity",
      "description": "Comparison with Postgres: Postgres uses a LALR(1) parser generated by Bison, which is more powerful but significantly more complex to modify than SQLite's recursive descent/Lemon approach. By choosing recursive descent + Pratt, we maintain high readability and easy debugging.",
      "anchor_target": "build-sqlite-m2"
    },
    {
      "id": "m3-reg-use",
      "title": "Register Usage Map",
      "description": "Visualizing how registers R1-R10 are used during a JOIN expression",
      "anchor_target": "build-sqlite-m3"
    },
    {
      "id": "m5-layout",
      "title": "Slotted Page Memory Map",
      "description": "Visualizes the Header at top, Pointers growing Down (0x0C++), and Cells growing Up from the Bottom (0xFFF--).",
      "anchor_target": "build-sqlite-m5"
    },
    {
      "id": "m5-split-visual",
      "title": "B-tree Split Sequence",
      "description": "Shows the 1->2 page transition and the parent key promotion.",
      "anchor_target": "build-sqlite-m5"
    },
    {
      "id": "m6-record-example",
      "title": "Example Record Encoding",
      "description": "Table: (1, 'ABC', NULL) -> [Size: 4][Type: 1][Type: 15][Type: 0][Data: 0x01][Data: 0x41 0x42 0x43]",
      "anchor_target": "build-sqlite-m6"
    },
    {
      "id": "m7_page_layout",
      "title": "Index Page Architecture",
      "description": "Shows the difference between Internal (RightChild) and Leaf (NextLeaf pointer at the same offset).",
      "anchor_target": "build-sqlite-m7"
    },
    {
      "id": "m7_double_lookup",
      "title": "Double Lookup Sequence",
      "description": "VM -> Index B+tree (Find RowID) -> Table B-tree (Find Data) -> ResultRow",
      "anchor_target": "build-sqlite-m7"
    }
  ],
  "external_reading": "",
  "running_criteria": [
    {
      "module_id": "build-sqlite-m10",
      "criteria": [
        "Recognizes WAL keywords and PRAGMA journal_mode=WAL",
        "Implements WALHeader and WALFrame binary serialization with cumulative checksums",
        "Manages a memory-mapped WAL Index for O(1) page searches",
        "Implements Snapshot Isolation via Read Marks in shared memory",
        "Executes Passive Checkpointing without blocking concurrent readers",
        "Detects and recovers from torn WAL writes using checksum validation"
      ]
    }
  ],
  "explained_concepts": [
    "the-pager-abstraction",
    "checkpointing-depth",
    "join-order-complexity",
    "fsm-concept",
    "pratt-parsing",
    "ast-concept",
    "vm-arch-comparison"
  ],
  "system_diagram_d2": null,
  "system_diagram_iteration": 0,
  "system_diagram_done": false,
  "project_structure_md": "",
  "project_charter_md": ""
}