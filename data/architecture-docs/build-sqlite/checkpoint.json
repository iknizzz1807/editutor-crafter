{
  "project_id": "build-sqlite",
  "meta": {
    "id": "build-sqlite",
    "name": "Build Your Own SQLite",
    "description": "Embedded SQL database with tokenizer, parser, bytecode compiler (VDBE), page-based B-tree/B+tree storage, buffer pool, query planner, and ACID transactions via rollback journal and WAL.",
    "difficulty": "expert",
    "estimated_hours": 105,
    "essence": "SQL tokenization and recursive-descent parsing producing ASTs, compiled into bytecode instructions executed by a virtual machine (VDBE), operating over a page-based storage engine with B-trees for clustered table storage and B+trees for secondary indexes, managed through a buffer pool with LRU eviction, and ACID guarantees via either rollback journal or write-ahead logging.",
    "why_important": "Building a database from scratch teaches the fundamental data structures and algorithms underlying all modern databases. These skills\u2014disk I/O management, B-tree indexing, bytecode execution, query optimization, and crash recovery\u2014 are directly applicable to backend engineering, distributed systems, and performance optimization roles.",
    "learning_outcomes": [
      "Build a lexer and recursive-descent parser converting SQL text into an AST",
      "Compile ASTs into bytecode instructions for a virtual machine (VDBE)",
      "Implement a buffer pool manager with LRU eviction and dirty page tracking",
      "Design page-based B-tree storage for tables (clustered) and B+tree for indexes",
      "Implement row storage with variable-length record encoding",
      "Build a cost-based query planner with statistics-driven cardinality estimation",
      "Implement ACID transactions with rollback journal for crash recovery",
      "Design WAL mode for improved concurrent read/write performance"
    ],
    "skills": [
      "SQL Parsing",
      "Bytecode Compilation",
      "Virtual Machine Execution",
      "Buffer Pool Management",
      "B-tree/B+tree Indexing",
      "Query Optimization",
      "Page-Based Storage",
      "Write-Ahead Logging",
      "Transaction Management",
      "Binary File Formats"
    ],
    "tags": [
      "acid",
      "btree",
      "build-from-scratch",
      "databases",
      "expert",
      "persistence",
      "query-engine",
      "sql",
      "virtual-machine"
    ],
    "architecture_doc": "architecture-docs/build-sqlite/index.md",
    "languages": {
      "recommended": [
        "C",
        "Rust",
        "Go"
      ],
      "also_possible": [
        "Java"
      ]
    },
    "resources": [
      {
        "name": "Let's Build a Simple Database",
        "url": "https://cstack.github.io/db_tutorial/",
        "type": "tutorial"
      },
      {
        "name": "SQLite Architecture",
        "url": "https://www.sqlite.org/arch.html",
        "type": "documentation"
      },
      {
        "name": "SQLite File Format",
        "url": "https://www.sqlite.org/fileformat2.html",
        "type": "documentation"
      },
      {
        "name": "SQLite VDBE Documentation",
        "url": "https://www.sqlite.org/vdbe.html",
        "type": "documentation"
      },
      {
        "name": "CMU 15-445 Database Systems",
        "url": "https://15445.courses.cs.cmu.edu",
        "type": "course"
      },
      {
        "name": "CodeCrafters SQLite Challenge",
        "url": "https://app.codecrafters.io/courses/sqlite/overview",
        "type": "tool"
      }
    ],
    "prerequisites": [
      {
        "type": "skill",
        "name": "B-tree data structure"
      },
      {
        "type": "skill",
        "name": "SQL basics"
      },
      {
        "type": "skill",
        "name": "File I/O and binary formats"
      },
      {
        "type": "skill",
        "name": "Basic compiler concepts (lexer, parser, AST)"
      }
    ],
    "milestones": [
      {
        "id": "build-sqlite-m1",
        "name": "SQL Tokenizer",
        "description": "Build a lexer that converts SQL text into a stream of typed tokens.",
        "acceptance_criteria": [
          "Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.) case-insensitively",
          "String literals enclosed in single quotes are parsed including escaped quotes ('it''s' \u2192 it's)",
          "Numeric literals including integers and floating-point values (42, 3.14, -7) are recognized as distinct token types",
          "Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon) are tokenized as distinct tokens",
          "Identifiers (table names, column names) support quoted identifiers with double quotes (\"column name\")",
          "Tokenizer reports error position (line and column) for unrecognized characters",
          "Token stream correctly tokenizes at least 20 diverse SQL statements in a test suite",
          "Tokenizer recognizes keywords like SELECT and INSERT case-insensitively",
          "String literals correctly handle escaped single quotes ('') and preserve internal content",
          "Numeric literals distinguish between integers (42) and floats (3.14)",
          "Double-quoted identifiers ('\"Table Name\"') are correctly tokenized as identifiers",
          "The tokenizer returns a list or stream of objects containing type, value, line, and column",
          "A test suite of 20+ SQL statements produces the expected token sequence"
        ],
        "pitfalls": [
          "Not handling escaped quotes in string literals ('it''s') causes premature string termination",
          "Keywords must be case-insensitive but identifiers may be case-sensitive depending on quoting\u2014handle both",
          "Unicode identifiers require careful handling; start simple with ASCII and document limitations",
          "Negative numbers may be ambiguous with subtraction operator; handle at parser level, not tokenizer"
        ],
        "concepts": [
          "Lexical analysis converts character stream to token stream",
          "Finite state machine drives character-by-character token recognition",
          "Token types classify each lexeme (keyword, identifier, literal, operator)",
          "Error reporting with source location enables useful diagnostics"
        ],
        "skills": [
          "String parsing",
          "State machine implementation",
          "Token classification",
          "Error reporting"
        ],
        "deliverables": [
          "Lexer producing token stream from SQL input string",
          "Keyword recognition for all supported SQL words",
          "String and numeric literal parsing with escape handling",
          "Operator and punctuation tokenization",
          "Error reporting with line and column position",
          "Test suite with at least 20 SQL statements"
        ],
        "estimated_hours": 4
      },
      {
        "id": "build-sqlite-m2",
        "name": "SQL Parser (AST)",
        "description": "Build a recursive-descent parser that converts the token stream into an Abstract Syntax Tree (AST).",
        "acceptance_criteria": [
          "SELECT parser produces AST with column list (including *), FROM clause, optional WHERE, ORDER BY, and LIMIT",
          "INSERT parser produces AST with target table, optional column names, and VALUES clause",
          "CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT, REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison (=,<,>) > AND > OR",
          "Parenthesized expressions override default precedence",
          "Parser produces clear error messages with token position for syntax errors",
          "Test suite covers at least 15 valid and 10 invalid SQL statements",
          "SELECT parser produces AST with column list, FROM, and optional WHERE/LIMIT",
          "INSERT parser handles target table and VALUES mapping",
          "CREATE TABLE parser extracts column names, types, and constraints (PRIMARY KEY, NOT NULL)",
          "Expression parser correctly handles NOT > AND > OR precedence",
          "Parenthesized expressions correctly override default precedence levels",
          "Parser provides error position (line/column) for syntax errors",
          "Test suite passes for 15+ valid and 10+ invalid SQL edge cases"
        ],
        "pitfalls": [
          "Left recursion in expression grammar causes infinite recursion in recursive-descent parsers\u2014use precedence climbing or Pratt parsing",
          "AND binds tighter than OR in SQL (unlike some programming languages)\u2014get this wrong and WHERE clauses evaluate incorrectly",
          "Not handling parenthesized sub-expressions breaks complex WHERE clauses",
          "NULL is a keyword, not an identifier\u2014treat it specially in expression parsing"
        ],
        "concepts": [
          "Recursive descent parsing uses mutually recursive functions for grammar rules",
          "AST represents the syntactic structure of a SQL statement as a tree",
          "Operator precedence determines evaluation order of expressions",
          "Pratt parsing or precedence climbing handles binary operators elegantly"
        ],
        "skills": [
          "Recursive function design",
          "Tree data structures",
          "Grammar rule encoding",
          "Precedence handling"
        ],
        "deliverables": [
          "SELECT statement parser producing AST",
          "INSERT statement parser producing AST",
          "CREATE TABLE parser producing AST with column definitions and constraints",
          "Expression parser with correct operator precedence",
          "Error reporting with token position for parse errors",
          "Test suite for valid and invalid SQL inputs"
        ],
        "estimated_hours": 7
      },
      {
        "id": "build-sqlite-m3",
        "name": "Bytecode Compiler (VDBE)",
        "description": "Compile parsed AST into bytecode instructions executed by a virtual machine. This is the execution engine of the database.",
        "acceptance_criteria": [
          "Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable, Rewind, Column, ResultRow, Next, Halt",
          "Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord, Insert, Halt",
          "Virtual machine executes bytecode programs step-by-step, processing one opcode per cycle",
          "VM maintains a register file (array of typed values) for intermediate computation",
          "EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable format",
          "WHERE clause compiles to conditional jump opcodes that skip non-matching rows",
          "Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under 100ms",
          "Compiler translates SELECT AST into opcodes including OpenTable, Rewind, Column, ResultRow, Next, and Halt",
          "Compiler translates INSERT AST into opcodes including OpenTable, MakeRecord, and Insert",
          "VM executes bytecode in a fetch-decode-execute loop, processing one opcode per cycle",
          "VM manages a register file of typed values for intermediate calculations",
          "WHERE clauses are correctly compiled into conditional jump opcodes (e.g., Gt, Le, Ne)",
          "The EXPLAIN command displays the human-readable opcode sequence for any valid SQL statement",
          "The VM executes a full table scan of 10,000 rows in under 100ms"
        ],
        "pitfalls": [
          "Directly interpreting AST nodes (tree-walking interpreter) is simpler but significantly slower than bytecode\u2014commit to bytecode",
          "Register allocation must handle nested expressions without clobbering intermediate values",
          "Opcodes for cursor management (open, rewind, next, close) must match the storage engine's iterator interface",
          "Missing a Halt opcode causes the VM to run past the end of the program into garbage memory"
        ],
        "concepts": [
          "Bytecode compilation translates high-level AST into low-level instruction sequence",
          "Virtual machine executes bytecode using a fetch-decode-execute loop",
          "Register-based VM uses a register file for operand storage (vs stack-based)",
          "Cursor opcodes abstract B-tree traversal for the VM"
        ],
        "skills": [
          "Bytecode generation",
          "Virtual machine implementation",
          "Register allocation",
          "Instruction set design"
        ],
        "deliverables": [
          "Bytecode instruction set with opcodes for table operations, comparisons, jumps, and output",
          "Compiler translating SELECT, INSERT, CREATE TABLE ASTs into bytecode",
          "Virtual machine executing bytecode with register file and program counter",
          "EXPLAIN command displaying bytecode for any SQL statement",
          "WHERE clause compilation to conditional jumps"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m4",
        "name": "Buffer Pool Manager",
        "description": "Implement a page cache that sits between the B-tree layer and disk, managing fixed-size pages with LRU eviction and dirty page tracking.",
        "acceptance_criteria": [
          "Buffer pool manages a configurable number of in-memory page frames (default 1000 pages)",
          "Pages are fixed-size (4096 bytes by default, configurable)",
          "FetchPage loads a page from disk into a free frame, or returns the cached frame if already resident",
          "LRU eviction selects the least recently used unpinned page for replacement when no free frames exist",
          "Dirty page tracking marks pages modified in memory; eviction writes dirty pages to disk before replacement",
          "Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations",
          "FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)",
          "Buffer pool hit rate is measurable and logged for performance tuning",
          "Buffer pool initializes with a fixed number of 4096-byte frames",
          "FetchPage returns the correct page from memory if already loaded (hit)",
          "FetchPage loads page from disk if not in memory (miss)",
          "LRU algorithm correctly identifies the least recently used page for eviction",
          "Pinned pages (count > 0) are never selected for eviction",
          "Dirty pages are written back to disk only when evicted or on FlushAll",
          "Buffer pool hit rate is tracked and accessible for performance metrics"
        ],
        "pitfalls": [
          "Evicting a pinned page causes data corruption or use-after-free\u2014enforce pin counting",
          "Not flushing dirty pages before eviction loses committed data",
          "Page ID collisions if page numbering is not globally unique across the database file",
          "Buffer pool deadlocks when B-tree operations pin too many pages simultaneously\u2014set pin limits"
        ],
        "concepts": [
          "Buffer pool caches disk pages in memory for fast access",
          "LRU (Least Recently Used) eviction approximates optimal page replacement",
          "Pin counting prevents eviction of actively-used pages",
          "Dirty page tracking enables write-back caching"
        ],
        "skills": [
          "Page cache implementation",
          "LRU eviction algorithm",
          "Pin/unpin lifecycle management",
          "Disk I/O management"
        ],
        "deliverables": [
          "Buffer pool with configurable frame count and page size",
          "FetchPage loading pages from disk or returning cached frames",
          "LRU eviction selecting least recently used unpinned page",
          "Dirty page tracking and write-back on eviction",
          "Pin/Unpin API for B-tree layer",
          "FlushAll for shutdown and checkpoint"
        ],
        "estimated_hours": 8
      },
      {
        "id": "build-sqlite-m5",
        "name": "B-tree Page Format & Table Storage",
        "description": "Implement the on-disk page structure for B-trees (tables) and B+trees (indexes), with row serialization and node splitting.",
        "acceptance_criteria": [
          "Page header contains page type (leaf/internal, table/index), cell count, free space pointer, and right-child pointer (internal only)",
          "Table B-tree leaf pages store rows keyed by rowid with variable-length record encoding",
          "Table B-tree internal pages store rowid separator keys and child page numbers",
          "Index B+tree leaf pages store (indexed column value, rowid) pairs with data only in leaves",
          "Index B+tree internal pages store only separator keys and child pointers (no row data)",
          "CREATE TABLE creates a B-tree root page and records the schema in a system catalog (sqlite_master equivalent)",
          "INSERT serializes a row and inserts into the correct B-tree leaf; node splitting creates a new page and promotes a separator key",
          "Full table scan traverses all leaf pages in rowid order, returning all rows",
          "Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer pool",
          "Page header correctly identifies Leaf vs Internal and Table vs Index types",
          "Slotted page format implements bidirectional growth (pointers vs cells)",
          "Table B-tree stores full records in leaf nodes keyed by rowid",
          "Index B+tree stores key/rowid pairs only in leaf nodes",
          "Node split algorithm correctly rebalances the tree and promotes keys to parents",
          "Varint implementation handles 1-9 byte encoding for 64-bit integers",
          "System catalog (sqlite_master) persists table root page numbers",
          "Full table scan successfully iterates through all leaf pages in order"
        ],
        "pitfalls": [
          "Conflating B-tree (data in all nodes) with B+tree (data only in leaves)\u2014tables use B-tree, indexes use B+tree in SQLite",
          "Cell overflow when a row exceeds page capacity requires overflow pages\u2014handle or document the size limit",
          "Page fragmentation after deletions wastes space\u2014track free space within pages",
          "Endianness must be consistent between write and read (SQLite uses big-endian for portability)",
          "Variable-length integer encoding (varint) must handle the full range of 64-bit integers"
        ],
        "concepts": [
          "B-tree stores key-value pairs in all nodes (used for rowid-keyed tables)",
          "B+tree stores data only in leaf nodes with linked leaf pages (used for indexes)",
          "Slotted page format uses cell pointers for variable-length records",
          "Node splitting maintains B-tree balance on insert overflow",
          "System catalog stores table and index schema metadata"
        ],
        "skills": [
          "Binary page format design",
          "B-tree/B+tree implementation",
          "Variable-length record encoding",
          "Node splitting algorithms"
        ],
        "deliverables": [
          "Page format with header, cell pointer array, and cell content area",
          "Table B-tree with leaf (row storage) and internal (separator + child pointer) pages",
          "Index B+tree with leaf (key + rowid) and internal (separator + child pointer) pages",
          "Row serialization with variable-length encoding for column values",
          "Node splitting on insert overflow with separator key promotion",
          "System catalog table storing schema metadata",
          "Full table scan via leaf page traversal"
        ],
        "estimated_hours": 12
      },
      {
        "id": "build-sqlite-m6",
        "name": "SELECT Execution & DML",
        "description": "Execute SELECT, INSERT, UPDATE, and DELETE via the bytecode VM, with row deserialization, projection, and filtering.",
        "acceptance_criteria": [
          "SELECT * FROM table returns all rows in rowid order via B-tree leaf scan",
          "SELECT col1, col2 returns only specified columns (projection)",
          "WHERE clause filters rows during scan, evaluating boolean expressions on deserialized column values",
          "INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data",
          "UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes",
          "DELETE removes rows matching WHERE; subsequent SELECT no longer returns them",
          "NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null",
          "Operations on non-existent tables return an error with the table name",
          "SELECT * returns all rows by iterating through the B-tree leaf sequence",
          "SELECT with column names correctly projects only the requested fields",
          "WHERE clause correctly filters rows using Three-Valued Logic (handling NULLs)",
          "INSERT adds a new row and updates the B-tree structure correctly",
          "UPDATE and DELETE modify/remove rows while maintaining B-tree integrity",
          "NOT NULL constraints reject invalid writes with a descriptive error",
          "Attempting to query a table not in the System Catalog returns an 'undefined table' error"
        ],
        "pitfalls": [
          "Column name case sensitivity: SQL standard is case-insensitive for identifiers; be consistent",
          "NULL handling in WHERE: NULL = NULL evaluates to NULL (falsy), not TRUE\u2014use IS NULL for null checks",
          "B-tree rebalancing after DELETE is complex; initially, mark rows as deleted and reclaim space during compaction",
          "Updating the primary key (rowid) requires delete + re-insert at the new position",
          "Memory management for large result sets\u2014stream results row-by-row, don't buffer all in memory"
        ],
        "concepts": [
          "Cursor pattern abstracts B-tree traversal for the VM",
          "Projection selects a subset of columns from each row",
          "Predicate evaluation filters rows during scan",
          "Three-valued logic (TRUE, FALSE, NULL) for SQL boolean expressions"
        ],
        "skills": [
          "B-tree cursor implementation",
          "Row deserialization",
          "Expression evaluation",
          "DML execution"
        ],
        "deliverables": [
          "Table scan operator iterating all rows via B-tree cursor",
          "Row deserialization from binary page format to typed column values",
          "Column projection selecting specified fields",
          "WHERE clause evaluation with three-valued logic",
          "INSERT, UPDATE, DELETE execution via bytecode VM",
          "NOT NULL and UNIQUE constraint enforcement during writes"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m7",
        "name": "Secondary Indexes",
        "description": "Implement secondary indexes using B+trees and integrate index lookups into query execution.",
        "acceptance_criteria": [
          "CREATE INDEX builds a B+tree index mapping (indexed column value \u2192 rowid) from existing table data",
          "Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries added/removed/updated)",
          "Index lookup retrieves rows matching an equality predicate without full table scan, verified by counting pages read",
          "Range scan on index returns rows within a value range using B+tree leaf traversal",
          "Query execution uses index scan when an indexed column appears in WHERE with equality or range predicate",
          "UNIQUE index rejects INSERT or UPDATE that would create duplicate values",
          "CREATE INDEX builds a B+tree mapping column values to rowids",
          "INSERT/UPDATE/DELETE operations maintain all associated indexes synchronously",
          "Index lookup (equality) avoids full table scan and visits significantly fewer pages",
          "Index range scan (BETWEEN or < >) traverses linked leaf pages",
          "UNIQUE index correctly rejects duplicate value insertions",
          "Bytecode VM can perform a 'Double Lookup' from index cursor to table cursor"
        ],
        "pitfalls": [
          "Forgetting to update indexes on INSERT/UPDATE/DELETE causes stale or missing index entries",
          "Index on a column with many NULL values\u2014NULLs must be handled consistently (SQLite allows multiple NULLs in UNIQUE index)",
          "Composite indexes (multi-column) require careful key comparison\u2014leftmost prefix must match for index to be useful",
          "Index maintenance overhead can make writes slower; only create indexes that benefit read patterns"
        ],
        "concepts": [
          "Secondary index maps indexed column values to primary keys (rowids)",
          "B+tree leaf traversal enables efficient range scans",
          "Index maintenance ensures indexes stay consistent with table data",
          "Covering index can satisfy a query without table lookup if all needed columns are in the index"
        ],
        "skills": [
          "B+tree index implementation",
          "Index maintenance on DML",
          "Index scan execution",
          "Unique constraint enforcement"
        ],
        "deliverables": [
          "CREATE INDEX building B+tree from existing table data",
          "Index maintenance on INSERT, UPDATE, DELETE",
          "Index equality lookup avoiding full table scan",
          "Index range scan using B+tree leaf traversal",
          "UNIQUE index constraint enforcement"
        ],
        "estimated_hours": 8
      },
      {
        "id": "build-sqlite-m8",
        "name": "Query Planner & Statistics",
        "description": "Implement a cost-based query planner that chooses between table scan and index scan based on collected statistics.",
        "acceptance_criteria": [
          "ANALYZE command collects statistics: row count per table, distinct value count per indexed column",
          "Cost model estimates pages read for full table scan (total_pages) and index scan (estimated_rows / rows_per_page)",
          "Planner selects index scan when estimated selectivity (matching_rows / total_rows) is below a threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too low",
          "EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated row count",
          "For multi-table queries (JOIN), planner estimates join cardinality and selects join order to minimize intermediate result size"
        ],
        "pitfalls": [
          "Without statistics (before ANALYZE), the planner has no data for cost estimation\u2014use default assumptions (e.g., assume 1M rows)",
          "Stale statistics after many inserts/deletes cause the planner to choose suboptimal plans\u2014recommend periodic ANALYZE",
          "Cardinality estimation errors compound through joins\u2014a 10x error in one table becomes 100x after a two-table join",
          "Plan search space explodes exponentially with number of tables in JOIN\u2014limit to dynamic programming for \u226410 tables"
        ],
        "concepts": [
          "Cost-based optimization compares estimated cost of alternative plans",
          "Cardinality estimation predicts the number of rows each operator produces",
          "Selectivity is the fraction of rows matching a predicate",
          "Statistics collection (ANALYZE) provides data for cost estimation",
          "Dynamic programming-based join ordering for multi-table queries"
        ],
        "skills": [
          "Statistics collection",
          "Cost model design",
          "Cardinality estimation",
          "Plan enumeration"
        ],
        "deliverables": [
          "ANALYZE command collecting table and index statistics",
          "Cost model estimating I/O for table scan vs index scan",
          "Plan selection choosing cheapest access path per table",
          "EXPLAIN command displaying chosen plan with cost estimates",
          "Join order optimization for multi-table queries"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m9",
        "name": "Transactions (Rollback Journal)",
        "description": "Implement ACID transactions using a rollback journal for crash recovery.",
        "acceptance_criteria": [
          "BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or ROLLBACK",
          "COMMIT makes all changes permanent by flushing dirty pages and removing the rollback journal",
          "ROLLBACK undoes all changes by restoring original pages from the rollback journal",
          "Rollback journal records original page contents BEFORE modification (for undo on crash)",
          "Changes are not visible to other connections until COMMIT (basic read isolation)",
          "Crash recovery on startup detects an existing rollback journal and automatically rolls back the incomplete transaction",
          "Journal file is fsync'd before modified pages are written to the database file (write ordering guarantee)",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the engine state",
          "A .db-journal file is created and contains original page data before any write to the main .db file",
          "The journal file is physically flushed to disk (fsync) before the main database is modified",
          "A manual ROLLBACK restores the state from the journal and clears the journal file",
          "Startup logic detects a 'Hot Journal' and automatically restores the database to a consistent state",
          "Writes are not visible to other database connections until the COMMIT is complete"
        ],
        "pitfalls": [
          "Writing modified pages to database before journal is fsync'd causes unrecoverable corruption on crash",
          "Partial page writes (torn pages) on crash can corrupt the database\u2014journal must contain complete original pages",
          "Lock ordering between multiple connections must be consistent to prevent deadlocks",
          "Long-running transactions holding locks block all other writers\u2014document the locking behavior"
        ],
        "concepts": [
          {
            "ACID": "Atomicity (rollback journal), Consistency (constraints), Isolation (locking), Durability (fsync)"
          },
          "Rollback journal records undo information (original page images) before modification",
          {
            "Write ordering": "journal fsync \u2192 database write \u2192 journal delete"
          },
          {
            "Crash recovery": "hot journal detected \u2192 restore original pages \u2192 delete journal"
          }
        ],
        "skills": [
          "Rollback journal implementation",
          "Write ordering enforcement",
          "Crash recovery logic",
          "Lock management"
        ],
        "deliverables": [
          "BEGIN/COMMIT/ROLLBACK command implementation",
          "Rollback journal recording original page contents before modification",
          "Write ordering ensuring journal is durable before database pages are modified",
          "Crash recovery detecting hot journal and restoring database to pre-transaction state",
          "ACID guarantee verification test suite"
        ],
        "estimated_hours": 10
      },
      {
        "id": "build-sqlite-m10",
        "name": "WAL Mode",
        "description": "Implement Write-Ahead Logging as an alternative to rollback journal, enabling concurrent readers during writes.",
        "acceptance_criteria": [
          "WAL mode appends modified pages to a separate WAL file instead of modifying the main database file",
          "Writers append to WAL; readers check WAL for the most recent version of a page before reading from the main database",
          "Multiple readers can execute queries concurrently while a single writer appends to the WAL",
          "Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database file",
          "WAL checkpoint is required to prevent unbounded WAL growth\u2014auto-checkpoint triggers after configurable page count (default 1000)",
          "Readers see a consistent snapshot: a reader that starts before a commit does not see that commit's changes (snapshot isolation for reads)",
          "WAL file corruption is detected via page checksums",
          "Writers append to a separate WAL file instead of modifying the main .db file",
          "Readers search the WAL for the most recent page version before falling back to the main file",
          "Writers and multiple readers can operate simultaneously without blocking",
          "Checkpointing copies WAL pages to the main database and truncates the WAL",
          "Automatic checkpoint triggers after 1000 pages (configurable)",
          "Readers use a consistent snapshot based on the WAL state at their start time",
          "Checksums are used to detect and reject corrupted WAL frames"
        ],
        "pitfalls": [
          "WAL grows unbounded without checkpointing\u2014auto-checkpoint is not optional, it's required",
          "Readers pinning old WAL frames prevent checkpoint from truncating\u2014long-running reads block WAL cleanup",
          "WAL and rollback journal are mutually exclusive modes\u2014switching requires careful state management",
          "Checkpoint must not run while readers are using WAL frames that would be overwritten",
          "WAL file corruption must be detected (checksums) to prevent propagating bad data to the main database"
        ],
        "concepts": [
          "WAL appends redo information (new page images) to a log file",
          "Readers use WAL index (wal-index) to find most recent page version",
          "Checkpoint merges WAL changes back into the main database file",
          {
            "Snapshot isolation": "each reader sees the database as of its start time"
          },
          {
            "WAL vs rollback journal": "WAL enables concurrent readers, rollback journal does not"
          }
        ],
        "skills": [
          "Write-ahead log implementation",
          "Snapshot isolation for readers",
          "Checkpoint algorithm",
          "Concurrent read/write coordination"
        ],
        "deliverables": [
          "WAL file format appending modified pages with checksums",
          "WAL reader looking up most recent page version before main database",
          "Checkpoint process copying WAL pages into main database",
          "Auto-checkpoint triggered by WAL page count threshold",
          "Concurrent reader support during active write transactions",
          "WAL mode toggle (PRAGMA journal_mode=WAL)"
        ],
        "estimated_hours": 12
      },
      {
        "id": "build-sqlite-m11",
        "name": "Aggregate Functions & JOIN",
        "description": "Implement aggregate functions (COUNT, SUM, AVG, MIN, MAX), GROUP BY, and basic JOIN execution.",
        "acceptance_criteria": [
          "COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values",
          "SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY groups rows by specified columns before applying aggregate functions",
          "HAVING filters groups after aggregation",
          "INNER JOIN combines rows from two tables matching a join condition",
          "Nested loop join is implemented as the baseline join algorithm",
          "JOIN with WHERE clause filters correctly after join",
          "COUNT(*) accurately counts rows including NULLs",
          "COUNT(col) ignores NULL values in the target column",
          "AVG returns a REAL/float even if input column is INTEGER",
          "GROUP BY correctly partitions aggregate states into buckets",
          "HAVING filters out aggregated groups based on result values",
          "INNER JOIN correctly combines rows from two tables using a nested loop",
          "JOIN with WHERE correctly filters rows before or during the join process"
        ],
        "pitfalls": [
          "AVG must handle integer division correctly (return REAL, not INTEGER)",
          "NULL handling in aggregates: COUNT(*) counts NULLs, COUNT(col) does not; SUM/AVG ignore NULLs",
          "GROUP BY without aggregate function is valid SQL but confusing\u2014handle correctly",
          "Nested loop join is O(n*m)\u2014acceptable for small tables but document the limitation"
        ],
        "concepts": [
          "Aggregate functions accumulate values across row groups",
          "GROUP BY partitions rows into groups for aggregation",
          "Nested loop join iterates all combinations of rows from two tables",
          "HAVING filters groups after aggregation (vs WHERE which filters before)"
        ],
        "skills": [
          "Aggregate computation",
          "Group-by execution",
          "Join algorithms",
          "NULL handling in aggregates"
        ],
        "deliverables": [
          "COUNT, SUM, AVG, MIN, MAX aggregate functions",
          "GROUP BY execution with hash-based or sort-based grouping",
          "HAVING clause filtering groups after aggregation",
          "Nested loop INNER JOIN execution",
          "Test suite covering aggregates with NULLs, empty tables, and multi-table joins"
        ],
        "estimated_hours": 14
      }
    ],
    "domain": "data-storage"
  },
  "blueprint": {
    "title": "Build Your Own SQLite",
    "overview": "This project constructs a complete embedded SQL database engine from first principles, following the actual architecture of SQLite\u2014the world's most deployed database. You'll build every layer: a tokenizer that converts SQL text into typed tokens, a recursive-descent parser producing ASTs, a bytecode compiler targeting a virtual machine (VDBE), and a page-based storage engine using B-trees for tables and B+trees for indexes. The storage layer includes a buffer pool with LRU eviction, variable-length record encoding, and ACID transactions via both rollback journal and write-ahead logging. This is not a toy\u2014by completion, you'll have a database capable of executing real SQL queries with crash recovery.",
    "design_philosophy": "SQLite's architecture is a masterclass in systems design: the bytecode VM decouples SQL semantics from storage mechanics, enabling independent optimization of query compilation and execution. The B-tree/B+tree distinction teaches why different access patterns demand different structures. ACID implementation reveals the fundamental tension between durability (fsync on every write) and performance (batching via WAL). This project forces you to confront every hidden assumption databases make: that disk writes are atomic (they're not\u2014torn pages), that memory is unlimited (it's not\u2014buffer pool eviction), that queries are instant (they're not\u2014query planning matters).",
    "is_build_your_own": true,
    "prerequisites": {
      "assumed_known": [
        "B-tree data structure (insertion, deletion, balancing)",
        "SQL basics (SELECT, INSERT, CREATE TABLE syntax)",
        "File I/O and binary formats (reading/writing bytes, endianness)",
        "Basic compiler concepts (what a lexer/parser does, what an AST is)"
      ],
      "must_teach_first": [
        {
          "concept": "B+tree vs B-tree distinction",
          "depth": "intermediate",
          "when": "Milestone 5"
        },
        {
          "concept": "Virtual machine execution model",
          "depth": "intermediate",
          "when": "Milestone 3"
        },
        {
          "concept": "ACID properties and crash semantics",
          "depth": "intermediate",
          "when": "Milestone 9"
        },
        {
          "concept": "Write-ahead logging vs shadow paging",
          "depth": "intermediate",
          "when": "Milestone 10"
        }
      ]
    },
    "milestones": [
      {
        "id": "build-sqlite-m1",
        "title": "SQL Tokenizer",
        "anchor_id": "anchor-tokenizer",
        "summary": "Build a finite-state-machine lexer converting SQL text into typed tokens with precise error location reporting.",
        "misconception": "Tokenization is just string splitting on whitespace\u2014split SELECT * FROM users and you're done.",
        "reveal": "Tokens have context-dependent boundaries: 'it''s' is ONE string token (the doubled quote is an escape), != and <> are the SAME operator token, and 3.14e-5 requires looking ahead 6 characters past the decimal. A tokenizer is a state machine, not a splitter.",
        "cascade": [
          "Compiler frontends \u2014 every language processor (SQL, JSON, programming languages) starts here; token boundaries define language syntax",
          "UTF-8 handling (cross-domain) \u2014 SQL identifiers can be Unicode; your tokenizer must decide: bytes or code points? This connects to text encoding in network protocols",
          "Error recovery \u2014 a production tokenizer must continue past errors to report multiple issues; this connects to IDE diagnostics and linter design",
          "Keyword tables \u2014 case-insensitive keyword matching (SELECT, select, Select) requires normalization; this pattern appears in DNS lookup and case-insensitive filesystems",
          "Streaming vs buffering \u2014 tokenizers can yield tokens lazily; this connects to iterator patterns and generator functions in all languages"
        ],
        "yaml_acceptance_criteria": [
          "Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.) case-insensitively",
          "String literals enclosed in single quotes are parsed including escaped quotes ('it''s' \u2192 it's)",
          "Numeric literals including integers and floating-point values (42, 3.14, -7) are recognized as distinct token types",
          "Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon) are tokenized as distinct tokens",
          "Identifiers (table names, column names) support quoted identifiers with double quotes (\"column name\")",
          "Tokenizer reports error position (line and column) for unrecognized characters",
          "Token stream correctly tokenizes at least 20 diverse SQL statements in a test suite",
          "The tokenizer returns a list or stream of objects containing type, value, line, and column",
          "A test suite of 20+ SQL statements produces the expected token sequence"
        ]
      },
      {
        "id": "build-sqlite-m2",
        "title": "SQL Parser (AST)",
        "anchor_id": "anchor-parser",
        "summary": "Build a recursive-descent parser converting token streams into Abstract Syntax Trees with proper operator precedence.",
        "misconception": "Parsing is just matching token patterns\u2014SELECT followed by columns followed by FROM followed by table. Check the boxes and build the tree.",
        "reveal": "Expression precedence is a trap: WHERE a OR b AND c means WHERE a OR (b AND c) because AND binds tighter. But WHERE NOT a = b means WHERE NOT (a = b), not WHERE (NOT a) = b. Recursive descent with precedence climbing handles this elegantly; naive pattern matching produces wrong query semantics.",
        "cascade": [
          "Language semantics \u2014 operator precedence exists in every language; understanding SQL's NOT > comparison > AND > OR unlocks C's precedence puzzles and mathematical notation",
          "Left recursion elimination \u2014 recursive descent can't handle left-recursive grammars (E \u2192 E + E); the transformation techniques apply to all parser generators",
          "AST design patterns \u2014 visitor pattern for tree traversal, builder pattern for construction; these appear in compilers, linting tools, and code generators",
          "Error recovery (cross-domain) \u2014 when parsing fails, how do you report a meaningful error and continue? Same problem in configuration file parsers and protocol decoders",
          "Grammar complexity classes \u2014 LL(1), LL(k), LR; understanding why SQL fits recursive descent (LL) but C++ doesn't connects to formal language theory"
        ],
        "yaml_acceptance_criteria": [
          "SELECT parser produces AST with column list (including *), FROM clause, optional WHERE, ORDER BY, and LIMIT",
          "INSERT parser produces AST with target table, optional column names, and VALUES clause",
          "CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT, REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)",
          "Expression parser correctly handles operator precedence: NOT > comparison (=,<,>) > AND > OR",
          "Parenthesized expressions override default precedence",
          "Parser produces clear error messages with token position for syntax errors",
          "Test suite covers at least 15 valid and 10 invalid SQL statements",
          "Parenthesized expressions correctly override default precedence levels",
          "Parser provides error position (line/column) for syntax errors",
          "Test suite passes for 15+ valid and 10+ invalid SQL edge cases"
        ]
      },
      {
        "id": "build-sqlite-m3",
        "title": "Bytecode Compiler (VDBE)",
        "anchor_id": "anchor-vdbe",
        "summary": "Compile ASTs into bytecode instructions for a register-based virtual machine with cursor operations for B-tree traversal.",
        "misconception": "Bytecode is overkill\u2014just walk the AST and execute operations directly. Python does it, Ruby does it, why not SQL?",
        "reveal": "Tree-walking interpretation makes optimization impossible. With bytecode, you can reorder instructions, eliminate redundant operations, and cache computation across statements. SQLite's VDBE is why the same query gets faster the second time\u2014bytecode can be saved and reused. The VM also abstracts storage: cursor opcodes (OpenTable, Next, Column) work identically whether the data is in memory, on disk, or in a B-tree.",
        "cascade": [
          "JIT compilation (cross-domain) \u2014 bytecode is the foundation; LuaJIT, V8, and the JVM all start with bytecode before JITting to native. Understanding VDBE is step one to understanding HotSpot",
          "Register-based vs stack-based VMs \u2014 Lua 5.0 switched from stack to register for 25% speedup; this design decision affects Python (stack), WebAssembly (stack), and Lua (register)",
          "Query plan caching \u2014 compiled bytecode can be cached; this pattern appears in prepared statements, regex compilation, and template engines",
          "Instruction set design \u2014 opcode count vs opcode complexity; SQLite has ~180 opcodes, WebAssembly has ~60. Trade-offs in decoder complexity vs instruction expressiveness",
          "Virtualization fundamentals \u2014 a VM implementing a fetch-decode-execute loop is the core of QEMU, the Java VM, and emulators; VDBE is the simplest real example"
        ],
        "yaml_acceptance_criteria": [
          "Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable, Rewind, Column, ResultRow, Next, Halt",
          "Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord, Insert, Halt",
          "Virtual machine executes bytecode programs step-by-step, processing one opcode per cycle",
          "VM maintains a register file (array of typed values) for intermediate computation",
          "EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable format",
          "WHERE clause compiles to conditional jump opcodes that skip non-matching rows",
          "Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under 100ms",
          "VM executes bytecode in a fetch-decode-execute loop, processing one opcode per cycle",
          "VM manages a register file of typed values for intermediate calculations",
          "WHERE clauses are correctly compiled into conditional jump opcodes (e.g., Gt, Le, Ne)",
          "The EXPLAIN command displays the human-readable opcode sequence for any valid SQL statement",
          "The VM executes a full table scan of 10,000 rows in under 100ms"
        ]
      },
      {
        "id": "build-sqlite-m4",
        "title": "Buffer Pool Manager",
        "anchor_id": "anchor-buffer-pool",
        "summary": "Implement a page cache with LRU eviction, pin counting, and dirty page tracking between the B-tree layer and disk.",
        "misconception": "The OS page cache handles this\u2014just read() and write() files and let the kernel do caching.",
        "reveal": "The kernel's page cache is LRU, but it doesn't know which pages are 'pinned' by active B-tree operations. Evicting a page mid-split corrupts the tree. Databases need their own buffer pool to: (1) pin pages during operations, (2) control exactly when dirty pages flush (crash recovery), and (3) know the difference between a clean page (can drop) and a dirty page (must write). This is why PostgreSQL, MySQL, and SQLite all have their own buffer pools.",
        "cascade": [
          "Operating system caches (cross-domain) \u2014 the buffer pool IS a cache, implementing the same algorithms as CPU caches (LRU, write-back) and web caches (hit rate, eviction). Learn one, understand all",
          "Double buffering problem \u2014 OS cache + database cache = redundant memory; direct I/O (O_DIRECT) bypasses the OS cache, a technique used by high-performance databases and message queues",
          "Pin counting \u2014 identical to reference counting in garbage collection; pinned pages are 'live objects' that can't be collected",
          "Write ordering \u2014 dirty pages must flush in a specific order for crash recovery; this constraint appears in filesystem journaling and distributed log systems",
          "Memory pressure \u2014 when the buffer pool is full, the database slows down; same problem as GC pressure in JVM heaps, L1 cache misses in CPU pipelines"
        ],
        "yaml_acceptance_criteria": [
          "Buffer pool manages a configurable number of in-memory page frames (default 1000 pages)",
          "Pages are fixed-size (4096 bytes by default, configurable)",
          "FetchPage loads a page from disk into a free frame, or returns the cached frame if already resident",
          "LRU eviction selects the least recently used unpinned page for replacement when no free frames exist",
          "Dirty page tracking marks pages modified in memory; eviction writes dirty pages to disk before replacement",
          "Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations",
          "FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)",
          "Buffer pool hit rate is measurable and logged for performance tuning",
          "Buffer pool initializes with a fixed number of 4096-byte frames",
          "FetchPage returns the correct page from memory if already loaded (hit)",
          "FetchPage loads page from disk if not in memory (miss)",
          "LRU algorithm correctly identifies the least recently used page for eviction",
          "Pinned pages (count > 0) are never selected for eviction",
          "Dirty pages are written back to disk only when evicted or on FlushAll",
          "Buffer pool hit rate is tracked and accessible for performance metrics"
        ]
      },
      {
        "id": "build-sqlite-m5",
        "title": "B-tree Page Format & Table Storage",
        "anchor_id": "anchor-btree-storage",
        "summary": "Design the on-disk page structure with slotted page format, variable-length records, and B-tree/B+tree node splitting.",
        "misconception": "B-trees and B+trees are basically the same thing\u2014both store sorted keys with O(log n) operations.",
        "reveal": "B-trees store data in ALL nodes (root, internal, leaves). B+trees store data ONLY in leaves, with internal nodes containing only separators. This changes EVERYTHING: B+tree leaves are linked for efficient range scans, B+tree internal nodes are smaller (more keys per page = shallower tree), but B-trees are faster for point queries (data at every level). SQLite uses B-trees for tables (clustered on rowid) and B+trees for indexes (range scan optimization).",
        "cascade": [
          "Clustered vs non-clustered indexes \u2014 understanding why table B-trees cluster data with keys while index B+trees separate them explains SQL Server, MySQL, and PostgreSQL storage differences",
          "Slotted page format (cross-domain) \u2014 the same bidirectional growth pattern (pointers from top, data from bottom) appears in stack frames, malloc implementations, and rope data structures",
          "Varint encoding \u2014 SQLite's 1-9 byte variable-length integers are identical to Protocol Buffers varints; this technique appears in every compact binary format",
          "Page fragmentation \u2014 deletions leave gaps; understanding free space management connects to filesystem allocation and memory allocator design",
          "Endianness \u2014 SQLite uses big-endian for cross-platform compatibility; this connects to network byte order and why x86 (little-endian) was controversial"
        ],
        "yaml_acceptance_criteria": [
          "Page header contains page type (leaf/internal, table/index), cell count, free space pointer, and right-child pointer (internal only)",
          "Table B-tree leaf pages store rows keyed by rowid with variable-length record encoding",
          "Table B-tree internal pages store rowid separator keys and child page numbers",
          "Index B+tree leaf pages store (indexed column value, rowid) pairs with data only in leaves",
          "Index B+tree internal pages store only separator keys and child pointers (no row data)",
          "CREATE TABLE creates a B-tree root page and records the schema in a system catalog (sqlite_master equivalent)",
          "INSERT serializes a row and inserts into the correct B-tree leaf; node splitting creates a new page and promotes a separator key",
          "Full table scan traverses all leaf pages in rowid order, returning all rows",
          "Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer pool",
          "Page header correctly identifies Leaf vs Internal and Table vs Index types",
          "Slotted page format implements bidirectional growth (pointers vs cells)",
          "Table B-tree stores full records in leaf nodes keyed by rowid",
          "Index B+tree stores key/rowid pairs only in leaf nodes",
          "Node split algorithm correctly rebalances the tree and promotes keys to parents",
          "Varint implementation handles 1-9 byte encoding for 64-bit integers",
          "System catalog (sqlite_master) persists table root page numbers",
          "Full table scan successfully iterates through all leaf pages in order"
        ]
      },
      {
        "id": "build-sqlite-m6",
        "title": "SELECT Execution & DML",
        "anchor_id": "anchor-dml-execution",
        "summary": "Execute SELECT, INSERT, UPDATE, and DELETE via bytecode VM with row deserialization, projection, and WHERE filtering.",
        "misconception": "NULL is just zero or empty string\u2014special case it in a few places and move on.",
        "reveal": "NULL is a third truth value. NULL = NULL evaluates to NULL (not TRUE). NULL AND TRUE is NULL. NULL OR FALSE is NULL. Every comparison, every join, every aggregate must handle NULL explicitly. This is three-valued logic, and it infects EVERYTHING: WHERE clauses, JOIN conditions, UNIQUE constraints, even index comparisons. SQLite's NULL handling alone has 50+ edge cases in the test suite.",
        "cascade": [
          "Three-valued logic (cross-domain) \u2014 NULL is SQL's Maybe/Option type; understanding SQL NULL teaches you why Rust has Option<T> and why null pointer exceptions exist",
          "Projection pushdown \u2014 selecting only needed columns isn't just about network bandwidth; deserializing fewer columns is faster. Same optimization in columnar stores (Parquet, Arrow)",
          "Cursor abstraction \u2014 the cursor pattern (open, next, close) for iterating B-tree leaves appears in file iteration, API pagination, and generator functions",
          "Update-in-place vs copy-on-write \u2014 UPDATE modifies rows in B-tree pages; understanding when to overwrite vs create new versions connects to MVCC and persistent data structures",
          "Memory management \u2014 streaming results row-by-row instead of buffering entire result sets; same constraint in video streaming, large file processing, and event streams"
        ],
        "yaml_acceptance_criteria": [
          "SELECT * FROM table returns all rows in rowid order via B-tree leaf scan",
          "SELECT col1, col2 returns only specified columns (projection)",
          "WHERE clause filters rows during scan, evaluating boolean expressions on deserialized column values",
          "INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data",
          "UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes",
          "DELETE removes rows matching WHERE; subsequent SELECT no longer returns them",
          "NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null",
          "Operations on non-existent tables return an error with the table name",
          "SELECT * returns all rows by iterating through the B-tree leaf sequence",
          "SELECT with column names correctly projects only the requested fields",
          "WHERE clause correctly filters rows using Three-Valued Logic (handling NULLs)",
          "INSERT adds a new row and updates the B-tree structure correctly",
          "UPDATE and DELETE modify/remove rows while maintaining B-tree integrity",
          "NOT NULL constraints reject invalid writes with a descriptive error",
          "Attempting to query a table not in the System Catalog returns an 'undefined table' error"
        ]
      },
      {
        "id": "build-sqlite-m7",
        "title": "Secondary Indexes",
        "anchor_id": "anchor-indexes",
        "summary": "Implement B+tree secondary indexes with automatic maintenance on DML and index lookup execution.",
        "misconception": "An index is just another copy of the data sorted differently\u2014point it at the table and you're done.",
        "reveal": "An index maps (column value \u2192 rowid), not (column value \u2192 row data). To get the actual row, you do a 'double lookup': index finds the rowid, then B-tree finds the row by rowid. This is why covering indexes (all needed columns in the index) are faster\u2014no double lookup. It's also why indexes slow down writes: every INSERT/UPDATE/DELETE must update ALL indexes on the table.",
        "cascade": [
          "Index selection \u2014 the query planner's hardest job; understanding why an index helps SELECT but hurts INSERT explains database performance tuning",
          "Covering indexes (cross-domain) \u2014 same idea as caching computed results; the index 'covers' the query just like a memoization cache covers a function call",
          "Index maintenance patterns \u2014 the write-amplification from index updates connects to LSM-tree compaction costs and SSD write amplification",
          "Unique constraints \u2014 UNIQUE index is both an optimization AND a constraint; dual purpose appears in unique hash tables and deduplication systems",
          "Composite index ordering \u2014 leftmost prefix rule (index on (a,b,c) helps WHERE a=? and WHERE a=? AND b=? but not WHERE b=?) connects to sort ordering and lexicographic comparison"
        ],
        "yaml_acceptance_criteria": [
          "CREATE INDEX builds a B+tree index mapping (indexed column value \u2192 rowid) from existing table data",
          "Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries added/removed/updated)",
          "Index lookup retrieves rows matching an equality predicate without full table scan, verified by counting pages read",
          "Range scan on index returns rows within a value range using B+tree leaf traversal",
          "Query execution uses index scan when an indexed column appears in WHERE with equality or range predicate",
          "UNIQUE index rejects INSERT or UPDATE that would create duplicate values",
          "CREATE INDEX builds a B+tree mapping column values to rowids",
          "INSERT/UPDATE/DELETE operations maintain all associated indexes synchronously",
          "Index lookup (equality) avoids full table scan and visits significantly fewer pages",
          "Index range scan (BETWEEN or < >) traverses linked leaf pages",
          "UNIQUE index correctly rejects duplicate value insertions",
          "Bytecode VM can perform a 'Double Lookup' from index cursor to table cursor"
        ]
      },
      {
        "id": "build-sqlite-m8",
        "title": "Query Planner & Statistics",
        "anchor_id": "anchor-query-planner",
        "summary": "Implement cost-based optimization choosing between table scan and index scan using collected statistics.",
        "misconception": "The query planner just picks the right index\u2014scan the WHERE clause, find a matching index, use it.",
        "reveal": "An index is SLOWER than a table scan when selecting more than ~20% of rows. Why? Random I/O. A table scan reads pages sequentially (1 page = many rows). An index lookup reads one page per row, scattered across the file. The planner must estimate: how many rows match? If > 20%, scan. If < 20%, index. Without statistics (ANALYZE), it guesses\u2014and wrong guesses mean 100x slower queries.",
        "cascade": [
          "Cost models (cross-domain) \u2014 every system has one: TCP congestion control, route planning (Google Maps), even CPU branch prediction. Understanding cost = selectivity \u00d7 access_method_cost is universally applicable",
          "Cardinality estimation \u2014 guessing how many rows match a predicate; same problem in stream processing (window sizing) and capacity planning",
          "Statistics staleness \u2014 after many inserts, statistics lie; same problem in adaptive systems (JIT compilation, auto-tuning databases)",
          "Join ordering \u2014 the hardest optimization; dynamic programming finds the optimal join order for N tables in O(3^N). This connects to traveling salesman and NP-hard optimization",
          "Plan caching \u2014 caching the chosen plan; but if statistics change, cached plans become suboptimal. Same tension as any cache invalidation problem"
        ],
        "yaml_acceptance_criteria": [
          "ANALYZE command collects statistics: row count per table, distinct value count per indexed column",
          "Cost model estimates pages read for full table scan (total_pages) and index scan (estimated_rows / rows_per_page)",
          "Planner selects index scan when estimated selectivity (matching_rows / total_rows) is below a threshold (e.g., 20%)",
          "Planner falls back to table scan when no suitable index exists or selectivity is too low",
          "EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated row count",
          "For multi-table queries (JOIN), planner estimates join cardinality and selects join order to minimize intermediate result size"
        ]
      },
      {
        "id": "build-sqlite-m9",
        "title": "Transactions (Rollback Journal)",
        "anchor_id": "anchor-transactions",
        "summary": "Implement ACID transactions using rollback journal for crash recovery with correct write ordering.",
        "misconception": "ACID is about locking\u2014prevent concurrent access and you're atomic and isolated.",
        "reveal": "ACID is about surviving crashes. The hard problem: power fails mid-transaction, mid-page-write. A 'torn page' has half old data, half new data. The rollback journal solves this by writing original pages to a separate file BEFORE modifying the database. On recovery: if journal exists, copy it back. The ordering is critical: journal must be fsync'd BEFORE database is modified. Get this wrong, and crashes corrupt data.",
        "cascade": [
          "Crash recovery (cross-domain) \u2014 every persistent system needs it: filesystems (journaling), message queues (durable queues), distributed systems (WAL replication). The journal pattern is universal",
          "fsync semantics \u2014 fsync is NOT instant; it can block for milliseconds. Understanding when to fsync (before, not after) connects to filesystem design and SSD internals",
          "Atomic writes \u2014 a 4KB page write is NOT atomic; SSDs write in 16KB chunks, HDDs can tear at 512 bytes. This connects to hardware realities every storage system must handle",
          "Lock ordering \u2014 multiple transactions need multiple locks; inconsistent ordering causes deadlocks. Same problem in OS mutexes and distributed locks",
          "Isolation levels \u2014 SQLite's basic isolation is one-at-a-time; understanding why it's not serializable connects to MVCC, snapshot isolation, and the ANSI isolation levels"
        ],
        "yaml_acceptance_criteria": [
          "BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or ROLLBACK",
          "COMMIT makes all changes permanent by flushing dirty pages and removing the rollback journal",
          "ROLLBACK undoes all changes by restoring original pages from the rollback journal",
          "Rollback journal records original page contents BEFORE modification (for undo on crash)",
          "Changes are not visible to other connections until COMMIT (basic read isolation)",
          "Crash recovery on startup detects an existing rollback journal and automatically rolls back the incomplete transaction",
          "Journal file is fsync'd before modified pages are written to the database file (write ordering guarantee)",
          "BEGIN/COMMIT/ROLLBACK commands correctly toggle the engine state",
          "A .db-journal file is created and contains original page data before any write to the main .db file",
          "The journal file is physically flushed to disk (fsync) before the main database is modified",
          "A manual ROLLBACK restores the state from the journal and clears the journal file",
          "Startup logic detects a 'Hot Journal' and automatically restores the database to a consistent state",
          "Writes are not visible to other database connections until the COMMIT is complete"
        ]
      },
      {
        "id": "build-sqlite-m10",
        "title": "WAL Mode",
        "anchor_id": "anchor-wal",
        "summary": "Implement Write-Ahead Logging enabling concurrent readers during writes with checkpoint-based durability.",
        "misconception": "WAL is just a faster rollback journal\u2014same idea, better implementation.",
        "reveal": "Rollback journal and WAL are OPPOSITES. Rollback journal: write OLD data to journal, then NEW data to database (undo logging). WAL: write NEW data to log, don't touch database until checkpoint (redo logging). This inversion changes everything: WAL enables concurrent readers (they read the old database while the writer appends to log), but requires periodic checkpointing (or WAL grows forever). Rollback journal blocks all readers during writes.",
        "cascade": [
          "Undo vs redo logging (cross-domain) \u2014 ARIES recovery algorithm, database recovery textbooks, distributed consensus (Paxos uses redo logs). Understanding this distinction is fundamental to all persistent systems",
          "Snapshot isolation \u2014 each reader sees the database as of a point in time; same concept in MVCC (PostgreSQL), copy-on-write (ZFS), and version control (Git commits)",
          "Checkpointing (cross-domain) \u2014 flushing accumulated changes; same pattern in game checkpoints, VM snapshots, and application state saves",
          "Concurrent read/write \u2014 the holy grail of databases; WAL achieves it without complex locking. Connects to read-copy-update (RCU) in Linux kernel and optimistic concurrency control",
          "Checksums for corruption detection \u2014 every WAL frame has a checksum; same technique in TCP, RAID, and filesystems. Understanding why checksums catch corruption but don't prevent it"
        ],
        "yaml_acceptance_criteria": [
          "WAL mode appends modified pages to a separate WAL file instead of modifying the main database file",
          "Writers append to WAL; readers check WAL for the most recent version of a page before reading from the main database",
          "Multiple readers can execute queries concurrently while a single writer appends to the WAL",
          "Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database file",
          "WAL checkpoint is required to prevent unbounded WAL growth\u2014auto-checkpoint triggers after configurable page count (default 1000)",
          "Readers see a consistent snapshot: a reader that starts before a commit does not see that commit's changes (snapshot isolation for reads)",
          "WAL file corruption is detected via page checksums",
          "Writers append to a separate WAL file instead of modifying the main .db file",
          "Readers search the WAL for the most recent page version before falling back to the main file",
          "Writers and multiple readers can operate simultaneously without blocking",
          "Checkpointing copies WAL pages to the main database and truncates the WAL",
          "Automatic checkpoint triggers after 1000 pages (configurable)",
          "Readers use a consistent snapshot based on the WAL state at their start time",
          "Checksums are used to detect and reject corrupted WAL frames"
        ]
      },
      {
        "id": "build-sqlite-m11",
        "title": "Aggregate Functions & JOIN",
        "anchor_id": "anchor-aggregates-joins",
        "summary": "Implement aggregate functions with GROUP BY/HAVING and nested loop join for multi-table queries.",
        "misconception": "JOINs just combine rows\u2014iterate table A, iterate table B, filter on condition. Simple nested loops.",
        "reveal": "JOIN order determines performance: joining a 1000-row table to a 1-row table can be 1000x faster if you iterate the 1-row table first. The query planner's hardest job is join ordering. And aggregates have a hidden complexity: GROUP BY must either sort (O(n log n)) or hash (O(n) but memory-heavy). COUNT(*), COUNT(col), SUM, AVG each have different NULL handling\u2014get it wrong and financial reports are incorrect.",
        "cascade": [
          "Join algorithms (cross-domain) \u2014 nested loop O(M\u00d7N), hash join O(M+N), sort-merge join O(M log M + N log N). Same algorithms appear in data pipelines, ETL systems, and distributed joins (Spark, Flink)",
          "Hash aggregation \u2014 GROUP BY is fundamentally a hash table; understanding hash collisions, resize costs, and memory limits connects to hash maps in every language",
          "NULL propagation in aggregates \u2014 COUNT(*) counts NULLs, COUNT(col) doesn't; SUM ignores NULL unless all NULL. These semantics appear in statistics libraries and data analysis tools",
          "HAVING vs WHERE \u2014 HAVING filters after aggregation, WHERE before. Understanding execution order connects to query optimization and pipeline stages",
          "Memory management for large groups \u2014 GROUP BY on high-cardinality columns can exhaust memory; streaming aggregation and external sorting are the solutions, same as external merge sort for large files"
        ],
        "yaml_acceptance_criteria": [
          "COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values",
          "SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries",
          "GROUP BY groups rows by specified columns before applying aggregate functions",
          "HAVING filters groups after aggregation",
          "INNER JOIN combines rows from two tables matching a join condition",
          "Nested loop join is implemented as the baseline join algorithm",
          "JOIN with WHERE clause filters correctly after join",
          "COUNT(*) accurately counts rows including NULLs",
          "COUNT(col) ignores NULL values in the target column",
          "AVG returns a REAL/float even if input column is INTEGER",
          "GROUP BY correctly partitions aggregate states into buckets",
          "HAVING filters out aggregated groups based on result values",
          "INNER JOIN correctly combines rows from two tables using a nested loop",
          "JOIN with WHERE correctly filters rows before or during the join process"
        ]
      }
    ],
    "diagrams": [
      {
        "id": "diag-satellite-map",
        "title": "SQLite Architecture: The Complete Map",
        "description": "Satellite view showing all components: SQL text enters Tokenizer \u2192 Parser \u2192 Compiler \u2192 VDBE \u2192 B-tree \u2192 Buffer Pool \u2192 Disk. Arrows show data flow. Each component labeled with its anchor ID. System catalog and WAL/journal shown as separate modules. This is the 'home base' diagram that every other diagram references.",
        "anchor_target": "anchor-sqlite-system",
        "level": "satellite"
      },
      {
        "id": "diag-token-state-machine",
        "title": "Tokenizer State Machine",
        "description": "State diagram showing character-by-character token recognition: START state branches to IN_KEYWORD, IN_STRING (with escape handling), IN_NUMBER (with decimal/exp branches), IN_OPERATOR. Shows error state transitions and token emission.",
        "anchor_target": "anchor-tokenizer",
        "level": "street"
      },
      {
        "id": "diag-token-stream-example",
        "title": "Token Stream: SELECT * FROM users WHERE id = 42",
        "description": "Trace example showing the SQL text with token boundaries marked, each token displayed as (type, value, line, column). Demonstrates keyword recognition, operator tokenization, and numeric literal handling.",
        "anchor_target": "anchor-tokenizer",
        "level": "microscopic"
      },
      {
        "id": "diag-ast-structure",
        "title": "AST Structure for SELECT Statement",
        "description": "Tree diagram showing AST nodes: SelectStmt as root with children: ColumnList (including ColumnRef and StarExpr), FromClause (TableRef), WhereClause (BinaryExpr with left=ColumnRef, op='=', right=Literal). Shows recursive structure.",
        "anchor_target": "anchor-parser",
        "level": "street"
      },
      {
        "id": "diag-precedence-climbing",
        "title": "Precedence Climbing: a OR b AND c = d",
        "description": "Before/after diagram showing naive left-to-right parsing vs correct precedence parsing. Shows tree structure transformation as precedence rules apply: NOT > comparison > AND > OR.",
        "anchor_target": "anchor-parser",
        "level": "microscopic"
      },
      {
        "id": "diag-bytecode-instruction-set",
        "title": "VDBE Instruction Set",
        "description": "Table showing key opcodes: OpenTable, Rewind, Column, ResultRow, Next, Halt for SELECT; OpenTable, MakeRecord, Insert, Halt for INSERT; comparison opcodes (Eq, Ne, Lt, Gt, Le, Ge) and jump opcodes. Each with operand format and semantics.",
        "anchor_target": "anchor-vdbe",
        "level": "street"
      },
      {
        "id": "diag-bytecode-select-trace",
        "title": "Bytecode Execution Trace: SELECT * FROM t WHERE x > 5",
        "description": "Step-by-step trace showing VM state: program counter, current opcode, registers, cursor state. Shows loop: Rewind \u2192 Column \u2192 compare \u2192 jump or ResultRow \u2192 Next \u2192 loop or Halt.",
        "anchor_target": "anchor-vdbe",
        "level": "microscopic"
      },
      {
        "id": "diag-vm-architecture",
        "title": "VDBE Virtual Machine Architecture",
        "description": "Architecture diagram showing: instruction array, program counter, register file (typed values), cursor array (pointing to B-tree positions), and result output. Fetch-decode-execute cycle highlighted.",
        "anchor_target": "anchor-vdbe",
        "level": "street"
      },
      {
        "id": "diag-buffer-pool-structure",
        "title": "Buffer Pool Internal Structure",
        "description": "Diagram showing: frame array (fixed-size page buffers), page table (page_id \u2192 frame_index), LRU list (doubly-linked), pin counts per frame, dirty bits per frame. Arrows showing FetchPage flow.",
        "anchor_target": "anchor-buffer-pool",
        "level": "street"
      },
      {
        "id": "diag-lru-eviction-flow",
        "title": "LRU Eviction: Finding a Victim Frame",
        "description": "Flowchart showing eviction decision: is head pinned? \u2192 yes, move to next \u2192 no, is it dirty? \u2192 yes, write to disk \u2192 evict. Shows LRU list manipulation.",
        "anchor_target": "anchor-buffer-pool",
        "level": "microscopic"
      },
      {
        "id": "diag-page-layout",
        "title": "Slotted Page Format",
        "description": "Binary layout diagram: page header (type, cell_count, free_space_ptr, right_child), cell pointer array growing down from header, free space, cell content area growing up from bottom. Shows bidirectional growth pattern.",
        "anchor_target": "anchor-btree-storage",
        "level": "microscopic"
      },
      {
        "id": "diag-btree-vs-bplustree",
        "title": "B-tree vs B+tree: Where's the Data?",
        "description": "Side-by-side comparison: B-tree with data in ALL nodes (root, internal, leaves) vs B+tree with data ONLY in leaves, internal nodes containing only keys. Shows leaf linking in B+tree for range scans.",
        "anchor_target": "anchor-btree-storage",
        "level": "street"
      },
      {
        "id": "diag-node-split",
        "title": "B-tree Node Split Sequence",
        "description": "Step-by-step diagram: (1) full leaf with insert pending, (2) split into two leaves, (3) separator key promoted to parent, (4) updated parent. Shows cell redistribution and pointer updates.",
        "anchor_target": "anchor-btree-storage",
        "level": "microscopic"
      },
      {
        "id": "diag-varint-encoding",
        "title": "Variable-Length Integer Encoding",
        "description": "Binary diagram showing varint format: high bit indicates continuation, 1-9 bytes encoding values from 0 to 2^64-1. Examples: 0x00 (single byte zero), 0x81 0x00 (two bytes for 128), 9-byte format for large values.",
        "anchor_target": "anchor-btree-storage",
        "level": "microscopic"
      },
      {
        "id": "diag-row-serialization",
        "title": "Row Serialization: Record Format",
        "description": "Binary layout showing: header length varint, column type codes (indicating type and size), then column values. Example row: (42, 'hello', 3.14) with byte-by-byte encoding.",
        "anchor_target": "anchor-btree-storage",
        "level": "microscopic"
      },
      {
        "id": "diag-three-valued-logic",
        "title": "Three-Valued Logic Truth Tables",
        "description": "Truth tables for AND, OR, NOT with TRUE, FALSE, NULL. Highlights: NULL AND TRUE = NULL, NULL OR FALSE = NULL, NOT NULL = NULL, NULL = NULL = NULL (not TRUE!).",
        "anchor_target": "anchor-dml-execution",
        "level": "street"
      },
      {
        "id": "diag-cursor-iteration",
        "title": "B-tree Cursor: Full Table Scan",
        "description": "Animation-ready diagram showing cursor movement: OpenTable (position at first leaf) \u2192 Column (read current) \u2192 Next (move to next entry, traverse to next leaf if needed) \u2192 Halt (when cursor passes last entry).",
        "anchor_target": "anchor-dml-execution",
        "level": "microscopic"
      },
      {
        "id": "diag-index-double-lookup",
        "title": "Index Double Lookup vs Table Scan",
        "description": "Two-path comparison: (1) Table scan reads pages sequentially, (2) Index lookup finds rowid in index B+tree, then fetches page from table B-tree. Shows why index is faster for selective queries (few lookups) but slower for non-selective (many random page accesses).",
        "anchor_target": "anchor-indexes",
        "level": "street"
      },
      {
        "id": "diag-index-maintenance",
        "title": "Index Maintenance on INSERT",
        "description": "Data flow diagram: INSERT into table B-tree triggers \u2192 find position in each index B+tree \u2192 insert (column_value, rowid) pair into each index. Shows write amplification: one logical insert = N physical inserts (1 table + N indexes).",
        "anchor_target": "anchor-indexes",
        "level": "microscopic"
      },
      {
        "id": "diag-cost-model-comparison",
        "title": "Cost Model: Table Scan vs Index Scan",
        "description": "Side-by-side cost calculation: Table scan cost = total_pages (sequential reads). Index scan cost = index_depth + (selectivity \u00d7 total_rows) \u00d7 table_page_fetch_cost. Shows crossover point at ~20% selectivity.",
        "anchor_target": "anchor-query-planner",
        "level": "street"
      },
      {
        "id": "diag-selectivity-estimation",
        "title": "Selectivity Estimation from Statistics",
        "description": "Example showing ANALYZE output: table has 10000 rows, column 'status' has 5 distinct values. Estimated selectivity for status='active' = 1/5 = 20%. Histogram for skewed distribution.",
        "anchor_target": "anchor-query-planner",
        "level": "microscopic"
      },
      {
        "id": "diag-rollback-journal-flow",
        "title": "Rollback Journal: Write Ordering",
        "description": "Sequence diagram showing transaction flow: BEGIN \u2192 write original page to journal \u2192 fsync journal \u2192 write modified page to database \u2192 COMMIT \u2192 delete journal. Critical ordering emphasized with fsync barriers.",
        "anchor_target": "anchor-transactions",
        "level": "street"
      },
      {
        "id": "diag-crash-recovery",
        "title": "Crash Recovery: Hot Journal Detection",
        "description": "Flowchart for startup recovery: check for .db-journal file \u2192 exists? \u2192 yes \u2192 copy journal pages back to database \u2192 delete journal \u2192 continue startup. No journal \u2192 normal startup.",
        "anchor_target": "anchor-transactions",
        "level": "street"
      },
      {
        "id": "diag-torn-page-problem",
        "title": "Torn Page: Why Journals Exist",
        "description": "Diagram showing 4KB page write interrupted: half old data, half new data. Without journal, this page is corrupt. With journal, original page is restored. Shows why atomic page writes don't exist.",
        "anchor_target": "anchor-transactions",
        "level": "microscopic"
      },
      {
        "id": "diag-wal-architecture",
        "title": "WAL Mode Architecture",
        "description": "Architecture diagram: Writer appends to WAL file (sequential), Readers check WAL-index for recent page versions before falling back to main database. Checkpoint process shown copying WAL to database.",
        "anchor_target": "anchor-wal",
        "level": "satellite"
      },
      {
        "id": "diag-wal-vs-rollback",
        "title": "WAL vs Rollback Journal: Undo vs Redo",
        "description": "Side-by-side comparison: Rollback journal stores OLD pages (undo on crash), WAL stores NEW pages (redo on recovery). Shows fundamental inversion: undo logging vs redo logging.",
        "anchor_target": "anchor-wal",
        "level": "street"
      },
      {
        "id": "diag-wal-frame-format",
        "title": "WAL Frame Format",
        "description": "Binary layout: frame header (page_number, commit_size, salt1, salt2, checksum1, checksum2) followed by page data (4096 bytes). Shows how checksums detect corruption.",
        "anchor_target": "anchor-wal",
        "level": "microscopic"
      },
      {
        "id": "diag-snapshot-isolation",
        "title": "Snapshot Isolation: Concurrent Readers",
        "description": "Timeline diagram: Writer commits at T1, T2, T3. Reader starting at T2 sees only commits through T1. Shows how each reader gets a consistent view despite ongoing writes.",
        "anchor_target": "anchor-wal",
        "level": "street"
      },
      {
        "id": "diag-checkpoint-process",
        "title": "Checkpoint: Merging WAL to Database",
        "description": "Step-by-step: (1) pause new writes, (2) copy committed WAL frames to database pages, (3) fsync database, (4) truncate WAL, (5) resume writes. Shows why checkpoints block briefly.",
        "anchor_target": "anchor-wal",
        "level": "microscopic"
      },
      {
        "id": "diag-nested-loop-join",
        "title": "Nested Loop Join Algorithm",
        "description": "Pseudocode and diagram: for each row in outer table, for each row in inner table, if join_condition matches, output combined row. Shows O(M\u00d7N) complexity and why join order matters.",
        "anchor_target": "anchor-aggregates-joins",
        "level": "street"
      },
      {
        "id": "diag-hash-aggregation",
        "title": "GROUP BY: Hash Aggregation",
        "description": "Data flow: input rows \u2192 hash on GROUP BY columns \u2192 hash buckets \u2192 aggregate state per bucket (count, sum, etc.) \u2192 output groups. Shows how hash table enables O(n) grouping.",
        "anchor_target": "anchor-aggregates-joins",
        "level": "microscopic"
      },
      {
        "id": "diag-aggregate-null-handling",
        "title": "Aggregate NULL Handling",
        "description": "Table examples showing: COUNT(*) counts all rows including NULLs, COUNT(col) ignores NULLs, SUM(col) ignores NULLs (returns NULL if all NULL), AVG(col) ignores NULLs. Same input table, different results.",
        "anchor_target": "anchor-aggregates-joins",
        "level": "microscopic"
      },
      {
        "id": "diag-having-vs-where",
        "title": "HAVING vs WHERE: Execution Order",
        "description": "Pipeline diagram: FROM \u2192 WHERE (filter rows) \u2192 GROUP BY (partition) \u2192 aggregate functions \u2192 HAVING (filter groups) \u2192 SELECT (project). Shows why HAVING can reference aggregates.",
        "anchor_target": "anchor-aggregates-joins",
        "level": "street"
      },
      {
        "id": "diag-system-catalog",
        "title": "System Catalog (sqlite_master)",
        "description": "Table structure: (type, name, tbl_name, rootpage, sql). Examples: table entry with root page number, index entry pointing to table, schema stored as SQL text. Shows bootstrapping problem.",
        "anchor_target": "anchor-btree-storage",
        "level": "street"
      },
      {
        "id": "diag-full-query-lifecycle",
        "title": "Full Query Lifecycle: SELECT * FROM users WHERE id = 42",
        "description": "End-to-end trace: SQL text \u2192 tokens \u2192 AST \u2192 bytecode \u2192 VM execution \u2192 B-tree cursor \u2192 buffer pool fetch \u2192 page deserialization \u2192 row filtering \u2192 result output. Connects all components.",
        "anchor_target": "anchor-sqlite-system",
        "level": "satellite"
      }
    ]
  },
  "accumulated_md": "# Build Your Own SQLite\n\nThis project constructs a complete embedded SQL database engine from first principles, following the actual architecture of SQLite\u2014the world's most deployed database. You'll build every layer: a tokenizer that converts SQL text into typed tokens, a recursive-descent parser producing ASTs, a bytecode compiler targeting a virtual machine (VDBE), and a page-based storage engine using B-trees for tables and B+trees for indexes. The storage layer includes a buffer pool with LRU eviction, variable-length record encoding, and ACID transactions via both rollback journal and write-ahead logging. This is not a toy\u2014by completion, you'll have a database capable of executing real SQL queries with crash recovery.\n\n",
  "current_ms_index": 0,
  "diagrams_to_generate": [
    {
      "id": "diag-satellite-map",
      "title": "SQLite Architecture: The Complete Map",
      "description": "Satellite view showing all components: SQL text enters Tokenizer \u2192 Parser \u2192 Compiler \u2192 VDBE \u2192 B-tree \u2192 Buffer Pool \u2192 Disk. Arrows show data flow. Each component labeled with its anchor ID. System catalog and WAL/journal shown as separate modules. This is the 'home base' diagram that every other diagram references.",
      "anchor_target": "anchor-sqlite-system",
      "level": "satellite"
    },
    {
      "id": "diag-token-state-machine",
      "title": "Tokenizer State Machine",
      "description": "State diagram showing character-by-character token recognition: START state branches to IN_KEYWORD, IN_STRING (with escape handling), IN_NUMBER (with decimal/exp branches), IN_OPERATOR. Shows error state transitions and token emission.",
      "anchor_target": "anchor-tokenizer",
      "level": "street"
    },
    {
      "id": "diag-token-stream-example",
      "title": "Token Stream: SELECT * FROM users WHERE id = 42",
      "description": "Trace example showing the SQL text with token boundaries marked, each token displayed as (type, value, line, column). Demonstrates keyword recognition, operator tokenization, and numeric literal handling.",
      "anchor_target": "anchor-tokenizer",
      "level": "microscopic"
    },
    {
      "id": "diag-ast-structure",
      "title": "AST Structure for SELECT Statement",
      "description": "Tree diagram showing AST nodes: SelectStmt as root with children: ColumnList (including ColumnRef and StarExpr), FromClause (TableRef), WhereClause (BinaryExpr with left=ColumnRef, op='=', right=Literal). Shows recursive structure.",
      "anchor_target": "anchor-parser",
      "level": "street"
    },
    {
      "id": "diag-precedence-climbing",
      "title": "Precedence Climbing: a OR b AND c = d",
      "description": "Before/after diagram showing naive left-to-right parsing vs correct precedence parsing. Shows tree structure transformation as precedence rules apply: NOT > comparison > AND > OR.",
      "anchor_target": "anchor-parser",
      "level": "microscopic"
    },
    {
      "id": "diag-bytecode-instruction-set",
      "title": "VDBE Instruction Set",
      "description": "Table showing key opcodes: OpenTable, Rewind, Column, ResultRow, Next, Halt for SELECT; OpenTable, MakeRecord, Insert, Halt for INSERT; comparison opcodes (Eq, Ne, Lt, Gt, Le, Ge) and jump opcodes. Each with operand format and semantics.",
      "anchor_target": "anchor-vdbe",
      "level": "street"
    },
    {
      "id": "diag-bytecode-select-trace",
      "title": "Bytecode Execution Trace: SELECT * FROM t WHERE x > 5",
      "description": "Step-by-step trace showing VM state: program counter, current opcode, registers, cursor state. Shows loop: Rewind \u2192 Column \u2192 compare \u2192 jump or ResultRow \u2192 Next \u2192 loop or Halt.",
      "anchor_target": "anchor-vdbe",
      "level": "microscopic"
    },
    {
      "id": "diag-vm-architecture",
      "title": "VDBE Virtual Machine Architecture",
      "description": "Architecture diagram showing: instruction array, program counter, register file (typed values), cursor array (pointing to B-tree positions), and result output. Fetch-decode-execute cycle highlighted.",
      "anchor_target": "anchor-vdbe",
      "level": "street"
    },
    {
      "id": "diag-buffer-pool-structure",
      "title": "Buffer Pool Internal Structure",
      "description": "Diagram showing: frame array (fixed-size page buffers), page table (page_id \u2192 frame_index), LRU list (doubly-linked), pin counts per frame, dirty bits per frame. Arrows showing FetchPage flow.",
      "anchor_target": "anchor-buffer-pool",
      "level": "street"
    },
    {
      "id": "diag-lru-eviction-flow",
      "title": "LRU Eviction: Finding a Victim Frame",
      "description": "Flowchart showing eviction decision: is head pinned? \u2192 yes, move to next \u2192 no, is it dirty? \u2192 yes, write to disk \u2192 evict. Shows LRU list manipulation.",
      "anchor_target": "anchor-buffer-pool",
      "level": "microscopic"
    },
    {
      "id": "diag-page-layout",
      "title": "Slotted Page Format",
      "description": "Binary layout diagram: page header (type, cell_count, free_space_ptr, right_child), cell pointer array growing down from header, free space, cell content area growing up from bottom. Shows bidirectional growth pattern.",
      "anchor_target": "anchor-btree-storage",
      "level": "microscopic"
    },
    {
      "id": "diag-btree-vs-bplustree",
      "title": "B-tree vs B+tree: Where's the Data?",
      "description": "Side-by-side comparison: B-tree with data in ALL nodes (root, internal, leaves) vs B+tree with data ONLY in leaves, internal nodes containing only keys. Shows leaf linking in B+tree for range scans.",
      "anchor_target": "anchor-btree-storage",
      "level": "street"
    },
    {
      "id": "diag-node-split",
      "title": "B-tree Node Split Sequence",
      "description": "Step-by-step diagram: (1) full leaf with insert pending, (2) split into two leaves, (3) separator key promoted to parent, (4) updated parent. Shows cell redistribution and pointer updates.",
      "anchor_target": "anchor-btree-storage",
      "level": "microscopic"
    },
    {
      "id": "diag-varint-encoding",
      "title": "Variable-Length Integer Encoding",
      "description": "Binary diagram showing varint format: high bit indicates continuation, 1-9 bytes encoding values from 0 to 2^64-1. Examples: 0x00 (single byte zero), 0x81 0x00 (two bytes for 128), 9-byte format for large values.",
      "anchor_target": "anchor-btree-storage",
      "level": "microscopic"
    },
    {
      "id": "diag-row-serialization",
      "title": "Row Serialization: Record Format",
      "description": "Binary layout showing: header length varint, column type codes (indicating type and size), then column values. Example row: (42, 'hello', 3.14) with byte-by-byte encoding.",
      "anchor_target": "anchor-btree-storage",
      "level": "microscopic"
    },
    {
      "id": "diag-three-valued-logic",
      "title": "Three-Valued Logic Truth Tables",
      "description": "Truth tables for AND, OR, NOT with TRUE, FALSE, NULL. Highlights: NULL AND TRUE = NULL, NULL OR FALSE = NULL, NOT NULL = NULL, NULL = NULL = NULL (not TRUE!).",
      "anchor_target": "anchor-dml-execution",
      "level": "street"
    },
    {
      "id": "diag-cursor-iteration",
      "title": "B-tree Cursor: Full Table Scan",
      "description": "Animation-ready diagram showing cursor movement: OpenTable (position at first leaf) \u2192 Column (read current) \u2192 Next (move to next entry, traverse to next leaf if needed) \u2192 Halt (when cursor passes last entry).",
      "anchor_target": "anchor-dml-execution",
      "level": "microscopic"
    },
    {
      "id": "diag-index-double-lookup",
      "title": "Index Double Lookup vs Table Scan",
      "description": "Two-path comparison: (1) Table scan reads pages sequentially, (2) Index lookup finds rowid in index B+tree, then fetches page from table B-tree. Shows why index is faster for selective queries (few lookups) but slower for non-selective (many random page accesses).",
      "anchor_target": "anchor-indexes",
      "level": "street"
    },
    {
      "id": "diag-index-maintenance",
      "title": "Index Maintenance on INSERT",
      "description": "Data flow diagram: INSERT into table B-tree triggers \u2192 find position in each index B+tree \u2192 insert (column_value, rowid) pair into each index. Shows write amplification: one logical insert = N physical inserts (1 table + N indexes).",
      "anchor_target": "anchor-indexes",
      "level": "microscopic"
    },
    {
      "id": "diag-cost-model-comparison",
      "title": "Cost Model: Table Scan vs Index Scan",
      "description": "Side-by-side cost calculation: Table scan cost = total_pages (sequential reads). Index scan cost = index_depth + (selectivity \u00d7 total_rows) \u00d7 table_page_fetch_cost. Shows crossover point at ~20% selectivity.",
      "anchor_target": "anchor-query-planner",
      "level": "street"
    },
    {
      "id": "diag-selectivity-estimation",
      "title": "Selectivity Estimation from Statistics",
      "description": "Example showing ANALYZE output: table has 10000 rows, column 'status' has 5 distinct values. Estimated selectivity for status='active' = 1/5 = 20%. Histogram for skewed distribution.",
      "anchor_target": "anchor-query-planner",
      "level": "microscopic"
    },
    {
      "id": "diag-rollback-journal-flow",
      "title": "Rollback Journal: Write Ordering",
      "description": "Sequence diagram showing transaction flow: BEGIN \u2192 write original page to journal \u2192 fsync journal \u2192 write modified page to database \u2192 COMMIT \u2192 delete journal. Critical ordering emphasized with fsync barriers.",
      "anchor_target": "anchor-transactions",
      "level": "street"
    },
    {
      "id": "diag-crash-recovery",
      "title": "Crash Recovery: Hot Journal Detection",
      "description": "Flowchart for startup recovery: check for .db-journal file \u2192 exists? \u2192 yes \u2192 copy journal pages back to database \u2192 delete journal \u2192 continue startup. No journal \u2192 normal startup.",
      "anchor_target": "anchor-transactions",
      "level": "street"
    },
    {
      "id": "diag-torn-page-problem",
      "title": "Torn Page: Why Journals Exist",
      "description": "Diagram showing 4KB page write interrupted: half old data, half new data. Without journal, this page is corrupt. With journal, original page is restored. Shows why atomic page writes don't exist.",
      "anchor_target": "anchor-transactions",
      "level": "microscopic"
    },
    {
      "id": "diag-wal-architecture",
      "title": "WAL Mode Architecture",
      "description": "Architecture diagram: Writer appends to WAL file (sequential), Readers check WAL-index for recent page versions before falling back to main database. Checkpoint process shown copying WAL to database.",
      "anchor_target": "anchor-wal",
      "level": "satellite"
    },
    {
      "id": "diag-wal-vs-rollback",
      "title": "WAL vs Rollback Journal: Undo vs Redo",
      "description": "Side-by-side comparison: Rollback journal stores OLD pages (undo on crash), WAL stores NEW pages (redo on recovery). Shows fundamental inversion: undo logging vs redo logging.",
      "anchor_target": "anchor-wal",
      "level": "street"
    },
    {
      "id": "diag-wal-frame-format",
      "title": "WAL Frame Format",
      "description": "Binary layout: frame header (page_number, commit_size, salt1, salt2, checksum1, checksum2) followed by page data (4096 bytes). Shows how checksums detect corruption.",
      "anchor_target": "anchor-wal",
      "level": "microscopic"
    },
    {
      "id": "diag-snapshot-isolation",
      "title": "Snapshot Isolation: Concurrent Readers",
      "description": "Timeline diagram: Writer commits at T1, T2, T3. Reader starting at T2 sees only commits through T1. Shows how each reader gets a consistent view despite ongoing writes.",
      "anchor_target": "anchor-wal",
      "level": "street"
    },
    {
      "id": "diag-checkpoint-process",
      "title": "Checkpoint: Merging WAL to Database",
      "description": "Step-by-step: (1) pause new writes, (2) copy committed WAL frames to database pages, (3) fsync database, (4) truncate WAL, (5) resume writes. Shows why checkpoints block briefly.",
      "anchor_target": "anchor-wal",
      "level": "microscopic"
    },
    {
      "id": "diag-nested-loop-join",
      "title": "Nested Loop Join Algorithm",
      "description": "Pseudocode and diagram: for each row in outer table, for each row in inner table, if join_condition matches, output combined row. Shows O(M\u00d7N) complexity and why join order matters.",
      "anchor_target": "anchor-aggregates-joins",
      "level": "street"
    },
    {
      "id": "diag-hash-aggregation",
      "title": "GROUP BY: Hash Aggregation",
      "description": "Data flow: input rows \u2192 hash on GROUP BY columns \u2192 hash buckets \u2192 aggregate state per bucket (count, sum, etc.) \u2192 output groups. Shows how hash table enables O(n) grouping.",
      "anchor_target": "anchor-aggregates-joins",
      "level": "microscopic"
    },
    {
      "id": "diag-aggregate-null-handling",
      "title": "Aggregate NULL Handling",
      "description": "Table examples showing: COUNT(*) counts all rows including NULLs, COUNT(col) ignores NULLs, SUM(col) ignores NULLs (returns NULL if all NULL), AVG(col) ignores NULLs. Same input table, different results.",
      "anchor_target": "anchor-aggregates-joins",
      "level": "microscopic"
    },
    {
      "id": "diag-having-vs-where",
      "title": "HAVING vs WHERE: Execution Order",
      "description": "Pipeline diagram: FROM \u2192 WHERE (filter rows) \u2192 GROUP BY (partition) \u2192 aggregate functions \u2192 HAVING (filter groups) \u2192 SELECT (project). Shows why HAVING can reference aggregates.",
      "anchor_target": "anchor-aggregates-joins",
      "level": "street"
    },
    {
      "id": "diag-system-catalog",
      "title": "System Catalog (sqlite_master)",
      "description": "Table structure: (type, name, tbl_name, rootpage, sql). Examples: table entry with root page number, index entry pointing to table, schema stored as SQL text. Shows bootstrapping problem.",
      "anchor_target": "anchor-btree-storage",
      "level": "street"
    },
    {
      "id": "diag-full-query-lifecycle",
      "title": "Full Query Lifecycle: SELECT * FROM users WHERE id = 42",
      "description": "End-to-end trace: SQL text \u2192 tokens \u2192 AST \u2192 bytecode \u2192 VM execution \u2192 B-tree cursor \u2192 buffer pool fetch \u2192 page deserialization \u2192 row filtering \u2192 result output. Connects all components.",
      "anchor_target": "anchor-sqlite-system",
      "level": "satellite"
    }
  ],
  "diagram_attempt": 0,
  "current_diagram_code": null,
  "current_diagram_meta": null,
  "last_error": null,
  "status": "writing",
  "phase": "atlas",
  "knowledge_map": [],
  "advanced_contexts": [],
  "tdd_blueprint": {},
  "tdd_accumulated_md": "",
  "tdd_current_mod_index": 0,
  "tdd_diagrams_to_generate": [],
  "external_reading": "",
  "running_criteria": [],
  "explained_concepts": [],
  "blueprint_warnings": []
}