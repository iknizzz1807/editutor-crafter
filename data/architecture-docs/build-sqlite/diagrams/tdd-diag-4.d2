vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # Token Stream Data Flow
| {near: top-center}

Lexer: Tokenizer {
  style.fill: "#E8D5B7"
  style.stroke: "#8B7355"
  
  input_stream: "Input Stream\n(char*)"
  current_char: "Current Char\n(int)"
  line: "Line Number\n(int)"
  column: "Column\n(int)"
  
  next_token: "next_token(): Token"
  peek_token: "peek_token(): Token"
  skip_whitespace: "skip_whitespace(): void"
  read_string: "read_string(): Token"
  read_number: "read_number(): Token"
  read_identifier: "read_identifier(): Token"
}

Token: Token Struct {
  shape: class
  style.fill: "#B5D8EB"
  style.stroke: "#4A90A4"
  
  type: "TokenType type"
  value: "char* value"
  line: "int line"
  column: "int column"
  length: "int length"
}

TokenQueue: Token Queue {
  style.fill: "#C8E6C9"
  style.stroke: "#388E3C"
  
  tokens: "Token* tokens[]"
  head: "int head"
  tail: "int tail"
  count: "int count"
  capacity: "int capacity"
  
  enqueue: "enqueue(Token): void"
  dequeue: "dequeue(): Token"
  peek: "peek(): Token"
  is_empty: "is_empty(): bool"
}

Parser: Parser {
  style.fill: "#E1BEE7"
  style.stroke: "#7B1FA2"
  
  token_queue: "TokenQueue* queue"
  current_token: "Token* current"
  peek_token: "Token* peek"
  error_handler: "ErrorHandler* err"
  
  parse: "parse(): ASTNode*"
  advance: "advance(): void"
  expect: "expect(TokenType): bool"
  match: "match(TokenType): bool"
}

Source: "Source Code\n(File/String)" {
  style.fill: "#FFF9C4"
  style.stroke: "#F9A825"
}

AST: "AST\n(Abstract Syntax Tree)" {
  style.fill: "#FFCCBC"
  style.stroke: "#E64A19"
}

Source -> Lexer: "read chars"
Lexer -> Token: "produce"
Token -> TokenQueue: "enqueue"
TokenQueue -> Parser: "dequeue"
Parser -> AST: "build"

flow_legend: |md
  **Data Flow Legend**
  - Yellow: Input source
  - Brown: Lexical analysis
  - Blue: Token structure
  - Green: Buffer/Queue
  - Purple: Parser
  - Orange: Output AST
| {near: bottom-center}