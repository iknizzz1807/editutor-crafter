{
  "title": "Project Apollo: Test Framework Design Document",
  "overview": "Project Apollo is a Python-based test framework built for learning, demonstrating how modern frameworks like pytest operate under the hood. The key architectural challenge it solves is providing a flexible, extensible system for test discovery, execution, and reporting while maintaining test isolation and supporting complex features like fixtures and assertions. This document guides the implementation through four progressive milestones, from basic discovery to a full-featured CLI.",
  "sections": [
    {
      "id": "context",
      "title": "1. Context and Problem Statement",
      "summary": "Explains why test frameworks are essential tools in software development, the core problems they solve, and compares existing approaches to highlight the design space.",
      "subsections": [
        {
          "id": "mental-model",
          "title": "Mental Model: The Test Conductor",
          "summary": "Frames the test framework as an orchestra conductor, responsible for finding musicians (tests), providing sheet music (fixtures), ensuring they play in the right order and isolation, and reporting the final performance."
        },
        {
          "id": "problem-space",
          "title": "The Core Problem: Automated Verification",
          "summary": "Describes the manual, error-prone process of testing code without a framework and defines the key requirements for automation: discovery, execution, assertion, and reporting."
        },
        {
          "id": "existing-approaches",
          "title": "Existing Approaches & Comparison",
          "summary": "Compares built-in frameworks (unittest), modern frameworks (pytest), and minimal libraries (assert) using a table of pros and cons."
        }
      ]
    },
    {
      "id": "goals",
      "title": "2. Goals and Non-Goals",
      "summary": "Defines the scope of the project by stating what the framework must achieve (Goals) and what is explicitly out of scope (Non-Goals).",
      "subsections": [
        {
          "id": "goals-list",
          "title": "Goals",
          "summary": "Lists the required features aligned with the four milestones: discovery, rich assertions, fixtures, and a CLI with reporting."
        },
        {
          "id": "non-goals-list",
          "title": "Non-Goals",
          "summary": "Explicitly states features not required for this educational project, such as distributed test execution, a plugin ecosystem, or a custom test language."
        }
      ]
    },
    {
      "id": "high-level-architecture",
      "title": "3. High-Level Architecture",
      "summary": "Presents the bird's-eye view of the system's components, their responsibilities, and how data flows between them. Includes a component diagram and recommended file structure.",
      "subsections": [
        {
          "id": "component-overview",
          "title": "Component Overview & Responsibilities",
          "summary": "Introduces the five core components: CLI Parser, Discoverer, Runner, Assertion Engine, and Reporter, detailing what each one does."
        },
        {
          "id": "file-structure",
          "title": "Recommended File/Module Structure",
          "summary": "Provides a skeleton Python project layout to organize the codebase, separating public API, core components, and internal utilities."
        }
      ]
    },
    {
      "id": "data-model",
      "title": "4. Data Model",
      "summary": "Describes the key data structures and types that flow through the system, such as TestCase, TestResult, and Fixture, using definition tables.",
      "subsections": [
        {
          "id": "core-types",
          "title": "Core Types",
          "summary": "Defines the primary objects like TestCase (represents a single test), TestResult (outcome of a test run), and Fixture (a resource for tests)."
        },
        {
          "id": "supporting-structures",
          "title": "Supporting Structures",
          "summary": "Defines auxiliary structures like TestSuite (a collection of tests), Configuration (CLI arguments), and various scopes for fixtures."
        }
      ]
    },
    {
      "id": "component-discovery",
      "title": "5. Component Design: Discovery & Execution (Milestone 1)",
      "summary": "Detailed design for the components that find and run tests, including test isolation and parallel execution.",
      "subsections": [
        {
          "id": "discoverer-component",
          "title": "The Discoverer",
          "summary": "Design of the component that scans modules and finds test functions/classes based on naming conventions."
        },
        {
          "id": "runner-component",
          "title": "The Runner & Isolation",
          "summary": "Design of the component that executes tests in controlled, isolated environments, including the mechanism for parallel execution."
        }
      ]
    },
    {
      "id": "component-assertions",
      "title": "6. Component Design: Assertions & Matchers (Milestone 2)",
      "summary": "Detailed design for the assertion library, including core assertions, exception checking, and a custom matcher API.",
      "subsections": [
        {
          "id": "assertion-engine",
          "title": "Assertion Engine",
          "summary": "Design of the central assertion logic, focusing on comparison, helpful error messages, and diff generation."
        },
        {
          "id": "matchers-api",
          "title": "Matchers API",
          "summary": "Design of the extensible API that allows users to define custom assertion logic with tailored failure messages."
        }
      ]
    },
    {
      "id": "component-fixtures",
      "title": "7. Component Design: Fixtures & Setup/Teardown (Milestone 3)",
      "summary": "Detailed design for the fixture system, supporting setup/teardown hooks and dependency injection with different scopes.",
      "subsections": [
        {
          "id": "fixture-system",
          "title": "Fixture System Architecture",
          "summary": "Design of the registry and lifecycle manager for fixtures, explaining how they are created, cached, and torn down based on scope."
        },
        {
          "id": "dependency-injection",
          "title": "Dependency Injection Mechanism",
          "summary": "Design of how fixtures are automatically provided to test functions by inspecting their parameter names."
        }
      ]
    },
    {
      "id": "component-reporting",
      "title": "8. Component Design: Reporting & CLI (Milestone 4)",
      "summary": "Detailed design for the command-line interface and the reporting system, including terminal output and JUnit XML generation.",
      "subsections": [
        {
          "id": "cli-parser",
          "title": "CLI Parser",
          "summary": "Design of the command-line argument parser that handles file patterns, test name filters, and output format flags."
        },
        {
          "id": "reporter",
          "title": "Reporter",
          "summary": "Design of the component that collects results, formats them for human-readable and machine-readable (JUnit XML) output, and calculates statistics."
        }
      ]
    },
    {
      "id": "interactions",
      "title": "9. Interactions and Data Flow",
      "summary": "Describes the sequence of operations from a user typing a command to receiving a test report, using sequence diagrams and data flow tables.",
      "subsections": [
        {
          "id": "main-sequence",
          "title": "Main Execution Sequence",
          "summary": "Step-by-step walkthrough of the most common happy path: CLI invocation -> discovery -> fixture setup -> test execution -> reporting."
        },
        {
          "id": "data-flow",
          "title": "Data Flow Between Components",
          "summary": "Details the key data structures passed between components (e.g., TestSuite from Discoverer to Runner) and their transformations."
        }
      ]
    },
    {
      "id": "error-handling",
      "title": "10. Error Handling and Edge Cases",
      "summary": "Catalogues potential failure modes (e.g., import errors, fixture teardown failures) and defines the system's recovery and reporting strategy for each.",
      "subsections": [
        {
          "id": "failure-modes",
          "title": "Common Failure Modes",
          "summary": "Lists categories of errors: test errors vs failures, discovery errors, fixture lifecycle errors, and system-level errors."
        },
        {
          "id": "recovery-strategy",
          "title": "Recovery & Reporting Strategy",
          "summary": "Defines how the framework should behave when errors occur (e.g., continue running other tests) and how to present error information clearly."
        }
      ]
    },
    {
      "id": "testing-strategy",
      "title": "11. Testing Strategy",
      "summary": "Explains how to test the test framework itself, including property-based checks and milestone-by-milestone verification checkpoints.",
      "subsections": [
        {
          "id": "framework-tests",
          "title": "Testing the Framework",
          "summary": "Recommends approaches for self-validation: using the framework to test itself, property-based testing for assertions, and golden master tests for output."
        },
        {
          "id": "milestone-checkpoints",
          "title": "Milestone Checkpoints",
          "summary": "For each of the four milestones, provides a concrete command to run and the expected output to verify correct implementation."
        }
      ]
    },
    {
      "id": "debugging-guide",
      "title": "12. Debugging Guide",
      "summary": "Provides a symptom-cause-fix table for common implementation bugs, along with specific debugging techniques like adding introspection logs.",
      "subsections": [
        {
          "id": "common-bugs",
          "title": "Common Bugs & Fixes",
          "summary": "Structured table listing symptoms (e.g., 'Test passes but fixture not cleaned up'), likely causes, and steps to fix."
        },
        {
          "id": "debugging-techniques",
          "title": "Debugging Techniques",
          "summary": "Suggests specific strategies like logging fixture lifecycle events or using Python's `-m pdb` to trace test execution."
        }
      ]
    },
    {
      "id": "future-extensions",
      "title": "13. Future Extensions",
      "summary": "Describes potential features that could be built on top of the current design, such as a plugin system, custom markers, or parameterized tests.",
      "subsections": [
        {
          "id": "extension-ideas",
          "title": "Ideas for Extension",
          "summary": "Lists and briefly describes possible advanced features, noting how the current architecture does or does not accommodate them."
        }
      ]
    },
    {
      "id": "glossary",
      "title": "14. Glossary",
      "summary": "Defines key terms used throughout the document, such as Fixture, Test Isolation, and JUnit XML.",
      "subsections": [
        {
          "id": "terms",
          "title": "Term Definitions",
          "summary": "Alphabetical list of terms with clear definitions and references to where they are first introduced."
        }
      ]
    }
  ],
  "diagrams": [
    {
      "id": "sys-component",
      "title": "System Component Diagram",
      "description": "Shows the five main components (CLI Parser, Discoverer, Runner, Assertion Engine, Reporter) and their interactions. Includes data flow arrows labeled with key data structures (e.g., Configuration, TestSuite, TestResult).",
      "type": "component",
      "relevant_sections": [
        "high-level-architecture",
        "interactions"
      ]
    },
    {
      "id": "data-model-diag",
      "title": "Data Model Relationships",
      "description": "Class diagram showing the relationships between core types: TestCase aggregates into TestSuite, TestResult is produced by running a TestCase, Fixture can be depended on by TestCase and other Fixtures. Show composition/aggregation lines.",
      "type": "class",
      "relevant_sections": [
        "data-model"
      ]
    },
    {
      "id": "test-execution-seq",
      "title": "Test Execution Sequence",
      "description": "Sequence diagram for running a single test with a fixture. Actors: User, CLI, Discoverer, Runner, Fixture Registry, Assertion Engine, Reporter. Show messages for discovery, fixture setup, test execution, assertion check, fixture teardown, and result recording.",
      "type": "sequence",
      "relevant_sections": [
        "component-discovery",
        "component-fixtures",
        "interactions"
      ]
    },
    {
      "id": "test-state-machine",
      "title": "Test Result State Machine",
      "description": "State machine for a TestResult. States: PENDING, RUNNING, PASSED, FAILED, ERRORED, SKIPPED. Transitions triggered by events: test_started, assertion_passed, assertion_failed, exception_raised, test_skipped.",
      "type": "state-machine",
      "relevant_sections": [
        "component-discovery",
        "data-model"
      ]
    },
    {
      "id": "fixture-scope-seq",
      "title": "Fixture Scope Lifecycle Sequence",
      "description": "Sequence diagram contrasting function-scoped vs. module-scoped fixture lifecycle across the execution of three tests in one module. Show fixture creation, caching, and teardown timing relative to test execution blocks.",
      "type": "sequence",
      "relevant_sections": [
        "component-fixtures"
      ]
    },
    {
      "id": "discovery-flowchart",
      "title": "Test Discovery Flowchart",
      "description": "Flowchart for the Discoverer's algorithm. Start at a directory path. Steps: collect .py files, import module, inspect members, filter by naming convention ('test_'), recursively inspect classes, yield TestCase objects. Include decision diamonds for 'is callable?', 'name starts with test?'.",
      "type": "flowchart",
      "relevant_sections": [
        "component-discovery"
      ]
    },
    {
      "id": "assertion-eval-flow",
      "title": "Assertion Evaluation Flow",
      "description": "Flowchart for the assertion engine evaluating `assert_equal(actual, expected)`. Steps: compute comparison, if match -> PASS; if not -> compute diff, format message with actual/expected/diff, raise AssertionError with formatted message.",
      "type": "flowchart",
      "relevant_sections": [
        "component-assertions"
      ]
    },
    {
      "id": "cli-workflow",
      "title": "CLI Workflow",
      "description": "Flowchart for the main CLI workflow. Start: parse arguments. Decision: if `--help` -> print help and exit. Otherwise: convert file patterns to paths, pass to Discoverer, get TestSuite, pass to Runner, collect results, pass to Reporter, output to stdout/file, set exit code (0 if all pass, else non-zero).",
      "type": "flowchart",
      "relevant_sections": [
        "component-reporting",
        "interactions"
      ]
    }
  ]
}