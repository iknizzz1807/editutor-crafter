{
  "types": {
    "TransformerConfig": "d_model int, num_heads int, d_k int, d_v int, seq_length int, vocab_size int, num_layers int, dropout_rate float, ffn_expansion int",
    "SimpleTokenizer": "encode method, decode method, vocab_size property, pad_token_id property, eos_token_id property",
    "TextDataset": "__getitem__ returning (input_tensor, target_tensor)",
    "MultiHeadAttention": "forward method, nn.Module",
    "TrainingConfig": "learning_rate float, batch_size int, num_epochs int, gradient_clip_norm float, weight_decay float, warmup_steps int, save_interval int, log_interval int, eval_interval int",
    "GenerationConfig": "temperature float, top_k int, top_p int, max_length int, pad_token_id int, eos_token_id int, repetition_penalty float, length_penalty float",
    "LayerNorm": "forward method with d_model and eps parameters",
    "FeedForwardNetwork": "forward method with expansion and projection layers",
    "TransformerBlock": "forward method combining attention and FFN with residuals",
    "TransformerTrainer": "training coordinator",
    "KVCache": "cache management with num_layers, num_heads, d_k, d_v, device parameters",
    "TextGenerator": "generation class",
    "SamplingStrategies": "static methods for different sampling approaches",
    "NumericalStabilityMonitor": "gradient_clip_norm float, attention_clip_value float",
    "TransformerInputValidator": "config TransformerConfig",
    "RuntimeMonitor": "monitoring utilities",
    "ExtensionConfig": "positional_encoding str, rope_base float, alibi_slopes Optional[torch.Tensor], use_flash_attention bool, attention_variant str, query_groups int, normalization_type str, normalization_placement str, use_mixed_precision bool, gradient_checkpointing bool, learning_rate_schedule str, warmup_steps int, distributed_backend str, gradient_sync_frequency int",
    "ExtensionRegistry": "_positional_encodings dict, _attention_variants dict, _normalization_types dict, _schedulers dict",
    "PerformanceMonitor": "metrics_history List[Dict[str, Any]], current_metrics Dict[str, Any], timers Dict[str, float]",
    "CosineAnnealingScheduler": "optimizer, max_lr float, total_steps int, warmup_steps int, min_lr float, current_step int",
    "AMPTrainer": "model, optimizer, scheduler, scaler GradScaler",
    "RotaryPositionalEmbedding": "d_model int, max_seq_length int, base float",
    "EnhancedAMPTrainer": "model nn.Module, optimizer torch.optim.Optimizer, config ExtensionConfig, scheduler Optional, scaler GradScaler, performance_monitor PerformanceMonitor",
    "ExtensionBenchmark": "model_configs List[TransformerConfig], benchmark_results dict",
    "MultiHeadAttentionWithRoPE": "rope RotaryPositionalEmbedding",
    "AttentionWeights": "torch.Tensor",
    "TokenSequence": "torch.Tensor",
    "Embeddings": "torch.Tensor",
    "Logits": "torch.Tensor",
    "TerminologyValidator": "PREFERRED_TERMS dict, validate_method_names method"
  },
  "methods": {
    "create_causal_mask(seq_length, device)": "creates lower triangular attention mask",
    "apply_causal_mask(scores, mask)": "applies mask to attention scores",
    "count_parameters(model)": "counts trainable parameters in model",
    "plot_attention_weights(weights, tokens, layer, head)": "visualizes attention as heatmap",
    "forward(x, mask)": "main forward pass through transformer block",
    "get_attention_weights(x, mask)": "extract attention weights for visualization",
    "test_shape_consistency(model, input_shape, device)": "verifies model maintains correct tensor shapes",
    "initialize_transformer_weights(module)": "initialize model weights using best practices",
    "check_gradient_flow(model, loss)": "analyze gradient magnitudes after backward pass",
    "encode(text: str, add_eos: bool) -> List[int]": "Convert text string to integer token sequence",
    "decode(tokens: List[int], skip_special_tokens: bool) -> str": "Convert token sequence back to text string",
    "__getitem__(idx: int) -> Tuple[torch.Tensor, torch.Tensor]": "Return input sequence and target labels for dataset index",
    "train(train_loader, val_loader) -> Dict[str, Any]": "Main training loop with gradient computation and optimization",
    "_compute_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor": "Compute cross-entropy loss with padding mask",
    "_validate(val_loader: DataLoader) -> float": "Compute validation loss without gradients",
    "_save_checkpoint(val_loss: float, filepath: str) -> None": "Save complete training state for resumption",
    "create_data_loaders(train_tokens, val_tokens, seq_length, batch_size, num_workers) -> Tuple[DataLoader, DataLoader]": "Create train and validation data loaders",
    "generate(prompt, max_new_tokens) -> str": "main text generation method",
    "greedy_sample(logits) -> tensor": "deterministic highest probability selection",
    "temperature_sample(logits, temperature) -> tensor": "temperature-scaled stochastic sampling",
    "top_k_sample(logits, k, temperature) -> tensor": "sample from k most likely tokens",
    "top_p_sample(logits, p, temperature) -> tensor": "nucleus sampling with cumulative probability",
    "update(layer_idx, new_keys, new_values) -> tuple": "update KV cache and return concatenated tensors",
    "clear()": "reset cache state for new sequence",
    "get_memory_info() -> dict": "return cache memory statistics",
    "stabilize_attention_scores(scores, temperature) -> tensor": "apply stability techniques to attention scores",
    "check_and_clip_gradients(model) -> dict": "clip gradients and return diagnostics",
    "validate_token_sequence(tokens, name) -> bool": "validate token tensor",
    "validate_attention_inputs(q, k, v, mask) -> bool": "validate attention inputs",
    "check_memory_usage(threshold) -> dict": "monitor GPU memory usage",
    "forward(x, mask) -> tensor": "forward pass with error handling",
    "_handle_training_step_errors(batch_idx, error) -> bool": "handle training errors",
    "_validate_generation_state(tokens, step) -> bool": "validate generation state",
    "encode(text, add_eos)": "Convert text string to integer token sequence",
    "decode(tokens, skip_special_tokens)": "Convert token sequence back to text string",
    "__getitem__(idx)": "Return input sequence and target labels for dataset index",
    "train(train_loader, val_loader)": "Main training loop with gradient computation and optimization",
    "generate(prompt, max_new_tokens)": "main text generation method",
    "greedy_sample(logits)": "deterministic highest probability selection",
    "temperature_sample(logits, temperature)": "temperature-scaled stochastic sampling",
    "top_k_sample(logits, k, temperature)": "sample from k most likely tokens",
    "top_p_sample(logits, p, temperature)": "nucleus sampling with cumulative probability",
    "create_data_loaders(train_tokens, val_tokens, seq_length, batch_size, num_workers)": "Create train and validation data loaders",
    "validate_attention_shapes(q, k, v, mask)": "comprehensive shape validation for attention inputs",
    "diagnose_attention_patterns(weights)": "analyze attention weight patterns for issues",
    "monitor_training_stability(current_loss, gradient_norm)": "detect training instability patterns",
    "validate_loss_computation(logits, targets, computed_loss)": "verify cross-entropy loss computation",
    "analyze_repetition_patterns(generated_text)": "quantify repetition types in generated text",
    "assess_coherence_quality(prompt, generated_text)": "evaluate text coherence dimensions",
    "optimize_sampling_parameters(prompt, num_samples)": "find optimal sampling parameter combinations",
    "register_positional_encoding(name, implementation_class)": "register a positional encoding implementation",
    "create_positional_encoding(name, config, model_config)": "create positional encoding instance from configuration",
    "timer(name)": "context manager for timing code sections",
    "record_memory_usage()": "record current GPU and system memory usage",
    "record_training_step(step, loss, learning_rate, batch_size, sequence_length)": "record metrics for a single training step",
    "get_performance_summary(last_n_steps)": "generate performance summary for recent training steps",
    "step()": "update learning rate for current training step",
    "_compute_lr()": "compute learning rate for current step",
    "get_last_lr()": "get current learning rate",
    "train_step(batch)": "execute single training step with mixed precision",
    "_compute_loss(logits, targets)": "compute cross-entropy loss with proper mixed precision handling",
    "forward(q, k, start_pos)": "apply rotary embeddings to query and key tensors",
    "_rotate_half(x)": "rotate half the dimensions negatively for RoPE computation",
    "train_step(batch, step)": "execute training step with all enabled optimizations",
    "_compute_loss_with_precision(logits, targets)": "compute loss with appropriate precision handling",
    "_apply_gradient_clipping(max_norm)": "apply gradient clipping and return gradient norm",
    "save_checkpoint(filepath, step, validation_loss)": "save training checkpoint with extension state",
    "benchmark_attention_variants(sequence_lengths, batch_sizes)": "benchmark different attention implementations",
    "benchmark_training_optimizations(num_steps)": "benchmark training optimization techniques",
    "scaled_dot_product_attention(query, key, value, mask, dropout_rate)": "compute scaled dot-product attention with optional masking",
    "validate_method_names(class_methods)": "check method names against preferred terminology",
    "print_tensor_info(tensor, name, expected_shape)": "print comprehensive tensor information with standardized terminology"
  },
  "constants": {
    "d_model": "model embedding dimension",
    "num_heads": "number of parallel attention heads",
    "d_k": "key dimension per attention head",
    "d_v": "value dimension per attention head",
    "seq_length": "maximum sequence length",
    "vocab_size": "vocabulary size",
    "dropout_rate": "dropout probability",
    "ffn_expansion": "expansion ratio for feed-forward hidden dimension",
    "pad_token_id": "padding token ID",
    "eos_token_id": "end of sequence token ID",
    "learning_rate": "base learning rate for optimizer",
    "batch_size": "number of sequences per training batch",
    "gradient_clip_norm": "maximum gradient norm for clipping",
    "warmup_steps": "number of steps for learning rate warmup",
    "temperature": "sampling randomness control parameter",
    "top_k": "number of top tokens to consider in sampling",
    "top_p": "cumulative probability threshold for nucleus sampling",
    "max_length": "maximum sequence length for generation",
    "repetition_penalty": "penalty factor for repeated tokens",
    "max_seq_length": "maximum sequence length for positional encoding",
    "base": "base value for frequency computation",
    "max_norm": "maximum gradient norm for clipping",
    "num_steps": "number of training steps for benchmarking"
  },
  "terms": {
    "self-attention": "attention mechanism where queries, keys, and values come from same sequence",
    "causal mask": "preventing attention to future positions",
    "scaled dot-product attention": "Q@K^T/sqrt(d_k) attention computation",
    "multi-head attention": "parallel attention heads with different learned projections",
    "autoregressive": "sequential text generation where each token depends on previous tokens",
    "vanishing gradients": "gradients becoming too small through layer chain",
    "attention weights": "softmax-normalized scores indicating token importance",
    "residual connections": "skip connections that add input to layer output for gradient flow",
    "decoder-only": "transformer architecture without encoder, used for generation tasks",
    "layer normalization": "normalization across feature dimension per token for training stability",
    "pre-normalization": "applying layer norm before sublayers rather than after",
    "feed-forward network": "two-layer MLP with expansion and projection for position-wise processing",
    "information refinement pipeline": "mental model of how transformer blocks iteratively improve token representations",
    "language modeling objective": "training objective to predict next token in sequence",
    "cross-entropy loss": "loss function measuring difference between predicted and actual token probabilities",
    "label shifting": "offset between input and target sequences for next-token prediction",
    "teacher forcing": "training strategy using actual previous tokens rather than model predictions",
    "gradient clipping": "limiting gradient norms to prevent training instability",
    "learning rate scheduling": "adaptive learning rate changes during training for better convergence",
    "mixed precision training": "using 16-bit floats for speed with 32-bit for critical operations",
    "greedy decoding": "deterministic selection of highest probability token at each step",
    "temperature sampling": "stochastic sampling with temperature-controlled randomness",
    "top-k sampling": "sampling restricted to k most likely tokens",
    "top-p sampling": "nucleus sampling using cumulative probability threshold",
    "KV caching": "storing key-value tensors to avoid recomputation during generation",
    "repetition penalty": "mechanism to reduce probability of recently generated tokens",
    "sampling strategy": "method for selecting next token from probability distribution",
    "nucleus sampling": "alternative term for top-p sampling",
    "iterative prediction": "mental model of generating text one token at a time",
    "numerical stability": "preventing overflow and underflow in computations",
    "softmax overflow": "exponential function producing infinity when inputs are too large",
    "gradient explosion": "gradients growing exponentially through layer chain",
    "LogSumExp trick": "numerical stability technique for softmax computation",
    "sequence length limits": "maximum token sequence length constraints",
    "token vocabulary bounds": "valid range of token IDs",
    "shape consistency checking": "validating tensor dimensions match requirements",
    "rotary position embedding": "position encoding that applies rotation matrices to queries and keys",
    "gradient scaling": "multiplying gradients to prevent underflow in mixed precision",
    "flash attention": "memory-efficient attention computation using tiling",
    "multi-query attention": "attention variant sharing key-value projections across heads",
    "grouped-query attention": "attention variant grouping heads to share key-value projections",
    "cosine annealing": "learning rate schedule following cosine curve",
    "RMSNorm": "normalization using root mean square without mean centering",
    "automatic mixed precision": "framework-integrated mixed precision with automatic casting",
    "gradient checkpointing": "trading computation for memory by recomputing activations",
    "ALiBi": "attention with linear biases for position encoding",
    "length extrapolation": "ability to handle sequences longer than training sequences",
    "distributed training": "training across multiple devices or machines",
    "data parallel": "replicating model across devices with batch splitting",
    "model parallel": "splitting model components across devices",
    "pipeline parallel": "dividing model into stages processed sequentially",
    "tensor parallel": "splitting individual operations across devices",
    "extension registry": "system for dynamically loading transformer enhancements",
    "performance monitoring": "tracking training metrics and resource usage",
    "selective focus": "intuitive explanation of how attention computes relevance-weighted summaries"
  }
}