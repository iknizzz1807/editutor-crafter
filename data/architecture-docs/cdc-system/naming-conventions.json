{
  "types": {
    "RawLogEntry": "fields: logSequenceNumber String, databaseType String, tableName String, operationType String, rowData Map<String, Object>, timestamp Long",
    "ChangeEvent": "fields: eventId String, sourceTable String, operationType String, beforeImage Map<String, Object>, afterImage Map<String, Object>, schemaVersionId String, transactionId String, commitTimestamp Long",
    "SchemaVersion": "fields: schemaId String, tableName String, version Integer, columnDefinitions Map<String, ColumnType>, compatibilityMode String",
    "AppConfig": "fields: database DatabaseConfig, kafka KafkaConfig, schema SchemaConfig, maxBatchSize int, backpressureThresholdMs long",
    "DatabaseConfig": "fields: type String, host String, port int, database String, username String, password String, slotName String",
    "KafkaConfig": "fields: bootstrapServers String, topicPrefix String, replicationFactor int, partitionsPerTable int, acks String, enableIdempotence boolean",
    "SchemaConfig": "fields: registryType String, registryUrl String",
    "HealthStatus": "enum: HEALTHY, DEGRADED, STOPPED",
    "ColumnType": "fields: sqlType String, javaClass Class<?>, nullable boolean, defaultValue Object",
    "LogConnector": "fields: config AppConfig, offsetStore OffsetStore, parser LogParser, buffer BlockingQueue<RawLogEntry>, lastProcessedLSN String, running boolean, workerThread Thread",
    "FileOffsetStore": "fields: offsetFile File, properties Properties",
    "OffsetStore": "interface methods: save(String, String), load(String)",
    "LogParser": "interface (methods implied)",
    "TransactionState": "fields: transactionId String, startLsn String, startTimestamp Long, operations List<TransactionOperation>, lastActivityTimestamp Long, lastOperationByKey Map<String, TransactionOperation>",
    "TransactionOperation": "fields: sequenceNumber int, tableName String, operationType String, primaryKey Map<String, Object>, rowData Map<String, Object>, timestamp Long",
    "ChangeEventBuilder": "fields: activeTransactions Map<String, TransactionState>, schemaCache Map<String, SchemaVersion>, deduplicationCache DeduplicationCache, transactionTimeoutMs long, nextOperationSequence int",
    "EventStreamer": "interface methods: initialize(AppConfig), start(), stop(), publish(List<ChangeEvent>), getDeliveryStatus(), isHealthy()",
    "KafkaEventPublisher": "implements EventStreamer, fields: kafkaProducer KafkaProducer<String, byte[]>, pendingEvents ConcurrentMap<String, ChangeEvent>, lastAckedLsnBySlot ConcurrentMap<String, String>, backpressureSignal AtomicBoolean, inFlightEventCount AtomicInteger",
    "DeliveryCallback": "implements Callback, fields: eventId String, event ChangeEvent, publisher KafkaEventPublisher",
    "StreamerMetrics": "fields: kafkaAdminClient AdminClient, scheduledExecutor ScheduledExecutorService, lagThresholdMs long",
    "SchemaChangeEvent": "fields: tableName String, oldSchemaId String, newSchemaId String, changeTimestamp Long, ddlStatement String",
    "FileBasedSchemaRegistry": "fields: schemaStorageDir Path, objectMapper ObjectMapper, cache Map<String, SchemaVersion>",
    "RecoveryCoordinator": "fields: circuitBreakerManager CircuitBreakerManager, offsetRecoveryService OffsetRecoveryService, config AppConfig, currentState RecoveryState",
    "CircuitBreakerManager": "fields: registry CircuitBreakerRegistry, breakers Map<String, CircuitBreaker>",
    "RecoveryState": "enum: NORMAL, DEGRADED, RECOVERING, HALTED",
    "RecoveryStrategy": "enum: RETRY_WITH_BACKOFF, RESET_AND_RECOVER, DEGRADE_FUNCTIONALITY, HALT_FOR_INTERVENTION",
    "EventSnapshotBuffer": "fields: buffer ChangeEvent[], capacity int, writeIndex AtomicInteger, size int",
    "GapDetectionEvent": "fields: previousLSN String, currentLSN String, gapSize long, detectionTimestamp Long, suspectedCause String",
    "ChaosMonkey": "fields: random Random",
    "OracleRedoLogParser": "fields: config OracleConfig, logMiner LogMinerHelper, typeMapper OracleDataTypeMapper, currentScn String",
    "DatabaseDetector": "fields: none (static methods)",
    "DatabaseDetector.DatabaseType": "enum: POSTGRESQL, MYSQL, ORACLE, SQL_SERVER",
    "OracleConfig": "fields: (extends DatabaseConfig with Oracle-specific fields)",
    "LogMinerHelper": "fields: (Oracle LogMiner utility class)",
    "OracleDataTypeMapper": "fields: (Oracle to Java type mapping utility)"
  },
  "methods": {
    "ConfigLoader.loadConfig(String, Class<T>) returns T": "Loads configuration from YAML file",
    "Main.main(String[])": "Application entry point",
    "CdcPipeline.initialize()": "Initializes all pipeline components",
    "CdcPipeline.start()": "Starts the pipeline processing",
    "CdcPipeline.stop()": "Stops the pipeline gracefully",
    "CdcPipeline.getHealthStatus() returns HealthStatus": "Returns current health status",
    "EventSerializer.serialize(ChangeEvent, SchemaVersion) returns byte[]": "Serializes a ChangeEvent into bytes",
    "EventSerializer.deserialize(byte[], SchemaVersion) returns ChangeEvent": "Deserializes bytes back into a ChangeEvent",
    "LogConnector.initialize()": "Prepares connector using config and offset store",
    "LogConnector.start()": "Begins the continuous reading loop",
    "LogConnector.stop()": "Gracefully stops the loop and persists state",
    "LogConnector.getNextBatch(int) returns List<RawLogEntry>": "Fetches next batch of parsed entries",
    "LogConnector.getLastProcessedLSN() returns String": "Returns last successfully processed LSN",
    "FileOffsetStore.save(String, String)": "Persists LSN for a slot to file",
    "FileOffsetStore.load(String) returns String": "Loads LSN for a slot from file",
    "ChangeEventBuilder.processEntry(RawLogEntry) returns List<ChangeEvent>": "Processes a single raw log entry through transaction state machine",
    "ChangeEventBuilder.processBatch(List<RawLogEntry>) returns List<ChangeEvent>": "Processes batch of entries efficiently",
    "ChangeEventBuilder.flushPendingTransactions() returns List<ChangeEvent>": "Forces all pending transactions to be emitted",
    "ChangeEventBuilder.getTransactionCount() returns int": "Returns number of active transactions",
    "ChangeEventBuilder.reset() returns void": "Clears all internal transaction state",
    "ChangeEventBuilder.buildEventsForTransaction(TransactionState) returns List<ChangeEvent>": "Converts transaction operations to events",
    "ChangeEventBuilder.generateEventId(TransactionState, TransactionOperation) returns String": "Generates deterministic event ID",
    "KafkaProducerFactory.createProducer(AppConfig) returns KafkaProducer<String, byte[]>": "Creates and configures a Kafka producer with reliability settings",
    "SchemaRegistry.registerSchema(tableName, newColumnDefs, compatibilityMode) returns SchemaVersion": "Registers a new schema version after compatibility check",
    "SchemaRegistry.getSchemaById(schemaId) returns SchemaVersion": "Retrieves a specific schema version",
    "SchemaRegistry.getLatestSchema(tableName) returns SchemaVersion": "Gets the most recent schema for a table",
    "SchemaRegistry.checkCompatibility(newColumnDefs, existingSchema, mode) returns boolean": "Performs a dry-run compatibility check",
    "SchemaRegistry.getSchemaHistory(tableName) returns List<SchemaVersion>": "Returns all schema versions for a table",
    "FileBasedSchemaRegistry.loadExistingSchemasIntoCache() returns void": "Loads persisted schemas from disk into memory on startup",
    "RecoveryCoordinator.handleComponentFailure(String, Throwable)": "Handles component failure and executes recovery strategy",
    "RecoveryCoordinator.onPipelineStart()": "Executes recovery procedures on pipeline startup",
    "RecoveryCoordinator.onPipelineStop()": "Executes graceful shutdown and state persistence",
    "RecoveryCoordinator.determineRecoveryStrategy(String, Throwable) returns RecoveryStrategy": "Determines appropriate recovery strategy based on component and error",
    "CircuitBreakerManager.getOrCreate(String) returns CircuitBreaker": "Gets or creates a circuit breaker for the given component",
    "CircuitBreakerManager.recordSuccess(String)": "Records a successful operation for circuit breaker statistics",
    "CircuitBreakerManager.recordFailure(String, Throwable)": "Records a failed operation for circuit breaker statistics",
    "CircuitBreakerManager.isCallPermitted(String) returns boolean": "Checks if calls are permitted for the given circuit breaker",
    "CircuitBreakerManager.getState(String) returns String": "Gets the current state of the circuit breaker",
    "EventSnapshotBuffer.add(ChangeEvent)": "Add event to circular buffer",
    "EventSnapshotBuffer.getRecentEvents(int) returns List<ChangeEvent>": "Get most recent events",
    "LogConnector.processBatchWithGapDetection(List<RawLogEntry>) returns List<RawLogEntry>": "Process log entries with gap detection",
    "LogConnector.calculateLsnGap(String, String) returns long": "Calculate gap between two LSNs",
    "ChaosMonkey.maybeFail(String, double)": "Randomly inject failures for testing",
    "DatabaseDetector.detect(DataSource) returns DatabaseType": "Auto-detects database type from JDBC connection",
    "OracleRedoLogParser.convertOracleValue(Object, String) returns Object": "Converts Oracle-specific values to Java types"
  },
  "constants": {
    "OPERATION_INSERT": "INSERT",
    "OPERATION_UPDATE": "UPDATE",
    "OPERATION_DELETE": "DELETE",
    "COMPATIBILITY_BACKWARD": "BACKWARD",
    "COMPATIBILITY_FORWARD": "FORWARD",
    "COMPATIBILITY_FULL": "FULL",
    "COMPATIBILITY_NONE": "NONE"
  },
  "terms": {
    "Write-Ahead Log (WAL)": "PostgreSQL's transaction log for crash recovery",
    "binlog": "MySQL's binary log recording database changes",
    "log sequence number (LSN)": "Pointer to a specific position in a transaction log",
    "transaction boundary": "The grouping of all changes that commit together atomically",
    "backpressure": "Mechanism to slow down data production when consumers are overwhelmed",
    "schema compatibility": "Rules governing whether a schema change breaks existing consumers",
    "at-least-once delivery": "Guarantee that each event is delivered at least once, possibly with duplicates",
    "backward compatibility": "New schema can read data written with old schema",
    "logical decoding": "Process of interpreting transaction log entries as logical row changes",
    "idempotency": "Property where repeating an operation yields the same result",
    "pipeline pattern": "Architectural pattern where data flows through series of processing stages",
    "Log Sequence Number (LSN)": "Pointer to a specific position in a transaction log",
    "replication slot": "(PostgreSQL) A feature that ensures WAL segments are retained until consumed by a logical replication consumer",
    "before/after image": "The state of a row before and after a change operation",
    "deterministic event ID": "An event identifier that can be regenerated from the same inputs",
    "partition key": "Value used to determine which Kafka partition a message is sent to",
    "idempotent producer": "Kafka producer configuration that prevents duplicate message submission during retries",
    "consumer lag": "Difference between latest produced offset and last consumed offset",
    "Schema Registry": "Centralized service that stores and manages schema versions for CDC",
    "Backward Compatibility": "New schema can read data written with old schema.",
    "Forward Compatibility": "Old schema can read data written with new schema",
    "Full Compatibility": "Combination of backward and forward compatibility",
    "Schema Evolution": "The process of managing changes to a data schema over time",
    "DDL (Data Definition Language)": "SQL statements that define database structure, e.g., CREATE, ALTER, DROP",
    "Type Coercion": "The automatic or implicit conversion of values from one data type to another",
    "Schema Change Event": "A special notification event emitted when a new schema version is registered",
    "circuit breaker": "A design pattern that stops cascading failures by failing fast when downstream services are unavailable",
    "exponential backoff": "A retry strategy where wait time doubles after each failure",
    "chaos engineering": "The practice of intentionally injecting failures to test system resilience",
    "graceful shutdown": "Shutdown procedure that completes in-flight work before terminating",
    "LSN recovery": "Process of resuming log reading from the correct log sequence number after failure",
    "transaction timeout": "Automatic abortion of transactions that exceed maximum allowed duration",
    "backpressure propagation": "Mechanism to slow down data production when consumers cannot keep up",
    "schema cache incoherency": "State where different components have different schema versions cached",
    "gap detection event": "Special event emitted when the CDC pipeline detects potentially missing data",
    "Gap Detection": "Process of identifying missing log sequence numbers in CDC pipeline",
    "Circular Buffer": "Fixed-size buffer that overwrites oldest data when full",
    "Chaos Engineering": "Practice of intentionally injecting failures to test system resilience",
    "Correlation ID": "Unique identifier propagated across system components to trace requests",
    "MDC (Mapped Diagnostic Context)": "Thread-local storage for contextual logging information in Java",
    "SCN (System Change Number)": "Oracle's equivalent of LSN, a numeric identifier for database changes",
    "LogMiner": "Oracle's utility for reading redo log files",
    "Redo Log": "Oracle's transaction log for crash recovery",
    "CDC Tables": "SQL Server's change data capture feature using special system tables",
    "Oplog": "MongoDB's operation log for replication"
  }
}