{
  "title": "Project CDC: Real-Time Change Data Capture System Design",
  "overview": "This document outlines the design for a Change Data Capture (CDC) system that streams database changes (INSERT, UPDATE, DELETE) to downstream consumers in real-time. The core challenge is reliably reading low-level database transaction logs (like PostgreSQL WAL or MySQL binlog), transforming them into structured change events, and delivering them with strong ordering guarantees, all while handling schema changes and database-specific complexities without impacting the source database's performance.",
  "sections": [
    {
      "id": "context-problem",
      "title": "1. Context and Problem Statement",
      "summary": "Explains the business need for real-time data synchronization, the limitations of polling-based solutions, and why reading transaction logs is the preferred but complex approach.",
      "subsections": [
        {
          "id": "analogy-intro",
          "title": "1.1 The Newspaper Analogy for CDC"
        },
        {
          "id": "problem-detail",
          "title": "1.2 The Technical Challenge of Log-Based CDC"
        },
        {
          "id": "existing-approaches",
          "title": "1.3 Existing Approaches and Trade-offs"
        }
      ]
    },
    {
      "id": "goals-nongoals",
      "title": "2. Goals and Non-Goals",
      "summary": "Defines the functional and non-functional requirements the system must meet, and explicitly states what is out of scope to prevent scope creep.",
      "subsections": [
        {
          "id": "goals",
          "title": "2.1 Goals (Must Have)"
        },
        {
          "id": "non-goals",
          "title": "2.2 Non-Goals (Explicitly Out of Scope)"
        }
      ]
    },
    {
      "id": "high-level-arch",
      "title": "3. High-Level Architecture",
      "summary": "Provides a bird's-eye view of the system components (Log Reader, Parser, Streamer, Schema Registry), their responsibilities, and how data flows between them, illustrated with a component diagram.",
      "subsections": [
        {
          "id": "component-overview",
          "title": "3.1 Component Overview and Responsibilities"
        },
        {
          "id": "file-structure",
          "title": "3.2 Recommended Project File Structure"
        }
      ]
    },
    {
      "id": "data-model",
      "title": "4. Data Model",
      "summary": "Describes the core data structures that flow through the system: RawLogEntry, ChangeEvent, and SchemaVersion. A class diagram illustrates their relationships.",
      "subsections": [
        {
          "id": "core-types",
          "title": "4.1 Core Type Definitions"
        },
        {
          "id": "serialization-format",
          "title": "4.2 Serialization Format for Events"
        }
      ]
    },
    {
      "id": "log-connector-parser",
      "title": "5. Component: Log Connector & Parser",
      "summary": "Details the component that connects to the database's transaction log, reads binary entries, and parses them into structured RawLogEntry objects. This is the foundation of Milestone 1.",
      "subsections": [
        {
          "id": "mental-model-parser",
          "title": "5.1 Mental Model: The Database Translator"
        },
        {
          "id": "interface-parser",
          "title": "5.2 Interface and State"
        },
        {
          "id": "behavior-parser",
          "title": "5.3 Internal Behavior and Algorithm"
        },
        {
          "id": "adr-db-specific-adapters",
          "title": "5.4 ADR: Database-Specific vs. Generic Parser"
        },
        {
          "id": "pitfalls-parser",
          "title": "5.5 Common Pitfalls in Log Parsing"
        },
        {
          "id": "impl-guidance-parser",
          "title": "5.6 Implementation Guidance"
        }
      ]
    },
    {
      "id": "change-event-builder",
      "title": "6. Component: Change Event Builder",
      "summary": "Covers the component that transforms parsed log entries into final ChangeEvent objects, handling transaction boundaries, deduplication, and attaching schema information.",
      "subsections": [
        {
          "id": "mental-model-builder",
          "title": "6.1 Mental Model: The Event Assembler"
        },
        {
          "id": "interface-builder",
          "title": "6.2 Interface and Transaction State"
        },
        {
          "id": "adr-event-ordering",
          "title": "6.3 ADR: Ordering and Deduplication Strategy"
        },
        {
          "id": "impl-guidance-builder",
          "title": "6.4 Implementation Guidance"
        }
      ]
    },
    {
      "id": "event-streamer",
      "title": "7. Component: Event Streamer & Delivery",
      "summary": "Describes how ChangeEvents are published to a message broker (Kafka), ensuring at-least-once delivery, ordering per primary key, and backpressure handling. This is the core of Milestone 2.",
      "subsections": [
        {
          "id": "mental-model-streamer",
          "title": "7.1 Mental Model: The Reliable Postal Service"
        },
        {
          "id": "interface-streamer",
          "title": "7.2 Interface and Delivery Semantics"
        },
        {
          "id": "adr-delivery-semantics",
          "title": "7.3 ADR: At-Least-Once vs. Exactly-Once Delivery"
        },
        {
          "id": "adr-partitioning",
          "title": "7.4 ADR: Topic Partitioning Strategy"
        },
        {
          "id": "pitfalls-streamer",
          "title": "7.5 Common Pitfalls in Event Delivery"
        },
        {
          "id": "impl-guidance-streamer",
          "title": "7.6 Implementation Guidance"
        }
      ]
    },
    {
      "id": "schema-registry-evolution",
      "title": "8. Component: Schema Registry & Evolution",
      "summary": "Explains the schema registry component that stores table schemas, validates compatibility of schema changes, and notifies consumers of updates, addressing Milestone 3.",
      "subsections": [
        {
          "id": "mental-model-schema",
          "title": "8.1 Mental Model: The Contract Librarian"
        },
        {
          "id": "interface-registry",
          "title": "8.2 Interface and Versioning"
        },
        {
          "id": "adr-compatibility-mode",
          "title": "8.3 ADR: Choosing a Schema Compatibility Mode"
        },
        {
          "id": "pitfalls-schema",
          "title": "8.4 Common Pitfalls in Schema Evolution"
        },
        {
          "id": "impl-guidance-registry",
          "title": "8.5 Implementation Guidance"
        }
      ]
    },
    {
      "id": "interactions-flow",
      "title": "9. Interactions and Data Flow",
      "summary": "Illustrates the end-to-end sequence of operations, from a database commit to a consumer receiving a change event, using sequence diagrams.",
      "subsections": [
        {
          "id": "normal-flow",
          "title": "9.1 Normal Operation Flow"
        },
        {
          "id": "schema-change-flow",
          "title": "9.2 Schema Change Flow"
        }
      ]
    },
    {
      "id": "error-handling",
      "title": "10. Error Handling and Edge Cases",
      "summary": "Details failure modes (database restart, network partition, corrupt log), detection mechanisms, and recovery strategies for each component.",
      "subsections": [
        {
          "id": "failure-modes",
          "title": "10.1 Failure Modes and Detection"
        },
        {
          "id": "recovery-strategies",
          "title": "10.2 Recovery Strategies"
        }
      ]
    },
    {
      "id": "testing-strategy",
      "title": "11. Testing Strategy",
      "summary": "Outlines a testing approach including unit tests for parsing logic, integration tests with an embedded database, and end-to-end validation of delivery guarantees.",
      "subsections": [
        {
          "id": "test-pyramid",
          "title": "11.1 Test Pyramid for CDC"
        },
        {
          "id": "milestone-checkpoints",
          "title": "11.2 Milestone Verification Checkpoints"
        }
      ]
    },
    {
      "id": "debugging-guide",
      "title": "12. Debugging Guide",
      "summary": "Provides a symptom-cause-fix table for common bugs learners encounter, along with techniques for inspecting log positions, event content, and consumer lag.",
      "subsections": [
        {
          "id": "common-bugs",
          "title": "12.1 Common Bugs and Fixes"
        },
        {
          "id": "inspection-techniques",
          "title": "12.2 Inspection and Tracing Techniques"
        }
      ]
    },
    {
      "id": "future-extensions",
      "title": "13. Future Extensions",
      "summary": "Suggests potential enhancements like support for additional databases (Oracle, SQL Server), exactly-once semantics, or a fully managed cloud service.",
      "subsections": [
        {
          "id": "extensions-list",
          "title": "13.1 Possible Extension Roadmap"
        }
      ]
    },
    {
      "id": "glossary",
      "title": "14. Glossary",
      "summary": "Defines key technical terms, acronyms, and domain-specific vocabulary used throughout this document.",
      "subsections": [
        {
          "id": "terms",
          "title": "14.1 Terms and Definitions"
        }
      ]
    }
  ],
  "diagrams": [
    {
      "id": "system-component",
      "title": "High-Level System Component Diagram",
      "description": "Shows the four main components (Log Connector/Parser, Change Event Builder, Event Streamer, Schema Registry) and their interactions with external systems (Source Database, Message Broker, Consumers). Include data flow arrows labeled with the core data types (RawLogEntry, ChangeEvent).",
      "type": "component",
      "relevant_sections": [
        "high-level-arch",
        "interactions-flow"
      ]
    },
    {
      "id": "data-model-class",
      "title": "Data Model Class Diagram",
      "description": "Illustrates the relationships between core data types: RawLogEntry, ChangeEvent, SchemaVersion, and TableSchema. Show composition (ChangeEvent has a SchemaVersion) and key fields like LSN/offset, operation type, before/after images.",
      "type": "class",
      "relevant_sections": [
        "data-model"
      ]
    },
    {
      "id": "parser-state-machine",
      "title": "Log Parser State Machine",
      "description": "Depicts the states of the Log Connector/Parser: IDLE, CONNECTING, READING_LOG, PAUSED (backpressure), ERROR. Transitions are triggered by events like 'connection successful', 'log entry read', 'consumer lag high', 'parse error'.",
      "type": "state-machine",
      "relevant_sections": [
        "log-connector-parser"
      ]
    },
    {
      "id": "event-delivery-sequence",
      "title": "Sequence Diagram: Normal Event Delivery",
      "description": "Shows the sequence from a database COMMIT to a consumer processing a change. Actors: Source DB, Log Parser, Event Builder, Event Streamer (Kafka Producer), Kafka Cluster, Consumer. Highlight the 'send event', 'ack', and 'commit offset' steps critical for at-least-once semantics.",
      "type": "sequence",
      "relevant_sections": [
        "interactions-flow",
        "event-streamer"
      ]
    },
    {
      "id": "schema-evolution-flowchart",
      "title": "Flowchart: Handling a Schema Change (ALTER TABLE)",
      "description": "A flowchart detailing the steps when a DDL event is detected: Parse DDL, extract new schema, query Registry for old version, run compatibility check (pass/fail), register new version, emit a special SchemaChangeEvent. Include decision diamonds for compatibility results.",
      "type": "flowchart",
      "relevant_sections": [
        "schema-registry-evolution",
        "interactions-flow"
      ]
    }
  ]
}