{
  "types": {
    "ValidationResult": "fields: expectation_type str, success bool, result dict, meta dict, timestamp datetime",
    "BaseExpectation": "fields: configuration dict, abstract validate method",
    "ExpectationSuite": "fields: name str, expectations list, created_at datetime",
    "SuiteResult": "fields: suite_name str, results list, success bool, meta dict",
    "QualityJSONEncoder": "custom JSON encoder for framework objects",
    "ProfileResult": "fields: dataset_name str, profile_data dict, created_at datetime, metadata dict",
    "DataProfiler": "fields: sample_size int, enable_sampling bool, statistics_cache dict",
    "StreamingStatistics": "fields: count int, mean float, m2 float, min_val float, max_val float",
    "ReservoirSample": "fields: sample_size int, reservoir list, items_seen int",
    "StreamingHistogram": "fields: max_bins int, bins dict, total_count int",
    "AnomalyResult": "fields: metric_name str, value float, is_anomaly bool, anomaly_score float, threshold_used float, detection_method str, timestamp datetime, context dict",
    "BaseAnomalyDetector": "fields: config dict, detection_method str",
    "StatisticalSummary": "fields: count int, mean float, std float, min_val float, max_val float, q1 float, q3 float, iqr float, percentiles dict, created_at datetime",
    "RollingStatistics": "fields: window_size int, decay_factor float, values list, weights list",
    "BaselineConfiguration": "fields: minimum_samples int, minimum_days int, update_frequency_hours int, seasonal_detection bool, decay_factor float",
    "BaselineManager": "fields: config BaselineConfiguration, baselines dict, rolling_stats dict, seasonal_patterns dict, last_update dict",
    "ContractDefinition": "fields: name str, version str, description str, schema dict, quality_rules dict, semantic_rules dict, sla_requirements dict, compatibility_rules dict, created_at datetime, created_by str, tags list, metadata dict",
    "ContractValidationResult": "fields: contract_name str, contract_version str, validation_status str, field_results dict, error_summary dict, validation_timestamp datetime, data_sample_info dict, suggested_actions list",
    "SchemaChange": "fields: change_type str, field_path str, old_definition dict, new_definition dict, compatibility CompatibilityType, impact_description str, migration_guidance str",
    "ContractEvolutionAnalysis": "fields: source_version str, target_version str, changes list, overall_compatibility CompatibilityType, breaking_changes_count int, recommended_version_bump str, consumer_impact_summary str",
    "CompatibilityType": "enum: BACKWARD_COMPATIBLE, FORWARD_COMPATIBLE, FULLY_COMPATIBLE, BREAKING_CHANGE",
    "ExecutionContext": "fields: session_id str, start_time datetime, dataset_reference str, estimated_rows Optional[int], detected_schema dict, completed_components set, running_components set, failed_components set, component_dependencies dict, component_results dict, shared_sample_data Optional[Any], performance_metrics dict",
    "WorkflowConfig": "fields: enable_parallel_execution bool, max_concurrent_components int, timeout_seconds int, fail_fast_on_critical_errors bool, enable_streaming_results bool",
    "WorkflowResult": "fields: session_id str, success bool, total_execution_time float, component_results dict, error_summary Optional[dict], quality_score Optional[float]",
    "AggregatedResult": "fields: session_id str, overall_quality_score float, executive_summary dict, technical_details dict, identified_correlations list, recommendations list, component_contributions dict",
    "ValidationRequest": "fields: session_id str, dataset_reference str, component_config dict, execution_mode str, timeout_seconds int, dependencies list",
    "ComponentResult": "fields: session_id str, component_name str, success_status bool, execution_time float, result_data dict, error_details dict, performance_metrics dict",
    "ProgressUpdate": "fields: session_id str, component_name str, progress_percentage float, estimated_completion datetime, current_operation str",
    "ContextUpdate": "fields: session_id str, update_type str, update_data dict, propagate_to_components list, update_priority int",
    "DataQualityError": "message str, category ErrorCategory, severity ErrorSeverity, component str, context dict, recoverable bool, suggested_actions list, timestamp datetime, traceback str",
    "ErrorCategory": "enum: VALIDATION_FAILURE, SYSTEM_ERROR, DATA_FORMAT_ERROR, RESOURCE_EXHAUSTION, CONFIGURATION_ERROR",
    "ErrorSeverity": "enum: LOW, MEDIUM, HIGH, CRITICAL",
    "ExpectationError": "inherits DataQualityError with expectation_name str",
    "ResourceExhaustionError": "inherits DataQualityError with resource_type str, current_usage float",
    "ResourceMonitor": "fields: memory_threshold float, cpu_threshold float, disk_threshold float, monitoring bool, callbacks dict, monitor_thread threading.Thread",
    "FailureDetector": "failure_history list, failure_patterns dict, component_health dict",
    "DegradationManager": "degradation_levels dict, resource_constraints dict",
    "ExpectationTestCase": "fields: name str, data DataFrame, config dict, expected_success bool, expected_metadata_checks list",
    "ExpectationTestSuite": "fields: expectation_class type, test_cases list",
    "PerformanceMonitor": "fields: measurements dict, memory_peaks dict, process psutil.Process",
    "StatisticalTestUtils": "utility class for statistical test validation",
    "DatasetGenerator": "utility class for synthetic test data generation",
    "ExpectationDebugger": "debugging utilities for expectation validation",
    "DistributedExecutionContext": "fields: cluster_session_id str, coordinator_address str, worker_assignments dict, global_schema dict, partition_results dict, global_baselines dict, coordination_state str",
    "SparkDataQualityRunner": "fields: spark SparkSession, logger Logger, _expectation_configs BroadcastVariable, _baseline_data BroadcastVariable, _schema_contracts BroadcastVariable",
    "DaskDataQualityRunner": "fields: client Client, logger Logger",
    "StreamingDataQualityProcessor": "fields: bootstrap_servers list, consumer_group str, logger Logger, streaming_stats dict, anomaly_baselines dict, expectation_cache dict, processing_thread Thread, stop_processing Event",
    "StreamingStatisticsCollector": "fields: decay_factor float, statistics dict",
    "FrameworkGlossary": "terminology management and validation utility"
  },
  "methods": {
    "validate(df) -> ValidationResult": "core expectation validation method",
    "to_dict() -> dict": "serialize error to dictionary",
    "add_expectation(expectation) -> None": "add expectation to suite",
    "run(df) -> SuiteResult": "execute all expectations in suite",
    "smart_sample(df, target_size, method) -> (DataFrame, dict)": "intelligent dataset sampling",
    "serialize_to_json(obj, filepath) -> str": "JSON serialization utility",
    "serialize_to_yaml(obj, filepath) -> str": "YAML serialization utility",
    "profile_dataset(df, dataset_name) -> ProfileResult": "main profiling method that analyzes entire dataset",
    "smart_sample(data_iterator, target_size, method) -> (list, dict)": "intelligent sampling function",
    "_profile_column(series, column_name) -> dict": "individual column analysis",
    "_infer_column_type(series) -> dict": "data type inference",
    "_compute_correlations(df) -> dict": "correlation analysis",
    "_analyze_missing_patterns(df) -> dict": "missing value pattern analysis",
    "get_quality_summary() -> dict": "extract quality summary from profile results",
    "detect_anomalies(metric_name, values, timestamps) -> List[AnomalyResult]": "core anomaly detection method",
    "add_measurement(metric_name, value, timestamp) -> None": "add measurement to baseline",
    "get_baseline(metric_name) -> StatisticalSummary": "retrieve baseline statistics",
    "get_seasonal_adjustment(metric_name, timestamp) -> float": "get seasonal adjustment factor",
    "_compute_z_score(value, mean, std) -> float": "compute Z-score for value",
    "_compute_iqr_score(value, q1, q3, iqr) -> float": "compute IQR-based anomaly score",
    "perform_ks_test(reference_data, current_data) -> Tuple[float, float]": "perform Kolmogorov-Smirnov test",
    "detect_seasonality(values, timestamps) -> Dict[str, Any]": "detect seasonal patterns in data",
    "register_contract(contract) -> bool": "Register new contract version with validation",
    "get_contract(name, version) -> ContractDefinition": "Retrieve contract by name and version",
    "validate_against_contract(data, contract_name, version) -> ContractValidationResult": "validate data against contract",
    "analyze_evolution(old_schema, new_schema) -> ContractEvolutionAnalysis": "analyze schema changes for compatibility",
    "register_consumer(team, contract_name, version) -> bool": "Register team as consumer of contract",
    "create_context(dataset_reference, **kwargs) -> str": "create new execution context and return session ID",
    "get_context(session_id) -> Optional[ExecutionContext]": "retrieve execution context by session ID",
    "mark_component_running(session_id, component_name) -> None": "mark component as currently executing",
    "mark_component_complete(session_id, component_name, result) -> None": "mark component as successfully completed",
    "mark_component_failed(session_id, component_name, error) -> None": "mark component as failed",
    "can_execute_component(session_id, component_name) -> bool": "check if component dependencies are satisfied",
    "register_component(name, component) -> None": "register a component for workflow execution",
    "execute_workflow(dataset, session_id) -> WorkflowResult": "execute complete validation workflow",
    "_execute_component(component_name, component, dataset, session_id) -> Any": "execute single component with error handling",
    "_build_execution_plan(session_id) -> List[List[str]]": "build execution plan respecting component dependencies",
    "get_execution_status(session_id) -> Dict[str, Any]": "get current execution status for monitoring",
    "aggregate_results(session_id) -> AggregatedResult": "combine all component results into unified assessment",
    "_identify_correlations(component_results) -> List[Dict]": "identify patterns that span multiple components",
    "_calculate_quality_score(component_results) -> float": "calculate weighted overall quality score",
    "_generate_recommendations(correlations, quality_score) -> List[str]": "generate actionable recommendations based on findings",
    "register_callback(event_type, callback) -> None": "register resource monitoring callback",
    "start_monitoring(interval) -> None": "begin resource monitoring",
    "stop_monitoring() -> None": "stop resource monitoring",
    "get_current_usage() -> dict": "get current resource usage",
    "detect_expectation_failures(results) -> List[DataQualityError]": "detect systematic expectation failures",
    "detect_profiling_failures(results) -> List[DataQualityError]": "identify profiling operation failures",
    "detect_anomaly_detection_failures(results) -> List[DataQualityError]": "identify anomaly detection failures",
    "analyze_failure_patterns() -> dict": "analyze historical failure patterns",
    "assess_degradation_needs(usage, errors) -> dict": "determine degradation levels needed",
    "implement_degradation(component, level) -> dict": "apply degradation configuration",
    "monitor_recovery_conditions() -> dict": "check recovery readiness",
    "assert_statistics_equal(actual, expected, tolerance) -> None": "assert statistical values equal within tolerance",
    "assert_distribution_similar(actual_data, expected_data, alpha) -> None": "assert distributions similar using KS test",
    "generate_test_distribution(dist_type, size, **params) -> Tuple": "generate test data with known properties",
    "create_clean_dataset(rows, cols) -> Tuple": "create dataset with no quality issues",
    "create_messy_dataset(rows, missing_rate, type_inconsistency_rate) -> Tuple": "create dataset with realistic quality issues",
    "add_test_case(test_case) -> None": "add test case to expectation test suite",
    "run_all_tests() -> Dict": "execute all expectation test cases",
    "measure_execution(operation_name)": "context manager for performance measurement",
    "assert_performance_bounds(operation_name, max_time, max_memory) -> None": "assert operation performance within bounds",
    "analyze_expectation_failure(validation_result) -> Dict": "provide detailed analysis of expectation failure",
    "validate_expectation_config(expectation, sample_data) -> Dict": "validate expectation configuration against sample data",
    "broadcast_configurations(expectations, baselines, contracts) -> None": "broadcast validation configurations to cluster nodes",
    "run_distributed_expectations(df, suite_name) -> DataFrame": "execute expectation suite across Spark DataFrame partitions",
    "profile_distributed_dataset(df, sample_size) -> dict": "generate statistical profile using distributed sampling",
    "detect_distributed_anomalies(df, metric_columns) -> DataFrame": "perform anomaly detection across distributed dataset",
    "create_validation_graph(data_path, expectations, profiling_config) -> dict": "build Dask computation graph for quality validation",
    "execute_distributed_validation(computation_graph) -> dict": "execute validation computation graph with monitoring",
    "register_expectation_processor(expectation_id, processor_func) -> None": "register expectation validation function for streaming",
    "start_stream_processing(input_topic, output_topic, error_topic) -> None": "start continuous stream processing of quality validation",
    "stop_stream_processing() -> None": "stop stream processing gracefully",
    "process_record(record) -> dict": "process single record through quality validation pipeline",
    "update_streaming_baseline(metric_name, value, timestamp) -> None": "update anomaly detection baseline with streaming data",
    "add_measurement(metric_name, value) -> None": "add measurement to streaming statistics",
    "get_statistics_summary(metric_name) -> dict": "get current statistics summary for metric",
    "create_kafka_topics(bootstrap_servers, topic_names, num_partitions) -> None": "create Kafka topics for streaming pipeline",
    "setup_streaming_monitoring(processor) -> None": "setup monitoring and alerting for streaming quality",
    "get_definition(term) -> Optional[Dict]": "retrieve term definition and context",
    "validate_terminology_usage(text) -> List[Dict]": "check terminology consistency in text",
    "generate_terminology_report(output_path) -> None": "create comprehensive terminology documentation",
    "validate_term_usage(term) -> Dict": "validate individual term against standards"
  },
  "constants": {
    "DEFAULT_SAMPLE_SIZE": "10000 rows for profiling operations",
    "SAMPLING_METHODS": "random, stratified, systematic sampling approaches",
    "DEFAULT_Z_SCORE_THRESHOLD": "3.0 for conservative anomaly detection",
    "DEFAULT_IQR_MULTIPLIER": "1.5 for standard outlier detection",
    "MINIMUM_BASELINE_SAMPLES": "1000 samples minimum for stable baselines",
    "DECAY_FACTOR": "0.95 for exponential decay weighting",
    "BACKWARD_COMPATIBLE": "schema change maintains backward compatibility",
    "FORWARD_COMPATIBLE": "schema change maintains forward compatibility",
    "BREAKING_CHANGE": "schema change breaks compatibility",
    "DEFAULT_MEMORY_THRESHOLD": "0.8 for memory pressure detection",
    "DEFAULT_CPU_THRESHOLD": "0.9 for CPU pressure detection",
    "CRITICAL_THRESHOLD": "0.95 for critical resource usage",
    "MONITORING_INTERVAL": "5.0 seconds for resource monitoring",
    "FAILURE_PATTERN_WINDOW": "time window for failure pattern analysis",
    "DEFAULT_DECAY_FACTOR": "0.95 for exponential decay weighting",
    "STREAMING_BATCH_SIZE": "100 records for micro-batch processing",
    "COORDINATION_TIMEOUT": "30 seconds for distributed coordination",
    "BASELINE_BOOTSTRAP_SAMPLES": "1000 samples for baseline establishment",
    "PREFERRED_TERMS": "mapping of deprecated terms to preferred alternatives",
    "DEPRECATED_TERMS": "list of terms no longer recommended for use"
  },
  "terms": {
    "expectation": "declarative data quality rule that can be validated against datasets",
    "validation result": "outcome of executing an expectation with pass/fail status and detailed metrics",
    "expectation suite": "collection of related expectations executed together as logical unit",
    "data profiling": "statistical analysis to understand dataset characteristics",
    "anomaly detection": "identification of data points that deviate significantly from expected patterns",
    "data contract": "formal specification of data structure and quality requirements",
    "schema validation": "verification that data conforms to expected structure and types",
    "data drift": "change in data distribution over time",
    "quality control": "systematic approach to ensuring data meets specified standards",
    "type inference": "automatic detection of column data types",
    "correlation analysis": "identification of relationships between columns",
    "missing value patterns": "systematic analysis of data incompleteness",
    "streaming statistics": "incremental computation maintaining bounded memory usage",
    "reservoir sampling": "fixed-size random sampling technique for streaming data",
    "statistical significance": "probability that observed patterns are not due to chance",
    "statistical baseline": "established statistical profile used to identify deviations",
    "Z-score analysis": "statistical method measuring standard deviations from mean",
    "IQR method": "interquartile range based outlier detection",
    "distribution comparison": "statistical testing to detect distribution changes",
    "seasonal adjustment": "compensation for cyclical patterns in data",
    "volume anomaly": "unexpected changes in data quantity or arrival patterns",
    "schema drift": "changes in data structure over time",
    "freshness monitoring": "tracking data delivery timeliness",
    "schema evolution": "process of changing data structure over time",
    "contract validation": "verification that data conforms to contract specifications",
    "breaking change": "modification that breaks compatibility with existing consumers",
    "semantic versioning": "version numbering scheme that communicates compatibility",
    "compatibility analysis": "assessment of schema change impact on existing integrations",
    "contract registry": "centralized storage for contract definitions and metadata",
    "producer-consumer agreement": "formal relationship between data providers and consumers",
    "workflow orchestration": "coordination of execution sequence from data ingestion through quality reporting",
    "execution context": "shared memory space where components coordinate activities",
    "result aggregation": "process of combining individual component results into coherent quality assessments",
    "component correlation": "identification of relationships between findings from different validation components",
    "quality score": "weighted numeric assessment of overall data quality across all components",
    "execution plan": "dependency-ordered sequence of component execution phases",
    "streaming results": "real-time result updates as validation progresses",
    "correlation pattern": "known signature of quality issues that manifest across multiple components",
    "conflict resolution": "logic for handling contradictory findings from different components",
    "temporal context": "consideration of time relationships when aggregating validation results",
    "failure mode analysis": "systematic examination of potential failure scenarios and detection strategies",
    "graceful degradation": "maintaining reduced functionality when components fail",
    "component isolation": "preventing failures from propagating across component boundaries",
    "error context preservation": "maintaining detailed error information while continuing operations",
    "progressive capability reduction": "prioritized reduction of features under resource constraints",
    "recovery coordination": "managing restoration of functionality across multiple components",
    "resource arbitration": "managing competing resource demands during recovery",
    "circuit breaker pattern": "failure isolation mechanism preventing cascading errors",
    "false positive cascade": "feedback loop where initial anomalies trigger more false positives",
    "quality score adjustment": "modifying quality scores to reflect reduced validation coverage",
    "baseline learning": "process of establishing statistical baseline from historical data",
    "statistical tolerance": "acceptable variation range for statistical test comparisons",
    "performance bounds": "maximum acceptable resource usage limits for operations",
    "integration testing": "testing component interactions and data flow",
    "milestone checkpoint": "concrete validation criteria for milestone completion",
    "synthetic data": "artificially generated data with known characteristics for testing",
    "test isolation": "ensuring tests don't interfere with each other through shared state",
    "performance regression": "deterioration in system performance over time",
    "performance monitoring": "systematic tracking of system resource usage and execution metrics",
    "statistical validation": "verification of statistical computation accuracy and result consistency",
    "memory profiling": "detailed analysis of memory allocation patterns and usage optimization",
    "baseline establishment": "process of creating statistical baseline from historical data for anomaly detection",
    "false positive": "incorrect identification of normal data patterns as anomalies",
    "computational complexity": "algorithmic efficiency analysis for data processing operations",
    "resource exhaustion": "system failure due to insufficient memory, CPU, or disk resources",
    "threshold tuning": "optimization of anomaly detection sensitivity parameters",
    "performance bottleneck": "system component that limits overall processing performance",
    "distributed processing": "scaling data quality validation across multiple compute nodes",
    "stream processing": "continuous validation of data records as they arrive",
    "coordinator-worker pattern": "distributed architecture with central coordination and worker delegation",
    "broadcast variables": "shared read-only data distributed to all cluster nodes",
    "task graph": "dependency-ordered computation plan for distributed execution",
    "micro-batching": "processing small groups of records together for efficiency",
    "baseline bootstrapping": "rapid establishment of anomaly detection baselines from initial stream data",
    "partition-aware execution": "validation logic that respects data partitioning boundaries",
    "hierarchical correlation analysis": "multi-level approach to detecting correlations in distributed data",
    "adaptive baseline": "anomaly detection baseline that evolves with changing data patterns",
    "exponential decay": "weighting scheme that emphasizes recent data over historical data",
    "lazy evaluation": "deferred computation strategy that optimizes distributed processing",
    "asynchronous result publishing": "non-blocking pattern for publishing validation results",
    "fault tolerance": "system ability to continue operating despite component failures",
    "data locality": "optimization principle minimizing data movement in distributed systems",
    "backpressure handling": "mechanism for managing processing rate differences between components",
    "windowing strategies": "approaches for grouping streaming data into analysis windows"
  }
}