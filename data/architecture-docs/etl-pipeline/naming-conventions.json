{
  "types": {
    "PipelineDefinition": "fields: id str, name str, description str, schedule str, tasks List[TaskDefinition], parameters dict, created_at datetime, version int",
    "TaskDefinition": "fields: id str, name str, type str, config dict, dependencies List[str], retry_policy RetryPolicy, timeout_seconds int",
    "RetryPolicy": "fields: max_attempts int, backoff_seconds int, exponential_backoff bool, retry_on_error_types List[str]",
    "TaskState": "enum: PENDING, WAITING, QUEUED, RUNNING, SUCCESS, FAILED, RETRYING, CANCELLED, SKIPPED",
    "TaskEvent": "enum: DEPENDENCIES_MET, EXECUTION_STARTED, EXECUTION_COMPLETED, EXECUTION_FAILED, RETRY_SCHEDULED, MAX_RETRIES_EXCEEDED, CANCELLED_BY_USER, UPSTREAM_FAILED",
    "TaskExecution": "fields: task_id str, pipeline_run_id str, state TaskState, attempt_count int, started_at datetime, completed_at datetime, error_message str, logs List[str], metrics Dict[str,float]",
    "CycleDetectionResult": "fields: has_cycle bool, cycle_path List[str]",
    "NodeColor": "enum: WHITE, GRAY, BLACK",
    "ExecutionPlan": "fields: execution_levels List[List[str]], critical_path List[str], estimated_duration int, resource_requirements Dict[str,int]",
    "WatermarkEntry": "fields: pipeline_id str, source_table str, watermark_column str, watermark_value Any, watermark_type str, created_at datetime, updated_at datetime",
    "SourceConnector": "abstract base class for data extraction",
    "DatabaseSourceConnector": "relational database source connector",
    "APISourceConnector": "REST API source connector",
    "ConnectionPoolManager": "database connection pool management",
    "WatermarkManager": "watermark persistence and updates",
    "DataType": "enum with STRING, INTEGER, FLOAT, DECIMAL, BOOLEAN, DATE, TIMESTAMP, JSON, BINARY",
    "ValidationResult": "fields: is_valid bool, errors List[str], warnings List[str]",
    "TypeConverter": "handles type conversion between different representations",
    "FieldDefinition": "fields: name str, data_type DataType, nullable bool, default_value Any, constraints List[str]",
    "SchemaDefinition": "fields: name str, version int, fields Dict[str,FieldDefinition]",
    "ScheduleConfig": "configuration for pipeline scheduling",
    "ScheduleStatus": "current state of pipeline schedule",
    "LoadResult": "result of data loading operation",
    "DataStream": "stream of data records",
    "MessageEvent": "fields: event_id str, timestamp datetime, source_component str, event_type str, pipeline_id str, run_id str, task_id Optional[str], payload Dict[str,Any], correlation_id Optional[str]",
    "AssetReference": "fields: asset_type str, asset_name str, asset_schema str, location_uri str, partition_info dict, column_lineage List[ColumnLineage]",
    "ErrorType": "enum: NETWORK_ERROR, TIMEOUT_ERROR, DATA_QUALITY_ERROR, RESOURCE_EXHAUSTION, PERMISSION_ERROR",
    "CircuitState": "enum: CLOSED, OPEN, HALF_OPEN",
    "DeadLetterMessage": "fields: task_id str, pipeline_run_id str, error_type str, error_message str, retry_count int, original_payload Dict[str, Any], failed_at datetime",
    "HealthStatus": "enum: HEALTHY, DEGRADED, UNHEALTHY",
    "HealthCheckResult": "fields: component str, status HealthStatus, message str, metrics Dict[str,float], timestamp float",
    "LogEntry": "fields: timestamp datetime, level str, component str, pipeline_id Optional[str], run_id Optional[str], task_id Optional[str], message str, raw_line str",
    "PerformanceAnalyzer": "class for real-time performance monitoring",
    "SystemHealthChecker": "class for component health verification",
    "LogCorrelator": "class for cross-component log correlation",
    "ContainerTaskSpec": "fields: task_id str, image str, command list, environment Dict[str, str], resource_limits Dict[str, str], volumes Dict[str, str], timeout_seconds int",
    "StreamCheckpoint": "fields: task_id str, stream_position str, processing_state Dict[str, Any], timestamp datetime",
    "ResourceRequirements": "fields: cpu_cores float, memory_gb float, gpu_count int, storage_gb float",
    "ExecutionTargetInfo": "fields: target_id str, target_type ExecutionTarget, available_resources ResourceRequirements, current_utilization ResourceRequirements",
    "ScalingMetrics": "fields: pipeline_queue_length int, average_task_wait_time float, resource_utilization float, error_rate float, timestamp datetime",
    "ScalingDecision": "fields: action str, target_instances int, reason str",
    "ExecutionTarget": "enum: LOCAL, CONTAINER, SPARK, SERVERLESS",
    "StreamTask": "abstract base class for stream processing tasks",
    "WindowedAggregationTask": "stream task for windowed aggregations",
    "ContainerTaskExecutor": "executes tasks in Kubernetes containers",
    "DistributedExecutionCoordinator": "coordinates task execution across targets",
    "AutoScalingController": "automatically scales system resources"
  },
  "methods": {
    "get_pipeline(pipeline_id) -> Optional[PipelineDefinition]": "retrieve pipeline definition by ID",
    "validate_pipeline(pipeline) -> List[str]": "Validate pipeline definition and return errors",
    "transition(execution, event, error_message) -> bool": "attempt state transition based on event",
    "extract(connection, query) -> DataStream": "extract data from source system",
    "load(connection, data, target) -> LoadResult": "load data stream to destination",
    "detect_cycles_dfs(adjacency_list) -> CycleDetectionResult": "detect cycles using DFS with three-color algorithm",
    "topological_sort_kahns(adjacency_list) -> List[List[str]]": "perform topological sort returning parallel execution levels",
    "calculate_critical_path(adjacency_list, task_durations) -> Tuple[List[str], int]": "find longest path through DAG",
    "parse_yaml_file(file_path) -> PipelineDefinition": "parse pipeline from YAML file",
    "parse_python_definition(definition_func) -> PipelineDefinition": "parse pipeline from Python function",
    "create_execution_plan(pipeline, task_durations) -> ExecutionPlan": "Create optimized execution plan with parallelization",
    "extract(query, options) -> Iterator[Dict[str, Any]]": "extract data from source system",
    "load(data_stream, target, options) -> LoadResult": "load data stream to destination",
    "get_schema(table_name) -> Dict[str, str]": "retrieve schema information",
    "get_watermark(pipeline_id, source_table, watermark_column) -> Optional[Any]": "retrieve current watermark value",
    "update_watermark(pipeline_id, source_table, watermark_column, watermark_value) -> bool": "atomically update watermark",
    "_build_incremental_query(base_query, watermark_info) -> str": "modify query for incremental extraction",
    "_handle_pagination(response) -> Optional[Dict[str, Any]]": "determine next page parameters",
    "convert(value, from_type, to_type)": "convert value between data types",
    "validate_record(record) -> ValidationResult": "validate single record against schema",
    "execute_transformation(template_name, parameters, connection_name) -> Iterator": "execute SQL transformation with parameters",
    "register_function(name, func)": "register UDF for execution",
    "execute_function(function_name, data, execution_mode) -> Iterator": "execute UDF on input data",
    "validate_data_stream(data_stream, schema_name, schema_version) -> Iterator": "validate data stream against schema",
    "check_schema_compatibility(old_schema, new_schema) -> ValidationResult": "check compatibility between schema versions",
    "register_schedule(pipeline_id, schedule_config) -> bool": "Register new pipeline schedule",
    "update_schedule(pipeline_id, schedule_config) -> bool": "Modify existing pipeline schedule",
    "pause_schedule(pipeline_id, reason) -> bool": "Temporarily disable pipeline execution",
    "resume_schedule(pipeline_id) -> bool": "Re-enable paused pipeline schedule",
    "get_next_run_time(pipeline_id) -> Optional[datetime]": "Calculate next scheduled execution",
    "get_schedule_status(pipeline_id) -> ScheduleStatus": "Retrieve current schedule state",
    "publish_event(topic, event)": "publish event to message broker topic",
    "subscribe_to_topic(topic, handler)": "register event handler for topic",
    "execute_pipeline(pipeline_id, run_id, parameters)": "coordinate complete pipeline execution",
    "transition(execution, event, error_message)": "attempt state transition based on event",
    "collect_sql_lineage(sql_query, input_tables, output_table, pipeline_run_id, task_id)": "extract lineage from SQL transformations",
    "collect_udf_lineage(function_name, input_schema, output_schema, pipeline_run_id, task_id)": "capture UDF transformation lineage",
    "get_data_lineage(asset_name, direction, max_depth)": "query lineage relationships for data asset",
    "get_pipeline(pipeline_id)": "retrieve pipeline definition by ID",
    "validate_pipeline(pipeline)": "validate pipeline definition and return errors",
    "should_retry(error_type, attempt_count) -> bool": "determine if operation should be retried based on policy",
    "calculate_delay(attempt_count, previous_delay) -> int": "calculate delay before next retry attempt with jitter",
    "execute_with_retry(operation, error_classifier, *args, **kwargs)": "execute operation with retry logic based on policy",
    "classify_error(exception) -> str": "classify exception into error type category for retry decision",
    "save_checkpoint(task_id, pipeline_run_id, checkpoint_data) -> bool": "save task checkpoint data for recovery purposes",
    "load_checkpoint(task_id, pipeline_run_id) -> Optional[Dict[str, Any]]": "load most recent checkpoint data for task recovery",
    "send_to_dead_letter_queue(message) -> bool": "send failed task to appropriate dead letter queue",
    "requeue_message(message_id, new_retry_policy) -> bool": "move message from dead letter queue back to normal processing",
    "check_system_resources() -> HealthCheckResult": "check basic system resource availability",
    "check_database_connectivity() -> HealthCheckResult": "verify database connection and basic operations",
    "parse_log_entry(log_line) -> Optional[LogEntry]": "parse structured log entry from log line",
    "correlate_pipeline_logs(log_entries, run_id) -> Dict[str, List[LogEntry]]": "group log entries by component for specific pipeline run",
    "find_error_context(log_entries, error_entry, context_seconds) -> List[LogEntry]": "find log entries around an error for context",
    "start_monitoring(pipeline_run_id)": "begin performance monitoring for pipeline run",
    "stop_monitoring() -> Dict[str, List[float]]": "stop monitoring and return collected samples",
    "execute_task(task_spec) -> Dict[str, Any]": "execute task in container environment",
    "process_event(event) -> Optional[Dict[str, Any]]": "process single stream event",
    "create_checkpoint() -> StreamCheckpoint": "create processing state checkpoint",
    "restore_from_checkpoint(checkpoint)": "restore from saved checkpoint",
    "register_execution_target(target_info)": "register new execution target",
    "select_execution_target(task_id, requirements) -> Optional[str]": "select optimal execution target",
    "estimate_task_resources(task_definition) -> ResourceRequirements": "estimate resource requirements",
    "determine_execution_target_type(task_definition, requirements) -> ExecutionTarget": "determine target type",
    "collect_metrics() -> ScalingMetrics": "collect scaling decision metrics",
    "should_scale_up(metrics) -> bool": "determine scale up necessity",
    "should_scale_down(metrics) -> bool": "determine scale down necessity",
    "execute_scaling_decision(decision)": "execute scaling action",
    "extract(connection, query)": "extract data from source system",
    "load(connection, data, target)": "load data stream to destination",
    "detect_cycles_dfs(adjacency_list)": "detect cycles using DFS with three-color algorithm",
    "topological_sort_kahns(adjacency_list)": "perform topological sort returning parallel execution levels",
    "get_watermark(pipeline_id, source_table, watermark_column)": "retrieve current watermark value",
    "update_watermark(pipeline_id, source_table, watermark_column, watermark_value)": "atomically update watermark",
    "validate_record(record)": "validate single record against schema",
    "should_retry(error_type, attempt_count)": "determine if operation should be retried",
    "calculate_delay(attempt_count, previous_delay)": "calculate delay before next retry attempt with jitter",
    "check_system_resources()": "check basic system resource availability",
    "parse_log_entry(log_line)": "parse structured log entry from log line"
  },
  "constants": {
    "TRANSITIONS": "state machine transition mapping between TaskState and TaskEvent",
    "DAG": "Directed Acyclic Graph",
    "CDC": "Change Data Capture",
    "DataType.STRING": "string data type",
    "DataType.INTEGER": "integer data type",
    "DataType.FLOAT": "float data type",
    "ValidationResult": "validation operation result",
    "ALLOW_CONCURRENT": "execution policy allowing parallel pipeline runs",
    "SKIP_ON_RUNNING": "execution policy skipping when pipeline already running",
    "CANCEL_PREVIOUS": "execution policy canceling previous runs",
    "QUEUE_SEQUENTIAL": "execution policy queueing sequential execution",
    "HEALTHY": "healthy system status",
    "DEGRADED": "degraded system status",
    "UNHEALTHY": "unhealthy system status",
    "LOCAL": "local execution target",
    "CONTAINER": "container execution target",
    "SPARK": "Spark execution target",
    "SERVERLESS": "serverless execution target",
    "PENDING": "initial task state",
    "WAITING": "task waiting for dependencies",
    "QUEUED": "task ready but waiting for resources",
    "RUNNING": "task actively executing",
    "SUCCESS": "task completed successfully",
    "FAILED": "task completed with errors",
    "RETRYING": "task scheduled for retry",
    "CANCELLED": "task execution cancelled",
    "SKIPPED": "task intentionally bypassed",
    "STRING": "variable-length text data type",
    "INTEGER": "whole number data type",
    "FLOAT": "floating-point numeric data type",
    "DECIMAL": "fixed-precision numeric data type",
    "BOOLEAN": "binary true/false data type",
    "DATE": "calendar date without time",
    "TIMESTAMP": "date and time data type",
    "JSON": "semi-structured nested data",
    "BINARY": "raw binary data type"
  },
  "terms": {
    "ETL": "Extract Transform Load data processing pattern",
    "DAG": "Directed Acyclic Graph for task dependencies",
    "idempotent": "operations producing same result when executed multiple times",
    "topological sort": "algorithm for determining task execution order",
    "checkpoint": "saving intermediate state for failure recovery",
    "lineage": "tracking data provenance and transformation history",
    "watermark": "high-water mark for tracking processed data",
    "adjacency list": "graph representation mapping nodes to neighbors",
    "cycle detection": "algorithm to find circular dependencies",
    "critical path": "longest dependency chain determining minimum execution time",
    "execution levels": "groups of tasks that can run in parallel",
    "in-degree": "number of incoming dependencies for a task",
    "watermarking": "tracking high-water mark for incremental extraction",
    "cursor-based pagination": "using opaque tokens to track pagination position",
    "change data capture": "real-time stream of database changes",
    "bulk loading": "optimized batch insertion technique",
    "upsert": "insert or update operation for handling conflicts",
    "schema mapping": "translation between source and destination data types",
    "connection pooling": "reusing database connections for performance",
    "incremental loading": "extracting only changed data since last run",
    "lookback window": "time buffer to handle clock skew",
    "staging table": "temporary table for atomic bulk loading",
    "schema evolution": "systematic management of schema changes over time",
    "type coercion": "automatic conversion between compatible data types",
    "UDF": "User-Defined Function for custom transformation logic",
    "template rendering": "applying parameters to templates for final output",
    "subprocess isolation": "running code in separate process for safety",
    "schema registry": "centralized catalog of schema definitions",
    "validation pipeline": "series of validation stages for data",
    "null semantics": "rules for handling null/missing values",
    "compatibility checking": "verification that changes don't break consumers",
    "orchestration": "coordination of pipeline execution with scheduling",
    "scheduler": "component triggering pipeline execution",
    "execution engine": "component running pipeline tasks with parallelization",
    "state machine": "formal model for task state transitions",
    "resource allocation": "assignment of compute resources to tasks",
    "metrics collection": "gathering performance and business metrics",
    "data lineage": "comprehensive tracking of data provenance",
    "alert suppression": "preventing duplicate alerts during outages",
    "exponential backoff": "retry strategy with increasing delays",
    "level-based parallelization": "executing tasks at same DAG level simultaneously",
    "message broker": "asynchronous communication system",
    "dependency resolution": "determining task execution order from prerequisites",
    "circuit breaker": "protection against cascading failures",
    "dead letter queue": "storage for messages that cannot be processed",
    "jitter": "randomness added to retry timing",
    "saga pattern": "breaking long transactions into compensatable steps",
    "two-phase commit": "distributed transaction protocol ensuring atomicity",
    "compensation transaction": "reverse operation undoing transaction effects",
    "health check": "verification of component operational status",
    "log correlation": "connecting related log entries across components",
    "performance profiling": "analysis of resource usage and bottlenecks",
    "symptom-cause mapping": "structured approach to diagnosing issues",
    "interactive debugging": "real-time investigation of system behavior",
    "resource monitoring": "tracking system resource utilization",
    "error context": "log entries surrounding error for diagnosis",
    "distributed execution": "running tasks across multiple machines",
    "horizontal scaling": "adding more instances to handle load",
    "auto-scaling": "automatically adjusting resource allocation",
    "cloud-native": "designed for cloud deployment patterns",
    "stream processing": "continuous processing of data streams",
    "machine learning pipeline": "ML workflow with specialized requirements",
    "real-time processing": "sub-second response time processing",
    "event-driven orchestration": "triggering pipelines from external events",
    "container orchestration": "managing containerized application deployment",
    "serverless integration": "using cloud functions for lightweight tasks",
    "checkpoint recovery": "resuming from saved processing state",
    "leader election": "choosing primary instance in cluster",
    "priority scheduling": "executing high-priority tasks first"
  }
}