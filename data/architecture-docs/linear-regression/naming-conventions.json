{
  "types": {
    "SimpleLinearRegression": "slope_ float, intercept_ float, is_fitted_ bool",
    "GradientDescentRegression": "slope_ float, intercept_ float, is_fitted_ bool, cost_history_ List[float], parameter_history_ List[Tuple[float, float]], learning_rate float, max_iterations int, tolerance float",
    "np.ndarray": "NumPy array for numerical data",
    "Dataset": "features np.ndarray, targets np.ndarray, feature_names List[str], n_samples int, n_features int, is_normalized bool, normalization_stats Dict[str, np.ndarray]",
    "ModelParameters": "slope_: float, intercept_: float, weights_: np.ndarray, is_fitted_: bool, fitting_method_: str, regularization_strength_: float, feature_count_: int",
    "TrainingHistory": "cost_history_ List[float], parameter_history_ List[Tuple[float, float]], weight_history_ List[np.ndarray], gradient_norms_ List[float], learning_rate float, max_iterations int, tolerance float, converged_ bool, final_iteration_ int, convergence_reason_ str",
    "PredictionResult": "predictions np.ndarray, residuals np.ndarray, confidence_intervals np.ndarray, r_squared float, mean_squared_error float, mean_absolute_error float, input_shape Tuple[int, int], model_type str",
    "DataHandler": "tolerance: float",
    "ConvergenceConfig": "cost_tolerance: float, gradient_tolerance: float, parameter_tolerance: float, relative_tolerance: float, consecutive_iterations: int, max_iterations: int",
    "ConvergenceDetector": "cost_history: List[float], parameter_history: List[np.ndarray], gradient_history: List[np.ndarray], consecutive_count: int, converged: bool, convergence_reason: str",
    "StandardScaler": "mean_ np.ndarray, std_ np.ndarray, is_fitted_ bool, n_features_ int",
    "MultipleLinearRegression": "learning_rate float, max_iterations int, tolerance float, regularization_strength float, weights_ np.ndarray, is_fitted_ bool, cost_history_ List[float], weight_history_ List[np.ndarray]",
    "DataValidator": "tolerance float, validation_results dict",
    "NumericalStabilityChecker": "parameter_history List, gradient_history List, cost_history List, stability_warnings List",
    "RegressionError": "error_code: str, suggested_fix: str, context: Dict",
    "DataValidationError": "inherits from RegressionError",
    "NumericalInstabilityError": "inherits from RegressionError",
    "ConvergenceError": "inherits from RegressionError",
    "DebugConfig": "log_level str, save_plots bool, plot_dir str, log_file str, checkpoint_interval int, enable_convergence_plots bool, enable_gradient_checking bool, numerical_precision_threshold float",
    "MLDebugger": "config DebugConfig, training_history List, gradient_history List, parameter_history List",
    "PolynomialFeatureGenerator": "degree int, include_bias bool, interaction_only bool, feature_names_ List[str], n_input_features_ int, n_output_features_ int",
    "PolynomialDataHandler": "base_handler DataHandler, poly_generator PolynomialFeatureGenerator, is_fitted_ bool",
    "CrossValidator": "k int, shuffle bool, random_state int, cv_results_ Dict[str,List[float]]",
    "LogisticRegression": "learning_rate float, max_iterations int, tolerance float, weights_ np.ndarray, is_fitted_ bool, cost_history_ List[float]",
    "ModelServer": "models Dict[str,Any], prediction_history List[Dict]"
  },
  "methods": {
    "fit(x, y)": "Train model on data, return self",
    "predict(x)": "Generate predictions for input x",
    "score(x, y)": "Calculate R-squared coefficient of determination",
    "load_csv_data(filename, feature_columns, target_column)": "Load CSV data into feature and target arrays",
    "validate_data(features, targets)": "Comprehensive validation for regression compatibility",
    "normalize_features(features)": "Apply z-score normalization",
    "generate_linear_data(n_samples, slope, intercept, noise_std, x_range)": "Create synthetic dataset",
    "_compute_cost(x, y, slope, intercept)": "Calculate mean squared error",
    "_compute_gradients(x, y, slope, intercept)": "Compute parameter gradients",
    "fit(x, y) -> self": "Train model on data",
    "predict(x) -> np.ndarray": "Generate predictions for input",
    "score(x, y) -> float": "Calculate R-squared coefficient",
    "load_csv_data(filename, feature_columns, target_column) -> Tuple[np.ndarray, np.ndarray]": "Load data from CSV",
    "validate_data(features, targets) -> None": "Check data compatibility",
    "normalize_features(features) -> np.ndarray": "Apply z-score normalization",
    "generate_linear_data(n_samples, slope, intercept, noise_std, x_range) -> Tuple[np.ndarray, np.ndarray]": "Create synthetic dataset",
    "_compute_cost(x, y, slope, intercept) -> float": "Calculate mean squared error",
    "_compute_gradients(x, y, slope, intercept) -> Tuple[float, float]": "Compute parameter gradients",
    "normalize_features() -> Dataset": "Apply z-score normalization",
    "apply_normalization(features) -> np.ndarray": "Apply stored normalization to new data",
    "set_single_regression_params(slope, intercept, method)": "Set parameters for single regression",
    "set_multiple_regression_params(weights, method, regularization)": "Set parameters for multiple regression",
    "validate_for_prediction(n_features)": "Validate model ready for prediction",
    "validate_array_dimensions(features, targets)": "check array shape compatibility",
    "validate_numerical_data(data, name)": "check for NaN/infinite values",
    "normalize_features()": "Apply z-score normalization and return new Dataset",
    "apply_normalization(features)": "Apply stored normalization to new data",
    "validate_regression_inputs(features, targets)": "comprehensive validation for regression compatibility",
    "compute_r_squared(y_true, y_pred)": "Calculate coefficient of determination",
    "generate_synthetic_data(n_samples, slope, intercept, noise_std, x_range)": "Create synthetic linear dataset",
    "check_convergence(cost, parameters, gradients, iteration) -> Tuple[bool, str]": "Multi-criteria convergence detection",
    "check_numerical_stability(parameters, gradients, cost, parameter_names) -> Tuple[bool, str]": "Comprehensive stability check",
    "safe_parameter_update(parameters, gradients, learning_rate) -> Tuple[np.ndarray, bool, str]": "Protected parameter updates",
    "fit(X, y) -> self": "Train model using gradient descent on multiple features",
    "predict(X) -> np.ndarray": "Generate predictions using fitted parameters",
    "score(X, y) -> float": "Calculate R-squared coefficient of determination",
    "_compute_cost(X, y, weights) -> float": "Compute logistic regression cost using cross-entropy loss",
    "_compute_gradients(X, y, weights) -> np.ndarray": "Compute gradients of logistic cost function",
    "fit_transform(X) -> np.ndarray": "Fit scaler and transform data in one step",
    "create_design_matrix(features) -> np.ndarray": "Construct design matrix with intercept column",
    "validate_regression_data(X, y) -> None": "Comprehensive validation for regression inputs",
    "create_design_matrix(features)": "Construct design matrix with intercept column",
    "check_convergence(cost, parameters, gradients, iteration)": "multi-criteria convergence detection",
    "check_numerical_stability(parameters, gradients, cost, iteration)": "real-time stability monitoring during optimization",
    "check_matrix_conditioning(X)": "design matrix conditioning analysis",
    "suggest_learning_rate_adjustment(current_lr, problem_type)": "adaptive learning rate recommendations",
    "analyze_convergence_quality()": "convergence confidence assessment",
    "fit(X, y)": "Train model on data return self",
    "predict(X)": "Generate predictions for input X",
    "assert_regression_parameters_close(actual_slope, actual_intercept, expected_slope, expected_intercept, tolerance)": "Helper for parameter validation",
    "validate_convergence_history(cost_history, tolerance)": "Validate gradient descent convergence properties",
    "simple_train_test_split(X, y, test_size, random_state)": "Simple train-test split for integration testing",
    "log_training_iteration(iteration, cost, parameters, gradients)": "Log detailed information for each training iteration",
    "check_gradient_correctness(compute_cost_func, compute_gradient_func, parameters, X, y, epsilon)": "Validate gradient computation using finite differences",
    "plot_convergence_diagnostics(cost_history, parameter_history, save_path)": "Generate comprehensive convergence diagnostic plots",
    "diagnose_convergence_issues(cost_history, gradient_history)": "Analyze convergence behavior and identify common problems",
    "validate_data_quality(X, y)": "Comprehensive data quality validation and reporting",
    "validate_gradients_with_finite_differences(model, X, y, epsilon, tolerance)": "Validate gradient computation using finite difference approximation",
    "diagnose_learning_rate_issues(cost_history, parameter_history, current_learning_rate)": "Analyze training behavior to diagnose learning rate problems",
    "detect_numerical_instabilities(parameters, gradients, cost, iteration)": "Real-time detection of numerical stability problems during training",
    "validate_milestone_1_implementation(model_class)": "Comprehensive validation for Milestone 1 Simple Linear Regression",
    "validate_milestone_2_implementation(model_class)": "Comprehensive validation for Milestone 2 Gradient Descent",
    "validate_milestone_3_implementation(model_class)": "Comprehensive validation for Milestone 3 Multiple Linear Regression",
    "debug_training_session(model, X, y, debug_config)": "Interactive debugging session for training problems",
    "score(X, y)": "Calculate R-squared coefficient of determination",
    "fit(X, feature_names) -> self": "Learn polynomial feature combinations from input data",
    "transform(X) -> np.ndarray": "Transform input features to polynomial features",
    "fit_transform(X, feature_names) -> np.ndarray": "Fit and transform in one step",
    "get_feature_names_out(input_features) -> List[str]": "Get descriptive names for output features",
    "prepare_polynomial_features(features, fit) -> np.ndarray": "Prepare polynomial features for training or prediction",
    "validate_model(model_class, X, y, model_params) -> Dict[str,float]": "Perform k-fold cross-validation on a model",
    "hyperparameter_search(model_class, X, y, param_grid) -> Dict[str,Any]": "Grid search with cross-validation for hyperparameter tuning",
    "simple_train_test_split(X, y, test_size, random_state) -> Tuple": "Simple implementation of train-test split",
    "_sigmoid(z) -> np.ndarray": "Compute sigmoid activation function",
    "predict_proba(X) -> np.ndarray": "Predict class probabilities",
    "load_model(model_path, model_name)": "Load trained model from disk",
    "predict(model_name, features) -> Dict[str,Any]": "Make prediction with error handling and logging"
  },
  "constants": {
    "tolerance": "1e-6 convergence threshold",
    "learning_rate": "0.01 default step size",
    "max_iterations": "1000 default iteration limit",
    "min_ratio": "10 samples per feature minimum",
    "TOLERANCE": "1e-10 for numerical comparisons",
    "DEFAULT_LEARNING_RATE": "0.01 safe starting rate",
    "MAX_ITERATIONS": "1000 default iteration limit",
    "MAX_PARAMETER_VALUE": "1e8 overflow threshold",
    "MIN_GRADIENT_NORM": "1e-12 underflow threshold",
    "MAX_GRADIENT_NORM": "1e8 explosion threshold",
    "regularization_strength": "0.0 default no regularization",
    "MAX_CONDITION_NUMBER": "1e12 matrix conditioning threshold",
    "MAX_LEARNING_RATE": "1.0 upper bound",
    "MIN_LEARNING_RATE": "1e-6 lower bound",
    "MIN_SAMPLES_PER_FEATURE": "10 minimum sample to feature ratio",
    "MIN_VARIANCE_THRESHOLD": "1e-10 zero variance detection",
    "degree": "2 default polynomial degree",
    "include_bias": "True default include constant term",
    "interaction_only": "False default include power terms",
    "k": "5 default number of cross-validation folds",
    "shuffle": "True default shuffle data in cross-validation",
    "test_size": "0.2 default test set proportion"
  },
  "terms": {
    "closed-form solution": "Direct analytical calculation",
    "gradient descent": "Iterative optimization using gradients",
    "cost function": "Mean squared error objective to minimize",
    "learning rate": "Step size for parameter updates",
    "convergence": "When optimization reaches stable solution",
    "R-squared": "Coefficient of determination measuring model fit",
    "least squares": "Optimization principle minimizing squared residuals",
    "vectorization": "Using matrix operations instead of loops",
    "z-score normalization": "Standardization to zero mean unit variance",
    "regularization": "Technique to prevent overfitting",
    "data leakage": "using future information to influence training",
    "broadcasting": "NumPy automatic array shape alignment",
    "fail-fast principle": "immediate informative errors vs silent failures",
    "residuals": "Difference between actual and predicted values",
    "ordinary least squares": "Standard linear regression optimization method",
    "normal equations": "Analytical solution to least squares problem",
    "coefficient of determination": "Statistical measure of model fit quality",
    "batch gradient descent": "full dataset gradient computation",
    "numerical stability": "Preventing overflow and underflow errors",
    "parameter update rule": "Core gradient descent step equation",
    "convergence detection": "multi-criteria stopping conditions",
    "gradient magnitude": "L2 norm of gradient vector",
    "cost improvement": "Reduction in cost between iterations",
    "hyperplane": "Multi-dimensional generalization of plane",
    "design matrix": "Feature matrix with intercept column",
    "Ridge regression": "Linear regression with L2 regularization",
    "gradient explosion": "gradients becoming extremely large indicating instability",
    "parameter divergence": "Parameters growing without bound",
    "matrix conditioning": "numerical stability of matrix operations",
    "overflow protection": "preventing values from exceeding representable range",
    "underflow detection": "identifying values below machine precision",
    "learning rate adaptation": "automatic adjustment of step size during optimization",
    "validation strictness": "balance between error prevention and experimentation allowance",
    "unit testing": "Testing individual components with synthetic data and known outcomes",
    "integration testing": "End-to-end testing with real datasets and cross-validation",
    "milestone checkpoints": "Clear success criteria for each learning milestone",
    "synthetic data": "Artificially generated datasets with known mathematical properties",
    "cross-validation": "technique for assessing model generalization using multiple train-test splits",
    "mathematical verification": "Validating implementation against analytical solutions",
    "edge case testing": "Testing system behavior under adverse conditions",
    "convergence validation": "Verifying gradient descent reaches stable solutions",
    "finite differences": "numerical approximation of derivatives",
    "gradient checking": "validation technique comparing analytical to numerical gradients",
    "numerical precision": "accuracy of floating-point computations",
    "parameter clipping": "constraining parameter values to safe ranges",
    "polynomial features": "transformed features including powers and interactions of original features",
    "k-fold": "cross-validation method that partitions data into k equal subsets",
    "hyperparameter tuning": "process of selecting optimal model configuration parameters",
    "sigmoid function": "logistic function that maps real numbers to probabilities between 0 and 1",
    "cross-entropy loss": "cost function for classification that measures probability prediction quality",
    "feature engineering": "process of creating new features from existing ones to improve model performance",
    "model serving": "production deployment of trained models via APIs for real-time predictions",
    "overfitting": "model memorization of training data leading to poor generalization",
    "production readiness": "system capabilities required for reliable deployment in real-world environments"
  }
}