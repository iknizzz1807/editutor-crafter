{
  "types": {
    "LogEntry": "fields: Timestamp time.Time, Labels Labels, Message string",
    "Labels": "map[string]string",
    "LogStream": "fields: Labels Labels, Entries []LogEntry",
    "TimeRange": "fields: Start time.Time, End time.Time",
    "Config": "fields: MaxFailures int, FailureWindow time.Duration, OpenDuration time.Duration, HalfOpenMaxCalls int",
    "Metrics": "fields: logsIngested int64, queriesExecuted int64",
    "PostingsList": "[]EntryReference",
    "EntryReference": "fields: ChunkID string, Offset uint32, Timestamp time.Time",
    "BloomFilter": "fields: BitArray []uint64, HashFunctions []hash.Hash, Parameters BloomParams",
    "BloomParams": "fields: ExpectedElements uint32, FalsePositiveRate float64, BitArraySize uint32, HashCount uint32",
    "ChunkHeader": "fields: Magic [4]byte, Version uint16, CompressionType uint8, StreamCount uint32, EntryCount uint64, UncompressedSize uint64, CompressedSize uint64, TimeRange TimeRange, CreatedAt time.Time",
    "StreamHeader": "fields: StreamID string, Labels Labels, EntryCount uint32, CompressedOffset uint64, CompressedSize uint64",
    "IndexSegment": "fields: SegmentID string, Terms map[string]*PostingsList, CreatedAt time.Time, ChunkIDs []string",
    "PartitionStats": "fields: EntryCount int64, UniqueTerms int64, DiskSize int64, AvgQueryTime time.Duration, LastAccessed time.Time",
    "HTTPServer": "HTTP ingestion server struct",
    "MemoryBuffer": "ring buffer implementation",
    "TCPHandler": "TCP syslog connection handler",
    "JSONParser": "JSON log parsing implementation",
    "QueryEngine": "query execution coordinator",
    "Token": "fields: Type TokenType, Value string, Position int, Line int, Column int",
    "Lexer": "fields: input string, position int, line int, column int, current rune",
    "ResultStream": "fields: entries chan, errors chan, cursor string",
    "QueryParams": "fields: TimeRange, Limit int, Timeout time.Duration, Format string, MaxMemory int64, StreamResults bool",
    "QueryMetadata": "fields: ExecutionTime time.Duration, ScannedEntries int64, ReturnedEntries int64, ScannedBytes int64, CacheHit bool",
    "WALRecord": "fields: RecordType uint8, Timestamp time.Time, ChunkID string, StreamID string, EntryCount uint32, DataSize uint64, Checksum uint32, Data []byte",
    "StorageEngine": "coordinates chunk storage, WAL, and retention",
    "StorageConfig": "defines storage engine parameters",
    "StorageMetrics": "tracks storage engine performance",
    "RetentionPolicy": "fields: PolicyID string, StreamSelector Labels, MaxAge time.Duration, MaxSize int64, MaxEntries int64, Priority int32, GracePeriod time.Duration",
    "TenantContext": "fields: TenantID string, Roles []string, Quotas ResourceQuotas, AuthMethod string, ExpiresAt time.Time",
    "ResourceQuotas": "fields: MaxIngestionRate int64, MaxStorageBytes int64, MaxActiveQueries int32, MaxQueryMemoryMB int64",
    "AuthService": "fields: publicKey *rsa.PublicKey, tenantConfig map[string]TenantConfig, defaultQuotas ResourceQuotas",
    "TenantConfig": "fields: Name string, Quotas ResourceQuotas, AllowedRoles []string, IPWhitelist []string",
    "TokenBucket": "fields: mu sync.Mutex, capacity int64, tokens float64, refillRate float64, lastRefill time.Time, name string",
    "HierarchicalLimiter": "fields: tenantBuckets map[string]*TokenBucket, streamBuckets map[string]*TokenBucket, mu sync.RWMutex, defaultQuotas ResourceQuotas",
    "AlertEngine": "fields: rules map[string]*AlertRule, rulesMu sync.RWMutex, notifier *NotificationManager, deduplicator *AlertDeduplicator, evaluationChan chan *LogEntry, stopChan chan struct{}",
    "AlertRule": "fields: RuleID string, TenantID string, Name string, Query string, Condition ThresholdCondition, Window time.Duration, Severity AlertSeverity, Enabled bool",
    "ThresholdCondition": "fields: Type string, Threshold float64, Operator string",
    "Alert": "fields: AlertID string, RuleID string, TenantID string, Severity AlertSeverity, Title string, Message string, TriggerEntry *LogEntry, Timestamp time.Time, Status AlertStatus",
    "HealthStatus": "string type for component health state",
    "CheckResult": "fields: Name string, Status HealthStatus, Message string, Timestamp time.Time, Details map[string]interface{}, Duration time.Duration",
    "HealthCheck": "interface with Name() string and Check(ctx context.Context) CheckResult methods",
    "Registry": "fields: checks map[string]HealthCheck, mutex sync.RWMutex",
    "State": "int type for circuit breaker state",
    "Breaker": "fields: config Config, state State, failures int, lastFailTime time.Time, halfOpenCalls int, mutex sync.RWMutex",
    "RecoveryManager": "fields: walPath string, storagePath string, logger *zap.Logger",
    "DegradationController": "fields: features map[string]bool, fallbacks map[string]func() error, mutex sync.RWMutex",
    "TraceContext": "fields: TraceID string, SpanID string, ParentSpan string, StartTime time.Time, Metadata map[string]string, Logger *zap.Logger",
    "IngestionHealthCheck": "fields: httpServer *HTTPServer, tcpHandler *TCPHandler, buffer *MemoryBuffer, parser *JSONParser, metrics *Metrics",
    "StorageHealthCheck": "fields: storageEngine *StorageEngine, walPath string, chunkPath string",
    "PerformanceMonitor": "fields: ingestionMetrics *IngestionMetrics, queryMetrics *QueryMetrics, storageMetrics *StorageMetrics, systemMetrics *SystemMetrics, alertThresholds map[string]float64",
    "ClusterCoordinator": "manages distributed system state and routing",
    "DistributedQueryEngine": "coordinates queries across storage nodes",
    "ShardManager": "handles data distribution and migration",
    "MLPipeline": "processes log streams through ML models",
    "AnomalyDetector": "identifies unusual patterns in log streams",
    "ClassificationEngine": "adds semantic tags automatically",
    "AnomalyScore": "fields: Score float64, Confidence float64, Factors []string"
  },
  "methods": {
    "LoadConfig() *Config": "loads extension configuration",
    "IncrementLogsIngested()": "thread-safe increment of ingestion counter",
    "GetStats() (int64, int64)": "returns current log and query counts",
    "NewLogEntry(timestamp time.Time, labels Labels, message string) (*LogEntry, error)": "creates validated LogEntry",
    "String() string": "canonical string representation",
    "Equal(other *LogEntry) bool": "compares LogEntry instances",
    "Clone() *LogEntry": "creates deep copy",
    "Hash() uint64": "consistent hash of labels",
    "Validate() error": "checks label validity",
    "Merge(other Labels) Labels": "combines label maps",
    "NewTimeRange(start, end time.Time) (*TimeRange, error)": "creates validated TimeRange",
    "Contains(timestamp time.Time) bool": "checks if timestamp in range",
    "Overlaps(other *TimeRange) bool": "checks range overlap",
    "Duration() time.Duration": "returns range length",
    "NewHTTPServer(config, parser, buffer, metrics) *HTTPServer": "creates HTTP ingestion server",
    "Start() error": "starts server listening",
    "Stop() error": "gracefully shuts down server",
    "handleLogIngestion(w, r)": "processes log ingestion requests",
    "Write(entry) error": "adds entry to buffer",
    "Read() (*LogEntry, error)": "removes entry from buffer",
    "Parse(data) (*LogEntry, error)": "converts raw data to LogEntry",
    "NewBloomParams(expectedElements uint32, falsePositiveRate float64) BloomParams": "calculates optimal bloom filter parameters",
    "NewBloomFilter(params BloomParams) *BloomFilter": "creates empty bloom filter",
    "Add(element string)": "inserts element into bloom filter",
    "MightContain(element string) bool": "tests probabilistic membership",
    "NewIndexSegment(segmentID string, timeRange TimeRange, expectedTerms uint32) *IndexSegment": "creates empty index segment",
    "AddTerm(term string, ref EntryReference)": "adds term-to-entry mapping",
    "LookupTerm(term string) *PostingsList": "retrieves postings list for term",
    "Finalize()": "prepares segment for read-only access",
    "BuildIndexSegment(segmentID string, timeRange TimeRange, entries []LogEntry) (*IndexSegment, error)": "builds index from log entries",
    "CompactSegments(sourceSegments []*IndexSegment, outputSegmentID string) (*IndexSegment, error)": "merges multiple segments",
    "PartitionQuery(query Query, availableSegments []*IndexSegment) []*IndexSegment": "determines segments to search",
    "NewLexer(input string) *Lexer": "creates lexer for query string",
    "NextToken() Token": "returns next lexical token",
    "ExecuteQuery(ctx, queryString, params) (*ResultStream, error)": "main query execution entry point",
    "executePlan(ctx, plan) (*ResultIterator, error)": "coordinates optimized plan execution",
    "executeIndexLookup(ctx, selectors, timeRange) ([]EntryReference, error)": "retrieves matching log entry references",
    "executeTextFilter(ctx, refs, filters) (<-chan *LogEntry, error)": "applies line filters to log content",
    "Next() (*LogEntry, error)": "returns next result entry from stream",
    "advance()": "moves lexer to next character",
    "NewStorageEngine(config) (*StorageEngine, error)": "creates configured storage engine",
    "WriteLogBatch(entries []LogEntry) error": "stores batch with WAL protection",
    "StartRecovery() error": "replays uncommitted WAL operations",
    "CompressChunkData(streams, algorithm) (*ChunkHeader, []byte, error)": "applies compression to chunk payload",
    "WriteWALRecord(record) error": "atomically appends record to WAL",
    "EvaluateRetentionPolicies() ([]string, error)": "identifies chunks eligible for cleanup",
    "SafeChunkDeletion(chunkIDs) error": "removes chunks while coordinating with queries",
    "AuthenticateRequest(r *http.Request) (*TenantContext, error)": "extracts and validates tenant context from HTTP request",
    "CheckPermission(requiredRole string) error": "verifies tenant has required role for operation",
    "TryConsume(tokens int64) bool": "attempts to consume tokens from bucket",
    "CheckRateLimit(tenantID, streamID string, tokens int64) error": "verifies operation allowed under current limits",
    "EvaluateLogEntry(entry *LogEntry)": "checks incoming log against all relevant rules",
    "AddRule(rule *AlertRule) error": "registers new alert rule for evaluation",
    "UpdateRule(ruleID string, updates *AlertRule) error": "modifies existing alert rule",
    "DeleteRule(ruleID string, tenantID string) error": "removes alert rule from evaluation",
    "NewRegistry() *Registry": "creates new health check registry",
    "RegisterCheck(check HealthCheck)": "adds health check to registry",
    "CheckAll(ctx context.Context) map[string]CheckResult": "executes all registered health checks",
    "ServeHTTP(w http.ResponseWriter, req *http.Request)": "HTTP handler for health endpoints",
    "NewBreaker(config Config) *Breaker": "creates circuit breaker with configuration",
    "Call(ctx context.Context, fn func() error) error": "executes function with circuit breaker protection",
    "beforeCall() (State, error)": "checks if call should be allowed",
    "afterCall(success bool)": "updates circuit breaker state based on result",
    "State() State": "returns current circuit breaker state",
    "PerformRecovery(ctx context.Context) error": "executes complete system recovery sequence",
    "ValidateSystemConsistency() error": "checks data integrity after recovery",
    "EnableGracefulDegradation(reason string) error": "switches to degraded operation mode",
    "RestoreNormalOperation() error": "attempts to restore full functionality",
    "NewTraceContext(operation, metadata) *TraceContext": "creates new trace context for pipeline operations",
    "StartSpan(operation) *TraceContext": "creates child span for nested operations",
    "RecordEvent(event, details)": "logs significant event within trace span",
    "Finish(err)": "completes span and records final metrics",
    "Check(ctx) CheckResult": "performs health verification",
    "Name() string": "returns component name for identification",
    "Dependencies() []string": "lists component dependencies",
    "RecordIngestionLatency(stage, duration)": "tracks latency for ingestion pipeline stage",
    "RecordQueryPerformance(query, metadata)": "captures comprehensive query execution metrics",
    "GeneratePerformanceReport() *PerformanceReport": "creates comprehensive performance analysis",
    "ExecuteQuery(ctx, query) *ResultStream": "coordinates distributed query execution",
    "ProcessLogEntry(entry) *AnomalyScore": "analyzes log entry for anomalies",
    "GetShardAssignment(labels) []string": "determines target shards for data",
    "MigrateShards(source, target) error": "moves data between nodes"
  },
  "constants": {
    "HTTP_PORT": "8080",
    "TCP_PORT": "1514",
    "UDP_PORT": "1514",
    "STORAGE_PATH": "./data",
    "BUFFER_SIZE": "10000",
    "CHUNK_SIZE": "1MB",
    "RETENTION_DAYS": "30",
    "TokenEOF": "end of input token",
    "TokenError": "lexical error token",
    "TokenLeftBrace": "{ token",
    "TokenRightBrace": "} token",
    "TokenPipe": "| token",
    "TokenEqual": "= token",
    "TokenString": "quoted string literal",
    "TokenIdentifier": "unquoted identifier",
    "WALRecordWrite": "1",
    "WALRecordCommit": "2",
    "WALRecordCheckpoint": "3",
    "SeverityInfo": "info alert level",
    "SeverityWarning": "warning alert level",
    "SeverityCritical": "critical alert level",
    "AlertStatusTriggered": "alert just fired",
    "StatusHealthy": "healthy component status",
    "StatusDegraded": "degraded component status",
    "StatusUnhealthy": "unhealthy component status",
    "StateClosed": "circuit breaker closed state",
    "StateHalfOpen": "circuit breaker half-open state",
    "StateOpen": "circuit breaker open state",
    "REPLICATION_FACTOR": "3",
    "SHARD_MIGRATION_TIMEOUT": "30 minutes",
    "ML_BATCH_SIZE": "1000",
    "ANOMALY_THRESHOLD": "0.95"
  },
  "terms": {
    "log aggregation": "collecting and centralizing log data from multiple sources",
    "label cardinality": "number of unique values for a label key",
    "inverted index": "maps terms to documents containing them",
    "bloom filter": "probabilistic data structure for fast negative lookups",
    "chunk": "time-windowed compressed storage unit",
    "write-ahead log": "durable transaction log ensuring no data loss",
    "backpressure": "flow control when downstream cannot keep up",
    "cardinality explosion": "exponential growth of unique label combinations",
    "time-based partitioning": "organizing data by time windows",
    "syslog": "standard protocol for log message transmission",
    "ring buffer": "circular buffer with fixed capacity",
    "ingestion pipeline": "sequence of processing stages for incoming logs",
    "false positive": "bloom filter incorrectly reports presence",
    "false negative": "impossible with bloom filters",
    "LogQL": "log query language inspired by Grafana Loki",
    "AST": "abstract syntax tree representing parsed query structure",
    "predicate pushdown": "optimization technique moving filters earlier in execution",
    "cursor-based pagination": "pagination using opaque position tokens",
    "streaming execution": "processing results incrementally without full materialization",
    "label selector": "LogQL component specifying which log streams to query",
    "line filter": "LogQL component matching against log message content",
    "retention policy": "configurable rules for automatic data cleanup based on age/size",
    "compression ratio": "percentage of storage saved through compression algorithms",
    "WAL rotation": "creating new WAL files when size limits exceeded",
    "retention cleanup": "automated deletion of expired chunks according to policies",
    "chunk boundaries": "time windows that define chunk organization",
    "stream-level compression": "compressing log streams separately within chunks",
    "grace period": "delay before actual deletion for retention policy recovery",
    "reference counting": "tracking active query access to prevent unsafe deletion",
    "multi-tenancy": "secure isolation between different organizations or teams",
    "tenant isolation": "preventing data leakage between tenants",
    "rate limiting": "controlling request frequency",
    "token bucket": "algorithm allowing burst traffic while enforcing sustained limits",
    "alert deduplication": "preventing notification spam",
    "hierarchical rate limiting": "nested rate limits at multiple scopes",
    "tenant ID injection": "attack where malicious clients manipulate tenant identification",
    "alert storm": "cascading alert generation that overwhelms notification systems",
    "resource leakage": "tenant activity affecting other tenants' performance",
    "authentication bypass": "security gaps in multi-protocol authentication enforcement",
    "graceful degradation": "maintaining reduced functionality during failures",
    "circuit breaker": "pattern preventing cascading failures",
    "health check": "automated test that verifies component functionality and status",
    "WAL replay": "process of reconstructing system state by replaying write-ahead log records",
    "split-brain": "scenario where network partitions create conflicting system state views",
    "cascade failure": "failure propagation where one component failure causes other components to fail",
    "alert fatigue": "condition where operators ignore alerts due to excessive false positives",
    "synthetic transaction": "artificial operation that simulates real user workflows for monitoring",
    "partition healing": "process of reconciling state differences after network partitions resolve",
    "unit testing": "component-level testing strategies verifying individual functions and classes",
    "integration testing": "end-to-end scenarios validating component interactions",
    "milestone checkpoints": "verification points after each development phase",
    "property-based testing": "automated testing using randomly generated inputs to verify invariants",
    "mock dependencies": "test doubles that simulate external services and resources",
    "test data generation": "creating realistic datasets for testing system behavior",
    "performance baselines": "established metrics for detecting performance regressions",
    "failure injection": "simulating error conditions to test recovery mechanisms",
    "tenant isolation testing": "verifying data and resource separation between tenants",
    "load testing": "validating system performance under realistic traffic volumes",
    "durability testing": "verifying data persistence guarantees across failures",
    "log flow tracing": "technique that tracks individual log entries through entire pipeline from ingestion to storage",
    "index health diagnostics": "specialized debugging for index corruption and inefficiency problems",
    "storage layer debugging": "comprehensive debugging of WAL, chunks, compression, and retention policies",
    "multi-tenant debugging": "specialized debugging to isolate tenant-specific problems from system-wide issues",
    "query performance profiling": "understanding LogQL query semantics and execution engine performance",
    "throughput cascade analysis": "technique measuring throughput at each pipeline stage to identify bottlenecks",
    "latency percentile analysis": "using different latency percentiles to reveal different types of performance problems",
    "memory usage pattern diagnosis": "identifying characteristic memory patterns that indicate specific underlying causes",
    "garbage collection impact analysis": "understanding how GC pressure affects log aggregation performance",
    "disk I/O performance analysis": "analyzing both sequential and random access patterns affecting ingestion and querying",
    "network performance analysis": "diagnosing network bottlenecks affecting log ingestion and query responses",
    "trace context propagation": "maintaining debugging context through entire processing pipeline",
    "health check infrastructure": "systematic component health verification with dependency tracking",
    "performance monitoring framework": "comprehensive system performance tracking across all components",
    "horizontal scaling": "adding more machines to handle load",
    "sharding": "partitioning data across multiple nodes",
    "consensus protocol": "algorithm ensuring distributed agreement",
    "scatter-gather": "query pattern that fans out and collects results",
    "anomaly detection": "identifying unusual patterns in data streams",
    "materialized views": "pre-computed query results updated incrementally",
    "streaming aggregation": "continuous computation over data streams",
    "model drift": "ML model accuracy degradation over time"
  }
}