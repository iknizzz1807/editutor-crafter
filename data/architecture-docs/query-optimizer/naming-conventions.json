{
  "types": {
    "ExecutionPlan": "fields: root OperatorNode, total_cost CostEstimate, optimization_metadata dict, plan_id str, created_timestamp datetime",
    "OperatorNode": "fields: operator_type OperatorType, children List[OperatorNode], properties dict, cost_estimate CostEstimate, output_schema List[str], estimated_rows int, node_id str",
    "CostEstimate": "fields: io_cost float, cpu_cost float, memory_cost float, estimated_rows int, startup_cost float, total_cost property, cost_factors dict, confidence_level float",
    "TableStatistics": "fields: table_name str, row_count int, page_count int, column_stats dict, last_updated datetime, sample_rate float, index_statistics dict, clustering_factor float",
    "JoinOrder": "sequence of table joins with cost information",
    "ParsedQuery": "fields: tables List[str], columns List[str], joins List[tuple], filters List[tuple]",
    "ColumnStatistics": "fields: column_name str, distinct_values int, null_count int, min_value Any, max_value Any, most_common_values List[tuple], histogram_buckets List[HistogramBucket], correlation_with_storage float, average_width int",
    "OperatorType": "enum: SCAN, FILTER, JOIN, PROJECT, SORT, AGGREGATE, UNION, INTERSECT, LIMIT",
    "JoinType": "enum: INNER, LEFT, RIGHT, FULL, CROSS",
    "HistogramBucket": "fields: bucket_id int, range_start Any, range_end Any, row_count int, distinct_count int, frequency float",
    "JoinPredicate": "join condition between tables",
    "JoinGraph": "analyzes table connectivity",
    "JoinCostCalculator": "estimates join algorithm costs",
    "DynamicProgrammingOptimizer": "implements DP join optimization",
    "AccessMethodOption": "fields: method_type str, index_name str, estimated_cost CostEstimate, estimated_rows int, properties dict",
    "PhysicalCostComparator": "compares costs between different physical operator implementations",
    "OptimizationRuleEngine": "applies rule-based transformations for plan optimization",
    "PlanCacheEntry": "cached execution plan with access metadata",
    "PlanCache": "thread-safe LRU cache for execution plans",
    "StalenessAssessment": "fields: confidence_multiplier float, needs_refresh bool, refresh_priority str, staleness_factors Dict[str, float]",
    "TestQuery": "test query with expected characteristics",
    "PlanQualityMetric": "quality metrics for plan evaluation",
    "TestDataGenerator": "generates test data for validation",
    "PlanComparisonUtility": "utilities for plan comparison",
    "MilestoneValidator": "validates milestone completion",
    "CostDebugTrace": "fields: component str, operation str, inputs dict, outputs dict, timestamp datetime, duration_ms float",
    "CostEstimationDebugger": "debugging support for cost estimation",
    "StatisticsValidator": "validates statistics accuracy",
    "JoinOrderingDebugger": "debugging utilities for join ordering",
    "JoinOrderingProfiler": "performance profiling for join optimization",
    "PlanGenerationDebugger": "debugging utilities for plan generation",
    "PlanComparator": "utilities for comparing execution plans",
    "OptimizerDebugCLI": "command-line debugging interface",
    "MultidimensionalBucket": "fields: bucket_id int, dimension_ranges List[Tuple], row_count int, distinct_count_per_dimension List[int], frequency float",
    "ColumnCorrelation": "fields: column_a str, column_b str, correlation_coefficient float, mutual_information float, sample_size int, confidence_level float",
    "SystemResources": "fields: cpu_cores int, memory_mb int, io_bandwidth_mbps float, network_bandwidth_mbps float, current_cpu_utilization float, current_memory_utilization float",
    "ParallelCostEstimate": "extends CostEstimate with fields: degree_of_parallelism int, coordination_overhead float, resource_contention_factor float, critical_path_cost float, parallelizable_fraction float",
    "ExecutionMetrics": "fields: operator_id str, estimated_rows int, actual_rows int, estimated_cost float, actual_runtime_ms float, memory_used_mb float, io_pages_read int, cpu_time_ms float",
    "ExchangeOperator": "extends OperatorNode for parallel data exchange boundaries",
    "MultidimensionalHistogram": "multi-dimensional statistical distribution for correlated columns",
    "CorrelationDetector": "discovers statistical dependencies between columns",
    "ParallelCostModel": "resource-aware cost estimation for parallel execution",
    "ExecutionMonitor": "instruments query execution for performance feedback",
    "QueryMonitor": "tracks performance of single query execution",
    "AdaptiveStatisticsManager": "updates statistics based on execution feedback",
    "ExtensibleQueryOptimizer": "query optimizer with pluggable extension support"
  },
  "methods": {
    "estimateCost(plan) returns CostEstimate": "calculate predicted execution cost for plan",
    "generatePlans(query) returns ExecutionPlan[]": "enumerate possible execution plans for query",
    "optimizeJoinOrder(tables) returns JoinOrder": "find efficient join sequence using dynamic programming",
    "collectStatistics(table) returns TableStatistics": "gather statistical information for cost estimation",
    "selectPhysicalOperators(logical_plan, stats) returns ExecutionPlan": "choose concrete physical operators for logical plan",
    "optimize_query(parsed_query) returns ExecutionPlan": "main optimization pipeline entry point",
    "traverse_preorder() returns OperatorNode[]": "traverse plan tree in pre-order",
    "calculate_total_cost() returns CostEstimate": "calculate and cache total execution cost",
    "pretty_print(show_costs=True) returns str": "generate indented tree representation",
    "add_child(child)": "add child operator to node",
    "traverse_preorder() returns Iterator[OperatorNode]": "traverse plan tree in pre-order",
    "traverse_postorder() returns Iterator[OperatorNode]": "traverse plan tree in post-order",
    "find_nodes_by_type(type) returns List[OperatorNode]": "find nodes matching operator type",
    "calculate_subtree_cost() returns CostEstimate": "calculate total cost for operator subtree",
    "pretty_print(indent=0, show_costs=True) returns str": "generate indented tree representation",
    "get_all_tables() returns List[str]": "extract table names from plan",
    "clone() returns ExecutionPlan": "create deep copy of execution plan",
    "add_cost(other) returns CostEstimate": "combine cost estimates",
    "scale_by_factor(factor) returns CostEstimate": "scale cost estimate by multiplier",
    "clone() returns OperatorNode": "create deep copy of subtree",
    "collectStatistics(table, sample_rate) returns TableStatistics": "gather statistical information for cost estimation",
    "estimate_filter_selectivity(table, column, operator, value) returns float": "predict fraction of rows surviving filter predicate",
    "estimate_join_cardinality(left_table, left_column, right_table, right_column) returns int": "estimate number of rows produced by join",
    "_enumerate_subsets_by_size(tables, size)": "generate table subsets for DP",
    "_find_optimal_partition(subset)": "find best way to split subset for joining",
    "_evaluate_join_cost(left_plan, right_plan, predicates)": "estimate costs for different join algorithms",
    "_create_base_case_plans(tables)": "initialize single-table access plans",
    "_check_connectivity(subset1, subset2)": "verify tables can be joined directly",
    "_build_join_plan(left, right, type, predicates)": "construct joined execution plan",
    "compare_scan_methods(table_name, predicates, available_indexes) returns AccessMethodOption": "compare sequential scan vs available index access methods",
    "_select_scan_operator(logical_node) returns OperatorNode": "choose between sequential scan and index access methods",
    "_select_join_algorithm(logical_node) returns OperatorNode": "choose join algorithm based on input characteristics",
    "_apply_optimization_rules(logical_plan) returns OperatorNode": "apply rule-based transformations to improve plan efficiency",
    "_estimate_memory_requirement(operator) returns float": "calculate memory needed for physical operator execution",
    "_validate_physical_plan(plan) returns bool": "verify that physical plan is executable and semantically correct",
    "get_plan(query_hash) returns Optional[ExecutionPlan]": "retrieve cached plan if available",
    "store_plan(query_hash, plan, optimization_cost)": "store optimized plan in cache",
    "invalidate_plans_for_table(table_name)": "remove cached plans that reference specific table",
    "compute_query_hash(sql) returns str": "generate stable hash for SQL query caching",
    "normalize_sql_text(sql) returns str": "normalize SQL text for consistent caching",
    "extract_query_parameters(sql) returns Tuple": "extract parameterizable literals from SQL query",
    "assess_staleness(table_stats, change_tracker) returns StalenessAssessment": "assess statistics staleness and compute confidence adjustments",
    "analyze_query_connectivity(parsed_query) returns Dict": "analyze join connectivity and detect cross products",
    "check_optimization_feasibility(parsed_query, available_stats) returns Dict": "check if optimization is feasible and identify potential failure modes",
    "select_fallback_strategy(failure_modes, query_complexity) returns str": "select appropriate fallback optimization strategy",
    "calculate_plan_confidence(plan, table_stats) returns float": "calculate overall confidence level for execution plan cost estimates",
    "adjust_costs_for_uncertainty(cost_estimate, confidence) returns CostEstimate": "adjust cost estimates based on confidence level",
    "create_table_statistics": "create table statistics for testing",
    "compare_plan_structure": "compare execution plan structures",
    "validate_plan_quality": "evaluate plan using quality metrics",
    "validate_milestone_1_plan_representation": "validate milestone 1 completion",
    "validate_milestone_2_cost_estimation": "validate milestone 2 completion",
    "validate_milestone_3_join_optimization": "validate milestone 3 completion",
    "validate_milestone_4_physical_planning": "validate milestone 4 completion",
    "validate_table_statistics(table_name, stats) returns Dict": "validate table statistics against actual database state",
    "trace_selectivity_calculation(table, column, operator, value, result)": "trace detailed selectivity estimation process",
    "compare_estimated_vs_actual_cardinality(plan, actual_results) returns Dict": "compare estimated vs actual cardinalities",
    "audit_statistics_freshness(table_names) returns Dict": "audit statistics freshness for multiple tables",
    "verify_row_count_accuracy(table_name, stored_count, tolerance) returns Tuple": "verify stored row count accuracy",
    "trace_dynamic_programming_execution(optimizer, tables) returns Dict": "trace complete DP execution",
    "validate_subset_enumeration(tables, generated_subsets) returns bool": "validate subset enumeration correctness",
    "analyze_cost_comparison_stability(cost_traces) returns Dict": "analyze cost comparison stability",
    "verify_join_predicate_connectivity(parsed_query) returns JoinGraph": "verify join predicate connectivity",
    "profile_optimization_phases(tables) returns Dict": "profile optimization phase timing",
    "validate_plan_structure(plan) returns List": "validate structural correctness of execution plan",
    "trace_schema_propagation(root_node) returns List": "trace schema propagation through tree",
    "validate_cost_accumulation(plan) returns Dict": "validate cost accumulation accuracy",
    "generate_plan_debug_output(plan, include_costs, include_schemas) returns str": "generate comprehensive debug output",
    "compare_plan_structures(plan1, plan2) returns Dict": "compare structural differences between plans",
    "explain_plan_selection_decision(alternatives, selected) returns str": "explain plan selection reasoning",
    "debug_query_optimization(sql_query, debug_options)": "debug complete optimization process",
    "validate_statistics_accuracy(table_names)": "validate statistics accuracy for tables",
    "benchmark_join_ordering(query_file, iterations)": "benchmark join ordering performance",
    "analyze_table_correlations(table_name, sample_data, column_names) returns List[ColumnCorrelation]": "analyze pairwise correlations between table columns",
    "should_build_joint_histogram(table_name, columns) returns bool": "determine if columns warrant multi-dimensional histogram",
    "build_histogram(sample_data)": "construct multi-dimensional histogram from sample data",
    "estimate_selectivity(predicates) returns float": "estimate selectivity for multi-dimensional predicates",
    "estimate_parallel_cost(operator, input_parallelism) returns ParallelCostEstimate": "estimate execution cost under parallel execution",
    "calculate_plan_parallelism(plan) returns Dict": "determine optimal parallelism for each operator",
    "estimate_exchange_cost(input_rows, input_parallelism) returns CostEstimate": "estimate data exchange costs between parallel regions",
    "start_query_monitoring(query_id, execution_plan) returns QueryMonitor": "begin monitoring query execution",
    "complete_query_monitoring(query_id) returns ExecutionTrace": "finish monitoring and return performance data",
    "record_operator_start(operator_id)": "record start of operator execution",
    "record_operator_completion(operator_id, actual_rows)": "record completion with actual performance data",
    "process_execution_feedback(execution_trace)": "analyze feedback and update statistics",
    "should_update_statistics(table_name, error_pattern) returns bool": "determine if statistics update is warranted",
    "register_statistics_extension(extension)": "add advanced statistics capability",
    "optimize_query_with_extensions(parsed_query) returns ExecutionPlan": "optimization pipeline with extension integration"
  },
  "constants": {
    "OPTIMIZATION_TIMEOUT": "maximum optimization time allowed",
    "MAX_JOIN_ENUMERATION": "threshold for switching to heuristic search",
    "SELECTIVITY_THRESHOLD": "cutoff for index vs sequential scan",
    "IO_PAGE_COST": "1.0 - base cost per I/O page read",
    "CPU_TUPLE_COST": "0.01 - base cost per tuple processed",
    "MEMORY_PAGE_COST": "0.001 - cost per memory page allocated",
    "SEQUENTIAL_MULTIPLIER": "0.3 - efficiency factor for sequential I/O",
    "RANDOM_MULTIPLIER": "2.0 - penalty factor for random I/O",
    "correlation_threshold": "0.3 - minimum correlation for joint histogram consideration",
    "contention_factors": "resource contention multipliers for different workload types",
    "confidence_threshold": "0.5 - minimum confidence for statistics updates"
  },
  "terms": {
    "query optimization": "process of finding efficient execution plans for SQL queries",
    "cost-based optimization": "operator selection using resource cost estimates",
    "selectivity": "fraction of rows surviving filter predicates",
    "cardinality": "number of rows in table or result",
    "join ordering": "problem of finding efficient sequence for multi-table joins",
    "predicate pushdown": "optimization moving filters closer to data sources",
    "search space pruning": "eliminating clearly suboptimal plans to reduce optimization time",
    "dynamic programming": "algorithm technique for join order optimization",
    "physical operator": "concrete implementation of logical operation",
    "logical operator": "abstract operation specification without implementation details",
    "plan tree": "hierarchical representation of query execution strategy",
    "operator node": "single operation in execution plan tree",
    "statistics collection": "gathering data characteristics for cost estimation",
    "cost estimation": "predicting resource consumption for query plans",
    "tree traversal": "algorithm for visiting all nodes in tree structure",
    "preorder traversal": "visiting parent before children",
    "postorder traversal": "visiting children before parent",
    "cost accumulation": "bottom-up calculation of total execution cost",
    "schema propagation": "passing column information through operator chain",
    "histogram": "statistical distribution representation for selectivity estimation",
    "left-deep tree": "join tree where right inputs are always base tables",
    "bushy tree": "join tree allowing intermediate results as both inputs",
    "cross product": "join without connecting predicates",
    "memoization": "caching optimization results for reuse",
    "physical planning": "process of selecting concrete operators and access methods for query execution",
    "access method selection": "choice between index scans and sequential scans based on data characteristics",
    "join algorithm selection": "decision process for choosing hash joins, nested loops, or merge joins",
    "rule-based optimization": "transformations applied using predetermined heuristics",
    "clustering factor": "measure of how well table storage order matches logical data order",
    "plan caching": "storing optimized execution plans for reuse across similar queries",
    "cache invalidation": "removing stale cached plans when underlying data changes",
    "plan template": "parameterized execution plan that can be instantiated with different values",
    "optimization pipeline": "sequence of transformation phases from SQL to executable plan",
    "component communication": "data exchange patterns between optimizer modules",
    "parametric matching": "template-based plan reuse with parameter substitution",
    "cache eviction": "removing entries from cache due to size or age constraints",
    "plan serialization": "converting execution plans to persistent storage format",
    "statistics staleness": "outdated statistical information affecting optimization quality",
    "confidence level": "reliability measure for cost estimates",
    "fallback strategy": "alternative optimization approach when primary method fails",
    "degenerate query": "query with problematic patterns like cross products",
    "uncertainty margin": "additional cost buffer for unreliable estimates",
    "plan quality validation": "verifying optimizer produces reasonable plans",
    "benchmark testing": "systematic quality assessment using standard queries",
    "regression testing": "ensuring changes don't degrade plan quality",
    "milestone checkpoints": "structured verification points for implementation phases",
    "component unit testing": "validating individual optimizer modules in isolation",
    "cost estimation debugging": "diagnosing and fixing cost calculation issues",
    "selectivity estimation": "predicting fraction of rows surviving filters",
    "cardinality estimation": "estimating number of rows produced by operations",
    "statistics validation": "verifying accuracy of collected table statistics",
    "join ordering debugging": "troubleshooting dynamic programming implementation",
    "plan generation debugging": "diagnosing plan tree construction issues",
    "plan validation": "verifying structural and semantic correctness",
    "debug tracing": "collecting detailed execution information for analysis",
    "multi-dimensional histograms": "statistical structures capturing correlations between multiple columns",
    "correlation detection": "automatic identification of column relationships for optimization",
    "machine learning-based estimation": "using trained models to predict query cardinalities and costs",
    "parallel cost modeling": "resource-aware cost estimation for multi-threaded execution",
    "resource-aware optimization": "plan selection considering system resource constraints",
    "exchange operators": "data movement boundaries between parallel execution regions",
    "execution feedback collection": "gathering actual performance metrics during query execution",
    "adaptive statistics maintenance": "updating statistics based on execution feedback",
    "plan reoptimization": "modifying execution plans during query execution",
    "runtime adaptive optimization": "feedback-driven improvement of optimization decisions",
    "degree of parallelism": "number of parallel threads executing an operator",
    "coordination overhead": "cost of synchronizing parallel execution threads",
    "resource contention": "performance degradation from competing resource usage",
    "execution monitoring": "instrumentation for collecting runtime performance data",
    "feedback processing": "analysis of execution data to improve future optimization"
  }
}