{
  "types": {
    "Document": "fields: id str, content str, metadata Dict, source_path str, content_type str, created_at datetime",
    "TextChunk": "fields: id str, content str, document_id str, chunk_index int, start_char int, end_char int, overlap_with_previous int, metadata Dict",
    "EmbeddingVector": "fields: vector List[float], model_name str, dimension int, normalized bool, text_hash str, created_at datetime",
    "SearchResult": "fields: chunk TextChunk, similarity_score float, rank int, retrieval_method str, component_scores Dict",
    "RAGResponse": "fields: query str, answer str, sources List[SearchResult], confidence_score Optional[float], generation_metadata Dict, retrieval_stats Dict, total_time float, created_at datetime",
    "DocumentLoader": "abstract base class for document loading",
    "EmbeddingCache": "file-based cache for embedding vectors",
    "RateLimiter": "fields: config RateLimitConfig, tokens float, last_update float",
    "RAGPipeline": "main orchestrator class",
    "ChunkingStrategy": "enum for chunking methods",
    "RAGSettings": "configuration management class",
    "RateLimitConfig": "fields: requests_per_second float, burst_size int, backoff_multiplier float, max_backoff float, jitter_factor float",
    "EmbeddingGenerator": "abstract base class for embedding generation",
    "OpenAIEmbeddingGenerator": "OpenAI API-based embedding generator",
    "VectorStore": "abstract base class with add_vectors, search, delete_vectors methods",
    "ChromaVectorStore": "Chroma implementation of VectorStore",
    "VectorSearchResult": "fields: chunk_id str, similarity_score float, chunk_content str, metadata Dict, document_id str",
    "SearchQuery": "fields: vector List[float], top_k int, metadata_filter Dict, min_similarity float",
    "SimilaritySearchEngine": "main search coordination class",
    "BM25KeywordSearch": "keyword search implementation",
    "HybridRetrieval": "combines vector and keyword search",
    "RetrievalMetrics": "fields: query_id str, retrieved_chunk_ids List[str], relevant_chunk_ids List[str], recall_at_k Dict[int, float], precision_at_k Dict[int, float], mrr_score float, ndcg_at_k Dict[int, float], retrieval_time float",
    "VectorUtils": "vector normalization and similarity utilities",
    "LLMConfig": "fields: provider LLMProvider, model_name str, api_key Optional[str], base_url Optional[str], max_tokens int, temperature float, timeout int",
    "GenerationResult": "fields: text str, finish_reason str, token_count int, model_name str, generation_time float, citations_detected List[str], confidence_indicators List[str], metadata Dict",
    "TokenBudget": "fields: total_tokens int, system_prompt_tokens int, user_query_tokens int, response_reserve_tokens int, available_for_context int",
    "ChunkAllocation": "fields: chunk_id str, content str, tokens int, relevance_score float, included bool",
    "RAGRequest": "fields: query str, request_id str, streaming bool, max_context_tokens int, metadata_filters Dict, created_at datetime",
    "CircuitBreaker": "fields: name str, state CircuitState, failure_count int, config CircuitBreakerConfig",
    "CircuitState": "enum: CLOSED, OPEN, HALF_OPEN",
    "MessageValidator": "validation utilities for message types",
    "EvaluationQuery": "fields: query_id str, question str, relevant_chunk_ids List[str], query_type str, difficulty str, domain str, expected_answer Optional[str], metadata Dict",
    "GenerationMetrics": "fields: query_id str, generated_answer str, faithfulness_score float, relevance_score float, completeness_score float, coherence_score float, groundedness_score float, judge_explanation str, generation_time float, token_count int",
    "EvaluationFramework": "main evaluation pipeline coordinating all evaluation components",
    "ABTestFramework": "infrastructure for controlled experiments",
    "EvaluationDatabase": "database for storing and querying evaluation results",
    "LLMJudge": "LLM-as-judge evaluation for generation quality",
    "EvaluationDataset": "fields: dataset_id str, queries List[EvaluationQuery], documents List[Document], annotation_metadata Dict, creation_date datetime, last_updated datetime, version str, domain str",
    "ContinuousEvaluation": "system monitoring RAG performance over time",
    "CircuitBreakerConfig": "fields: failure_threshold int, recovery_timeout int, success_threshold int, timeout int",
    "ErrorAwareDocumentLoader": "document loader with error handling",
    "DiagnosticReport": "fields: component str, timestamp str, status str, metrics Dict, issues List, recommendations List, execution_time float",
    "ReasoningState": "enum: ANALYZING_QUERY, GATHERING_EVIDENCE, SYNTHESIZING_ANSWER, COMPLETE, FAILED",
    "ReasoningHop": "fields: hop_id str, query str, retrieved_chunks List[SearchResult], confidence_score float, dependencies Set[str], evidence_type str, timestamp datetime",
    "ReasoningContext": "fields: original_query str, decomposed_queries List[str], completed_hops Dict[str, ReasoningHop], current_state ReasoningState, evidence_graph Dict[str, List[str]], confidence_threshold float, max_hops int",
    "MultiHopRetrieval": "coordinator for multi-step retrieval reasoning",
    "QueryExpansionRetrieval": "retrieval with query reformulation and expansion",
    "LearnedSparseRetrieval": "SPLADE-based sparse retrieval implementation",
    "RAGMetric": "fields: name str, value float, tags Dict[str, str], timestamp float, metric_type str",
    "QualityAlert": "fields: alert_id str, component str, metric_name str, current_value float, threshold float, severity str, description str, timestamp float",
    "MetricsCollector": "abstract base class for collecting system metrics",
    "RetrievalQualityMonitor": "monitors retrieval quality metrics",
    "GenerationQualityMonitor": "monitors generation quality using LLM judge",
    "MonitoringSystem": "coordinates metric collection and alerting",
    "UserFeedback": "fields: feedback_id str, query str, response_id str, user_id str, feedback_type str, rating Optional[int], text_feedback Optional[str], helpful_sources List[str], unhelpful_sources List[str], timestamp float",
    "FeedbackCollector": "collects explicit and implicit user feedback",
    "FeedbackProcessor": "processes feedback to improve system",
    "ProcessingJob": "fields: job_id str, job_type str, input_data Dict[str, Any], priority int, retry_count int, max_retries int, created_at float, assigned_worker Optional[str]",
    "DistributedRAGProcessor": "coordinates distributed processing across workers",
    "ShardingConfig": "fields: total_shards int, replication_factor int, shard_assignment Dict[str, int], shard_nodes Dict[int, List[str]]",
    "DistributedVectorStore": "sharded vector storage with distributed search",
    "HorizontalScaler": "automatic resource scaling based on metrics",
    "CacheKey": "fields: namespace str, primary_key str, version str, parameters Dict[str, Any]",
    "MultiLayerCache": "hierarchical caching with local and distributed layers",
    "EmbeddingCacheOptimized": "specialized caching for embedding vectors",
    "QueryResultCache": "caching for search results with similarity matching",
    "CacheCoherence": "maintains cache consistency across updates"
  },
  "methods": {
    "load(file_path) -> Document": "abstract method to load document from file",
    "_generate_doc_id(file_path) -> str": "create consistent document identifier",
    "_extract_metadata(file_path) -> Dict": "extract file system metadata",
    "get(text, model_name) -> Optional[List[float]]": "retrieve cached embedding",
    "put(text, model_name, embedding) -> None": "store embedding in cache",
    "call_with_backoff(api_function, *args, **kwargs) -> Any": "execute API call with rate limiting",
    "ingest_document(file_path) -> str": "add new document to RAG system",
    "query(question, top_k=5) -> RAGResponse": "answer question using RAG pipeline",
    "generate_document_id(file_path) -> str": "create consistent document identifier",
    "generate_chunk_id(document_id, chunk_index) -> str": "create globally unique chunk ID",
    "extract_file_metadata(file_path) -> Dict": "extract file system metadata",
    "to_dict() -> Dict": "serialize model for storage",
    "from_dict(data) -> Model": "deserialize model from storage",
    "normalize() -> EmbeddingVector": "return normalized copy for cosine similarity",
    "cosine_similarity(other) -> float": "calculate similarity with another embedding",
    "get_source_documents() -> List[str]": "extract unique document IDs from sources",
    "format_citations() -> str": "format sources as citation text",
    "generate_embeddings(texts, chunk_ids) -> List[EmbeddingVector]": "generate embedding vectors for input texts",
    "get_dimension() -> int": "return embedding vector dimension",
    "get_model_name() -> str": "return model name for cache keys",
    "get(text, model_name) -> Optional[EmbeddingVector]": "retrieve cached embedding",
    "generate_cache_key(text, model_name, model_version) -> str": "generate consistent cache key",
    "cleanup_expired() -> int": "remove expired cache entries",
    "get_stats() -> Dict[str, Any]": "get cache performance statistics",
    "add_vectors(chunk_ids, vectors, metadata)": "store vectors with metadata",
    "search(query) -> List[VectorSearchResult]": "perform similarity search",
    "normalize_vector(vector) -> List[float]": "normalize to unit length",
    "cosine_similarity(vec1, vec2) -> float": "calculate similarity score",
    "hybrid_search(query, top_k) -> List[SearchResult]": "combined vector and keyword search",
    "get_stats() -> Dict": "database performance metrics",
    "delete_vectors(chunk_ids)": "remove stored vectors",
    "batch_normalize_vectors(vectors) -> List[List[float]]": "efficiently normalize multiple vectors",
    "generate(prompt, **kwargs) -> GenerationResult": "generate complete response from LLM",
    "stream_generate(prompt, **kwargs) -> Generator[str, None, None]": "generate streaming token-by-token response",
    "count_tokens(text) -> int": "count tokens in text for specific provider",
    "select_chunks(search_results, token_budget, selection_strategy) -> List[SearchResult]": "select chunks within token budget",
    "calculate(total, system_prompt, user_query, response_reserve, token_counter) -> TokenBudget": "calculate token allocation across components",
    "truncate_chunk(content, max_tokens, preserve_sentences) -> str": "truncate content while preserving readability",
    "generate_response(request) -> RAGResponse": "generate complete RAG response with citations",
    "stream_response(request) -> Generator[str, None, None]": "generate streaming RAG response",
    "query(request) -> RAGResponse": "answer question using RAG pipeline",
    "stream_query(request) -> AsyncIterator[str]": "generate streaming RAG response",
    "get_source_document() -> str": "extract source document ID",
    "get_text_preview(max_chars) -> str": "get truncated text for display",
    "acquire(tokens_needed) -> bool": "acquire tokens from rate limiting bucket",
    "call(func, *args, **kwargs) -> Any": "execute function call through circuit breaker",
    "calculate(query_id, retrieved_ids, relevant_ids, retrieval_time) -> RetrievalMetrics": "calculate all retrieval metrics for a single query",
    "store_retrieval_metrics(run_id, metrics)": "store retrieval evaluation results in database",
    "store_generation_metrics(run_id, metrics)": "store generation evaluation results in database",
    "compare_runs(run_id1, run_id2) -> Dict[str, float]": "compare metrics between two evaluation runs",
    "evaluate_answer(query, context, answer) -> GenerationMetrics": "evaluate generated answer using LLM-as-judge",
    "run_evaluation(dataset, run_id) -> Tuple[List[RetrievalMetrics], List[GenerationMetrics]]": "execute complete evaluation pipeline on dataset",
    "optimize_chunk_size(dataset, chunk_sizes) -> Dict[int, float]": "A/B test different chunk sizes and return performance scores",
    "identify_failure_modes(metrics, threshold) -> List[str]": "find queries with consistently poor retrieval performance",
    "run_ab_test(control_config, treatment_config, dataset, alpha) -> Dict": "run statistical A/B test comparing two configurations",
    "schedule_evaluation(config, frequency) -> EvaluationJob": "set up recurring evaluation runs",
    "detect_regressions(current, baseline) -> List[Regression]": "identify performance drops from baseline",
    "alert_stakeholders(regressions, severity)": "notify team of quality issues",
    "recommend_optimizations(results) -> List[Recommendation]": "suggest system improvements based on evaluation results",
    "acquire(tokens_needed=1) -> bool": "acquire tokens from rate limiting bucket",
    "wait_for_capacity(tokens_needed=1) -> None": "wait until rate limiter has capacity",
    "load_with_recovery(file_path) -> Optional[Document]": "load document with error handling and recovery",
    "_validate_file_access(file_path) -> bool": "validate file exists and is readable",
    "_detect_content_quality(content) -> float": "analyze content quality and return confidence score",
    "verify_milestone_1() -> bool": "verify Milestone 1 implementation meets acceptance criteria",
    "assert_embedding_similarity(embedding1, embedding2, min_similarity, message)": "assert that two embeddings meet minimum similarity threshold",
    "assert_search_result_quality(query, results, expected_content_keywords, min_results)": "assert that search results contain expected content indicators",
    "assert_rag_response_quality(response, query, min_sources, max_response_tokens)": "assert that RAG response meets quality and grounding criteria",
    "run_comprehensive_diagnosis(document_path, test_query) -> Dict": "execute complete pipeline diagnosis",
    "diagnose_document_ingestion(file_path) -> DiagnosticReport": "analyze document ingestion pipeline",
    "validate_similarity_search(test_cases) -> Dict[str, float]": "validate similarity search using known pairs",
    "diagnose_embedding_quality(text_pairs) -> Dict[str, Any]": "diagnose embedding quality using similarity relationships",
    "assess_answer_faithfulness(query, context, answer) -> Tuple[float, str]": "evaluate answer faithfulness to sources",
    "detect_hallucination_patterns(test_cases) -> Dict[str, List[str]]": "identify systematic hallucination patterns",
    "analyze_error_logs(time_window_hours) -> Dict[str, Any]": "analyze recent error logs for patterns",
    "multi_hop_search(query) -> List[SearchResult]": "perform multi-step retrieval with reasoning",
    "expanded_search(query, top_k) -> List[SearchResult]": "search with query expansion and reranking",
    "encode_sparse(text) -> Dict[int, float]": "generate sparse representation using SPLADE",
    "sparse_search(query, top_k) -> List[SearchResult]": "search using learned sparse representations",
    "collect_metrics() -> List[RAGMetric]": "gather system performance and quality metrics",
    "start_monitoring(collection_interval)": "begin continuous metric collection and alerting",
    "collect_explicit_feedback(feedback)": "store user-provided ratings and corrections",
    "track_implicit_feedback(user_id, query, response_id, interaction_data)": "extract signals from user behavior",
    "process_feedback_batch(batch_size)": "integrate feedback into system improvements",
    "create_experiment(experiment_config) -> str": "set up controlled A/B test",
    "analyze_experiment(experiment_id) -> ExperimentResults": "compute statistical results for experiment",
    "submit_document_ingestion(document_path) -> str": "queue document for distributed processing",
    "submit_batch_embedding(texts, chunk_ids) -> str": "queue batch embedding generation",
    "distributed_search(query) -> List[SearchResult]": "search across sharded vector stores",
    "add_documents_distributed(documents)": "insert documents across shards",
    "register_scaling_policy(service_name, scaling_policy)": "configure automatic scaling rules",
    "evaluate_scaling_decisions()": "check metrics and scale resources",
    "generate_embedding_key(text, model_name, model_version) -> CacheKey": "create consistent cache key for embeddings",
    "get_cached_embedding(text, model_name, model_version) -> Optional[List[float]]": "retrieve cached embedding if available",
    "get_similar_results(query_embedding, search_params) -> Optional[List[SearchResult]]": "find cached results for similar queries",
    "invalidate_document_update(document_id)": "clear caches affected by document changes",
    "invalidate_model_update(model_name, old_version)": "clear caches for model version updates"
  },
  "constants": {
    "DEFAULT_CHUNK_SIZE": "typical chunk size in characters",
    "DEFAULT_OVERLAP": "default overlap between chunks",
    "EMBEDDING_DIMENSION": "vector dimension for embeddings",
    "DEFAULT_TOP_K": "default number of search results to consider",
    "MIN_SIMILARITY_THRESHOLD": "minimum acceptable similarity score for relevance",
    "DEFAULT_RECALL_THRESHOLD": "0.7 - minimum acceptable recall@10 for production",
    "DEFAULT_FAITHFULNESS_THRESHOLD": "0.8 - minimum acceptable faithfulness score",
    "STATISTICAL_SIGNIFICANCE_ALPHA": "0.05 - p-value threshold for A/B tests",
    "MINIMUM_SAMPLE_SIZE": "50 - minimum queries needed for reliable evaluation",
    "DEFAULT_FAILURE_THRESHOLD": "5 failures before opening circuit",
    "DEFAULT_RECOVERY_TIMEOUT": "60 seconds before trying half-open",
    "DEFAULT_REQUESTS_PER_SECOND": "10.0 maximum request rate",
    "DEFAULT_BURST_SIZE": "20 maximum burst requests",
    "MIN_CONTENT_LENGTH": "50 minimum characters for valid document",
    "QUALITY_THRESHOLD": "0.8 minimum content quality score",
    "DEFAULT_MAX_HOPS": "5 maximum reasoning steps for multi-hop retrieval",
    "DEFAULT_EXPANSION_COUNT": "5 maximum query expansions",
    "MONITORING_INTERVAL_SECONDS": "60.0 default metric collection frequency",
    "CACHE_TTL_SECONDS": "3600 default cache expiration time",
    "FEEDBACK_PROCESSING_BATCH_SIZE": "100 feedback items per processing batch",
    "SCALING_COOLDOWN_SECONDS": "300 minimum time between scaling operations",
    "QUALITY_ALERT_THRESHOLD": "0.8 minimum acceptable quality score"
  },
  "terms": {
    "RAG": "Retrieval Augmented Generation - combines search with LLM generation",
    "embedding vector": "dense numerical representation of text for semantic similarity",
    "chunking": "splitting documents into smaller pieces for processing",
    "vector database": "specialized database for storing and searching embedding vectors",
    "similarity search": "finding most semantically similar text passages",
    "hallucination": "when LLMs generate plausible but factually incorrect information",
    "context window": "maximum number of tokens an LLM can process at once",
    "semantic search": "search based on meaning rather than exact keywords",
    "cosine similarity": "metric for measuring similarity between vectors",
    "token": "basic unit of text processing for language models",
    "rate limiting": "controlling API request frequency to stay within quotas",
    "exponential backoff": "retry strategy with increasing delays",
    "cache hit rate": "percentage of requests served from cache",
    "vector normalization": "scaling vectors to unit length for cosine similarity",
    "batch processing": "processing multiple items together for efficiency",
    "circuit breaker": "automatic failure protection mechanism",
    "semantic fingerprint": "unique pattern representing text meaning",
    "token consumption": "API usage measured in text tokens processed",
    "jitter": "random delay added to prevent synchronized retries",
    "hybrid retrieval": "combining vector similarity with keyword search",
    "reciprocal rank fusion": "algorithm for combining multiple result rankings",
    "approximate nearest neighbor": "fast algorithm for similarity search",
    "BM25": "probabilistic ranking function for keyword search",
    "metadata filtering": "restricting search to documents with specific attributes",
    "grounding": "ensuring LLM responses are based on retrieved context",
    "streaming": "token-by-token response delivery for real-time user feedback",
    "citation drift": "gradual loss of source attribution accuracy in long responses",
    "prompt injection": "malicious or accidental interference with prompt instructions through retrieved content",
    "token budgeting": "allocating context window space across different prompt components",
    "context prioritization": "ranking and selecting most relevant chunks when space is limited",
    "asynchronous coordination": "managing concurrent operations and dependencies between pipeline stages",
    "token bucket": "rate limiting algorithm with refillable capacity",
    "backpressure": "system response to overload by slowing input processing",
    "coordination context": "tracking structure for request progress through pipeline",
    "graceful degradation": "maintaining reduced functionality when components fail",
    "recall at K": "fraction of relevant chunks found in top-K retrieved results",
    "mean reciprocal rank": "MRR - metric focusing on rank of first relevant result",
    "faithfulness": "accuracy of generated answer to retrieved context sources",
    "relevance": "how well generated answer addresses the user's specific question",
    "completeness": "coverage of important aspects needed for thorough answer",
    "coherence": "logical flow and readability of generated response",
    "groundedness": "proper citation and source attribution in generated answers",
    "LLM-as-judge": "using powerful LLM to evaluate quality of generated responses",
    "ground truth": "human-annotated correct answers for evaluation dataset",
    "inter-annotator agreement": "consistency between different human evaluators",
    "statistical significance": "confidence that observed differences aren't due to chance",
    "A/B testing": "controlled experiment comparing system variants",
    "evaluation dataset": "curated test queries with ground truth for measuring system quality",
    "synthetic dataset": "artificially generated queries and answers for evaluation",
    "continuous evaluation": "automated monitoring of system performance over time",
    "performance regression": "decrease in system quality metrics over time",
    "failure mode analysis": "systematic identification of query types with poor performance",
    "metric gaming": "optimizing individual metrics in ways that hurt overall quality",
    "evaluation-production mismatch": "differences between test and real environments that invalidate metrics",
    "circuit state": "current operational mode of circuit breaker protection",
    "failure threshold": "number of failures before circuit opens",
    "recovery timeout": "time before attempting service recovery",
    "multi-hop reasoning": "retrieval process requiring multiple search steps to gather related information",
    "query expansion": "generating alternative phrasings to improve retrieval coverage",
    "learned sparse retrieval": "neural approaches like SPLADE that predict term importance weights",
    "SPLADE": "Sparse Lexical and Expansion Model for neural sparse retrieval",
    "production monitoring": "comprehensive observability for AI system quality and performance",
    "user feedback loops": "systematic collection and integration of user corrections and preferences",
    "A/B testing infrastructure": "controlled experimentation framework for AI system improvements",
    "distributed processing": "scaling RAG components across multiple machines",
    "horizontal scaling": "adding more machines rather than upgrading individual components",
    "cache coherence": "maintaining consistency across distributed cache layers",
    "implicit feedback": "user behavior signals like query refinement and session patterns",
    "explicit feedback": "direct user ratings and corrections",
    "evidence aggregation": "combining information from multiple retrieval rounds",
    "query decomposition": "breaking complex questions into focused sub-queries",
    "reasoning context": "accumulated state during multi-step retrieval process",
    "cache invalidation": "removing stale data when underlying information changes",
    "shard assignment": "distributing documents across database partitions",
    "traffic allocation": "directing user requests to different system variants for testing",
    "metric collection": "gathering performance and quality measurements",
    "alert fatigue": "reduced responsiveness due to too many notifications",
    "auto-scaling policies": "rules for automatically adjusting resource allocation",
    "service mesh": "infrastructure layer providing observability and reliability for distributed services"
  }
}