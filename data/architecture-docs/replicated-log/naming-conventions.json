{
  "types": {
    "LogEntry": "fields: SequenceNumber uint64, Term uint64, Timestamp time.Time, Data []byte, Checksum uint32",
    "NodeState": "enum: StateFollower, StateCandidate, StateLeader",
    "NodeMetadata": "fields: NodeID string, Address string, State NodeState, Role NodeRole, CurrentTerm uint64, LastHeartbeat time.Time, LastLogIndex uint64, IsHealthy bool",
    "ClusterState": "fields: CurrentTerm uint64, LeaderID string, Nodes map[string]*NodeMetadata, QuorumSize int, LastUpdated time.Time",
    "NodeConfig": "fields: NodeID string, ListenAddr string, DataDir string, Peers []string, HeartbeatInterval time.Duration, ElectionTimeout time.Duration, LogRetention time.Duration, QuorumSize int",
    "HTTPTransport": "fields: client *http.Client, connections map[string]*http.Client, mu sync.RWMutex, timeout time.Duration",
    "ReplicationError": "fields: NodeID string, Err error",
    "TimeoutError": "fields: Operation string, Duration time.Duration",
    "StorageConfig": "fields: SegmentSizeBytes int64, RetentionPeriod time.Duration, SyncInterval time.Duration",
    "LogEngine": "fields: dataDir string, currentSegment *Segment, segments map[uint64]*Segment, index *MemoryIndex, wal *WriteAheadLog, nextSeqNum uint64",
    "Segment": "segment file management structure",
    "WriteAheadLog": "durability guarantees for writes",
    "MemoryIndex": "in-memory mapping from sequence numbers to file offsets",
    "Compactor": "background compaction process",
    "RequestVoteArgs": "fields: Term uint64, CandidateID string, LastLogIndex uint64, LastLogTerm uint64",
    "RequestVoteReply": "fields: Term uint64, VoteGranted bool",
    "AppendEntriesArgs": "fields: Term uint64, LeaderID string, PrevLogIndex uint64, PrevLogTerm uint64, Entries []LogEntry, LeaderCommitIndex uint64",
    "AppendEntriesReply": "fields: Term uint64, Success bool, ConflictIndex uint64, ConflictTerm uint64",
    "HeartbeatMessage": "fields: SenderID string, Term uint64, LeaderID string, LastLogIndex uint64, Timestamp time.Time, ClusterSize int",
    "HeartbeatResponse": "fields: ReceiverID string, Term uint64, LastLogIndex uint64, Success bool",
    "FailureDetector": "failure detection coordinator",
    "RecoveryCoordinator": "node recovery manager",
    "HeartbeatTransport": "network transport for heartbeats",
    "ReplicatedLogClient": "fields: transport Transport, clusterNodes []string, currentLeader string, leaderTerm uint64, leaderAddr string, cacheExpiry time.Time, retryPolicy RetryPolicy, consistency ConsistencyLevel, timeout time.Duration, subscriptions map[string]*Subscription",
    "RetryPolicy": "fields: MaxAttempts int, InitialInterval time.Duration, MaxInterval time.Duration, Multiplier float64, Jitter bool, MaxDuration time.Duration",
    "AppendOptions": "fields: Timeout time.Duration, RequiredReplicas int, SyncMode SyncMode, RetryPolicy RetryPolicy",
    "ReadOptions": "fields: Consistency ConsistencyLevel, Timeout time.Duration, PreferLocal bool, MaxStaleness time.Duration, ReadTimestamp uint64",
    "SubscribeOptions": "fields: BufferSize int, ReconnectPolicy ReconnectPolicy, FilterFunc FilterFunc",
    "WriteCoordinator": "coordination logic for write operations",
    "ReadCoordinator": "coordination logic for read operations",
    "ElectionCoordinator": "coordination logic for leader elections",
    "CoordinationError": "distributed operation error type",
    "PartitionHandler": "manages cluster behavior during network partitions",
    "NodeFailureDetector": "tracks node health and handles failure scenarios",
    "IntegrityValidator": "handles checksum computation and corruption detection",
    "NodeHealth": "fields: NodeID string, LastHeartbeat time.Time, State HealthState, FailureCount int, RecoveryAttempts int",
    "TestCluster": "fields: nodes map[string]*TestNode, basePort int, dataDir string, mu sync.RWMutex",
    "TestNode": "fields: ID string, Port int, DataDir string, Process *exec.Cmd, LogFile *os.File, Started bool",
    "SystemHealth": "comprehensive health status structure",
    "LagDiagnosis": "replication lag analysis results",
    "OperationTrace": "distributed operation timeline",
    "ChaosController": "chaos engineering test coordinator",
    "DebugServer": "debug endpoint HTTP server",
    "Logger": "structured logging implementation",
    "LogStatus": "fields: FirstSequenceNum uint64, LastSequenceNum uint64, SegmentCount int, TotalSizeBytes int64, IndexSizeBytes int64",
    "ReplicationStatus": "fields: NodeID string, LastReplicatedSeq uint64, ReplicationLag int64, LastHeartbeat time.Time, IsHealthy bool",
    "NodeRole": "enum: RoleVoter, RoleReadReplica, RoleWitness",
    "PartitionStrategy": "interface: GetShard(key []byte) int",
    "ShardDirectory": "interface: GetCluster(shardID int) []string",
    "ShardedClient": "client with cluster topology awareness",
    "DatacenterSync": "message for cross-DC replication",
    "DatacenterHeartbeat": "message for DC failure detection",
    "SnapshotManager": "fields: logEngine *LogEngine, storageDir string, retentionCount int, compressionType string, mu sync.Mutex",
    "SnapshotStorage": "persists snapshot files",
    "RestoreCoordinator": "rebuilds state from snapshots",
    "CompactionController": "removes superseded log entries",
    "StreamManager": "manages active streams",
    "StreamConsumer": "delivers entries to clients",
    "StreamCheckpoint": "tracks consumer progress",
    "StreamBuffer": "handles backpressure",
    "ReadReplicaManager": "fields: nodeID string, leader string, replicationLag time.Duration, lastSyncIndex uint64, transport Transport, logEngine *LogEngine, mu sync.RWMutex",
    "DatacenterCoordinator": "fields: localDC string, remoteDCs map[string]*RemoteDatacenter, syncInterval time.Duration, compressionType string, conflictPolicy ConflictResolutionPolicy, transport Transport, mu sync.RWMutex",
    "SnapshotMetadata": "snapshot file metadata"
  },
  "methods": {
    "LoadConfig(filename string) (*NodeConfig, error)": "loads node configuration from file with defaults",
    "SendRPC(ctx context.Context, addr string, method string, request interface{}, response interface{}) error": "sends RPC request and decodes response",
    "StartServer(addr string, handlers map[string]http.HandlerFunc) error": "starts HTTP server with RPC handlers",
    "fsync()": "forces data to disk for durability",
    "ComputeChecksum(entry *LogEntry) uint32": "calculates CRC32 checksum for log entry",
    "VerifyChecksum(entry *LogEntry) bool": "validates log entry checksums",
    "NewClusterState(nodeIDs []string, quorumSize int) *ClusterState": "creates new cluster state with initial membership",
    "AddNode(nodeID, address string)": "safely adds new node to cluster membership",
    "RemoveNode(nodeID string)": "safely removes node from cluster membership",
    "HasQuorum() bool": "checks if quorum available",
    "NewLogEngine(dataDir string, config *StorageConfig) (*LogEngine, error)": "creates new log storage engine",
    "Append(ctx context.Context, term uint64, data []byte) (uint64, error)": "adds new entry to log and returns sequence number",
    "Get(ctx context.Context, seqNum uint64) (*api.LogEntry, error)": "retrieves log entry by sequence number",
    "GetRange(ctx context.Context, startSeq, endSeq uint64) ([]*api.LogEntry, error)": "retrieves multiple entries in range",
    "rotateSegmentIfNeeded() error": "creates new segment if current exceeds size limit",
    "Close() error": "gracefully shuts down log engine",
    "Start(ctx context.Context) error": "begins failure detection process",
    "Stop() error": "gracefully shuts down detector",
    "SendHeartbeat(ctx context.Context, addr string, msg *HeartbeatMessage) (*HeartbeatResponse, error)": "sends heartbeat to node",
    "UpdateNodeHealth(nodeID string, healthy bool, heartbeat time.Time)": "updates node health status",
    "StartRecovery(ctx context.Context) error": "initiates recovery process",
    "synchronizeLog(ctx context.Context, leaderID string) error": "catches up on missed entries",
    "discoverClusterLeader(ctx context.Context) (string, error)": "finds current leader",
    "validateConsistency(ctx context.Context, leaderID string) error": "verifies log consistency",
    "NewReplicatedLogClient(nodes []string, options ...ClientOption) (*ReplicatedLogClient, error)": "creates new client with cluster node list and options",
    "Append(ctx context.Context, data []byte, options AppendOptions) (uint64, error)": "appends data to replicated log",
    "Get(ctx context.Context, seqNum uint64, options ReadOptions) (*api.LogEntry, error)": "retrieves single entry by sequence number",
    "GetRange(ctx context.Context, startSeq, endSeq uint64, options ReadOptions) ([]*api.LogEntry, error)": "retrieves multiple entries in range",
    "Subscribe(ctx context.Context, startSeq uint64, options SubscribeOptions) (<-chan api.LogEntry, error)": "creates streaming subscription from starting sequence",
    "getCurrentLeader(ctx context.Context) (string, string, error)": "discovers and caches current cluster leader",
    "invalidateLeaderCache()": "clears cached leader information",
    "SendRPC(ctx context.Context, addr, method string, request, response interface{}) error": "sends RPC request to specified node",
    "Retry(ctx context.Context, policy RetryPolicy, operation func() error) error": "executes operation with retry logic",
    "RetryableError(err error) bool": "determines if error should trigger retry",
    "getCurrentLeader(ctx) (string, string, error)": "discovers and caches current cluster leader",
    "Append(ctx, data, options) (uint64, error)": "appends data to log and returns sequence number",
    "Get(ctx, seqNum, options) (*LogEntry, error)": "retrieves single entry by sequence number",
    "CoordinateWrite(ctx, data) (uint64, error)": "handles complete write operation flow",
    "CoordinateRead(ctx, seqNum, opts) (*LogEntry, error)": "handles read requests with consistency levels",
    "StartElection(ctx) error": "initiates new leader election round",
    "HandleVoteRequest(ctx, args) (*RequestVoteReply, error)": "processes incoming vote requests",
    "BecomeLeader(ctx) error": "transitions node to leader and initializes state",
    "ReplicateEntry(ctx, entry, prevLogIndex, prevLogTerm) error": "sends entry to specific follower",
    "StartElection(ctx context.Context) error": "initiates new leader election round",
    "HandleVoteRequest(ctx context.Context, args *RequestVoteArgs) (*RequestVoteReply, error)": "processes incoming vote requests",
    "BecomeLeader(ctx context.Context) error": "transitions node to leader role",
    "DetectPartition(ctx context.Context) error": "analyzes connectivity to identify network splits",
    "HandleQuorumLoss(ctx context.Context) error": "responds to loss of majority quorum",
    "RecoverFromPartition(ctx context.Context) error": "handles partition healing and reconciliation",
    "StartFailureDetection(ctx context.Context) error": "begins monitoring node health",
    "HandleNodeFailure(ctx context.Context, nodeID string) error": "processes detected node failures",
    "HandleNodeRecovery(ctx context.Context, nodeID string) error": "processes node recovery after failure",
    "NewTestCluster(nodeCount int) (*TestCluster, error)": "creates new test cluster with specified number of nodes",
    "StartAll(ctx context.Context) error": "starts all nodes in the cluster",
    "StopNode(nodeID string) error": "gracefully stops a specific node",
    "KillNode(nodeID string) error": "forcefully terminates a node to simulate crash",
    "RestartNode(ctx context.Context, nodeID string) error": "restarts a previously stopped node",
    "WaitForLeader(timeout time.Duration) (string, error)": "waits for leader election and returns leader ID",
    "GetNodeEndpoints() []string": "returns HTTP endpoints for all running nodes",
    "Cleanup() error": "stops all nodes and removes temporary directories",
    "Get(ctx context.Context, seqNum uint64, options ReadOptions) (*LogEntry, error)": "retrieves entry by sequence number",
    "GetSystemHealth(ctx) (*SystemHealth, error)": "comprehensive health check across all subsystems",
    "DiagnoseReplicationLag(ctx, nodeID) (*LagDiagnosis, error)": "analyzes replication performance issues",
    "TraceDistributedOperation(ctx, traceID) (*OperationTrace, error)": "follows operation across multiple nodes",
    "InjectNetworkPartition(ctx, group1, group2) error": "simulates network partition between node groups",
    "SimulateNodeFailure(ctx, nodeID, duration) error": "causes node to become unresponsive",
    "writeEntry(level, component, operation, message, term, seqNum, fields, ctx)": "writes structured log entry",
    "RegisterHandlers(mux)": "registers debug HTTP endpoints",
    "GetTraceID(ctx) string": "extracts trace ID from context",
    "StartReplication(ctx context.Context) error": "begins receiving updates from leader",
    "ServeRead(ctx context.Context, seqNum uint64, options ReadOptions) (*LogEntry, error)": "handles read requests with staleness bounds",
    "CreateSnapshot(ctx context.Context, upToSequence uint64) (*SnapshotMetadata, error)": "generates consistent point-in-time capture",
    "RestoreFromSnapshot(ctx context.Context, snapshotID string) error": "rebuilds log engine state from snapshot",
    "StartCrossDCSync(ctx context.Context) error": "begins asynchronous replication to remote datacenters",
    "SyncToRemoteDC(ctx context.Context, dcID string, entries []*LogEntry) error": "replicates local changes to specific remote datacenter",
    "GetShard(key []byte) int": "determines shard for partition strategy",
    "GetCluster(shardID int) []string": "maps shard IDs to cluster endpoints",
    "Get(ctx context.Context, seqNum uint64) (*LogEntry, error)": "retrieves log entry by sequence number"
  },
  "constants": {
    "StateFollower": "follower node state",
    "StateCandidate": "candidate node state",
    "StateLeader": "leader node state",
    "ErrNotLeader": "error when operation sent to non-leader node",
    "ErrNoLeader": "error when no leader currently elected",
    "ErrInvalidSequence": "error for invalid sequence number",
    "ErrLogCorrupted": "log file corrupted",
    "ErrQuorumNotMet": "insufficient nodes for quorum",
    "ErrNodeNotFound": "node not found in cluster",
    "ErrAlreadyExists": "entry already exists",
    "QuorumSize": "minimum nodes required for decisions",
    "ElectionTimeout": "150-300ms timeout for leader election process",
    "HeartbeatInterval": "100ms periodic liveness signal timing",
    "SuspectedTimeout": "3 seconds default",
    "FailedTimeout": "6 seconds default",
    "ConsistencyStrong": "strong consistency requiring leader reads",
    "ConsistencyEventual": "read from any replica",
    "DefaultRetryAttempts": "5",
    "DefaultTimeout": "5 seconds",
    "CacheExpiryTime": "60 seconds",
    "ErrSplitBrain": "multiple leaders detected",
    "ErrStaleLeader": "leader term lower than follower",
    "ErrPartitionHealed": "previously failed nodes reachable",
    "HealthStateHealthy": "node healthy state",
    "HealthStateSuspected": "node suspected state",
    "HealthStateFailed": "node failed state",
    "HealthStateRecovering": "node recovering state",
    "RoleVoter": "participates in leader election",
    "RoleReadReplica": "receives updates, serves reads only",
    "RoleWitness": "participates in elections, no data",
    "ConsistencyLinearizable": "read from leader only",
    "ConsistencyReadYourWrites": "ensure client sees own writes",
    "ConsistencyBounded": "read from replica within staleness bound"
  },
  "terms": {
    "distributed log": "ordered sequence of immutable records replicated across multiple machines",
    "primary-backup replication": "replication strategy where one node handles writes and others maintain copies",
    "quorum": "minimum nodes required for cluster decisions",
    "split-brain": "multiple nodes believing they are leader",
    "leader election": "process of selecting coordinator node",
    "heartbeat": "periodic liveness signal between nodes",
    "failure detection": "mechanism to identify crashed or unreachable nodes",
    "log compaction": "process of removing old log entries to reclaim disk space",
    "sequence number": "monotonically increasing identifier for log entries",
    "term": "leadership epoch number",
    "follower": "node that receives and applies updates from the leader",
    "candidate": "node that is requesting votes to become the leader",
    "fsync": "system call that forces data to be written to disk for durability",
    "segment files": "individual files containing ranges of log entries",
    "write-ahead log": "durability mechanism that records operations before applying them",
    "in-memory index": "fast lookup structure mapping sequence numbers to file offsets",
    "append-only semantics": "property where data is only added, never modified in place",
    "two-phase detection": "Healthy -> Suspected -> Failed state progression",
    "thundering herd": "multiple nodes acting simultaneously causing overload",
    "recovery": "process of failed node rejoining cluster",
    "catch-up synchronization": "replaying missed log entries during recovery",
    "cluster reintegration": "recovered node resuming full participation",
    "primary discovery": "process of locating current cluster leader among nodes",
    "client failover": "automatic switching to new leader when primary fails",
    "consistency levels": "guarantees about data freshness for read operations",
    "leader cache": "client-side storage of current leader information",
    "retry policy": "strategy for handling and recovering from operation failures",
    "subscription stream": "continuous delivery of new log entries to clients",
    "connection pooling": "reuse of network connections to improve performance",
    "exponential backoff": "retry strategy with increasing delays between attempts",
    "read-your-writes consistency": "guarantee that client sees its own writes immediately",
    "strong consistency": "guarantee that reads return most recent committed data",
    "eventual consistency": "guarantee that all replicas will converge but reads may return stale data",
    "write coordination": "process of replicating writes across cluster nodes",
    "cluster convergence": "process of achieving consistent state across all nodes",
    "checksum validation": "process to detect data corruption",
    "cascading failures": "failure of components triggering other failures",
    "partition recovery": "healing process after network split",
    "state reconciliation": "process of resolving divergent cluster states",
    "corruption detection": "identifying data integrity violations",
    "hardware failure recovery": "restoration after physical component failure",
    "chaos engineering": "intentional failure injection to verify system resilience",
    "end-to-end testing": "complete system validation under realistic conditions",
    "milestone verification": "incremental validation at each development stage",
    "throughput testing": "measuring system capacity under load",
    "latency testing": "measuring response time characteristics",
    "scalability testing": "validating performance as system size increases",
    "network partition": "scenario where nodes cannot communicate",
    "failure injection": "controlled introduction of system failures",
    "invariant checking": "continuous validation of system properties",
    "performance baseline": "reference measurements for comparison",
    "distributed tracing": "end-to-end visibility into request flow through cluster",
    "false positives": "healthy nodes incorrectly marked as failed",
    "trace correlation": "linking related events across nodes using trace IDs",
    "state invariants": "properties that must always hold true",
    "partition simulation": "controlled testing of network split scenarios",
    "structured logging": "consistent log format enabling automated analysis",
    "read replicas": "read-only nodes that can serve eventually consistent reads without participating in write quorum decisions",
    "horizontal sharding": "partitioning log space across multiple independent replicated log clusters",
    "multi-datacenter replication": "replicating data across datacenters for disaster recovery and geographic locality",
    "snapshot": "point-in-time state captures that allow truncating old log entries",
    "log stream processing": "processing log entries as they arrive for event sourcing and real-time analytics",
    "transaction support": "atomic operations across multiple log entries with ACID properties",
    "hierarchical replication": "replication model with strong consistency within datacenters and eventual consistency across datacenters",
    "copy-on-write": "snapshot strategy that creates consistent snapshots without blocking write operations",
    "consumer groups": "scalable stream processing where consumers automatically distribute partition assignments",
    "two-phase commit": "distributed transaction protocol for atomic operations across multiple shards"
  }
}