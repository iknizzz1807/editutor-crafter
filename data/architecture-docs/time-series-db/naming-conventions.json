{
  "types": {
    "DataPoint": "fields: Timestamp time.Time, Value float64",
    "SeriesKey": "fields: Measurement string, Tags map[string]string",
    "Series": "fields: Name string, Columns []string, Values [][]interface{}, Tags map[string]string",
    "TimeRange": "fields: Start time.Time, End time.Time",
    "Query": "fields: Measurement string, Tags map[string]string, TimeRange TimeRange, Aggregate AggregateFunction, GroupByWindow time.Duration, Field string",
    "Segment": "fields: path string, file *os.File, mu sync.RWMutex, size int64, maxSize int64, closed bool, firstID uint64, lastID uint64",
    "SegmentConfig": "fields: MaxSizeBytes int64, SyncInterval time.Duration, SyncOnWrite bool",
    "StorageEngine": "fields: config Config, wal *wal.WAL, memtables *MemtableManager, tsmReader *tsm.Reader, tsmWriter *tsm.Writer, mu sync.RWMutex, seriesIndex map[string]*SeriesMetadata, tsmFiles []*TSMFileRef, blockCache *BlockCache, flushCh chan *memtable.Memtable, compactCh chan compactionPlan, stopCh chan struct{}",
    "Config": "fields: DataDir string, MaxMemtableSize int64, WalSegmentSize int64, BlockCacheSize int64, FlushConcurrency int",
    "SeriesMetadata": "fields: (Conceptual - e.g., ID uint64, LastTimestamp time.Time)",
    "AggregateFunction": "enum: AggregateNone, AggregateSum, AggregateAvg, AggregateMin, AggregateMax, AggregateCount",
    "TSMFile": "fields: path string, file *os.File, mmap []byte, index TSMIndex, mu sync.RWMutex",
    "TSMIndex": "fields: entries map[string][]IndexEntry",
    "IndexEntry": "fields: MinTime uint64, MaxTime uint64, Offset uint64, Size uint32",
    "BlockHeader": "fields: MinTime uint64, MaxTime uint64",
    "CompressedBlock": "fields: Timestamps []byte, Values []byte, Checksum uint32",
    "TSMHeader": "fields: Magic uint32, Version uint64",
    "TSMWriter": "fields: file *os.File, buf *bytes.Buffer, index map[string][]IndexEntry, blockSize int",
    "TSMReader": "fields: path string, data []byte, index TSMIndex",
    "Memtable": "fields: data map[string][]DataPoint, size int64, mu sync.RWMutex, maxOutOfOrderWindow time.Duration",
    "QueryPlan": "fields: SeriesKeys []models.SeriesKey, FileBlocks map[string][]BlockRef, Query models.Query",
    "BlockRef": "fields: FilePath string, Entry storage.IndexEntry",
    "Parser": "fields: buf []byte, pos int",
    "SeriesScanner": "fields: seriesKey models.SeriesKey, plan *QueryPlan, current models.DataPoint, fileIndex int, blockIter *BlockIterator, points []models.DataPoint, pointIdx int",
    "WindowAggregator": "fields: input Iterator, window time.Duration, aggregate models.AggregateFunction, windowStart time.Time, sum float64, count int64, min float64, max float64, result models.DataPoint, ready bool",
    "RetentionPolicy": "fields: Measurement string, Duration time.Duration, Default bool",
    "TTLEnforcer": "fields: policies map[string]RetentionPolicy, interval time.Duration, stopCh chan struct{}, wg sync.WaitGroup, mu sync.RWMutex",
    "TSMFileRef": "fields: Path string, MinTime time.Time, MaxTime time.Time, Measurement string, Tombstoned bool, TombstoneTime time.Time",
    "CompactionPlan": "fields: Level int, InputFiles []string, OutputFile string, TimeRange TimeRange, Series []string",
    "CompactionStats": "fields: Level int, FilesIn int, FilesOut int, BytesIn int64, BytesOut int64, Duration time.Duration",
    "CompactionManager": "fields: plans chan CompactionPlan, stats map[int]CompactionStats, mu sync.RWMutex, stopCh chan struct{}",
    "DownsamplePolicy": "fields: Measurement string, AfterDuration time.Duration, Granularity time.Duration, Aggregate AggregateFunction",
    "RollupSeriesKey": "fields: OriginalKey SeriesKey, Granularity time.Duration, Function AggregateFunction",
    "Downsampler": "fields: policies []DownsamplePolicy, levelMap map[int]time.Duration",
    "Job": "fields: Name string, Interval time.Duration, Run func(ctx context.Context) error, Jitter time.Duration",
    "Scheduler": "fields: jobs []*Job, cancel context.CancelFunc, wg sync.WaitGroup, mu sync.RWMutex",
    "CompactionPlanner": "fields: storage *storage.Engine, levelConfig map[int]LevelConfig",
    "LevelConfig": "fields: MaxFiles int, MaxSizeBytes int64, TargetSize int64, DownsampleGranularity time.Duration",
    "Executor": "fields: storage *storage.Engine, dataDir string, maxConcurrent int",
    "QueryRequest": "fields: Query string",
    "QueryResponse": "fields: Results []Result",
    "Result": "fields: Series []Series, Error string",
    "Point": "fields: Measurement string, Tags map[string]string, Fields map[string]interface{}, Timestamp time.Time",
    "Server": "fields: router *mux.Router, storage StorageEngine, queryEngine QueryEngine, logger *zap.Logger, httpServer *http.Server",
    "responseWriter": "fields: ResponseWriter http.ResponseWriter, statusCode int",
    "StreamWriter": "fields: w http.ResponseWriter, encoder *json.Encoder, flushed bool",
    "TempoError": "fields: Code string, Message string, Severity ErrorSeverity, Component string, Operation string, InnerError error, StackTrace []byte, Timestamp time.Time",
    "RecoveryAction": "fields: Name string, Description string, Execute func() error",
    "DiskMonitor": "fields: dataDir string, threshold float64, checkInterval time.Duration, stopCh chan struct{}",
    "GoldenFile": "fields: Name string, Path string, Checksum string",
    "TimeSeriesGenerator": "fields: rng *rand.Rand",
    "OperationRecorder": "fields: mu sync.Mutex, file *os.File, encoder *json.Encoder, enabled bool",
    "RecordedOperation": "fields: ID string, Timestamp time.Time, Type string, Component string, Arguments json.RawMessage, Result json.RawMessage, Error string, Duration time.Duration",
    "StorageBackend": "interface with Type(), Write(), Read(), Delete(), Exists(), List(), Stats() methods",
    "BackendStats": "fields: TotalBytes int64, UsedBytes int64, FileCount int64, LatencyMS float64",
    "TierManager": "fields: backends map[string]StorageBackend, policies []TierPolicy, metadata MetadataStore, mu sync.RWMutex",
    "TierPolicy": "fields: SourceTier string, DestinationTier string, Condition PolicyCondition, BatchSize int",
    "PolicyCondition": "fields: AgeOlderThan time.Duration, AccessOlderThan time.Duration, SizeGreaterThan int64",
    "QueryRouter": "fields: shardMap *ShardMap, nodeClients map[string]*NodeClient, merger *ResultMerger",
    "ResultMerger": "fields: mergeStrategy MergeStrategy"
  },
  "methods": {
    "NewSegment(path string, firstID uint64, config SegmentConfig) returns (*Segment, error)": "creates or opens a WAL segment file",
    "WriteEntry(data []byte) returns (uint64, error)": "appends an entry to the segment",
    "Scan(fn func(id uint64, data []byte) error) returns error": "reads all entries from the segment",
    "Close() returns error": "closes the segment file",
    "Delete() returns error": "removes the segment file from disk",
    "NewStorageEngine(config Config) returns (*StorageEngine, error)": "initializes the storage engine",
    "WritePoint(ctx context.Context, seriesKey models.SeriesKey, point models.DataPoint) returns error": "Writes a single data point to the storage engine",
    "Query(ctx context.Context, query models.Query) returns (QueryResult, error)": "Executes a range query",
    "FlushMemtable(mt *memtable.Memtable) returns error": "flushes an immutable memtable to TSM files",
    "Compact(plan compactionPlan) returns error": "runs compaction on selected TSM files",
    "TimeRange.Contains(t time.Time) returns bool": "checks if a timestamp is within the range",
    "TimeRange.Overlaps(other TimeRange) returns bool": "checks if two time ranges overlap",
    "TimeRange.Duration() returns time.Duration": "returns the length of the time range",
    "SeriesKey.String() returns string": "returns a canonical string representation",
    "NewSeriesKey(measurement string, tags map[string]string) returns SeriesKey": "creates a SeriesKey",
    "NewSeries(key SeriesKey) returns *Series": "creates a new Series",
    "Series.InsertPoint(p DataPoint)": "inserts a DataPoint into the Series while maintaining sorted order",
    "AggregateFunction.String() returns string": "returns a string representation of the aggregate function",
    "WriteHeader(w io.Writer) returns error": "Writes TSM header to writer",
    "ReadHeader(r io.Reader) returns (TSMHeader, error)": "Reads and validates TSM header",
    "NewTSMWriter(path string, blockSize int) returns (*TSMWriter, error)": "Creates new TSM writer",
    "WriteSeries(key string, points []DataPoint) returns error": "Writes series to TSM file",
    "Finish() returns error": "Writes index and footer, closes file",
    "compressBlock(timestamps []uint64, values []float64) returns ([]byte, []byte, error)": "Compresses timestamps and values",
    "OpenTSMReader(path string) returns (*TSMReader, error)": "Opens and memory-maps TSM file",
    "ReadBlock(seriesKey string, blockIndex int) returns ([]DataPoint, error)": "Reads and decompresses specific block",
    "TimeRange() returns (uint64, uint64)": "Returns min/max timestamps in file",
    "decompressTimestamps(data []byte, count int) returns ([]uint64, error)": "Decodes delta-of-delta compressed timestamps",
    "decompressValues(data []byte, count int) returns ([]float64, error)": "Decodes Gorilla XOR compressed values",
    "Insert(seriesKey SeriesKey, point DataPoint) returns error": "adds a point to memtable maintaining sorted order",
    "GetRange(seriesKey SeriesKey, tr TimeRange) returns []DataPoint": "returns points for series within time range",
    "Size() returns int64": "returns approximate memory usage",
    "Flush() returns map[string][]DataPoint": "returns all data and clears memtable",
    "WritePoint(ctx context.Context, seriesKey SeriesKey, point DataPoint) returns error": "writes a single data point",
    "WritePointsBatch(ctx context.Context, points []DataPoint) returns error": "writes multiple data points efficiently",
    "flushMemtable(mt *Memtable) returns error": "flushes memtable to TSM files",
    "ParseQuery(input string) returns": "Parses query string into models.Query",
    "Planner.Plan(query *models.Query) returns": "Creates QueryPlan from query and storage state",
    "SeriesScanner.Next() returns bool": "Advances to next point for the series",
    "SeriesScanner.At() returns models.DataPoint": "returns current point",
    "SeriesScanner.Err() returns error": "returns any error",
    "SeriesScanner.Close() returns error": "releases resources",
    "WindowAggregator.Next() returns bool": "Advances to next aggregated bucket",
    "WindowAggregator.At() returns models.DataPoint": "returns current aggregated point",
    "alignToWindow(t time.Time, window time.Duration) returns time.Time": "aligns timestamp to window start using fixed epoch",
    "NewScheduler() returns *Scheduler": "creates a new background job scheduler",
    "AddJob(job *Job)": "registers a job to be run periodically",
    "Start()": "begins periodic disk monitoring",
    "Stop()": "gracefully stops TTL enforcement",
    "NewTTLEnforcer(storage *storage.Engine, defaultTTL time.Duration) returns *TTLEnforcer": "creates a new TTL enforcer",
    "SetPolicy(measurement string, ttl time.Duration)": "sets a TTL policy for a measurement",
    "Run(ctx context.Context) returns error": "executes one TTL enforcement cycle",
    "Plan() returns ([]CompactionPlan, error)": "generates compaction plans for overdue levels",
    "ShouldDownsample(currentLevel, nextLevel int) returns bool": "checks if compaction to next level requires downsampling",
    "Execute(ctx context.Context, plan CompactionPlan) returns error": "runs a single compaction plan",
    "mergeSeriesPoints(seriesKey string, inputFiles []string) returns ([]storage.DataPoint, error)": "combines points for a series across multiple files",
    "applyDownsampling(points []storage.DataPoint, window time.Duration, aggFunc storage.AggregateFunction) returns ([]storage.DataPoint, error)": "reduces point granularity through aggregation",
    "NewParser(input string) returns *Parser": "creates a new query parser",
    "ParseQuery() returns (*models.Query, error)": "parses query string into models.Query",
    "NewServer(storage StorageEngine, queryEngine QueryEngine, logger *zap.Logger) returns *Server": "creates new HTTP server",
    "Start(addr string) returns error": "starts HTTP server",
    "Shutdown(ctx context.Context) returns error": "gracefully shuts down server",
    "handleWrite(w http.ResponseWriter, r *http.Request)": "Handles write API endpoint",
    "handleQuery(w http.ResponseWriter, r *http.Request)": "Handles query API endpoint",
    "handlePrometheusWrite(w http.ResponseWriter, r *http.Request)": "handles Prometheus remote write",
    "handlePrometheusRead(w http.ResponseWriter, r *http.Request)": "handles Prometheus remote read",
    "recoveryMiddleware(next http.Handler) returns http.Handler": "HTTP middleware for panic recovery",
    "loggingMiddleware(next http.Handler) returns http.Handler": "HTTP middleware for request logging",
    "parseLineProtocolPoint(lpPoint *Point) returns (models.SeriesKey, models.DataPoint, error)": "converts line protocol point to internal models",
    "NewStreamWriter(w http.ResponseWriter) returns *StreamWriter": "Creates a writer for streaming JSON results",
    "StreamWriter.WritePoint(p models.DataPoint) returns error": "Streams a single data point as JSON",
    "StreamWriter.Close() returns error": "Finalizes the JSON array stream",
    "IsDiskFullError(err error) returns bool": "checks if error is due to disk full",
    "GetRecoveryActions(err error) returns []RecoveryAction": "returns recommended recovery actions for an error",
    "RecoverFromCrash(ctx context.Context) returns (int64, error)": "recovers unflushed data from WAL after a crash",
    "RepairCorruptSegment(segmentPath string) returns ([]byte, error)": "attempts to salvage data from corrupt WAL segment",
    "ExecuteQueryWithTimeout(ctx context.Context, plan *QueryPlan, timeout time.Duration) returns (QueryResult, error)": "executes query with timeout",
    "executePlan(ctx context.Context, plan *QueryPlan) returns (QueryResult, error)": "executes query plan with resource tracking",
    "monitorLoop()": "checks disk space periodically",
    "checkDiskSpace()": "checks disk usage and triggers emergency actions",
    "emergencyCompaction() returns error": "triggers aggressive compaction to free space",
    "LoadGoldenFile(t *testing.T, version, name string) returns GoldenFile": "Loads golden file from testdata directory",
    "CompareToGolden(t *testing.T, golden GoldenFile, current []byte) returns bool": "Compares current data to golden file checksum",
    "UpdateGoldenFile(t *testing.T, version, name string, data []byte)": "Updates golden file with current data when UPDATE_GOLDEN env var is set",
    "NewTimeSeriesGenerator(seed int64) returns *TimeSeriesGenerator": "Creates time series generator with seeded RNG",
    "GenerateSequentialTimestamps(n int, start time.Time, interval time.Duration, jitter float64) returns []time.Time": "Generates sequential timestamps with optional jitter",
    "GenerateRandomWalkValues(n int, start, volatility float64) returns []float64": "Generates values following random walk",
    "GenerateMixedValues(n int) returns []float64": "Generates values including NaN, Inf, and other edge cases",
    "CheckProperty(t *testing.T, name string, f interface{}, config *quick.Config) returns bool": "Runs property test with helpful error reporting",
    "NewOperationRecorder(filePath string) returns (*OperationRecorder, error)": "creates a new recorder writing to the given file",
    "Record(opType, component string, args interface{}) returns func(result interface{}, err error)": "starts timing an operation and returns a function to complete the recording",
    "ReplayOperations(filePath string, handler func(op RecordedOperation) error) returns error": "reads recorded operations and executes them through a handler",
    "RouteQuery(ctx context.Context, query *models.Query) returns (*RoutedQuery, error)": "analyzes a query and routes it to appropriate nodes",
    "ExecuteRoutedQuery(ctx context.Context, routed *RoutedQuery) returns (*models.QueryResult, error)": "executes a routed query across multiple nodes",
    "Merge(results []*models.QueryResult) returns (*models.QueryResult, error)": "merges multiple query results into one"
  },
  "constants": {
    "AggregateSum": "'sum'",
    "AggregateAvg": "'avg'",
    "AggregateMin": "'min'",
    "AggregateMax": "'max'",
    "AggregateCount": "'count'",
    "MagicNumber": "0x16D1D1A5",
    "Version": "1",
    "BlockHeaderSize": "16",
    "IndexEntrySize": "28",
    "DefaultMaxPointsPerBlock": "1024",
    "DefaultCompactionInterval": "1 * time.Hour",
    "DefaultTTLGracePeriod": "5 * time.Minute",
    "Level0MaxFiles": "4",
    "Level1TargetSize": "100 * 1024 * 1024",
    "Level2TargetSize": "500 * 1024 * 1024",
    "SeverityDebug": "debug severity level",
    "SeverityInfo": "info severity level",
    "SeverityWarning": "warning severity level",
    "SeverityError": "error severity level",
    "SeverityCritical": "critical severity level",
    "UPDATE_GOLDEN": "Environment variable to enable golden file updates"
  },
  "terms": {
    "Time-series data": "sequential measurements indexed by time",
    "Cardinality": "number of unique time series",
    "Columnar storage": "data layout storing values of the same column together",
    "Delta-of-delta encoding": "Compression storing difference between consecutive differences",
    "Gorilla compression": "XOR-based compression for floating-point values",
    "Time-Structured Merge Tree": "storage engine optimized for time-series data",
    "Write amplification": "extra writes caused by storage engine overhead",
    "Temporal locality": "data accessed together is stored together in time",
    "Memtable": "in-memory buffer for writes before flushing to disk",
    "Write-ahead log": "durability log written before acknowledging writes",
    "Predicate pushdown": "Applying filters as early as possible in query execution",
    "Compaction": "background process merging and optimizing storage files",
    "Measurement": "container for related data, akin to a table name",
    "Tags": "indexed key-value metadata used to identify and filter series",
    "Field": "the actual measured value (float64)",
    "Series Key": "unique identifier formed by a Measurement and a complete set of Tags",
    "TSM": "Time-Structured Merge tree storage format",
    "Memory-mapped files": "File access via virtual memory for zero-copy reads",
    "Gorilla XOR compression": "XOR-based compression for floating-point values",
    "Columnar layout": "Data layout storing values of same column together",
    "Block-based storage": "Organizing data into fixed-size blocks for efficient access",
    "CRC32 checksum": "Cyclic redundancy check for data integrity",
    "Out-of-order writes": "data points arriving with timestamps not in chronological order",
    "Tolerance window": "maximum allowed time difference for out-of-order points",
    "Backpressure": "mechanism to throttle writes when system is overloaded",
    "Segment rotation": "closing full WAL segment and starting new one",
    "Group commit": "batching multiple write acknowledgments with one fsync",
    "Iterator model": "Execution model where each operator implements a Next() method to pull data",
    "Tumbling windows": "Contiguous, non-overlapping time intervals for grouping",
    "Downsampling": "process of reducing data resolution through aggregation for older data",
    "Block pruning": "skipping storage blocks that cannot contain data matching query predicates",
    "Retention Policy": "defines how long data should be kept before automatic deletion",
    "TTL (Time-To-Live)": "duration after which data is considered expired",
    "Compaction Level": "hierarchical tier in compaction strategy determining file size and characteristics",
    "Rollup Series": "pre-computed aggregated time series at lower granularity",
    "Tombstoned": "state where a file is marked for deletion but not yet removed",
    "Grace Period": "time delay before physically deleting tombstoned files",
    "Write Amplification": "extra writes caused by compaction rewriting data multiple times",
    "Level-Based Compaction": "strategy organizing files into tiers with progressively larger sizes",
    "line protocol": "text-based format for writing time-series data points",
    "Prometheus remote read/write": "protocol for Prometheus to use external storage",
    "Grafana data source API": "endpoints Grafana expects from a time-series database",
    "tumbling windows": "contiguous, non-overlapping time intervals for grouping",
    "Streaming serialization": "Serializing and sending results incrementally as they are produced",
    "Chunked encoding": "HTTP transfer encoding allowing a response to be sent in pieces",
    "tolerance window": "maximum allowed time difference for out-of-order points",
    "grace period": "time delay before physically deleting tombstoned files",
    "clock skew": "time difference between distributed system clocks",
    "quarantine": "isolating corrupt files from normal operations",
    "backpressure": "mechanism to throttle writes when system is overloaded",
    "Golden File Testing": "Testing against versioned reference files for format stability",
    "Property-Based Testing": "Testing properties or invariants hold for all possible inputs",
    "Fuzz Testing": "Testing with randomly generated inputs to find edge cases",
    "Compression Ratio": "Ratio of uncompressed size to compressed size",
    "Round-Trip Correctness": "Property that data is identical after compression and decompression",
    "Time Window Alignment": "Process of aligning timestamps to fixed window boundaries",
    "Predicate Pushdown": "Optimization applying filters early in query execution to reduce scanned data",
    "digital archaeology": "debugging approach that pieces together clues from logs, file artifacts, and runtime behavior",
    "breadcrumb trails": "logging approach where each entry traces a point's journey through the system",
    "round-trip invariant": "property that compress â†’ decompress yields identical data",
    "golden files": "versioned reference files for format stability testing",
    "property-based testing": "testing properties or invariants hold for all possible inputs",
    "time-travel debugging": "recording operations and replaying them in a controlled environment",
    "stress testing with anomaly injection": "deliberately introducing failures to verify recovery mechanisms",
    "structured logging": "logging with consistent fields and machine-readable format",
    "heap profiling": "analyzing memory allocation patterns",
    "goroutine leak": "unbounded growth of goroutines due to resources not being released",
    "LRU cache": "Least Recently Used cache eviction policy",
    "Kahan summation": "algorithm for summing floating-point numbers with reduced precision loss",
    "consistent hashing": "hashing technique that minimizes reorganization when nodes are added/removed",
    "shard": "logical partition of data distributed across nodes",
    "tiered storage": "storage architecture with multiple performance/cost tiers",
    "materialized view": "pre-computed query result stored for fast access",
    "stream processing": "processing data in real-time as it arrives",
    "adaptive compression": "compression that selects algorithm based on data patterns",
    "incremental view maintenance": "updating materialized views as new data arrives"
  }
}