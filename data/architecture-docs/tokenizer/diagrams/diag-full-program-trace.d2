direction: right
vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 4
  }
}

# L1: Tokenizer Integration Trace
# Validates Milestone 4 requirements: End-to-end correctness of 'if (x >= 42) { return true; }'

input_layer: {
  label: "INPUT LAYER"
  direction: down
  
  source: {
    label: "SOURCE INPUT (UTF-8)"
    content: |md
    c
    if (x >= 42) { return true; }
    
    |
  }
  
  metadata: {
    shape: sql_table
    label: "Input Buffer (buffer.c)"
    row1: "Size | 29 bytes"
    row2: "Encoding | UTF-8"
    row3: "Lines | 1"
  # label_bottom: "Reference: Milestone 4.1"  # removed: label_bottom is not valid D2
  }
}

processing_layer: {
  label: "SCANNER LAYER"
  direction: down
  
  token_struct: {
    shape: sql_table
    label: "struct Token (scanner.py)"
    row1: "0x00 | TokenType | type"
    row2: "0x08 | str       | lexeme"
    row3: "0x10 | int       | line"
    row4: "0x18 | int       | column"
  # label_bottom: "Total: 32 bytes (64-bit PyObject pointers)"  # removed: label_bottom is not valid D2
  }
  
  scanner_impl: {
    label: "Scanner Logic (scanner.py)"
    methods: |'md
    python
    def scan_tokens(self) -> list[Token]:
        while not self.is_at_end():
            self.start = self.current
            self._scan_token()
        return self.tokens

    def _scan_token(self) -> Token | None:
        c = self.advance()
        if c == '>':
            if self._match('='):
                self.add_token(GREATER_EQUAL)
    
    '|
  }
}

output_layer: {
  label: "OUTPUT LAYER"
  direction: down
  
  tokens_stream: {
    label: "Token Stream (list[Token])"
    grid-columns: 4
    grid-gap: 20
    
    t1: {
      shape: sql_table
      label: "Token[0]"
      type: "KEYWORD"
      lexeme: "'if'"
      pos: "1:1"
    }
    
    t2: {
      shape: sql_table
      label: "Token[1]"
      type: "PUNCTUATION"
      lexeme: "'('"
      pos: "1:4"
    }
    
    t3: {
      shape: sql_table
      label: "Token[2]"
      type: "IDENTIFIER"
      lexeme: "'x'"
      pos: "1:5"
    }
    
    t4: {
      shape: sql_table
      label: "Token[3]"
      type: "GREATER_EQUAL"
      lexeme: "'>='"
      pos: "1:7"
    }
    
    t5: {
      shape: sql_table
      label: "Token[4]"
      type: "NUMBER"
      lexeme: "'42'"
      pos: "1:10"
    }
    
    t6: {
      shape: sql_table
      label: "Token[5]"
      type: "PUNCTUATION"
      lexeme: "')'"
      pos: "1:12"
    }
    
    t7: {
      shape: sql_table
      label: "Token[6]"
      type: "PUNCTUATION"
      lexeme: "'{'"
      pos: "1:14"
    }
    
    t8: {
      shape: sql_table
      label: "Token[7]"
      type: "KEYWORD"
      lexeme: "'return'"
      pos: "1:16"
    }
    
    t9: {
      shape: sql_table
      label: "Token[8]"
      type: "KEYWORD"
      lexeme: "'true'"
      pos: "1:23"
    }
    
    t10: {
      shape: sql_table
      label: "Token[9]"
      type: "PUNCTUATION"
      lexeme: "';'"
      pos: "1:27"
    }
    
    t11: {
      shape: sql_table
      label: "Token[10]"
      type: "PUNCTUATION"
      lexeme: "'}'"
      pos: "1:29"
    }
    
    t12: {
      shape: sql_table
      label: "Token[11]"
      type: "EOF"
      lexeme: "''"
      pos: "1:30"
    }
  }
}

# Data Walk / Flow
input_layer.source -> processing_layer.scanner_impl: "char_stream | 29 bytes | 'if (x >= 42)...'"
processing_layer.scanner_impl -> output_layer.tokens_stream: "Token[] | ~384 bytes | [T0..T11]"

# Logic Trace Annotations
processing_layer.token_struct.row3 -> output_layer.tokens_stream.t4.pos: "Line tracking validation" {
  style.stroke-dash: 5
}
processing_layer.scanner_impl -> output_layer.tokens_stream.t4: "Lookahead match(1) logic ('>=')" {
  style.stroke: blue
}
processing_layer.scanner_impl -> output_layer.tokens_stream.t8: "Keyword trie lookup" {
  style.stroke: purple
}