direction: right
vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 4
  }
}

# L0 SATELLITE CONTEXT: Tokenizer / Lexical Analysis
# Input String: ">==" (0x3E 0x3D 0x3D)

input_layer: {
  direction: down
  label: "MEMORY: INPUT BUFFER"

  input_buffer: {
    shape: sql_table
    label: "char* source (scanner.c)"
    
    idx0: "0x00 | 0x3E | '>'"
    idx1: "0x01 | 0x3D | '='"
    idx2: "0x02 | 0x3D | '='"
    idx3: "0x03 | 0x00 | '\\0'"
    label_bottom: "Total: 4 bytes"
  }
  
  scanner_ptr: {
    shape: class
    label: "struct Scanner (scanner.h)"
    fields: |md
      c
      const char* start;   // current token start
      const char* current; // lookahead pointer
      int line;            // tracking
      
    |
  }
}

processing_layer: {
  direction: down
  label: "LOGIC: MAXIMAL MUNCH (Lexer.c)"

  step_1: {
    direction: right
    label: "PHASE 1: Greedy Match '>='"
    
    state_v1: {
      shape: sql_table
      label: "Iter 1: Match"
      ptr: "current: 0 -> 2"
      token: "match: GREATER_EQUAL"
    }
    
    code_1: |md
      python
      # Maximal Munch: Look ahead for '='
      char = advance() # consumes '>'
      if char == '>' and match('='):
          # consumes '=' at idx 1
          return make_token(TOK_GREATER_EQUAL)
      
    |
    state_v1 -> code_1: "peek(1) == '='"
  }

  step_2: {
    direction: right
    label: "PHASE 2: Next Match '='"
    
    state_v2: {
      shape: sql_table
      label: "Iter 2: Match"
      ptr: "current: 2 -> 3"
      token: "match: EQUAL"
    }
    
    code_2: |md
      python
      # Start at idx 2
      char = advance() # consumes '='
      if char == '=' and match('='):
          # peek(1) is '\0', match fails
          return make_token(TOK_EQUAL_EQUAL)
      else:
          # Fallback to single '='
          return make_token(TOK_EQUAL)
      
    |
    state_v2 -> code_2: "peek(1) == '\\0'"
  }

  step_1 -> step_2: "PC Step | offset += 2"
}

output_layer: {
  direction: down
  label: "EMITTED: TOKEN STREAM"

  token_0: {
    shape: sql_table
    label: "Token[0] (0x10 bytes)"
    type: "type   | TOK_GREATER_EQUAL"
    lexeme: "lexeme | \">=\""
    line: "line   | 1"
  }

  token_1: {
    shape: sql_table
    label: "Token[1] (0x10 bytes)"
    type: "type   | TOK_EQUAL"
    lexeme: "lexeme | \"=\""
    line: "line   | 1"
  }

  token_eof: {
    shape: sql_table
    label: "Token[2] (0x10 bytes)"
    type: "type   | TOK_EOF"
    lexeme: "lexeme | \"\""
    line: "line   | 1"
  }

  token_0 -> token_1 -> token_eof
}

# Implementation Data Flows
input_layer.input_buffer.idx0 -> processing_layer.step_1.code_1: "0x3E | 1 byte"
input_layer.input_buffer.idx1 -> processing_layer.step_1.code_1: "0x3D | 1 byte"
input_layer.input_buffer.idx2 -> processing_layer.step_2.code_2: "0x3D | 1 byte"

processing_layer.step_1 -> output_layer.token_0: "Emit | struct Token"
processing_layer.step_2 -> output_layer.token_1: "Emit | struct Token"

annotation: {
  shape: text
  label: "MAXIMAL MUNCH PRINCIPLE:\nThe lexer always groups the longest possible sequence of characters that form a valid token ('>=' before '=') to resolve ambiguity."
  near: bottom-center
  style: {
    font-size: 14
    bold: true
    fill: "#f8f9fa"
    stroke: "#dee2e6"
  }
}

# Flow connection between major layers
input_layer -> processing_layer -> output_layer: "char* -> Scanner -> Token[]"