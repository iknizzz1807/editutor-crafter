direction: right
vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 4
  }
}

# ── GLOBAL CLASSES ──
classes: {
  logic_block: {
    style: {
      fill: "white"
      stroke-width: 2
    }
  }
  data_node: {
    shape: rectangle
    style: {
      fill: "#E1F5FE"
      stroke: "#01579B"
    }
  }
  hot_path: {
    style: {
      stroke: "#D32F2F"
      stroke-width: 3
    }
  }
}

# ── SYSTEM LAYERS ──

input_layer: {
  label: "SOURCE INPUT (UTF-8)"
  direction: down

  source_a: {
    shape: code
    label: "Input A: 'iffy '"
  }
  
  source_b: {
    shape: code
    label: "Input B: 'if '"
  }
}

scanner_logic: {
  label: "SCANNER ENGINE (scanner.py)"
  direction: down
  
  scanner_class: {
    shape: class
    label: "class Scanner"
    fields: |md
      python
      source: str
      start: int    # Lexeme start index
      current: int  # Cursor index
      
    |
    methods: |md
      python
      def _scan_identifier(self) -> Token:
          while peek().isalnum() or peek() == '_':
              advance()
          text = source[start:current]
          type = KEYWORDS.get(text, IDENTIFIER)
          return make_token(type)
      
    |
  }

  maximal_munch: {
    label: "MAXIMAL MUNCH FSM"
    direction: right
    
    state_start: "START"
    state_ident: "IDENTIFIER\n(is_alpha or '_')"
    state_done: "COMPLETE"
    
    state_start -> state_ident: "char.isalpha()"
    state_ident -> state_ident: "char.isalnum() || '_'"
    state_ident -> state_done: "!isalnum() && !'_'"
  }
}

lookup_layer: {
  label: "KEYWORD LOOKUP TABLE"
  
  keyword_map: {
    shape: sql_table
    label: "dict[str, TokenType] (Static Map)"
    
    row1: "'if'     | KEYWORD"
    row2: "'else'   | KEYWORD"
    row3: "'while'  | KEYWORD"
    row4: "'return' | KEYWORD"
    row5: "'true'   | KEYWORD"
    row6: "'false'  | KEYWORD"
    row7: "'null'   | KEYWORD"
    label_bottom: "O(1) Hash Lookup"
  }
}

output_tokens: {
  label: "TOKEN STREAM"
  direction: down
  
  token_a: {
    shape: rectangle
    label: "Token(IDENTIFIER, 'iffy', 1:1)"
  }
  
  token_b: {
    shape: rectangle
    label: "Token(KEYWORD, 'if', 2:1)"
  }
}

# ── DATA FLOW TRACES (IMPLEMENTATION PATH) ──

# Trace A: 'iffy'
input_layer.source_a -> scanner_logic.scanner_class: "Stream | len=5 | 'i','f','f','y',' '"
scanner_logic.scanner_class -> scanner_logic.maximal_munch: "Loop | current: 0 -> 4"
scanner_logic.maximal_munch -> lookup_layer.keyword_map: "Lexeme | 'iffy' | Lookup"
lookup_layer.keyword_map -> output_tokens.token_a: "Result | NOT_FOUND -> IDENTIFIER"

# Trace B: 'if'
input_layer.source_b -> scanner_logic.scanner_class: "Stream | len=3 | 'i','f',' '" {
  style.stroke: "#2E7D32"
}
scanner_logic.scanner_class -> scanner_logic.maximal_munch: "Loop | current: 0 -> 2" {
  style.stroke: "#2E7D32"
}
scanner_logic.maximal_munch -> lookup_layer.keyword_map: "Lexeme | 'if' | Lookup" {
  style.stroke: "#2E7D32"
}
lookup_layer.keyword_map -> output_tokens.token_b: "Result | FOUND -> KEYWORD" {
  style.stroke: "#2E7D32"
}

# ── ANNOTATIONS ──
legend: {
  near: bottom-right
  
  note: |md
    **Rule: Scan First, Lookup Second**
    1. `_scan_identifier` greedily consumes all valid characters.
    2. Boundary is reached when `peek()` is not alphanumeric.
    3. Final string is checked against hash table.
    
    *Prevents 'iffy' being broken into KEYWORD('if') + IDENTIFIER('fy').*
  |
}