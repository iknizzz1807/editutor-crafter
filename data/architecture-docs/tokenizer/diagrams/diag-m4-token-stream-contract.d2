direction: right
vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 4
  }
}

# ── GLOBAL CLASSES ──
classes: {
  logic_gate: {
    style: {
      fill: "#E4DBFE"
      stroke: "#333333"
      border-radius: 5
    }
  }
  contract: {
    style: {
      stroke: "#6772E5"
      stroke-width: 4
      stroke-dash: 5
    }
  }
}

# ── COMPONENT: SCANNER ──
scanner_module: {
  label: "CHARACTER WORLD (scanner.py)"
  direction: down
  
  scanner_class: {
    shape: class
    label: "Scanner"
    
    fields: |`md
      python
      # State tracking (64-bit offsets)
      0x00 | str      | source   # Raw input buffer
      0x08 | int      | current  # Cursor byte offset
      0x10 | int      | line     # 1-indexed row
      0x18 | int      | column   # 1-indexed column
      
    `|
    
    methods: |`md
      python
      def next_token(self) -> Token: ...
      def _advance(self) -> str: ...
      def _peek(self, offset: int = 0) -> str: ...
      def _is_at_end(self) -> bool: ...
      
    `|
  }

  internal_fsm: {
    label: "Lexical FSM"
    class: logic_gate
    
    direction: down
    m1: "Position Tracking"
    m2: "Maximal Munch (Greedy)"
    m3: "Lookahead (k=1)"
    
    m1 -> m2 -> m3
  }

  scanner_class -> internal_fsm: "triggers transition"
}

# ── THE INTERFACE CONTRACT ──
token_contract: {
  shape: sql_table
  label: "struct Token (tokens.py)"
  class: contract
  
  row1: "0x00 | TokenType | type    | Category ID"
  row2: "0x08 | str       | lexeme  | Slice of source"
  row3: "0x10 | int       | line    | Diagnostic Y"
  row4: "0x18 | int       | column  | Diagnostic X"
  
  label_bottom: "Total: 32 bytes | Immutable Data Transfer Object"
}

# ── COMPONENT: PARSER ──
parser_module: {
  label: "TOKEN WORLD (parser.py)"
  direction: down
  
  parser_class: {
    shape: class
    label: "Parser"
    
    logic: |`md
      python
      def parse(self) -> ASTNode:
          tokens = []
          while not self._is_at_end():
              tokens.append(self._consume())
          return self._build_ast(tokens)
      
    `|
    
    methods: |`md
      python
      def _consume(self, type: TokenType) -> Token: ...
      def _match(self, *types: TokenType) -> bool: ...
      def _peek(self) -> Token: ...
      
    `|
  }
  
  parser_logic: {
    label: "Recursive Descent Engine"
    class: logic_gate
    
    p1: "Match Production"
    p2: "AST Node Allocation"
    p3: "Panic Mode Recovery"
    
    p1 -> p2 -> p3
  }
  
  parser_class -> parser_logic: "invokes"
}

# ── DATA FLOWS ──
scanner_module.scanner_class -> token_contract: "Yields Token | 32 bytes | {type: IDENTIFIER}" {
  style: {
    stroke: "#6772E5"
    stroke-width: 2
    animated: true
  }
}

token_contract -> parser_module.parser_class: "Read Fields | O(1) Access | token.lexeme" {
  style: {
    stroke: "#6772E5"
    stroke-width: 2
  }
}

# ── ANNOTATIONS ──
explanation: |`md
  ### The Lexical Boundary Contract
  - **Abstraction**: The Parser is decoupled from character-level concerns (CRLF vs LF, escape sequences).
  - **Single Source of Truth**: The Token's `line` and `column` are the only coordinates for error reporting.
  - **The EOF Promise**: Scanner guarantees a sentinel `TokenType.EOF` to prevent Parser bounds-checking errors.
  - **Memory Layout**: Token uses pointer-width offsets (8-byte) for alignment in 64-bit Python runtimes.
`| {
  near: bottom-center
  style: {
    font-size: 14
    stroke: "#6772E5"
    fill: "#F0F2FF"
  }
}