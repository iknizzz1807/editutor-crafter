direction: right
vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 3
  }
}

# SATELLITE MAP: TOKENIZER SYSTEM ARCHITECTURE (L0)
# Comprehensive Blueprint for Milestones M1-M4

input_layer: {
  direction: down
  label: "PHASE 0: RAW INPUT BUFFER"
  
  source_buffer: {
    shape: sql_table
    label: "struct Source (input.h)"
    row1: "0x00 | char*    | data"
    row2: "0x08 | uint64_t | length"
    row3: "0x10 | uint64_t | capacity"
    label_bottom: "Total: 24 bytes"
  }

  input_stream: |md
    c
    // Raw Input Stream Example
    if (x >= 42) {
        return "val";
    }
    
  |
}

scanner_core: {
  direction: down
  label: "PHASE 1: SCANNER ENGINE (M1)"
  tooltip: "Foundation lexical scanning engine"

  m1_anchor: "Milestone 1: Foundation" {
    style: {
      stroke: blue
      stroke-width: 4
      fill: "#E4DBFE"
    }
  }

  scanner_class: {
    shape: class
    label: "class Scanner (scanner.py)"
    fields: |md
      python
      source: str        # Raw input buffer
      tokens: list[Token]# Output token stream
      start: int         # Offset to start of lexeme
      current: int       # Offset to lookahead ptr
      line: int          # 1-based current line
      column: int        # 1-based current col
      start_column: int  # Snapshot for token start
      
    |
    methods: |md
      python
      advance() -> str   # Consume char + update offsets
      peek() -> str      # Lookahead LA(1)
      peek_next() -> str # Lookahead LA(2)
      is_at_end() -> bool# Boundary check
      
    |
  }

  pos_tracking: {
    label: "Position Manager"
    style.stroke-dash: 3
    style.fill: "#E4DBFE"
    "\\n": "\n"
    "\\n" -> line_inc: "line += 1; col = 1"
    "char" -> col_inc: "col += 1"
  }
}

lexical_logic: {
  direction: down
  label: "PHASE 2: STATE MACHINE (M2/M3)"
  
  maximal_munch: {
    label: "M2: Greedy Matching"
    tooltip: "Consume longest valid lexeme (e.g., '>=' over '>')"
    
    m2_anchor: "Milestone 2: Operators" {
      style.stroke: blue
      style.stroke-width: 4
    }

    match_logic: |md
      python
      def match(expected: str) -> bool:
          if is_at_end(): return False
          if peek() != expected: return False
          current += 1
          return True
      
    |
  }

  modes: {
    label: "M3: Context Modes"
    direction: right

    m3_anchor: "Milestone 3: Modes" {
      style.stroke: blue
      style.stroke-width: 4
    }

    normal_mode: "NORMAL\n(Dispatch)" {style.fill: "#C7F1FF"}
    string_mode: "IN_STRING\n(Escapes)" {style.fill: "#FFE7CB"}
    comment_mode: "IN_COMMENT\n(Skip)" {style.fill: "#DEE1EB"}
    
    normal_mode -> string_mode: "\""
    string_mode -> normal_mode: "\""
    normal_mode -> comment_mode: "// or /*"
    comment_mode -> normal_mode: "\\n or */"
  }

  lookup_tables: {
    shape: sql_table
    label: "Keyword Registry"
    row1: "'if'     | TokenType.IF"
    row2: "'while'  | TokenType.WHILE"
    row3: "'return' | TokenType.RETURN"
    row4: "'true'   | TokenType.TRUE"
    label_bottom: "O(1) Static HashMap"
  }
}

emission_layer: {
  direction: down
  label: "PHASE 3: EMISSION & RECOVERY (M4)"

  token_struct: {
    shape: sql_table
    label: "struct Token (token.py)"
    row1: "0x00 | TokenType | type"
    row2: "0x04 | str       | lexeme"
    row3: "0x0C | int       | line"
    row4: "0x10 | int       | column"
    label_bottom: "Total: 32 bytes (Aligned)"
  }

  error_handler: {
    label: "M4: Error Recovery"
    style.fill: "#FE7070"
    
    m4_anchor: "Milestone 4: Recovery" {
      style.stroke: blue
      style.stroke-width: 4
    }

    "Panic Mode" -> "Emit ERROR Token": "M4 Trigger"
    "Emit ERROR Token" -> "Skip-One Char"
    "Skip-One Char" -> "Resume Scan"
  }
}

# --- GLOBAL DATA FLOW ---

input_layer.source_buffer -> scanner_core.scanner_class: "UTF-8 | 1 byte/step | 'i'"

scanner_core.scanner_class -> lexical_logic: "peek() | LA(1) | '>'"
lexical_logic.maximal_munch -> scanner_core.scanner_class: "match('=') | current++"

lexical_logic.modes -> emission_layer.token_struct: "Lexeme | STRING | '\"val\"'"
lexical_logic.lookup_tables -> emission_layer.token_struct: "Static Map | KEYWORD | 'if'"

emission_layer.token_struct -> final_output: "list[Token] | N bytes"

final_output: {
  shape: cylinder
  label: "Token Stream\n(Parser Input)"
  style.fill: "#ACE1AF"
}