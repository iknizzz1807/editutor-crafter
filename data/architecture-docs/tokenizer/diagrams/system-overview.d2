direction: right
vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 3
  }
}
title: |'md
  # Tokenizer / Lexer — System Blueprint
  **Python Character-Level FSM Lexer for C-like Language**
  Milestones: [M1 Foundation](#tokenizer-m1) | [M2 Multi-Char](#tokenizer-m2) | [M3 Strings+Comments](#tokenizer-m3) | [M4 Integration](#tokenizer-m4)
'| {near: top-center}
layer0_types: "LAYER 0 — Data Structures\n(token_type.py · token_class.py)" {
  style: {
    fill: "#EEF2FF"
    stroke: "#6366F1"
    stroke-width: 2
    border-radius: 8
  }
  token_type_enum: TokenType Enum {
    style: {
      fill: "#C7D2FE"
      stroke: "#4338CA"
      border-radius: 6
      font-size: 13
    }
    label: |py
      # token_type.py  [M1: tokenizer-m1]
      from enum import Enum, auto
      class TokenType(Enum):
        # Literals
        NUMBER      = auto()  # 42, 3.14
        STRING      = auto()  # "hello"
        # Names
        IDENTIFIER  = auto()  # x, myVar
        KEYWORD     = auto()  # if, while
        # Operators (M1: single-char)
        OPERATOR    = auto()  # +, -, *, /
        # Operators (M2: two-char)
        ASSIGN        = auto()  # =
        EQUALS        = auto()  # ==
        NOT_EQUAL     = auto()  # !=
        LESS_THAN     = auto()  # <
        LESS_EQUAL    = auto()  # <=
        GREATER_THAN  = auto()  # >
        GREATER_EQUAL = auto()  # >=
        # Structural
        PUNCTUATION = auto()  # ( ) { } [ ] ; ,
        # Control
        EOF   = auto()  # end sentinel
        ERROR = auto()  # skip-one recovery
    |
  }
  token_dataclass: Token Dataclass {
    style: {
      fill: "#C7D2FE"
      stroke: "#4338CA"
      border-radius: 6
      font-size: 13
    }
    label: |py
      # token_class.py  [M1: tokenizer-m1]
      @dataclass(frozen=True)
      class Token:
        # Field          Type    Offset  Size
        type:   TokenType  # +0     8B  (enum ref)
        lexeme: str        # +8     8B  (str ref → raw chars)
        line:   int        # +16    8B  (1-based)
        column: int        # +24    8B  (1-based, start of lexeme)
        # Total object: ~56B (CPython overhead + 4 fields)
        def __repr__(self) -> str:
          return f"Token({self.type.name}, \
                  {self.lexeme!r}, \
                  {self.line}:{self.column})"
      # Example:
      # Token(GREATER_EQUAL, ">=", 3, 7)
    |
  }
  keyword_table: "_KEYWORDS Lookup Table\nO(1) avg — dict[str, TokenType]" {
    style: {
      fill: "#E0E7FF"
      stroke: "#4338CA"
      border-radius: 6
      font-size: 13
    }
    label: |py
      # scanner.py  [M2: tokenizer-m2]
      _KEYWORDS: dict[str, TokenType] = {
        "if":     TokenType.KEYWORD,
        "else":   TokenType.KEYWORD,
        "while":  TokenType.KEYWORD,
        "return": TokenType.KEYWORD,
        "true":   TokenType.KEYWORD,
        "false":  TokenType.KEYWORD,
        "null":   TokenType.KEYWORD,
      }
      # Size: 7 entries, ~500B dict overhead
    |
  }
  dispatch_table: "_SINGLE_CHAR_TOKENS\ndict[str, TokenType] — O(1)" {
    style: {
      fill: "#E0E7FF"
      stroke: "#4338CA"
      border-radius: 6
      font-size: 13
    }
    label: |py
      # scanner.py  [M1: tokenizer-m1]
      _SINGLE_CHAR_TOKENS = {
        '+': OPERATOR,  '-': OPERATOR,
        '*': OPERATOR,  # '/' handled separately (M3)
        '(': PUNCTUATION, ')': PUNCTUATION,
        '{': PUNCTUATION, '}': PUNCTUATION,
        '[': PUNCTUATION, ']': PUNCTUATION,
        ';': PUNCTUATION, ',': PUNCTUATION,
      }
      # 11 entries after removing '/' in M3
    |
  }
}
layer1_scanner: "LAYER 1 — Scanner State Machine\n(scanner.py)" {
  style: {
    fill: "#F0FDF4"
    stroke: "#16A34A"
    stroke-width: 2
    border-radius: 8
  }
  scanner_state: "Scanner Internal State\n[M1: tokenizer-m1]" {
    style: {
      fill: "#BBF7D0"
      stroke: "#15803D"
      border-radius: 6
      font-size: 13
    }
    label: |py
      # scanner.py
      class Scanner:
        source: str        # full input string (immutable)
        tokens: list[Token] # output accumulator
        # Cursor offsets (byte indices into source)
        start:        int  # left edge of current lexeme
        current:      int  # next char to read (exclusive)
        # Human-readable position
        line:         int  # 1-based, increments on \n
        column:       int  # 1-based, resets to 1 after \n
        start_column: int  # snapshot before advance()
        tab_width:    int  # default=1
        # Invariant: source[start:current] == current lexeme
        # Invariant: current <= len(source)
        # Invariant: tokens[-1].type == EOF always
    |
  }
  primitives: "Primitive Operations\n[M1: tokenizer-m1]" {
    style: {
      fill: "#BBF7D0"
      stroke: "#15803D"
      border-radius: 6
      font-size: 13
    }
    label: |py
      # scanner.py — ALL char consumption via advance()
      def is_at_end(self) -> bool:
        # O(1): current >= len(source)
        return self.current >= len(self.source)
      def advance(self) -> str:
        # Consumes 1 char, updates position
        # Called by EVERY scanner path (invariant)
        char = self.source[self.current]
        self.current += 1
        if char == '\n':
          self.line += 1; self.column = 1
        elif char == '\t':
          self.column += self.tab_width
        else:
          self.column += 1
        return char   # returns: str(1 char)
      def peek(self) -> str:
        # LA(1): no consume, returns '' at EOF
        if self.is_at_end(): return ''
        return self.source[self.current]
      def _peek_next(self) -> str:
        # LA(2): for float dot disambiguation
        if self.current + 1 >= len(self.source): return ''
        return self.source[self.current + 1]
      def _match(self, expected: str) -> bool:
        # Conditional consume — maximal munch primitive
        # [M2: tokenizer-m2]
        if self.is_at_end(): return False
        if self.source[self.current] != expected: return False
        self.advance()   # consume if match
        return True
    |
  }
  make_token: "_make_token() — Token Constructor\n[M1: tokenizer-m1]" {
    style: {
      fill: "#DCFCE7"
      stroke: "#15803D"
      border-radius: 6
      font-size: 13
    }
    label: |py
      def _make_token(self, token_type: TokenType) -> Token:
        # O(K): single slice at token boundary
        # K = len of current lexeme
        lexeme = self.source[self.start:self.current]
        return Token(token_type, lexeme,
                     self.line, self.start_column)
        # start_column snapshotted BEFORE first advance()
        # Avoids O(n^2) incremental string concat
    |
  }
}
layer2_scanloop: "LAYER 2 — Scan Dispatch & Sub-Scanners\n(scanner.py)" {
  style: {
    fill: "#FFFBEB"
    stroke: "#D97706"
    stroke-width: 2
    border-radius: 8
  }
  scan_tokens_loop: "scan_tokens() — Main Loop\n[M1: tokenizer-m1]" {
    style: {
      fill: "#FDE68A"
      stroke: "#B45309"
      border-radius: 6
      font-size: 13
    }
    label: |py
      def scan_tokens(self) -> list[Token]:
        # O(N) single pass over source of length N
        while not self.is_at_end():
          token = self._scan_token()
          if token is not None:      # None = whitespace
            self.tokens.append(token)  # ERROR tokens included
        # Always append EOF sentinel (parser contract)
        self.tokens.append(
          Token(TokenType.EOF, '', self.line, self.column))
        return self.tokens
        # Error recovery: ERROR tokens ARE valid list members
        # Multi-error: all errors collected, never halts
    |
  }
  scan_token_dispatch: "_scan_token() — FSM Dispatch\n[M1+M2+M3: all milestones]" {
    style: {
      fill: "#FDE68A"
      stroke: "#B45309"
      border-radius: 6
      font-size: 13
    }
    label: |'py
      def _scan_token(self) -> Token | None:
        self.start = self.current          # pin lexeme start
        self.start_column = self.column    # snapshot BEFORE advance
        char = self.advance()              # consume 1 char
        # M2: Two-char operators (maximal munch)
        if char == '=':
          return self._make_token(
            EQUALS if self._match('=') else ASSIGN)
        elif char == '!':
          if self._match('='): return self._make_token(NOT_EQUAL)
          return Token(ERROR, char, self.line, self.start_column)
        elif char == '<':
          return self._make_token(
            LESS_EQUAL if self._match('=') else LESS_THAN)
        elif char == '>':
          return self._make_token(
            GREATER_EQUAL if self._match('=') else GREATER_THAN)
        # M3: Division / Comment dispatch
        elif char == '/':
          if self._match('/'):
            self._skip_line_comment(); return None
          elif self._match('*'):
            return self._skip_block_comment()
          else:
            return self._make_token(OPERATOR)  # division
        # M3: String literals
        elif char == '"':
          return self._scan_string()
        # M2: Number literals
        elif char.isdigit():
          return self._scan_number()
        # M2: Identifiers + keywords
        elif char.isalpha() or char == '_':
          return self._scan_identifier()
        # M1: Single-char tokens
        elif char in self._SINGLE_CHAR_TOKENS:
          return self._make_token(
            self._SINGLE_CHAR_TOKENS[char])
        # M1: Whitespace — silent consume
        elif char in (' ', '\t', '\r', '\n'):
          return None
        # M1: Skip-one error recovery
        else:
          return Token(ERROR, char,
                       self.line, self.start_column)
    '|
  }
  sub_scanners: "Sub-Scanners\n[M2+M3: tokenizer-m2, tokenizer-m3]" {
    style: {
      fill: "#FEF3C7"
      stroke: "#B45309"
      border-radius: 6
      font-size: 13
    }
    label: |'py
      # M2: _scan_number()  [tokenizer-m2]
      # States: INTEGER → (dot + digit?) → FLOAT
      def _scan_number(self) -> Token:
        while self.peek().isdigit():
          self.advance()           # consume INT digits
        # LA(2) for float: dot must be followed by digit
        if self.peek() == '.' and self._peek_next().isdigit():
          self.advance()           # consume '.'
          while self.peek().isdigit():
            self.advance()         # consume FLOAT digits
        return self._make_token(NUMBER)
        # "42" → NUMBER("42") | "3.14" → NUMBER("3.14")
        # "3." → NUMBER("3") then ERROR(".")  ← by design
      # M2: _scan_identifier()  [tokenizer-m2]
      # Rule: SCAN FIRST, lookup SECOND (iffy ≠ if+fy)
      def _scan_identifier(self) -> Token:
        while self.peek().isalnum() or self.peek() == '_':
          self.advance()
        text = self.source[self.start:self.current]  # O(K) slice
        ttype = self._KEYWORDS.get(text, IDENTIFIER)
        return self._make_token(ttype)
      # M3: _skip_line_comment()  [tokenizer-m3]
      def _skip_line_comment(self) -> None:
        while not self.is_at_end() and self.peek() != '\n':
          self.advance()  # consume until \n (NOT the \n itself)
      # M3: _skip_block_comment()  [tokenizer-m3]
      def _skip_block_comment(self) -> Token | None:
        err_line, err_col = self.line, self.start_column
        while not self.is_at_end():
          char = self.advance()   # all chars via advance()
          if char == '*' and self.peek() == '/':
            self.advance()        # consume closing '/'
            return None           # success
        # EOF without */  → ERROR pinned to opening /*
        return Token(ERROR, '/*', err_line, err_col)
      # M3: _scan_string()  [tokenizer-m3]
      def _scan_string(self) -> Token:
        while not self.is_at_end() and self.peek() != '"':
          if self.peek() == '\n':  # no multiline strings
            return Token(ERROR,
              self.source[self.start:self.current],
              self.line, self.start_column)  # pin to opening "
          if self.peek() == '\\':  # escape: jump-over
            self.advance()         # consume backslash
            if not self.is_at_end():
              self.advance()       # consume escaped char
          else:
            self.advance()
        if self.is_at_end():
          return Token(ERROR,
            self.source[self.start:self.current],
            self.line, self.start_column)   # pin to opening "
        self.advance()             # consume closing '"'
        return self._make_token(STRING)
        # "hello\nworld" → STRING('"hello\\nworld"', raw lexeme)
    '|
  }
}
layer3_tests: "LAYER 3 — Test Suite & Integration\n(test_foundation.py · test_multi_char.py · test_strings_comments.py · test_integration.py)" {
  style: {
    fill: "#FFF1F2"
    stroke: "#E11D48"
    stroke-width: 2
    border-radius: 8
  }
  m1_tests: "M1 Tests — Foundation\n[tokenizer-m1]" {
    style: {
      fill: "#FFE4E6"
      stroke: "#BE123C"
      border-radius: 6
      font-size: 12
    }
    label: |py
      # test_foundation.py
      # Input: ""  → [EOF(1,1)]
      # Input: "+-*/" → [OP+, OP-, OP*, OP/, EOF]
      # Input: "(){};," → [6×PUNCT, EOF]
      # Input: "  \t\n  " → [EOF only]
      # Input: "(\n+" → LParen(1,1), Plus(2,1), EOF
      # Input: "@" → [ERROR(@,1,1), EOF]
      # Input: "@+" → [ERROR(@), OP(+), EOF]  ← skip-one
      # Input: "@#$" → [ERROR×3, EOF]  ← multi-error
    |
  }
  m2_tests: "M2 Tests — Multi-Char & Maximal Munch\n[tokenizer-m2]" {
    style: {
      fill: "#FFE4E6"
      stroke: "#BE123C"
      border-radius: 6
      font-size: 12
    }
    label: |py
      # test_multi_char.py
      # MAXIMAL MUNCH CANONICAL:
      # ">=="  → GREATER_EQUAL(">=") + ASSIGN("=") + EOF
      #           NOT GREATER_THAN + EQUALS
      # "=="   → EQUALS("==") [1 token not 2]
      # "!="   → NOT_EQUAL("!=")
      # "3.14" → NUMBER("3.14")
      # "3."   → NUMBER("3") + ERROR(".")
      # "iffy" → IDENTIFIER("iffy")  [NOT KEYWORD(if)+ID(fy)]
      # "if "  → KEYWORD("if")
      # CANONICAL INTEGRATION:
      # "if (x >= 42) { return true; }"
      #  → KEYWORD(if) PUNCT(() IDENT(x) GTE(>=)
      #    NUMBER(42) PUNCT()) PUNCT({)
      #    KEYWORD(return) KEYWORD(true) PUNCT(;)
      #    PUNCT(}) EOF   [12 tokens exactly]
    |
  }
  m3_tests: "M3 Tests — Strings & Comments\n[tokenizer-m3]" {
    style: {
      fill: "#FFE4E6"
      stroke: "#BE123C"
      border-radius: 6
      font-size: 12
    }
    label: |py
      # test_strings_comments.py
      # MODE ISOLATION:
      # '"hello // world"' → STRING [1 token, not comment]
      # '/* "str" */'      → EOF   [string inside comment ignored]
      # "// comment\n+"    → OP(+) line=2  [comment produces nothing]
      # "/* \n */"         → EOF, line tracking through \n
      # "/* unterminated"  → ERROR("/*", 1, 1)  [pin to opener]
      # '"hello\nworld"'   → ERROR(at opening ")  [no multiline]
      # '"say \"hi\""'     → STRING [escape jump-over works]
      # '/* /* */'         → EOF + IDENT(x) [non-nesting]
      # POSITION AFTER BLOCK COMMENT:
      # "a\n/*\n\n*/\nb"  → ID(a,1,1) ID(b,5,1)
    |
  }
  m4_tests: "M4 Tests — Integration & Performance\n[tokenizer-m4]" {
    style: {
      fill: "#FFE4E6"
      stroke: "#BE123C"
      border-radius: 6
      font-size: 12
    }
    label: |py
      # test_integration.py  test_position.py  test_boundaries.py
      # GOLDEN TEST (canonical_source.txt):
      #   /* block */ if (x >= 42.0) { return "R" + x; }
      #   → KEYWORD(if)@5:1, PUNCT(()@5:4, IDENT(x)@5:5,
      #     GTE(>=)@5:7, NUMBER(42.0)@5:10 ... EOF@11:1
      # ERROR RECOVERY:
      #   "@if" → ERROR(@,1,1) + KEYWORD(if,1,2)  [skip-one]
      #   "@#$%" → 4×ERROR, EOF  [all collected 1 pass]
      # POSITION DRIFT (100-line test):
      #   "x1\nx2\n...x100" → token[i].line == i+1, col==1
      # CRLF: "\r\n" counts as 1 line (not 2)
      # BOUNDARY: "" → [EOF(1,1)] only
      # BOUNDARY: "a"*10000 → IDENTIFIER, len==10000
      # PERFORMANCE: 10,000 lines < 1.0s
      #   perf_counter() wrapping scan_tokens()
      #   Avoid O(n^2): slice ONCE at token boundary
    |
  }
}
layer4_pipeline: "LAYER 4 — Compiler Pipeline Position" {
  style: {
    fill: "#F0F9FF"
    stroke: "#0284C7"
    stroke-width: 2
    border-radius: 8
  }
  source_input: "Source String\nraw str input" {
    style: {
      fill: "#BAE6FD"
      stroke: "#0369A1"
      border-radius: 6
    }
    label: |md
      **Input**
      `"if (x >= 42) { return true; }"`
      - Type: `str`
      - Size: N bytes
      - Encoding: UTF-8/internal
    |
  }
  tokenizer_box: "Scanner / Tokenizer\nscanner.py — O(N)" {
    style: {
      fill: "#7DD3FC"
      stroke: "#0369A1"
      border-radius: 6
      bold: true
    }
    label: |md
      **THIS PROJECT**
      Character-level FSM
      4 modes: NORMAL / STRING /
      LINE_COMMENT / BLOCK_COMMENT
      Position tracking: line + col
      Error recovery: skip-one
      Output: `list[Token]`
    |
  }
  parser_box: "Parser (downstream)\nNot implemented" {
    style: {
      fill: "#E0F2FE"
      stroke: "#0369A1"
      border-radius: 6
    }
    label: |md
      **Consumer of token stream**
      Reads `list[Token]`
      Dispatches on `token.type`
      Never sees raw `str` again
      Builds AST
    |
  }
  error_reporter: "Error Reporter\nFilter ERROR tokens" {
    style: {
      fill: "#FEE2E2"
      stroke: "#DC2626"
      border-radius: 6
    }
    label: |md
      **Multi-error display**
      `errors = [t for t in tokens`
      `  if t.type == ERROR]`
      Reports: lexeme, line, col
      All errors in 1 pass
    |
  }
  source_input -> tokenizer_box: "str | N chars | \"if (x >= 42)...\"" {
    style: {
      stroke: "#0284C7"
      font-size: 11
    }
  }
  tokenizer_box -> parser_box: "list[Token] | ~N tokens | Token(KEYWORD,'if',1,1)" {
    style: {
      stroke: "#16A34A"
      font-size: 11
    }
  }
  tokenizer_box -> error_reporter: "ERROR tokens | 0..M errors | Token(ERROR,'@',3,7)" {
    style: {
      stroke: "#DC2626"
      stroke-dash: 4
      font-size: 11
    }
  }
}
fsm_states: "FSM State Transitions\n(Conceptual — encoded as call stack)" {
  style: {
    fill: "#FAF5FF"
    stroke: "#7C3AED"
    stroke-width: 2
    border-radius: 8
  }
  normal_state: NORMAL {
    shape: circle
    style: {
      fill: "#DDD6FE"
      stroke: "#6D28D9"
      double-border: true
    }
  }
  string_state: IN_STRING {
    shape: circle
    style: {
      fill: "#F3E8FF"
      stroke: "#7C3AED"
    }
  }
  line_comment_state: IN_LINE_COMMENT {
    shape: circle
    style: {
      fill: "#F3E8FF"
      stroke: "#7C3AED"
    }
  }
  block_comment_state: IN_BLOCK_COMMENT {
    shape: circle
    style: {
      fill: "#F3E8FF"
      stroke: "#7C3AED"
    }
  }
  normal_state -> string_state: "char==\" | enter _scan_string()" {
    style.stroke: "#7C3AED"
  }
  string_state -> normal_state: "peek==\" consumed | emit STRING" {
    style.stroke: "#16A34A"
  }
  string_state -> normal_state: "\\n or EOF | emit ERROR (pin to opener)" {
    style.stroke: "#DC2626"
    style.stroke-dash: 4
  }
  normal_state -> line_comment_state: "// matched | _skip_line_comment()" {
    style.stroke: "#7C3AED"
  }
  line_comment_state -> normal_state: "peek==\\n or EOF | return None" {
    style.stroke: "#16A34A"
  }
  normal_state -> block_comment_state: "/* matched | _skip_block_comment()" {
    style.stroke: "#7C3AED"
  }
  block_comment_state -> normal_state: "*/ found | return None" {
    style.stroke: "#16A34A"
  }
  block_comment_state -> normal_state: "EOF | emit ERROR(/*) at opener" {
    style.stroke: "#DC2626"
    style.stroke-dash: 4
  }
}
maximal_munch_trace: "Maximal Munch Trace: \">==\" → GTE + ASSIGN\n[M2: tokenizer-m2]" {
  style: {
    fill: "#ECFDF5"
    stroke: "#059669"
    stroke-width: 2
    border-radius: 8
  }
  label: |py
    # Input: ">=="   current=0
    # Step 1: _scan_token() called
    #   start=0, start_column=1
    #   advance() → '>', current=1, column=2
    #   char=='>' → branch to '>'-handler
    #   _match('='): source[1]=='=' → True
    #     advance() consumes '=', current=2, column=3
    #   _make_token(GREATER_EQUAL)
    #   lexeme = source[0:2] = ">="
    #   → emit Token(GREATER_EQUAL, ">=", 1, 1)
    #
    # Step 2: _scan_token() called
    #   start=2, start_column=3
    #   advance() → '=', current=3, column=4
    #   char=='=' → branch to '='-handler
    #   _match('='): is_at_end() → True → False
    #   _make_token(ASSIGN)
    #   lexeme = source[2:3] = "="
    #   → emit Token(ASSIGN, "=", 1, 3)
    #
    # Result: [GREATER_EQUAL(">="), ASSIGN("="), EOF]
    # NOT:    [GREATER_THAN(">"),   EQUALS("=="), EOF]
  |
}
position_tracking_detail: "Position Tracking Detail\n[M1: tokenizer-m1 — advance() invariant]" {
  style: {
    fill: "#FFFBEB"
    stroke: "#D97706"
    stroke-width: 2
    border-radius: 8
  }
  label: |py
    # INVARIANT: ALL char consumption goes through advance()
    # This includes: _scan_string(), _skip_block_comment()
    # Result: line/column always in sync with char stream
    # source = "(\n+"
    #          ^01^2  (indices)
    # advance() → '(' : column=2
    # advance() → '\n': line=2, column=1   ← reset
    # advance() → '+' : column=2
    # Token LParen: line=1, col=1  (start_column snapshot)
    # Token Plus:   line=2, col=1  (start_column snapshot)
    # CRLF handling: '\r' → whitespace (col+1), '\n' → line++
    # Tab handling:  '\t' → column += tab_width (default=1)
    # Error pinning: start_column captured BEFORE advance()
    # Position drift test (100 lines):
    # for i, token in enumerate(non_eof_tokens):
    #   assert token.line == i + 1   # zero drift
    #   assert token.column == 1
  |
}
error_recovery_detail: "Skip-One Error Recovery\n[M1+M4: tokenizer-m1, tokenizer-m4]" {
  style: {
    fill: "#FFF1F2"
    stroke: "#E11D48"
    stroke-width: 2
    border-radius: 8
  }
  label: |py
    # Input: "@if"
    # _scan_token() call 1:
    #   advance() → '@'  current=1
    #   else branch → Token(ERROR, '@', 1, 1)
    #   advance() already moved past '@'
    #   return ERROR token
    # scan_tokens() appends ERROR, calls _scan_token() again
    # _scan_token() call 2:
    #   start=1, start_column=2
    #   advance() → 'i', then _scan_identifier()
    #   consumes 'f', text="if" → KEYWORD
    #   → Token(KEYWORD, "if", 1, 2)
    #
    # Result: [ERROR(@,1,1), KEYWORD(if,1,2), EOF]
    # Recovery skips EXACTLY 1 char — never more
    # All errors are values in list[Token] — never exceptions
    # Multi-error: "@#$%" → 4×ERROR, all in one pass
    # Performance target: 10,000 lines < 1.0s (O(N))
    # Anti-pattern: string concat in loop → O(N^2) — AVOID
  |
}
layer0_types -> layer1_scanner: "Token, TokenType | ~56B/token | Token(KEYWORD,'if',1,1)" {
  style: {
    stroke: "#6366F1"
    font-size: 11
  }
}
layer1_scanner -> layer2_scanloop: "advance()/peek()/_match() | O(1) | char='='" {
  style: {
    stroke: "#16A34A"
    font-size: 11
  }
}
layer2_scanloop -> layer3_tests: "list[Token] | N tokens | verified token-by-token" {
  style: {
    stroke: "#D97706"
    font-size: 11
  }
}
layer2_scanloop -> layer4_pipeline: "list[Token] | contract API | Token stream" {
  style: {
    stroke: "#0284C7"
    font-size: 11
  }
}
layer2_scanloop -> fsm_states: "mode transitions | implicit via call stack" {
  style: {
    stroke: "#7C3AED"
    font-size: 11
  }
}
maximal_munch_trace -> layer2_scanloop: "illustrates _scan_token dispatch" {
  style: {
    stroke: "#059669"
    stroke-dash: 3
    font-size: 11
  }
}
position_tracking_detail -> layer1_scanner: "advance() invariant" {
  style: {
    stroke: "#D97706"
    stroke-dash: 3
    font-size: 11
  }
}
error_recovery_detail -> layer2_scanloop: "skip-one in else branch" {
  style: {
    stroke: "#E11D48"
    stroke-dash: 3
    font-size: 11
  }
}
milestone_index: "Milestone Index" {
  style: {
    fill: "#F8FAFC"
    stroke: "#64748B"
    stroke-width: 1
    border-radius: 6
  }
  label: |'md
    | ID | Name | Key Deliverable |
    |---|---|---|
    | **tokenizer-m1** | Token Types & Scanner Foundation | TokenType enum, Token dataclass, advance/peek, single-char dispatch, skip-one error |
    | **tokenizer-m2** | Multi-Char Tokens & Maximal Munch | _match(), two-char operators, _scan_number() LA(2), _scan_identifier() + keyword lookup |
    | **tokenizer-m3** | Strings & Comments | _scan_string() escape jump-over, _skip_line_comment(), _skip_block_comment(), mode isolation |
    | **tokenizer-m4** | Integration Testing & Error Recovery | Golden tests, position drift validation, multi-error collection, 10k-line perf < 1s |
  '|
}