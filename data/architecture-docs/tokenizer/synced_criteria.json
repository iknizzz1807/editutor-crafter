[
  {
    "milestone_id": "tokenizer-m1",
    "criteria": [
      "TokenType enum defines exactly 8 variants: NUMBER, STRING, IDENTIFIER, KEYWORD, OPERATOR, PUNCTUATION, EOF, ERROR — each using auto() and inheriting from Enum",
      "Token dataclass stores four fields: type (TokenType), lexeme (str), line (int), column (int) — with no additional required fields",
      "Scanner.__init__ initializes start, current (both 0), line (1), column (1), and start_column (1) position trackers alongside source and tokens list",
      "advance() consumes source[current], increments current, updates line (increment on \\n, reset column to 1) and column (increment on non-newline), and returns the consumed character",
      "peek() returns source[current] without incrementing current; returns empty string '' when is_at_end() is True",
      "is_at_end() returns True when current >= len(source)",
      "_scan_token() snapshots start = current and start_column = column BEFORE calling advance(), ensuring token position reflects where the token begins",
      "All 12 single-character tokens (+, -, *, /, (, ), {, }, [, ], ;, ,) are dispatched via lookup table and emitted with correct TokenType (OPERATOR or PUNCTUATION)",
      "Whitespace characters (space, tab, \\r, \\n) are consumed by advance() without emitting any token; _scan_token() returns None for whitespace",
      "scan_tokens() appends a final EOF token with empty lexeme after exhausting input; EOF is always the last token in the list",
      "Unrecognized characters produce an ERROR token carrying the offending character as lexeme, with correct line and column of that character",
      "After emitting an ERROR token, scanning continues from the next character — all errors in a single input are collected (not just the first)",
      "Column resets to 1 after a newline: a token on line 2 column 1 reports line=2, column=1 regardless of content on line 1",
      "Windows \\r\\n line endings do not double-count newlines: \\r is consumed silently, only \\n increments line",
      "Empty string input produces exactly one token: EOF at line=1, column=1",
      "Token lexeme contains the exact raw source text (source[start:current]) for every token type including single-character tokens"
    ]
  },
  {
    "milestone_id": "tokenizer-m2",
    "criteria": [
      "TokenType enum is extended with distinct variants: ASSIGN (=), EQUALS (==), NOT_EQUAL (!=), LESS_THAN (<), LESS_EQUAL (<=), GREATER_THAN (>), GREATER_EQUAL (>=), in addition to the existing OPERATOR, NUMBER, IDENTIFIER, KEYWORD, PUNCTUATION, EOF, ERROR",
      "A _match(expected) helper method is implemented: it peeks at the current character, consumes it and returns True only if it equals expected, otherwise returns False without consuming — implementing conditional single-character lookahead",
      "Two-character operators ==, !=, <=, >= are each recognized as a single token using the _match() lookahead: '=' followed by '=' emits EQUALS('=='), not two ASSIGN tokens",
      "Single '=' not followed by '=' emits ASSIGN('=') — maximal munch does not overconsume",
      "'<' not followed by '=' emits LESS_THAN('<'); '<=' emits LESS_EQUAL('<=') as a single token",
      "'>' not followed by '=' emits GREATER_THAN('>'); '>=' emits GREATER_EQUAL('>=') as a single token",
      "Input '>== ' tokenizes as GREATER_EQUAL('>=') then ASSIGN('=') — demonstrating correct maximal munch across three characters",
      "'!' followed by '=' emits NOT_EQUAL('!='); '!' not followed by '=' emits an ERROR token",
      "Integer literals (sequences of one or more digits) are scanned by _scan_number() and emitted as NUMBER tokens with the exact digit string as lexeme (e.g., NUMBER('42'), NUMBER('0'))",
      "Float literals (digit sequence, dot, digit sequence) are scanned as a single NUMBER token (e.g., NUMBER('3.14'), NUMBER('0.5'))",
      "Trailing-dot input '3.' emits NUMBER('3') then an ERROR or PUNCTUATION for the bare dot — the dot is NOT consumed as part of the float",
      "Identifiers (starting with letter or underscore, continuing with alphanumeric or underscore) are scanned by _scan_identifier() and emitted as IDENTIFIER tokens",
      "Keyword lookup occurs AFTER scanning the complete identifier text — the complete lexeme is looked up in the _KEYWORDS dict; if found, KEYWORD is emitted; otherwise IDENTIFIER",
      "All seven keywords (if, else, while, return, true, false, null) emit KEYWORD tokens with exact keyword text as lexeme",
      "Identifier 'iffy' emits a single IDENTIFIER('iffy') token — NOT KEYWORD('if') + IDENTIFIER('fy')",
      "Identifiers containing keyword substrings (e.g., 'while_loop', 'return_value', 'nullify') emit a single IDENTIFIER token, not split on the keyword",
      "Scanning 'if (x >= 42) { return true; }' produces exactly: KEYWORD('if'), PUNCTUATION('('), IDENTIFIER('x'), GREATER_EQUAL('>='), NUMBER('42'), PUNCTUATION(')'), PUNCTUATION('{'), KEYWORD('return'), KEYWORD('true'), PUNCTUATION(';'), PUNCTUATION('}'), EOF('')",
      "Token positions (line, column) are correct for all new token types — start_column is snapshotted before advance() is called at the start of each _scan_token() invocation",
      "A _peek_next() method is implemented that returns the character two positions ahead without consuming, used for trailing-dot disambiguation in float scanning",
      "All M1 tests continue to pass — single-character tokens, whitespace consumption, EOF, and error recovery are unaffected by M2 additions"
    ]
  },
  {
    "milestone_id": "tokenizer-m4",
    "criteria": [
      "Integration test verifies token-by-token that 'if (x >= 42) { return true; }' produces exactly [KEYWORD('if'), PUNCTUATION('('), IDENTIFIER('x'), GREATER_EQUAL('>='), NUMBER('42'), PUNCTUATION(')'), PUNCTUATION('{'), KEYWORD('return'), KEYWORD('true'), PUNCTUATION(';'), PUNCTUATION('}'), EOF('')] — 12 tokens total, types and lexemes both verified",
      "Error recovery: after encountering an unrecognized character (e.g. '@'), the scanner emits a single ERROR token for that character and the very next call to _scan_token() starts fresh at the character immediately following — verified by asserting the token after an ERROR is the correct next valid token",
      "Multiple errors in a single input (e.g. '@#$%') are ALL reported in one scan_tokens() pass — verified by counting ERROR tokens in the returned list equals the number of bad characters",
      "Error tokens contain the exact offending character as their lexeme and the correct line/column of that character — verified for errors on both line 1 and subsequent lines",
      "Error recovery skips exactly ONE character: confirmed by 'Scanner(\"@if\").scan_tokens()' producing ERROR('@') then KEYWORD('if'), not ERROR('@i') then IDENTIFIER('f')",
      "Empty input ('') produces exactly one token: EOF at line 1, column 1 — no other tokens, no crash",
      "Single valid character inputs ('+', '(', '0', 'x') each produce exactly 2 tokens: the appropriate token type plus EOF",
      "Single invalid character input ('@') produces exactly 2 tokens: ERROR('@') plus EOF",
      "A 10,000-character identifier (e.g. 'a' * 10000) is tokenized as a single IDENTIFIER token whose lexeme length is exactly 10,000",
      "Identifiers that begin with a keyword but are longer (e.g. 'iffy', 'returning', 'trueness') are tokenized as IDENTIFIER, not as KEYWORD followed by residual IDENTIFIER",
      "A complete multi-line program (containing line comments, block comments, string literals, all operator types, and keywords) produces zero ERROR tokens when the source is valid",
      "Line numbers are accurate with zero drift across a 100-line input: every token on line N reports line == N, verified for all 100 tokens",
      "Column resets to 1 on the first token of each new line, verified explicitly after newlines",
      "Windows line endings (\\r\\n) count as exactly ONE line increment, not two — verified by checking that 3 lines separated by \\r\\n produce tokens on lines 1, 2, and 3",
      "Position of ERROR token is the column of the bad character itself, not some adjacent position — verified for an error character in the middle of a line",
      "After a multi-line block comment spanning N lines, the first token after the comment reports the correct line number (opening line + N) — verified with a concrete 4-line block comment",
      "Tokenizing a 10,000-line synthetic program (covering all token types) completes in under 1.0 second as measured by time.perf_counter()",
      "No O(n^2) behavior: a 10,000-character string literal and a 10,000-character sequence of error characters both complete in under 0.5 seconds each",
      "scan_tokens() always appends EOF as the final token regardless of input content, including inputs that end mid-string or mid-comment",
      "A complete test suite (organized as individual test functions) covers all four milestone acceptance criteria and can be run standalone with 'python test_scanner.py' or 'pytest test_scanner.py'"
    ]
  },
  {
    "module_id": "tokenizer-m1",
    "criteria": [
      "TokenType enum defines Number, String, Identifier, Keyword, Operator, Punctuation, EOF, Error",
      "Token dataclass includes type, lexeme, line, and column",
      "Scanner.advance() correctly increments line on '\\n' and resets column",
      "Scanner.peek() returns current character without advancing",
      "Scanner emits ERROR token for unknown characters and continues",
      "Scanner always appends EOF token at end of input",
      "Whitespace (space, tab, cr, lf) is consumed without emitting tokens"
    ]
  },
  {
    "module_id": "tokenizer-m2",
    "criteria": [
      "TokenType updated with EQUALS, NOT_EQUAL, LESS_EQUAL, GREATER_EQUAL, ASSIGN, LESS_THAN, GREATER_THAN",
      "_match(expected) correctly implements conditional advance",
      "_peek_next() correctly looks two characters ahead",
      "_scan_number() rejects trailing dots as floats via _peek_next()",
      "_scan_identifier() performs keyword lookup after greedy character consumption",
      "Maximal munch correctly resolves >== as GREATER_EQUAL followed by ASSIGN",
      "Keyword lookup table implemented as a hash map (dict)"
    ]
  },
  {
    "module_id": "tokenizer-m3",
    "criteria": [
      "Scanner handles // line comments by discarding all characters until newline or EOF.",
      "Scanner handles /* */ block comments, correctly ignoring all characters (including quotes) until */.",
      "Scanner correctly identifies division / operator when not followed by / or *.",
      "Scanner handles double-quoted string literals and stores the raw lexeme including quotes.",
      "Escape sequences (\\\", \\\\, \\n, etc.) inside strings do not prematurely terminate the string.",
      "Unterminated string literals (reaching EOF or newline) produce an ERROR token.",
      "Unterminated block comments produce an ERROR token.",
      "Error tokens for strings/comments point to the opening delimiter's line/column.",
      "Line and column counts remain accurate after multi-line comments or strings.",
      "Mode isolation is verified: comment markers inside strings are treated as literal text."
    ]
  },
  {
    "module_id": "tokenizer-m4",
    "criteria": [
      "verify_canonical_expression_token_stream",
      "verify_skip_one_error_recovery_no_swallowing",
      "verify_multi_error_collection_in_single_pass",
      "verify_zero_position_drift_over_50_lines",
      "verify_performance_10k_lines_under_1_second",
      "verify_max_length_identifier_linear_time",
      "verify_crlf_handling_single_line_increment",
      "verify_eof_at_end_of_all_inputs"
    ]
  }
]