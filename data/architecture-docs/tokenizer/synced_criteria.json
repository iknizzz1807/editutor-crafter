[
  {
    "milestone_id": "tokenizer-m1",
    "criteria": [
      "TokenType enum defines named variants for all categories: NUMBER, STRING, IDENTIFIER, KEYWORD, PLUS, MINUS, STAR, SLASH, ASSIGN, EQUAL, NOT_EQUAL, LESS, LESS_EQ, GREATER, GREATER_EQ, BANG, LPAREN, RPAREN, LBRACE, RBRACE, LBRACKET, RBRACKET, SEMICOLON, COMMA, EOF, ERROR",
      "Token is a dataclass with four fields: type (TokenType), lexeme (str), line (int), column (int)",
      "Token supports equality comparison via dataclass __eq__ so tests can assert Token(...) == Token(...)",
      "Token.__repr__ returns a human-readable string including type name, lexeme, line and column",
      "Scanner.__init__ accepts source string and initializes current=0, line=1, column=1",
      "Scanner.is_at_end() returns True when and only when self.current >= len(self.source)",
      "Scanner.peek() returns self.source[self.current] without modifying self.current, and returns '\\0' (null character) when is_at_end() is True",
      "Scanner.advance() returns the character at self.current, increments self.current by 1, increments self.line and resets self.column to 1 on '\\n', increments self.column by 1 for all other characters",
      "Scanner._skip_whitespace() consumes space, tab, '\\r', and '\\n' characters by calling self.advance() (not direct index manipulation), emitting no tokens",
      "tok_line and tok_col are captured after _skip_whitespace() but before the call to advance() that reads the first character of the token",
      "Single-character tokens (+, -, *, /, (, ), {, }, [, ], ;, ,) are recognized via a dictionary lookup and emitted as the correct TokenType with lexeme equal to the single character",
      "Whitespace-only input (spaces, tabs, newlines) produces exactly one token: EOF",
      "Scanner.scan_tokens() returns a list of Token objects where the final element is always an EOF token with type TokenType.EOF",
      "EOF token has an empty string lexeme ('') and carries the line and column of the position immediately after the last character (or 1:1 for empty input)",
      "Unrecognized characters (e.g., @, #, $) produce an Error token with the offending character as the lexeme and the correct (line, column) of that character",
      "After emitting an Error token, scanning continues from the next character (error recovery: subsequent valid tokens are still emitted correctly)",
      "Newline character causes self.line to increment by 1 and self.column to reset to 1, so the first character on the new line is reported at column 1",
      "Windows line ending '\\r\\n' increments self.line exactly once (not twice), because only '\\n' triggers the line increment",
      "Multi-line input: each token's reported line and column matches the position of its first character in the source, verified across at least three lines",
      "Empty string input ('') to Scanner produces exactly one token: Token(TokenType.EOF, '', 1, 1)",
      "Single valid character input produces two tokens: the recognized token followed by EOF"
    ]
  },
  {
    "milestone_id": "tokenizer-m2",
    "criteria": [
      "The `_match(expected)` helper method peeks at the next character and consumes it only if it equals `expected`, returning True on consumption and False otherwise, without bypassing `advance()` position tracking",
      "Two-character operators `==`, `!=`, `<=`, `>=` are each emitted as a single token with lexeme exactly matching the two-character sequence (e.g., `Token(TokenType.EQUAL, '==', line, col)`)",
      "Single `=` is emitted as `Token(TokenType.ASSIGN, '=', ...)` only when the next character is not `=`",
      "Single `<` is emitted as `Token(TokenType.LESS, '<', ...)` when not followed by `=`; `<=` is emitted as `Token(TokenType.LESS_EQ, '<=', ...)`",
      "Single `>` is emitted as `Token(TokenType.GREATER, '>', ...)` when not followed by `=`; `>=` is emitted as `Token(TokenType.GREATER_EQ, '>=', ...)`",
      "Single `!` is emitted as `Token(TokenType.BANG, '!', ...)` when not followed by `=`; `!=` is emitted as `Token(TokenType.NOT_EQUAL, '!=', ...)`",
      "Input `>==` is tokenized as `[GREATER_EQ('>=', 1:1), ASSIGN('=', 1:3), EOF('', 1:4)]` — maximal munch consumes `>=` greedily, leaving the second `=` for the next token",
      "Integer literals (sequences of one or more digits: `0`, `42`, `1000`) are scanned as `Token(TokenType.NUMBER, lexeme, ...)` where `lexeme` is the exact digit string",
      "Float literals (digits, single dot, digits: `3.14`, `0.5`, `100.0`) are scanned as `Token(TokenType.NUMBER, lexeme, ...)` where `lexeme` includes the decimal point",
      "A `_peek_next()` method exists that looks two characters ahead without consuming, returning `'\\0'` when fewer than two characters remain",
      "Trailing-dot input `3.` scans as `Token(NUMBER, '3', ...)` followed by a separate token for `.` — the dot is NOT consumed as part of the number because no digit follows it",
      "Leading-dot input `.5` does NOT produce a NUMBER token — `.` is not a digit so `_scan_number` is never triggered; `.` is emitted as an ERROR or punctuation token",
      "Input `42abc` produces `[NUMBER('42', 1:1), IDENTIFIER('abc', 1:3)]` — the number and identifier are separate tokens with correct positions",
      "Identifiers begin with a letter (`a-z`, `A-Z`) or underscore (`_`), followed by zero or more letters, digits, or underscores",
      "The keyword table `KEYWORDS` maps exactly `if`, `else`, `while`, `return`, `true`, `false`, `null` to `TokenType.KEYWORD`",
      "Keywords are detected using scan-then-lookup: the full identifier lexeme is scanned first, then `KEYWORDS.get(lexeme, TokenType.IDENTIFIER)` determines the token type",
      "A prefix of a keyword inside a longer identifier is NOT matched as a keyword: `iffy` emits `IDENTIFIER('iffy')`, `if_x` emits `IDENTIFIER('if_x')`, `returnValue` emits `IDENTIFIER('returnValue')`",
      "All seven keywords (`if`, `else`, `while`, `return`, `true`, `false`, `null`) emit `TokenType.KEYWORD` tokens with lexeme equal to the exact keyword string",
      "The canonical statement `if (x >= 42) { return true; }` produces exactly: `KEYWORD('if')`, `LPAREN('(')`, `IDENTIFIER('x')`, `GREATER_EQ('>=')`, `NUMBER('42')`, `RPAREN(')')`, `LBRACE('{')`, `KEYWORD('return')`, `KEYWORD('true')`, `SEMICOLON(';')`, `RBRACE('}')`, `EOF('')` with correct line and column for each token",
      "Token positions (line, column) for all multi-character tokens reflect the position of the token's FIRST character, not its last",
      "The `SINGLE_CHAR_TOKENS` dictionary no longer contains `=`, `!`, `<`, `>` — these are dispatched through `_scan_operator()` for maximal munch handling",
      "The scanning dispatch in `next_token()` checks `char.isdigit()` before `char.isalpha()` to ensure digit-starting inputs trigger number scanning, not identifier scanning"
    ]
  },
  {
    "milestone_id": "tokenizer-m3",
    "criteria": [
      "String literals delimited by double quotes are scanned and returned as STRING tokens with lexeme including the surrounding quote characters",
      "The lexeme of a STRING token stores the raw source characters exactly as they appear in the source, including backslash and escape letter (e.g., source '\"hello\\nworld\"' produces lexeme '\"hello\\\\nworld\"')",
      "Escape sequences \\n, \\t, \\r, \\\", \\\\ inside strings are accepted as valid and included in the raw lexeme without modification",
      "An invalid escape sequence (e.g., '\\q', '\\z') inside a string produces an ERROR token whose line and column point to the opening quote of the string",
      "A string terminated by EOF before a closing quote produces an ERROR token whose line and column point to the opening quote character",
      "A string containing a bare newline (unescaped \\n) before a closing quote produces an ERROR token whose line and column point to the opening quote character",
      "A backslash at EOF inside a string (e.g., '\"hello\\\\') produces an ERROR token at the opening quote position",
      "Single-line comments beginning with '//' consume all characters up to (but not including) the newline or EOF and produce no token in the output stream",
      "Multi-line block comments beginning with '/*' consume all characters including newlines up to and including the first '*/' sequence and produce no token in the output stream",
      "Line numbers (self.line) are correctly incremented for every newline character consumed inside a multi-line block comment",
      "Line numbers (self.line) are correctly incremented for every newline character consumed inside a string literal being scanned",
      "An unterminated block comment (no closing '*/') produces an ERROR token with lexeme '/*' and line/column pointing to the opening '/*' position",
      "Multi-line block comments do NOT nest: '/* /* */' terminates at the first '*/' encountered, leaving any subsequent text as normal source code",
      "A '/' character followed by neither '/' nor '*' is emitted as a SLASH token (division operator) with correct line and column",
      "Comment syntax (//, /*, */) appearing inside a string literal is treated as ordinary string content and does not trigger comment scanning",
      "Tokens appearing after a multi-line block comment carry correct line and column numbers reflecting the actual source position after all consumed newlines",
      "The _scan_string method uses a list buffer joined with ''.join() rather than repeated string concatenation to accumulate the lexeme"
    ]
  },
  {
    "milestone_id": "tokenizer-m4",
    "criteria": [
      "The token stream for 'if (x >= 42) { return true; }' produces exactly Token(KEYWORD,'if',1,1), Token(LPAREN,'(',1,4), Token(IDENTIFIER,'x',1,5), Token(GREATER_EQ,'>=',1,7), Token(NUMBER,'42',1,10), Token(RPAREN,')',1,12), Token(LBRACE,'{',1,14), Token(KEYWORD,'return',1,16), Token(KEYWORD,'true',1,23), Token(SEMICOLON,';',1,27), Token(RBRACE,'}',1,29), Token(EOF,'',1,30) — verified token-by-token with type, lexeme, line, and column",
      "A complete multi-line source program (containing at least one single-line comment, one block comment spanning multiple lines, identifiers, operators, number and string literals, and keywords on separate lines) is tokenized with zero ERROR tokens and all token positions (line, column) verified accurate",
      "Error recovery: after an unrecognized character (e.g., '@'), scanning resumes from the next character — the character after '@' is correctly tokenized as its own token with the correct type and position",
      "Multiple invalid characters in a single input (e.g., '@#$') each produce a separate ERROR token with the correct lexeme (single character) and the correct line and column position for each",
      "Error tokens do not corrupt the position of surrounding valid tokens: an identifier following an ERROR token has the correct column number",
      "Empty string input produces exactly one token: Token(EOF, '', 1, 1)",
      "Single valid character input (e.g., '+') produces the correct token followed by Token(EOF, '', 1, 2)",
      "Single newline input produces Token(EOF, '', 2, 1) — the newline is consumed as whitespace and line tracking is updated",
      "An identifier of 500 characters is scanned as a single IDENTIFIER token with the full 500-character lexeme",
      "An identifier that begins with a keyword prefix (e.g., 'iffy', 'returnValue', 'if_x') is correctly classified as IDENTIFIER, not KEYWORD",
      "All token line numbers in any scan result are in the range [1, newline_count + 1]",
      "All token column numbers in any scan result are >= 1",
      "On any single source line, successive token column numbers are strictly increasing (non-decreasing with no backwards movement)",
      "After a multi-line block comment spanning N lines, the first token following the comment has line number = (line of '/*' opening) + N",
      "The position (line, column) of a STRING token is the position of the opening quote character, not the closing quote",
      "The ERROR token for an unterminated string reports the line and column of the opening '\"', not of the EOF or newline where the error was detected",
      "The ERROR token for an unterminated block comment has lexeme '/*' and reports the line and column of the opening '/*'",
      "Tokenizing a 10,000-line source file (containing a realistic mix of comments, identifiers, operators, number literals, and string literals) completes in under 1.0 second as measured by time.perf_counter()",
      "The last token in every scan_tokens() result is always EOF, including after error tokens, and the total token count matches the number of actual tokens (no doubles or missing tokens)",
      "Adjacent tokens with no whitespace between them (e.g., 'a+b', 'x>=y', '42+3') are correctly separated into individual tokens with accurate column positions"
    ]
  },
  {
    "module_id": "tokenizer-m1",
    "criteria": [
      "TokenType enumeration defines all 28 categories (Number, String, Identifier, Keyword, Operators, Punctuation, EOF, Error).",
      "Token dataclass stores type, lexeme, line, and column.",
      "Scanner.advance() correctly increments line and resets column on '\\n'.",
      "Scanner.peek() returns '\\0' sentinel at end of file.",
      "Scanner tracks position (line/column) accurately across multiple lines.",
      "Whitespace (space, tab, \\r, \\n) is consumed without emitting tokens.",
      "Single-character tokens (+, -, *, /, (, ), {, }, [, ], ;, ,) are recognized and emitted.",
      "Invalid characters produce an Error token with correct source position.",
      "EOF token is emitted as the final element of the token stream.",
      "Position captured for tokens points to the first character of the lexeme, not the trailing whitespace."
    ]
  },
  {
    "module_id": "tokenizer-m2",
    "criteria": [
      "Implement _match helper for conditional consumption without position tracking drift",
      "Implement _peek_next for 2-character lookahead in float scanning",
      "Apply maximal munch logic to ==, !=, <=, >= pairs",
      "Implement scan-then-lookup for identifiers vs keywords",
      "Verify float scanning correctly ignores trailing dots (3. is integer 3)",
      "Ensure keyword check only happens on complete identifier lexemes (no prefix bugs)",
      "Maintain exact 1-indexed column/line positions through multi-character tokens"
    ]
  },
  {
    "module_id": "tokenizer-m3",
    "criteria": [
      "Recognizes single-line comments // up to newline or EOF",
      "Recognizes multi-line comments /* */ with correct newline tracking",
      "Emits SLASH token for lone / characters",
      "Handles string literals with double quotes and escape sequences",
      "Correctly handles VALID_ESCAPES: \\n, \\t, \\r, \\\", \\\\",
      "Produces ERROR for unterminated strings at the opening quote position",
      "Produces ERROR for unterminated block comments at the /* position",
      "Maintains O(n) performance for strings using list accumulation",
      "Correctly ignores comment delimiters within string literals"
    ]
  },
  {
    "module_id": "tokenizer-m4",
    "criteria": [
      "Token type enumeration defines all categories: Number, String, Identifier, Keyword, Operator, Punctuation, EOF, Error",
      "Token struct stores: type, lexeme (raw text), line number, and column number",
      "Scanner has advance() consuming and returning current character, peek() inspecting next character without consuming",
      "Two-character operators: ==, !=, <=, >= are recognized as single tokens",
      "Maximal munch: '==' is emitted as Equals-Equals, never as Assign + Assign",
      "Integer and Float literals correctly scanned",
      "Keywords identified by lookup table after scanning identifier",
      "String literals with escape sequences handled",
      "Single and Multi-line comments filtered out",
      "Error recovery: after encountering an invalid character, scanning continues from the next character",
      "Integration test with complete multi-line program passes",
      "Performance: tokenizing a 10,000-line input completes in under 1 second"
    ]
  }
]