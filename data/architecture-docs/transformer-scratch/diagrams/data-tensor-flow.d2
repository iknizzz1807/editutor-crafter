classes: {
  stage: {
    style.fill: "#1a1a2e"
    style.stroke: "#3fb950"
    style.font-color: "#e6edf3"
    style.bold: true
  }
  tensor: {
    style.fill: "#16213e"
    style.stroke: "#8b949e"
    style.font-color: "#e6edf3"
  }
  attention: {
    style.fill: "#0f3460"
    style.stroke: "#3fb950"
    style.font-color: "#e6edf3"
  }
}

title: "Data and Tensor Shape Flow" {
  near: top-center
  style.font-size: 18
  style.bold: true
  style.font-color: "#e6edf3"
}

input_tokens: "Input Tokens\n[batch_size, seq_len]" {
  class: stage
  shape: rectangle
}

token_embeddings: "Token Embeddings\n[batch_size, seq_len, d_model]" {
  class: tensor
}

positional_embeddings: "Positional Embeddings\n[batch_size, seq_len, d_model]" {
  class: tensor
}

embedded_input: "Embedded Input\n[batch_size, seq_len, d_model]" {
  class: stage
}

attention_block: "Multi-Head Attention" {
  class: attention
  
  qkv_projection: "Q, K, V Projections\n[batch_size, seq_len, d_model]" {
    class: tensor
  }
  
  reshaped_heads: "Reshaped for Heads\n[batch_size, num_heads, seq_len, d_head]" {
    class: tensor
  }
  
  attention_scores: "Attention Scores\n[batch_size, num_heads, seq_len, seq_len]" {
    class: tensor
  }
  
  attention_output: "Attention Output\n[batch_size, num_heads, seq_len, d_head]" {
    class: tensor
  }
  
  concatenated: "Concatenated\n[batch_size, seq_len, d_model]" {
    class: tensor
  }
}

ffn_block: "Feed Forward Network" {
  class: attention
  
  ffn_intermediate: "FFN Intermediate\n[batch_size, seq_len, d_ff]" {
    class: tensor
  }
  
  ffn_output: "FFN Output\n[batch_size, seq_len, d_model]" {
    class: tensor
  }
}

layer_output: "Layer Output\n[batch_size, seq_len, d_model]" {
  class: stage
}

final_logits: "Final Logits\n[batch_size, seq_len, vocab_size]" {
  class: stage
}

input_tokens -> token_embeddings: embed
input_tokens -> positional_embeddings: "encode position"
token_embeddings -> embedded_input: add
positional_embeddings -> embedded_input: add

embedded_input -> attention_block.qkv_projection: project
attention_block.qkv_projection -> attention_block.reshaped_heads: reshape
attention_block.reshaped_heads -> attention_block.attention_scores: "compute scores"
attention_block.attention_scores -> attention_block.attention_output: "apply attention"
attention_block.attention_output -> attention_block.concatenated: "concat heads"

attention_block.concatenated -> ffn_block.ffn_intermediate: expand
ffn_block.ffn_intermediate -> ffn_block.ffn_output: "project back"

ffn_block.ffn_output -> layer_output: "residual + norm"
layer_output -> final_logits: "output projection"

dimensions_note: |md
  **Key Dimensions:**
  - batch_size: Number of sequences
  - seq_len: Sequence length
  - d_model: Model dimension (512/768)
  - num_heads: Attention heads (8/12)
  - d_head: d_model / num_heads
  - d_ff: FFN dimension (2048/3072)
  - vocab_size: Vocabulary size
| {
  shape: page
  near: bottom-right
  style.fill: "#1a1a2e"
  style.stroke: "#3fb950"
  style.font-color: "#e6edf3"
}