vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    forward: "#4A90D9"
    backward: "#D94A4A"
    qkv: "#7B68EE"
    softmax: "#50C878"
    output: "#FFB347"
    grad_v: "#E74C3C"
    grad_qk: "#9B59B6"
  }
}

title: |md
  # Gradient Flow Through Attention
  ## Backward Pass: dL/dOutput -> dL/dQ, dL/dK, dL/dV
| {near: top-center}

direction: right

Forward_Pass: {
  label: Forward Pass (for reference)
  style.fill: "#E8F4FD"
  style.stroke: ${colors.forward}
  style.stroke-width: 2
  
  X: {
    label: "Input X\n[batch, seq, d_model]"
    style.fill: white
  }
  
  Q_proj: {
    label: "Q = X @ W_Q"
    style.fill: ${colors.qkv}
    style.font-color: white
  }
  K_proj: {
    label: "K = X @ W_K"
    style.fill: ${colors.qkv}
    style.font-color: white
  }
  V_proj: {
    label: "V = X @ W_V"
    style.fill: ${colors.qkv}
    style.font-color: white
  }
  
  Scores: {
    label: "Scores\nQ @ K^T / sqrt(d_k)\n[batch, seq, seq]"
    style.fill: ${colors.softmax}
  }
  
  Weights: {
    label: "Attn Weights\nsoftmax(Scores)\n[batch, seq, seq]"
    style.fill: ${colors.softmax}
  }
  
  Output: {
    label: "Output\nWeights @ V\n[batch, seq, d_v]"
    style.fill: ${colors.output}
  }
  
  Loss: {
    label: Loss L
    shape: circle
    style.fill: ${colors.backward}
    style.font-color: white
  }
  
  X -> Q_proj
  X -> K_proj
  X -> V_proj
  
  Q_proj -> Scores
  K_proj -> Scores
  
  Scores -> Weights
  Weights -> Output
  V_proj -> Output
  Output -> Loss
}

Backward_Pass: {
  label: Backward Pass (Gradient Computation)
  style.fill: "#FDE8E8"
  style.stroke: ${colors.backward}
  style.stroke-width: 2
  
  grad_output: {
    label: "dL/dOutput\n[batch, seq, d_v]"
    style.fill: ${colors.backward}
    style.font-color: white
  }
  
  grad_V: {
    label: |md
      **dL/dV**
      
      = Weights^T @ dL/dOutput
      
      [batch, seq, d_v]
    |
    style.fill: ${colors.grad_v}
    style.font-color: white
  }
  
  grad_Weights: {
    label: |md
      **dL/dWeights**
      
      = dL/dOutput @ V^T
      
      [batch, seq, seq]
    |
    style.fill: ${colors.grad_v}
    style.font-color: white
  }
  
  grad_Scores: {
    label: |md
      **dL/dScores**
      
      = dL/dWeights ⊙ 
        (Weights - Weights^2)
      
      (softmax derivative)
    |
    style.fill: ${colors.grad_qk}
    style.font-color: white
  }
  
  grad_Q: {
    label: |md
      **dL/dQ**
      
      = dL/dScores @ K / sqrt(d_k)
      
      [batch, seq, d_k]
    |
    style.fill: ${colors.grad_qk}
    style.font-color: white
  }
  
  grad_K: {
    label: |md
      **dL/dK**
      
      = dL/dScores^T @ Q / sqrt(d_k)
      
      [batch, seq, d_k]
    |
    style.fill: ${colors.grad_qk}
    style.font-color: white
  }
  
  grad_output -> grad_V: "V gradient\n(direct path)"
  grad_output -> grad_Weights: "Weight gradient"
  grad_Weights -> grad_Scores: "Through\nsoftmax"
  grad_Scores -> grad_Q: "Q gradient"
  grad_Scores -> grad_K: "K gradient"
}

Key_Insight: {
  style.fill: "#FFF9E6"
  style.stroke: "#B8860B"
  
  insight1: {
    label: |md
      ### Key Insight 1: V Gradient is Weighted
      
      If position 5 attends 80% to position 2's value,
      then **80% of position 5's error** flows to
      position 2's value embedding.
      
      
      dL/dV[2] += 0.8 x dL/dOutput[5]
      
    |
  }
  
  insight2: {
    label: |md
      ### Key Insight 2: Q/K Gradients Flow Through Attention
      
      Q and K gradients depend on how much they
      **affected the attention distribution**.
      
      Positions that were **strongly attended to**
      receive larger gradient signals.
    |
  }
  
  insight3: {
    label: |md
      ### Key Insight 3: No Vanishing Through Time
      
      Unlike RNNs where gradients flow through
      t -> t-1 -> t-2 -> ...,
      
      Attention provides **direct gradient paths**
      from any position to any other position
      in a single backward pass.
    |
  }
}

Gradient_Flow_Example: {
  label: Concrete Example: 4-Token Sequence
  style.fill: "#F5F5F5"
  
  positions: {
    label: Attention Weights for Position 2
    style.fill: white
    
    pos0: {
      label: "Pos 0\nw=0.05"
      style.fill: "#E8E8E8"
    }
    pos1: {
      label: "Pos 1\nw=0.15"
      style.fill: "#D0D0D0"
    }
    pos2: {
      label: "Pos 2\nw=0.10"
      style.fill: "#B8B8B8"
    }
    pos3: {
      label: "Pos 3\nw=0.70"
      style.fill: ${colors.backward}
      style.font-color: white
    }
  }
  
  flow_diagram: {
    label: ""
    
    error_source: {
      label: "dL/dOutput[2]\n= 1.0 (gradient)"
      shape: circle
      style.fill: ${colors.backward}
      style.font-color: white
    }
    
    v0: {label: "dL/dV[0]\n= 0.05"}
    v1: {label: "dL/dV[1]\n= 0.15"}
    v2: {label: "dL/dV[2]\n= 0.10"}
    v3: {
      label: "dL/dV[3]\n= 0.70"
      style.fill: ${colors.backward}
      style.font-color: white
    }
    
    error_source -> v0: "5%" {style.stroke-width: 1; style.stroke: "#CCCCCC"}
    error_source -> v1: "15%" {style.stroke-width: 2; style.stroke: "#999999"}
    error_source -> v2: "10%" {style.stroke-width: 2; style.stroke: "#666666"}
    error_source -> v3: "70%" {style.stroke-width: 5; style.stroke: ${colors.backward}; style.animated: true}
  }
  
  positions -> flow_diagram: "Gradient flows to V" {style.stroke-dash: 3}
}

Why_This_Matters: {
  label: Why This Matters for Training
  style.fill: "#E8F5E9"
  style.stroke: "#4CAF50"
  
  benefit1: {
    label: |md
      ### Direct Gradient Highways
      
      Error signals flow directly from output
      to relevant input positions—no degradation
      through long chains of transformations.
    |
  }
  
  benefit2: {
    label: |md
      ### Content-Aware Learning
      
      The model learns to attend to positions
      that actually help the task, and those
      positions receive stronger updates.
    |
  }
  
  benefit3: {
    label: |md
      ### Parallel Gradient Computation
      
      All positions' gradients computed
      simultaneously via matrix operations—
      fully parallelizable on GPU.
    |
  }
}

Math_Summary: {
  label: Complete Gradient Equations
  style.fill: "#F0F0F0"
  
  equations: {
    label: |latex
      \begin{aligned}
      \frac{\partial L}{\partial V} &= A^T \cdot \frac{\partial L}{\partial O} \\[10pt]
      \frac{\partial L}{\partial A} &= \frac{\partial L}{\partial O} \cdot V^T \\[10pt]
      \frac{\partial L}{\partial S} &= A \odot \left(\frac{\partial L}{\partial A} - \left(A \cdot \frac{\partial L}{\partial A}\right)_{\text{row-sum}}\right) \\[10pt]
      \frac{\partial L}{\partial Q} &= \frac{\partial L}{\partial S} \cdot K \cdot \frac{1}{\sqrt{d_k}} \\[10pt]
      \frac{\partial L}{\partial K} &= \frac{\partial L}{\partial S}^T \cdot Q \cdot \frac{1}{\sqrt{d_k}}
      \end{aligned}
    |
  }
  
  legend: {
    label: |md
      Where:
      - **A** = attention weights (after softmax)
      - **S** = attention scores (before softmax)
      - **O** = output
      - **⊙** = element-wise multiplication
    |
    style.fill: white
  }
}

Forward_Pass -> Backward_Pass: "loss.backward()" {
  style.stroke: ${colors.backward}
  style.stroke-width: 3
  style.stroke-dash: 5
  style.animated: true
}

Backward_Pass -> Key_Insight
Key_Insight -> Gradient_Flow_Example
Gradient_Flow_Example -> Why_This_Matters
Why_This_Matters -> Math_Summary