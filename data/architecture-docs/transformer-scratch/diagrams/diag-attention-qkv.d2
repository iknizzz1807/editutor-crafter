vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    query: "#4A90D9"
    key: "#7B68EE"
    value: "#50C878"
    input: "#F5F5F5"
    header: "#2C3E50"
    flow: "#6C7A89"
  }
}

title: |md
  # Query-Key-Value Decomposition
  ## The Learnable Projections Behind Attention
| {near: top-center}

direction: right

input_matrix: {
  label: "Input X\n[batch, seq_len, d_model]"
  style: {
    fill: ${colors.input}
    stroke: "#34495E"
    stroke-width: 2
    font: mono
    font-size: 20
    bold: true
  }
}

projections_container: {
  label: "Learned Linear Projections\n(Only Trainable Parameters in Attention)"
  style: {
    fill: "#FAFAFA"
    stroke: "#BDC3C7"
    stroke-dash: 3
    border-radius: 8
  }
  
  W_Q: {
    label: "W_Q"
    style: {
      fill: ${colors.query}
      stroke: "#2980B9"
      font: mono
      font-color: white
      bold: true
      font-size: 18
    }
    tooltip: |md
      Query projection matrix
      Shape: [d_model, d_k]
      Learns: "What to search for"
    |
  }
  
  W_K: {
    label: "W_K"
    style: {
      fill: ${colors.key}
      stroke: "#5B4FCF"
      font: mono
      font-color: white
      bold: true
      font-size: 18
    }
    tooltip: |md
      Key projection matrix
      Shape: [d_model, d_k]
      Learns: "What to be matched against"
    |
  }
  
  W_V: {
    label: "W_V"
    style: {
      fill: ${colors.value}
      stroke: "#27AE60"
      font: mono
      font-color: white
      bold: true
      font-size: 18
    }
    tooltip: |md
      Value projection matrix
      Shape: [d_model, d_v]
      Learns: "What information to return"
    |
  }
}

output_matrices: {
  label: "Projected Representations"
  style: {
    fill: "#FAFAFA"
    stroke: "#BDC3C7"
    stroke-dash: 3
    border-radius: 8
  }
  
  Q: {
    label: "Q\n(Query)\n[batch, seq_len, d_k]"
    style: {
      fill: ${colors.query}
      stroke: "#2980B9"
      font-color: white
      bold: true
      font-size: 16
      border-radius: 6
    }
  }
  
  K: {
    label: "K\n(Key)\n[batch, seq_len, d_k]"
    style: {
      fill: ${colors.key}
      stroke: "#5B4FCF"
      font-color: white
      bold: true
      font-size: 16
      border-radius: 6
    }
  }
  
  V: {
    label: "V\n(Value)\n[batch, seq_len, d_v]"
    style: {
      fill: ${colors.value}
      stroke: "#27AE60"
      font-color: white
      bold: true
      font-size: 16
      border-radius: 6
    }
  }
}

database_analogy: {
  label: "Database Retrieval Analogy"
  style: {
    fill: "#FFF9E6"
    stroke: "#F1C40F"
    stroke-width: 2
    border-radius: 8
  }
  
  db_query: {
    label: |md
      **Query (Q)**
      
      SELECT * FROM table
      WHERE column LIKE '%search_term%'
      
      → "What am I looking for?"
    |
    style.fill: ${colors.query}
    style.font-color: white
    style.opacity: 0.9
  }
  
  db_key: {
    label: |md
      **Key (K)**
      
      CREATE INDEX idx ON table(column)
      
      → "What can be matched against?"
    |
    style.fill: ${colors.key}
    style.font-color: white
    style.opacity: 0.9
  }
  
  db_value: {
    label: |md
      **Value (V)**
      
      SELECT data, info, content
      FROM table
      
      → "What data to retrieve?"
    |
    style.fill: ${colors.value}
    style.font-color: white
    style.opacity: 0.9
  }
  
  db_query -> db_key: "Match\n(similarity)" {
    style: {
      stroke: "#F39C12"
      stroke-width: 2
      stroke-dash: 3
    }
  }
  db_key -> db_value: "Retrieve\n(weighted)" {
    style: {
      stroke: "#F39C12"
      stroke-width: 2
      stroke-dash: 3
    }
  }
}

attention_output: {
  label: "Attention Output\n[batch, seq_len, d_v]"
  style: {
    fill: "#E8F6F3"
    stroke: "#1ABC9C"
    stroke-width: 3
    font: mono
    bold: true
    font-size: 16
    border-radius: 8
    shadow: true
  }
}

formula_box: |md
  ## The Attention Formula
  
  $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
  
  **Key insight**: Only Q, K, V projections contain learnable parameters.
  The attention computation itself is **pure linear algebra**—no parameters!
|
formula_box.near: bottom-center
formula_box.style: {
  fill: "#F8F9FA"
  stroke: "#DEE2E6"
  border-radius: 8
}

legend: {
  near: top-right
  style: {
    fill: "#F8F9FA"
    stroke: "#DEE2E6"
    border-radius: 6
  }
  
  title: |md
    **Dimension Legend**
  |
  
  item1: |md
    - `batch`: Batch size (parallel sequences)
    - `seq_len`: Sequence length (tokens)
    - `d_model`: Model dimension (e.g., 512)
    - `d_k`: Key/Query dimension (typically d_model/h)
    - `d_v`: Value dimension (typically d_model/h)
  |
}

input_matrix -> projections_container.W_Q: "x @ W_Q\n[b,s,d] @ [d,d_k]\n→ [b,s,d_k]" {
  style: {
    stroke: ${colors.query}
    stroke-width: 2
    font-size: 11
  }
}

input_matrix -> projections_container.W_K: "x @ W_K" {
  style: {
    stroke: ${colors.key}
    stroke-width: 2
    font-size: 11
  }
}

input_matrix -> projections_container.W_V: "x @ W_V" {
  style: {
    stroke: ${colors.value}
    stroke-width: 2
    font-size: 11
  }
}

projections_container.W_Q -> output_matrices.Q: {
  style: {
    stroke: ${colors.query}
    stroke-width: 3
    animated: true
  }
}

projections_container.W_K -> output_matrices.K: {
  style: {
    stroke: ${colors.key}
    stroke-width: 3
    animated: true
  }
}

projections_container.W_V -> output_matrices.V: {
  style: {
    stroke: ${colors.value}
    stroke-width: 3
    animated: true
  }
}

output_matrices.Q -> attention_output: "QK^T/√d_k → softmax → @V" {
  style: {
    stroke: ${colors.query}
    stroke-width: 2
  }
}

output_matrices.K -> attention_output: {
  style: {
    stroke: ${colors.key}
    stroke-width: 2
  }
}

output_matrices.V -> attention_output: {
  style: {
    stroke: ${colors.value}
    stroke-width: 2
  }
}

back_link: {
  label: "← Back to Satellite Map"
  link: "#transformer-architecture-map"
  near: top-left
  style: {
    fill: transparent
    stroke: transparent
    font-color: "#3498DB"
    underline: true
  }
}