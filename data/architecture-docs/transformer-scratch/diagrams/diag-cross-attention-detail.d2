vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    encoder_blue: "#3B82F6"
    decoder_green: "#10B981"
    query_purple: "#8B5CF6"
    attention_orange: "#F59E0B"
    output_teal: "#14B8A6"
    header_bg: "#1E293B"
    data_flow: "#60A5FA"
  }
}

title: |md
  # Cross-Attention Mechanics
  ### Decoder Queries Encoder Representations
| {near: top-center}

direction: right

# === ENCODER OUTPUT (Source of K, V) ===
encoder_output: {
  label: "Encoder Final Output"
  style: {
    fill: "${colors.encoder_blue}"
    stroke: "#1E40AF"
    stroke-width: 3
    font-color: white
    shadow: true
  }
  
  memory_grid: {
    grid-columns: 4
    grid-gap: 4
    
    mem_0: {
      label: "pos 0"
      shape: rectangle
      style.fill: "#93C5FD"
      style.stroke: "${colors.encoder_blue}"
    }
    mem_1: {
      label: "pos 1"
      shape: rectangle
      style.fill: "#93C5FD"
      style.stroke: "${colors.encoder_blue}"
    }
    mem_2: {
      label: "..."
      shape: rectangle
      style.fill: "#DBEAFE"
      style.stroke: "${colors.encoder_blue}"
    }
    mem_n: {
      label: "pos n"
      shape: rectangle
      style.fill: "#93C5FD"
      style.stroke: "${colors.encoder_blue}"
    }
  }
  
  shape_note: ||md
    **Shape:** `[batch, src_len, d_model]`
    Contains encoded source understanding
  ||
  shape_note.style.fill: transparent
  shape_note.style.font-size: 12
}

# === DECODER STATE (Source of Q) ===
decoder_state: {
  label: "Decoder Current State"
  style: {
    fill: "${colors.decoder_green}"
    stroke: "#047857"
    stroke-width: 3
    font-color: white
    shadow: true
  }
  
  state_grid: {
    grid-columns: 3
    grid-gap: 4
    
    dec_0: {
      label: "pos 0"
      shape: rectangle
      style.fill: "#6EE7B7"
      style.stroke: "${colors.decoder_green}"
    }
    dec_1: {
      label: "pos 1"
      shape: rectangle
      style.fill: "#6EE7B7"
      style.stroke: "${colors.decoder_green}"
    }
    dec_t: {
      label: "pos t"
      shape: rectangle
      style.fill: "#6EE7B7"
      style.stroke: "${colors.decoder_green}"
    }
  }
  
  dec_note: ||md
    **Shape:** `[batch, tgt_len, d_model]`
    Current generation context
  ||
  dec_note.style.fill: transparent
  dec_note.style.font-size: 12
}

# === PROJECTION LAYER ===
projections: {
  label: "Learned Projections"
  style: {
    fill: "#1E293B"
    stroke: "#475569"
    stroke-width: 2
    font-color: white
  }
  
  W_Q: {
    label: "W_Q\n(Decoder)"
    shape: hexagon
    style: {
      fill: "${colors.query_purple}"
      font-color: white
      bold: true
    }
  }
  
  W_K: {
    label: "W_K\n(Encoder)"
    shape: hexagon
    style: {
      fill: "${colors.encoder_blue}"
      font-color: white
      bold: true
    }
  }
  
  W_V: {
    label: "W_V\n(Encoder)"
    shape: hexagon
    style: {
      fill: "${colors.encoder_blue}"
      font-color: white
      bold: true
    }
  }
}

# === Q, K, V TENSORS ===
qkv_tensors: {
  label: "Projected Tensors"
  style: {
    fill: "#0F172A"
    stroke: "#334155"
    font-color: white
  }
  
  Q_tensor: {
    label: "Q\nQuery"
    shape: cylinder
    style: {
      fill: "${colors.query_purple}"
      font-color: white
      bold: true
      stroke: "#6D28D9"
    }
    tooltip: "What I'm looking for\n[from decoder]"
  }
  
  K_tensor: {
    label: "K\nKey"
    shape: cylinder
    style: {
      fill: "${colors.encoder_blue}"
      font-color: white
      bold: true
      stroke: "#1E40AF"
    }
    tooltip: "What to match against\n[from encoder]"
  }
  
  V_tensor: {
    label: "V\nValue"
    shape: cylinder
    style: {
      fill: "${colors.encoder_blue}"
      font-color: white
      bold: true
      stroke: "#1E40AF"
    }
    tooltip: "What to retrieve\n[from encoder]"
  }
  
  shapes_label: ||md
    **Q:** `[batch, tgt_len, d_k]`
    **K:** `[batch, src_len, d_k]`
    **V:** `[batch, src_len, d_v]`
  ||
  shapes_label.style.fill: transparent
  shapes_label.style.font-size: 11
}

# === ATTENTION COMPUTATION ===
attention_compute: {
  label: "Cross-Attention Computation"
  style: {
    fill: "#451A03"
    stroke: "${colors.attention_orange}"
    stroke-width: 3
    font-color: white
  }
  
  step1: {
    label: "Step 1: Similarity"
    shape: rectangle
    style: {
      fill: "#78350F"
      font-color: white
    }
    
    formula1: ||md
      `scores = Q @ K^T`
      `scores /= sqrt(d_k)`
      
      **Result:** `[batch, tgt_len, src_len]`
    ||
    formula1.style.fill: transparent
    formula1.style.font-size: 12
  }
  
  step2: {
    label: "Step 2: Weights"
    shape: rectangle
    style: {
      fill: "#78350F"
      font-color: white
    }
    
    formula2: ||md
      `attn = softmax(scores, dim=-1)`
      
      Each query position gets
      distribution over source
    ||
    formula2.style.fill: transparent
    formula2.style.font-size: 12
  }
  
  step3: {
    label: "Step 3: Retrieve"
    shape: rectangle
    style: {
      fill: "#78350F"
      font-color: white
    }
    
    formula3: ||md
      `output = attn @ V`
      
      **Result:** `[batch, tgt_len, d_v]`
    ||
    formula3.style.fill: transparent
    formula3.style.font-size: 12
  }
  
  step1 -> step2 -> step3: {
    style: {
      stroke: "${colors.attention_orange}"
      stroke-width: 2
      animated: true
    }
  }
}

# === ATTENTION WEIGHT VISUALIZATION ===
attention_viz: {
  label: "Attention Weight Matrix [tgt_len × src_len]"
  style: {
    fill: "#0F172A"
    stroke: "#475569"
    font-color: white
  }
  
  grid-columns: 5
  grid-gap: 2
  
  h0: {
    label: "src→"
    shape: text
    style.font-size: 10
    style.font-color: "#94A3B8"
  }
  h1: {
    label: "0"
    shape: text
    style.font-size: 10
    style.font-color: "${colors.encoder_blue}"
  }
  h2: {
    label: "1"
    shape: text
    style.font-size: 10
    style.font-color: "${colors.encoder_blue}"
  }
  h3: {
    label: "..."
    shape: text
    style.font-size: 10
  }
  h4: {
    label: "n"
    shape: text
    style.font-size: 10
    style.font-color: "${colors.encoder_blue}"
  }
  
  r0: {
    label: "t=0"
    shape: text
    style.font-size: 10
    style.font-color: "${colors.query_purple}"
  }
  r1: {
    label: "t=1"
    shape: text
    style.font-size: 10
    style.font-color: "${colors.query_purple}"
  }
  r2: {
    label: "t=2"
    shape: text
    style.font-size: 10
    style.font-color: "${colors.query_purple}"
  }
  
  c00: {
    label: ""
    shape: square
    style.fill: "#FCD34D"
    tooltip: "alpha=0.72 (high attention)"
  }
  c01: {
    label: ""
    shape: square
    style.fill: "#FEF3C7"
    tooltip: "alpha=0.18"
  }
  c02: {
    label: ""
    shape: square
    style.fill: "#FFFBEB"
    tooltip: "alpha=0.05"
  }
  c03: {
    label: ""
    shape: square
    style.fill: "#FFFBEB"
    tooltip: "alpha=0.05"
  }
  
  c10: {
    label: ""
    shape: square
    style.fill: "#FEF3C7"
    tooltip: "alpha=0.15"
  }
  c11: {
    label: ""
    shape: square
    style.fill: "#FCD34D"
    tooltip: "alpha=0.68 (high attention)"
  }
  c12: {
    label: ""
    shape: square
    style.fill: "#FEF3C7"
    tooltip: "alpha=0.12"
  }
  c13: {
    label: ""
    shape: square
    style.fill: "#FFFBEB"
    tooltip: "alpha=0.05"
  }
  
  c20: {
    label: ""
    shape: square
    style.fill: "#FFFBEB"
    tooltip: "alpha=0.03"
  }
  c21: {
    label: ""
    shape: square
    style.fill: "#FEF3C7"
    tooltip: "alpha=0.20"
  }
  c22: {
    label: ""
    shape: square
    style.fill: "#FCD34D"
    tooltip: "alpha=0.61 (high attention)"
  }
  c23: {
    label: ""
    shape: square
    style.fill: "#FEF3C7"
    tooltip: "alpha=0.16"
  }
}

# === OUTPUT ===
cross_attn_output: {
  label: "Cross-Attention Output"
  style: {
    fill: "${colors.output_teal}"
    stroke: "#0D9488"
    stroke-width: 3
    font-color: white
    shadow: true
  }
  
  output_desc: ||md
    **Shape:** `[batch, tgt_len, d_model]`
    
    Each decoder position now has:
    - Self-attention context (from decoder)
    - **Source context (from encoder)**
    
    Combined via Add and Norm
  ||
  output_desc.style.fill: transparent
  output_desc.style.font-size: 12
}

# === CONNECTIONS ===

encoder_output -> projections.W_K: "K source" {
  style: {
    stroke: "${colors.encoder_blue}"
    stroke-width: 3
    animated: true
  }
}

encoder_output -> projections.W_V: "V source" {
  style: {
    stroke: "${colors.encoder_blue}"
    stroke-width: 3
    animated: true
  }
}

decoder_state -> projections.W_Q: "Q source" {
  style: {
    stroke: "${colors.decoder_green}"
    stroke-width: 3
    animated: true
  }
}

projections.W_Q -> qkv_tensors.Q_tensor: {
  style.stroke: "${colors.query_purple}"
  style.stroke-width: 2
}

projections.W_K -> qkv_tensors.K_tensor: {
  style.stroke: "${colors.encoder_blue}"
  style.stroke-width: 2
}

projections.W_V -> qkv_tensors.V_tensor: {
  style.stroke: "${colors.encoder_blue}"
  style.stroke-width: 2
}

qkv_tensors.Q_tensor -> attention_compute: "Query" {
  style.stroke: "${colors.query_purple}"
  style.stroke-width: 2
}

qkv_tensors.K_tensor -> attention_compute: "Key" {
  style.stroke: "${colors.encoder_blue}"
  style.stroke-width: 2
}

qkv_tensors.V_tensor -> attention_compute: "Value" {
  style.stroke: "${colors.encoder_blue}"
  style.stroke-width: 2
}

attention_compute -> cross_attn_output: {
  style: {
    stroke: "${colors.attention_orange}"
    stroke-width: 3
    animated: true
  }
}

attention_compute.step2 -> attention_viz: "weights" {
  style: {
    stroke: "${colors.attention_orange}"
    stroke-dash: 3
  }
}

# === KEY INSIGHT BOX ===
insight: {
  label: "Key Insight"
  style: {
    fill: "#1E1B4B"
    stroke: "#6366F1"
    stroke-width: 2
    font-color: white
    border-radius: 8
  }
  near: bottom-center
  
  insight_text: ||md
    ## Cross-Attention vs Self-Attention

    | Component | Self-Attention | Cross-Attention |
    |-----------|----------------|-----------------|
    | **Q from** | Same layer | **Decoder** |
    | **K from** | Same layer | **Encoder** |
    | **V from** | Same layer | **Encoder** |

    The decoder **queries** the encoder's understanding.
    This is how translation/conditioning happens:

    "Given what I've generated so far (Q),
    what's relevant in the source (K,V)?"
  ||
  insight_text.style.fill: transparent
  insight_text.style.font-size: 13
}

# === DATA FLOW ANNOTATION ===
flow_annotation: {
  style: {
    fill: transparent
    stroke: "transparent"
  }
  near: top-right
  
  legend: ||md
    ### Color Legend
    - Blue: Encoder data (K, V source)
    - Green: Decoder data (Q source)
    - Purple: Query tensor
    - Orange: Attention computation
    - Teal: Output
  ||
  legend.style.fill: "#1E293B"
  legend.style.font-color: white
  legend.style.border-radius: 4
}

# Back link
back_to_map: {
  label: "Back to Architecture Map"
  link: "#transformer-architecture"
  shape: rectangle
  style: {
    fill: "#374151"
    font-color: white
    stroke: "#6B7280"
  }
  near: top-left
}