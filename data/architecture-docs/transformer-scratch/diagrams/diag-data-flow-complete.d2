vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    input: "#E8F5E9"
    embed: "#C8E6C9"
    encoder: "#A5D6A7"
    decoder: "#81C784"
    output: "#66BB6A"
    loss: "#43A047"
    highlight: "#FF6B6B"
    dataflow: "#2196F3"
    metadata: "#9C27B0"
    padding: "#E0E0E0"
  }
}

title: |md
  # End-to-End Data Flow: Training Example Trace
  **Input**: `[8, 12, 3]` (tokenized sequence) | **Batch**: 2, **Seq**: 3, **d_model**: 64, **vocab**: 1000
| {
  near: top-center
}

direction: right

input_stage: Input Tokens {
  style.fill: ${colors.input}
  link: "#input-tokens"
  
  token_tensor: |md
    **Tensor Shape**: `[batch=2, seq_len=3]`
    
    
    [[8, 12,  3],
     [8, 12,  3]]
    
    
    *einops: `b s`*
  |
  
  byte_offset: |md
    **Memory Layout** (row-major):
    
    | Offset | Value |
    |--------|-------|
    | 0x00   | 8     |
    | 0x04   | 12    |
    | 0x08   | 3     |
    | 0x0C   | 8     |
    | 0x10   | 12    |
    | 0x14   | 3     |
  |
}

input_stage.token_tensor -> embed_stage.lookup: "indices lookup" {
  style.stroke: ${colors.dataflow}
  style.stroke-width: 3
}

embed_stage: Embedding Layer {
  style.fill: ${colors.embed}
  link: "#embedding-layer"
  
  lookup: Embedding Lookup {
    style.fill: "#B9F6CA"
    
    embedding_table: |md
      **Weight Matrix**: `[vocab=1000, d_model=64]`
      
      Lookup for token 8:
      `row_8 = W[8, :]  # [64]`
      
      *One 64-dim vector per token*
    |
  }
  
  pos_encoding: Positional Encoding {
    style.fill: "#B9F6CA"
    
    pos_formula: |md
      **Sinusoidal Encoding**:
      
      `PE[pos, 2i]   = sin(pos/10000^(2i/d))`
      `PE[pos, 2i+1] = cos(pos/10000^(2i/d))`
      
      Position 0: `[0, 0, 0, ...]`
      Position 1: `[sin(0), cos(0), ...]`
      Position 2: `[sin(0), cos(0), ...]`
    |
  }
  
  embedded: |md
    **After Token + Position Embedding**:
    
    Shape: `[batch=2, seq_len=3, d_model=64]`
    
    `x[b, s, d] = token_emb[token[b,s]] + PE[s, d]`
    
    *einops: `b s d_model`*
  |
  
  lookup -> embedded
  pos_encoding -> embedded
}

embed_stage.embedded -> encoder_stack.input: "embedded sequence" {
  style.stroke: ${colors.dataflow}
  style.stroke-width: 3
}

encoder_stack: Encoder Stack (xN layers) {
  style.fill: ${colors.encoder}
  link: "#encoder-stack"
  
  input: |md
    **Input to Encoder**
    Shape: `[2, 3, 64]`
    *einops: `b s d_model`*
  |
  
  layer_0: Encoder Layer 0 {
    style.fill: "#C8E6C9"
    
    self_attn_0: Self-Attention {
      style.fill: "#DCEDC8"
      
      qkv_0: |md
        **Q, K, V Projections**:
        
        `Q = x @ W_Q  # [2, 3, 64]`
        `K = x @ W_K  # [2, 3, 64]`
        `V = x @ W_V  # [2, 3, 64]`
        
        *einops: each `b s d_k`*
      |
      
      attn_scores_0: |md
        **Attention Scores**:
        
        `scores = Q @ K^T / sqrt(64)  # [2, 3, 3]`
        
        | Q\K  | pos0 | pos1 | pos2 |
        |------|------|------|------|
        | pos0 | 0.92 | 0.05 | 0.03 |
        | pos1 | 0.10 | 0.85 | 0.05 |
        | pos2 | 0.08 | 0.12 | 0.80 |
      |
      
      attn_output_0: |md
        **Attention Output**:
        
        `out = softmax(scores) @ V  # [2, 3, 64]`
      |
    }
    
    add_norm_0: Add and Norm {
      style.fill: "#DCEDC8"
      
      formula_0: |md
        **Residual + LayerNorm**:
        
        `x = LayerNorm(x + attention(x))  # [2, 3, 64]`
      |
    }
    
    ffn_0: Feed-Forward {
      style.fill: "#DCEDC8"
      
      ffn_formula_0: |md
        **FFN(x) = ReLU(xW1 + b1)W2 + b2**:
        
        `hidden = ReLU(x @ W1)  # [2, 3, 256]`
        `out = hidden @ W2      # [2, 3, 64]`
        
        *Expansion: 64 -> 256 -> 64*
      |
    }
    
    add_norm_0b: Add and Norm {
      style.fill: "#DCEDC8"
      label: "LayerNorm(x + ffn)"
    }
    
    self_attn_0 -> add_norm_0 -> ffn_0 -> add_norm_0b
  }
  
  layer_n: Encoder Layer N-1 {
    style.fill: "#C8E6C9"
    style.stroke-dash: 3
    style.multiple: true
    label: "... (N-1 more layers)"
  }
  
  encoder_output: |md
    **Encoder Output (Memory)**:
    Shape: `[2, 3, 64]`
    
    Used by decoder for cross-attention
    *einops: `b src_len d_model`*
  |
  
  input -> layer_0 -> layer_n -> encoder_output
}

encoder_stack.encoder_output -> decoder_stack.decoder_input: "encoder memory (K, V)" {
  style.stroke: ${colors.metadata}
  style.stroke-width: 2
  style.stroke-dash: 5
}

target_stage: Target Tokens {
  style.fill: ${colors.input}
  
  target_input: |md
    **Decoder Input** (shifted right):
    
    
    [[<SOS>, 8, 12],
     [<SOS>, 8, 12]]
    
    
    Shape: `[batch=2, seq_len=3]`
    
    *Teacher forcing: input is ground truth*
  |
}

target_stage.target_input -> decoder_stack.decoder_input: "target embeddings" {
  style.stroke: ${colors.dataflow}
  style.stroke-width: 2
}

decoder_stack: Decoder Stack (xN layers) {
  style.fill: ${colors.decoder}
  link: "#decoder-stack"
  
  decoder_input: |md
    **Decoder Input** (embedded + positional):
    Shape: `[2, 3, 64]`
    *einops: `b tgt_len d_model`*
  |
  
  dec_layer_0: Decoder Layer 0 {
    style.fill: "#E8F5E9"
    
    masked_self_attn: Masked Self-Attention {
      style.fill: "#C8E6C9"
      
      causal_mask: |md
        **Causal Mask** (applied before softmax):
        
        
        [[0, -inf, -inf],
         [0,    0, -inf],
         [0,    0,    0]]
        
        
        Position i cannot see positions > i
      |
      
      masked_scores: |md
        **Masked Attention Scores**:
        
        `scores = Q @ K^T / sqrt(64)`
        `scores = scores.masked_fill(mask, -inf)`
        `attn = softmax(scores) @ V`
        
        Shape: `[2, 3, 64]`
      |
    }
    
    cross_attn: Cross-Attention {
      style.fill: "#FFF9C4"
      
      cross_qkv: |md
        **Cross-Attention Sources**:
        
        | Component | Source | Shape |
        |-----------|--------|-------|
        | Q | decoder | `[2, 3, 64]` |
        | K | encoder | `[2, 3, 64]` |
        | V | encoder | `[2, 3, 64]` |
        
        **Decoder queries encoder memory**
      |
      
      cross_scores: |md
        **Cross-Attention Scores**:
        
        `scores = Q_dec @ K_enc^T / sqrt(64)  # [2, 3, 3]`
        
        No causal mask - decoder sees all encoder output
      |
    }
    
    dec_ffn: FFN {
      style.fill: "#DCEDC8"
      label: "64 -> 256 -> 64"
    }
    
    masked_self_attn -> cross_attn -> dec_ffn
  }
  
  dec_layer_n: Decoder Layer N-1 {
    style.fill: "#E8F5E9"
    style.stroke-dash: 3
    style.multiple: true
    label: "... (N-1 more layers)"
  }
  
  decoder_output: |md
    **Decoder Output**:
    Shape: `[2, 3, 64]`
    
    *einops: `b tgt_len d_model`*
  |
  
  decoder_input -> dec_layer_0 -> dec_layer_n -> decoder_output
}

decoder_stack.decoder_output -> output_stage.logits_input: "decoder hidden states" {
  style.stroke: ${colors.dataflow}
  style.stroke-width: 3
}

output_stage: Output Projection {
  style.fill: ${colors.output}
  link: "#output-projection"
  
  logits_input: |md
    **Input to Output Layer**:
    Shape: `[2, 3, 64]`
  |
  
  output_proj: Linear Projection {
    style.fill: "#A5D6A7"
    
    proj_weights: |md
      **Output Projection**:
      
      `logits = x @ W_out + b_out  # W_out: [64, 1000]`
      
      Projects d_model -> vocab_size
    |
  }
  
  logits_output: |md
    **Logits**:
    Shape: `[batch=2, seq_len=3, vocab=1000]`
    
    `logits[b, s, v] = score for vocab v at position s in batch b`
    
    *einops: `b s vocab`*
  |
  
  logits_input -> output_proj -> logits_output
}

output_stage.logits_output -> loss_stage.logits_in: "raw scores" {
  style.stroke: ${colors.dataflow}
  style.stroke-width: 3
}

loss_stage: Loss Computation {
  style.fill: ${colors.loss}
  link: "#loss-computation"
  
  logits_in: |md
    **Logits**: `[2, 3, 1000]`
  |
  
  softmax: Softmax {
    style.fill: "#81C784"
    
    probs: |md
      **Probability Distribution**:
      
      `probs = softmax(logits, dim=-1)  # [2, 3, 1000]`
      
      Each position: sum to 1.0
      
      Example at position 0:
      `[0.001, 0.003, ..., 0.85, ..., 0.0001]`
      
      *token 8 has highest prob*
    |
  }
  
  targets: Target Labels {
    style.fill: ${colors.input}
    
    target_tokens: |md
      **Target Labels** (shifted left):
      
      
      [[8, 12, 3],
       [8, 12, 3]]
      
      
      Shape: `[2, 3]`
      
      *Predict token i from input i-1*
    |
  }
  
  cross_entropy: Cross-Entropy Loss {
    style.fill: "#FFCDD2"
    
    loss_formula: |md
      **Loss Computation**:
      
      `loss = -log(probs[b, s, target[b, s]])`
      
      Averaged over batch x sequence:
      
      `L = (1/6) * sum(-log(p_correct))`
      
      Shape: scalar
    |
    
    loss_value: |md
      **Example Loss Values**:
      
      | Batch | Pos | Target | Prob | -log(p) |
      |-------|-----|--------|------|---------|
      | 0     | 0   | 8      | 0.85 | 0.162   |
      | 0     | 1   | 12     | 0.72 | 0.329   |
      | 0     | 2   | 3      | 0.91 | 0.094   |
      
      **Total Loss**: ~0.195 (scalar)
    |
  }
  
  logits_in -> softmax -> cross_entropy
  targets -> cross_entropy
}

legend: {
  near: bottom-right
  style.fill: "#FAFAFA"
  style.stroke: "#E0E0E0"
  
  title: "**Legend**"
  
  dataflow_legend: "Data Flow Arrow" {
    style.fill: transparent
    style.stroke: ${colors.dataflow}
  }
  
  memory_legend: "Encoder Memory (K,V)" {
    style.fill: transparent
    style.stroke: ${colors.metadata}
    style.stroke-dash: 5
  }
  
  shapes: |md
    - **Green gradient**: Processing stages
    - **Boxes with code**: Tensor shapes
    - **Arrows**: Data dependencies
  |
}

shape_summary: {
  near: bottom-left
  style.fill: "#F5F5F5"
  
  summary: |md
    ## Shape Transformation Summary
    
    | Stage | Shape | einops |
    |-------|-------|--------|
    | Input tokens | `[2, 3]` | `b s` |
    | Embeddings | `[2, 3, 64]` | `b s d` |
    | Q, K, V | `[2, 3, 64]` each | `b s d_k` |
    | Attention scores | `[2, 3, 3]` | `b s_q s_k` |
    | Encoder output | `[2, 3, 64]` | `b s d` |
    | Decoder output | `[2, 3, 64]` | `b s d` |
    | Logits | `[2, 3, 1000]` | `b s vocab` |
    | Loss | scalar | - |
  |
}

back_to_map: {
  near: top-right
  label: "Back to Architecture Map"
  link: "#transformer-architecture-map"
  style.fill: "#E3F2FD"
  style.stroke: "#1976D2"
}