vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    input: "#E8F4FD"
    attention: "#FFE4B5"
    cross_attn: "#E6E6FA"
    ffn: "#D4EDDA"
    residual: "#F8D7DA"
    norm: "#D1ECF1"
    output: "#C3E6CB"
    decoder_flow: "#4A90D9"
    encoder_flow: "#E67E22"
  }
}

title: |md
  # Decoder Layer Architecture
  **Single Decoder Block with Dual Attention Mechanisms**
| {near: top-center}

direction: down

input: "Decoder Input x" {
  style.fill: ${colors.input}
  style.stroke: "#2E86AB"
  style.stroke-width: 2
  width: 180
  link: "#decoder-input"
  
  shape_note: |md
    Shape: [batch, seq_len, d_model]
    From: Previous decoder layer or embedding
  |
}

input -> masked_self_attn: "x ∈ ℝ^{B×L×d}"

masked_self_attn: "Masked Self-Attention" {
  style.fill: ${colors.attention}
  style.stroke: "#D4A574"
  style.stroke-width: 2
  width: 220
  link: "#masked-self-attention"
  
  q_proj: "Q = W_Q · x" {
    style.fill: "#FFF8DC"
  }
  k_proj: "K = W_K · x" {
    style.fill: "#FFF8DC"
  }
  v_proj: "V = W_V · x" {
    style.fill: "#FFF8DC"
  }
  
  note: |md
    **Causal Mask Applied**
    Prevents attending to future
    positions during generation
  |
}

residual1: "Add & Norm 1" {
  style.fill: ${colors.norm}
  style.stroke: "#17A2B8"
  style.stroke-width: 2
  width: 140
  link: "#add-norm-1"
  
  operation: |md
    output = LayerNorm(x + attn_out)
  |
}

cross_attn: "Cross-Attention" {
  style.fill: ${colors.cross_attn}
  style.stroke: "#9B59B6"
  style.stroke-width: 2
  width: 240
  link: "#cross-attention"
  
  q_decoder: "Q = W_Q · decoder_x" {
    style.fill: "#E8F4FD"
    style.stroke: ${colors.decoder_flow}
  }
  k_encoder: "K = W_K · encoder_out" {
    style.fill: "#FDEBD0"
    style.stroke: ${colors.encoder_flow}
  }
  v_encoder: "V = W_V · encoder_out" {
    style.fill: "#FDEBD0"
    style.stroke: ${colors.encoder_flow}
  }
  
  note: |md
    **Query from Decoder**
    **K, V from Encoder Output**
    Decoder "asks" encoder for info
  |
}

residual2: "Add & Norm 2" {
  style.fill: ${colors.norm}
  style.stroke: "#17A2B8"
  style.stroke-width: 2
  width: 140
  link: "#add-norm-2"
}

ffn: "Feed-Forward Network" {
  style.fill: ${colors.ffn}
  style.stroke: "#28A745"
  style.stroke-width: 2
  width: 200
  link: "#decoder-ffn"
  
  linear1: "Linear(d_model → d_ff)" {
    style.fill: "#E8F5E9"
  }
  activation: "GELU / ReLU" {
    style.fill: "#C8E6C9"
  }
  linear2: "Linear(d_ff → d_model)" {
    style.fill: "#E8F5E9"
  }
  
  dim_note: |md
    d_ff = 4 × d_model (typically)
    Expansion → Nonlinearity → Projection
  |
}

residual3: "Add & Norm 3" {
  style.fill: ${colors.norm}
  style.stroke: "#17A2B8"
  style.stroke-width: 2
  width: 140
  link: "#add-norm-3"
}

output: "Layer Output" {
  style.fill: ${colors.output}
  style.stroke: "#155724"
  style.stroke-width: 2
  width: 180
  link: "#decoder-output"
  
  out_note: |md
    Shape: [batch, seq_len, d_model]
    To: Next decoder layer or final projection
  |
}

encoder_input: "Encoder Output" {
  style.fill: "#FDEBD0"
  style.stroke: ${colors.encoder_flow}
  style.stroke-width: 2
  style.stroke-dash: 5
  width: 160
  link: "#encoder-output-source"
  
  enc_note: |md
    Shape: [batch, src_len, d_model]
    Frozen during decoder forward pass
  |
}

residual_conn1: "Residual Connection 1" {
  shape: text
  near: center-right
  style.font-color: "#DC3545"
  style.font-size: 12
}

residual_conn2: "Residual Connection 2" {
  shape: text
  near: center-right
  style.font-color: "#DC3545"
  style.font-size: 12
}

residual_conn3: "Residual Connection 3" {
  shape: text
  near: center-right
  style.font-color: "#DC3545"
  style.font-size: 12
}

input -> residual1: "residual" {
  style.stroke: ${colors.residual}
  style.stroke-dash: 3
  style.animated: true
}

masked_self_attn -> residual1: "attn_out"

residual1 -> cross_attn: "x' ∈ ℝ^{B×L×d}"

residual1 -> residual2: "residual" {
  style.stroke: ${colors.residual}
  style.stroke-dash: 3
  style.animated: true
}

cross_attn -> residual2: "cross_attn_out"

encoder_input -> cross_attn: "encoder_output\n(K, V source)" {
  style.stroke: ${colors.encoder_flow}
  style.stroke-width: 2
  style.stroke-dash: 5
}

residual2 -> ffn: "x'' ∈ ℝ^{B×L×d}"

residual2 -> residual3: "residual" {
  style.stroke: ${colors.residual}
  style.stroke-dash: 3
  style.animated: true
}

ffn -> residual3: "ffn_out"

residual3 -> output: "final_out ∈ ℝ^{B×L×d}"

legend: {
  near: bottom-right
  style.fill: "#F8F9FA"
  style.stroke: "#DEE2E6"
  width: 280
  
  title: "Legend"
  
  dec_flow: "Decoder Data Flow" {
    style.fill: ${colors.input}
    style.stroke: ${colors.decoder_flow}
  }
  enc_flow: "Encoder Data Flow" {
    style.fill: "#FDEBD0"
    style.stroke: ${colors.encoder_flow}
    style.stroke-dash: 5
  }
  res_conn: "Residual Connection" {
    style.fill: ${colors.residual}
    style.stroke-dash: 3
  }
  
  dec_flow -> legend
  enc_flow -> legend
  res_conn -> legend
}

dimensions: {
  near: top-right
  style.fill: "#FFF3CD"
  style.stroke: "#856404"
  width: 200
  
  title: |md
    **Typical Dimensions**
    (GPT-2 Small)
  |
  
  d_model: "d_model = 768"
  d_ff: "d_ff = 3072 (4×)"
  n_heads: "n_heads = 12"
  d_k: "d_k = d_v = 64"
}

attention_comparison: {
  near: bottom-left
  style.fill: "#E8E8E8"
  style.stroke: "#666666"
  width: 320
  
  title: "Self-Attention vs Cross-Attention"
  
  self: |md
    **Masked Self-Attention**
    - Q, K, V all from decoder input
    - Causal mask prevents future peeking
    - Learns decoder-internal dependencies
  |
  
  cross: |md
    **Cross-Attention**
    - Q from decoder, K/V from encoder
    - No causal mask (can see all encoder)
    - Learns source-target alignment
  |
  
  self.style.fill: ${colors.attention}
  cross.style.fill: ${colors.cross_attn}
}