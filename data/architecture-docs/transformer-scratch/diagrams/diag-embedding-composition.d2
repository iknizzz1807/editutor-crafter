vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    token_blue: "#4A90D9"
    pos_purple: "#9B59B6"
    combined_green: "#27AE60"
    header_gray: "#34495E"
    flow_red: "#E74C3C"
  }
}

title: |md
  # Token + Positional Embedding Composition
  `output = (token_emb × √d_model) + pos_emb`
| {near: top-center}

direction: right

tokens: Input Tokens {
  style.fill: "${colors.token_blue}"
  style.stroke: "#2E6DA4"
  style.font-color: white
  
  example: |md
    **Batch × Seq Len**
    
    [[542, 12, 891, 23, 542, 7]]
    
    Token IDs from vocabulary
  |
}

token_table: Token Embedding Table {
  style.fill: "${colors.token_blue}"
  style.stroke: "${colors.token_blue}"
  
  shape: sql_table
  
  id: int {constraint: primary_key}
  vec_0: float32
  vec_1: float32
  "vec_dots": "..."
  vec_511: float32
}

token_table.label: "[vocab_size, d_model]\n~50,000 × 512"

pos_table: Positional Encoding Table {
  style.fill: "${colors.pos_purple}"
  style.stroke: "${colors.pos_purple}"
  
  shape: sql_table
  
  pos: int {constraint: primary_key}
  pe_0: float32
  pe_1: float32
  "pe_dots": "..."
  pe_511: float32
}

pos_table.label: "[max_seq_len, d_model]\n512 × 512 (fixed)"

lookup: Embedding Lookup {
  style.fill: "#ECF0F1"
  style.stroke: "${colors.header_gray}"
  
  operation: |md
    python
    tok_emb = embedding_table[token_ids]
    # Shape: [batch, seq, d_model]
    
    
    Direct indexing—no learned operation
  |
}

scale: Scale by √d_model {
  style.fill: "#FDF2E9"
  style.stroke: "${colors.flow_red}"
  
  why: |md
    **Why scale?**
    
    Embeddings initialized ~N(0, 0.02)
    Variance ≈ 0.02² = 0.0004
    
    After scale by √512 ≈ 22.6:
    Variance ≈ 1.0
    
    Matches positional encoding magnitude
  |
  
  formula: |latex
    \text{scaled} = \text{tok\_emb} \times \sqrt{d_{model}}
  |
}

addition: Element-wise Addition {
  style.fill: "${colors.combined_green}"
  style.stroke: "#1E8449"
  style.font-color: white
  
  op: |md
    python
    combined = scaled_tok + pos_enc
    # Same shape: [batch, seq, d_model]
    
    
    **Broadcasting**: pos_enc expands across batch dim
  |
}

output: Combined Representation {
  style.fill: "${colors.combined_green}"
  style.stroke: "#1E8449"
  
  result: |md
    **Output**: [batch, seq_len, d_model]
    
    Each position now encodes:
    1. **What** token this is (semantics)
    2. **Where** it appears (position)
    
    Ready for transformer layers!
  |
}

tokens -> lookup: token IDs
token_table -> lookup: weight matrix
lookup -> scale: tok_emb\n[batch, seq, 512]

pos_table -> addition: pos_enc\n[seq, 512]
scale -> addition: scaled_tok\n[batch, seq, 512]

addition -> output: combined\n[batch, seq, 512]

why_addition: Why Addition (Not Concatenation)? {
  near: bottom-center
  
  explanation: |md
    ### Concatenation would require:
    - `d_model` doubled to 1024
    - All subsequent layers doubled in size
    - Position and token info kept **separate**
    
    ### Addition works because:
    - Same dimension maintained throughout
    - Position info **modulates** token representation
    - Linear layers can learn to disentangle if needed
    - Both signals flow through same pathways
    
    **Key insight**: In high dimensions (512+), adding two vectors preserves enough information to recover both. The network learns to separate them.
  |
}

dims: Dimension Trace {
  near: top-right
  style.fill: "#F8F9FA"
  style.stroke: "#BDC3C7"
  
  trace: ||md
    | Stage | Shape | Notes |
    |-------|-------|-------|
    | Input tokens | [B, S] | Integer IDs |
    | Token lookup | [B, S, D] | D=512 |
    | After scale | [B, S, D] | Same shape |
    | Position enc | [S, D] | Broadcast to B |
    | **Combined** | **[B, S, D]** | Ready for layers |
  ||
}

gradient: Gradient Flow {
  near: bottom-right
  style.fill: "#FDEDEC"
  style.stroke: "${colors.flow_red}"
  
  note: |md
    **Backpropagation**:
    
    ∂L/∂tok_emb receives gradients from ALL positions.
    
    Token embedding table is **shared** across:
    - Input embeddings
    - Output projection (often)
    
    This weight tying regularizes the model.
  |
}

back_link: {
  label: "← Back to Transformer Map"
  link: "#transformer-satellite"
  near: top-left
  style.font-color: "${colors.token_blue}"
}