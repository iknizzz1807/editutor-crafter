vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Stacked Encoder-Decoder Architecture
  The Transformer Information Funnel
| {near: top-center}

direction: right

input_emb: Input Embeddings\n+ Positional Encoding {
  style.fill: "#E8F4FD"
  style.stroke: "#2196F3"
  style.stroke-width: 2
}

target_emb: Target Embeddings\n+ Positional Encoding {
  style.fill: "#FFF3E0"
  style.stroke: "#FF9800"
  style.stroke-width: 2
}

Encoder: Encoder Stack (N=6) {
  style.fill: "#E3F2FD"
  style.stroke: "#1976D2"
  style.stroke-width: 3
  
  L1: Layer 1 {
    style.fill: white
    self_attn: Multi-Head\nSelf-Attention {
      style.fill: "#BBDEFB"
    }
    add_norm1: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    ff1: Feed Forward {
      style.fill: "#F3E5F5"
    }
    add_norm2: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    
    self_attn -> add_norm1
    add_norm1 -> ff1
    ff1 -> add_norm2
  }
  
  L2: Layer 2 {
    style.fill: white
    self_attn: Multi-Head\nSelf-Attention {
      style.fill: "#BBDEFB"
    }
    add_norm1: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    ff1: Feed Forward {
      style.fill: "#F3E5F5"
    }
    add_norm2: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    
    self_attn -> add_norm1
    add_norm1 -> ff1
    ff1 -> add_norm2
  }
  
  dots_enc: "⋮" {
    shape: text
    style.font-size: 40
  }
  
  LN: Layer N {
    style.fill: white
    self_attn: Multi-Head\nSelf-Attention {
      style.fill: "#BBDEFB"
    }
    add_norm1: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    ff1: Feed Forward {
      style.fill: "#F3E5F5"
    }
    add_norm2: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    
    self_attn -> add_norm1
    add_norm1 -> ff1
    ff1 -> add_norm2
  }
  
  L1 -> L2: "abstract\nrepresentations" {
    style.stroke: "#1976D2"
    style.stroke-width: 2
  }
  L2 -> dots_enc
  dots_enc -> LN
}

Decoder: Decoder Stack (N=6) {
  style.fill: "#FFF8E1"
  style.stroke: "#F57C00"
  style.stroke-width: 3
  
  DL1: Layer 1 {
    style.fill: white
    masked_attn: Masked\nSelf-Attention {
      style.fill: "#FFECB3"
    }
    add_norm1: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    cross_attn: Cross-\nAttention {
      style.fill: "#FFCC80"
      style.stroke: "#FF5722"
      style.stroke-width: 3
    }
    add_norm2: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    ff1: Feed Forward {
      style.fill: "#F3E5F5"
    }
    add_norm3: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    
    masked_attn -> add_norm1
    add_norm1 -> cross_attn
    cross_attn -> add_norm2
    add_norm2 -> ff1
    ff1 -> add_norm3
  }
  
  DL2: Layer 2 {
    style.fill: white
    masked_attn: Masked\nSelf-Attention {
      style.fill: "#FFECB3"
    }
    add_norm1: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    cross_attn: Cross-\nAttention {
      style.fill: "#FFCC80"
      style.stroke: "#FF5722"
      style.stroke-width: 3
    }
    add_norm2: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    ff1: Feed Forward {
      style.fill: "#F3E5F5"
    }
    add_norm3: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    
    masked_attn -> add_norm1
    add_norm1 -> cross_attn
    cross_attn -> add_norm2
    add_norm2 -> ff1
    ff1 -> add_norm3
  }
  
  dots_dec: "⋮" {
    shape: text
    style.font-size: 40
  }
  
  DLN: Layer N {
    style.fill: white
    masked_attn: Masked\nSelf-Attention {
      style.fill: "#FFECB3"
    }
    add_norm1: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    cross_attn: Cross-\nAttention {
      style.fill: "#FFCC80"
      style.stroke: "#FF5722"
      style.stroke-width: 3
    }
    add_norm2: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    ff1: Feed Forward {
      style.fill: "#F3E5F5"
    }
    add_norm3: Add & Norm {
      shape: diamond
      style.fill: "#C8E6C9"
    }
    
    masked_attn -> add_norm1
    add_norm1 -> cross_attn
    cross_attn -> add_norm2
    add_norm2 -> ff1
    ff1 -> add_norm3
  }
  
  DL1 -> DL2: "refined\ngeneration" {
    style.stroke: "#F57C00"
    style.stroke-width: 2
  }
  DL2 -> dots_dec
  dots_dec -> DLN
}

output_linear: Linear {
  style.fill: "#FCE4EC"
  style.stroke: "#E91E63"
}
output_softmax: Softmax {
  style.fill: "#FCE4EC"
  style.stroke: "#E91E63"
}
output_probs: Output Probabilities\nP(token | context) {
  style.fill: "#F8BBD9"
  style.stroke: "#C2185B"
  style.stroke-width: 2
}

input_emb -> Encoder.L1: "source\nsequence" {
  style.stroke: "#1976D2"
  style.stroke-width: 2
}

target_emb -> Decoder.DL1: "target\nprefix" {
  style.stroke: "#F57C00"
  style.stroke-width: 2
}

Encoder.L1.add_norm2 -> Decoder.DL1.cross_attn: "K₁, V₁\n(level 1)" {
  style.stroke: "#4CAF50"
  style.stroke-width: 3
  style.stroke-dash: 4
}

Encoder.L2.add_norm2 -> Decoder.DL2.cross_attn: "K₂, V₂\n(level 2)" {
  style.stroke: "#4CAF50"
  style.stroke-width: 3
  style.stroke-dash: 4
}

Encoder.LN.add_norm2 -> Decoder.DLN.cross_attn: "Kₙ, Vₙ\n(level N)" {
  style.stroke: "#4CAF50"
  style.stroke-width: 3
  style.stroke-dash: 4
}

Decoder.DLN.add_norm3 -> output_linear
output_linear -> output_softmax
output_softmax -> output_probs

legend: {
  near: bottom-center
  
  info_flow: |md
    **Information Flow**
    - Blue: Encoder self-attention (bidirectional context)
    - Orange: Decoder masked self-attention (causal)
    - Green dashed: Cross-attention (encoder→decoder)
    - Purple: Feed-forward transformations
  |
  
  key_insight: |md
    **Key Insight**: Each decoder layer queries its corresponding
    encoder layer representation. Early layers attend to surface
    features; deeper layers attend to abstract semantics.
  |
}

cross_attn_detail: Cross-Attention Mechanism {
  near: top-right
  style.fill: "#FFF3E0"
  style.stroke: "#FF5722"
  
  q_source: Q from Decoder\n"what do I need?" {
    style.fill: "#FFECB3"
  }
  k_source: K from Encoder\n"what's available?" {
    style.fill: "#BBDEFB"
  }
  v_source: V from Encoder\n"what to retrieve?" {
    style.fill: "#BBDEFB"
  }
  cross_op: Attention(Q,K,V) {
    style.fill: "#FFCC80"
  }
  
  q_source -> cross_op
  k_source -> cross_op
  v_source -> cross_op
}