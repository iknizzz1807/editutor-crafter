vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    input: "#E8F4FD"
    attention: "#FFE4B5"
    norm: "#E6E6FA"
    ffn: "#D5F5E3"
    residual: "#FFE4E1"
    output: "#F0FFF0"
  }
}

title: |md
  # Transformer Encoder Layer (Post-LN Variant)
  Residual connections around each sublayer, LayerNorm applied AFTER addition
| {near: top-center}

direction: down

input_stream: "Input Stream [batch, seq_len, d_model=512]" {
  style.fill: ${colors.input}
  style.stroke: "#4A90D9"
  style.stroke-width: 2
  
  input_tensor: |md
    
    x ∈ ℝ^(B × L × 512)
    
    Position embeddings + token embeddings
  |
}

input_stream -> mha_block: "x₀"

mha_block: "Multi-Head Self-Attention" {
  style.fill: ${colors.attention}
  style.stroke: "#D4A017"
  style.stroke-width: 2
  
  qkv_proj: "Q, K, V Projections" {
    style.fill: "#FFF8DC"
    
    q_proj: "W_Q: Linear(512, 512)"
    k_proj: "W_K: Linear(512, 512)"
    v_proj: "W_V: Linear(512, 512)"
    
    q_proj.style.fill: "#FFF8DC"
    k_proj.style.fill: "#FFF8DC"
    v_proj.style.fill: "#FFF8DC"
  }
  
  attention_compute: "Scaled Dot-Product" {
    style.fill: "#FFEFD5"
    
    scores: |md
      Scores = QK^T / √d_k
      Shape: [B, H, L, L]
    |
    
    softmax: "Softmax(dim=-1)"
    
    weighted: |md
      Output = weights × V
      Shape: [B, L, 512]
    |
  }
  
  output_proj: "Output Projection" {
    style.fill: "#FFF8DC"
    w_o: "W_O: Linear(512, 512)"
  }
  
  qkv_proj -> attention_compute
  attention_compute -> output_proj
}

mha_block -> residual1_add: "attention_output"

residual_stream_1: "Residual Stream" {
  style.fill: ${colors.residual}
  style.stroke: "#CD5C5C"
  style.stroke-dash: 3
  
  copy1: |md
    ┌─────────────────┐
    │ x₀ (identity)   │
    │ [B, L, 512]     │
    └─────────────────┘
  |
}

residual_stream_1 -> residual1_add: "x₀ (skip connection)"

residual1_add: Add {
  shape: circle
  style.fill: "#FFB6C1"
  style.stroke: "#8B0000"
  style.stroke-width: 2
  width: 60
  height: 60
}

residual1_add -> norm1: "x₀ + MHA(x₀)"

norm1: "LayerNorm 1" {
  style.fill: ${colors.norm}
  style.stroke: "#9370DB"
  style.stroke-width: 2
  
  ln1_compute: |md
    
    μ = mean(x, dim=-1)
    σ² = var(x, dim=-1)
    x̂ = (x - μ) / √(σ² + ε)
    y = γ ⊙ x̂ + β
    
    Learnable: γ, β ∈ ℝ^512
  |
}

norm1 -> ffn_block: "x₁ = LN(x₀ + MHA(x₀))"

ffn_block: "Feed-Forward Network" {
  style.fill: ${colors.ffn}
  style.stroke: "#228B22"
  style.stroke-width: 2
  
  expand: "Linear(512, 2048)" {
    style.fill: "#E0FFE0"
    label: "Expand 4×\nW₁: 512 → 2048"
  }
  
  activation: "GELU / ReLU" {
    style.fill: "#90EE90"
    label: "Element-wise\nnon-linearity"
  }
  
  contract: "Linear(2048, 512)" {
    style.fill: "#E0FFE0"
    label: "Contract\nW₂: 2048 → 512"
  }
  
  ffn_formula: |md
    
    FFN(x) = W₂(GELU(W₁x + b₁)) + b₂
    
    Params: 512×2048 + 2048×512 = 1,048,576
  |
  
  expand -> activation -> contract
}

ffn_block -> residual2_add: "ffn_output"

residual_stream_2: "Residual Stream" {
  style.fill: ${colors.residual}
  style.stroke: "#CD5C5C"
  style.stroke-dash: 3
  
  copy2: |md
    ┌─────────────────┐
    │ x₁ (identity)   │
    │ [B, L, 512]     │
    └─────────────────┘
  |
}

residual_stream_2 -> residual2_add: "x₁ (skip connection)"

residual2_add: Add {
  shape: circle
  style.fill: "#FFB6C1"
  style.stroke: "#8B0000"
  style.stroke-width: 2
  width: 60
  height: 60
}

residual2_add -> norm2: "x₁ + FFN(x₁)"

norm2: "LayerNorm 2" {
  style.fill: ${colors.norm}
  style.stroke: "#9370DB"
  style.stroke-width: 2
  
  ln2_compute: |md
    
    Same formula as LN₁
    Separate γ₂, β₂ ∈ ℝ^512
    
  |
}

norm2 -> output_stream: "x₂ = LN(x₁ + FFN(x₁))"

output_stream: "Output to Next Layer" {
  style.fill: ${colors.output}
  style.stroke: "#2E8B57"
  style.stroke-width: 2
  
  out_tensor: |md
    
    x_out ∈ ℝ^(B × L × 512)
    
    Same shape as input (residual design)
  |
  
  next_layer: "→ Encoder Layer N+1"
}

legend: Legend {
  near: bottom-right
  style.fill: white
  style.stroke: gray
  style.stroke-dash: 2
  
  res_leg: "Residual Path" {
    style.fill: ${colors.residual}
    style.stroke-dash: 3
  }
  
  attn_leg: "Attention Block" {
    style.fill: ${colors.attention}
  }
  
  norm_leg: LayerNorm {
    style.fill: ${colors.norm}
  }
  
  ffn_leg: "Feed-Forward" {
    style.fill: ${colors.ffn}
  }
}

annotations: |md
  **Post-LN Characteristics:**
  - LayerNorm AFTER residual addition
  - Original transformer design (Vaswani et al., 2017)
  - Can have training instability at depth
  - Gradient must flow through LN to reach earlier layers
  
  **Parameter Count (d_model=512, d_ff=2048):**
  - Self-Attn: 4 × 512² = 1,048,576
  - FFN: 2 × 512 × 2048 = 2,097,152
  - LayerNorm: 2 × 2 × 512 = 2,048
  - **Total per layer: ~3.15M params**
| {near: bottom-left}

back_to_map: "↩ Back to Architecture Map" {
  near: top-right
  link: "#transformer-architecture-map"
  style.fill: "#F0F0F0"
  style.stroke: "#999"
  style.stroke-dash: 3
}