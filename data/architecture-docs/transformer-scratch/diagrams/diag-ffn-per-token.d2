vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    input: "#E8F4FD"
    hidden: "#FFF3E0"
    output: "#E8F5E9"
    highlight: "#FF6B6B"
    dim-label: "#666666"
  }
}

title: |md
  # Position-Wise Feed-Forward Network
  Each token processed independently ‚Ä¢ No cross-position mixing
| {near: top-center}

direction: right

input: {
  label: Input Tensor
  style.fill: ${colors.input}
  style.stroke: "#1976D2"
  
  dim_label: |md
    **[batch, seq_len, d_model]**
    e.g., [4, 128, 512]
  | {shape: text}
  
  token_vis: {
    grid-columns: 4
    grid-gap: 4
    
    t0: "t‚ÇÄ" {shape: square; width: 40; style.fill: "#BBDEFB"}
    t1: "t‚ÇÅ" {shape: square; width: 40; style.fill: "#BBDEFB"}
    t2: "t‚ÇÇ" {shape: square; width: 40; style.fill: "#BBDEFB"}
    t3: "..." {shape: square; width: 40; style.fill: transparent; style.stroke-dash: 3}
  }
}

w1: {
  label: "W‚ÇÅ Linear\nProjection"
  style.fill: "#F3E5F5"
  style.stroke: "#7B1FA2"
  style.bold: true
  
  details: |md
    **Expands dimensionality**
    
    Weight: [d_model, d_ff]
    Bias: [d_ff]
    
    d_ff = 4 √ó d_model
    (typically 2048 for d_model=512)
  | {shape: text; style.font-size: 12}
}

hidden: {
  label: Hidden Activation
  style.fill: ${colors.hidden}
  style.stroke: "#E65100"
  
  dim_label: |md
    **[batch, seq_len, d_ff]**
    e.g., [4, 128, 2048]
  | {shape: text}
  
  relu_box: {
    label: ReLU(x) = max(0, x)
    style.fill: "#FFCC80"
    style.stroke: "#E65100"
    style.bold: true
  }
  
  activation_vis: {
    grid-columns: 4
    grid-gap: 4
    
    h0: "h‚ÇÄ" {shape: square; width: 40; style.fill: "#FFE0B2"}
    h1: "h‚ÇÅ" {shape: square; width: 40; style.fill: "#FFE0B2"}
    h2: "h‚ÇÇ" {shape: square; width: 40; style.fill: "#FFE0B2"}
    h3: "..." {shape: square; width: 40; style.fill: transparent; style.stroke-dash: 3}
  }
}

w2: {
  label: "W‚ÇÇ Linear\nProjection"
  style.fill: "#F3E5F5"
  style.stroke: "#7B1FA2"
  style.bold: true
  
  details: |md
    **Projects back to model dim**
    
    Weight: [d_ff, d_model]
    Bias: [d_model]
  | {shape: text; style.font-size: 12}
}

output: {
  label: Output Tensor
  style.fill: ${colors.output}
  style.stroke: "#388E3C"
  
  dim_label: |md
    **[batch, seq_len, d_model]**
    e.g., [4, 128, 512]
  | {shape: text}
  
  token_vis: {
    grid-columns: 4
    grid-gap: 4
    
    o0: "o‚ÇÄ" {shape: square; width: 40; style.fill: "#C8E6C9"}
    o1: "o‚ÇÅ" {shape: square; width: 40; style.fill: "#C8E6C9"}
    o2: "o‚ÇÇ" {shape: square; width: 40; style.fill: "#C8E6C9"}
    o3: "..." {shape: square; width: 40; style.fill: transparent; style.stroke-dash: 3}
  }
}

input -> w1: "each token:\n[d_model] ‚Üí [d_ff]" {
  style.stroke: "#1976D2"
  style.stroke-width: 2
}

w1 -> hidden: "4√ó expansion" {
  style.stroke: "#7B1FA2"
  style.stroke-width: 2
}

hidden -> w2: "each token:\n[d_ff] ‚Üí [d_model]" {
  style.stroke: "#7B1FA2"
  style.stroke-width: 2
}

w2 -> output: "4√ó contraction" {
  style.stroke: "#388E3C"
  style.stroke-width: 2
}

independence_note: ||md
  ### üîë KEY INSIGHT: Position-Wise = Token-Independent
  
  Each token's FFN computation is **completely independent**:
  
  
  for each position i:
      hidden[i] = ReLU(input[i] @ W1 + b1)
      output[i] = hidden[i] @ W2 + b2
  
  
  - Token t‚ÇÄ's output depends ONLY on t‚ÇÄ's input
  - Token t‚ÇÅ's output depends ONLY on t‚ÇÅ's input
  - No information flows between positions here
  - That's what **attention** is for!
  
  **Contrast with Attention:**
  - Attention: output[i] = weighted sum of ALL positions
  - FFN: output[i] = function of position i ONLY
||
independence_note.near: bottom-center
independence_note.shape: rectangle
independence_note.style.fill: "#FFFDE7"
independence_note.style.stroke: "#FBC02D"
independence_note.style.stroke-width: 2

formula_box: ||md
  ### FFN Formula
  
  $$\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$$
  
  Or equivalently (two-layer MLP):
  
  $$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$
  
  **Parameter count**: 
  - W‚ÇÅ: d_model √ó d_ff = 512 √ó 2048 = 1,048,576
  - b‚ÇÅ: d_ff = 2,048
  - W‚ÇÇ: d_ff √ó d_model = 2048 √ó 512 = 1,048,576
  - b‚ÇÇ: d_model = 512
  - **Total: ~2.1M parameters per FFN layer**
||
formula_box.near: center-left
formula_box.shape: rectangle
formula_box.style.fill: "#F5F5F5"
formula_box.style.stroke: "#9E9E9E"

variants_note: ||md
  ### Common Variants
  
  | Variant | Activation | Notes |
  |---------|------------|-------|
  | Original | ReLU | max(0, x) |
  | GPT-2 | GELU | Gaussian Error LU |
  | LLaMA | SwiGLU | Gated variant |
  | T5 | ReLU | Same as original |
  
  The 4√ó expansion ratio is standard but
  can be tuned (e.g., 8/3√ó in some models).
||
variants_note.near: center-right
variants_note.shape: rectangle
variants_note.style.fill: "#E3F2FD"
variants_note.style.stroke: "#1565C0"