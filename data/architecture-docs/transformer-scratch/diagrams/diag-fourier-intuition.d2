vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    header: "#E8D5B7"
    math: "#B8D4E8"
    insight: "#D4E8D4"
    signal: "#F0E4D4"
    low_freq: "#A8D8A8"
    high_freq: "#D8A8A8"
  }
}

title: |md
  # Positional Encoding as Fourier Features
  *Sinusoidal position embeddings are learnable frequency decompositions*
| {near: top-center}

direction: right

Fourier_Intuition: {
  style.fill: ${colors.header}
  label: |md
    ## The Fourier Perspective
    
    Every dimension of positional encoding
    is a **basis function** at a specific frequency:
    
    - Low frequencies: capture long-range position
    - High frequencies: capture fine-grained order
    
    The model learns to **decode position** by
    combining these frequency components—just
    like Fourier analysis in reverse.
  |
}

Position_Signal: {
  style.fill: ${colors.signal}
  label: Position Signal
  
  pos_axis: {
    shape: sequence_diagram
    p0: pos=0
    p1: pos=1
    p2: pos=2
    p3: pos=3
    p4: pos=4
    p5: pos=5
    
    p0 -> p1 -> p2 -> p3 -> p4 -> p5
  }
  
  raw_position: {
    label: "Raw position: 0, 1, 2, 3, 4, 5..."
    shape: text
    style.font: mono
  }
  
  question: {
    label: "How does the model\n'read' position?"
    shape: diamond
    style.fill: "#FFF8DC"
  }
}

Frequency_Bank: {
  style.fill: ${colors.math}
  label: |md
    ## Frequency Bank (d_model/2 frequencies)
    
    Each pair of dimensions = one frequency channel:
    
    PE(pos, 2i)   = sin(pos / 10000^(2i/d))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
    
    
    Frequency ω_i = 10000^(-2i/d_model)
  |
  
  freq_table: {
    shape: sql_table
    dim_pair: int
    frequency: float
    wavelength: int
    
    0: {constraint: primary_key}
    1: "1.0"
    2: "1"
    
    1: {}
    2: "0.5"
    3: "2"
    
    2: {}
    3: "0.25"
    4: "4"
    
    4: {}
    5: "0.0625"
    6: "16"
    
    5: {}
    6: "0.0039"
    7: "256"
    
    6: {}
    7: "0.0001"
    8: "10000"
  }
  
  note: {
    label: |md
      **Wavelength range**: 1 to 10,000 positions
      
      Low freq (dim 0): changes every position
      High freq (dim 256): changes every 10k positions
    |
    shape: text
    style.font-size: 14
  }
}

Basis_Visualization: {
  style.fill: "#F5F5F5"
  label: |md
    ## Basis Functions (sin components shown)
  |
  
  grid-columns: 5
  grid-gap: 4
  
  basis_legend: {
    grid-columns: 3
    grid-gap: 8
    
    low: {
      label: "Low Freq"
      style.fill: ${colors.low_freq}
    }
    mid: {
      label: "Mid Freq"
      style.fill: ${colors.header}
    }
    high: {
      label: "High Freq"
      style.fill: ${colors.high_freq}
    }
  }
  
  f0: {
    label: |md
      **ω=1**
      
      pos: 0 1 2 3 4
      sin: ○→●→○→●→○
    |
    style.fill: ${colors.low_freq}
  }
  
  f1: {
    label: |md
      **ω=0.5**
      
      pos: 0 1 2 3 4
      sin: ○→○→●→○→○
    |
    style.fill: ${colors.low_freq}
  }
  
  f2: {
    label: |md
      **ω=0.25**
      
      pos: 0 1 2 3 4
      sin: ○→○→○→○→◐
    |
    style.fill: ${colors.header}
  }
  
  f3: {
    label: |md
      **ω=0.06**
      
      pos: 0 1 2 3 4
      sin: ○→○→○→○→○
    |
    style.fill: ${colors.high_freq}
  }
  
  f4: {
    label: |md
      **ω=0.0001**
      
      pos: 0 1 2 3 4
      sin: ●→●→●→●→●
    |
    style.fill: ${colors.high_freq}
  }
}

Position_Signal -> Frequency_Bank: "Project onto\nfrequency basis"
Frequency_Bank -> Basis_Visualization: "Each dimension\n= one frequency"

Decoding_Position: {
  style.fill: ${colors.insight}
  label: |md
    ## How the Model "Reads" Position
    
    The model learns linear combinations of basis
    functions to decode absolute position:
    
    
    pos ≈ Σ w_i · sin(pos · ω_i) + w'_i · cos(pos · ω_i)
    
    
    This is **learned Fourier inversion**:
    - Attention layers learn weights `w_i`
    - Combination of frequencies → position estimate
    - No explicit position needed in architecture
  |
  
  analogy: {
    label: ||md
      ### Analogy: Audio Spectrogram
      
      | Audio | Position Encoding |
      |-------|-------------------|
      | Time | Token position |
      | Frequency bands | Encoding dimensions |
      | Spectrogram | PE matrix |
      | Pitch detection | Position detection |
    ||
    style.fill: "#FFFFFF"
  }
}

Basis_Visualization -> Decoding_Position: "Model learns to\ndecode position\nfrom frequency pattern"

Why_Sinusoids: {
  style.fill: "#E8E8F0"
  label: |md
    ## Why Sinusoids? (vs Learned Embeddings)
    
    **Extrapolation**: Sinusoids extend to any sequence length
    
    PE(10001) = sin(10001/λ)  # Works for unseen lengths
    
    
    **Relative position encoding**:
    
    PE(pos+k) can be expressed as linear function of PE(pos)
    
    Because: sin(a+b) = sin(a)cos(b) + cos(a)sin(b)
    
    **Smoothness**: Nearby positions have similar encodings
    
    ||PE(pos) - PE(pos+1)|| is small
  |
}

Decoding_Position -> Why_Sinusoids: "Sinusoid properties\nenable this"

Memory_Layout: {
  style.fill: "#F0F0F0"
  label: |md
    ## PE Matrix Structure [max_seq, d_model]
  |
  
  grid-columns: 1
  grid-gap: 0
  
  header_row: {
    grid-columns: 6
    grid-gap: 0
    
    h0: {label: "pos"; style.fill: "#CCCCCC"}
    h1: {label: "sin(ω₀)"; style.fill: ${colors.low_freq}}
    h2: {label: "cos(ω₀)"; style.fill: ${colors.low_freq}}
    h3: {label: "sin(ω₁)"; style.fill: ${colors.header}}
    h4: {label: "cos(ω₁)"; style.fill: ${colors.header}}
    h5: {label: "..."; style.fill: "#DDDDDD"}
  }
  
  row0: {
    grid-columns: 6
    grid-gap: 0
    
    r0_0: {label: "0"; style.fill: "#CCCCCC"}
    r0_1: {label: "0.00"; style.fill: ${colors.low_freq}}
    r0_2: {label: "1.00"; style.fill: ${colors.low_freq}}
    r0_3: {label: "0.00"; style.fill: ${colors.header}}
    r0_4: {label: "1.00"; style.fill: ${colors.header}}
    r0_5: {label: "..."; style.fill: "#DDDDDD"}
  }
  
  row1: {
    grid-columns: 6
    grid-gap: 0
    
    r1_0: {label: "1"; style.fill: "#CCCCCC"}
    r1_1: {label: "0.84"; style.fill: ${colors.low_freq}}
    r1_2: {label: "0.54"; style.fill: ${colors.low_freq}}
    r1_3: {label: "0.51"; style.fill: ${colors.header}}
    r1_4: {label: "0.86"; style.fill: ${colors.header}}
    r1_5: {label: "..."; style.fill: "#DDDDDD"}
  }
  
  row2: {
    grid-columns: 6
    grid-gap: 0
    
    r2_0: {label: "2"; style.fill: "#CCCCCC"}
    r2_1: {label: "0.91"; style.fill: ${colors.low_freq}}
    r2_2: {label: "-0.42"; style.fill: ${colors.low_freq}}
    r2_3: {label: "0.88"; style.fill: ${colors.header}}
    r2_4: {label: "0.47"; style.fill: ${colors.header}}
    r2_5: {label: "..."; style.fill: "#DDDDDD"}
  }
  
  row_ellipsis: {
    label: "..."
    shape: text
  }
  
  row511: {
    grid-columns: 6
    grid-gap: 0
    
    r511_0: {label: "511"; style.fill: "#CCCCCC"}
    r511_1: {label: "~0"; style.fill: ${colors.low_freq}}
    r511_2: {label: "~1"; style.fill: ${colors.low_freq}}
    r511_3: {label: "~0"; style.fill: ${colors.header}}
    r511_4: {label: "~1"; style.fill: ${colors.header}}
    r511_5: {label: "..."; style.fill: "#DDDDDD"}
  }
}

Fourier_Intuition -> Memory_Layout: "Each column =\none frequency"