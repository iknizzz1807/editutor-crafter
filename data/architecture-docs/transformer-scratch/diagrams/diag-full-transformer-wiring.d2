vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    embedding: "#E8F5E9"
    encoder: "#E3F2FD"
    decoder: "#FFF3E0"
    attention: "#F3E5F5"
    output: "#FFEBEE"
    flow: "#1976D2"
    cross: "#7B1FA2"
    residual: "#9E9E9E"
  }
}

title: |md
  # Complete Transformer Architecture
  ### Data Flow & Dimensional Analysis
| {near: top-center}

direction: right

source_input: "Source Input\n\"The cat sat\"" {
  shape: rectangle
  style.fill: ${colors.embedding}
  style.stroke: "#4CAF50"
  link: "#source-embeddings"
}

source_tokenizer: "Tokenizer\n[101, 2769, 4937, 3314, 102]" {
  shape: rectangle
  style.fill: ${colors.embedding}
  style.stroke: "#4CAF50"
}

source_input -> source_tokenizer: tokenize {
  style.stroke: ${colors.flow}
}

source_embed: Source Embeddings {
  style.fill: ${colors.embedding}
  style.stroke: "#4CAF50"
  link: "#embedding-layer"
  
  token_embed: "Token Embedding\n[batch, seq_len, d_model]" {
    style.fill: "#C8E6C9"
  }
  
  pos_embed: "Positional Encoding\n[sinusoidal]" {
    style.fill: "#C8E6C9"
  }
  
  combined: Combined {
    style.fill: "#A5D6A7"
    label: "[batch, seq_len, 512]"
  }
}

source_tokenizer -> source_embed: embed {
  style.stroke: ${colors.flow}
}

encoder_stack: "Encoder Stack (×6)" {
  style.fill: ${colors.encoder}
  style.stroke: "#1976D2"
  link: "#encoder-stack"
  
  enc_layer_1: "Layer 1" {
    style.fill: "#BBDEFB"
    
    enc_mha_1: "Multi-Head\nAttention" {
      style.fill: ${colors.attention}
      style.stroke: "#7B1FA2"
    }
    
    enc_add_1: "Add & Norm" {
      style.fill: "#E1F5FE"
    }
    
    enc_ffn_1: "Feed Forward\nNetwork" {
      style.fill: "#B3E5FC"
    }
    
    enc_add_2: "Add & Norm" {
      style.fill: "#E1F5FE"
    }
  }
  
  enc_layer_n: "Layer 6" {
    style.fill: "#BBDEFB"
    
    enc_mha_n: "Multi-Head\nAttention" {
      style.fill: ${colors.attention}
      style.stroke: "#7B1FA2"
    }
    
    enc_add_n: "Add & Norm" {
      style.fill: "#E1F5FE"
    }
    
    enc_ffn_n: "Feed Forward\nNetwork" {
      style.fill: "#B3E5FC"
    }
    
    enc_add_n2: "Add & Norm" {
      style.fill: "#E1F5FE"
    }
  }
  
  enc_dots: "..." {
    shape: text
    style.font-size: 40
  }
}

source_embed -> encoder_stack: "[batch, src_len, 512]" {
  style.stroke: ${colors.flow}
  style.stroke-width: 3
}

encoder_output: "Encoder Output\n[batch, src_len, 512]" {
  style.fill: ${colors.encoder}
  style.stroke: "#1976D2"
  style.bold: true
}

encoder_stack -> encoder_output {
  style.stroke: ${colors.flow}
  style.stroke-width: 3
}

target_input: "Target Input\n\"Le chat assis\"" {
  shape: rectangle
  style.fill: ${colors.embedding}
  style.stroke: "#FF9800"
}

target_tokenizer: "Tokenizer\n[BOS, 571, 9452, 12489]" {
  shape: rectangle
  style.fill: ${colors.embedding}
  style.stroke: "#FF9800"
}

target_input -> target_tokenizer: tokenize {
  style.stroke: ${colors.flow}
}

target_embed: Target Embeddings {
  style.fill: ${colors.embedding}
  style.stroke: "#FF9800"
  
  tgt_token: "Token Embedding\n[batch, tgt_len, d_model]" {
    style.fill: "#FFE0B2"
  }
  
  tgt_pos: "Positional Encoding\n[sinusoidal]" {
    style.fill: "#FFE0B2"
  }
  
  tgt_combined: Combined {
    style.fill: "#FFCC80"
    label: "[batch, tgt_len, 512]"
  }
}

target_tokenizer -> target_embed: embed {
  style.stroke: ${colors.flow}
}

decoder_stack: "Decoder Stack (×6)" {
  style.fill: ${colors.decoder}
  style.stroke: "#FF9800"
  link: "#decoder-stack"
  
  dec_layer_1: "Layer 1" {
    style.fill: "#FFE0B2"
    
    dec_mha_1: "Masked\nSelf-Attention" {
      style.fill: ${colors.attention}
      style.stroke: "#E91E63"
    }
    
    dec_add_1: "Add & Norm" {
      style.fill: "#FFF8E1"
    }
    
    dec_cross_1: "Cross-Attention" {
      style.fill: ${colors.attention}
      style.stroke: ${colors.cross}
      link: "#cross-attention"
    }
    
    dec_add_2: "Add & Norm" {
      style.fill: "#FFF8E1"
    }
    
    dec_ffn_1: "Feed Forward\nNetwork" {
      style.fill: "#FFECB3"
    }
    
    dec_add_3: "Add & Norm" {
      style.fill: "#FFF8E1"
    }
  }
  
  dec_layer_n: "Layer 6" {
    style.fill: "#FFE0B2"
    
    dec_mha_n: "Masked\nSelf-Attention" {
      style.fill: ${colors.attention}
      style.stroke: "#E91E63"
    }
    
    dec_add_n: "Add & Norm" {
      style.fill: "#FFF8E1"
    }
    
    dec_cross_n: "Cross-Attention" {
      style.fill: ${colors.attention}
      style.stroke: ${colors.cross}
    }
    
    dec_add_n2: "Add & Norm" {
      style.fill: "#FFF8E1"
    }
    
    dec_ffn_n: "Feed Forward\nNetwork" {
      style.fill: "#FFECB3"
    }
    
    dec_add_n3: "Add & Norm" {
      style.fill: "#FFF8E1"
    }
  }
  
  dec_dots: "..." {
    shape: text
    style.font-size: 40
  }
}

target_embed -> decoder_stack: "[batch, tgt_len, 512]" {
  style.stroke: ${colors.flow}
  style.stroke-width: 3
}

encoder_output -> decoder_stack.dec_layer_1.dec_cross_1: "K, V\n[batch, src_len, 512]" {
  style.stroke: ${colors.cross}
  style.stroke-width: 3
  style.stroke-dash: 5
}

encoder_output -> decoder_stack.dec_layer_n.dec_cross_n: "K, V\n[batch, src_len, 512]" {
  style.stroke: ${colors.cross}
  style.stroke-width: 3
  style.stroke-dash: 5
}

decoder_output: "Decoder Output\n[batch, tgt_len, 512]" {
  style.fill: ${colors.decoder}
  style.stroke: "#FF9800"
  style.bold: true
}

decoder_stack -> decoder_output {
  style.stroke: ${colors.flow}
  style.stroke-width: 3
}

output_proj: "Output Projection" {
  style.fill: ${colors.output}
  style.stroke: "#D32F2F"
  link: "#output-projection"
  
  linear: "Linear\n[512 -> vocab_size]" {
    style.fill: "#FFCDD2"
  }
}

decoder_output -> output_proj: "[batch, tgt_len, 512]" {
  style.stroke: ${colors.flow}
  style.stroke-width: 3
}

logits: "Logits\n[batch, tgt_len, vocab_size]" {
  style.fill: ${colors.output}
  style.stroke: "#D32F2F"
  style.bold: true
}

output_proj -> logits {
  style.stroke: ${colors.flow}
  style.stroke-width: 3
}

softmax: Softmax {
  style.fill: ${colors.output}
  style.stroke: "#D32F2F"
}

logits -> softmax: raw scores

probabilities: "Probabilities\nP(token | context)" {
  style.fill: ${colors.output}
  style.stroke: "#D32F2F"
}

softmax -> probabilities

next_token: Next Token {
  shape: diamond
  style.fill: "#4CAF50"
  style.stroke: "#2E7D32"
}

probabilities -> next_token: "argmax or\nsample"

legend: |md
  ### Legend
  - **Green**: Embedding layers
  - **Blue**: Encoder components
  - **Orange**: Decoder components
  - **Purple**: Attention mechanisms
  - **Red**: Output projection
  - **Dashed purple**: Cross-attention (encoder-decoder)
  - **Gray arrows**: Residual connections (within layers)
|
legend.near: bottom-right
legend.style.fill: "#FAFAFA"
legend.style.stroke: "#E0E0E0"
legend.style.border-radius: 8

dimensions_table: |md
  ### Dimensional Analysis
  | Stage | Shape |
  |-------|-------|
  | Source tokens | `[batch, src_len]` |
  | Source embeddings | `[batch, src_len, 512]` |
  | Encoder output | `[batch, src_len, 512]` |
  | Target tokens | `[batch, tgt_len]` |
  | Target embeddings | `[batch, tgt_len, 512]` |
  | Decoder output | `[batch, tgt_len, 512]` |
  | Logits | `[batch, tgt_len, vocab_size]` |
  
  **d_model = 512** (typical)
  **vocab_size approx 30,000-50,000**
|
dimensions_table.near: bottom-left
dimensions_table.style.fill: "#FAFAFA"
dimensions_table.style.stroke: "#E0E0E0"
dimensions_table.style.border-radius: 8

residual_note: |md
  #### Residual Connections
  Each sublayer uses:
  
  `output = x + Sublayer(LayerNorm(x))`
  
  Enables gradient flow through deep stacks.
|
residual_note.near: center-left
residual_note.style.fill: "#F5F5F5"
residual_note.style.stroke: ${colors.residual}
residual_note.style.border-radius: 4
residual_note.style.font-size: 14

attention_detail: |md
  #### Attention Heads
  - **8 heads** per layer
  - **d_k = d_v = 64** (512 / 8)
  - Heads attend in parallel
  - Outputs concatenated
|
attention_detail.near: center-right
attention_detail.style.fill: ${colors.attention}
attention_detail.style.stroke: "#7B1FA2"
attention_detail.style.border-radius: 4
attention_detail.style.font-size: 14