vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Attention Head Specialization
  ### Multi-head attention learns diverse retrieval strategies through independent projections
| {near: top-center}

legend: {
  near: bottom-center
  style.fill: transparent
  style.stroke: "#CBD6E0"
  style.stroke-dash: 3
  
  head1_legend: "Syntactic Dependencies" {
    shape: text
    style.font-color: "#E53935"
  }
  head2_legend: "Semantic Relationships" {
    shape: text
    style.font-color: "#43A047"
  }
  head3_legend: "Positional Proximity" {
    shape: text
    style.font-color: "#1E88E5"
  }
}

input: Input Embedding X {
  width: 500
  style.fill: "#E3F2FD"
  style.stroke: "#1565C0"
  style.stroke-width: 2
  
  dim_label: |md
    `[batch, seq_len, d_model=512]`
  |
}

input -> heads_container: "Broadcast to all heads"

heads_container: {
  direction: right
  style.fill: transparent
  
  head1: Head 1: Syntactic {
    width: 280
    style.fill: "#FFEBEE"
    style.stroke: "#E53935"
    style.stroke-width: 2
    
    projections1: "Independent Q₁, K₁, V₁\nProjections (d_k = 64)" {
      style.fill: "#FFCDD2"
      style.stroke: "#E53935"
    }
    
    pattern1: |md
      **Learns:**
      - Subject-verb agreement
      - Modifier-head relations  
      - Clause boundaries
    |
    
    example1: Example Attention {
      style.fill: white
      style.stroke: "#EF9A9A"
      seq: |md
        `"The cat that chased"`
        
        **Query**: "chased" (verb)
        **Attends to**: "cat" (subject)
        **Weight**: 0.87
      |
    }
    
    projections1 -> pattern1
    pattern1 -> example1
  }
  
  head2: Head 2: Semantic {
    width: 280
    style.fill: "#E8F5E9"
    style.stroke: "#43A047"
    style.stroke-width: 2
    
    projections2: "Independent Q₂, K₂, V₂\nProjections (d_k = 64)" {
      style.fill: "#C8E6C9"
      style.stroke: "#43A047"
    }
    
    pattern2: |md
      **Learns:**
      - Coreference chains
      - Topic continuity
      - Entity relationships
    |
    
    example2: Example Attention {
      style.fill: white
      style.stroke: "#A5D6A7"
      seq: |md
        `"Apple released iPhone. It"`
        
        **Query**: "It" (pronoun)
        **Attends to**: "Apple" (entity)
        **Weight**: 0.92
      |
    }
    
    projections2 -> pattern2
    pattern2 -> example2
  }
  
  head3: Head 3: Positional {
    width: 280
    style.fill: "#E3F2FD"
    style.stroke: "#1E88E5"
    style.stroke-width: 2
    
    projections3: "Independent Q₃, K₃, V₃\nProjections (d_k = 64)" {
      style.fill: "#BBDEFB"
      style.stroke: "#1E88E5"
    }
    
    pattern3: |md
      **Learns:**
      - Local context windows
      - Adjacent token patterns
      - N-gram dependencies
    |
    
    example3: Example Attention {
      style.fill: white
      style.stroke: "#90CAF9"
      seq: |md
        `"quick brown fox jumps"`
        
        **Query**: "fox" (position 3)
        **Attends to**: "brown" (position 2)
        **Weight**: 0.71
      |
    }
    
    projections3 -> pattern3
    pattern3 -> example3
  }
}

concat: Concatenate Head Outputs {
  width: 500
  style.fill: "#F3E5F5"
  style.stroke: "#7B1FA2"
  style.stroke-width: 2
  
  concat_label: |md
    `[batch, seq_len, h × d_k] = [batch, seq_len, 512]`
    
    Head outputs concatenated, then projected through W_O
  |
}

heads_container.head1 -> concat: "output₁"
heads_container.head2 -> concat: "output₂"
heads_container.head3 -> concat: "output₃"

mechanism_explainer: |md
  ## Why Independent Projections Enable Specialization
  
  Each head learns its own `W_Q^i`, `W_K^i`, `W_V^i` matrices:
  
  
  Q_i = X @ W_Q^i    # Shape: [batch, seq, 64]
  K_i = X @ W_K^i    # Different projection → different query semantics
  V_i = X @ W_V^i    # Different projection → different value extraction
  
  
  **Gradient flow**: Each head receives independent gradients, allowing
  `W_Q^1` to learn syntactic queries while `W_Q^2` learns semantic queries.
  
  **No enforced diversity**: Specialization emerges naturally from training—
  the model discovers that distributing patterns across heads is optimal.
| {near: center-right}

backlink: "← Back to Satellite Map" {
  link: "#transformer-architecture"
  near: top-left
  style.stroke-dash: 3
  style.fill: transparent
}