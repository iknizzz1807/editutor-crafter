vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    warmup: "#22C55E"
    decay: "#3B82F6"
    fixed: "#EF4444"
    danger: "#DC2626"
    safe: "#16A34A"
    grid: "#374151"
    axis: "#9CA3AF"
    highlight: "#FBBF24"
  }
}

title: |md
  # Learning Rate Schedule: Warmup + Inverse-Sqrt Decay
  `lr = d_model⁻⁰·⁵ × min(step⁻⁰·⁵, step × warmup_steps⁻¹·⁵)`
| {near: top-center}

direction: right

warmup_phase: {
  label: "PHASE 1: LINEAR WARMUP"
  style.fill: "#E8F5E9"
  style.stroke: "#22C55E"
  style.border-radius: 8

  warmup_curve: {
    label: |md
      **Warmup Region (step < warmup_steps)**
      
      lr = d_model⁻⁰·⁵ × step × warmup_steps⁻¹·⁵
      
      Linear increase from 0 → peak_lr
    |
    shape: rectangle
    style.fill: "#C8E6C9"
  }

  warmup_why: {
    label: |md
      ## Why Warmup Prevents Early Instability
      
      **Problem at step=0:**
      - Random weight initialization
      - Gradients are chaotic (some 100x larger than others)
      - Large LR amplifies noise → weight explosion
      
      **Warmup Solution:**
      1. Start with tiny LR (e.g., 1e-7)
      2. Model learns to "walk before running"
      3. Adam's momentum buffers stabilize
      4. By step=warmup_steps, gradients are well-behaved
      
      **Without warmup:**
      - Step 1: lr=0.001, grad=100 → ΔW=0.1 (too large!)
      - Weights diverge, loss becomes NaN
    |
    shape: rectangle
    style.fill: "#E8F5E9"
    style.stroke: "#16A34A"
    style.border-radius: 6
  }

  warmup_value: {
    label: |md
      **Typical warmup_steps:**
      - Base transformer: 4,000
      - BERT: 10,000
      - GPT-2: 2,000
      
      **Peak LR ≈ 1e-3 to 5e-4**
    |
    shape: text
  }
}

decay_phase: {
  label: "PHASE 2: INVERSE-SQRT DECAY"
  style.fill: "#E3F2FD"
  style.stroke: "#3B82F6"
  style.border-radius: 8

  decay_curve: {
    label: |md
      **Decay Region (step ≥ warmup_steps)**
      
      lr = d_model⁻⁰·⁵ × step⁻⁰·⁵
      
      Gradual decay: peak_lr × √(warmup_steps/step)
    |
    shape: rectangle
    style.fill: "#BBDEFB"
  }

  decay_why: {
    label: |md
      ## Why Decay Prevents Late-Training Oscillation
      
      **Problem at step=100k+:**
      - Model near local minimum
      - Fixed LR keeps "kicking" weights
      - Loss oscillates instead of converging
      
      **Decay Solution:**
      1. As step→∞, lr→0
      2. Smaller steps = finer convergence
      3. Allows settling into narrow minima
      
      **Inverse-sqrt specifically:**
      - Slower than exponential (not too aggressive)
      - Faster than 1/t (still meaningful at 1M steps)
      - Mathematically principled for Adam optimizer
    |
    shape: rectangle
    style.fill: "#E8F5E9"
    style.stroke: "#16A34A"
    style.border-radius: 6
  }

  decay_trajectory: {
    label: |md
      **LR at key milestones:**
      - Step 4,000 (peak): lr = 5.0e-4
      - Step 16,000: lr = 2.5e-4 (÷2)
      - Step 64,000: lr = 1.25e-4 (÷4)
      - Step 256,000: lr = 6.25e-5 (÷8)
    |
    shape: text
  }
}

fixed_lr_comparison: {
  label: "COMPARISON: FIXED LEARNING RATE"
  style.fill: "#FFEBEE"
  style.stroke: "#EF4444"
  style.border-radius: 8

  fixed_curve: {
    label: |md
      **Fixed LR (naive approach)**
      
      lr = constant (e.g., 1e-4)
      
      Simple but suboptimal
    |
    shape: rectangle
    style.fill: "#FFCDD2"
  }

  fixed_problems: {
    label: |md
      ## Fixed LR Failure Modes
      
      **If lr is too HIGH (e.g., 1e-3):**
      - Early training: NaN loss, divergence
      - Never converges, oscillates forever
      
      **If lr is too LOW (e.g., 1e-5):**
      - Early training: painfully slow progress
      - 10x more steps to reach same loss
      - May get stuck in bad local minima
      
      **Goldilocks problem:**
      - No single LR works for all 1M+ steps
      - Early steps need small LR
      - Middle steps need large LR
      - Late steps need tiny LR
      
      **→ Warmup+Decay solves all three!**
    |
    shape: rectangle
    style.fill: "#FFEBEE"
    style.stroke: "#DC2626"
    style.border-radius: 6
  }
}

mathematical_form: {
  label: "COMPLETE FORMULA"
  style.fill: "#FFF8E1"
  style.stroke: "#FBBF24"
  style.border-radius: 8
  style.stroke-width: 2

  formula: {
    label: |md
      python
      def get_lr(step, d_model=512, warmup_steps=4000):
          """
          Transformer learning rate schedule (Vaswani et al., 2017)
          """
          # Scale factor based on model dimension
          scale = d_model ** -0.5
          
          # Take minimum of warmup ramp and decay curve
          lr = scale * min(
              step ** -0.5,           # Decay: 1/√step
              step * warmup_steps ** -1.5  # Warmup: step/warmup^1.5
          )
          
          return lr
      
      
      **At step=warmup_steps:** Both terms equal → peak LR
      - Warmup term: warmup_steps × warmup_steps⁻¹·⁵ = warmup_steps⁻⁰·⁵
      - Decay term: warmup_steps⁻⁰·⁵
      - These are identical at the transition point!
    |
    shape: rectangle
  }
}

gradient_scale_note: {
  label: |md
    ### Why d_model⁻⁰·⁵ scaling?
    
    Larger models (d_model=2048 vs 512) have:
    - More parameters → larger gradients in aggregate
    - The √d_model normalization keeps gradient magnitudes
      consistent across model sizes
    
    This is the same principle as √d_k scaling in attention!
  |
  shape: rectangle
  style.fill: "#BBDEFB"
  style.border-radius: 6
  near: bottom-center
}

warmup_phase -> decay_phase: "step reaches warmup_steps" {
  style.stroke: "#22C55E"
  style.stroke-width: 2
  style.animated: true
}

fixed_lr_comparison -> warmup_phase: "fixed LR too high\n→ NaN at start" {
  style.stroke: "#DC2626"
  style.stroke-dash: 4
}

fixed_lr_comparison -> decay_phase: "fixed LR too low\n→ slow convergence" {
  style.stroke: "#DC2626"
  style.stroke-dash: 4
}

mathematical_form -> warmup_phase: "first term dominates" {
  style.stroke: "#22C55E"
  style.stroke-dash: 3
}

mathematical_form -> decay_phase: "second term dominates" {
  style.stroke: "#3B82F6"
  style.stroke-dash: 3
}