vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Padding & Causal Masking in Scaled Dot-Product Attention
| {near: top-center}

direction: right

padding_section: {
  label: "Panel 1: Padding Mask"

  padding_input: {
    label: "Input Sequence\n[batch, seq_len]"
    shape: rectangle
    style.fill: "#E8F4FD"
    style.stroke: "#2E86AB"
    style.stroke-width: 2
    
    tokens: {
      grid-columns: 5
      grid-gap: 0
      
      t0: "The" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"}
      t1: "cat" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"}
      t2: "sat" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"}
      t3: "<PAD>" {style.fill: "#FFB4B4"; style.stroke: "#C75050"}
      t4: "<PAD>" {style.fill: "#FFB4B4"; style.stroke: "#C75050"}
    }
  }
  
  mask_shape: {
    label: "Mask Shape\n[batch, 1, 1, seq_len]"
    shape: rectangle
    style.fill: "#FFF9E6"
    style.stroke: "#D4A500"
    style.stroke-dash: 3
  }
  
  padding_mask: {
    label: "Padding Mask\n(1 = keep, 0 = mask)"
    shape: rectangle
    style.fill: "#F0F0F0"
    style.stroke: "#666666"
    
    mask_grid: {
      grid-columns: 5
      grid-gap: 0
      
      m0: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      m1: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      m2: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      m3: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      m4: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
    }
  }
  
  scores_before: {
    label: "Scores BEFORE Mask\n[batch, heads, seq, seq]"
    shape: rectangle
    style.fill: "#E6F3FF"
    style.stroke: "#4A90D9"
    
    before_grid: {
      grid-columns: 5
      grid-gap: 0
      
      b00: "2.1" {style.fill: "#E6F3FF"; style.font: mono; style.font-size: 14}
      b01: "1.8" {style.fill: "#E6F3FF"; style.font: mono; style.font-size: 14}
      b02: "0.5" {style.fill: "#E6F3FF"; style.font: mono; style.font-size: 14}
      b03: "1.2" {style.fill: "#E6F3FF"; style.font: mono; style.font-size: 14}
      b04: "0.9" {style.fill: "#E6F3FF"; style.font: mono; style.font-size: 14}
    }
  }
  
  scores_after: {
    label: "Scores AFTER Mask\nmask == 0 → -inf"
    shape: rectangle
    style.fill: "#E6F3FF"
    style.stroke: "#4A90D9"
    
    after_grid: {
      grid-columns: 5
      grid-gap: 0
      
      a00: "2.1" {style.fill: "#C5E8B7"; style.font: mono; style.font-size: 14}
      a01: "1.8" {style.fill: "#C5E8B7"; style.font: mono; style.font-size: 14}
      a02: "0.5" {style.fill: "#C5E8B7"; style.font: mono; style.font-size: 14}
      a03: "-∞" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14; style.font-color: "#C75050"}
      a04: "-∞" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14; style.font-color: "#C75050"}
    }
  }
  
  weights_final: {
    label: "Attention Weights\n(after softmax)"
    shape: rectangle
    style.fill: "#F5E6FF"
    style.stroke: "#8B5CF6"
    
    weights_grid: {
      grid-columns: 5
      grid-gap: 0
      
      w00: "0.42" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      w01: "0.31" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      w02: "0.27" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      w03: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      w04: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
    }
  }
  
  padding_input -> mask_shape: "create_mask(tokens == PAD)"
  mask_shape -> padding_mask: "unsqueeze(1)\nunsqueeze(2)"
  padding_mask -> scores_before: "broadcast to\n[batch, heads, seq, seq]"
  scores_before -> scores_after: "masked_fill(mask == 0, -inf)"
  scores_after -> weights_final: "softmax(dim=-1)\n-inf → 0.0"
}

causal_section: {
  label: "Panel 2: Causal Mask (Decoder)"
  
  causal_concept: {
    label: |md
      **Causal Constraint:**
      Position i can only attend to positions ≤ i
      (prevents "seeing the future")
    |
    shape: rectangle
    style.fill: "#FFF9E6"
    style.stroke: "#D4A500"
  }
  
  causal_mask: {
    label: "Causal Mask (Lower Triangular)\n1 = can attend, 0 = forbidden"
    shape: rectangle
    style.fill: "#F0F0F0"
    style.stroke: "#666666"
    
    causal_grid: {
      grid-columns: 5
      grid-gap: 0
      
      c00: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c01: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      c02: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      c03: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      c04: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      c10: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c11: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c12: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      c13: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      c14: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      c20: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c21: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c22: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c23: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      c24: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      c30: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c31: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c32: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c33: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c34: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      c40: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c41: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c42: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c43: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      c44: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
    }
  }
  
  position_labels: {
    grid-columns: 1
    grid-rows: 5
    grid-gap: 0
    
    p0: "pos 0" {style.font: mono; style.font-size: 12; style.fill: transparent; style.stroke: transparent}
    p1: "pos 1" {style.font: mono; style.font-size: 12; style.fill: transparent; style.stroke: transparent}
    p2: "pos 2" {style.font: mono; style.font-size: 12; style.fill: transparent; style.stroke: transparent}
    p3: "pos 3" {style.font: mono; style.font-size: 12; style.fill: transparent; style.stroke: transparent}
    p4: "pos 4" {style.font: mono; style.font-size: 12; style.fill: transparent; style.stroke: transparent}
  }
  
  scores_matrix: {
    label: "Attention Weights Matrix\n[seq_len, seq_len]"
    shape: rectangle
    style.fill: "#F5E6FF"
    style.stroke: "#8B5CF6"
    
    att_grid: {
      grid-columns: 5
      grid-gap: 0
      
      att00: "1.0" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att01: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      att02: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      att03: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      att04: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      
      att10: "0.3" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att11: "0.7" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att12: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      att13: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      att14: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      
      att20: "0.1" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att21: "0.2" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att22: "0.7" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att23: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      att24: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      
      att30: "0.15" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att31: "0.25" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att32: "0.3" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att33: "0.3" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att34: "0.0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 14}
      
      att40: "0.1" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att41: "0.1" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att42: "0.2" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att43: "0.3" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
      att44: "0.3" {style.fill: "#D4B8FF"; style.font: mono; style.font-size: 14}
    }
  }
  
  time_arrow: {
    label: |md
      ↑
      Time
    |
    shape: text
    style.font-size: 20
  }
  
  causal_concept -> causal_mask: "torch.tril(\ntorch.ones(seq, seq))"
  causal_mask -> scores_matrix: "Apply mask\n→ softmax"
  position_labels -> causal_mask: "query\npositions"
  time_arrow -> causal_mask: ""
}

broadcast_section: {
  label: "Broadcasting: Mask → Attention Weights"
  
  mask_small: {
    label: "Mask\n[batch, 1, 1, seq]"
    shape: rectangle
    style.fill: "#FFF9E6"
    style.stroke: "#D4A500"
    
    small_grid: {
      grid-columns: 5
      grid-gap: 0
      
      s0: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono; style.font-size: 12}
      s1: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono; style.font-size: 12}
      s2: "1" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono; style.font-size: 12}
      s3: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 12}
      s4: "0" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono; style.font-size: 12}
    }
  }
  
  attention_large: {
    label: "Attention Weights\n[batch, heads, seq, seq]"
    shape: rectangle
    style.fill: "#F5E6FF"
    style.stroke: "#8B5CF6"
    
    large_grid: {
      grid-columns: 5
      grid-gap: 0
      
      l00: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l01: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l02: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l03: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      l04: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      l10: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l11: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l12: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l13: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      l14: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      l20: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l21: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l22: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l23: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      l24: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      l30: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l31: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l32: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l33: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      l34: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      
      l40: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l41: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l42: "✓" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"; style.font: mono}
      l43: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
      l44: "✗" {style.fill: "#FFB4B4"; style.stroke: "#C75050"; style.font: mono}
    }
  }
  
  broadcast_note: {
    label: |md
      **Broadcasting Rules:**
      - Dim 0 (batch): direct copy
      - Dim 1 (heads): broadcast 1→H
      - Dim 2 (query): broadcast 1→S  
      - Dim 3 (key): direct match
    |
    shape: rectangle
    style.fill: "#E8F4FD"
    style.stroke: "#2E86AB"
  }
  
  mask_small -> attention_large: "PyTorch\nbroadcasts\nautomatically" {style.stroke: "#8B5CF6"; style.stroke-width: 2; style.animated: true}
  broadcast_note -> attention_large: "applies to\neach row"
}

code_section: {
  label: "Implementation Code"
  
  padding_code: {
    label: ||py
      # Padding mask: [batch, seq_len] → [batch, 1, 1, seq_len]
      def create_padding_mask(tokens, pad_id):
          mask = (tokens != pad_id)
          return mask.unsqueeze(1).unsqueeze(2)
      
      # Apply before softmax
      scores = scores.masked_fill(mask == 0, float('-inf'))
    ||
    shape: rectangle
    style.fill: "#1E1E1E"
    style.font-color: "#D4D4D4"
    style.font: mono
  }
  
  causal_code: {
    label: ||py
      # Causal mask: [seq_len, seq_len]
      def create_causal_mask(seq_len, device):
          return torch.tril(
              torch.ones(seq_len, seq_len, device=device)
          ).bool()
      
      # Apply before softmax
      scores = scores.masked_fill(causal_mask == 0, float('-inf'))
    ||
    shape: rectangle
    style.fill: "#1E1E1E"
    style.font-color: "#D4D4D4"
    style.font: mono
  }
  
  combined_code: {
    label: ||py
      # Combined mask (padding AND causal)
      combined_mask = padding_mask & causal_mask
      scores = scores.masked_fill(combined_mask == 0, float('-inf'))
      attention_weights = F.softmax(scores, dim=-1)
      # -inf positions become exactly 0.0
    ||
    shape: rectangle
    style.fill: "#1E1E1E"
    style.font-color: "#D4D4D4"
    style.font: mono
  }
  
  padding_code -> causal_code
  causal_code -> combined_code
}

legend: {
  label: "Legend"
  near: bottom-center
  
  keep: "1 / ✓ = Keep (attend)" {style.fill: "#C5E8B7"; style.stroke: "#6B9B5A"}
  mask: "0 / ✗ / -∞ = Mask (ignore)" {style.fill: "#FFB4B4"; style.stroke: "#C75050"}
  active: "Non-zero weight" {style.fill: "#D4B8FF"; style.stroke: "#8B5CF6"}
}

padding_section -> causal_section: "Same operation,\ndifferent mask logic"
causal_section -> broadcast_section: "Mask shape must\nbroadcast correctly"
broadcast_section -> code_section: "Implementation"