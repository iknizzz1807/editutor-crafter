vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Masked Cross-Entropy Loss
  | {near: top-center}

direction: right

classes: {
  tensor: {
    shape: rectangle
    style: {
      fill: "#E8F4FD"
      stroke: "#2196F3"
      stroke-width: 2
      font: mono
      font-size: 14
    }
  }
  operation: {
    shape: hexagon
    style: {
      fill: "#FFF3E0"
      stroke: "#FF9800"
      stroke-width: 2
    }
  }
  mask_node: {
    style: {
      fill: "#FFEBEE"
      stroke: "#F44336"
      stroke-dash: 4
    }
  }
  data: {
    style: {
      fill: "#E8F5E9"
      stroke: "#4CAF50"
      stroke-width: 2
    }
  }
  result: {
    style: {
      fill: "#F3E5F5"
      stroke: "#9C26B0"
      stroke-width: 3
      bold: true
    }
  }
}

section_inputs: Input Tensors {
  logits: {
    label: "Logits\n[batch, seq, vocab]"
    class: tensor
    tooltip: "Raw model outputs\nExample: [2, 5, 10000]"
  }
  
  targets: {
    label: "Targets\n[batch, seq]"
    class: tensor
    tooltip: "Ground truth token IDs\nContains PAD tokens (e.g., 0)"
  }
  
  padding_mask: {
    label: "Padding Mask\n[batch, seq]"
    class: tensor
    style.fill: "#FFCDD2"
    tooltip: "1 = real token\n0 = padding token"
  }
}

section_compute: Loss Computation Pipeline {
  shift_logits: {
    label: "Shift for Autoregressive\nlogits[:, :-1, :]"
    class: operation
    tooltip: "Remove last position\n(model predicts next token)"
  }
  
  shift_targets: {
    label: "Shift Targets\ntargets[:, 1:]"
    class: operation
  }
  
  shift_mask: {
    label: "Shift Mask\npadding_mask[:, 1:]"
    class: operation
  }
  
  log_softmax: {
    label: "Log-Softmax\nlog(softmax(logits))"
    class: operation
    tooltip: "Numerically stable:\nlog_softmax = logits - max - log(sum(exp))"
  }
  
  gather: {
    label: "Gather Target Log-Probs\n-log_prob[target_id]"
    class: operation
    tooltip: "Extract log-prob\nof correct token"
  }
  
  apply_mask: {
    label: "Apply Padding Mask\nloss * mask"
    class: mask_node
    tooltip: "Zero out loss at\npadding positions"
  }
  
  mean: {
    label: "Mean over\nvalid tokens"
    class: result
    tooltip: "Average loss only over\nnon-padding positions"
  }
}

section_output: Output {
  final_loss: {
    label: "Scalar Loss"
    class: result
    style: {
      fill: "#CE93D8"
      font-size: 18
    }
  }
}

logits -> shift_logits: "[dim=-1]"
targets -> shift_targets
padding_mask -> shift_mask

shift_logits -> log_softmax: "[batch, seq-1, vocab]"
log_softmax -> gather: "[batch, seq-1, vocab]"
shift_targets -> gather: "indices"

gather -> apply_mask: "[batch, seq-1]\nper-token losses"
shift_mask -> apply_mask: "broadcast"

apply_mask -> mean: "masked losses\nsum / valid_count"
mean -> final_loss

section_detail: Detail View - Masking Effect {
  before_mask: {
    label: "Before Mask\n[batch, seq-1]"
    class: tensor
    
    example_before: {
      label: |md
        token:  [The, cat, sat, `<PAD>`, `<PAD>`]
        loss:   [2.1, 0.8, 1.5, 3.2, 4.1]
      |
      shape: text
      style.font: mono
      style.font-size: 12
    }
  }
  
  after_mask: {
    label: "After Mask\n[batch, seq-1]"
    class: tensor
    style.fill: "#C8E6C9"
    
    example_after: {
      label: |md
        token:  [The, cat, sat, `<PAD>`, `<PAD>`]
        loss:   [2.1, 0.8, 1.5, **0.0**, **0.0**]
      |
      shape: text
      style.font: mono
      style.font-size: 12
    }
  }
  
  mask_values: {
    label: "Mask Values"
    class: tensor
    style.fill: "#FFCDD2"
    
    mask_ex: {
      label: |md
        mask:   [1,   1,   1,   0,       0]
      |
      shape: text
      style.font: mono
      style.font-size: 12
    }
  }
  
  before_mask -> after_mask: "elementwise Ã— mask"
  mask_values -> after_mask
}

legend: {
  near: bottom-center
  shape: rectangle
  style: {
    fill: transparent
    stroke: "#666"
    stroke-dash: 2
  }
  
  legend_title: Legend {
    shape: text
    style.bold: true
  }
  
  t1: Tensor/Data {class: tensor}
  t2: Operation {class: operation}
  t3: Masking Step {class: mask_node}
  t4: Final Result {class: result}
}

formula: |md
  ## Mathematical Formulation
  
  
  # Per-token cross-entropy loss
  loss_t = -log(softmax(logits_t)[target_t])
  
  # Apply padding mask
  masked_loss_t = loss_t * mask_t
  
  # Mean over valid tokens only
  total_loss = sum(masked_loss) / sum(mask)
  
  
  **Key insight**: `sum(mask)` counts only non-padding tokens,
  ensuring gradient magnitudes are independent of sequence length.
| {near: center-left}