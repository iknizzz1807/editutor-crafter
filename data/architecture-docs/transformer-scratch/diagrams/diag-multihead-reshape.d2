vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # Multi-Head Attention: Tensor Reshaping Pipeline
  ## The Dimension Permutation That Enables Parallel Head Computation
| {near: top-center}

classes: {
  tensor: {
    shape: rectangle
    style: {
      fill: "#E8F4F8"
      stroke: "#2B6CB0"
      stroke-width: 2
      font: mono
      font-size: 14
    }
  }
  operation: {
    shape: hexagon
    style: {
      fill: "#FED7D7"
      stroke: "#C53030"
      stroke-width: 2
      font-size: 13
    }
  }
  dim_label: {
    shape: text
    style: {
      font-size: 11
      font-color: "#4A5568"
    }
  }
  data_flow: {
    style: {
      stroke: "#3182CE"
      stroke-width: 2
      animated: true
    }
  }
  reshape_op: {
    shape: diamond
    style: {
      fill: "#FEFCBF"
      stroke: "#D69E2E"
      stroke-width: 2
    }
  }
}

input: "[batch, seq_len, d_model]" {
  class: tensor
  width: 180
  height: 60
  link: "#input-tensor"
}

input_label: ||md
  **Input Embedding**
  - batch = 2
  - seq_len = 512
  - d_model = 512
||
input_label.class: dim_label

input -> W_QKV: "Linear Projections" {
  class: data_flow
}

W_QKV: "W_Q, W_K, W_V" {
  class: operation
  width: 140
}

W_QKV_label: ||md
  **3 Parallel Projections**
  Each: `nn.Linear(d_model, d_model)`
  Output: `[batch, seq, d_model]`
||
W_QKV_label.class: dim_label

W_QKV -> projected: "Q, K, V tensors" {
  class: data_flow
}

projected: "[batch, seq_len, h×d_k]" {
  class: tensor
  width: 180
  height: 60
  link: "#projected-tensor"
}

projected_label: ||md
  **Projected (h=8 heads)**
  - d_k = d_model / h = 64
  - h × d_k = 8 × 64 = 512
  - Same d_model, different semantic
||
projected_label.class: dim_label

projected -> reshape1: "view()" {
  class: data_flow
  style.stroke-dash: 3
}

reshape1: "RESHAPE" {
  class: reshape_op
  width: 100
  link: "#reshape-operation"
}

reshape1_label: ||md
  **Split Heads**
  `.view(batch, seq, h, d_k)`
  Last dim becomes h × d_k
||
reshape1_label.class: dim_label

reshape1 -> split_heads: {
  class: data_flow
}

split_heads: "[batch, seq_len, h, d_k]" {
  class: tensor
  width: 180
  height: 60
}

split_heads_label: ||md
  **Heads Unpacked**
  h=8 heads visible in dim 2
  Each head: d_k=64 dimensions
||
split_heads_label.class: dim_label

split_heads -> transpose1: "transpose(1,2)" {
  class: data_flow
  style.stroke-dash: 3
}

transpose1: "TRANSPOSE" {
  class: reshape_op
  width: 100
  link: "#transpose-operation"
}

transpose1_label: ||md
  **Move heads forward**
  `.transpose(1, 2)`
  Swap seq_len ↔ h
||
transpose1_label.class: dim_label

transpose1 -> parallel_heads: {
  class: data_flow
}

parallel_heads: "[batch, h, seq_len, d_k]" {
  class: tensor
  width: 180
  height: 60
  style.fill: "#C6F6D5"
  style.stroke: "#276749"
  link: "#parallel-tensor"
}

parallel_heads_label: ||md
  **Parallel Computation Ready!**
  Each head: [batch, seq, d_k]
  Matmul broadcasts over h dimension
  All 8 heads computed simultaneously
||
parallel_heads_label.class: dim_label

parallel_heads -> attention_block: {
  class: data_flow
  style.stroke: "#276749"
}

attention_block: "Scaled Dot-Product\nAttention (per head)" {
  class: operation
  width: 160
  style.fill: "#BEE3F8"
  style.stroke: "#2B6CB0"
  link: "#attention-computation"
}

attention_block_label: ||md
  **Attention Math**
  
  scores = Q @ K^T / √d_k
  weights = softmax(scores)
  output = weights @ V
  
  Shape: [batch, h, seq, d_k]
||
attention_block_label.class: dim_label

attention_block -> attn_output: {
  class: data_flow
  style.stroke: "#276749"
}

attn_output: "[batch, h, seq_len, d_k]" {
  class: tensor
  width: 180
  height: 60
  style.fill: "#C6F6D5"
  style.stroke: "#276749"
}

attn_output_label: ||md
  **Attention Output**
  Same shape as input to attention
  8 independent context vectors
||
attn_output_label.class: dim_label

attn_output -> transpose2: "transpose(1,2)" {
  class: data_flow
  style.stroke-dash: 3
}

transpose2: "TRANSPOSE" {
  class: reshape_op
  width: 100
}

transpose2_label: ||md
  **Restore sequence dim**
  `.transpose(1, 2)`
  Back to [batch, seq, h, d_k]
||
transpose2_label.class: dim_label

transpose2 -> concat_heads: {
  class: data_flow
}

concat_heads: "[batch, seq_len, h, d_k]" {
  class: tensor
  width: 180
  height: 60
}

concat_heads -> reshape2: "contiguous().view()" {
  class: data_flow
  style.stroke-dash: 3
}

reshape2: "RESHAPE" {
  class: reshape_op
  width: 100
}

reshape2_label: ||md
  **Concatenate Heads**
  `.view(batch, seq, h×d_k)`
  Merge last 2 dims
||
reshape2_label.class: dim_label

reshape2 -> merged: {
  class: data_flow
}

merged: "[batch, seq_len, d_model]" {
  class: tensor
  width: 180
  height: 60
}

merged -> W_O: {
  class: data_flow
}

W_O: "W_O" {
  class: operation
  width: 100
}

W_O_label: ||md
  **Output Projection**
  `nn.Linear(d_model, d_model)`
  Mix information across heads
||
W_O_label.class: dim_label

W_O -> output: {
  class: data_flow
}

output: "[batch, seq_len, d_model]" {
  class: tensor
  width: 180
  height: 60
  style.fill: "#E9D8FD"
  style.stroke: "#6B46C1"
  link: "#output-tensor"
}

output_label: ||md
  **Final Output**
  Ready for next layer
  Same shape as input
||
output_label.class: dim_label

legend: {
  near: bottom-center
  grid-columns: 4
  grid-gap: 20
  
  leg_tensor: "Tensor" {
    class: tensor
    width: 80
    height: 30
  }
  leg_op: "Linear" {
    class: operation
    width: 80
    height: 30
  }
  leg_reshape: "Reshape" {
    class: reshape_op
    width: 80
    height: 30
  }
  leg_parallel: "Parallel" {
    class: tensor
    width: 80
    height: 30
    style.fill: "#C6F6D5"
    style.stroke: "#276749"
  }
}

memory_layout: {
  near: top-left
  ml_title: "Memory Layout: Why Transpose Matters" {
    shape: text
    style.font-size: 16
    style.bold: true
  }
  
  before: ||md
    **Before transpose(1,2):**
    
    [batch=2, seq=512, h=8, d_k=64]
    
    Memory: tokens grouped by position
    Position 0: [h0_d0...h0_d63, h1_d0...h7_d63]
    Position 1: [h0_d0...h0_d63, h1_d0...h7_d63]
    ...
    
    Head data is **scattered** across memory
||
  
  after: ||md
    **After transpose(1,2):**
    
    [batch=2, h=8, seq=512, d_k=64]
    
    Memory: tokens grouped by head
    Head 0: [pos0_d0...pos0_d63, pos1_d0...pos511_d63]
    Head 1: [pos0_d0...pos0_d63, pos1_d0...pos511_d63]
    ...
    
    Head data is **contiguous** — enables parallel matmul
||
  
  before -> after: "transpose(1,2)" {
    style: {
      stroke: "#D69E2E"
      stroke-width: 2
    }
  }
}

code_example: {
  near: top-right
  ce_title: "PyTorch Implementation" {
    shape: text
    style.font-size: 16
    style.bold: true
  }
  
  code: ||python
    class MultiHeadAttention(nn.Module):
        def __init__(self, d_model, h):
            super().__init__()
            self.h = h
            self.d_k = d_model // h
            
            self.W_Q = nn.Linear(d_model, d_model)
            self.W_K = nn.Linear(d_model, d_model)
            self.W_V = nn.Linear(d_model, d_model)
            self.W_O = nn.Linear(d_model, d_model)
        
        def forward(self, x):
            batch, seq, _ = x.shape
            
            # 1. Project
            Q = self.W_Q(x)  # [batch, seq, d_model]
            K = self.W_K(x)
            V = self.W_V(x)
            
            # 2. Reshape for parallel heads
            Q = Q.view(batch, seq, self.h, self.d_k)
            Q = Q.transpose(1, 2)  # [batch, h, seq, d_k]
            
            K = K.view(batch, seq, self.h, self.d_k)
            K = K.transpose(1, 2)
            
            V = V.view(batch, seq, self.h, self.d_k)
            V = V.transpose(1, 2)
            
            # 3. Attention (broadcasts over h)
            scores = Q @ K.transpose(-2, -1) / sqrt(self.d_k)
            weights = F.softmax(scores, dim=-1)
            attn_out = weights @ V  # [batch, h, seq, d_k]
            
            # 4. Reshape back
            attn_out = attn_out.transpose(1, 2)
            attn_out = attn_out.contiguous()
            attn_out = attn_out.view(batch, seq, -1)
            
            # 5. Output projection
            return self.W_O(attn_out)
||
}

shape_summary: {
  near: bottom-right
  ss_title: "Shape Transformation Summary" {
    shape: text
    style.font-size: 16
    style.bold: true
  }
  
  table: ||md
    | Step | Operation | Shape |
    |------|-----------|-------|
    | Input | - | `[B, L, D]` |
    | Project | `W_Q(x)` | `[B, L, D]` |
    | Split | `.view(B, L, H, D/H)` | `[B, L, H, d_k]` |
    | **Transpose** | `.transpose(1,2)` | `[B, H, L, d_k]` |
    | Attention | `softmax(QK^T/√d_k)V` | `[B, H, L, d_k]` |
    | Transpose | `.transpose(1,2)` | `[B, L, H, d_k]` |
    | Merge | `.view(B, L, D)` | `[B, L, D]` |
    | Output | `W_O(x)` | `[B, L, D]` |
    
    **B=batch, L=seq_len, D=d_model, H=num_heads, d_k=D/H**
||
}