vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Pre-LN vs Post-LN: Gradient Flow Comparison
  ## Why Pre-LayerNorm Enables Training Deep Transformers
| {near: top-center}

direction: right

PostLN: {
  label: Post-LayerNorm\n(Vaswani et al. 2017)
  style.fill: "#FFE4E1"
  style.stroke: "#CD5C5C"
  style.stroke-width: 3
  
  post_input: xₗ {
    style.fill: "#E8E8E8"
  }
  
  post_attn_block: Attention\nBlock {
    style.fill: "#87CEEB"
  }
  
  post_add1: + {
    shape: circle
    style.fill: "#FFFFFF"
  }
  
  post_ln1: LayerNorm {
    style.fill: "#FFA07A"
    style.stroke: "#CD5C5C"
    style.stroke-width: 3
  }
  
  post_ffn_block: FFN\nBlock {
    style.fill: "#87CEEB"
  }
  
  post_add2: + {
    shape: circle
    style.fill: "#FFFFFF"
  }
  
  post_ln2: LayerNorm {
    style.fill: "#FFA07A"
    style.stroke: "#CD5C5C"
    style.stroke-width: 3
  }
  
  post_output: xₗ₊₁ {
    style.fill: "#E8E8E8"
  }
  
  post_input -> post_attn_block
  post_attn_block -> post_add1
  post_input -> post_add1: residual {
    style.stroke-dash: 3
    style.stroke: "#228B22"
  }
  post_add1 -> post_ln1
  post_ln1 -> post_ffn_block
  post_ffn_block -> post_add2
  post_ln1 -> post_add2: residual {
    style.stroke-dash: 3
    style.stroke: "#228B22"
  }
  post_add2 -> post_ln2
  post_ln2 -> post_output
}

PreLN: {
  label: Pre-LayerNorm\n(Stable for Deep Models)
  style.fill: "#E0FFE0"
  style.stroke: "#228B22"
  style.stroke-width: 3
  
  pre_input: xₗ {
    style.fill: "#E8E8E8"
  }
  
  pre_ln1: LayerNorm {
    style.fill: "#90EE90"
    style.stroke: "#228B22"
    style.stroke-width: 2
  }
  
  pre_attn_block: Attention\nBlock {
    style.fill: "#87CEEB"
  }
  
  pre_add1: + {
    shape: circle
    style.fill: "#FFFFFF"
  }
  
  pre_ln2: LayerNorm {
    style.fill: "#90EE90"
    style.stroke: "#228B22"
    style.stroke-width: 2
  }
  
  pre_ffn_block: FFN\nBlock {
    style.fill: "#87CEEB"
  }
  
  pre_add2: + {
    shape: circle
    style.fill: "#FFFFFF"
  }
  
  pre_output: xₗ₊₁ {
    style.fill: "#E8E8E8"
  }
  
  pre_input -> pre_ln1
  pre_ln1 -> pre_attn_block
  pre_attn_block -> pre_add1
  pre_input -> pre_add1: residual {
    style.stroke-dash: 3
    style.stroke: "#228B22"
    style.stroke-width: 3
  }
  pre_add1 -> pre_ln2
  pre_ln2 -> pre_ffn_block
  pre_ffn_block -> pre_add2
  pre_add1 -> pre_add2: residual {
    style.stroke-dash: 3
    style.stroke: "#228B22"
    style.stroke-width: 3
  }
  pre_add2 -> pre_output
}

PostLN <-> PreLN

gradient_box: {
  label: Gradient Flow Analysis
  style.fill: "#F5F5F5"
  style.stroke: "#666666"
  style.stroke-dash: 5
  
  post_grad: |md
    ### Post-LN Gradient Path
    
    ∂L/∂xₗ → LayerNorm → + → LayerNorm → + → ...
    
    
    **Problem**: Gradient MUST flow through LayerNorm
    - LayerNorm introduces scaling factor σ⁻¹
    - When σ is small → gradient amplification
    - When σ is large → gradient vanishing
    - Creates **gradient bottleneck**
    - Deep models (>12 layers) become unstable
  | {
    style.fill: "#FFE4E1"
  }
  
  pre_grad: |md
    ### Pre-LN Gradient Path
    
    ∂L/∂xₗ₊₁ → + → + → ... (clean residual)
                  ↓
               LayerNorm (parallel)
    
    
    **Advantage**: Gradient has CLEAN residual path
    - Direct gradient highway via residuals
    - LayerNorm gradients flow in parallel
    - No multiplicative bottleneck
    - **Enables training 100+ layer transformers**
  | {
    style.fill: "#E0FFE0"
  }
  
  post_grad -> pre_grad
}

equation_box: {
  style.fill: "#FFFACD"
  
  post_eq: |md
    **Post-LN Forward:**
    
    x' = x + Attention(x)
    out = LayerNorm(x')
    
    Gradient: ∂L/∂x = ∂L/∂out · ∂LN/∂x'
  |
  
  pre_eq: |md
    **Pre-LN Forward:**
    
    x' = x + Attention(LN(x))
    out = x'  (no final LN on output)
    
    Gradient: ∂L/∂x = ∂L/∂out · I (identity!)
  |
  
  post_eq <-> pre_eq
}

legend: {
  near: bottom-center
  style.fill: "transparent"
  
  residual_path: Residual Path {
    style.stroke: "#228B22"
    style.stroke-dash: 3
    shape: text
  }
  
  ln_bottleneck: LayerNorm (Potential Bottleneck) {
    style.fill: "#FFA07A"
    shape: text
  }
  
  ln_safe: LayerNorm (Safe Position) {
    style.fill: "#90EE90"
    shape: text
  }
}

PostLN -> gradient_box: "Analyze\ngradient\nflow" {
  style.stroke: "#CD5C5C"
  style.stroke-width: 2
}

PreLN -> gradient_box: "Analyze\ngradient\nflow" {
  style.stroke: "#228B22"
  style.stroke-width: 2
}

gradient_box -> equation_box: "Mathematical\nformulation"

key_insight: |md
  ## Key Insight
  
  In **Post-LN**, the residual connection is **inside** the LayerNorm:
  
  output = LayerNorm(x + Sublayer(x))
  
  Gradient must pass through LayerNorm's σ⁻¹ scaling.
  
  In **Pre-LN**, the residual connection is **outside**:
  
  output = x + Sublayer(LayerNorm(x))
  
  Gradient flows through identity path → stable training.
| {near: bottom-center}

depth_comparison: {
  near: center-right
  
  shallow: |md
    ### Shallow Models (<12 layers)
    Post-LN works fine
    - Gradient still manageable
    - Original transformer used this
  |
  
  deep: |md
    ### Deep Models (12-100+ layers)
    Pre-LN essential
    - GPT-2/3, BERT-large use Pre-LN
    - Without it: gradient explosion/vanishing
  |
  
  shallow -> deep: "Scale up"
}