vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    encoder: "#E3F2FD"
    decoder: "#FFF3E0"
    attention: "#F3E5F5"
    embed: "#E8F5E9"
    output: "#FFEBEE"
    active: "#4CAF50"
    flow: "#2196F3"
    cross_flow: "#FF9800"
  }
}

title: |md
  # Transformer Architecture Map
  **Satellite View — Complete Data Flow**
| {near: top-center}

direction: right

# === INPUT SIDE ===
inputs: Input Tokens {
  style.fill: ${colors.embed}
  style.stroke: "#388E3C"
}

embeddings: Token Embeddings\n+ Positional Encoding {
  style.fill: ${colors.embed}
  style.stroke: "#388E3C"
  link: "#anchor-positional-encoding"
}

inputs -> embeddings: "tokenize + embed"

# === ENCODER STACK ===
encoder: Encoder Stack {
  style.fill: ${colors.encoder}
  style.stroke: "#1565C0"
  
  enc_self_attn: Multi-Head\nSelf-Attention {
    style.fill: ${colors.attention}
    link: "#anchor-m2-multihead"
  }
  
  enc_ffn: Feed-Forward\nNetwork {
    style.fill: ${colors.encoder}
    link: "#anchor-m3-ffn"
  }
  
  enc_residual: "Add & Norm\n×2" {
    style.fill: "#BBDEFB"
  }
  
  enc_self_attn <-> enc_residual: "residual\nconnection"
  enc_residual <-> enc_ffn: "residual\nconnection"
}

encoder.link: "#anchor-m4-encoder-stack"

enc_note: |md
  ×N layers
| {near: top-right}

embeddings -> encoder.enc_self_attn: "encoder input"

# === CROSS-ATTENTION BRIDGE ===
cross_attn: Cross-Attention\nBridge {
  style.fill: ${colors.attention}
  style.stroke: ${colors.cross_flow}
  style.stroke-width: 3
  link: "#anchor-m4-cross-attention"
  
  K_V: "K, V from\nencoder"
  Q: "Q from\ndecoder"
  
  K_V -> Q: "attention\nmechanism"
}

encoder.enc_ffn -> cross_attn.K_V: "encoder output\nprovides K, V" {
  style.stroke: ${colors.cross_flow}
  style.stroke-width: 3
  style.animated: true
}

# === DECODER STACK ===
decoder: Decoder Stack {
  style.fill: ${colors.decoder}
  style.stroke: "#E65100"
  
  dec_masked_attn: Masked Multi-Head\nSelf-Attention {
    style.fill: ${colors.attention}
    link: "#anchor-m2-multihead"
  }
  
  dec_cross_attn: Cross-Attention\n(from encoder) {
    style.fill: ${colors.attention}
    style.stroke: ${colors.cross_flow}
  }
  
  dec_ffn: Feed-Forward\nNetwork {
    style.fill: ${colors.decoder}
    link: "#anchor-m3-ffn"
  }
  
  dec_residual: "Add & Norm\n×3" {
    style.fill: "#FFE0B2"
  }
  
  dec_masked_attn <-> dec_residual
  dec_residual <-> dec_cross_attn
  dec_cross_attn <-> dec_ffn
}

decoder.link: "#anchor-m5-decoder-stack"

causal_label: |md
  **Causal Mask**
  (no peeking ahead)
| {near: bottom-center}

embeddings -> decoder.dec_masked_attn: "shifted output\nembeddings" {
  style.stroke-dash: 5
  style.stroke: "#888"
}

cross_attn.Q -> decoder.dec_cross_attn: "Q joins cross-attention" {
  style.stroke: ${colors.cross_flow}
  style.stroke-dash: 3
}

# === OUTPUT SIDE ===
output_proj: Linear Projection\nto Vocabulary {
  style.fill: ${colors.output}
  style.stroke: "#C62828"
  link: "#anchor-m6-output"
}

softmax: Softmax {
  style.fill: ${colors.output}
  shape: circle
}

output_tokens: Output Tokens {
  style.fill: ${colors.output}
  style.stroke: "#C62828"
}

decoder.dec_ffn -> output_proj
output_proj -> softmax
softmax -> output_tokens: "predicted token"

# === GENERATION LOOP ===
generation_loop: Autoregressive\nGeneration Loop {
  style.fill: transparent
  style.stroke: "#9C27B0"
  style.stroke-dash: 5
  link: "#anchor-m6-generation"
}

output_tokens -> generation_loop: "sampled token"
generation_loop -> decoder.dec_masked_attn: "append to input\nfor next step" {
  style.stroke: "#9C27B0"
  style.stroke-dash: 5
  style.animated: true
}

# === MILESTONE ANCHORS ===
milestones: |md
  ## Milestone Anchors
  
  - **M1**: Scaled Dot-Product Attention
  - **M2**: Multi-Head Attention  
  - **M3**: Feed-Forward + Residuals
  - **M4**: Encoder-Decoder Architecture
  - **M5**: Training Pipeline
  - **M6**: Inference & Generation
| {near: bottom-left}

# === LEGEND ===
legend: Legend {
  near: bottom-right
  
  self_attn: Self-Attention {
    style.fill: ${colors.attention}
    shape: rectangle
  }
  
  cross_attn_leg: Cross-Attention {
    style.fill: ${colors.attention}
    style.stroke: ${colors.cross_flow}
    style.stroke-width: 2
    shape: rectangle
  }
  
  data_flow: "→ Data Flow"
  gen_loop: "⤴ Generation Loop"
  
  self_attn -> cross_attn_leg
}