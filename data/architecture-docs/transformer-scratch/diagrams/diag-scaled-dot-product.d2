vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    query: "#4A90D9"
    key: "#50C878"
    value: "#FFB347"
    attention: "#9B59B6"
    output: "#E74C3C"
    mask: "#95A5A6"
  }
}

title: |md
  # Scaled Dot-Product Attention
  ### The Core Operation Behind Every Transformer
| {near: top-center}

direction: right

stage1: Input Projections {
  style.fill: "#F8F9FA"
  style.stroke: "#DEE2E6"
  
  Q: Query (Q) {
    shape: rectangle
    width: 140
    height: 80
    style.fill: ${colors.query}
    style.stroke: "#2E5A8B"
    style.font-color: white
    style.bold: true
  }
  
  K: Key (K) {
    shape: rectangle
    width: 140
    height: 80
    style.fill: ${colors.key}
    style.stroke: "#2E7D4D"
    style.font-color: white
    style.bold: true
  }
  
  V: Value (V) {
    shape: rectangle
    width: 140
    height: 80
    style.fill: ${colors.value}
    style.stroke: "#CC8A2E"
    style.font-color: white
    style.bold: true
  }
  
  Q_shape: |md
    `[batch, seq_len, d_k]`
  |
  
  K_shape: |md
    `[batch, seq_len, d_k]`
  |
  
  V_shape: |md
    `[batch, seq_len, d_v]`
  |
}

stage2: Dot Product {
  style.fill: "#FEF3E2"
  style.stroke: "#F0AD4E"
  
  K_transpose: K^T {
    shape: rectangle
    width: 120
    height: 80
    style.fill: ${colors.key}
    style.stroke: "#2E7D4D"
    style.font-color: white
    style.opacity: 0.7
  }
  
  K_T_shape: |md
    `[batch, d_k, seq_len]`
  |
  
  scores_raw: Raw Scores {
    shape: rectangle
    width: 160
    height: 100
    style.fill: ${colors.attention}
    style.stroke: "#6C3483"
    style.font-color: white
    style.bold: true
    label: "QK^T\n(unscaled)"
  }
  
  scores_raw_shape: |md
    `[batch, seq_len, seq_len]`
  |
  
  warning_box: |md
    ⚠️ **Problem**: Values grow with d_k
    - d_k=512 → scores range [-50, +50]
    - Large inputs → softmax saturation
    - Gradients vanish → training fails
  | {
    style.fill: "#FDEDEC"
    style.stroke: "#E74C3C"
    style.font-size: 14
  }
}

stage3: Scale Operation {
  style.fill: "#E8F6F3"
  style.stroke: "#1ABC9C"
  
  scale_factor: Scale Factor {
    shape: diamond
    width: 100
    height: 100
    style.fill: "#1ABC9C"
    style.stroke: "#16A085"
    style.font-color: white
    style.bold: true
    label: "÷ √d_k"
  }
  
  scale_explanation: |md
    **Why √d_k?**
    
    If Q, K elements have variance σ²:
    - Dot product variance = d_k × σ²
    - Scaling by 1/√d_k normalizes to σ²
    
    **Result**: Stable softmax inputs
  | {
    style.fill: "#D5F5E3"
    style.stroke: "#27AE60"
  }
  
  scores_scaled: Scaled Scores {
    shape: rectangle
    width: 160
    height: 100
    style.fill: ${colors.attention}
    style.stroke: "#6C3483"
    style.font-color: white
    style.bold: true
    label: "QK^T / √d_k"
  }
  
  scores_scaled_shape: |md
    `[batch, seq_len, seq_len]`
  |
  
  comparison: |md
    **Before scaling**: [-50, +50]
    **After scaling**: [-2.2, +2.2]
    
    ✓ Softmax produces distributed weights
    ✓ Gradients remain healthy
  | {
    style.fill: "#D5F5E3"
    style.stroke: "#27AE60"
  }
}

stage4: Masking {
  style.fill: "#F4F6F7"
  style.stroke: ${colors.mask}
  
  mask_input: Mask? {
    shape: diamond
    width: 80
    height: 80
    style.fill: ${colors.mask}
    style.stroke: "#7F8C8D"
    style.font-color: white
  }
  
  mask_types: Mask Types {
    style.fill: white
    style.stroke: ${colors.mask}
    
    padding: Padding Mask {
      style.fill: "#EBF5FB"
      style.stroke: "#3498DB"
    }
    
    causal: Causal Mask {
      style.fill: "#FDEDEC"
      style.stroke: "#E74C3C"
    }
  }
  
  padding_explain: |md
    **Padding**: Set `PAD` positions to `-inf`
    - Before softmax
    - Becomes 0.0 after softmax
  | {style.fill: "#EBF5FB"}
  
  causal_explain: |md
    **Causal**: Set future positions to `-inf`
    - Upper triangular
    - Decoder can't peek ahead
  | {style.fill: "#FDEDEC"}
  
  mask_apply: Apply Mask {
    shape: rectangle
    width: 160
    height: 80
    style.fill: ${colors.mask}
    style.stroke: "#7F8C8D"
    style.font-color: white
    label: "scores.masked_fill(\n  mask, -inf)"
  }
  
  masked_note: |md
    **Critical**: Mask BEFORE softmax
    Softmax converts -inf → 0.0
  | {
    style.fill: "#FDEDEC"
    style.stroke: "#E74C3C"
  }
}

stage5: Softmax {
  style.fill: "#F5EEF8"
  style.stroke: "#8E44AD"
  
  softmax_op: Softmax {
    shape: rectangle
    width: 160
    height: 80
    style.fill: ${colors.attention}
    style.stroke: "#6C3483"
    style.font-color: white
    style.bold: true
    label: "softmax(dim=-1)"
  }
  
  softmax_math: |md
    softmax(x_i) = e^(x_i - max(x)) / Σ_j e^(x_j - max(x))
    
    **Numerical Stability Trick**:
    Subtract max before exponentiating
    - Prevents overflow (e^1000 → inf)
    - PyTorch does this automatically
  | {
    style.fill: "#E8DAEF"
    style.stroke: "#8E44AD"
  }
  
  weights: Attention Weights {
    shape: rectangle
    width: 160
    height: 100
    style.fill: ${colors.attention}
    style.stroke: "#6C3483"
    style.font-color: white
    style.bold: true
    label: "Attention\nWeights (A)"
  }
  
  weights_shape: |md
    `[batch, seq_len, seq_len]`
    **Rows sum to 1.0**
  |
}

stage6: Weighted Sum {
  style.fill: "#FDF2E9"
  style.stroke: ${colors.output}
  
  multiply_op: A × V {
    shape: rectangle
    width: 160
    height: 80
    style.fill: ${colors.output}
    style.stroke: "#C0392B"
    style.font-color: white
    style.bold: true
  }
  
  multiply_explain: |md
    **Weighted Average of Values**
    
    For position i:
    output_i = Σ_j (A_ij × V_j)
    
    - A_ij: How much i attends to j
    - V_j: Value at position j
    - Result: Context-aware representation
  | {
    style.fill: "#FDEBD0"
    style.stroke: "#E67E22"
  }
  
  output: Output {
    shape: rectangle
    width: 160
    height: 100
    style.fill: ${colors.output}
    style.stroke: "#C0392B"
    style.font-color: white
    style.bold: true
    label: "Context\nVectors"
  }
  
  output_shape: |md
    `[batch, seq_len, d_v]`
    **Same shape as input!**
  |
}

summary: Complete Formula {
  near: bottom-center
  style.fill: "#2C3E50"
  style.stroke: "#1A252F"
  style.font-color: white
  
  formula: |md
    Attention(Q, K, V) = softmax(QK^T / √d_k) × V
  | {
    style.font-size: 24
    style.fill: "#34495E"
    style.font-color: white
  }
  
  key_points: |md
    ### Key Insights
    1. **No learnable parameters** in attention itself—only in Q, K, V projections
    2. **O(n²) memory**: Must store [seq_len, seq_len] attention weights
    3. **Gradient flows** through attention proportional to attention weights
    4. **Masking enables** both padding handling and autoregressive generation
  | {
    style.fill: "#34495E"
    style.font-color: "#ECF0F1"
  }
}

flow_legend: Legend {
  near: top-right
  style.fill: white
  style.stroke: "#BDC3C7"
  
  title: |md
    **Tensor Shapes**
  |
  
  batch: "[batch, ...]" {style.fill: "#ECF0F1"}
  seq: "[..., seq_len, ...]" {style.fill: "#D5DBDB"}
  dk: "[..., d_k/d_v]" {style.fill: "#D5F5E3"}
}

connections: {
  stage1.Q -> stage2.K_transpose: "transpose\n(-2, -1)" {
    style.stroke: ${colors.key}
    style.animated: true
  }
  
  stage1.Q -> stage2.scores_raw: "matmul" {
    style.stroke: ${colors.query}
    style.animated: true
  }
  
  stage2.K_transpose -> stage2.scores_raw: {
    style.stroke: ${colors.key}
    style.animated: true
  }
  
  stage2.scores_raw -> stage3.scale_factor: {
    style.stroke: ${colors.attention}
    style.stroke-width: 3
  }
  
  stage3.scale_factor -> stage3.scores_scaled: {
    style.stroke: "#1ABC9C"
    style.stroke-width: 3
  }
  
  stage3.scores_scaled -> stage4.mask_input: {
    style.stroke: ${colors.attention}
  }
  
  stage4.mask_input -> stage4.mask_apply: "if mask\nprovided" {
    style.stroke: ${colors.mask}
    style.stroke-dash: 3
  }
  
  stage4.mask_apply -> stage5.softmax_op: {
    style.stroke: ${colors.mask}
  }
  
  stage3.scores_scaled -> stage5.softmax_op: "no mask" {
    style.stroke: ${colors.attention}
    style.stroke-dash: 3
  }
  
  stage5.softmax_op -> stage5.weights: {
    style.stroke: ${colors.attention}
    style.stroke-width: 3
  }
  
  stage5.weights -> stage6.multiply_op: {
    style.stroke: ${colors.attention}
    style.animated: true
  }
  
  stage1.V -> stage6.multiply_op: {
    style.stroke: ${colors.value}
    style.animated: true
  }
  
  stage6.multiply_op -> stage6.output: {
    style.stroke: ${colors.output}
    style.stroke-width: 3
  }
}