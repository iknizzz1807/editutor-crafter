vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Softmax Numerical Stability
  ### Why max-subtraction prevents overflow
| {near: top-center}

direction: right

before: {
  label: "NAIVE SOFTMAX (OVERFLOW RISK)"
  style: {
    fill: "#FFE4E1"
    stroke: "#DC143C"
    stroke-width: 3
    border-radius: 8
  }

  logits_b: {
    label: "Input Logits"
    shape: rectangle
    style.fill: white
    style.stroke: "#DC143C"
    width: 200
    
    val1_b: "xâ‚ = 100"
    val2_b: "xâ‚‚ = 101"
    val3_b: "xâ‚ƒ = 102"
  }

  formula_b: {
    label: ||md
      **Formula:**
      `softmax(xáµ¢) = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)`
    ||
    shape: rectangle
    style.fill: "#FFF0F0"
    style.stroke: "#DC143C"
  }

  exp_b: {
    label: "exp(100) â‰ˆ 2.7 Ã— 10â´Â³"
    shape: rectangle
    style.fill: "#FFE4E1"
    style.stroke: "#DC143C"
    width: 200
    
    e1_b: "exp(100) = 2.688e+43"
    e2_b: "exp(101) = 7.307e+43"
    e3_b: "exp(102) = 1.986e+44"
  }

  overflow_b: {
    label: ||md
      ### âš ï¸ OVERFLOW DANGER
      
      If xáµ¢ = 1000:
      - `exp(1000)` = **inf** in float32
      - Result: NaN or inf
      - **Gradients destroyed**
    ||
    shape: diamond
    style: {
      fill: "#FFE4E1"
      stroke: "#DC143C"
      stroke-width: 3
    }
    width: 220
  }

  logits_b -> formula_b: "apply"
  formula_b -> exp_b: "compute\nexponentials"
  exp_b -> overflow_b: "large values\nâ†’ overflow risk"
}

arrow_node: {
  label: "MAX-SUBTRACTION\nTRANSFORMATION"
  shape: rectangle
  style: {
    fill: "#4169E1"
    stroke: "#4169E1"
    font-color: white
    font-size: 14
    bold: true
  }
}

after: {
  label: "STABLE SOFTMAX (NUMERICALLY SAFE)"
  style: {
    fill: "#E8F5E9"
    stroke: "#228B22"
    stroke-width: 3
    border-radius: 8
  }

  logits_a: {
    label: "Input Logits"
    shape: rectangle
    style.fill: white
    style.stroke: "#228B22"
    width: 200
    
    val1_a: "xâ‚ = 100"
    val2_a: "xâ‚‚ = 101"
    val3_a: "xâ‚ƒ = 102"
  }

  subtract_a: {
    label: ||md
      **Step 1: Find max**
      `max(x) = 102`
      
      **Step 2: Subtract max**
      `x' = x - max(x)`
    ||
    shape: rectangle
    style.fill: "#E8F5E9"
    style.stroke: "#228B22"
    width: 200
  }

  shifted_a: {
    label: "Shifted Logits"
    shape: rectangle
    style.fill: white
    style.stroke: "#228B22"
    width: 200
    
    s1_a: "x'â‚ = 100 - 102 = -2"
    s2_a: "x'â‚‚ = 101 - 102 = -1"
    s3_a: "x'â‚ƒ = 102 - 102 = 0"
  }

  formula_a: {
    label: ||md
      **Stable Formula:**
      `softmax(xáµ¢) = exp(xáµ¢ - max) / Î£â±¼ exp(xâ±¼ - max)`
    ||
    shape: rectangle
    style.fill: "#E8F5E9"
    style.stroke: "#228B22"
    width: 240
  }

  exp_a: {
    label: "Safe Exponentials"
    shape: rectangle
    style.fill: white
    style.stroke: "#228B22"
    width: 200
    
    e1_a: "exp(-2) = 0.1353"
    e2_a: "exp(-1) = 0.3679"
    e3_a: "exp(0) = 1.0000"
  }

  result_a: {
    label: "âœ“ STABLE RESULT"
    shape: rectangle
    style.fill: "#C8E6C9"
    style.stroke: "#228B22"
    width: 200
    
    r1_a: "pâ‚ = 0.0900 (9.0%)"
    r2_a: "pâ‚‚ = 0.2447 (24.5%)"
    r3_a: "pâ‚ƒ = 0.6652 (66.5%)"
    r_sum: "Sum = 1.0000 âœ“"
  }

  logits_a -> subtract_a: "find max\n& subtract"
  subtract_a -> shifted_a: "shifted\nvalues"
  shifted_a -> formula_a: "apply"
  formula_a -> exp_a: "compute\nexponentials"
  exp_a -> result_a: "normalize"
}

arrow_node -> after.logits_a

proof: {
  label: ||md
    ## Mathematical Equivalence Proof
    
    **Naive:** `exp(xáµ¢) / Î£â±¼ exp(xâ±¼)`
    
    **Stable:** `exp(xáµ¢ - m) / Î£â±¼ exp(xâ±¼ - m)` where `m = max(x)`
    
    **Proof:**
    
    exp(xáµ¢ - m)     exp(xáµ¢) Â· exp(-m)     exp(xáµ¢)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Î£â±¼ exp(xâ±¼ - m)   Î£â±¼ exp(xâ±¼)Â·exp(-m)    Î£â±¼ exp(xâ±¼)
    
    
    The `exp(-m)` factor cancels out â€” **identical results**!
  ||
  near: bottom-center
  shape: rectangle
  style: {
    fill: "#E3F2FD"
    stroke: "#1976D2"
    border-radius: 8
  }
  width: 600
}

key_insight: {
  label: ||md
    ### ğŸ”‘ KEY INSIGHT
    
    | Logit Range | Naive exp(x) | Stable exp(x - max) |
    |-------------|--------------|---------------------|
    | x = 100     | 2.7 Ã— 10â´Â³   | 0.135 (shifted: -2) |
    | x = 1000    | **OVERFLOW** | 0.135 (shifted: -2) |
    | x = 10000   | **OVERFLOW** | 0.135 (shifted: -2) |
    
    **Any input range becomes [-âˆ, 0] after shift**
    â†’ exp() always in [0, 1]
    â†’ **No overflow possible**
  ||
  near: center-left
  shape: rectangle
  style: {
    fill: "#FFF8E1"
    stroke: "#FF8F00"
    border-radius: 8
  }
  width: 380
}

pytorch_note: {
  label: ||md
    **PyTorch Implementation Note:**
    
    `torch.softmax()` and `F.softmax()` automatically
    apply max-subtraction internally.
    
    python
    # These are equivalent and stable:
    F.softmax(logits, dim=-1)
    
    # Manual stable version:
    shifted = logits - logits.max(dim=-1, keepdim=True)[0]
    exp_shifted = torch.exp(shifted)
    result = exp_shifted / exp_shifted.sum(dim=-1, keepdim=True)
    
  ||
  near: bottom-right
  shape: rectangle
  style: {
    fill: "#F3E5F5"
    stroke: "#7B1FA2"
    border-radius: 8
  }
  width: 350
}

gradient_flow: {
  label: ||md
    ### Why This Matters for Training
    
    **Without stability:**
    - Large logits â†’ inf â†’ NaN gradients
    - Backpropagation stops
    - Model cannot learn
    
    **With max-subtraction:**
    - All exp() values in [0, 1]
    - Gradients flow correctly
    - Training converges
  ||
  near: center-right
  shape: rectangle
  style: {
    fill: "#FFEBEE"
    stroke: "#C62828"
    border-radius: 8
  }
  width: 320
}