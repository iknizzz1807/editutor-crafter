vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Temperature Scaling Effect on Softmax Distributions
| {near: top-center}

direction: right

logits_source: Logits Source {
  shape: rectangle
  style.fill: "#E8E8E8"
  style.stroke: "#666666"
  
  raw_logits: |md
    **Raw Logits**
    
    [2.0, 1.0, 0.5, 0.3, 0.1]
    
  |
  
  note: |md
    These represent unnormalized scores from the model's final layer.
    Higher values = model's preferred tokens.
  | {
    style.font-size: 12
  }
}

temperature_op: Temperature Division {
  shape: diamond
  style.fill: "#FFF3CD"
  style.stroke: "#856404"
  
  formula: |md
    **scaled = logits / T**
  |
  
  params: |md
    - T < 1: Sharpen distribution
    - T = 1: No change (default)
    - T > 1: Flatten distribution
  |
}

low_temp: T = 0.1 (Near-Greedy) {
  style.fill: "#D4EDDA"
  style.stroke: "#155724"
  
  scaled_logits_low: |md
    **Scaled Logits**
    
    [20.0, 10.0, 5.0, 3.0, 1.0]
    
  |
  
  softmax_low: Softmax Output {
    shape: rectangle
    style.fill: "#C3E6CB"
    
    probs_low: |md
      
      [0.9999, 0.0001, 0.0000, 0.0000, 0.0000]
      
    |
  }
  
  interpretation_low: |md
    **Effect: Peaked Distribution**
    
    - Almost all probability on highest logit
    - Near-deterministic selection
    - Use for: factual tasks, code generation
    - Risk: Repetitive, overconfident output
  | {
    style.font-size: 11
  }
  
  scaled_logits_low -> softmax_low: "softmax()"
}

normal_temp: T = 1.0 (Normal) {
  style.fill: "#D1ECF1"
  style.stroke: "#0C5460"
  
  scaled_logits_normal: |md
    **Scaled Logits**
    
    [2.0, 1.0, 0.5, 0.3, 0.1]
    
  |
  
  softmax_normal: Softmax Output {
    shape: rectangle
    style.fill: "#BEE5EB"
    
    probs_normal: |md
      
      [0.502, 0.185, 0.112, 0.092, 0.075]
      
    |
  }
  
  interpretation_normal: |md
    **Effect: Balanced Distribution**
    
    - Preserves model's learned uncertainty
    - Top choice ~50%, others still viable
    - Standard for most generation tasks
    - Good balance of quality & diversity
  | {
    style.font-size: 11
  }
  
  scaled_logits_normal -> softmax_normal: "softmax()"
}

high_temp: T = 2.0 (Flat/Random) {
  style.fill: "#F8D7DA"
  style.stroke: "#721C24"
  
  scaled_logits_high: |md
    **Scaled Logits**
    
    [1.0, 0.5, 0.25, 0.15, 0.05]
    
  |
  
  softmax_high: Softmax Output {
    shape: rectangle
    style.fill: "#F5C6CB"
    
    probs_high: |md
      
      [0.276, 0.221, 0.174, 0.158, 0.143]
      
    |
  }
  
  interpretation_high: |md
    **Effect: Flattened Distribution**
    
    - Nearly uniform probabilities
    - High randomness in selection
    - Use for: creative writing, brainstorming
    - Risk: Incoherent, off-topic output
  | {
    style.font-size: 11
  }
  
  scaled_logits_high -> softmax_high: "softmax()"
}

logits_source -> temperature_op: "raw logits"
temperature_op -> low_temp: "÷ 0.1"
temperature_op -> normal_temp: "÷ 1.0"
temperature_op -> high_temp: "÷ 2.0"

legend: {
  near: bottom-center
  
  shape: rectangle
  style.fill: "#F8F9FA"
  style.stroke: "#DEE2E6"
  
  content: |md
    **Key Insight**: Temperature controls the "entropy" of the output distribution.
    
    - **Low T**: Reduces entropy → confident, deterministic outputs
    - **High T**: Increases entropy → diverse, creative outputs
    
    The mathematical effect: Division by T scales the difference between logits.
    Large T → small differences → soft softmax. Small T → large differences → hard softmax.
  |
}

distribution_comparison: Distribution Visualization {
  grid-columns: 3
  grid-gap: 20
  
  low_bar: {
    shape: rectangle
    width: 50
    height: 200
    style.fill: "#28A745"
    label: ""
  }
  
  low_bar_label: "T=0.1" {
    shape: text
  }
  
  normal_bar: {
    shape: rectangle
    width: 150
    height: 200
    style.fill: "#17A2B8"
    label: ""
  }
  
  normal_bar_label: "T=1.0" {
    shape: text
  }
  
  high_bar: {
    shape: rectangle
    width: 200
    height: 200
    style.fill: "#DC3545"
    label: ""
  }
  
  high_bar_label: "T=2.0" {
    shape: text
  }
}

formula_box: Mathematical Foundation {
  style.fill: "#FFF9E6"
  style.stroke: "#FFCC00"
  
  softmax_formula: |latex
    \text{softmax}(z_i, T) = \frac{e^{z_i/T}}{\sum_{j} e^{z_j/T}}
  |
  
  gradient_note: |md
    **Gradient Effect**: Higher T produces smoother gradients
    during backpropagation, which can help training stability
    (used in label smoothing / knowledge distillation).
  | {
    style.font-size: 11
  }
}

arrow_annotation: |md
  **Temperature (T)** is a hyperparameter that controls randomness in token selection.
  Applied BEFORE softmax, not after.
| {
  near: top-right
  shape: text
  style.font-size: 13
  style.italic: true
}