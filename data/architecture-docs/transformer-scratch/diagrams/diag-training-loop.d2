vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
  colors: {
    input: "#E3F2FD"
    process: "#FFF3E0"
    output: "#E8F5E9"
    loss: "#FFEBEE"
    grad: "#F3E5F5"
    step: "#ECEFF1"
  }
}

title: |md
  # Training Loop with Teacher Forcing
  ## Step-by-Step Transformer Training Iteration
| {near: top-center}

direction: right

# Step 1: Prepare Shifted Targets
step1: "Step 1: Prepare Shifted Targets" {
  style.fill: ${colors.input}
  style.stroke: "#1976D2"
  style.stroke-width: 2
  
  targets: "Original Targets\n[1, 2, 3, 4, 5]" {
    shape: rectangle
    style.fill: white
    style.stroke: "#1976D2"
  }
  
  bos: "<BOS> token" {
    shape: circle
    style.fill: "#FFCDD2"
    style.stroke: "#D32F2F"
  }
  
  concat_op: "Concat\nOperation" {
    shape: diamond
    style.fill: "#BBDEFB"
  }
  
  decoder_input: "Decoder Input\n[<BOS>, 1, 2, 3, 4]\n(shifted right)" {
    shape: rectangle
    style.fill: white
    style.stroke: "#388E3C"
    style.stroke-width: 2
  }
  
  targets_truncated: "Loss Targets\n[1, 2, 3, 4, 5]\n(targets[1:])" {
    shape: rectangle
    style.fill: white
    style.stroke: "#7B1FA2"
    style.stroke-dash: 3
  }
  
  bos -> concat_op: prepend
  targets -> concat_op: "targets[:-1]"
  concat_op -> decoder_input: "decoder_input =\n<BOS> + targets[:-1]"
  targets -> targets_truncated: "targets[1:]\n(for loss)"
}

# Step 2: Forward Pass
step2: "Step 2: Forward Pass" {
  style.fill: ${colors.process}
  style.stroke: "#F57C00"
  style.stroke-width: 2
  
  encoder_input: "Encoder Input\n[batch, src_len, d_model]" {
    style.fill: white
    style.stroke: "#F57C00"
  }
  
  decoder_input_2: "Decoder Input\n[<BOS>, 1, 2, 3, 4]" {
    style.fill: white
    style.stroke: "#388E3C"
  }
  
  encoder: "Encoder\nStack" {
    shape: rectangle
    style.fill: "#FFE0B2"
    style.stroke: "#E65100"
    style.multiple: true
  }
  
  decoder: "Decoder\nStack" {
    shape: rectangle
    style.fill: "#C8E6C9"
    style.stroke: "#2E7D32"
    style.multiple: true
  }
  
  logits: "Logits\n[batch, tgt_len, vocab_size]" {
    style.fill: white
    style.stroke: "#7B1FA2"
  }
  
  encoder_input -> encoder: "src tokens"
  decoder_input_2 -> decoder: "shifted tgt"
  encoder -> decoder: "encoder output\ncross-attention K,V"
  decoder -> logits: projection
}

# Step 3: Compute Loss
step3: "Step 3: Compute Loss" {
  style.fill: ${colors.loss}
  style.stroke: "#D32F2F"
  style.stroke-width: 2
  
  logits_3: "Logits\n[batch, tgt_len, vocab_size]" {
    style.fill: white
    style.stroke: "#7B1FA2"
  }
  
  targets_3: "Loss Targets\n[1, 2, 3, 4, 5]" {
    style.fill: white
    style.stroke: "#7B1FA2"
  }
  
  cross_entropy: CrossEntropyLoss {
    shape: rectangle
    style.fill: "#FFCDD2"
    style.stroke: "#B71C1C"
  }
  
  loss: "Loss Tensor\nscalar" {
    shape: circle
    style.fill: "#FFCDD2"
    style.stroke: "#D32F2F"
    style.stroke-width: 2
  }
  
  logits_3 -> cross_entropy: predictions
  targets_3 -> cross_entropy: "ground truth"
  cross_entropy -> loss: "CrossEntropy(\n  logits.view(-1, vocab),\n  targets.view(-1)\n)"
}

# Step 4: Backward Pass
step4: "Step 4: Backward Pass" {
  style.fill: ${colors.grad}
  style.stroke: "#7B1FA2"
  style.stroke-width: 2
  
  loss_4: Loss {
    shape: circle
    style.fill: "#FFCDD2"
    style.stroke: "#D32F2F"
  }
  
  backward_op: "loss.backward()" {
    shape: rectangle
    style.fill: "#E1BEE7"
    style.stroke: "#4A148C"
    style.font: mono
  }
  
  gradients: "Gradients\n∂L/∂W for all params" {
    shape: rectangle
    style.fill: white
    style.stroke: "#7B1FA2"
  }
  
  loss_4 -> backward_op: trigger
  backward_op -> gradients: "compute\n(autograd)"
}

# Step 5: Gradient Clipping
step5: "Step 5: Clip Gradients" {
  style.fill: ${colors.step}
  style.stroke: "#455A64"
  style.stroke-width: 2
  
  grads_in: "Raw Gradients\n(possibly large)" {
    style.fill: white
    style.stroke: "#7B1FA2"
  }
  
  clip_op: "clip_grad_norm_(\n  model.parameters(),\n  max_norm=1.0\n)" {
    shape: rectangle
    style.fill: "#CFD8DC"
    style.stroke: "#263238"
    style.font: mono
    style.font-size: 20
  }
  
  grads_out: "Clipped Gradients\n||grad|| ≤ max_norm" {
    style.fill: white
    style.stroke: "#2E7D32"
  }
  
  grad_norm: "Gradient Norm\n(before/after)" {
    shape: text
    style.font-color: "#455A64"
  }
  
  grads_in -> clip_op
  clip_op -> grads_out
  clip_op -> grad_norm: "returns\n||grad||_2"
}

# Step 6: Optimizer Step
step6: "Step 6: Optimizer Step" {
  style.fill: ${colors.output}
  style.stroke: "#388E3C"
  style.stroke-width: 2
  
  grads_step: "Clipped Gradients" {
    style.fill: white
    style.stroke: "#2E7D32"
  }
  
  optimizer: "optimizer.step()\n(AdamW)" {
    shape: rectangle
    style.fill: "#C8E6C9"
    style.stroke: "#1B5E20"
    style.font: mono
  }
  
  zero_grad: "optimizer.zero_grad()\n(reset for next iter)" {
    shape: rectangle
    style.fill: "#B3E5FC"
    style.stroke: "#01579B"
    style.font: mono
  }
  
  params_before: "Params θ_t" {
    shape: rectangle
    style.fill: "#FFF9C4"
    style.stroke: "#F57F17"
  }
  
  params_after: "Params θ_{t+1}" {
    shape: rectangle
    style.fill: "#C8E6C9"
    style.stroke: "#1B5E20"
  }
  
  params_before -> optimizer
  grads_step -> optimizer
  optimizer -> params_after: "θ_{t+1} = θ_t - α·grad"
  zero_grad -> optimizer: "reset\n.grad to 0"
}

# Main Flow Connections
step1 -> step2: "decoder_input\nprepared"
step2 -> step3: logits
step3 -> step4: loss
step4 -> step5: gradients
step5 -> step6: "clipped grads"

# Teacher Forcing Explanation
teacher_forcing: |md
  ## Teacher Forcing
  
  **What it is**: During training, the decoder receives the **ground truth** 
  previous token, not its own prediction.
  
  **Why**: 
  - Stabilizes training (no error accumulation)
  - Faster convergence
  - Parallel computation of all positions
  
  **Trade-off**: Model may struggle at inference when its own 
  predictions differ from training distribution.
  
  **Decoder Input**: `<BOS> + targets[:-1]`
  **Loss Targets**: `targets[1:]`
| {
  near: bottom-center
  style.fill: "#FFFDE7"
  style.stroke: "#FBC02D"
  style.stroke-width: 2
  width: 600
}

# Dimension Annotations
dims: {
  near: top-right
  style.fill: transparent
  
  batch_dim: "batch_size = 32"
  seq_dim: "src_len = 128, tgt_len = 64"
  model_dim: "d_model = 512"
  vocab_dim: "vocab_size = 30,000"
}

# Gradient Flow Legend
legend: {
  near: bottom-right
  style.fill: white
  style.stroke: "#455A64"
  
  title: "Gradient Flow" {
    style.bold: true
  }
  
  forward: "Forward: Input → Loss" {
    style.fill: ${colors.process}
  }
  backward_leg: "Backward: Loss → Gradients" {
    style.fill: ${colors.grad}
  }
  update: "Update: Gradients → New Params" {
    style.fill: ${colors.output}
  }
}

# Pseudocode Summary
pseudocode: |md
  python
  # Training Loop with Teacher Forcing
  for batch in dataloader:
      # Step 1: Prepare shifted targets
      decoder_input = torch.cat([BOS, targets[:, :-1]], dim=1)
      loss_targets = targets[:, 1:]
      
      # Step 2: Forward pass
      logits = model(src, decoder_input)
      
      # Step 3: Compute loss
      loss = F.cross_entropy(
          logits.reshape(-1, vocab_size),
          loss_targets.reshape(-1),
          ignore_index=PAD_ID
      )
      
      # Step 4: Backward pass
      loss.backward()
      
      # Step 5: Gradient clipping
      grad_norm = torch.nn.utils.clip_grad_norm_(
          model.parameters(), max_norm=1.0
      )
      
      # Step 6: Optimizer step
      optimizer.step()
      optimizer.zero_grad()
  
| {
  near: center-left
  style.fill: "#263238"
  style.font-color: "#ECEFF1"
  width: 450
}