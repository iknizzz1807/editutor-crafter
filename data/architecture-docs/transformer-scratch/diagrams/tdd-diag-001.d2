vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # ScaledDotProductAttention Architecture
  **sizeof(ScaledDotProductAttention) â‰ˆ 3 Ã— d_modelÂ² Ã— 4 bytes**
| {near: top-center}

direction: right

classes: {
  tensor: {
    shape: rectangle
    style: {
      fill: "#E3F2FD"
      stroke: "#1976D2"
      font: mono
    }
  }
  layer: {
    style: {
      fill: "#F3E5F5"
      stroke: "#7B1FA2"
      bold: true
    }
  }
  operation: {
    shape: diamond
    style: {
      fill: "#FFF3E0"
      stroke: "#E65100"
    }
  }
}

ScaledDotProductAttention: {
  style: {
    fill: "#FAFAFA"
    stroke: "#424242"
    stroke-width: 2
    border-radius: 8
  }
  
  header: |md
    **class ScaledDotProductAttention(nn.Module)**
  | {
    shape: rectangle
    style: {
      fill: "#7B1FA2"
      font-color: white
      bold: true
    }
  }
  
  fields: {
    shape: class
    style.fill: "#F5F5F5"
    
    "d_k: int": "dimension for Q, K projections"
    "W_Q: nn.Linear": "Linear(d_model, d_model, bias=False)"
    "W_K: nn.Linear": "Linear(d_model, d_model, bias=False)"
    "W_V: nn.Linear": "Linear(d_model, d_model, bias=False)"
  }
  
  methods: {
    shape: class
    style.fill: "#F5F5F5"
    
    "+forward(x, mask=None)": "(output, attention_weights)"
  }
}

input_tensor: {
  class: tensor
  label: "x\n[batch, seq_len, d_model]"
}

W_Q: {
  class: layer
  label: "W_Q\nnn.Linear"
}
W_K: {
  class: layer
  label: "W_K\nnn.Linear"
}
W_V: {
  class: layer
  label: "W_V\nnn.Linear"
}

Q: {
  class: tensor
  label: "Q\n[batch, seq_len, d_k]"
}
K: {
  class: tensor
  label: "K\n[batch, seq_len, d_k]"
}
V: {
  class: tensor
  label: "V\n[batch, seq_len, d_v]"
}

matmul_1: {
  class: operation
  label: "Q @ K^T"
}

scale: {
  class: operation
  label: "/ âˆšd_k"
}

mask_op: {
  class: operation
  label: "mask\n(Ã— -inf)"
}

softmax: {
  class: operation
  label: "softmax\n(dim=-1)"
}

attention_weights: {
  class: tensor
  label: "weights\n[batch, seq_len, seq_len]"
}

matmul_2: {
  class: operation
  label: "weights @ V"
}

output: {
  class: tensor
  label: "output\n[batch, seq_len, d_v]"
}

input_tensor -> W_Q
input_tensor -> W_K
input_tensor -> W_V

W_Q -> Q
W_K -> K
W_V -> V

Q -> matmul_1
K -> matmul_1: "transpose"

matmul_1 -> scale: "scores\n[batch, seq, seq]"

scale -> mask_op: "scaled\n[batch, seq, seq]"

mask_op -> softmax: "masked\n[batch, seq, seq]"

softmax -> attention_weights

attention_weights -> matmul_2
V -> matmul_2

matmul_2 -> output

legend: {
  near: bottom-center
  shape: rectangle
  style.fill: transparent
  
  key: |md
    **Color Legend:**
    ğŸ”µ Blue = Input/Output Tensors
    ğŸŸ£ Purple = Learned Layers (nn.Linear)
    ğŸŸ  Orange = Stateless Operations
  |
}

annotation: |md
  **Key Invariants:**
  - `d_k = d_v = d_model` in single-head attention
  - `W_Q, W_K, W_V` have **no bias** in original Transformer
  - Softmax is applied **per row** (dim=-1), each row sums to 1.0
  - Masking applies `-inf` **before** softmax, never after
| {near: bottom-right}