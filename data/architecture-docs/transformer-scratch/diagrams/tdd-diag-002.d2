vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Attention Data Flow
  **Scaled Dot-Product Attention: Input → Output**
| {near: top-center}

direction: right

# Input
input: "Input x" {
  shape: rectangle
  width: 160
  style: {
    fill: "#E8F4FD"
    stroke: "#2E86AB"
    stroke-width: 2
    font-color: "#1A365D"
    bold: true
  }
}

input_shape: |md
  **Shape:** `[batch, seq_len, d_model]`
| {
  shape: text
  style.font-size: 12
  style.font-color: "#4A5568"
}

# Q, K, V Projections Container
projections: "Linear Projections" {
  style: {
    fill: "#FAFAFA"
    stroke: "#CBD5E0"
    stroke-dash: 3
  }
  
  W_Q: "W_Q" {
    shape: rectangle
    width: 100
    style: {
      fill: "#E9D8FD"
      stroke: "#805AD5"
      stroke-width: 2
    }
  }
  
  W_K: "W_K" {
    shape: rectangle
    width: 100
    style: {
      fill: "#E9D8FD"
      stroke: "#805AD5"
      stroke-width: 2
    }
  }
  
  W_V: "W_V" {
    shape: rectangle
    width: 100
    style: {
      fill: "#E9D8FD"
      stroke: "#805AD5"
      stroke-width: 2
    }
  }
}

# Q, K, V tensors
Q: "Query (Q)" {
  shape: rectangle
  width: 140
  style: {
    fill: "#C6F6D5"
    stroke: "#38A169"
    stroke-width: 2
    bold: true
  }
}

Q_shape: |md
  `[batch, seq_len, d_k]`
| {
  shape: text
  style.font-size: 11
  style.font-color: "#2D3748"
}

K: "Key (K)" {
  shape: rectangle
  width: 140
  style: {
    fill: "#C6F6D5"
    stroke: "#38A169"
    stroke-width: 2
    bold: true
  }
}

K_shape: |md
  `[batch, seq_len, d_k]`
| {
  shape: text
  style.font-size: 11
  style.font-color: "#2D3748"
}

V: "Value (V)" {
  shape: rectangle
  width: 140
  style: {
    fill: "#C6F6D5"
    stroke: "#38A169"
    stroke-width: 2
    bold: true
  }
}

V_shape: |md
  `[batch, seq_len, d_v]`
| {
  shape: text
  style.font-size: 11
  style.font-color: "#2D3748"
}

# Scores computation
K_T: "K^T" {
  shape: rectangle
  width: 80
  style: {
    fill: "#FEFCBF"
    stroke: "#D69E2E"
    stroke-width: 2
  }
}

K_T_shape: |md
  `[batch, d_k, seq_len]`
| {
  shape: text
  style.font-size: 11
  style.font-color: "#2D3748"
}

scores_raw: "Q @ K^T" {
  shape: rectangle
  width: 120
  style: {
    fill: "#FED7D7"
    stroke: "#E53E3E"
    stroke-width: 2
    bold: true
  }
}

scores_shape: |md
  `[batch, seq_len, seq_len]`
| {
  shape: text
  style.font-size: 11
  style.font-color: "#2D3748"
}

scores_scaled: "÷ √d_k" {
  shape: oval
  width: 80
  style: {
    fill: "#FEEBC8"
    stroke: "#DD6B20"
    stroke-width: 2
  }
}

# Masking (optional)
mask: "Mask (optional)" {
  shape: diamond
  width: 100
  style: {
    fill: "#FED7E2"
    stroke: "#D53F8C"
    stroke-width: 2
  }
}

mask_note: |md
  Padding: `-inf`
  Causal: upper-tri `-inf`
| {
  shape: text
  style.font-size: 10
  style.font-color: "#553C9A"
}

# Softmax
softmax: "Softmax" {
  shape: rectangle
  width: 140
  style: {
    fill: "#B2F5EA"
    stroke: "#319795"
    stroke-width: 2
    bold: true
  }
}

softmax_note: |md
  **dim=-1**
  Rows sum to 1.0
| {
  shape: text
  style.font-size: 11
  style.font-color: "#234E52"
}

attention_weights: "Attention Weights" {
  shape: rectangle
  width: 150
  style: {
    fill: "#C6F6D5"
    stroke: "#38A169"
    stroke-width: 2
    bold: true
  }
}

weights_shape: |md
  `[batch, seq_len, seq_len]`
| {
  shape: text
  style.font-size: 11
  style.font-color: "#2D3748"
}

# Final output
output: "Output" {
  shape: rectangle
  width: 160
  style: {
    fill: "#BEE3F8"
    stroke: "#3182CE"
    stroke-width: 3
    bold: true
  }
}

output_shape: |md
  `[batch, seq_len, d_v]`
| {
  shape: text
  style.font-size: 12
  style.font-color: "#1A365D"
}

# Connections
input -> projections: "" {
  style.stroke-width: 2
  style.stroke: "#2E86AB"
}

projections.W_Q -> Q: "Linear" {
  style.stroke-width: 2
  style.stroke: "#805AD5"
}

projections.W_K -> K: "Linear" {
  style.stroke-width: 2
  style.stroke: "#805AD5"
}

projections.W_V -> V: "Linear" {
  style.stroke-width: 2
  style.stroke: "#805AD5"
}

K -> K_T: "transpose" {
  style.stroke-width: 2
  style.stroke: "#D69E2E"
  style.stroke-dash: 3
}

Q -> scores_raw: "" {
  style.stroke-width: 2
  style.stroke: "#38A169"
}

K_T -> scores_raw: "" {
  style.stroke-width: 2
  style.stroke: "#D69E2E"
}

scores_raw -> scores_scaled: "" {
  style.stroke-width: 2
  style.stroke: "#E53E3E"
}

scores_scaled -> mask: "" {
  style.stroke-width: 2
  style.stroke: "#DD6B20"
}

mask -> softmax: "" {
  style.stroke-width: 2
  style.stroke: "#D53F8C"
}

softmax -> attention_weights: "" {
  style.stroke-width: 2
  style.stroke: "#319795"
}

attention_weights -> output: "" {
  style.stroke-width: 2
  style.stroke: "#38A169"
}

V -> output: "" {
  style.stroke-width: 2
  style.stroke: "#38A169"
  style.stroke-dash: 5
}

# Formula annotation
formula: |md
  ## Formula
  
  Attention(Q,K,V) = softmax(QK^T / √d_k) V
  
| {
  near: bottom-right
  shape: text
  style: {
    fill: "#F7FAFC"
    stroke: "#A0AEC0"
    stroke-width: 1
    border-radius: 8
    font: mono
  }
}