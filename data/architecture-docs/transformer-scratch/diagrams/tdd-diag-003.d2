vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # QK^T Matrix Multiplication
  ## Step-by-Step: Computing Attention Scores
| {near: top-center}

step_1: {
  label: "Step 1: Initial State"
  
  q_matrix: {
    label: "Q (Query)"
    shape: rectangle
    style.fill: "#E4DBFE"
    style.stroke: "#7C5DDE"
    
    annotation: |md
      **Shape:** `[batch, seq_len, d_k]`
      
      Row i = query vector for position i
      "What am I looking for?"
    |
  }
  
  k_matrix: {
    label: "K (Key)"
    shape: rectangle
    style.fill: "#C7F1FF"
    style.stroke: "#3B9EC9"
    
    annotation: |md
      **Shape:** `[batch, seq_len, d_k]`
      
      Row j = key vector for position j
      "What do I contain?"
    |
  }
  
  arrow: {
    shape: text
    label: "→"
    style.font-size: 40
  }
}

step_2: {
  label: "Step 2: Transpose K"
  
  k_transposed: {
    label: "K^T (Key Transposed)"
    shape: rectangle
    style.fill: "#C7F1FF"
    style.stroke: "#3B9EC9"
    
    annotation: |md
      **Shape:** `[batch, d_k, seq_len]`
      
      - Swap last two dimensions
      - Column j now holds key for position j
      - `K.transpose(-2, -1)` in PyTorch
    |
  }
  
  operation: {
    label: "Transpose Operation"
    shape: diamond
    style.fill: "#FFF9C9"
    style.stroke: "#C9A227"
  }
}

step_3: {
  label: "Step 3: Matrix Multiplication"
  
  before_mult: {
    label: ""
    
    left: {
      label: "Q"
      shape: rectangle
      width: 80
      height: 120
      style.fill: "#E4DBFE"
      style.stroke: "#7C5DDE"
    }
    
    mult_symbol: {
      label: "×"
      shape: text
      style.font-size: 36
      style.bold: true
    }
    
    right: {
      label: "K^T"
      shape: rectangle
      width: 120
      height: 80
      style.fill: "#C7F1FF"
      style.stroke: "#3B9EC9"
    }
    
    left -> mult_symbol -> right
  }
  
  dimension_check: {
    label: "Dimension Compatibility Check"
    shape: rectangle
    style.fill: "#ACE1AF"
    style.stroke: "#2E8B57"
    
    check: |md
      `[batch, seq_len, d_k]` × `[batch, d_k, seq_len]`
      
      ✓ Inner dims match: `d_k` = `d_k`
      
      → Result: `[batch, seq_len, seq_len]`
    |
  }
}

step_4: {
  label: "Step 4: Result — Scores Matrix"
  
  scores_matrix: {
    label: "scores"
    shape: rectangle
    style.fill: "#FFE7CB"
    style.stroke: "#D4853B"
    
    matrix_detail: {
      cell_ij: {
        label: "scores[i,j]"
        shape: rectangle
        width: 60
        style.fill: "#FFD700"
        style.stroke: "#B8860B"
        style.bold: true
      }
      
      explanation: |md
        **scores[i,j]** = Q[i] · K[j]
        
        = Σ(Q[i,k] × K[j,k]) for k in 0..d_k-1
        
        **Meaning:** How much does position i
        want to attend to position j?
      |
    }
  }
  
  interpretation: {
    label: "Interpretation"
    shape: rectangle
    style.fill: transparent
    style.stroke-dash: 3
    
    content: |md
      - **High score** → Q[i] similar to K[j] → strong attention
      - **Low score** → Q[i] different from K[j] → weak attention
      - **Diagonal** → self-similarity (i attending to itself)
    |
  }
}

step_5: {
  label: "Step 5: Next — Scaling"
  
  formula: {
    label: "Complete Formula"
    shape: rectangle
    style.fill: "#F6F9FC"
    style.stroke: "#CBD6E0"
    
    content: |md
      python
      scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)
      
      
      Division by √d_k prevents softmax saturation
    |
  }
  
  arrow_to_next: {
    shape: text
    label: "→ Apply softmax to get attention weights"
    style.font-size: 20
    style.italic: true
  }
}

step_1 -> step_2: "transpose K"
step_2 -> step_3: "prepare multiply"
step_3 -> step_4: "compute"
step_4 -> step_5: "scale"