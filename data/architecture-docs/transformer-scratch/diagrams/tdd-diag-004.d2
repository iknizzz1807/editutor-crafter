vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # Softmax Scaling Effect
  ## How 1/√d_k Prevents Gradient Vanishing
| {near: top-center}

before_scaling: "Before Scaling (d_k = 512)" {
  style.fill: "#2d1f3d"
  style.stroke: "#8b5cf6"
  style.font-color: white
  
  scores_raw: "Raw Scores (QK^T)" {
    shape: class
    style.fill: "#7c3aed"
    style.font-color: white
    style.stroke: "#a78bfa"
    
    row1: "[-45.2, -32.1, 28.7, 51.3]"
    row2: "[-12.8, 67.4, -23.9, 44.2]"
    row3: "[38.1, -56.2, 71.8, -19.4]"
  }
  
  scores_raw.width: 280
  
  problem: |md
    **Problem:**
    - Variance ≈ d_k = 512
    - Values range from -56 to +72
    - Exponentials overflow/underflow
  |
  
  softmax_raw: "Softmax Output" {
    shape: class
    style.fill: "#dc2626"
    style.font-color: white
    style.stroke: "#f87171"
    
    row1: "[0.00, 0.00, 0.00, 1.00]"
    row2: "[0.00, 1.00, 0.00, 0.00]"
    row3: "[0.00, 0.00, 1.00, 0.00]"
  }
  
  softmax_raw.width: 280
  
  gradient_raw: "Gradient Flow" {
    shape: class
    style.fill: "#450a0a"
    style.font-color: "#fca5a5"
    style.stroke: "#dc2626"
    
    row1: "≈ 0.00 (vanishes)"
    row2: "≈ 0.00 (vanishes)"
    row3: "≈ 0.00 (vanishes)"
  }
  
  gradient_raw.width: 280
  
  scores_raw -> softmax_raw: "softmax()"
  softmax_raw -> gradient_raw: "∂L/∂scores"
}

arrow: "" {
  shape: text
  label: "→ ÷ √512"
  style.font-size: 32
  style.bold: true
  style.font-color: "#22c55e"
}

after_scaling: "After Scaling (÷ √512 ≈ 22.6)" {
  style.fill: "#1a2e1a"
  style.stroke: "#22c55e"
  style.font-color: white
  
  scores_scaled: "Scaled Scores (QK^T / √d_k)" {
    shape: class
    style.fill: "#16a34a"
    style.font-color: white
    style.stroke: "#4ade80"
    
    row1: "[-2.00, -1.42, 1.27, 2.27]"
    row2: "[-0.57, 2.98, -1.06, 1.96]"
    row3: "[1.69, -2.49, 3.18, -0.86]"
  }
  
  scores_scaled.width: 280
  
  solution: |md
    **Solution:**
    - Variance normalized to ≈ 1
    - Values range from -2.5 to +3.2
    - Stable exponentials
  |
  
  softmax_scaled: "Softmax Output" {
    shape: class
    style.fill: "#15803d"
    style.font-color: white
    style.stroke: "#22c55e"
    
    row1: "[0.03, 0.06, 0.23, 0.68]"
    row2: "[0.10, 0.62, 0.07, 0.21]"
    row3: "[0.21, 0.02, 0.57, 0.20]"
  }
  
  softmax_scaled.width: 280
  
  gradient_scaled: "Gradient Flow" {
    shape: class
    style.fill: "#052e16"
    style.font-color: "#86efac"
    style.stroke: "#22c55e"
    
    row1: "0.03, 0.06, 0.23, 0.68"
    row2: "0.10, 0.62, 0.07, 0.21"
    row3: "0.21, 0.02, 0.57, 0.20"
  }
  
  gradient_scaled.width: 280
  
  scores_scaled -> softmax_scaled: "softmax()"
  softmax_scaled -> gradient_scaled: "∂L/∂scores"
}

before_scaling -> arrow -> after_scaling

formula: |latex
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
| {near: bottom-center}

explanation: |md
  **Why √d_k?**
  
  If Q, K elements have variance σ², then Q·K has variance d_k × σ².
  
  Dividing by √d_k restores variance to σ².
  
  This keeps softmax inputs in a range where gradients are healthy.
|

before_scaling.style.stroke-width: 3
after_scaling.style.stroke-width: 3