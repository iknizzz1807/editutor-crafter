vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Padding Mask Application
  ## Algorithm: From Token IDs to Attention Mask
| {near: top-center}

direction: right

step1: {
  label: "Step 1: Token IDs Input"
  style.fill: "#E8E4F0"
  style.stroke: "#7B68A6"
  style.stroke-width: 2
  
  tokens: {
    shape: sql_table
    label: "tokens [batch=2, seq=6]"
    style.fill: "#E8E4F0"
    
    batch0: "int" {constraint: "[72, 145, 89, 0, 0, 0]"}
    batch1: "int" {constraint: "[33, 67, 128, 45, 0, 0]"}
  }
  
  legend: |md
    **0 = PAD token ID**
    Non-zero = real tokens
  |
}

step2: {
  label: "Step 2: Boolean Mask"
  style.fill: "#E3F2FD"
  style.stroke: "#1565C0"
  style.stroke-width: 2
  
  code: |python
    mask = (tokens != PAD_ID)
    # True = real token, False = PAD
  |
  
  bool_mask: {
    shape: sql_table
    label: "mask [batch=2, seq=6]"
    style.fill: "#E3F2FD"
    
    b0: "bool" {constraint: "[T, T, T, F, F, F]"}
    b1: "bool" {constraint: "[T, T, T, T, F, F]"}
  }
}

step3: {
  label: "Step 3: Unsqueeze for Broadcasting"
  style.fill: "#FFF3E0"
  style.stroke: "#EF6C00"
  style.stroke-width: 2
  
  code: |python
    mask = mask.unsqueeze(1)
    # Insert dim at position 1
  |
  
  unsqueezed: {
    shape: sql_table
    label: "mask [batch=2, 1, seq=6]"
    style.fill: "#FFF3E0"
    
    b0: "bool" {constraint: "[[T, T, T, F, F, F]]"}
    b1: "bool" {constraint: "[[T, T, T, T, F, F]]"}
  }
  
  note: |md
    **Why?** Attention scores are `[batch, seq, seq]`.
    Need `[batch, 1, seq]` to broadcast across query dimension.
  |
}

step4: {
  label: "Step 4: Apply to Scores"
  style.fill: "#FFEBEE"
  style.stroke: "#C62828"
  style.stroke-width: 2
  
  scores_before: {
    shape: sql_table
    label: "scores [batch, seq, seq] (before)"
    style.fill: "#FFCDD2"
    
    row0: "float" {constraint: "[2.1, 0.5, -0.3, 1.8, 0.9, -1.2]"}
    row1: "float" {constraint: "[0.7, 1.9, 0.2, -0.5, 1.1, 0.4]"}
    more: "..." {constraint: "more rows"}
  }
  
  code: |python
    scores = scores.masked_fill(
      mask == 0, float('-inf')
    )
  |
  
  scores_after: {
    shape: sql_table
    label: "scores [batch, seq, seq] (after)"
    style.fill: "#FFCDD2"
    
    row0: "float" {constraint: "[2.1, 0.5, -0.3, -inf, -inf, -inf]"}
    row1: "float" {constraint: "[0.7, 1.9, 0.2, -0.5, -inf, -inf]"}
    more: "..." {constraint: "more rows"}
  }
}

step5: {
  label: "Step 5: Softmax Result"
  style.fill: "#E8F5E9"
  style.stroke: "#2E7D32"
  style.stroke-width: 2
  
  code: |python
    attn_weights = F.softmax(scores, dim=-1)
    # softmax(-inf) = 0.0
  |
  
  result: {
    shape: sql_table
    label: "attn_weights [batch, seq, seq]"
    style.fill: "#C8E6C9"
    
    row0: "float" {constraint: "[0.55, 0.17, 0.07, 0.0, 0.0, 0.0]"}
    row1: "float" {constraint: "[0.20, 0.40, 0.12, 0.08, 0.0, 0.0]"}
    more: "..." {constraint: "more rows"}
  }
  
  note: |md
    **Key insight**: Padding positions get **zero attention weight**.
    Rows still sum to 1.0 (valid probability distribution).
  |
}

step1 -> step2: "tokens != 0" {
  style.stroke: "#1565C0"
  style.stroke-width: 2
  style.bold: true
}

step2 -> step3: ".unsqueeze(1)" {
  style.stroke: "#EF6C00"
  style.stroke-width: 2
  style.bold: true
}

step3 -> step4: "broadcast" {
  style.stroke: "#C62828"
  style.stroke-width: 2
  style.bold: true
}

step4 -> step5: "F.softmax" {
  style.stroke: "#2E7D32"
  style.stroke-width: 2
  style.bold: true
}

broadcast_viz: {
  label: "Broadcasting Visualization"
  style.fill: "#FAFAFA"
  style.stroke: "#9E9E9E"
  style.stroke-dash: 3
  
  mask_shape: {
    label: "mask [2, 1, 6]"
    style.fill: "#FFF3E0"
    
    grid-columns: 6
    grid-gap: 0
    
    c1_1: "T" {style.fill: "#A5D6A7"}
    c1_2: "T" {style.fill: "#A5D6A7"}
    c1_3: "T" {style.fill: "#A5D6A7"}
    c1_4: "F" {style.fill: "#EF9A9A"}
    c1_5: "F" {style.fill: "#EF9A9A"}
    c1_6: "F" {style.fill: "#EF9A9A"}
  }
  
  arrow: "multiply" {
    style.font-size: 24
  }
  
  scores_shape: {
    label: "scores [2, 6, 6]"
    style.fill: "#FFCDD2"
    
    grid-columns: 6
    grid-gap: 0
    
    s1_1: "." {style.fill: "#FFCDD2"}
    s1_2: "." {style.fill: "#FFCDD2"}
    s1_3: "." {style.fill: "#FFCDD2"}
    s1_4: "." {style.fill: "#FFCDD2"}
    s1_5: "." {style.fill: "#FFCDD2"}
    s1_6: "." {style.fill: "#FFCDD2"}
    s2_1: "." {style.fill: "#FFCDD2"}
    s2_2: "." {style.fill: "#FFCDD2"}
    s2_3: "." {style.fill: "#FFCDD2"}
    s2_4: "." {style.fill: "#FFCDD2"}
    s2_5: "." {style.fill: "#FFCDD2"}
    s2_6: "." {style.fill: "#FFCDD2"}
    s3_1: "..." {style.fill: "#FFCDD2"}
    s3_2: "" {style.fill: "#FFCDD2"}
    s3_3: "" {style.fill: "#FFCDD2"}
    s3_4: "" {style.fill: "#FFCDD2"}
    s3_5: "" {style.fill: "#FFCDD2"}
    s3_6: "" {style.fill: "#FFCDD2"}
  }
  
  result_shape: {
    label: "masked [2, 6, 6]"
    style.fill: "#C8E6C9"
    
    grid-columns: 6
    grid-gap: 0
    
    r1_1: "." {style.fill: "#A5D6A7"}
    r1_2: "." {style.fill: "#A5D6A7"}
    r1_3: "." {style.fill: "#A5D6A7"}
    r1_4: "-inf" {style.fill: "#EF5350"; style.font-color: white; style.bold: true}
    r1_5: "-inf" {style.fill: "#EF5350"; style.font-color: white; style.bold: true}
    r1_6: "-inf" {style.fill: "#EF5350"; style.font-color: white; style.bold: true}
    r2_1: "." {style.fill: "#A5D6A7"}
    r2_2: "." {style.fill: "#A5D6A7"}
    r2_3: "." {style.fill: "#A5D6A7"}
    r2_4: "." {style.fill: "#A5D6A7"}
    r2_5: "-inf" {style.fill: "#EF5350"; style.font-color: white; style.bold: true}
    r2_6: "-inf" {style.fill: "#EF5350"; style.font-color: white; style.bold: true}
    r3_1: "..." {style.fill: "#C8E6C9"}
    r3_2: "" {style.fill: "#C8E6C9"}
    r3_3: "" {style.fill: "#C8E6C9"}
    r3_4: "" {style.fill: "#C8E6C9"}
    r3_5: "" {style.fill: "#C8E6C9"}
    r3_6: "" {style.fill: "#C8E6C9"}
  }
  
  mask_shape -> arrow -> scores_shape -> result_shape
}

implementation: {
  label: "Complete Implementation"
  style.fill: "#263238"
  style.stroke: "#37474F"
  
  code: |python
    def create_padding_mask(tokens: torch.Tensor, pad_id: int = 0) -> torch.Tensor:
        """
        Create attention mask for padding tokens.
        
        Args:
            tokens: [batch, seq_len] - token IDs
            pad_id: int - padding token ID (default 0)
        
        Returns:
            mask: [batch, 1, seq_len] - True for real tokens, False for PAD
        """
        mask = (tokens != pad_id).unsqueeze(1)
        return mask
    
    def apply_padding_mask(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        Apply padding mask to attention scores.
        
        Args:
            scores: [batch, seq_len, seq_len] - attention scores
            mask: [batch, 1, seq_len] - padding mask
        
        Returns:
            masked_scores: scores with -inf at padding positions
        """
        return scores.masked_fill(mask == 0, float('-inf'))
  |
}

critical_note: {
  near: bottom-right
  label: "Critical Warning"
  style.fill: "#FFF9C4"
  style.stroke: "#F57F17"
  style.stroke-width: 2
  style.border-radius: 8
  
  content: |md
    **CRITICAL**: Mask BEFORE softmax!
    
    Wrong: `softmax(scores) * mask`
    - Result doesn't sum to 1
    
    Correct: `softmax(masked_fill(scores, -inf))`
    - `-inf` becomes `0.0` after softmax
    - Valid probability distribution
  |
}