vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Softmax Numerical Stability
  Why subtract max before exponentiating
| {near: top-center}

direction: right

before: {
  label: "WITHOUT Max Subtraction"
  style.fill: "#2d1f1f"
  style.stroke: "#ff4444"
  style.stroke-width: 3

  input_before: |md
    ### Input Vector x
    
    x = [1000, 1001, 1002]
    
  | {style.fill: "#1a1a1a"}

  compute_before: |md
    ### Direct Computation
    
    exp(1000) = ∞  (overflow!)
    exp(1001) = ∞  (overflow!)
    exp(1002) = ∞  (overflow!)
    
    sum = ∞ + ∞ + ∞ = ∞
    
    softmax(x) = [∞/∞, ∞/∞, ∞/∞]
               = [NaN, NaN, NaN]
    
  | {
    style.fill: "#3d1a1a"
    style.stroke: "#ff6666"
    style.font-color: "#ff9999"
  }

  result_before: |md
    ### Result
    # ❌ OVERFLOW
    All values become **NaN**
    
    Gradients = 0 everywhere
    
    Training **FAILS**
  | {
    style.fill: "#4a1a1a"
    style.font-color: "#ff4444"
  }

  input_before -> compute_before -> result_before
}

arrow: "→" {
  shape: text
  style.font-size: 60
  style.font-color: "#888888"
}

after: {
  label: "WITH Max Subtraction"
  style.fill: "#1f2d1f"
  style.stroke: "#44ff44"
  style.stroke-width: 3

  input_after: |md
    ### Input Vector x
    
    x = [1000, 1001, 1002]
    max(x) = 1002
    
  | {style.fill: "#1a1a1a"}

  compute_after: |md
    ### Stable Computation
    
    x - max = [-2, -1, 0]
    
    exp(-2) ≈ 0.1353
    exp(-1) ≈ 0.3679
    exp(0)  = 1.0000
    
    sum = 1.5032
    
    softmax(x) = [0.0900, 0.2447, 0.6652]
    
  | {
    style.fill: "#1a3d1a"
    style.stroke: "#66ff66"
    style.font-color: "#99ff99"
  }

  result_after: |md
    ### Result
    # ✓ STABLE
    Valid probability distribution
    
    Rows sum to **1.0**
    
    Gradients flow **correctly**
  | {
    style.fill: "#1a4a1a"
    style.font-color: "#44ff44"
  }

  input_after -> compute_after -> result_after
}

proof: {
  near: bottom-center
  label: "Mathematical Equivalence"
  style.fill: "#1a1a2e"
  style.stroke: "#4a4a6a"
  style.border-radius: 8

  content: |md
    ### Why This Works
    
    Original: softmax(x_i) = exp(x_i) / Σ exp(x_j)
    
    With max: softmax(x_i) = exp(x_i - max) / Σ exp(x_j - max)
    
    
    exp(x_i - max)     exp(x_i) × exp(-max)
    ─────────────── = ───────────────────────── = exp(x_i) / Σ exp(x_j)
    Σ exp(x_j - max)   exp(-max) × Σ exp(x_j)
    
    
    **Same result, numerically stable!**
  |
}

legend: {
  near: bottom-right
  style.fill: "#1a1a1a"
  style.border-radius: 4

  overflow: "█ OVERFLOW: exp(x) > 10^308" {
    style.fill: "#3d1a1a"
    style.font-color: "#ff4444"
  }
  stable: "█ STABLE: exp(x-max) ≤ 1" {
    style.fill: "#1a3d1a"
    style.font-color: "#44ff44"
  }
}

note: |md
  **PyTorch Note:** `F.softmax()` handles this automatically, but you must understand it for:
  - Custom implementations
  - Log-softmax stability  
  - Interview questions
| {
  near: bottom-left
  style.fill: "#2a2a3a"
  style.font-color: "#aaaacc"
}