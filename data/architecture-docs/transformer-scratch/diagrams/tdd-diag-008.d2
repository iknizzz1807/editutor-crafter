vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: "Attention Gradient Flow (Backward Pass)" {
  near: top-center
  shape: text
  style: {
    font-size: 32
    bold: true
    underline: true
  }
}

direction: right

forward_pass: "Forward Pass (Cached)" {
  style.fill: "#E8E8E8"
  style.stroke: "#888888"
  style.stroke-dash: 3
  
  Q_cached: "Q [B, L, d_k]" {
    style.fill: "#E8E8E8"
  }
  K_cached: "K [B, L, d_k]" {
    style.fill: "#E8E8E8"
  }
  V_cached: "V [B, L, d_v]" {
    style.fill: "#E8E8E8"
  }
  scores_cached: "scores [B, L, L]" {
    style.fill: "#E8E8E8"
  }
  attn_cached: "A [B, L, L]" {
    style.fill: "#E8E8E8"
  }
  
  Q_cached -> scores_cached
  K_cached -> scores_cached
  scores_cached -> attn_cached
  V_cached -> attn_cached
}

backward_pass: "Backward Pass (Gradient Flow)" {
  style.fill: "#FFF0F0"
  style.stroke: "#CC0000"
  
  dL_dOutput: "∂L/∂output\n[B, L, d_v]" {
    style.fill: "#FFCCCC"
    style.stroke: "#CC0000"
    style.bold: true
  }
  
  dL_dV: "∂L/∂V\n[B, L, d_v]" {
    style.fill: "#FFE6CC"
    style.stroke: "#CC6600"
  }
  
  dL_dA: "∂L/∂A\n[B, L, L]" {
    style.fill: "#FFCCCC"
    style.stroke: "#CC0000"
  }
  
  dL_dScores: "∂L/∂scores\n[B, L, L]" {
    style.fill: "#FFCCCC"
    style.stroke: "#CC0000"
    style.bold: true
  }
  
  dL_dQ: "∂L/∂Q\n[B, L, d_k]" {
    style.fill: "#CCE5FF"
    style.stroke: "#0066CC"
    style.bold: true
  }
  
  dL_dK: "∂L/∂K\n[B, L, d_k]" {
    style.fill: "#CCE5FF"
    style.stroke: "#0066CC"
    style.bold: true
  }
  
  dL_dOutput -> dL_dV: "∂L/∂V = A^T @ ∂L/∂output" {
    style.stroke: "#CC6600"
    style.stroke-width: 2
  }
  
  dL_dOutput -> dL_dA: "∂L/∂A = ∂L/∂output @ V^T" {
    style.stroke: "#CC0000"
    style.stroke-width: 2
  }
  
  dL_dA -> dL_dScores: "softmax Jacobian\nJ_ij = A_i(δ_ij - A_j)" {
    style.stroke: "#CC0000"
    style.stroke-width: 2
    style.stroke-dash: 3
  }
  
  dL_dScores -> dL_dQ: "∂L/∂Q = ∂L/∂scores @ K × 1/√d_k" {
    style.stroke: "#0066CC"
    style.stroke-width: 2
  }
  
  dL_dScores -> dL_dK: "∂L/∂K = ∂L/∂scores^T @ Q × 1/√d_k" {
    style.stroke: "#0066CC"
    style.stroke-width: 2
  }
}

forward_pass.attn_cached -> backward_pass.dL_dOutput: "loss.backward()" {
  style.stroke: "#CC0000"
  style.stroke-width: 3
  style.animated: true
}

softmax_detail: "Softmax Backward Detail" {
  near: bottom-center
  style.fill: "#FFFFEE"
  style.stroke: "#888800"
  
  formula: |md
    
    Given: A = softmax(scores)
    
    ∂L/∂scores_i = Σ_j (∂L/∂A_j × ∂A_j/∂scores_i)
    
    Where: ∂A_j/∂scores_i = A_i(δ_ij - A_j)
    
    Simplified:
    ∂L/∂scores = A ⊙ (∂L/∂A - (A ⊙ ∂L/∂A) @ 1^T)
    
  |
}

gradient_notes: |md
  **Key Gradient Flow Properties:**
  
  1. **∂L/∂V**: Weighted by attention scores
     - If position i attends 80% to position j,
       80% of position i's error flows to V_j
  
  2. **Softmax Jacobian**: Creates mixing
     - Diagonal term: A_i × (1 - A_i) 
     - Off-diagonal: -A_i × A_j
     - High attention → higher gradient sensitivity
  
  3. **∂L/∂Q, ∂L/∂K**: Scaled by 1/√d_k
     - Same scaling as forward pass
     - Gradient magnitude preserved
| {
  near: center-left
  shape: rectangle
  style.fill: "#EEF8FF"
  style.stroke: "#0066CC"
}

legend: {
  near: bottom-right
  style.fill: "#F5F5F5"
  style.stroke: "#666666"
  
  cached: "Cached tensors (forward)" {
    style.fill: "#E8E8E8"
  }
  gradient: "Gradient tensors (backward)" {
    style.fill: "#FFCCCC"
  }
  output: "Output gradients" {
    style.fill: "#CCE5FF"
  }
  
  cached -> gradient: "∂L/∂..." {
    style.stroke-dash: 2
  }
}