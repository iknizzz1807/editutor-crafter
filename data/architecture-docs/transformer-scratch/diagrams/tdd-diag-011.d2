vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # MultiHeadAttention Architecture
  `nn.Module` — Projects input through parallel attention heads
| {near: top-center}

direction: right

classes: {
  projection: {
    style: {
      fill: "#E8E4F0"
      stroke: "#6B5B95"
      stroke-width: 2
      border-radius: 4
    }
  }
  config: {
    style: {
      fill: "#F5F5F5"
      stroke: "#9E9E9E"
      stroke-dash: 3
      border-radius: 4
    }
  }
  tensor: {
    shape: rectangle
    style: {
      fill: "#E3F2FD"
      stroke: "#1976D2"
      border-radius: 2
    }
  }
  method: {
    style: {
      fill: "#E8F5E9"
      stroke: "#388E3C"
      border-radius: 4
    }
  }
}

MultiHeadAttention: {
  shape: class
  style: {
    fill: white
    stroke: "#333"
    stroke-width: 2
  }

  +d_model: int
  +num_heads: int
  +d_k: int
  +d_v: int
  
  -W_Q: "nn.Linear(d_model, d_model)"
  -W_K: "nn.Linear(d_model, d_model)"
  -W_V: "nn.Linear(d_model, d_model)"
  -W_O: "nn.Linear(d_model, d_model)"
  
  +__init__(d_model: int, num_heads: int): void
  +forward(x: Tensor, mask: Optional[Tensor]): (Tensor, Tensor)
  -split_heads(x: Tensor): Tensor
  -combine_heads(x: Tensor): Tensor
}

Config: {
  label: "Configuration"
  class: config
  
  d_model: "512 (embedding dim)"
  num_heads: "8 (parallel heads)"
  d_k: "64 = d_model / num_heads"
  d_v: "64 = d_model / num_heads"
}

W_Q: {
  class: projection
  label: "W_Q\nQuery Projection\nnn.Linear(512, 512)"
}

W_K: {
  class: projection
  label: "W_K\nKey Projection\nnn.Linear(512, 512)"
}

W_V: {
  class: projection
  label: "W_V\nValue Projection\nnn.Linear(512, 512)"
}

W_O: {
  class: projection
  label: "W_O\nOutput Projection\nnn.Linear(512, 512)"
}

Input: {
  class: tensor
  label: "Input x\n[batch, seq_len, 512]"
}

Output: {
  class: tensor
  label: "Output\n[batch, seq_len, 512]"
}

AttentionWeights: {
  class: tensor
  label: "attn_weights\n[batch, num_heads, seq, seq]"
}

Forward: {
  class: method
  label: |md
    **forward(x, mask=None)**
    python
    batch, seq_len, _ = x.shape
    
    # Project to Q, K, V
    Q = self.W_Q(x)  # [b, s, 512]
    K = self.W_K(x)
    V = self.W_V(x)
    
    # Split into heads
    Q = split_heads(Q)  # [b, 8, s, 64]
    K = split_heads(K)
    V = split_heads(V)
    
    # Scaled dot-product attention
    scores = Q @ K.T / sqrt(d_k)
    if mask: scores = mask_fill(scores, -inf)
    weights = softmax(scores, dim=-1)
    heads = weights @ V
    
    # Combine heads
    concat = combine_heads(heads)
    return self.W_O(concat), weights
    
  |
}

SplitHeads: {
  label: |md
    **split_heads(x)**
    
    [b, s, 512] → [b, 8, s, 64]
    python
    x.view(b, s, 8, 64)
     .transpose(1, 2)
    
  |
  style: {
    fill: "#FFF8E1"
    stroke: "#F57C00"
    border-radius: 4
  }
}

CombineHeads: {
  label: |md
    **combine_heads(x)**
    
    [b, 8, s, 64] → [b, s, 512]
    python
    x.transpose(1, 2)
     .contiguous()
     .view(b, s, 512)
    
  |
  style: {
    fill: "#FFF8E1"
    stroke: "#F57C00"
    border-radius: 4
  }
}

ParallelHeads: {
  label: |md
    # Parallel Attention Heads
    
    Each head learns different attention patterns:
    
    - **Head 1**: Syntax dependencies
    - **Head 2**: Coreference resolution  
    - **Head 3**: Positional patterns
    - **...**: ...
    - **Head 8**: Semantic similarity
  |
  style: {
    fill: "#FCE4EC"
    stroke: "#C2185B"
    border-radius: 8
  }
  
  Head1: {shape: circle; style.fill: "#F8BBD9"}
  Head2: {shape: circle; style.fill: "#F8BBD9"}
  Head3: {shape: circle; style.fill: "#F8BBD9"}
  Head4: {shape: circle; style.fill: "#F8BBD9"}
  Head5: {shape: circle; style.fill: "#F8BBD9"}
  Head6: {shape: circle; style.fill: "#F8BBD9"}
  Head7: {shape: circle; style.fill: "#F8BBD9"}
  Head8: {shape: circle; style.fill: "#F8BBD9"}
  
  Head1 -> Head2 -> Head3 -> Head4 -> Head5 -> Head6 -> Head7 -> Head8
}

Input -> W_Q
Input -> W_K
Input -> W_V

W_Q -> SplitHeads
W_K -> SplitHeads
W_V -> SplitHeads

SplitHeads -> ParallelHeads: "8 parallel\nattention ops"
ParallelHeads -> CombineHeads
CombineHeads -> W_O
W_O -> Output

MultiHeadAttention -> Config: "stores"
MultiHeadAttention -> W_Q: owns
MultiHeadAttention -> W_K: owns
MultiHeadAttention -> W_V: owns
MultiHeadAttention -> W_O: owns
MultiHeadAttention -> Forward: provides
MultiHeadAttention -> AttentionWeights: returns

Forward -> SplitHeads: calls
Forward -> CombineHeads: calls

sizeof: |md
  **sizeof(MultiHeadAttention)**
  
  4 × nn.Linear(512, 512) = 4 × 262,144 params
  Total: 1,048,576 trainable parameters
| {near: bottom-center}