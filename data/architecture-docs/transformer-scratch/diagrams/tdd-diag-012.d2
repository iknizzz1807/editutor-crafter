vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # Head Splitting Transform
  `MultiHeadAttention` input reshape operation
| {near: top-center}

step1_input: "Input Tensor" {
  shape: rectangle
  width: 200
  style.fill: "#E8F4FD"
  style.stroke: "#2196F3"
  style.stroke-width: 2
  
  dims_input: |md
    **[batch, seq, d_model]**
    
    - batch: batch size
    - seq: sequence length  
    - d_model: embedding dimension
  |
}

step2_reshape: "Reshape Operation" {
  shape: rectangle
  width: 200
  style.fill: "#FFF3E0"
  style.stroke: "#FF9800"
  style.stroke-width: 2
  
  reshape_op: |md
    `.view(batch, seq, num_heads, d_k)`
    
    Where:
    - `d_model = num_heads × d_k`
    - `d_k = d_model / num_heads`
  |
}

step3_output: "Reshaped Tensor" {
  shape: rectangle
  width: 200
  style.fill: "#E8F5E9"
  style.stroke: "#4CAF50"
  style.stroke-width: 2
  
  dims_reshape: |md
    **[batch, seq, num_heads, d_k]**
    
    Last dimension split into:
    - num_heads: attention heads
    - d_k: key/query dimension per head
  |
}

step4_transpose: "Transpose Operation" {
  shape: rectangle
  width: 200
  style.fill: "#F3E5F5"
  style.stroke: "#9C27B0"
  style.stroke-width: 2
  
  transpose_op: |md
    `.transpose(1, 2)`
    
    Swap seq and num_heads dims
  |
}

step5_final: "Final Tensor" {
  shape: rectangle
  width: 200
  style.fill: "#FFEBEE"
  style.stroke: "#F44336"
  style.stroke-width: 2
  
  dims_final: |md
    **[batch, num_heads, seq, d_k]**
    
    Ready for parallel attention:
    - Each head operates independently
    - Matrix multiplication across seq
  |
}

step1_input -> step2_reshape: "x.view(batch, seq,\nnum_heads, d_k)" {
  style.stroke: "#2196F3"
  style.stroke-width: 2
  style.bold: true
}

step2_reshape -> step3_output: "reshaped" {
  style.stroke: "#FF9800"
  style.stroke-width: 2
  style.bold: true
}

step3_output -> step4_transpose: ".transpose(1, 2)" {
  style.stroke: "#4CAF50"
  style.stroke-width: 2
  style.bold: true
}

step4_transpose -> step5_final: "transposed" {
  style.stroke: "#9C27B0"
  style.stroke-width: 2
  style.bold: true
}

annotation: |md
  ### Why Transpose?
  
  PyTorch batch matrix multiplication (`@`) operates on last two dimensions.
  
  With shape `[batch, num_heads, seq, d_k]`:
  - Each (batch, head) pair is processed independently
  - `Q @ K.transpose(-2, -1)` computes attention per head
  - Result: `[batch, num_heads, seq, seq]` attention weights
| {
  near: bottom-center
  style.fill: "#FAFAFA"
  style.stroke: "#E0E0E0"
  style.stroke-dash: 3
}

memory_layout: "Memory Layout Visualization" {
  near: center-right
  
  before: "Before (contiguous)" {
    shape: rectangle
    style.fill: "#E3F2FD"
    label: |md
      d_model=512
      ┌─────────────────────────────┐
      │   all 512 dims together     │
      └─────────────────────────────┘
    |
  }
  
  after: "After (interleaved)" {
    shape: rectangle
    style.fill: "#E8F5E9"
    label: |md
      num_heads=8, d_k=64
      ┌────┬────┬────┬────┬────┬────┬────┬────┐
      │h0  │h1  │h2  │h3  │h4  │h5  │h6  │h7  │
      │64d │64d │64d │64d │64d │64d │64d │64d │
      └────┴────┴────┴────┴────┴────┴────┴────┘
    |
  }
  
  before -> after: "reshape" {
    style.stroke-dash: 3
  }
}