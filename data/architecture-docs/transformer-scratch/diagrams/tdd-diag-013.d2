vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Parallel Head Attention
  All heads attend simultaneously via batched matmul
| {near: top-center}

direction: right

classes: {
  input: {
    shape: rectangle
    style: {
      fill: "#E8D5F2"
      stroke: "#7B4B94"
      stroke-width: 2
      font-color: "#4A2860"
    }
  }
  projection: {
    shape: rectangle
    style: {
      fill: "#FFE5B4"
      stroke: "#CC8800"
      stroke-width: 2
    }
  }
  head: {
    shape: rectangle
    style: {
      fill: "#C7F1FF"
      stroke: "#0088AA"
      stroke-width: 2
    }
  }
  operation: {
    shape: diamond
    style: {
      fill: "#B5AFF6"
      stroke: "#6B5AA6"
      stroke-width: 2
    }
  }
  output: {
    shape: rectangle
    style: {
      fill: "#ACE1AF"
      stroke: "#2E8B57"
      stroke-width: 2
      font-color: "#1A5235"
    }
  }
}

input: "Input X\n[batch, seq, d_model]" {
  class: input
  width: 140
}

split: "Reshape\n[batch, seq, num_heads, d_k]" {
  class: operation
  width: 160
}

proj_q: "W_Q: Linear(d_model, d_model)" {
  class: projection
  width: 180
}
proj_k: "W_K: Linear(d_model, d_model)" {
  class: projection
  width: 180
}
proj_v: "W_V: Linear(d_model, d_model)" {
  class: projection
  width: 180
}

head_box: "Parallel Heads (batched)" {
  style.fill: "#F5F5F5"
  style.stroke: "#666666"
  style.stroke-dash: 3
  
  heads_grid: {
    grid-columns: 3
    grid-gap: 20
    
    h0_q: "Q₀\n[batch, seq, d_k]" {class: head; width: 90}
    h1_q: "Q₁\n[batch, seq, d_k]" {class: head; width: 90}
    h2_q: "Q_h\n[batch, seq, d_k]" {class: head; width: 90}
    
    h0_k: "K₀\n[batch, seq, d_k]" {class: head; width: 90}
    h1_k: "K₁\n[batch, seq, d_k]" {class: head; width: 90}
    h2_k: "K_h\n[batch, seq, d_k]" {class: head; width: 90}
    
    h0_v: "V₀\n[batch, seq, d_k]" {class: head; width: 90}
    h1_v: "V₁\n[batch, seq, d_k]" {class: head; width: 90}
    h2_v: "V_h\n[batch, seq, d_k]" {class: head; width: 90}
  }
  
  dots: "..." {
    shape: text
    style.font-size: 20
  }
}

batched_attn: "Batched Attention\nmatmul for all heads\n[batch, num_heads, seq, seq]" {
  class: operation
  width: 200
}

concat: "Concat Heads\n[batch, seq, d_model]" {
  class: output
  width: 180
}

final_proj: "W_O: Linear(d_model, d_model)" {
  class: projection
  width: 180
}

output: "Output\n[batch, seq, d_model]" {
  class: output
  width: 140
}

input -> proj_q: "project"
input -> proj_k: "project"
input -> proj_v: "project"

proj_q -> head_box: "reshape to\n[batch, num_heads, seq, d_k]"
proj_k -> head_box
proj_v -> head_box

head_box -> batched_attn: "Q @ Kᵀ / √d_k\nsoftmax"
batched_attn -> concat: "@ V"
concat -> final_proj
final_proj -> output

note: |md
  python
  # Single batched operation for all heads
  Q = W_Q(x).view(batch, seq, heads, d_k)
  Q = Q.transpose(1, 2)  # [batch, heads, seq, d_k]
  
  # Same for K, V...
  
  # Parallel attention - all heads in one matmul
  scores = Q @ K.transpose(-2, -1) / sqrt(d_k)
  attn = softmax(scores)
  out = attn @ V  # [batch, heads, seq, d_k]
  
  # Reshape back
  out = out.transpose(1, 2).contiguous()
  out = out.view(batch, seq, d_model)
  
| {near: bottom-center}