vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Multi-Head Attention: Complete Data Flow
  **Shapes at each transformation stage**
| {near: top-center}

direction: right

input: "Input X\n[batch, seq_len, d_model]" {
  style: {
    fill: "#E8D5F2"
    stroke: "#7B2D8E"
    stroke-width: 2
    font: mono
  }
}

projections: "Q, K, V Projections" {
  style.fill: "#F5F0F7"
  
  Wq: "W_Q\n[d_model, d_model]" {
    style: {
      fill: "#FFE4B5"
      stroke: "#CC8800"
    }
  }
  Wk: "W_K\n[d_model, d_model]" {
    style: {
      fill: "#FFE4B5"
      stroke: "#CC8800"
    }
  }
  Wv: "W_V\n[d_model, d_model]" {
    style: {
      fill: "#FFE4B5"
      stroke: "#CC8800"
    }
  }
}

qkv: "Linear Projections" {
  style.fill: "#F5F0F7"
  
  Q: "Q\n[batch, seq_len, d_model]" {
    style: {
      fill: "#B8D4E8"
      stroke: "#2E6B8A"
      bold: true
    }
  }
  K: "K\n[batch, seq_len, d_model]" {
    style: {
      fill: "#B8D4E8"
      stroke: "#2E6B8A"
      bold: true
    }
  }
  V: "V\n[batch, seq_len, d_model]" {
    style: {
      fill: "#B8D4E8"
      stroke: "#2E6B8A"
      bold: true
    }
  }
}

reshape: "Reshape for Heads" {
  style.fill: "#F0F8E8"
  
  Q_heads: "Q\n[batch, seq_len, h, d_k]\n→ transpose →\n[batch, h, seq_len, d_k]" {
    style: {
      fill: "#C8E6C9"
      stroke: "#388E3C"
      font-size: 18
    }
  }
  K_heads: "K\n[batch, seq_len, h, d_k]\n→ transpose →\n[batch, h, seq_len, d_k]" {
    style: {
      fill: "#C8E6C9"
      stroke: "#388E3C"
      font-size: 18
    }
  }
  V_heads: "V\n[batch, seq_len, h, d_v]\n→ transpose →\n[batch, h, seq_len, d_v]" {
    style: {
      fill: "#C8E6C9"
      stroke: "#388E3C"
      font-size: 18
    }
  }
}

attention: "Scaled Dot-Product Attention\n(parallel across h heads)" {
  style: {
    fill: "#FFF3E0"
    stroke: "#E65100"
    stroke-width: 2
  }
  
  scores: "scores = QK^T / √d_k\n[batch, h, seq_len, seq_len]" {
    style: {
      fill: "#FFCC80"
      stroke: "#E65100"
    }
  }
  
  weights: "weights = softmax(scores)\n[batch, h, seq_len, seq_len]" {
    style: {
      fill: "#FFCC80"
      stroke: "#E65100"
    }
  }
  
  head_outputs: "head_i = weights × V\n[batch, h, seq_len, d_v]" {
    style: {
      fill: "#FFCC80"
      stroke: "#E65100"
      bold: true
    }
  }
}

concat: "Concatenate Heads" {
  style.fill: "#E3F2FD"
  
  transposed: "transpose\n[batch, seq_len, h, d_v]" {
    style: {
      fill: "#90CAF9"
      stroke: "#1565C0"
    }
  }
  
  concatenated: "concat\n[batch, seq_len, h × d_v]\n= [batch, seq_len, d_model]" {
    style: {
      fill: "#90CAF9"
      stroke: "#1565C0"
      bold: true
    }
  }
}

output_proj: "Output Projection" {
  style.fill: "#F5F0F7"
  
  Wo: "W_O\n[d_model, d_model]" {
    style: {
      fill: "#FFE4B5"
      stroke: "#CC8800"
    }
  }
}

output: "Output\n[batch, seq_len, d_model]" {
  style: {
    fill: "#E8D5F2"
    stroke: "#7B2D8E"
    stroke-width: 2
    font: mono
    bold: true
  }
}

# Connections
input -> projections: "× W_Q, W_K, W_V"
projections -> qkv: "linear"
qkv -> reshape: "view + transpose"
reshape -> attention: "per-head ops"
attention -> concat: "transpose + view"
concat -> output_proj: "× W_O"
output_proj -> output: "final"

# Legend
legend: {
  near: bottom-center
  style.fill: transparent
  
  note: |md
    **Key Dimensions:**
    - `d_model` = 512 (total model dimension)
    - `h` = 8 (number of attention heads)
    - `d_k = d_v = d_model / h = 64` (per-head dimension)
    
    **Total Parameters per MHA:**
    - 4 × d_model² = 4 × 512² = 1,048,576
  |
}