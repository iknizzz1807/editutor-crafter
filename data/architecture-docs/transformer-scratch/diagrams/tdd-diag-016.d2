vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # W_O Output Projection
  ### Mixing Information Across Attention Heads
| {near: top-center}

direction: right

step1: "Step 1: Multi-Head Outputs" {
  style.fill: "#E8F4FD"
  style.stroke: "#2196F3"
  style.stroke-width: 2
  
  head0: "Head 0\n[batch, seq, d_k]" {
    style.fill: "#BBDEFB"
    shape: rectangle
  }
  head1: "Head 1\n[batch, seq, d_k]" {
    style.fill: "#BBDEFB"
    shape: rectangle
  }
  headn: "Head h\n[batch, seq, d_k]" {
    style.fill: "#BBDEFB"
    style.stroke-dash: 3
    shape: rectangle
  }
  
  note1: |md
    Each head learns **different attention patterns**:
    - Head 0: syntactic dependencies
    - Head 1: semantic similarity  
    - Head h: positional relationships
  |
  note1.near: bottom-center
}

step2: "Step 2: Concatenate" {
  style.fill: "#FFF3E0"
  style.stroke: "#FF9800"
  style.stroke-width: 2
  
  concat: "Concatenated\n[batch, seq, h × d_k]\n= [batch, seq, d_model]" {
    style.fill: "#FFE0B2"
    shape: rectangle
    width: 180
  }
  
  note2: |md
    Concatenate along **last dimension**:
    
    concat = torch.cat([head_0, head_1, ..., head_h], dim=-1)
  |
  note2.near: bottom-center
}

step3: "Step 3: W_O Projection" {
  style.fill: "#E8F5E9"
  style.stroke: "#4CAF50"
  style.stroke-width: 2
  
  wo_matrix: "W_O Matrix\n[d_model, d_model]" {
    style.fill: "#C8E6C9"
    shape: rectangle
    width: 140
  }
  
  output: "Multi-Head Output\n[batch, seq, d_model]" {
    style.fill: "#A5D6A7"
    shape: rectangle
    width: 180
    style.bold: true
  }
  
  wo_matrix -> output: "Linear\nProjection"
  
  note3: |md
    **W_O mixes information across heads:**
    - Not just dimension matching
    - Learns which heads matter for each task
    - Creates unified representation
  |
  note3.near: bottom-center
}

step1.head0 -> step2.concat
step1.head1 -> step2.concat
step1.headn -> step2.concat: "..."

step2.concat -> step3.wo_matrix

legend: "Why W_O Matters" {
  near: bottom-center
  style.fill: "#F5F5F5"
  style.stroke: "#9E9E9E"
  
  mixing: |md
    ### Information Mixing
    Without W_O: heads are isolated, no cross-head communication
    
    With W_O: learned combination of head outputs
    
    output = concat @ W_O
    
    
    Position i can use information from:
    - Head 0's attention to syntax
    - Head 3's attention to semantics
    - All weighted by learned W_O values
  |
  
  shape_info: |md
    ### Shape Transformation
    | Tensor | Shape |
    |--------|-------|
    | Each head output | [batch, seq, d_k] |
    | Concatenated | [batch, seq, h×d_k] |
    | W_O weight | [d_model, d_model] |
    | Final output | [batch, seq, d_model] |
    
    Where d_model = h × d_k (typically 512 = 8 × 64)
  |
}

math_detail: "Mathematical Formulation" {
  near: top-right
  style.fill: "#F3E5F5"
  style.stroke: "#9C27B0"
  
  formula: |latex
    \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,...,\text{head}_h)W^O
  |
  
  expanded: |md
    head_i = Attention(Q·W_Q_i, K·W_K_i, V·W_V_i)
    
    output = [head_0 | head_1 | ... | head_h] @ W_O
           = [batch, seq, d_model]
    
    
    **W_O is learned** — gradient flows through all heads,
    allowing the model to discover optimal head combinations.
  |
}

gradient_flow: "Gradient Flow Through W_O" {
  near: bottom-right
  style.fill: "#FFEBEE"
  style.stroke: "#F44336"
  
  backward: |md
    ### Backpropagation
    
    ∂L/∂W_O = concat^T @ ∂L/∂output
    ∂L/∂concat = ∂L/∂output @ W_O^T
    
    
    Each head receives gradients **proportional to its contribution** 
    as determined by W_O weights.
    
    Heads that aren't useful → small W_O weights → 
    small gradients → eventually pruned by sparsity
  |
}