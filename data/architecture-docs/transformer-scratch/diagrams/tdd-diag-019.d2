vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # Contiguous Tensor Handling
  ## When `.contiguous()` is Required Before `.view()`
| {near: top-center}

STEP_1: {
  label: "Step 1: Understand Memory Layout"
  
  explanation: |md
    **Key Concept**: Tensors have logical shape and physical memory layout.
    A tensor is **contiguous** if elements are stored sequentially in memory
    according to the stride pattern.
  |
  
  memory_layout: {
    label: "Memory Layout Visualization"
    
    contiguous_tensor: {
      label: "Contiguous Tensor\nshape: [2, 3]"
      style.fill: "#E8F4FD"
      
      data: |md
        Memory addresses: 0, 1, 2, 3, 4, 5
        Elements: [1, 2, 3, 4, 5, 6]
        Strides: [3, 1] â† row-major order
      |
    }
    
    non_contiguous_tensor: {
      label: "Non-Contiguous Tensor\n(transposed)"
      style.fill: "#FFE4E4"
      
      data: |md
        Memory addresses: 0, 3, 1, 4, 2, 5
        Elements: [1, 4, 2, 5, 3, 6]
        Strides: [1, 3] â† NOT sequential!
      |
    }
    
    contiguous_tensor -> non_contiguous_tensor: ".transpose(0, 1)\nbreaks contiguity" {
      style.stroke: red
      style.stroke-width: 3
      style.bold: true
    }
  }
}

STEP_2: {
  label: "Step 2: Common Operations That Break Contiguity"
  
  operations_grid: {
    grid-columns: 2
    grid-gap: 30
    
    breaks_contiguity: {
      label: "âŒ BREAKS Contiguity"
      style.fill: "#FFE4E4"
      style.stroke: red
      
      op1: ".transpose(dim0, dim1)"
      op2: ".permute(*dims)"
      op3: ".t() (2D only)"
      op4: ".narrow()"
      op5: ".expand() (sometimes)"
      op6: ".select(dim, idx)"
      
      op1.style.font: mono
      op2.style.font: mono
      op3.style.font: mono
      op4.style.font: mono
      op5.style.font: mono
      op6.style.font: mono
    }
    
    preserves_contiguity: {
      label: "âœ“ PRESERVES Contiguity"
      style.fill: "#E4FFE4"
      style.stroke: green
      
      op1: ".view(*shape)"
      op2: ".reshape(*shape)"
      op3: ".flatten()"
      op4: ".squeeze() / .unsqueeze()"
      op5: ".contiguous()"
      op6: ".clone()"
      
      op1.style.font: mono
      op2.style.font: mono
      op3.style.font: mono
      op4.style.font: mono
      op5.style.font: mono
      op6.style.font: mono
    }
  }
}

STEP_3: {
  label: "Step 3: The Error Scenario"
  
  error_example: {
    label: "RuntimeError Example"
    style.fill: "#2D2D2D"
    style.font-color: white
    style.font: mono
    
    code: |py
      x = torch.randn(2, 3)        # Contiguous âœ“
      y = x.transpose(0, 1)        # Now [3, 2], NON-contiguous âœ—
      z = y.view(6)                # ðŸ’¥ RuntimeError!
      
      # Error message:
      # "view size is not compatible with input
      #  tensor's strides and shape"
    |
  }
  
  why_fails: {
    label: "Why This Fails"
    style.fill: "#FFF3E0"
    
    explanation: |md
      `.view()` requires:
      1. Total elements unchanged
      2. **Memory must be contiguous**
      
      After `.transpose()`, elements are not stored
      sequentially. `.view()` cannot reinterpret
      non-sequential memory as a different shape.
    |
  }
  
  error_example -> why_fails: "explains" {
    style.stroke-dash: 3
  }
}

STEP_4: {
  label: "Step 4: The Solution"
  
  solutions: {
    grid-columns: 2
    grid-gap: 40
    
    solution_a: {
      label: "Solution A: Add .contiguous()"
      style.fill: "#E8F5E9"
      style.stroke: green
      style.stroke-width: 3
      
      code: |py
        x = torch.randn(2, 3)
        y = x.transpose(0, 1)
        
        # âœ“ Explicit contiguous
        z = y.contiguous().view(6)
        
        # Memory is copied to make
        # it contiguous first
      |
      code.style.font: mono
    }
    
    solution_b: {
      label: "Solution B: Use .reshape()"
      style.fill: "#E8F5E9"
      style.stroke: green
      style.stroke-width: 3
      
      code: |py
        x = torch.randn(2, 3)
        y = x.transpose(0, 1)
        
        # âœ“ reshape handles it
        z = y.reshape(6)
        
        # reshape = view if contiguous
        # reshape = contiguous().view() if not
      |
      code.style.font: mono
    }
  }
  
  recommendation: {
    label: |md
      **Recommendation**: Use `.reshape()` for flexibility.
      Use `.view()` only when you KNOW the tensor is contiguous
      (e.g., right after creation or a `.contiguous()` call).
    |
    style.fill: "#E3F2FD"
  }
}

STEP_5: {
  label: "Step 5: In Transformer Attention"
  
  attention_context: {
    label: "Real-World Case: Multi-Head Attention"
    
    before_fix: {
      label: "âŒ BEFORE (Buggy)"
      style.fill: "#FFE4E4"
      style.stroke: red
      
      code: |py
        # After transpose for multi-head
        # shape: [batch, heads, seq, d_k]
        Q = Q.view(batch, heads, seq, d_k)
        
        # Transpose breaks contiguity
        Q = Q.transpose(1, 2)
        
        # ðŸ’¥ This will fail!
        Q_flat = Q.view(batch, seq, -1)
      |
      code.style.font: mono
    }
    
    after_fix: {
      label: "âœ“ AFTER (Fixed)"
      style.fill: "#E4FFE4"
      style.stroke: green
      
      code: |py
        # After transpose for multi-head
        Q = Q.view(batch, heads, seq, d_k)
        Q = Q.transpose(1, 2)
        
        # âœ“ Add contiguous() before view
        Q_flat = Q.contiguous().view(batch, seq, -1)
        
        # OR use reshape
        Q_flat = Q.reshape(batch, seq, -1)
      |
      code.style.font: mono
    }
    
    before_fix -> after_fix: "fix" {
      style.stroke: green
      style.stroke-width: 3
      style.bold: true
    }
  }
}

STEP_6: {
  label: "Step 6: Performance Implications"
  
  performance_note: {
    label: "Performance Trade-off"
    
    contiguous_cost: {
      label: ".contiguous() Cost"
      style.fill: "#FFF8E1"
      
      details: |md
        **Memory Copy**: Yes, triggers allocation + copy
        **When to use**: Only when `.view()` is required
        
        python
        x.contiguous()  # O(n) memory copy
        
      |
    }
    
    reshape_smart: {
      label: ".reshape() Smart Behavior"
      style.fill: "#E8F5E9"
      
      details: |md
        **No-Op if Contiguous**: Just like `.view()`
        **Copy if Needed**: Same as `.contiguous().view()`
        
        python
        # Contiguous input â†’ zero copy
        # Non-contiguous â†’ automatic copy
        x.reshape(shape)
        
      |
    }
    
    best_practice: {
      label: "Best Practice"
      style.fill: "#E3F2FD"
      style.stroke: blue
      
      details: |md
        1. **Prefer `.reshape()`** for safety
        2. **Use `.view()`** when performance-critical
           AND you control the tensor's history
        3. **Check with `.is_contiguous()`** in debug builds
      |
    }
  }
}

STEP_7: {
  label: "Step 7: Debug Checklist"
  
  checklist: {
    label: "Debug Checklist for view() Errors"
    
    item1: |md
      - [ ] Check `.is_contiguous()` before `.view()`
    |
    item1.shape: rectangle
    item1.style.fill: "#F5F5F5"
    
    item2: |md
      - [ ] Trace tensor through transpose/permute operations
    |
    item2.shape: rectangle
    item2.style.fill: "#F5F5F5"
    
    item3: |md
      - [ ] Verify total elements match: `x.numel() == new_shape.prod()`
    |
    item3.shape: rectangle
    item3.style.fill: "#F5F5F5"
    
    item4: |md
      - [ ] Use `.reshape()` as drop-in replacement to test
    |
    item4.shape: rectangle
    item4.style.fill: "#F5F5F5"
    
    item5: |md
      - [ ] Add `.contiguous()` after any `.transpose()` or `.permute()`
    |
    item5.shape: rectangle
    item5.style.fill: "#F5F5F5"
  }
}

STEP_1 -> STEP_2 -> STEP_3 -> STEP_4 -> STEP_5 -> STEP_6 -> STEP_7: "sequence" {
  style.stroke: "#666"
  style.stroke-dash: 5
}

summary: {
  label: ||md
    ## Summary
    
    | Operation | Requires Contiguity | Solution |
    |-----------|---------------------|----------|
    | `.view()` | **YES** | Add `.contiguous()` before |
    | `.reshape()` | NO | Handles automatically |
    | After transpose/permute | Check! | Usually needs `.contiguous()` |
  ||
  style.fill: "#FAFAFA"
  style.stroke: "#333"
  near: bottom-center
}