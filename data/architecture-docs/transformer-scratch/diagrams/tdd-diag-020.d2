vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Multi-Head Gradient Flow
  Backpropagation through parallel attention heads
| {near: top-center}

direction: right

classes: {
  forward: {
    style: {
      fill: "#E8F4FD"
      stroke: "#2E86AB"
    }
  }
  gradient: {
    style: {
      fill: "#FDE8E8"
      stroke: "#C41E3A"
      font-color: "#C41E3A"
    }
  }
  operation: {
    style: {
      fill: "#F0F0F0"
      stroke: "#666666"
    }
  }
  head: {
    style: {
      fill: "#E8F8E8"
      stroke: "#228B22"
    }
  }
}

Forward_Pass: {
  label: "Forward Pass →"
  style.fill: transparent
  
  Input: "Input X\n[batch, seq, d_model]" {
    class: forward
  }
  
  W_QKV: "W_Q, W_K, W_V\n[learned projections]" {
    class: operation
  }
  
  Reshape: "Reshape\n[batch, seq, n_heads, d_k]\n→ transpose →\n[batch, n_heads, seq, d_k]" {
    class: operation
  }
  
  Head_i: {
    label: "Head i"
    class: head
    Attn_i: "SDP Attention\nsoftmax(Q_iK_i^T/√d_k)V_i" {
      class: operation
    }
  }
  
  Concat: "Concat\n[batch, n_heads, seq, d_k]\n→ reshape →\n[batch, seq, d_model]" {
    class: operation
  }
  
  W_O: "W_O\n[learned projection]\n[d_model, d_model]" {
    class: operation
  }
  
  Output: "Output\n[batch, seq, d_model]" {
    class: forward
  }
  
  Input -> W_QKV: "X"
  W_QKV -> Reshape: "Q, K, V"
  Reshape -> Head_i: "Q_i, K_i, V_i"
  Head_i.Attn_i -> Concat: "head_i"
  Concat -> W_O: "concatenated"
  W_O -> Output: "final"
}

Gradient_Flow: {
  label: "← Gradient Flow (∂L/∂X)"
  style.fill: transparent
  
  Grad_Output: "∂L/∂Output\n[batch, seq, d_model]" {
    class: gradient
  }
  
  Grad_WO: "∂L/∂W_O\naccumulated across\nbatch × seq" {
    class: gradient
  }
  
  Grad_Concat: "∂L/∂Concat\n[batch, seq, d_model]" {
    class: gradient
  }
  
  Grad_Head_i: {
    label: "Head i Gradients"
    class: head
    Grad_V_i: "∂L/∂V_i\nweighted by\nattention scores" {
      class: gradient
    }
    Grad_K_i: "∂L/∂K_i\nvia softmax\n+ dot product" {
      class: gradient
    }
    Grad_Q_i: "∂L/∂Q_i\nvia softmax\n+ dot product" {
      class: gradient
    }
  }
  
  Grad_Reshape: "∂L/∂Q, ∂L/∂K, ∂L/∂V\n[batch, seq, d_model]" {
    class: gradient
  }
  
  Grad_WQKV: "∂L/∂W_Q, ∂L/∂W_K, ∂L/∂W_V\naccumulated across\nbatch × seq" {
    class: gradient
  }
  
  Grad_Input: "∂L/∂Input\n[batch, seq, d_model]" {
    class: gradient
  }
  
  Grad_Output -> Grad_WO: "matmul\ntranspose"
  Grad_WO -> Grad_Concat: "matmul\nW_O^T"
  Grad_Concat -> Grad_Head_i.Grad_V_i: "split + reshape"
  Grad_Head_i.Grad_Q_i -> Grad_Reshape: "concat gradients"
  Grad_Head_i.Grad_K_i -> Grad_Reshape
  Grad_Head_i.Grad_V_i -> Grad_Reshape
  Grad_Reshape -> Grad_WQKV: "accumulate"
  Grad_WQKV -> Grad_Input: "matmul\nW_Q^T + W_K^T + W_V^T"
}

Legend: {
  near: bottom-center
  Forward: {
    class: forward
    label: "Forward tensors"
  }
  Backward: {
    class: gradient
    label: "Gradient tensors"
  }
  Op: {
    class: operation
    label: "Operations"
  }
}

Annotation: |md
  ## Key Gradient Flow Properties
  
  1. **∂L/∂V_i** = attention_weights_i × ∂L/∂head_output_i
     - Gradients weighted by how much attention was paid
     
  2. **∂L/∂Q_i, ∂L/∂K_i** flow through softmax
     - Softmax gradient = p(1-p) for attended positions
     - Larger for uncertain (soft) attention
     
  3. **∂L/∂X** = ∂L/∂Q × W_Q^T + ∂L/∂K × W_K^T + ∂L/∂V × W_V^T
     - Gradients from all three projections sum together
     
  4. **W_O gradient** = ∂L/∂Output^T × concatenated_heads
     - Accumulated across batch and sequence dimensions
| {near: bottom-right}