vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Position-Wise Feed-Forward Network Architecture
  **Two linear transformations with ReLU activation**
| {near: top-center}

direction: right

classes: {
  input_output: {
    shape: rectangle
    style: {
      fill: "#E8F4FD"
      stroke: "#2196F3"
      stroke-width: 2
      font-color: "#1565C0"
    }
  }
  linear_layer: {
    shape: rectangle
    style: {
      fill: "#F3E5F5"
      stroke: "#9C27B0"
      stroke-width: 2
      font-color: "#7B1FA2"
    }
  }
  activation: {
    shape: diamond
    style: {
      fill: "#FFF3E0"
      stroke: "#FF9800"
      stroke-width: 2
      font-color: "#E65100"
    }
  }
  dropout: {
    shape: circle
    style: {
      fill: "#FFEBEE"
      stroke: "#F44336"
      stroke-width: 2
      font-color: "#C62828"
    }
  }
  annotation: {
    shape: text
    style: {
      font: mono
      font-size: 14
      font-color: "#616161"
    }
  }
}

input: "[batch, seq_len, d_model]" {
  class: input_output
  label: "Input Tensor\nx"
}

linear1: Linear1 {
  class: linear_layer
  label: "W₁ · x + b₁\n\nnn.Linear(d_model, d_ff)"
}

dim_annotation1: |md
  `d_model` → `d_ff`
  
  Typical: 512 → 2048
  Expansion: 4×
| {
  near: top-right
  class: annotation
}

relu: ReLU {
  class: activation
  label: "max(0, z)"
}

relu_annotation: |md
  Non-linearity
  
  
  z = W₁·x + b₁
  a = max(0, z)
  
| {
  near: top-right
  class: annotation
}

dropout: Dropout {
  class: dropout
  label: "p=0.1"
}

dropout_annotation: |md
  Random zeroing
  
  Regularization during
  training only
| {
  near: top-right
  class: annotation
}

linear2: Linear2 {
  class: linear_layer
  label: "W₂ · a + b₂\n\nnn.Linear(d_ff, d_model)"
}

dim_annotation2: |md
  `d_ff` → `d_model`
  
  Typical: 2048 → 512
  Projection back
| {
  near: top-right
  class: annotation
}

output: "[batch, seq_len, d_model]" {
  class: input_output
  label: "Output Tensor\nFFN(x)"
}

input -> linear1: "d_model=512"
linear1 -> relu: "d_ff=2048"
relu -> dropout: "d_ff=2048"
dropout -> linear2: "d_ff=2048"
linear2 -> output: "d_model=512"

legend: {
  near: bottom-center
  
  shape_info: |md
    **Dimension Flow:**
    
    [batch, seq_len, 512] → Linear1 → [batch, seq_len, 2048] → ReLU → [batch, seq_len, 2048]
                           → Dropout → [batch, seq_len, 2048] → Linear2 → [batch, seq_len, 512]
    
    
    **Mathematical Definition:**
    
    FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
    
    
    **Parameter Count (d_model=512, d_ff=2048):**
    - W₁: 512 × 2048 = 1,048,576
    - b₁: 2048
    - W₂: 2048 × 512 = 1,048,576
    - b₂: 512
    - **Total: ~2.1M parameters per FFN layer**
  |
}

residual_box: Residual Connection (outside FFN) {
  style: {
    stroke: "#4CAF50"
    stroke-dash: 5
    fill: transparent
  }
  
  note: |md
    Applied by Transformer block:
    
    output = x + FFN(x)
    
    (FFN does NOT include residual internally)
  |
}