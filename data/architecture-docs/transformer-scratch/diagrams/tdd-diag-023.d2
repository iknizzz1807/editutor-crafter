vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # Token Embedding Architecture
  `nn.Embedding` → √d_model scaling → shape transformation
| {near: top-center}

classes: {
  tensor: {
    style: {
      fill: "#E8F4FD"
      stroke: "#2563EB"
      font: mono
    }
  }
  operation: {
    shape: diamond
    style: {
      fill: "#FEF3C7"
      stroke: "#D97706"
    }
  }
  params: {
    style: {
      fill: "#F3E8FF"
      stroke: "#9333EA"
      font: mono
    }
  }
}

Input: "Input Tokens" {
  class: tensor
  shape: class
  token_ids: "torch.Tensor"
  shape_anno: "[batch, seq_len]"
  dtype: "torch.long"
  example: "[[101, 2054, 2003, ...]]"
}

Embedding: "Embedding Table\n(nn.Embedding)" {
  class: params
  shape: class
  vocab_size: "int (e.g., 30522)"
  d_model: "int (e.g., 512)"
  weight: "Parameter"
  weight_shape: "[vocab_size, d_model]"
  init: "Normal(0, 1)"
  total_params: "vocab_size × d_model"
}

Lookup: {
  class: operation
  label: "Table\nLookup"
}

RawEmbed: "Raw Embeddings" {
  class: tensor
  shape: class
  values: "torch.Tensor"
  shape_anno: "[batch, seq_len, d_model]"
  note: "Retrieved from weight matrix"
  dtype: "torch.float32"
}

Scale: {
  class: operation
  label: "× √d_model"
}

ScaleNote: |md
  **Why scale?**
  
  Matches positional encoding magnitude.
  
  With d_model=512:
  √512 ≈ 22.6
| {near: center-left}

Output: "Scaled Embeddings" {
  class: tensor
  shape: class
  values: "torch.Tensor"
  shape_anno: "[batch, seq_len, d_model]"
  dtype: "torch.float32"
  range: "≈ Normal(0, 1)"
}

Input -> Embedding: "token_ids index\ninto rows"
Embedding -> Lookup: "weight matrix\n[vocab, d_model]"
Lookup -> RawEmbed: "embeddings[selected_ids]\ngather operation"
RawEmbed -> Scale: "multiply by\nsqrt(d_model)"
Scale -> Output: "ready for\n+ positional encoding"

Code: |md
  python
  class TokenEmbedding(nn.Module):
      def __init__(self, vocab_size: int, d_model: int):
          super().__init__()
          self.embedding = nn.Embedding(vocab_size, d_model)
          self.scale = math.sqrt(d_model)
      
      def forward(self, x: torch.Tensor) -> torch.Tensor:
          # x: [batch, seq_len] (long)
          # out: [batch, seq_len, d_model] (float32)
          return self.embedding(x) * self.scale
  
| {near: bottom-center}

MemoryLayout: "Memory Layout" {
  direction: right
  
  Table: "Embedding Weight Matrix" {
    style.fill: "#F3E8FF"
    shape: class
    row0: "token 0 → [e₀₀, e₀₁, ..., e₀₅₁₁]"
    row1: "token 1 → [e₁₀, e₁₁, ..., e₁₅₁₁]"
    dots: "..."
    rowV: "token V → [eᵥ₀, eᵥ₁, ..., eᵥ₅₁₁]"
  }
  
  Index: "Indexing Operation" {
    style.fill: "#E8F4FD"
    shape: class
    input: "token_ids[i,j] = k"
    output: "result[i,j,:] = weight[k,:]"
  }
  
  Table -> Index: "gather rows"
}

Dims: "Shape Transformation" {
  grid-columns: 3
  grid-gap: 20
  
  step1: "Input" {
    class: tensor
    label: "[batch, seq_len]"
  }
  step2: "Lookup" {
    class: tensor
    label: "[batch, seq_len, d_model]"
  }
  step3: "Scale" {
    class: tensor
    label: "[batch, seq_len, d_model]"
  }
  
  arrow1: "→"
  arrow2: "→"
}

Dims.arrow1.shape: text
Dims.arrow2.shape: text
Dims.step2.style.stroke: "#22C55E"
Dims.step2.style.bold: true