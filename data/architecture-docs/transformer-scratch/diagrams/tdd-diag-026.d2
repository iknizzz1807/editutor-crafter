vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Positional Encoding Storage: `register_buffer` vs `nn.Parameter`
| {near: top-center}

direction: right

classes: {
  tensor-box: {
    style: {
      fill: "#E8F4FD"
      stroke: "#2E86AB"
      stroke-width: 2
      border-radius: 4
    }
  }
  param-box: {
    style: {
      fill: "#FFF3E0"
      stroke: "#E65100"
      stroke-width: 2
      border-radius: 4
    }
  }
  optimizer-box: {
    shape: diamond
    style: {
      fill: "#E8F5E9"
      stroke: "#2E7D32"
      stroke-width: 2
    }
  }
  no-update: {
    style: {
      fill: "#FFEBEE"
      stroke: "#C62828"
      stroke-width: 2
      stroke-dash: 3
    }
  }
}

section_register_buffer: "register_buffer('pe', tensor)" {
  style.fill: white
  style.stroke: "#2E86AB"
  style.stroke-width: 3

  pe_tensor: |md
    **PE Tensor**
    
    pe: [max_len, d_model]
    
    *Sinusoidal encoding*
    *Computed once, fixed forever*
  | {class: tensor-box}

  pe_tensor -> state_dict: Saved to .state_dict()
  pe_tensor -> gpu_transfer: Moved with .to(device)
  pe_tensor -> optimizer_check: Checked by optimizer?
}

section_nn_parameter: "nn.Parameter(tensor)" {
  style.fill: white
  style.stroke: "#E65100"
  style.stroke-width: 3

  learned_tensor: |md
    **Learned Embedding**
    
    self.pe = nn.Parameter(
      torch.randn(max_len, d_model)
    )
    
    *Random init, updated every step*
  | {class: param-box}

  learned_tensor -> state_dict2: Saved to .state_dict()
  learned_tensor -> gpu_transfer2: Moved with .to(device)
  learned_tensor -> optimizer_check2: Checked by optimizer?
}

optimizer_check: "Optimizer sees this?" {
  class: no-update
  label: |md
    **NO**
    
    `requires_grad = False`
    
    (inherently for buffers)
  |
}

optimizer_check2: "Optimizer sees this?" {
  class: optimizer-box
  label: |md
    **YES**
    
    `requires_grad = True`
    
    (by default for Parameters)
  |
}

optimizer_check -> gradient_flow: |md
    No gradient
    No update
| {
  style.stroke: "#C62828"
  style.stroke-dash: 5
}

optimizer_check2 -> gradient_flow2: |md
    ∂L/∂pe computed
    pe ← pe - lr × ∂L/∂pe
| {
  style.stroke: "#2E7D32"
  style.animated: true
}

gradient_flow: |md
  **Behavior**
  
  Position encoding is **constant**.
  
  Same sinusoidal pattern
  for all training steps.
| {
  style.fill: "#FFEBEE"
  style.stroke: "#C62828"
  style.stroke-dash: 3
}

gradient_flow2: |md
  **Behavior**
  
  Position encoding is **learned**.
  
  Model discovers optimal
  positional representations.
| {
  style.fill: "#E8F5E9"
  style.stroke: "#2E7D32"
}

footer: ||md
  ## Key Insight
  
  | Method | `requires_grad` | Optimizer Updates | Use Case |
  |--------|-----------------|-------------------|----------|
  | `register_buffer` | `False` | ✗ | Fixed sinusoidal PE |
  | `nn.Parameter` | `True` | ✓ | Learned position embeddings |
  
  Both are saved in `state_dict()` and moved with `.to(device)`.
  The difference is **gradient computation** and **optimizer updates**.
|| {near: bottom-center}