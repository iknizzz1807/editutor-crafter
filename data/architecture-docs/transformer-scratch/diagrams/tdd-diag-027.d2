vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

classes: {
  header: {
    style: {
      fill: "#7C3AED"
      font-color: white
      bold: true
    }
  }
  data: {
    style: {
      fill: "#3B82F6"
      font-color: white
    }
  }
  process: {
    style: {
      fill: "#10B981"
      font-color: white
    }
  }
  tensor: {
    style: {
      fill: "#1E293B"
      font-color: "#94A3B8"
      font: mono
    }
  }
  annotation: {
    shape: text
    style: {
      font-size: 12
      font-color: "#64748B"
    }
  }
}

title: |md
  # Combined Embedding Flow
  Token → Embedding → Positional Encoding → Output
| {near: top-center}

input: "Token IDs\n[batch, seq_len]" {
  class: data
  width: 140
}

lookup: "Embedding\nLookup\nnn.Embedding" {
  class: process
  width: 120
}

embeddings: "Raw Embeddings\n[batch, seq_len, d_model]" {
  class: tensor
  width: 180
}

scale_op: "× √d_model" {
  class: process
  width: 100
}

scaled: "Scaled Embeddings\n[batch, seq_len, d_model]" {
  class: tensor
  width: 180
}

pe_lookup: "Positional\nEncoding\nLookup/Table" {
  class: process
  width: 120
}

pe_values: "PE Vectors\n[1, seq_len, d_model]" {
  class: tensor
  width: 160
}

add_op: "+" {
  class: process
  shape: circle
  width: 50
}

combined: "Combined\nEmbeddings\n[batch, seq_len, d_model]" {
  class: tensor
  width: 180
}

dropout_op: "Dropout\n(p=0.1)" {
  class: process
  width: 100
}

output: "Final Output\n[batch, seq_len, d_model]" {
  class: data
  width: 160
  style: {
    fill: "#059669"
    font-color: white
    bold: true
  }
}

input -> lookup: "indices"
lookup -> embeddings: "float32"
embeddings -> scale_op
scale_op -> scaled

pe_lookup -> pe_values: "pos 0..seq_len-1"

scaled -> add_op
pe_values -> add_op

add_op -> combined
combined -> dropout_op
dropout_op -> output

note_scaling: |md
  Scaling by √d_model ensures  
  embeddings have similar  
  magnitude to positional  
  encodings for stable training
| {
  near: center-left
  class: annotation
}

note_pe: |md
  PE = sin/cos at different  
  frequencies - allows model  
  to learn relative positions
| {
  near: bottom-left
  class: annotation
}

note_broadcast: |md
  PE broadcasts across  
  batch dimension
| {
  near: bottom-center
  class: annotation
}

note_dropout: |md
  Regularization - randomly  
  zeros embeddings during  
  training only
| {
  near: bottom-right
  class: annotation
}

legend: {
  near: top-right
  Data: {class: data}
  Process: {class: process}
  Tensor: {class: tensor}
}