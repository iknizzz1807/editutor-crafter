vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

shape: sequence_diagram

TestRunner
EmbeddingModule: "Embedding Module\n(d_model=512)"
TokenEmbedding: "Token Embedding\n(vocab_size=30000, d_model=512)"
PositionalEncoding: "Positional Encoding\n(max_len=512, d_model=512)"

TestRunner."Setup: d_model=512, vocab_size=30000"

setup_phase: {
  TestRunner -> EmbeddingModule: "__init__(d_model=512, vocab_size=30000)"
  EmbeddingModule -> TokenEmbedding: "nn.Embedding(30000, 512)"
  EmbeddingModule -> PositionalEncoding: "PositionalEncoding(512, max_len=512)"
  TestRunner <- EmbeddingModule: "module initialized"
}

forward_pass: {
  TestRunner -> EmbeddingModule: "forward(token_ids)\ntoken_ids.shape = [2, 10]"
  
  EmbeddingModule.t1 -> TokenEmbedding.t1: "embed(token_ids)\nlookup table"
  TokenEmbedding.t1 -> EmbeddingModule.t1: "token_embeds\n[2, 10, 512]"
  
  EmbeddingModule.t2 -> PositionalEncoding.t2: "pos_enc[:10, :]\n[10, 512]"
  PositionalEncoding.t2 -> EmbeddingModule.t2: "positional encoding\n[10, 512]"
  
  EmbeddingModule.t3: "output = token_embeds + pos_enc"
}

verification: {
  EmbeddingModule -> TestRunner: "output tensor\n[2, 10, 512]"
  
  TestRunner."Verify 1: output.shape == [2, 10, 512]"
  TestRunner."Verify 2: output.dtype == torch.float32"
  TestRunner."Verify 3: PE added (output ≠ token_embeds)"
}

TestRunner."✓ All assertions passed"

TestRunner -> EmbeddingModule: "forward(padded_ids)\ntoken_ids = [2, 5, 10] (variable len)"
EmbeddingModule -> TestRunner: "output [2, 5, 10, 512]"

TestRunner."✓ Handles variable batch sizes"