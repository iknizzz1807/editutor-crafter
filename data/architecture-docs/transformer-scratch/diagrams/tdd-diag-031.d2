vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # EncoderLayer Architecture (Post-LN)
  sizeof=64 bytes (one cache line per token)
| {near: top-center}

input: "Input\n[batch, seq, d_model]" {
  style.fill: "#E4DBFE"
  style.stroke: "#7C5CBF"
}

sublayer1: "Sublayer 1: Self-Attention" {
  style.fill: "#C7F1FF"
  style.stroke: "#2B8CAF"
  
  self_attn: Self-Attention {
    shape: class
    "W_Q": "Linear(d_model to d_k)"
    "W_K": "Linear(d_model to d_k)"
    "W_V": "Linear(d_model to d_v)"
    "forward(Q, K, V, mask)": Tensor
  }
  
  dropout1: "Dropout(p=0.1)"
}

residual1: Add {
  shape: circle
  style.fill: "#FFE7CB"
  style.stroke: "#E8A838"
}

norm1: LayerNorm {
  shape: class
  weight: "Parameter[d_model]"
  bias: "Parameter[d_model]"
  eps: "float = 1e-6"
  "forward(x)": Tensor
  style.fill: "#F6C889"
}

sublayer2: "Sublayer 2: Feed-Forward" {
  style.fill: "#C7F1FF"
  style.stroke: "#2B8CAF"
  
  ff: FeedForward {
    shape: class
    "W_1": "Linear(d_model to d_ff)"
    "W_2": "Linear(d_ff to d_model)"
    activation: "GELU()"
    dropout: "Dropout(p=0.1)"
  }
}

residual2: Add {
  shape: circle
  style.fill: "#FFE7CB"
  style.stroke: "#E8A838"
}

norm2: LayerNorm {
  shape: class
  weight: "Parameter[d_model]"
  bias: "Parameter[d_model]"
  eps: "float = 1e-6"
  "forward(x)": Tensor
  style.fill: "#F6C889"
}

output: "Output\n[batch, seq, d_model]" {
  style.fill: "#E4DBFE"
  style.stroke: "#7C5CBF"
}

input -> sublayer1.self_attn: "x"
sublayer1.self_attn -> sublayer1.dropout1: "attn_out"
sublayer1.dropout1 -> residual1: "sublayer_output"
input -> residual1: "residual skip" {
  style.stroke: "#7C5CBF"
  style.stroke-dash: 5
}
residual1 -> norm1: "x + sublayer(x)"
norm1 -> sublayer2.ff: "norm1_out"
sublayer2.ff -> residual2: "ff_out"
norm1 -> residual2: "residual skip" {
  style.stroke: "#7C5CBF"
  style.stroke-dash: 5
}
residual2 -> norm2: "x + FFN(x)"
norm2 -> output: "encoder_output"

legend: {
  near: bottom-center
  style.fill: transparent
  
  header: |md
    **Residual Connection Flow:**
    `output = LayerNorm(x + Sublayer(x))`
  |
  
  residual_legend: "Residual Skip" {
    style.stroke: "#7C5CBF"
    style.stroke-dash: 5
  }
  
  data_legend: "Data Flow" {
    style.stroke: "#2B8CAF"
  }
}

data_flow_note: |md
  **Post-LN Pattern:**
  1. Self-Attention -> Dropout -> Add(residual) -> LayerNorm
  2. FeedForward -> Dropout -> Add(residual) -> LayerNorm
  
  **Why Post-LN:** Original transformer uses Post-LN.
  Pre-LN (LayerNorm before sublayer) provides more 
  stable gradients for deep networks.
| {near: bottom-right}