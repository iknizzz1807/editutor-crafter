vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # EncoderLayer Architecture: Pre-LN vs Post-LN
| {near: top-center}

direction: right

classes: {
  input_block: {
    style: {
      fill: "#E8E8E8"
      stroke: "#666666"
      stroke-width: 2
      border-radius: 4
      font: mono
      font-size: 20
    }
  }
  
  norm_block: {
    style: {
      fill: "#9B59B6"
      stroke: "#7D3C98"
      stroke-width: 2
      border-radius: 4
      font-color: white
      font: mono
      bold: true
    }
  }
  
  attention_block: {
    style: {
      fill: "#3498DB"
      stroke: "#2980B9"
      stroke-width: 2
      border-radius: 4
      font-color: white
      font: mono
      bold: true
    }
  }
  
  ffn_block: {
    style: {
      fill: "#27AE60"
      stroke: "#1E8449"
      stroke-width: 2
      border-radius: 4
      font-color: white
      font: mono
      bold: true
    }
  }
  
  residual_arrow: {
    style: {
      stroke: "#E67E22"
      stroke-width: 3
      stroke-dash: 4
    }
  }
  
  flow_arrow: {
    style: {
      stroke: "#2C3E50"
      stroke-width: 2
    }
  }
  
  add_block: {
    shape: circle
    style: {
      fill: "#E74C3C"
      stroke: "#C0392B"
      stroke-width: 2
      font-color: white
      bold: true
      font-size: 24
    }
  }
  
  output_block: {
    style: {
      fill: "#E8E8E8"
      stroke: "#666666"
      stroke-width: 2
      border-radius: 4
      font: mono
      font-size: 20
    }
  }
  
  section_container: {
    style: {
      stroke: "#BDC3C7"
      stroke-dash: 3
      fill: transparent
    }
  }
}

PreLN: "Pre-LayerNorm (Modern)" {
  class: section_container
  
  preln_input: "x\n[batch, seq, d_model]" {
    class: input_block
  }
  
  preln_norm1: "LayerNorm" {
    class: norm_block
  }
  
  preln_attn: "Self-Attention" {
    class: attention_block
  }
  
  preln_add1: "+" {
    class: add_block
  }
  
  preln_norm2: "LayerNorm" {
    class: norm_block
  }
  
  preln_ffn: "FFN\n(d_model → 4d → d_model)" {
    class: ffn_block
  }
  
  preln_add2: "+" {
    class: add_block
  }
  
  preln_output: "out\n[batch, seq, d_model]" {
    class: output_block
  }
  
  preln_formula: |md
    **Mathematical Form:**

    out = x + FFN(LayerNorm(x + Attn(LayerNorm(x))))

    **Key Property:** Norm applied BEFORE each sublayer
  | {
    style.fill: transparent
  }
  
  preln_input -> preln_norm1: "forward" {class: flow_arrow}
  preln_norm1 -> preln_attn: "Q, K, V" {class: flow_arrow}
  preln_attn -> preln_add1: "attn_out" {class: flow_arrow}
  preln_input -> preln_add1: "residual" {
    class: residual_arrow
    style: {
      stroke: "#E67E22"
      animated: true
    }
  }
  preln_add1 -> preln_norm2: "sublayer_out" {class: flow_arrow}
  preln_norm2 -> preln_ffn: "expanded" {class: flow_arrow}
  preln_ffn -> preln_add2: "ffn_out" {class: flow_arrow}
  preln_add1 -> preln_add2: "residual" {
    class: residual_arrow
    style: {
      stroke: "#E67E22"
      animated: true
    }
  }
  preln_add2 -> preln_output: "output" {class: flow_arrow}
}

PostLN: "Post-LayerNorm (Original 2017)" {
  class: section_container
  
  postln_input: "x\n[batch, seq, d_model]" {
    class: input_block
  }
  
  postln_attn: "Self-Attention" {
    class: attention_block
  }
  
  postln_add1: "+" {
    class: add_block
  }
  
  postln_norm1: "LayerNorm" {
    class: norm_block
  }
  
  postln_ffn: "FFN\n(d_model → 4d → d_model)" {
    class: ffn_block
  }
  
  postln_add2: "+" {
    class: add_block
  }
  
  postln_norm2: "LayerNorm" {
    class: norm_block
  }
  
  postln_output: "out\n[batch, seq, d_model]" {
    class: output_block
  }
  
  postln_formula: |md
    **Mathematical Form:**

    out = LayerNorm(x + FFN(LayerNorm(x + Attn(x))))

    **Key Property:** Norm applied AFTER residual addition
  | {
    style.fill: transparent
  }
  
  postln_input -> postln_attn: "Q, K, V" {class: flow_arrow}
  postln_attn -> postln_add1: "attn_out" {class: flow_arrow}
  postln_input -> postln_add1: "residual" {
    class: residual_arrow
    style: {
      stroke: "#E67E22"
      animated: true
    }
  }
  postln_add1 -> postln_norm1: "sublayer_out" {class: flow_arrow}
  postln_norm1 -> postln_ffn: "expanded" {class: flow_arrow}
  postln_ffn -> postln_add2: "ffn_out" {class: flow_arrow}
  postln_norm1 -> postln_add2: "residual" {
    class: residual_arrow
    style: {
      stroke: "#E67E22"
      animated: true
    }
  }
  postln_add2 -> postln_norm2: "pre_norm" {class: flow_arrow}
  postln_norm2 -> postln_output: "output" {class: flow_arrow}
}

PreLN -> PostLN: "Evolution\n(2017 → 2020+)" {
  style: {
    stroke: "#95A5A6"
    stroke-dash: 5
    font-color: "#7F8C8D"
  }
}

Comparison: ||md
  ## Pre-LN vs Post-LN Trade-offs

  | Aspect | Pre-LN | Post-LN |
  |--------|--------|---------|
  | **Training Stability** | Better | Requires warmup |
  | **Gradient Flow** | Direct path to input | Through all norms |
  | **Learning Rate Warmup** | Often unnecessary | Required |
  | **Final Output Scale** | Unnormalized | Normalized |
  | **Original Paper** | GPT-2/3, LLaMA | Vaswani et al. 2017 |

  **Why Pre-LN Won:**
  - Residual connections preserve gradient magnitude
  - LayerNorm before sublayer prevents activation explosion
  - Enables training without learning rate warmup
  - Better for very deep networks (more than 12 layers)
|| {
  near: bottom-right
  style: {
    fill: "#FAFAFA"
    stroke: "#BDC3C7"
    border-radius: 8
    font-size: 18
  }
}

Legend: {
  near: top-right
  style.fill: transparent
  
  leg_norm: "LayerNorm" {class: norm_block}
  leg_attn: "Self-Attention" {class: attention_block}
  leg_ffn: "FFN" {class: ffn_block}
  leg_add: "+" {class: add_block}
  
  leg_note: |md
    **Residual** = Orange dashed arrows
    **Data flow** = Dark solid arrows
  | {style.fill: transparent}
}