direction: right
layout-engine: elk
theme-id: 200

title: |md
  # DecoderLayer Architecture
  **Masked Self-Attention → Cross-Attention → Feed-Forward Network**
  Each sublayer wrapped with residual connection + layer normalization
| {near: top-center}

classes: {
  input_output: {
    style: {
      fill: "#E8F4FD"
      stroke: "#2563EB"
      stroke-width: 2
      border-radius: 8
    }
  }
  attention: {
    style: {
      fill: "#FEF3C7"
      stroke: "#D97706"
      stroke-width: 2
      border-radius: 6
    }
  }
  norm: {
    style: {
      fill: "#DBEAFE"
      stroke: "#3B82F6"
      stroke-width: 2
      border-radius: 4
    }
  }
  ffn: {
    style: {
      fill: "#D1FAE5"
      stroke: "#059669"
      stroke-width: 2
      border-radius: 6
    }
  }
  residual: {
    style: {
      stroke: "#6366F1"
      stroke-width: 2
      stroke-dash: 4
    }
  }
  data_flow: {
    style: {
      stroke: "#1F2937"
      stroke-width: 2
    }
  }
  annotation: {
    shape: text
    style: {
      font-size: 14
      italic: true
      fill: transparent
      stroke: transparent
    }
  }
}

input: "Decoder Input\nx ∈ ℝ^{batch×seq×d_model}" {
  class: input_output
  shape: rectangle
  width: 140
}

sublayer1: {
  label: "Sublayer 1"
  width: 200
  
  masked_self_attn: "Masked\nSelf-Attention" {
    class: attention
    width: 120
    tooltip: |md
      Queries, Keys, Values all from decoder input
      Causal mask prevents attending to future positions
      Q, K, V ∈ ℝ^{batch×seq×d_model}
    |
  }
  
  add_norm1: "Add &\nLayerNorm" {
    class: norm
    width: 80
    tooltip: |md
      output = LayerNorm(x + Sublayer(x))
      Residual preserves gradient flow
      Norm stabilizes training
    |
  }
  
  residual1: |md
    **Residual 1**
    x₁ = LayerNorm(x + Attn(x))
  |
  
  masked_self_attn -> add_norm1: "attn_output"
}

sublayer2: {
  label: "Sublayer 2"
  width: 200
  
  cross_attn: "Cross-\nAttention" {
    class: attention
    width: 120
    style.fill: "#FEE2E2"
    style.stroke: "#DC2626"
    tooltip: |md
      Q from decoder, K & V from encoder
      Queries decoder state against encoder outputs
      Q ∈ ℝ^{batch×seq_dec×d_model}
      K, V ∈ ℝ^{batch×seq_enc×d_model}
    |
  }
  
  add_norm2: "Add &\nLayerNorm" {
    class: norm
    width: 80
  }
  
  residual2: |md
    **Residual 2**
    x₂ = LayerNorm(x₁ + CrossAttn(x₁))
  |
  
  cross_attn -> add_norm2: "cross_attn_output"
}

sublayer3: {
  label: "Sublayer 3"
  width: 200
  
  ffn: "Feed-Forward\nNetwork" {
    class: ffn
    width: 120
    tooltip: |md
      FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
      d_ff = 4 × d_model (typically)
      Expansion: 512 → 2048 → 512
    |
  }
  
  add_norm3: "Add &\nLayerNorm" {
    class: norm
    width: 80
  }
  
  residual3: |md
    **Residual 3**
    x₃ = LayerNorm(x₂ + FFN(x₂))
  |
  
  ffn -> add_norm3: "ffn_output"
}

output: "DecoderLayer\nOutput\n∈ ℝ^{batch×seq×d_model}" {
  class: input_output
  shape: rectangle
  width: 140
}

encoder_input: "Encoder Output\n(memory)\n∈ ℝ^{batch×seq_enc×d_model}" {
  class: input_output
  style.fill: "#FCE7F3"
  style.stroke: "#DB2777"
  shape: rectangle
  width: 140
}

input -> sublayer1.masked_self_attn: "x" {class: data_flow}

sublayer1.add_norm1 -> sublayer2.cross_attn: "x₁" {class: data_flow}
sublayer2.add_norm2 -> sublayer3.ffn: "x₂" {class: data_flow}
sublayer3.add_norm3 -> output: "x₃" {class: data_flow}

encoder_input -> sublayer2.cross_attn: "K, V" {
  class: data_flow
  style.stroke: "#DB2777"
  style.stroke-dash: 3
}

legend: {
  near: bottom-center
  width: 600
  
  att_box: "" {width: 20; height: 20; style.fill: "#FEF3C7"; style.stroke: "#D97706"}
  att_label: "Self-Attention"
  
  cross_box: "" {width: 20; height: 20; style.fill: "#FEE2E2"; style.stroke: "#DC2626"}
  cross_label: "Cross-Attention"
  
  ffn_box: "" {width: 20; height: 20; style.fill: "#D1FAE5"; style.stroke: "#059669"}
  ffn_label: "Feed-Forward"
  
  norm_box: "" {width: 20; height: 20; style.fill: "#DBEAFE"; style.stroke: "#3B82F6"}
  norm_label: "Add & Norm"
}

data_flow_legend: |md
  **Data Flow:** Solid arrows show tensor flow through the decoder layer
  **Encoder → Decoder:** Dashed pink arrows show encoder outputs feeding cross-attention
  **Residual Connections:** Each sublayer output is added to its input before normalization
| {
  near: bottom-right
  shape: text
  style.font-size: 12
}

dimensions_note: |md
  **Dimension Preservation:**
  - Input: [batch, seq_len, d_model]
  - Every sublayer preserves shape
  - Output: [batch, seq_len, d_model]
  - Enables stacking N decoder layers
| {
  near: bottom-left
  shape: text
  style.font-size: 12
}