vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Cross-Attention Mechanism
  Query from Decoder, Key/Value from Encoder
| {near: top-center}

direction: right

decoder_hidden: Decoder Hidden State {
  shape: rectangle
  style.fill: "#E8D5F0"
  style.stroke: "#9B59B6"
  
  label: |md
    **Shape:** `[batch, seq_len_dec, d_model]`
    
    Each decoder position has a hidden state
    that queries encoder information.
  |
}

encoder_output: Encoder Output {
  shape: rectangle
  style.fill: "#D5E8F0"
  style.stroke: "#3498DB"
  
  label: |md
    **Shape:** `[batch, seq_len_enc, d_model]`
    
    Full encoder context — all positions
    available for attention.
  |
}

projections: Q/K/V Projections {
  shape: rectangle
  style.fill: "#F5F5F5"
  style.stroke: "#666666"
  
  W_Q_decoder: {
    shape: rectangle
    style.fill: "#E8D5F0"
    label: |md
      **W_Q** (Decoder)
      `nn.Linear(d_model, d_k)`
    |
  }
  
  W_K_encoder: {
    shape: rectangle
    style.fill: "#D5E8F0"
    label: |md
      **W_K** (Encoder)
      `nn.Linear(d_model, d_k)`
    |
  }
  
  W_V_encoder: {
    shape: rectangle
    style.fill: "#D5E8F0"
    label: |md
      **W_V** (Encoder)
      `nn.Linear(d_model, d_v)`
    |
  }
}

Q: Query Q {
  shape: rectangle
  style.fill: "#E8D5F0"
  style.stroke: "#9B59B6"
  style.bold: true
  
  label: |md
    **Shape:** `[batch, seq_len_dec, d_k]`
    
    "What do I need from encoder?"
  |
}

K: Key K {
  shape: rectangle
  style.fill: "#D5E8F0"
  style.stroke: "#3498DB"
  style.bold: true
  
  label: |md
    **Shape:** `[batch, seq_len_enc, d_k]`
    
    "What information do I encode?"
  |
}

V: Value V {
  shape: rectangle
  style.fill: "#D5E8F0"
  style.stroke: "#3498DB"
  style.bold: true
  
  label: |md
    **Shape:** `[batch, seq_len_enc, d_v]`
    
    "What content do I provide?"
  |
}

attention_scores: Attention Scores {
  shape: rectangle
  style.fill: "#FFF3CD"
  style.stroke: "#FFC107"
  
  label: |md
    **scores = Q @ K^T / √d_k**
    
    **Shape:** `[batch, seq_len_dec, seq_len_enc]`
    
    Each decoder position scores
    similarity with all encoder positions.
  |
}

attention_weights: Attention Weights {
  shape: rectangle
  style.fill: "#D4EDDA"
  style.stroke: "#28A745"
  
  label: |md
    **weights = softmax(scores, dim=-1)**
    
    **Shape:** `[batch, seq_len_dec, seq_len_enc]`
    
    Probability distribution over
    encoder positions (rows sum to 1).
  |
}

context: Context Vector {
  shape: rectangle
  style.fill: "#C3E6CB"
  style.stroke: "#1E7E34"
  style.bold: true
  
  label: |md
    **context = weights @ V**
    
    **Shape:** `[batch, seq_len_dec, d_v]`
    
    Weighted combination of encoder
    values based on attention weights.
  |
}

decoder_output: Decoder Output {
  shape: rectangle
  style.fill: "#E8D5F0"
  style.stroke: "#9B59B6"
  style.double-border: true
  
  label: |md
    **Combined with decoder hidden**
    
    `output = LayerNorm(context + decoder_hidden)`
    
    Feed to next decoder layer.
  |
}

legend: |md
  ## Key Insight
  
  **Cross-attention differs from self-attention:**
  - Q comes from **decoder** (current generation)
  - K, V come from **encoder** (source context)
  
  This allows the decoder to "look up" relevant
  encoder information while generating each token.
  
  
  # Pseudocode
  Q = W_Q(decoder_hidden)  # From decoder
  K = W_K(encoder_output)  # From encoder
  V = W_V(encoder_output)  # From encoder
  
  scores = Q @ K.T / sqrt(d_k)
  weights = softmax(scores)
  context = weights @ V
  
| {near: bottom-center}

decoder_hidden -> projections.W_Q_decoder: decoder state
encoder_output -> projections.W_K_encoder: encoder output
encoder_output -> projections.W_V_encoder: encoder output

projections.W_Q_decoder -> Q
projections.W_K_encoder -> K
projections.W_V_encoder -> V

Q -> attention_scores: Q
K -> attention_scores: K^T

attention_scores -> attention_weights: softmax

attention_weights -> context: weights
V -> context: V

context -> decoder_output

# Dimension flow annotation
dim_flow: |md
  **Dimension Flow:**
  
  Decoder: [B, L_dec, D] → Q: [B, L_dec, d_k]
  Encoder: [B, L_enc, D] → K: [B, L_enc, d_k]
  Encoder: [B, L_enc, D] → V: [B, L_enc, d_v]
  
  Scores: [B, L_dec, L_enc] (each dec pos × each enc pos)
  Context: [B, L_dec, d_v]
  
| {near: center-left}