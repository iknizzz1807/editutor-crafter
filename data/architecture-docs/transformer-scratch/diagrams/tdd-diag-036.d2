vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Decoder Stack Data Flow
  Target Input → Masked Self-Attention → Cross-Attention → Output
| {near: top-center}

direction: down

classes: {
  input_class: {
    style: {
      fill: "#E8F5E9"
      stroke: "#4CAF50"
      stroke-width: 2
    }
  }
  masked_attn_class: {
    style: {
      fill: "#E3F2FD"
      stroke: "#2196F3"
      stroke-width: 2
    }
  }
  cross_attn_class: {
    style: {
      fill: "#FFF3E0"
      stroke: "#FF9800"
      stroke-width: 2
    }
  }
  ffn_class: {
    style: {
      fill: "#F3E5F5"
      stroke: "#9C27B0"
      stroke-width: 2
    }
  }
  output_class: {
    style: {
      fill: "#FFEBEE"
      stroke: "#F44336"
      stroke-width: 2
    }
  }
  residual_class: {
    style: {
      stroke: "#607D8B"
      stroke-dash: 3
      animated: true
    }
  }
  data_flow_class: {
    style: {
      stroke: "#333333"
      stroke-width: 2
    }
  }
  mask_class: {
    shape: hexagon
    style: {
      fill: "#FFCDD2"
      stroke: "#E53935"
    }
  }
  encoder_class: {
    style: {
      fill: "#E0E0E0"
      stroke: "#757575"
      stroke-dash: 5
    }
  }
}

# Input Stage
target_input: "Target Input\n[batch, tgt_len, d_model]" {
  class: input_class
  tooltip: |md
    Shifted-right target sequence
    During training: ground truth shifted by 1
    During inference: previously generated tokens
  |
}

pos_encoding_tgt: "+ Positional\nEncoding" {
  class: input_class
  tooltip: |md
    Sinusoidal positional encodings
    Added to target embeddings
    Same dimension as d_model
  |
}

# Masking
causal_mask: "Causal Mask\n(Upper Triangle)" {
  class: mask_class
  tooltip: |md
    Prevents attending to future positions
    [[0, 1, 1, 1],
     [0, 0, 1, 1],
     [0, 0, 0, 1],
     [0, 0, 0, 0]]
  |
}

# Decoder Layers (showing N=6 as stacked)
decoder_stack: "Decoder Stack (N=6 Layers)" {
  
  layer_1: "Layer 1" {
    direction: down
    
    masked_self_attn: "Masked Self-Attention\n(Q, K, V from target)" {
      class: masked_attn_class
      tooltip: |md
        Queries, Keys, Values all from target
        Causal mask applied before softmax
        Output: [batch, tgt_len, d_model]
      |
    }
    
    add_norm_1: "Add & Norm" {
      class: masked_attn_class
    }
    
    cross_attn: "Cross-Attention\n(Q from target, K,V from encoder)" {
      class: cross_attn_class
      tooltip: |md
        Queries from decoder state
        Keys, Values from encoder output
        Attends over source sequence
      |
    }
    
    add_norm_2: "Add & Norm" {
      class: cross_attn_class
    }
    
    ffn: "Feed-Forward\nNetwork" {
      class: ffn_class
      tooltip: |md
        Two linear layers with ReLU
        d_model → d_ff → d_model
        Typically d_ff = 4 * d_model
      |
    }
    
    add_norm_3: "Add & Norm" {
      class: ffn_class
    }
    
    masked_self_attn -> add_norm_1
    add_norm_1 -> cross_attn
    cross_attn -> add_norm_2
    add_norm_2 -> ffn
    ffn -> add_norm_3
  }
  
  layer_2_to_N: "Layers 2...N\n(same structure)" {
    style: {
      fill: "#FAFAFA"
      stroke: "#BDBDBD"
      stroke-dash: 5
    }
    style.multiple: true
  }
  
  layer_1 -> layer_2_to_N
}

# Encoder Output (external input)
encoder_output: "Encoder Output\n[batch, src_len, d_model]" {
  class: encoder_class
  tooltip: |md
    Final encoder layer output
    Provides K, V for cross-attention
    Computed once, reused each decoder layer
  |
}

# Final Output Stage
final_layer_norm: "Final Layer Norm" {
  class: output_class
}

linear_proj: "Linear Projection\n[d_model → vocab_size]" {
  class: output_class
  tooltip: |md
    Projects to vocabulary logits
    [batch, tgt_len, vocab_size]
    Often shares weights with embedding
  |
}

softmax_output: "Softmax\n→ Probabilities" {
  class: output_class
  tooltip: |md
    Converts logits to probabilities
    [batch, tgt_len, vocab_size]
    Sum to 1 over vocabulary
  |
}

output_tokens: "Output Tokens\n(Predictions)" {
  class: output_class
  tooltip: |md
    Generated token sequence
    argmax or sampling from probs
    Fed back as input during inference
  |
}

# Data Flow Connections
target_input -> pos_encoding_tgt: "embeddings" {class: data_flow_class}
pos_encoding_tgt -> decoder_stack.layer_1.masked_self_attn: "x₀" {class: data_flow_class}
causal_mask -> decoder_stack.layer_1.masked_self_attn: "mask" {
  style: {
    stroke: "#E53935"
    stroke-dash: 3
  }
}

# Residual connections (conceptual)
decoder_stack.layer_1.masked_self_attn -> decoder_stack.layer_1.add_norm_1: "+residual" {class: residual_class}
decoder_stack.layer_1.cross_attn -> decoder_stack.layer_1.add_norm_2: "+residual" {class: residual_class}
decoder_stack.layer_1.ffn -> decoder_stack.layer_1.add_norm_3: "+residual" {class: residual_class}

# Encoder to cross-attention
encoder_output -> decoder_stack.layer_1.cross_attn: "K, V" {
  style: {
    stroke: "#FF9800"
    stroke-width: 3
    animated: true
  }
}

# Flow through stack
decoder_stack -> final_layer_norm {class: data_flow_class}
final_layer_norm -> linear_proj {class: data_flow_class}
linear_proj -> softmax_output {class: data_flow_class}
softmax_output -> output_tokens {class: data_flow_class}

# Inference feedback loop (dashed)
output_tokens -> target_input: "next step\n(inference)" {
  style: {
    stroke: "#9E9E9E"
    stroke-dash: 5
    animated: true
  }
}

# Legend
legend: {
  near: bottom-right
  title: "Legend"
  
  data_flow: "Data Flow" {
    class: data_flow_class
  }
  residual: "Residual\nConnection" {
    class: residual_class
  }
  
  self_attn_legend: "Masked\nSelf-Attention" {
    class: masked_attn_class
  }
  cross_attn_legend: "Cross-Attention" {
    class: cross_attn_class
  }
  ffn_legend: "Feed-Forward" {
    class: ffn_class
  }
}

# Dimension annotations
dims: |md
  ## Tensor Dimensions
  
  **Throughout decoder:**
  - Batch size: `B`
  - Target length: `T`
  - Model dimension: `d_model`
  - Vocabulary size: `V`
  
  **Key shapes:**
  - Input/Output per layer: `[B, T, d_model]`
  - Attention weights: `[B, T, T]` (self) or `[B, T, S]` (cross)
  - Final logits: `[B, T, V]`
| {near: bottom-left}