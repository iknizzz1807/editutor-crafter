vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Pre-LN vs Post-LN Comparison
  ## Layer Normalization Placement in Transformer Blocks
| {near: top-center}

direction: right

classes: {
  input_block: {
    shape: rectangle
    style: {
      fill: "#E8E8E8"
      stroke: "#666666"
      stroke-width: 2
    }
  }
  attention_block: {
    shape: rectangle
    style: {
      fill: "#B8D4E8"
      stroke: "#2E86AB"
      stroke-width: 2
    }
  }
  ffn_block: {
    shape: rectangle
    style: {
      fill: "#D4E8B8"
      stroke: "#6B8E23"
      stroke-width: 2
    }
  }
  norm_block: {
    shape: rectangle
    style: {
      fill: "#E8D4B8"
      stroke: "#CD853F"
      stroke-width: 2
    }
  }
  residual_arrow: {
    style: {
      stroke: "#666666"
      stroke-dash: 4
      stroke-width: 2
    }
  }
  gradient_arrow: {
    style: {
      stroke: "#FF4444"
      stroke-width: 3
      animated: true
    }
  }
}

PostLN: {
  label: "Post-LN (Original Transformer)"
  
  x_post: Input/Output {
    class: input_block
    width: 140
  }
  
  post_attn: Multi-Head\nAttention {
    class: attention_block
    width: 140
  }
  
  post_add1: Add {
    shape: circle
    width: 50
    style.fill: "#F0F0F0"
  }
  
  post_norm1: Layer\nNorm {
    class: norm_block
    width: 140
  }
  
  post_ffn: Feed\nForward {
    class: ffn_block
    width: 140
  }
  
  post_add2: Add {
    shape: circle
    width: 50
    style.fill: "#F0F0F0"
  }
  
  post_norm2: Layer\nNorm {
    class: norm_block
    width: 140
  }
  
  post_out: Output {
    class: input_block
    width: 140
  }
  
  post_annotation: |md
    **Post-LN Gradient Flow:**
    - Must pass through BOTH LayerNorms
    - Gradients diminish through norms
    - Harder to train deep networks
    - Requires warmup + LR scheduling
  |
  post_annotation.near: bottom-center
  
  x_post -> post_attn: Q, K, V
  post_attn -> post_add1
  x_post -> post_add1: residual {
    class: residual_arrow
  }
  post_add1 -> post_norm1
  post_norm1 -> post_ffn
  post_ffn -> post_add2
  post_norm1 -> post_add2: residual {
    class: residual_arrow
  }
  post_add2 -> post_norm2
  post_norm2 -> post_out
}

PreLN: {
  label: "Pre-LN (Modern Variant)"
  
  x_pre: Input/Output {
    class: input_block
    width: 140
  }
  
  pre_norm1: Layer\nNorm {
    class: norm_block
    width: 140
  }
  
  pre_attn: Multi-Head\nAttention {
    class: attention_block
    width: 140
  }
  
  pre_add1: Add {
    shape: circle
    width: 50
    style.fill: "#F0F0F0"
  }
  
  pre_norm2: Layer\nNorm {
    class: norm_block
    width: 140
  }
  
  pre_ffn: Feed\nForward {
    class: ffn_block
    width: 140
  }
  
  pre_add2: Add {
    shape: circle
    width: 50
    style.fill: "#F0F0F0"
  }
  
  pre_out: Output {
    class: input_block
    width: 140
  }
  
  pre_annotation: |md
    **Pre-LN Gradient Flow:**
    - Direct path through residuals
    - LayerNorms in side branches
    - Stable gradients regardless of depth
    - No warmup required
  |
  pre_annotation.near: bottom-center
  
  x_pre -> pre_norm1
  pre_norm1 -> pre_attn: Q, K, V
  pre_attn -> pre_add1
  x_pre -> pre_add1: residual {
    class: residual_arrow
  }
  pre_add1 -> pre_norm2
  pre_norm2 -> pre_ffn
  pre_ffn -> pre_add2
  pre_add1 -> pre_add2: residual {
    class: residual_arrow
  }
  pre_add2 -> pre_out
}

PostLN <-> PreLN: |md
  **Key Difference:**
  LayerNorm position
  relative to residual
  addition
|

comparison_table: ||md
  | Aspect | Post-LN | Pre-LN |
  |--------|---------|--------|
  | **Norm Position** | After residual | Before sublayer |
  | **Gradient Path** | Through norm | Direct residual |
  | **Training Stability** | Unstable | Stable |
  | **Warmup Required** | Yes | No |
  | **Deep Networks** | Struggles | Handles well |
  | **Used In** | Original paper | Modern LLMs |
||
comparison_table.near: bottom-center
comparison_table.style.fill: "#FAFAFA"
comparison_table.style.stroke: "#DDDDDD"