vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

shape: sequence_diagram

TrainingLoop: Training Loop
Loss: Loss Computation
Backward: loss.backward()
GradCheck: Gradient Verification
Q_Proj: W_Q.weight
K_Proj: W_K.weight
V_Proj: W_V.weight
Out_Proj: W_O.weight
FFN: FFN Layers
DeadCheck: Dead Layer Detection

TrainingLoop.t -> Loss.t: Forward pass complete
Loss.t -> Backward.t: loss_value

Backward.t -> Q_Proj.grad: "∂L/∂W_Q"
Backward.t -> K_Proj.grad: "∂L/∂W_K"
Backward.t -> V_Proj.grad: "∂L/∂W_V"
Backward.t -> Out_Proj.grad: "∂L/∂W_O"
Backward.t -> FFN.grad: "∂L/∂FFN"

GradCheck.t -> Q_Proj.check: Check .grad exists
Q_Proj.check -> GradCheck.t: "grad shape: [d_model, d_model]"

GradCheck.t -> K_Proj.check: Check .grad exists
K_Proj.check -> GradCheck.t: "grad shape: [d_model, d_model]"

GradCheck.t -> V_Proj.check: Check .grad exists
V_Proj.check -> GradCheck.t: "grad shape: [d_model, d_model]"

GradCheck.t -> Out_Proj.check: Check .grad exists
Out_Proj.check -> GradCheck.t: "grad shape: [d_model, d_model]"

GradCheck.t -> FFN.check: Check .grad exists
FFN.check -> GradCheck.t: grad shape matches

GradCheck.t -> DeadCheck.t: All grads present

DeadCheck.t -> DeadCheck.t: Check for zero gradients
DeadCheck.t -> DeadCheck.t: Check for NaN values
DeadCheck.t -> DeadCheck.t: "Check for exploding grads (>1e6)"

DeadCheck.t -> TrainingLoop.t: |md
  **Gradient Health Report:**
  - All parameters have gradients ✓
  - No dead layers detected ✓
  - No NaN values ✓
  - Max grad norm: 2.34 ✓
|

DeadCheck."Potential Issues"

TrainingLoop.dead_scenario: Dead Layer Scenario {
  TrainingLoop.t2 -> Loss.t2: Forward with broken skip connection
  Loss.t2 -> Backward.t2: loss_value
  Backward.t2 -> FFN.grad2: "∂L/∂FFN = 0.0"
  
  GradCheck.t2 -> FFN.check2: Verify gradient
  FFN.check2 -> GradCheck.t2: |md
    **WARNING: grad ≈ 0**
    Layer is dead!
|
  
  GradCheck.t2 -> DeadCheck.t2: Dead layer found: FFN
  DeadCheck.t2.style.fill: "#FFB6C1"
  DeadCheck.t2 -> TrainingLoop.t2: |md
  **DIAGNOSIS:**
  - Broken residual connection
  - Learning rate too low
  - Vanishing gradient through depth
|
}