vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Transformer Forward Pass
  Complete data flow: Source → Encoder → Decoder ← Target → Logits
| {near: top-center}

direction: right

source_tokens: "Source Tokens\n[source_seq_len]" {
  style.fill: "#E8D5E8"
  style.stroke: "#8B5A8B"
}

source_embed: "Source Embedding\n+ Positional Encoding\n[batch, src_len, d_model]" {
  style.fill: "#D4B8D4"
  style.stroke: "#8B5A8B"
}

encoder_stack: "Encoder Stack\n(N × Encoder Layers)" {
  style.fill: "#C9A8C9"
  style.stroke: "#7B4A7B"
  
  enc_self_attn: "Self-Attention\n(masked by padding)" {
    style.fill: "#B89AB8"
  }
  enc_ffn: "Feed-Forward Network" {
    style.fill: "#B89AB8"
  }
  enc_residual: "Residual + LayerNorm" {
    style.fill: "#A88AA8"
  }
  
  enc_self_attn -> enc_residual
  enc_ffn -> enc_residual
}

encoder_output: "Encoder Output\n[batch, src_len, d_model]" {
  style.fill: "#C9A8C9"
  style.stroke: "#7B4A7B"
}

target_tokens: "Target Tokens\n[target_seq_len]" {
  style.fill: "#D5E8D5"
  style.stroke: "#5A8B5A"
}

target_embed: "Target Embedding\n+ Positional Encoding\n[batch, tgt_len, d_model]" {
  style.fill: "#B8D4B8"
  style.stroke: "#5A8B5A"
}

decoder_stack: "Decoder Stack\n(N × Decoder Layers)" {
  style.fill: "#A8C9A8"
  style.stroke: "#4A7B4A"
  
  dec_self_attn: "Masked Self-Attention\n(causal + padding mask)" {
    style.fill: "#9AB89A"
  }
  dec_cross_attn: "Cross-Attention\n(Q from decoder,\nK,V from encoder)" {
    style.fill: "#9AB89A"
  }
  dec_ffn: "Feed-Forward Network" {
    style.fill: "#9AB89A"
  }
  dec_residual: "Residual + LayerNorm" {
    style.fill: "#8AA88A"
  }
  
  dec_self_attn -> dec_cross_attn
  dec_cross_attn -> dec_ffn
  dec_ffn -> dec_residual
}

decoder_output: "Decoder Output\n[batch, tgt_len, d_model]" {
  style.fill: "#A8C9A8"
  style.stroke: "#4A7B4A"
}

output_projection: "Output Projection\nLinear(d_model → vocab_size)" {
  style.fill: "#D5D5E8"
  style.stroke: "#5A5A8B"
}

logits: "Logits\n[batch, tgt_len, vocab_size]" {
  style.fill: "#E8E8F0"
  style.stroke: "#4A4A7B"
}

softmax_output: "Softmax\n→ Probabilities" {
  style.fill: "#E8E8F0"
  style.stroke: "#4A4A7B"
  shape: diamond
}

loss: "Cross-Entropy Loss\n(training)" {
  style.fill: "#E8D5D5"
  style.stroke: "#8B5A5A"
  shape: oval
}

# Main forward flow
source_tokens -> source_embed: "embed + pos_encode"
source_embed -> encoder_stack: "[batch, src_len, d_model]"
encoder_stack -> encoder_output: "contextualized\nrepresentations"

target_tokens -> target_embed: "embed + pos_encode\n(shifted right)"
target_embed -> decoder_stack: "[batch, tgt_len, d_model]"

# Cross-attention connection (encoder → decoder)
encoder_output -> decoder_stack.dec_cross_attn: "K, V tensors\nfor cross-attention" {
  style.stroke: "#8B5A8B"
  style.stroke-dash: 5
}

decoder_stack -> decoder_output: "[batch, tgt_len, d_model]"
decoder_output -> output_projection: "linear projection"
output_projection -> logits: "unnormalized scores"
logits -> softmax_output: "normalization"
softmax_output -> loss: "vs target labels"

# Legend
legend: {
  near: bottom-center
  style.fill: transparent
  
  info: |md
    **Data Flow Conventions:**
    - **Purple**: Source/Encoder pathway
    - **Green**: Target/Decoder pathway  
    - **Dashed**: Cross-attention (encoder-to-decoder)
    - **Blue**: Output/Logits pathway
  |
}