vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Masked Cross-Entropy Loss Computation
  ## Step-by-Step Algorithm
| {near: top-center}

step_1: {
  label: "Step 1: Compute Raw Logits"
  
  input_tensor: {
    label: "Model Output\nlogits"
    shape: rectangle
    style.fill: "#E8D5F2"
    width: 180
  }
  
  input_annotation: |md
    Shape: [batch, seq_len, vocab_size]
    dtype: float32
  |
  
  step1_note: |md
    Raw predictions from final linear layer.
    No softmax applied yet.
  | {near: bottom-center}
}

step_2: {
  label: "Step 2: Flatten for CrossEntropy"
  
  before_flat: {
    label: "Before"
    shape: rectangle
    style.fill: "#D4E8F2"
    width: 200
  }
  
  shape_3d_note: |md
    [batch, seq, vocab]
    [2, 5, 10000]
  |
  
  arrow_flat: {
    label: "reshape"
    shape: rectangle
    width: 80
  }
  
  after_flat: {
    label: "After"
    shape: rectangle
    style.fill: "#D4E8F2"
    width: 200
  }
  
  shape_2d_note: |md
    [batch*seq, vocab]
    [10, 10000]
  |
  
  before_flat -> arrow_flat -> after_flat
  
  step2_note: |md
    PyTorch CrossEntropyLoss expects:
    - logits: [N, C] 
    - targets: [N]
  | {near: bottom-center}
}

step_3: {
  label: "Step 3: Create Padding Mask"
  
  target_tensor: {
    label: "Target Tokens\n[batch, seq_len]"
    shape: rectangle
    style.fill: "#F2E8D5"
    width: 160
  }
  
  targets_note: |md
    [[32, 451, 128, PAD, PAD],
     [32, 451, 789, 234, PAD]]
  |
  
  mask_op: {
    label: "targets == PAD_ID"
    shape: diamond
    style.fill: "#F2D5D5"
  }
  
  mask_result: {
    label: "Padding Mask\n[batch, seq_len]"
    shape: rectangle
    style.fill: "#D5F2E8"
    width: 160
  }
  
  mask_vals_note: |md
    [[0, 0, 0, 1, 1],
     [0, 0, 0, 0, 1]]
  |
  
  target_tensor -> mask_op -> mask_result
  
  legend: |md
    0 = compute loss (real token)
    1 = ignore (padding token)
  | {near: bottom-right}
}

step_4: {
  label: "Step 4: Map to ignore_index"
  
  before_ignore: {
    label: "Before"
    shape: rectangle
    style.fill: "#F2E8D5"
    width: 160
  }
  
  before_vals_note: |md
    [[32, 451, 128, PAD, PAD],
     [32, 451, 789, 234, PAD]]
  |
  
  ignore_op: {
    label: "mask * (-100)"
    shape: diamond
    style.fill: "#F2D5D5"
  }
  
  after_ignore: {
    label: "After\n(ignore_index=-100)"
    shape: rectangle
    style.fill: "#E8F2D5"
    width: 160
  }
  
  after_vals_note: |md
    [[32, 451, 128, -100, -100],
     [32, 451, 789, 234, -100]]
  |
  
  before_ignore -> ignore_op -> after_ignore
  
  step4_note: |md
    CrossEntropyLoss ignores positions
    where target == ignore_index
  | {near: bottom-center}
}

step_5: {
  label: "Step 5: Compute Per-Token Loss"
  
  logits_flat: {
    label: "Logits\n[N, vocab_size]"
    shape: rectangle
    style.fill: "#E8D5F2"
    width: 140
  }
  
  targets_flat: {
    label: "Targets\n[N]"
    shape: rectangle
    style.fill: "#E8F2D5"
    width: 140
  }
  
  loss_fn: {
    label: "CrossEntropyLoss\nreduction='none'"
    shape: rectangle
    style.fill: "#F2D5E8"
    width: 160
  }
  
  per_token: {
    label: "Per-Token Loss\n[N]"
    shape: rectangle
    style.fill: "#D5E8F2"
    width: 140
  }
  
  losses_note: |md
    [2.1, 0.3, 5.2, 0.0, 0.0,
     2.1, 0.3, 1.1, 3.4, 0.0]
  |
  
  logits_flat -> loss_fn
  targets_flat -> loss_fn
  loss_fn -> per_token
  
  step5_note: |md
    CrossEntropy computes:
    -log(softmax(logits)[target])
    Returns 0.0 for ignore_index positions
  | {near: bottom-center}
}

step_6: {
  label: "Step 6: Reduction Methods"
  
  per_token_input: {
    label: "Per-Token Losses\n[N]"
    shape: rectangle
    style.fill: "#D5E8F2"
    width: 120
  }
  
  reduction_gate: {
    label: "Reduction"
    shape: diamond
    style.fill: "#F2E8D5"
  }
  
  mean_branch: {
    label: "reduction='mean'"
    shape: rectangle
    style.fill: "#D5F2D5"
    width: 140
  }
  
  mean_formula: |md
    sum(losses) / num_valid
    (excludes padding)
  |
  
  sum_branch: {
    label: "reduction='sum'"
    shape: rectangle
    style.fill: "#D5F2D5"
    width: 140
  }
  
  sum_formula: |md
    sum(losses)
    (excludes padding)
  |
  
  none_branch: {
    label: "reduction='none'"
    shape: rectangle
    style.fill: "#D5F2D5"
    width: 140
  }
  
  none_formula: |md
    returns [N] tensor
    (for custom handling)
  |
  
  per_token_input -> reduction_gate
  reduction_gate -> mean_branch: "default"
  reduction_gate -> sum_branch
  reduction_gate -> none_branch
}

step_7: {
  label: "Step 7: Final Scalar Loss"
  
  final_loss: {
    label: "Scalar Loss"
    shape: circle
    style.fill: "#F2D5D5"
    width: 100
  }
  
  loss_val: |md
    1.73
  |
  
  backprop_arrow: {
    label: "backward()"
    shape: rectangle
    style.fill: "#E8E8E8"
    width: 100
  }
  
  final_loss -> backprop_arrow
  
  step7_note: |md
    Single scalar for optimizer step.
    Gradients flow only through
    non-padded positions.
  | {near: bottom-center}
}

flow: {
  step_1 -> step_2: "flatten"
  step_2 -> step_3: "parallel"
  step_3 -> step_4: "apply mask"
  step_4 -> step_5: "compute"
  step_5 -> step_6: "reduce"
  step_6 -> step_7: "output"
}

flow_arrow: (step_1 -> step_2)[0] {
  style.stroke: "#666"
  style.stroke-width: 2
}

impl_box: {
  label: "Implementation"
  width: 600
  
  code: ||py
    def masked_cross_entropy(logits, targets, pad_id, reduction='mean'):
        """
        logits: [batch, seq_len, vocab_size]
        targets: [batch, seq_len]
        """
        batch, seq_len, vocab = logits.shape
        
        # Flatten for CrossEntropyLoss
        logits_flat = logits.view(-1, vocab)
        targets_flat = targets.view(-1)
        
        # Create mask and set padding to ignore_index
        padding_mask = (targets == pad_id)
        targets_masked = targets.masked_fill(padding_mask, -100)
        targets_flat = targets_masked.view(-1)
        
        # Compute loss (ignore_index=-100)
        loss_fn = nn.CrossEntropyLoss(
            ignore_index=-100,
            reduction=reduction
        )
        loss = loss_fn(logits_flat, targets_flat)
        
        return loss
  ||
}

impl_box.style.fill: "#F5F5F5"
impl_box.style.stroke: "#999"
impl_box.style.stroke-dash: 3

dimension_legend: {
  label: "Dimension Reference"
}

shapes_table: ||md
  | Variable | Shape | Notes |
  |----------|-------|-------|
  | logits | [B, S, V] | Raw model output |
  | targets | [B, S] | Token IDs |
  | logits_flat | [B*S, V] | Reshaped for loss |
  | targets_flat | [B*S] | Reshaped targets |
  | per_token_loss | [B*S] | Before reduction |
  | loss | scalar | After reduction |
  
  B = batch_size, S = seq_len, V = vocab_size
||

dimension_legend.style.fill: "#FFF9E6"
dimension_legend.style.stroke: "#E6C200"