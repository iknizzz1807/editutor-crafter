vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

shape: sequence_diagram

TrainingLoop: {
  style.stroke: "#6366f1"
  style.stroke-dash: 3
}

Model
Loss
GradClip: Gradient Clipper
Optimizer
Scheduler
Logger

# Epoch Loop marker
epoch: {
  Model -> Model: set_train_mode()
}

# Forward Pass
Model.t1 -> Model.t1: forward(batch)
Model.t1 -> Loss.t1: logits, targets
Loss.t1 -> Loss.t1: compute_loss()

# Loss Computation Result
Loss.t1 -> Logger: log(loss_value) {
  style.stroke: blue
  style.font-color: blue
}

# Backward Pass
Loss.t1 -> Model.t2: loss.backward() {
  style.stroke: "#ef4444"
  style.animated: true
  style.font-color: "#ef4444"
}

# Gradient Clipping
Model.t2 -> GradClip: get_grads()
GradClip -> GradClip: clip_norm_(max_norm=1.0)
GradClip -> Model.t3: clipped_grads {
  style.stroke: "#f59e0b"
  style.font-color: "#f59e0b"
}

# Optimizer Step
Model.t3 -> Optimizer: step() {
  style.stroke: "#22c55e"
  style.font-color: "#22c55e"
}

Optimizer -> Model.t4: updated_params

# Zero Gradients
Model.t4 -> Optimizer: zero_grad()

# Scheduler Step
Optimizer -> Scheduler: step_count
Scheduler -> Scheduler: update_lr()
Scheduler -> Logger: log(learning_rate) {
  style.stroke: "#8b5cf6"
  style.font-color: "#8b5cf6"
}

# Step Complete
Logger -> Logger: step_complete

# Annotations
Model."|'md **Phase 1**: Forward pass\nComputes logits through\nall transformer layers |'"
Loss."|'md **Phase 2**: Loss computation\nCrossEntropyLoss for\nlanguage modeling |'"
GradClip."|'md **Phase 3**: Gradient clipping\nPrevents exploding gradients\nmax_norm=1.0 typical |'"
Optimizer."|'md **Phase 4**: Weight update\nAdamW: m_t, v_t state\n+ weight decay |'"
Scheduler."|'md **Phase 5**: LR schedule\nWarmup + cosine decay\nor linear decay |'"