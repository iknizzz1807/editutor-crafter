vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Label Smoothing Formula
  ## Regularization for Transformer Training
| {near: top-center}

direction: right

input_label: Input Label {
  shape: rectangle
  style.fill: "#E8D5F2"
  style.stroke: "#8B5CF6"
  style.stroke-width: 2
  
  y_true: {
    shape: rectangle
    label: "y_true = [0, 0, 1, 0, 0]\n(True class: index 2)"
    style.fill: "#C4B5FD"
  }
}

smoothed_label: Smoothed Target Distribution {
  shape: rectangle
  style.fill: "#D1FAE5"
  style.stroke: "#059669"
  style.stroke-width: 2
  
  formula_box: {
    shape: rectangle
    style.fill: "#A7F3D0"
    
    true_class: |md
      **True class (i = y):**
      `(1 - ε) = 0.9`
    |
    
    other_classes: |md
      **Other classes (i ≠ y):**
      `ε / (K-1) = 0.1/4 = 0.025`
    |
  }
  
  result: {
    shape: rectangle
    label: "y_smooth = [0.025, 0.025, 0.9, 0.025, 0.025]"
    style.fill: "#6EE7B7"
    style.bold: true
  }
}

params: Parameters {
  shape: rectangle
  style.fill: "#FEF3C7"
  style.stroke: "#D97706"
  style.stroke-width: 2
  
  epsilon: {
    shape: rectangle
    label: "ε = 0.1\n(smoothing factor)"
    style.fill: "#FDE68A"
  }
  
  num_classes: {
    shape: rectangle
    label: "K = 5\n(number of classes)"
    style.fill: "#FDE68A"
  }
}

code_impl: Implementation {
  shape: rectangle
  style.fill: "#DBEAFE"
  style.stroke: "#2563EB"
  style.stroke-width: 2
  
  pytorch: |py
    import torch.nn.functional as F
    
    def label_smoothing_loss(
        logits,           # [batch, K]
        targets,          # [batch] (class indices)
        epsilon=0.1       # smoothing factor
    ):
        K = logits.size(-1)
        
        # Create smoothed targets
        smooth_targets = torch.zeros_like(logits)
        smooth_targets.fill_(epsilon / (K - 1))
        smooth_targets.scatter_(
            1, 
            targets.unsqueeze(1), 
            1.0 - epsilon
        )
        
        # Compute KL divergence loss
        log_probs = F.log_softmax(logits, dim=-1)
        loss = -(smooth_targets * log_probs).sum(dim=-1)
        return loss.mean()
    
    # Or use PyTorch built-in:
    criterion = nn.CrossEntropyLoss(
        label_smoothing=0.1
    )
  |
}

input_label -> smoothed_label: Apply smoothing\nformula
params -> smoothed_label: ε, K values
smoothed_label -> code_impl: Use in\nloss computation

annotation: |md
  **Why Label Smoothing?**
  
  1. **Prevents overconfidence**: Model can't assign 100% probability
  2. **Improves calibration**: Predictions better reflect true uncertainty
  3. **Acts as regularizer**: Reduces overfitting to noisy labels
  4. **Common values**: ε ∈ [0.05, 0.15] (typically 0.1)
  
  **Effect on gradients**: Distributes error signal across all classes,
  preventing gradient explosion from extreme confidence.
| {near: bottom-center}