vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # GPU Memory Budget: Training a Transformer
  sizeof=16GB (typical consumer GPU)
| {near: top-center}

direction: right

GPU_Memory: "GPU Memory (16GB)" {
  style.stroke: "#333"
  style.stroke-width: 3
  style.fill: "#f8f9fa"
  
  Parameters: "Model Parameters" {
    style.fill: "#9b59b6"
    style.font-color: white
    
    embedding: "Embedding Weights\nvocab × d_model"
    attention: "Attention Weights\n4 × d_model² per layer"
    ffns: "FFN Weights\n2 × d_model × d_ff per layer"
    layer_norm: "LayerNorm\n2 × d_model per norm"
  }
  
  Gradients: "Gradients (∂L/∂θ)" {
    style.fill: "#e67e22"
    style.font-color: white
    
    grad_embed: "∂Embedding\nsame shape as params"
    grad_attn: "∂Attention\nsame shape as params"
    grad_ffn: "∂FFN\nsame shape as params"
  }
  
  Activations: "Forward Activations" {
    style.fill: "#3498db"
    style.font-color: white
    
    input_acts: "Input Activations\nbatch × seq × d_model"
    attn_acts: "Attention Activations\nQ, K, V, scores, output"
    ffn_acts: "FFN Activations\nintermediate + output"
  }
  
  Optimizer: "Optimizer States" {
    style.fill: "#27ae60"
    style.font-color: white
    
    momentum: "First Moment (m)\nAdam: same shape as params"
    velocity: "Second Moment (v)\nAdam: same shape as params"
  }
}

Memory_Breakdown: ||md
  | Component | Size Formula | 6-layer 512d |
  |-----------|--------------|--------------|
  | Parameters | 4Ld² + 8Ldd_ff + 2Vd | ~25M params = 100MB |
  | Gradients | Same as params | 100MB |
  | Activations | ~20Lbd² | 1.2GB (b=32, s=512) |
  | Optimizer (Adam) | 2× params | 200MB |
  | **Peak Training** | | **~1.6GB** |
||
Memory_Breakdown.near: bottom-left

Annotation: |md
  **Key Insight**: Activations dominate memory during training.
  
  - **Inference**: ~2× params (weights + single pass activations)
  - **Training**: ~4× params (weights + gradients + 2× optimizer states + activations)
  
  **Memory Optimization Strategies**:
  1. **Gradient checkpointing**: Trade compute for memory (recompute activations)
  2. **Mixed precision (FP16)**: Halve activation/gradient memory
  3. **ZeRO optimizer**: Shard optimizer states across GPUs
  4. **Offloading**: Move optimizer states to CPU
| {near: bottom-center}

Parameters -> Gradients: backward {
  style.stroke-dash: 3
  style.stroke: "#333"
}
Gradients -> Optimizer: update {
  style.stroke-dash: 3
  style.stroke: "#333"
}
Activations -> Gradients: ∂L/∂θ {
  style.stroke: "#e67e22"
  style.stroke-dash: 5
}

size_annotation: |md
  `sizeof(params) = N × 4 bytes (FP32)`
  `sizeof(gradients) = N × 4 bytes`
  `sizeof(optimizer) = N × 8 bytes (Adam: m + v)`
  `sizeof(activations) = O(L × batch × seq × d)`
| {
  shape: text
  style.font: mono
  style.font-size: 14
  near: bottom-right
}