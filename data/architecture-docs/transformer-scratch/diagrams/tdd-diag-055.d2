vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Greedy Decoding Algorithm
  **Autoregressive Token Generation**
| {near: top-center}

direction: down

classes: {
  state: {
    style: {
      fill: "#E8F4FD"
      stroke: "#2563EB"
      stroke-width: 2
      border-radius: 8
    }
  }
  changed: {
    style: {
      fill: "#FEE2E2"
      stroke: "#DC2626"
      stroke-width: 3
      bold: true
    }
  }
  step_label: {
    shape: circle
    width: 40
    style: {
      fill: "#1E40AF"
      font-color: white
      bold: true
    }
  }
  note: {
    shape: text
    style: {
      font-size: 14
      italic: true
      font-color: "#6B7280"
    }
  }
}

Step1: {
  step_num: {
    class: step_label
    label: "1"
  }
  
  Encode: {
    class: state
    label: Encoder\nForward Pass
    
    src_emb: "Source Embeddings\n[batch, src_len, d_model]"
    enc_out: "Encoder Output\n[batch, src_len, d_model]"
    
    src_emb -> enc_out: encode()
  }
  
  note1: |md
    Encode entire source sequence once.
    Cache encoder outputs for cross-attention.
  | {class: note}
}

Step2: {
  step_num: {
    class: step_label
    label: "2"
  }
  
  Init: {
    class: state
    label: Initialize Decoder
    
    bos: "<BOS>\n[batch, 1]"
    dec_out: "Decoder Output\n[batch, 1, d_model]"
    
    bos -> dec_out: decode(enc_out, bos)
  }
  
  note2: |md
    Start generation with beginning-of-sequence token.
    Decoder attends to encoder outputs via cross-attention.
  | {class: note}
}

Step3: {
  step_num: {
    class: step_label
    label: "3"
  }
  
  Predict: {
    class: state
    label: "Predict Next Token"
    
    logits: "Logits\n[batch, vocab_size]"
    probs: "Probabilities\n[batch, vocab_size]"
    
    dec_out: "Decoder Output\n[batch, t, d_model]"
    lm_head: "LM Head\nLinear(d_model → vocab)"
    
    dec_out -> lm_head: project
    lm_head -> logits: softmax
    logits -> probs
  }
  
  note3: |md
    Project decoder output to vocabulary logits.
    Apply softmax to get probability distribution.
  | {class: note}
}

Step4: {
  step_num: {
    class: step_label
    label: "4"
  }
  
  Select: {
    class: state
    label: Greedy Selection
    
    probs2: "Probabilities\n[batch, vocab_size]"
    next_token: "next_token = argmax(probs)\n[batch, 1]"
    
    probs2 -> next_token: argmax
  }
  
  note4: |md
    Select token with highest probability.
    No sampling, no beam search—pure greedy.
  | {class: note}
}

Step5: {
  step_num: {
    class: step_label
    label: "5"
  }
  
  Append: {
    class: state
    label: Append to Sequence
    
    prev_seq: "Generated: [<BOS>, t₁, ..., tₙ]\n[batch, n+1]"
    appended: "Generated: [<BOS>, t₁, ..., tₙ, tₙ₊₁]\n[batch, n+2]"
    
    prev_seq -> appended: concat
  }
  
  note5: |md
    Append selected token to generated sequence.
    Feed extended sequence back to decoder.
  | {class: note}
}

Step6: {
  step_num: {
    class: step_label
    label: "6"
  }
  
  Check: {
    class: state
    label: Termination Check
    
    condition: "next_token == <EOS> ?"
    eos_check: "OR max_length reached ?"
    done: "DONE\nReturn sequence"
    loop: "Loop to Step 3"
    
    condition -> done: Yes
    condition -> loop: No
    eos_check -> done: Yes
  }
  
  note6: |md
    Continue until end-of-sequence token or max length.
    Return final generated sequence.
  | {class: note}
}

Step1.Encode -> Step2.Init: encoded_output
Step2.Init -> Step3.Predict: decoder_output
Step3.Predict -> Step4.Select: probabilities
Step4.Select -> Step5.Append: next_token
Step5.Append -> Step6.Check: extended_sequence

Step6.Check -> Step3.Predict: continue\ngeneration {
  style: {
    stroke: "#059669"
    stroke-dash: 5
    animated: true
  }
}

Summary: |md
  
  def greedy_decode(encoder, decoder, src, max_len=512):
      # Step 1: Encode source
      enc_out = encoder(src)                    # [batch, src_len, d_model]
      
      # Step 2: Initialize with <BOS>
      generated = torch.full((batch, 1), BOS_ID) # [batch, 1]
      
      # Steps 3-6: Autoregressive loop
      for _ in range(max_len):
          # Step 3: Predict
          dec_out = decoder(generated, enc_out)  # [batch, t, d_model]
          logits = lm_head(dec_out[:, -1, :])    # [batch, vocab_size]
          probs = F.softmax(logits, dim=-1)
          
          # Step 4: Greedy select
          next_token = torch.argmax(probs, dim=-1, keepdim=True)
          
          # Step 5: Append
          generated = torch.cat([generated, next_token], dim=1)
          
          # Step 6: Check termination
          if next_token.item() == EOS_ID:
              break
      
      return generated
  
|