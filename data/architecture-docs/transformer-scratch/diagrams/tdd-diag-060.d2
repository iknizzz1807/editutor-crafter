vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # KV Cache Architecture
  **Shape: [batch, num_heads, seq_len, d_k] per layer**
| {near: top-center}

direction: right

classes: {
  cache_block: {
    style: {
      fill: "#E8F4FD"
      stroke: "#2E86AB"
      stroke-width: 2
      border-radius: 4
    }
  }
  layer_label: {
    style: {
      fill: "#F0E6FF"
      stroke: "#7B4B94"
      font-color: "#4A2C6A"
      bold: true
    }
  }
  dimension: {
    style: {
      font: mono
      font-size: 18
      font-color: "#555"
    }
  }
  pointer: {
    shape: circle
    width: 40
    style: {
      fill: "#FF6B6B"
      stroke: "#C92A2A"
      font-color: white
      bold: true
    }
  }
}

KV_Cache: {
  style.fill: "#FAFAFA"
  style.stroke: "#333"
  style.stroke-width: 3
  
  Header: |md
    **Per-Layer Cache Structure**
    ─────────────────────
  |
  
  Layer_0: {
    class: layer_label
    K_cache_0: {
      class: cache_block
      label: "K₀ Cache"
      
      dims_k0: |md
        **[batch, heads, seq_len, d_k]**
        ────────────────────────
        • batch: 1-32 (typical)
        • heads: 8-16
        • seq_len: grows +1/gen
        • d_k: 64 (d_model/heads)
      | {class: dimension}
    }
    
    V_cache_0: {
      class: cache_block
      label: "V₀ Cache"
      
      dims_v0: |md
        **[batch, heads, seq_len, d_v]**
        ────────────────────────
        • Same shape as K
        • d_v = d_k typically
        • Pre-computed projections
      | {class: dimension}
    }
    
    K_cache_0 -> V_cache_0: paired {style.stroke-dash: 3}
  }
  
  Layer_1: {
    class: layer_label
    K_cache_1: {
      class: cache_block
      label: "K₁ Cache"
    }
    V_cache_1: {
      class: cache_block
      label: "V₁ Cache"
    }
    K_cache_1 -> V_cache_1: paired {style.stroke-dash: 3}
  }
  
  Layer_N: {
    class: layer_label
    label: "Layer N-1"
    K_cache_N: {
      class: cache_block
      label: "K_{n-1} Cache"
      style.fill: "#FFE8E8"
    }
    V_cache_N: {
      class: cache_block
      label: "V_{n-1} Cache"
      style.fill: "#FFE8E8"
    }
    K_cache_N -> V_cache_N: paired {style.stroke-dash: 3}
  }
  
  ellipsis: {
    shape: text
    label: "⋮"
    style.font-size: 40
  }
  
  Layer_0 -> Layer_1
  Layer_1 -> ellipsis
  ellipsis -> Layer_N
}

Memory_Layout: {
  style.fill: "#FFF9E6"
  style.stroke: "#B8860B"
  
  title_mem: |md
    **Memory Growth Pattern**
  |
  
  T0: {
    shape: rectangle
    width: 60
    height: 40
    label: "T=0\nlen=1"
    style.fill: "#90EE90"
  }
  
  T1: {
    shape: rectangle
    width: 90
    height: 40
    label: "T=1\nlen=2"
    style.fill: "#FFD700"
  }
  
  T2: {
    shape: rectangle
    width: 120
    height: 40
    label: "T=2\nlen=3"
    style.fill: "#FFA500"
  }
  
  Tn: {
    shape: rectangle
    width: 180
    height: 40
    label: "T=n\nlen=n+1"
    style.fill: "#FF6B6B"
  }
  
  T0 -> T1: append {style.stroke: "#666"}
  T1 -> T2: append {style.stroke: "#666"}
  T2 -> Tn: "...append" {style.stroke-dash: 5}
}

Operations: {
  style.fill: "#F0FFF0"
  style.stroke: "#228B22"
  
  title_ops: |md
    **Cache Operations**
  |
  
  Prefill: {
    shape: rectangle
    style.fill: "#98FB98"
    label: |md
      **PREFILL** (prompt processing)
      ─────────────────────────
      
      # Process entire prompt at once
      K_cache = W_K(prompt)  # [B,H,L,d_k]
      V_cache = W_V(prompt)  # [B,H,L,d_v]
      
    |
  }
  
  Decode: {
    shape: rectangle
    style.fill: "#DDA0DD"
    label: |md
      **DECODE** (token generation)
      ─────────────────────────
      
      # Process single new token
      new_k = W_K(new_token)  # [B,H,1,d_k]
      new_v = W_V(new_token)  # [B,H,1,d_v]
      K_cache = cat([K_cache, new_k], dim=2)
      V_cache = cat([V_cache, new_v], dim=2)
      
    |
  }
  
  Prefill -> Decode: "streaming" {style.animated: true}
}

KV_Cache -> Memory_Layout: "grows with seq_len" {style.stroke: "#B8860B"; style.stroke-dash: 3}
KV_Cache -> Operations: "accessed during" {style.stroke: "#228B22"; style.stroke-dash: 3}

Annotation: |md
  ### Key Invariant
  
  At generation step `t`, cache contains projections for **all previous tokens**:
  
  
  K_cache.shape = [batch, num_heads, t, d_k]
  V_cache.shape = [batch, num_heads, t, d_v]
  
  
  **Memory cost**: `O(L × n_layers × 2 × batch × heads × seq_len × d_k)`
  
  For Llama-2-70B: ~2MB per token per batch element
| {near: bottom-center}