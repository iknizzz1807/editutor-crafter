vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

direction: right

title: |md
  # Sampling with Temperature
  ### Scaled Logits → Softmax → Multinomial Sampling → Next Token
| {near: top-center}

step1: Step 1: Scaled Logits {
  style.fill: "#E8E4F0"
  style.stroke: "#7B68A6"
  style.stroke-width: 2
  
  logits: |md
    **logits** `[vocab_size]`

    Raw output from final linear layer

    Example (vocab=5):

    [2.1, -0.5, 1.3, 0.8, -1.2]
  |
}

arrow1: {
  shape: text
  label: "÷ τ"
  style.font-size: 24
  style.bold: true
  style.font-color: "#7B68A6"
}

step2: Step 2: Temperature Scaling {
  style.fill: "#D4E5F7"
  style.stroke: "#4A90D9"
  style.stroke-width: 2
  
  formula: |md
    **scaled** = logits / τ

    τ = **temperature** hyperparameter

    | τ | Effect |
    |---|--------|
    | < 1 | Sharper distribution |
    | = 1 | Original distribution |
    | > 1 | Flatter distribution |
  |
  
  scaled_example: |md
    Example (τ = 0.8):

    [2.625, -0.625, 1.625, 1.0, -1.5]


    Peaks amplified → more confident sampling
  |
}

arrow2: {
  shape: text
  label: "softmax"
  style.font-size: 20
  style.bold: true
  style.font-color: "#4A90D9"
}

step3: Step 3: Softmax → Probabilities {
  style.fill: "#D5F5E3"
  style.stroke: "#27AE60"
  style.stroke-width: 2
  
  softmax_formula: |md
    **P(i)** = exp(scaled_i) / Σ exp(scaled_j)
  |
  
  probs: |md
    **probs** `[vocab_size]`

    Example (after τ=0.8 scaling):

    [0.52, 0.02, 0.23, 0.13, 0.10]


    • Sums to 1.0 ✓
    • Highest prob = most likely token
  |
}

arrow3: {
  shape: text
  label: "sample"
  style.font-size: 20
  style.bold: true
  style.font-color: "#27AE60"
}

step4: Step 4: Multinomial Sampling {
  style.fill: "#FCF3CF"
  style.stroke: "#F39C12"
  style.stroke-width: 2
  
  sampling: |md
    **torch.multinomial(probs, num_samples=1)**

    Draw one sample from categorical distribution

    Token 0: 52% chance ████████████
    Token 1:  2% chance █
    Token 2: 23% chance ██████
    Token 3: 13% chance ████
    Token 4: 10% chance ███
  |
  
  note: |md
    **Key insight**: Non-deterministic!

    Same input can produce different outputs.
    Enables diverse, creative generation.
  |
}

arrow4: {
  shape: text
  label: "idx"
  style.font-size: 20
  style.bold: true
  style.font-color: "#F39C12"
}

step5: Step 5: Next Token {
  style.fill: "#FADBD8"
  style.stroke: "#E74C3C"
  style.stroke-width: 2
  
  output: |md
    **next_token_id** = sampled index

    Example result: `0` (token "the")

    Append to sequence:

    [..., "quick", "brown"] + ["the"]
  |
  
  loop: |md
    **Loop back** to generate next token:

    New input = [prev_tokens + next_token]
    Repeat until `<EOS>` or max_length
  |
}

step1 -> arrow1 -> step2 -> arrow2 -> step3 -> arrow3 -> step4 -> arrow4 -> step5

legend: {
  near: bottom-center
  
  temp_comparison: {
    label: Temperature Comparison
    shape: rectangle
    style.fill: white
    style.stroke: "#666"
    
    low_temp: Low τ (0.5) {
      style.fill: "#FADBD8"
      dist: |md
        [0.85, 0.01, 0.08, 0.04, 0.02]

        Nearly deterministic
        Repeated, safe outputs
      |
    }
    
    high_temp: High τ (1.5) {
      style.fill: "#D5F5E3"
      dist: |md
        [0.35, 0.12, 0.24, 0.17, 0.12]

        More uniform
        Creative, surprising outputs
      |
    }
  }
}

gradient_box: |md
  ### Gradient Flow (Training Only)

  During training, gradients flow through:


  ∂L/∂logits = ∂L/∂probs × ∂softmax/∂logits


  Temperature affects gradient scale:
  - Low τ → larger gradients on peak tokens
  - High τ → smaller, distributed gradients

  **Note**: Temperature is typically NOT learned—set manually.
| {
  near: center-left
  style.fill: "#F8F9FA"
  style.stroke: "#DEE2E6"
  style.stroke-dash: 3
}