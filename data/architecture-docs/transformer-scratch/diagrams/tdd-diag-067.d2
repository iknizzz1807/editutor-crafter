vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 200
  }
}

title: |md
  # Causal Mask Update During Generation
  Step-by-step: Growing sequence length → mask expansion → correct slicing
| {near: top-center}

direction: right

step1: {
  label: "Step 1: Initial State (t=0)"
  
  seq_t0: {
    label: "Sequence Length = 1"
    style.fill: "#E8F4FD"
    style.stroke: "#2E86AB"
    
    token_t0: "<START>"
    token_t0.style.fill: "#ACE1AF"
    token_t0.style.bold: true
  }
  
  mask_t0: {
    label: "Causal Mask [1×1]"
    style.fill: "#FFF9C4"
    style.stroke: "#F9A825"
    
    m00: "0"
    m00.style.fill: "#90EE90"
    m00.style.bold: true
  }
  
  note_t0: |md
    - Single token, single position
    - Mask allows self-attention only
    - `tril(ones(1,1))` = [[1]]
  |
}

step2: {
  label: "Step 2: After First Generation (t=1)"
  
  seq_t1: {
    label: "Sequence Length = 2"
    style.fill: "#E8F4FD"
    style.stroke: "#2E86AB"
    
    token_t1_0: "<START>"
    token_t1_0.style.fill: "#D3D3D3"
    
    token_t1_1: "the"
    token_t1_1.style.fill: "#ACE1AF"
    token_t1_1.style.bold: true
  }
  
  mask_t1: {
    label: "Causal Mask [2×2]"
    style.fill: "#FFF9C4"
    style.stroke: "#F9A825"
    
    grid-columns: 2
    grid-gap: 0
    
    m10_0: "0"; m10_1: "1"
    m11_0: "0"; m11_1: "0"
    
    m10_0.style.fill: "#90EE90"; m10_1.style.fill: "#FF6B6B"
    m11_0.style.fill: "#90EE90"; m11_1.style.fill: "#90EE90"
    
    m10_1.style.bold: true
  }
  
  legend_t1: {
    label: "Legend"
    style.fill: transparent
    
    allowed: "0 = Can Attend"
    allowed.style.fill: "#90EE90"
    
    blocked: "1 = Blocked (-inf)"
    blocked.style.fill: "#FF6B6B"
  }
  
  note_t1: |md
    - New token appended
    - Mask grows to [2×2]
    - Position 1 blocked from future (none yet)
    - `tril(ones(2,2))` creates lower-triangular
  |
}

step3: {
  label: "Step 3: After Second Generation (t=2)"
  
  seq_t2: {
    label: "Sequence Length = 3"
    style.fill: "#E8F4FD"
    style.stroke: "#2E86AB"
    
    token_t2_0: "<START>"
    token_t2_0.style.fill: "#D3D3D3"
    
    token_t2_1: "the"
    token_t2_1.style.fill: "#D3D3D3"
    
    token_t2_2: "cat"
    token_t2_2.style.fill: "#ACE1AF"
    token_t2_2.style.bold: true
  }
  
  mask_t2: {
    label: "Causal Mask [3×3]"
    style.fill: "#FFF9C4"
    style.stroke: "#F9A825"
    
    grid-columns: 3
    grid-gap: 0
    
    m20_0: "0"; m20_1: "1"; m20_2: "1"
    m21_0: "0"; m21_1: "0"; m21_2: "1"
    m22_0: "0"; m22_1: "0"; m22_2: "0"
    
    m20_0.style.fill: "#90EE90"; m20_1.style.fill: "#FF6B6B"; m20_2.style.fill: "#FF6B6B"
    m21_0.style.fill: "#90EE90"; m21_1.style.fill: "#90EE90"; m21_2.style.fill: "#FF6B6B"
    m22_0.style.fill: "#90EE90"; m22_1.style.fill: "#90EE90"; m22_2.style.fill: "#90EE90"
    
    m20_1.style.bold: true; m20_2.style.bold: true
    m21_2.style.bold: true
  }
  
  note_t2: |md
    - Upper triangle = blocked (future)
    - Each row: only past + self visible
    - Position 2 sees [START, the, cat]
    - Autoregressive constraint enforced
  |
}

step4: {
  label: "Step 4: KV Cache Slicing"
  
  kvcache: {
    label: "KV Cache State"
    style.fill: "#E1BEE7"
    style.stroke: "#7B1FA2"
    
    k_cache: "K Cache [seq_len, d_k]"
    k_cache.style.fill: "#CE93D8"
    
    v_cache: "V Cache [seq_len, d_v]"
    v_cache.style.fill: "#CE93D8"
    
    new_k: "new_k [1, d_k] ← current token"
    new_k.style.fill: "#ACE1AF"
    new_k.style.bold: true
    
    new_v: "new_v [1, d_v] ← current token"
    new_v.style.fill: "#ACE1AF"
    new_v.style.bold: true
  }
  
  slice_op: {
    label: "Slicing Operation"
    style.fill: "#FFF3E0"
    style.stroke: "#E65100"
    
    code: |md
      python
      # At generation step t:
      # Full causal mask
      full_mask = torch.tril(
          torch.ones(max_len, max_len)
      )
      
      # Slice for current seq_len
      mask = full_mask[:seq_len, :seq_len]
      
      # Or build fresh each step:
      mask = torch.tril(
          torch.ones(seq_len, seq_len)
      )
      
    |
  }
  
  attention_op: {
    label: "Attention with Sliced Mask"
    style.fill: "#E8F5E9"
    style.stroke: "#2E7D32"
    
    formula: |md
      
      scores = Q @ K^T / sqrt(d_k)
      
      # Apply causal mask
      scores = scores.masked_fill(
          mask == 0, 
          float('-inf')
      )
      
      weights = softmax(scores)
      output = weights @ V
      
    |
  }
  
  kvcache -> slice_op: "seq_len"
  slice_op -> attention_op: "mask [seq_len, seq_len]"
  kvcache -> attention_op: "K, V"
}

step5: {
  label: "Step 5: Common Pitfalls"
  
  pitfall1: {
    label: "❌ Pitfall: Stale Mask"
    style.fill: "#FFCDD2"
    style.stroke: "#C62828"
    
    desc: |md
      **Problem**: Using mask from previous step
      
      python
      # Wrong - mask size doesn't match!
      old_mask = torch.tril(torch.ones(2, 2))
      # Now seq_len = 3, but mask is [2×2]
      scores[..., :2, :2].masked_fill(...)
      
      
      **Result**: Shape mismatch or incorrect attention
    |
  }
  
  pitfall2: {
    label: "❌ Pitfall: Off-by-One"
    style.fill: "#FFCDD2"
    style.stroke: "#C62828"
    
    desc: |md
      **Problem**: Incorrect slice bounds
      
      python
      # Wrong - includes one extra position
      mask = full_mask[:seq_len+1, :seq_len+1]
      
      
      **Result**: Model cheats by seeing next token
    |
  }
  
  correct: {
    label: "✓ Correct Implementation"
    style.fill: "#C8E6C9"
    style.stroke: "#2E7D32"
    
    desc: |md
      python
      def get_causal_mask(seq_len, device):
          """Build fresh causal mask each step."""
          return torch.tril(
              torch.ones(seq_len, seq_len, 
                         device=device)
          ).bool()
      
      # Usage in generation loop:
      for step in range(max_new_tokens):
          curr_len = tokens.shape[1]
          mask = get_causal_mask(curr_len, device)
          
          # Apply before softmax
          scores = scores.masked_fill(
              ~mask, float('-inf')
          )
      
    |
  }
}

step1 -> step2: "generate token" {style.animated: true}
step2 -> step3: "generate token" {style.animated: true}
step3 -> step4: "apply slicing"
step4 -> step5: "verify correctness"