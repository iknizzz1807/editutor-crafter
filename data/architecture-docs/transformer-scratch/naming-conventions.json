{
  "types": {
    "ScaledDotProductAttention": "dropout: Dropout layer",
    "MultiHeadAttention": "d_model: int, num_heads: int, dropout: float, w_q: Linear, w_k: Linear, w_v: Linear, w_o: Linear, attention: ScaledDotProductAttention",
    "PositionwiseFeedForward": "d_model: int, d_ff: int, dropout: float, w_1: Linear, w_2: Linear, activation: ReLU",
    "TransformerConfig": "d_model: int, num_heads: int, d_ff: int, num_encoder_layers: int, num_decoder_layers: int, vocab_size: int, max_seq_len: int, dropout: float, pad_token_id: int, d_k: int",
    "ScopeValidator": "ALLOWED_DEPENDENCIES: set, FORBIDDEN_PATTERNS: dict",
    "TensorShapeTracker": "enabled: bool, shapes: List",
    "SinusoidalPositionEncoding": "d_model: int, max_seq_len: int, pe: buffer",
    "TokenEmbedding": "vocab_size: int, d_model: int, pad_token_id: int, embed: nn.Embedding, scale: float",
    "EncoderLayer": "d_model: int, num_heads: int, d_ff: int, self_attention: MultiHeadAttention, feed_forward: PositionwiseFeedForward, residual_connections: List[nn.Module]",
    "DecoderLayer": "d_model: int, num_heads: int, d_ff: int, self_attention: MultiHeadAttention, cross_attention: MultiHeadAttention, feed_forward: PositionwiseFeedForward, residual_connections: List[nn.Module]",
    "PreNormResidualConnection": "norm: LayerNorm, dropout: Dropout",
    "PostNormResidualConnection": "norm: LayerNorm, dropout: Dropout",
    "SublayerOutput": "output: Tensor, attention_weights: Optional[Tensor]",
    "Transformer": "config: TransformerConfig, encoder: TransformerEncoder, decoder: TransformerDecoder",
    "TransformerTrainer": "model: Transformer, optimizer: Optimizer, criterion: Loss, scheduler: LRScheduler",
    "AutoregressiveGenerator": "model: nn.Module, device: torch.device, max_length: int, start_token_id: int, end_token_id: int, pad_token_id: int",
    "LabelSmoothingLoss": "vocab_size: int, smoothing: float, ignore_index: int, reduction: str",
    "TrainingMetrics": "window_size: int, losses: List[float], step_times: List[float], step_count: int",
    "GradientMonitor": "gradient_norms: Dict, hooks: List",
    "TransformerDataFlow": "model: nn.Module, shape_tracker: TensorShapeTracker, gradient_monitor: GradientMonitor",
    "NumericalStabilityUtils": "static utility class for numerical operations",
    "TensorShapeValidator": "enabled: bool, validation_history: List",
    "MemoryTracker": "device: torch.device, memory_snapshots: List",
    "TransformerTrainingStabilizer": "model: nn.Module, config: TransformerConfig, shape_validator: TensorShapeValidator, memory_tracker: MemoryTracker, gradient_monitor: GradientMonitor",
    "TransformerTestData": "vocab_size: int, d_model: int, max_seq_len: int, pad_token_id: int",
    "ReferenceAttention": "static utility class for attention validation",
    "ReferencePositionalEncoding": "static utility class for position encoding validation",
    "MilestoneChecker": "test_data: TransformerTestData, results: Dict[str, Dict]",
    "GradientFlowAnalyzer": "model: nn.Module, gradient_norms: Dict[str, List[float]], hooks: List",
    "TensorShapeDebugger": "static utility class for shape debugging",
    "AttentionDebugger": "attention_histories: Dict, entropy_histories: Dict",
    "TrainingDebugger": "window_size: int, loss_history: List, gradient_norm_history: List",
    "GPTBlock": "d_model: int, attention: MultiHeadAttention, feed_forward: PositionwiseFeedForward, ln1: LayerNorm, ln2: LayerNorm",
    "GPTModel": "config: TransformerConfig, embedding: TokenEmbedding, position_encoding: PositionalEncoding, blocks: List[GPTBlock], ln_f: LayerNorm, lm_head: Linear",
    "BERTLayer": "d_model: int, self_attention: MultiHeadAttention, feed_forward: PositionwiseFeedForward, norm1: LayerNorm, norm2: LayerNorm",
    "MaskedLanguageModelHead": "d_model: int, vocab_size: int, projection: Linear, activation: GELU",
    "PatchEmbedding": "img_size: int, patch_size: int, num_patches: int, proj: Conv2d, cls_token: Parameter, pos_embed: Parameter",
    "VisionTransformer": "config: TransformerConfig, patch_embed: PatchEmbedding, encoder: TransformerEncoder, classification_head: Linear",
    "LinearAttention": "d_model: int, num_heads: int, feature_dim: int",
    "PerformerAttention": "d_model: int, num_heads: int, num_features: int, projection_matrix: Tensor",
    "LocalAttention": "d_model: int, num_heads: int, window_size: int",
    "StridedAttention": "d_model: int, num_heads: int, stride: int",
    "MixedPrecisionTrainer": "model: nn.Module, optimizer: Optimizer, scaler: GradScaler",
    "WarmupCosineSchedule": "optimizer: Optimizer, warmup_steps: int, total_steps: int",
    "AdamWOptimizer": "params: List, lr: float, weight_decay: float, betas: Tuple",
    "StochasticDepthLayer": "layer: nn.Module, drop_prob: float, training: bool",
    "TerminologyConfig": "MODEL_DIMENSION: str, HEAD_DIMENSION: str, FEEDFORWARD_DIMENSION: str, BATCH_FIRST_ORDERING: bool, ATTENTION_MASK_VALUE: float, TEACHER_FORCING_TERM: str, AUTOREGRESSIVE_TERM: str",
    "TerminologyValidator": "config: TerminologyConfig, forbidden_terms: dict",
    "GlossaryManager": "glossary_path: str, terms: dict"
  },
  "methods": {
    "forward(query, key, value, mask=None)": "Apply attention mechanism and return output and weights",
    "create_padding_mask(sequence, pad_token_id=0)": "Create mask to hide padding tokens",
    "create_causal_mask(size)": "Create lower triangular mask for autoregressive decoding",
    "get_device()": "Return best available compute device",
    "get_device() -> torch.device": "Return best available compute device",
    "move_to_device(tensors: dict, device: torch.device) -> dict": "Move dictionary of tensors to specified device",
    "validate_dependencies(module_name: str) -> None": "Check if module stays within allowed dependencies",
    "validate_implementation_complexity(code: str) -> None": "Check if implementation complexity aligns with educational goals",
    "validate_tensor_shape(tensor, expected_shape, tensor_name)": "Validate tensor has expected shape",
    "check_attention_shapes(queries, keys, values, mask)": "Validate tensors for attention have compatible shapes",
    "create_padding_mask(sequences, pad_token_id)": "Create mask to hide padding tokens",
    "create_causal_mask(size, device)": "Create lower triangular mask for autoregressive decoding",
    "move_to_device(tensors, device)": "Move tensor dictionary to device",
    "split_heads(x)": "Split tensor into multiple attention heads",
    "combine_heads(x)": "Combine multiple attention heads back into single tensor",
    "create_multi_head_attention(config)": "Factory function to create multi-head attention from config",
    "forward(x) -> torch.Tensor": "Apply embedding or encoding transformation",
    "combine_embeddings(token_emb, pos_emb) -> torch.Tensor": "Add token and position embeddings",
    "validate_embedding_shapes(token_emb, pos_emb) -> None": "Check shape compatibility",
    "forward(x, mask=None)": "Apply GPT block with causal self-attention",
    "forward(x, encoder_output, self_attention_mask=None, cross_attention_mask=None)": "Apply decoder layer with three sublayers",
    "create_residual_connection(d_model, dropout, pre_norm)": "Factory function for residual connections",
    "validate_sublayer_shapes(input_tensor, output_tensor, layer_name)": "Validate shape consistency for residual connections",
    "get_attention_weights()": "Return stored attention weights for visualization",
    "forward(src, tgt, src_mask, tgt_mask, memory_mask)": "Complete forward pass through transformer",
    "encode(src, src_mask)": "Encoder-only forward pass",
    "decode_step(tgt, memory, tgt_mask)": "Single autoregressive decoding step",
    "train_step(batch)": "Execute single training step with loss computation",
    "validate(dataloader)": "Run validation loop returning metrics",
    "init_transformer_weights(module)": "Initialize all transformer parameters",
    "generate_greedy(src, src_mask)": "Generate sequence using greedy decoding",
    "generate_beam_search(src, src_mask, beam_width, length_penalty)": "Generate sequences using beam search",
    "generate_sampling(src, src_mask, temperature, top_k)": "Generate sequence using top-k sampling",
    "collate_translation_batch(batch, pad_token_id)": "Collate function for translation datasets",
    "create_masks(src, tgt, pad_token_id)": "Create all necessary masks for transformer training",
    "forward(logits, targets)": "Compute label smoothing loss",
    "track(tensor, name)": "Track tensor shape and return unchanged",
    "validate_shape(tensor, expected_shape, name)": "Validate tensor has expected shape",
    "forward_with_tracking(src, tgt, masks)": "Execute forward pass with tensor tracking",
    "analyze_attention_flow(attention_weights)": "Analyze attention patterns for insights",
    "diagnose_gradient_flow(model, sample_input)": "Comprehensive gradient flow analysis",
    "visualize_attention_patterns(weights, tokens, layer, head)": "Create attention visualization",
    "stable_softmax(logits, dim, mask, temperature)": "Compute numerically stable softmax with masking",
    "check_tensor_health(tensor, name)": "Comprehensive tensor health check for debugging",
    "validate_attention_shapes(queries, keys, values, mask)": "Validate tensor shapes for attention computation",
    "validate_multi_head_shapes(tensor, expected_shape, operation)": "Validate shapes during multi-head attention operations",
    "snapshot(label)": "Take memory snapshot with label for tracking",
    "get_memory_summary()": "Generate human-readable memory usage summary",
    "get_gradient_summary()": "Get summary statistics of gradient norms",
    "create_learning_rate_scheduler(optimizer, warmup_steps)": "Create learning rate scheduler with warmup",
    "validate_training_step(batch)": "Comprehensive validation before training step",
    "stable_forward_pass(src, tgt, src_mask, tgt_mask)": "Forward pass with stability monitoring",
    "compute_stable_loss(logits, targets, ignore_index)": "Compute loss with numerical stability",
    "monitor_gradient_step(loss, optimizer, max_grad_norm)": "Execute backward pass with gradient monitoring",
    "diagnose_training_issues(loss_history, attention_weights)": "Comprehensive diagnosis of training stability issues",
    "initialize_transformer_for_stability(model, config)": "Initialize Transformer parameters for training stability",
    "create_simple_attention_case()": "Create simple attention test case with hand-calculable results",
    "create_causal_mask_case(seq_len)": "Create test case for causal mask verification",
    "create_padding_mask_case()": "Create test case with padding tokens for mask verification",
    "create_translation_batch(batch_size)": "Create synthetic translation batch for end-to-end testing",
    "scaled_dot_product_attention(queries, keys, values, mask)": "Reference implementation with explicit steps for validation",
    "create_sinusoidal_encoding(max_len, d_model)": "Reference implementation of sinusoidal positional encoding",
    "check_milestone_1()": "Verify Milestone 1: Scaled Dot-Product Attention",
    "check_milestone_2()": "Verify Milestone 2: Multi-Head Attention",
    "register_gradient_hooks()": "Register backward hooks to capture gradients",
    "analyze_gradient_flow(input_batch)": "Run forward/backward pass and analyze gradients",
    "diagnose_attention_gradients()": "Diagnose attention-specific gradient issues",
    "diagnose_attention_shapes(queries, keys, values, mask)": "Diagnose attention tensor shape issues",
    "diagnose_multi_head_shapes(input_tensor, num_heads, d_model)": "Diagnose multi-head attention shape issues",
    "diagnose_multi_head_shapes(input_tensor, num_heads, d_model, operation)": "Diagnose multi-head attention shape issues",
    "print_summary()": "Print summary of all tracked tensor shapes",
    "register_gradient_hooks(model)": "Register backward hooks to capture gradients",
    "diagnose_gradient_flow(threshold_vanishing, threshold_exploding)": "Diagnose gradient flow issues",
    "analyze_attention_patterns(attention_weights, layer_name)": "Analyze attention patterns for health indicators",
    "diagnose_masking_errors(attention_weights, input_tokens, pad_token_id, is_causal)": "Diagnose potential masking errors",
    "diagnose_loss_curve(recent_losses)": "Analyze loss curve patterns for training issues",
    "validate_training_step(model, batch, loss_value)": "Comprehensive validation of a training step",
    "extract_patches(images)": "Convert images to sequence of patch embeddings",
    "create_causal_mask(seq_len, device)": "Create lower triangular mask for autoregressive decoding",
    "create_adamw_optimizer(model, lr, weight_decay)": "Create AdamW optimizer with proper parameter grouping",
    "create_warmup_cosine_schedule(optimizer, warmup_steps, total_steps)": "Create learning rate scheduler with warmup and cosine decay",
    "training_step(batch)": "Execute training step with mixed precision",
    "generate_greedy(src, max_length)": "Generate sequence using greedy decoding",
    "apply_stochastic_depth(x, layer, drop_prob)": "Apply stochastic depth regularization",
    "patch_to_sequence(patches)": "Convert 2D patches to 1D sequence for transformer processing",
    "sequence_to_patches(sequence, height, width)": "Convert sequence back to 2D spatial arrangement",
    "cross_modal_attention(text_tokens, image_patches, mask)": "Apply attention between different modalities",
    "validate_tensor_name(name: str) -> bool": "validate tensor follows naming conventions",
    "validate_docstring(docstring: str) -> list": "check docstring for terminology consistency",
    "validate_variable_names(code: str) -> list": "check variable names follow conventions",
    "extract_terms_from_code(source_dir: str) -> dict": "extract technical terms from implementation code",
    "validate_consistency() -> list": "check glossary consistency with implementation",
    "generate_cross_references() -> dict": "create cross-reference map between terms"
  },
  "constants": {
    "d_model": "model dimension",
    "num_heads": "number of attention heads",
    "d_k": "key/query dimension per head",
    "d_ff": "feed-forward hidden dimension",
    "pad_token_id": "padding token identifier",
    "TRANSFORMER_BASE": "Standard transformer configuration",
    "TRANSFORMER_SMALL": "Smaller transformer for experimentation",
    "vocab_size": "vocabulary size",
    "max_seq_len": "maximum sequence length",
    "dropout": "dropout rate",
    "weight_tying": "Whether to tie embedding and output projection weights",
    "max_length": "Maximum generation length",
    "start_token_id": "Token ID for sequence start",
    "end_token_id": "Token ID for sequence end",
    "beam_width": "Number of beams in beam search",
    "temperature": "Temperature for softmax",
    "smoothing": "Label smoothing factor",
    "grad_clip_norm": "Gradient clipping threshold",
    "warmup_steps": "Number of learning rate warmup steps",
    "max_grad_norm": "Gradient clipping threshold",
    "threshold_vanishing": "Threshold for detecting vanishing gradients",
    "threshold_exploding": "Threshold for detecting exploding gradients",
    "window_size": "Local attention window size",
    "TRANSFORMER_GPT": "GPT-style configuration with decoder-only architecture",
    "TRANSFORMER_BERT": "BERT-style configuration with encoder-only architecture",
    "VISION_TRANSFORMER_BASE": "Base ViT configuration for image classification",
    "patch_size": "Size of image patches for Vision Transformer",
    "img_size": "Input image resolution",
    "total_steps": "Total training steps for schedule",
    "drop_path_rate": "Stochastic depth probability",
    "stride": "Stride for strided attention pattern",
    "feature_dim": "Dimension for linear attention features",
    "num_features": "Number of random features for Performer attention",
    "d_v": "value dimension per head",
    "seq_len": "sequence length",
    "batch_size": "number of sequences per batch"
  },
  "terms": {
    "attention mechanism": "method for computing weighted combinations of sequence positions",
    "query-key-value paradigm": "framework where queries search through keys to retrieve values",
    "scaled dot-product attention": "core attention computation using softmax(QK^T/âˆšd_k)V",
    "multi-head attention": "parallel attention computation across multiple representation subspaces",
    "sequential bottleneck": "Computational constraint preventing parallel processing in RNNs",
    "vanishing gradients": "Problem where gradients become too small to drive learning in deep networks",
    "causal mask": "lower triangular mask preventing attention to future positions",
    "padding mask": "mask hiding padding tokens from attention computation",
    "encoder-decoder paradigm": "two-stage processing architecture with source understanding and target generation",
    "tensor shape conventions": "Standardized dimension ordering throughout the system",
    "batch-first ordering": "tensor layout with batch dimension first",
    "model dimension": "Core embedding dimension d_model",
    "attention head": "Parallel attention computation subspace",
    "sequence length": "Number of tokens in input sequence",
    "derived parameters": "Parameters computed from primary configuration",
    "head splitting": "process of partitioning embeddings across attention heads",
    "uniform splitting": "Equal dimension allocation across all attention heads",
    "parallel computation": "Simultaneous processing of multiple attention heads",
    "output projection": "Linear transformation that combines multi-head attention outputs",
    "head specialization": "Natural emergence of distinct attention patterns in different heads",
    "tensor reshaping": "operations to reorganize tensor dimensions",
    "broadcasting": "Automatic expansion of tensors to compatible shapes",
    "contiguous memory": "Optimized tensor layout for efficient computation",
    "sinusoidal positional encoding": "mathematical position representation using sine and cosine functions",
    "token embedding": "Learnable mapping from vocabulary indices to dense vectors",
    "position-wise feed-forward": "Independent transformation applied to each sequence position",
    "embedding scaling": "Multiplication by sqrt(d_model) to balance token and position magnitudes",
    "position blindness problem": "Attention mechanism's inability to distinguish token order without explicit positional information",
    "parameter sharing": "Using same weights for related computations",
    "residual connection": "skip connection adding input to sublayer output",
    "sublayer composition": "Pattern of combining attention and feed-forward with residual connections",
    "layer normalization": "normalization across feature dimension for each position",
    "pre-normalization": "Apply layer norm before sublayer operations",
    "post-normalization": "Apply layer norm after residual addition",
    "masked self-attention": "Self-attention with causal mask for autoregressive generation",
    "cross-attention": "Attention from decoder queries to encoder keys/values",
    "gradient highway": "Direct gradient path through residual connections",
    "vanishing gradient problem": "Gradients becoming too small in deep networks",
    "teacher forcing": "training strategy using ground truth as decoder input",
    "autoregressive generation": "sequential generation where each token depends on previous ones",
    "layer stack composition": "How multiple encoder and decoder layers work together in sequence",
    "encoder stack": "Multiple encoder layers processing source sequences",
    "decoder stack": "Multiple decoder layers generating target sequences",
    "output projection layer": "Linear transformation mapping decoder outputs to vocabulary logits",
    "weight tying": "Parameter sharing between embedding and output projection layers",
    "model initialization": "Weight initialization strategy for stable training",
    "gradient flow": "path gradients take during backpropagation",
    "representation hierarchy": "Progressive abstraction through stacked layers",
    "vocabulary distribution": "Probability distribution over vocabulary tokens",
    "logit computation": "Computing raw scores before softmax normalization",
    "cross-entropy loss": "standard loss function for classification problems",
    "label smoothing": "regularization distributing probability mass away from one-hot targets",
    "beam search": "decoding strategy maintaining multiple candidate sequences",
    "greedy decoding": "selecting highest probability token at each step",
    "exposure bias": "mismatch between training with ground truth and inference with model predictions",
    "gradient clipping": "Technique to prevent exploding gradients by rescaling when norm exceeds threshold",
    "perplexity": "Interpretable metric derived from cross-entropy loss",
    "forward pass sequence": "Step-by-step data flow from input tokens to output predictions",
    "attention information flow": "How attention weights propagate information across sequence positions",
    "numerical stability": "maintaining precision and avoiding overflow/underflow",
    "softmax overflow": "Exponential function overflow when computing attention probabilities",
    "gradient explosion": "Gradients becoming extremely large during backpropagation",
    "attention collapse": "pathological attention patterns becoming uniform or extremely peaked",
    "learning rate warmup": "Gradually increasing learning rate from zero during initial training steps",
    "mixed precision training": "Using 16-bit precision for forward/backward passes and 32-bit for parameter updates",
    "quadratic memory scaling": "Memory requirements that grow as square of sequence length",
    "attention memory scaling": "Memory consumption pattern specific to attention mechanisms",
    "dynamic batching": "Grouping sequences of similar lengths to optimize memory usage",
    "gradient accumulation": "Accumulating gradients over multiple mini-batches before parameter updates",
    "attention entropy": "Measure of attention distribution sharpness across positions",
    "residual connection scaling": "Proper initialization and scaling of skip connections",
    "unit testing approach": "Testing individual components with known inputs and expected outputs",
    "integration testing": "End-to-end testing of the complete transformer pipeline",
    "milestone verification checkpoints": "What to verify after completing each milestone",
    "performance benchmarks": "Training convergence and inference speed verification",
    "training convergence benchmarks": "Verify that the Transformer implementation can learn effectively on standard tasks",
    "inference speed benchmarks": "Establish performance expectations for generation and help identify computational bottlenecks",
    "memory usage benchmarks": "Verify that the Transformer implementation scales reasonably with batch size and sequence length",
    "tensor shape debugging": "systematic diagnosis of tensor dimension mismatches",
    "attention mechanism debugging": "diagnosis of attention computation and masking issues",
    "gradient flow analysis": "monitoring gradient magnitudes through network layers",
    "training dynamics debugging": "analysis of loss curves and convergence behavior",
    "attention pattern health": "assessment of attention distribution quality",
    "masking logic verification": "validation of causal and padding mask implementation",
    "memory usage analysis": "profiling memory consumption patterns",
    "performance bottleneck analysis": "identification of computational limiting factors",
    "shape validation strategy": "systematic approach to tensor dimension checking",
    "decoder-only architecture": "Transformer variant using only decoder stack with causal masking for autoregressive generation",
    "encoder-only architecture": "Transformer variant using only encoder stack with bidirectional attention for understanding tasks",
    "causal language modeling": "Training objective predicting next tokens given previous context",
    "masked language modeling": "Training objective predicting masked tokens using bidirectional context",
    "patch embedding": "Converting image patches to vector representations for sequence processing",
    "vision transformer": "Transformer architecture adapted for image processing using patch tokenization",
    "linear attention": "Attention variants that reduce quadratic complexity to linear or near-linear",
    "sparse attention": "Attention patterns that restrict which positions can attend to each other",
    "stochastic depth": "Regularization technique randomly skipping entire layers during training",
    "cross-modal attention": "Attention mechanism operating between different input modalities",
    "multimodal fusion": "Combining information from multiple input modalities in a unified model",
    "bidirectional context": "Access to both past and future context in sequence processing",
    "parameter scaling": "Strategies for effectively growing model size while maintaining training stability",
    "architectural variants": "Different configurations of the base Transformer for specialized use cases"
  }
}