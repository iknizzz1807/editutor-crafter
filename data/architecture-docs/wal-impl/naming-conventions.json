{
  "types": {
    "LogRecord": "enum with Redo/Undo/Commit/Abort/Checkpoint variants",
    "RedoRecord": "fields: lsn LSN, txn_id TransactionId, page_id PageId, offset u32, after_image Vec<u8>",
    "UndoRecord": "fields: lsn LSN, txn_id TransactionId, page_id PageId, offset u32, before_image Vec<u8>",
    "CommitRecord": "fields: lsn LSN, txn_id TransactionId",
    "AbortRecord": "fields: lsn LSN, txn_id TransactionId",
    "CheckpointRecord": "fields: lsn LSN, active_transactions Vec<TransactionId>, dirty_pages Vec<PageId>",
    "LSN": "type alias for u64",
    "TransactionId": "type alias for u64",
    "PageId": "type alias for u32",
    "LogSegment": "fields: file File, path PathBuf, current_size u64, max_size u64, first_lsn Option<LSN>, last_lsn Option<LSN>",
    "DatabasePage": "fields: page_id PageId, data Vec<u8>, lsn u64",
    "MockDatabase": "fields: pages Arc<RwLock<HashMap<PageId, DatabasePage>>>, page_size usize",
    "WalError": "enum with Io, Corruption, InvalidRecord, CheckpointInProgress, TruncationFailed, MasterRecordCorrupted variants",
    "WalResult": "type alias for Result<T, WalError>",
    "LogWriter": "fields: current_segment Arc<Mutex<LogSegment>>, buffer_pool Arc<Mutex<BufferPool>>, base_path PathBuf, segment_counter Arc<Mutex<u64>>, next_lsn Arc<Mutex<LSN>>",
    "RecoveryManager": "fields: log_reader LogReader, storage Arc<dyn Storage>, transaction_table TransactionTable, dirty_page_table DirtyPageTable, checkpoint_lsn Option<LSN>",
    "CheckpointManager": "fields: log_writer Arc<LogWriter>, storage Arc<dyn Storage>, state_machine CheckpointStateMachine, log_directory PathBuf, current_checkpoint_lsn Option<LSN>",
    "LogReader": "component for reading log records",
    "Storage": "trait for storage abstraction",
    "MemoryStorage": "in-memory storage implementation",
    "TransactionState": "fields: status TransactionStatus, last_lsn LSN, undo_next_lsn Option<LSN>",
    "TransactionStatus": "enum with Active, Committed, Aborted variants",
    "BufferPool": "fields: small_buffers VecDeque<Vec<u8>>, medium_buffers VecDeque<Vec<u8>>, large_buffers VecDeque<Vec<u8>>, total_allocated usize, max_total_size usize",
    "TransactionTable": "fields: transactions HashMap<TransactionId, TransactionState>",
    "DirtyPageTable": "fields: pages HashMap<PageId, LSN>",
    "CheckpointState": "enum with Idle, Collecting, Writing, Complete variants",
    "CheckpointStateMachine": "fields: state Arc<Mutex<CheckpointState>>",
    "MasterRecord": "fields: magic_number u32, version u32, checkpoint_lsn LSN, checkpoint_segment u64, checkpoint_offset u64, log_directory PathBuf, creation_timestamp u64, checksum u32",
    "TransactionCoordinator": "fields: log_writer Arc<LogWriter>, recovery_manager Arc<Mutex<RecoveryManager>>, checkpoint_manager Arc<CheckpointManager>, storage Arc<dyn Storage>, active_transactions Arc<RwLock<HashMap<TransactionId, TransactionContext>>>, transaction_counter Arc<Mutex<u64>>, shutdown_signal Arc<Mutex<bool>>",
    "TransactionContext": "fields: txn_id TransactionId, start_time Instant, operations Vec<PendingOperation>, locks_held Vec<PageId>, status TransactionStatus",
    "PendingOperation": "fields: page_id PageId, offset u32, before_image Vec<u8>, after_image Vec<u8>",
    "CrashSimulator": "fields: crash_points Arc<Mutex<HashSet<String>>>, crash_probability f64, current_operation Arc<Mutex<String>>, crash_count Arc<Mutex<u32>>",
    "DiskSpaceStatus": "enum with Normal, Warning, Critical, Emergency variants",
    "DiskMonitor": "fields: log_directory PathBuf, warning_threshold u64, critical_threshold u64, emergency_threshold u64, reserved_space u64",
    "CorruptionDetector": "fields: consecutive_errors usize, max_consecutive_errors usize, total_records_scanned usize, corrupted_records usize",
    "PartialWriteHandler": "fields: recovery_mode bool, last_valid_lsn Option<LSN>, truncation_point Option<u64>",
    "RecoveryErrorHandler": "fields: recovery_attempts u32, max_recovery_attempts u32, last_successful_checkpoint Option<LSN>, recovery_log_file File",
    "GoldenState": "independent correctness oracle for property-based testing",
    "LogFileParser": "fields: reader BufReader<File>, current_offset u64, file_size u64, validation_mode ValidationMode",
    "GoldenStateOracle": "fields: pages HashMap<PageId, Vec<u8>>, committed_transactions HashSet<TransactionId>, active_transactions HashMap<TransactionId, Vec<PendingOperation>>, transaction_commit_order Vec<(TransactionId, LSN)>",
    "ParsedRecord": "fields: offset u64, record LogRecord, raw_data Vec<u8>, crc_valid bool, validation_errors Vec<String>",
    "LogAnalysis": "fields: total_records usize, transaction_summary HashMap<TransactionId, TransactionSummary>, lsn_gaps Vec<(LSN, LSN)>, corruption_count usize, checkpoint_consistency CheckpointAnalysis, performance_metrics LogPerformanceMetrics",
    "ValidationMode": "enum with Strict, Permissive, Recovery variants",
    "ValidationReport": "fields: pages_correct usize, pages_incorrect usize, page_discrepancies Vec<PageDiscrepancy>, transaction_issues Vec<TransactionValidationError>, overall_success bool",
    "WalExtensionConfig": "fields: group_commit GroupCommitConfig, compression CompressionConfig, parallel_recovery ParallelRecoveryConfig, partitioning PartitioningConfig",
    "GroupCommitConfig": "fields: enabled bool, max_batch_size usize, batch_timeout Duration, adaptive_batching bool",
    "CompressionConfig": "fields: enabled bool, algorithm CompressionAlgorithm, compression_level u32, min_segment_size u64",
    "CompressionAlgorithm": "enum: None, Lz4, Zstd, Snappy",
    "ParallelRecoveryConfig": "fields: enabled bool, worker_threads usize, batch_size usize",
    "PartitioningConfig": "fields: enabled bool, partition_count usize, partition_strategy PartitionStrategy",
    "ExtensionRegistry": "fields: group_commit Option<Box<dyn GroupCommitManager>>, compression Option<Box<dyn CompressionEngine>>, logical_replication Option<Box<dyn ChangeStreamPublisher>>",
    "CommitCoordinator": "batch management component",
    "CommitBatch": "fields: pending transactions, completion notifications",
    "CommitWaiter": "synchronization primitive",
    "BatchMetrics": "observability tracking",
    "CompressionEngine": "trait for compression algorithms",
    "CompressedSegment": "compressed storage format",
    "CompressionMetadata": "segment compression info",
    "CompressionBuffer": "memory management",
    "RedoCoordinator": "parallel orchestration",
    "RedoWorker": "operation execution",
    "DependencyTracker": "conflict detection",
    "RedoScheduler": "load balancing",
    "RecoveryTarget": "target specification",
    "TimestampMapper": "time mapping",
    "TargetValidator": "consistency checking",
    "PITRManager": "recovery orchestration",
    "LogicalLogRecord": "change capture record",
    "ChangeStreamReader": "stream processing",
    "ReplicationFilter": "change filtering",
    "ChangePublisher": "event publishing",
    "DistributedLogWriter": "multi-node coordination",
    "ConsensusManager": "ordering protocol",
    "GlobalLSNManager": "LSN coordination",
    "DistributedRecovery": "multi-node recovery",
    "LogPartitionManager": "partition coordination",
    "PartitionedLogWriter": "multi-stream writing",
    "CrossLogRecovery": "multi-log recovery",
    "PartitionLoadBalancer": "load distribution",
    "RecoveryDomain": "partition management",
    "DependencyGraph": "cross-domain coordination",
    "GlobalTransactionTable": "cross-partition state",
    "RecoveryCoordinator": "domain orchestration",
    "StreamingCheckpointer": "continuous checkpointing",
    "CheckpointStreamer": "background I/O",
    "CheckpointRegistry": "generation management",
    "IncrementalTracker": "change tracking"
  },
  "methods": {
    "serialize() -> Vec<u8>": "convert log record to binary format",
    "deserialize(data: &[u8]) -> WalResult<LogRecord>": "parse binary data to log record",
    "lsn() -> LSN": "extract LSN from log record",
    "txn_id() -> Option<TransactionId>": "extract transaction ID where applicable",
    "append(data: &[u8]) -> WalResult<u64>": "append data to log segment, return offset",
    "force_sync() -> WalResult<()>": "fsync log segment to disk",
    "is_full() -> bool": "check if segment needs rotation",
    "write_data(offset: usize, data: &[u8], lsn: u64)": "update page data with LSN tracking",
    "get_page(page_id: PageId) -> Option<DatabasePage>": "retrieve page from mock database",
    "update_page(page: DatabasePage)": "store modified page in mock database",
    "write_and_force(&mut self, record: &LogRecord) -> WalResult<LSN>": "write log record with immediate fsync for durability",
    "recover(&mut self) -> WalResult<()>": "perform complete ARIES recovery",
    "analysis_pass(&mut self) -> WalResult<AnalysisResult>": "analysis phase of recovery",
    "analysis_pass(&mut self) -> WalResult<()>": "analysis phase of recovery",
    "append(&mut self, data: &[u8]) -> WalResult<u64>": "append data to log segment, return file offset",
    "sync(&mut self) -> WalResult<()>": "force sync segment to disk via fsync",
    "flush_buffer(&mut self) -> WalResult<()>": "write buffered records without fsync",
    "force_sync(&mut self) -> WalResult<()>": "fsync current segment to disk",
    "is_full(&self) -> bool": "check if segment has reached size limit",
    "write_data(&self, page_id: PageId, offset: usize, data: &[u8], lsn: u64)": "update page data with LSN tracking",
    "get_page(&self, page_id: PageId) -> Option<DatabasePage>": "retrieve page from mock database",
    "update_page(&self, page: DatabasePage)": "store modified page in mock database",
    "calculate_crc32(data: &[u8]) -> u32": "calculate CRC32 checksum for integrity",
    "validate_checksum(data: &[u8], expected: u32) -> bool": "verify data matches expected checksum",
    "rotate_segment(&mut self) -> WalResult<()>": "create new segment when current is full",
    "update_lsn_range(&mut self, lsn: LSN)": "track LSN range for segment",
    "redo_pass(&mut self) -> WalResult<()>": "redo phase replaying committed changes",
    "undo_pass(&mut self) -> WalResult<()>": "undo phase rolling back active transactions",
    "apply_redo_record(&mut self, record: &RedoRecord) -> WalResult<()>": "apply single redo record to target page",
    "apply_undo_record(&mut self, record: &UndoRecord) -> WalResult<LSN>": "apply single undo record and generate CLR",
    "get_or_create(&mut self, txn_id: TransactionId, lsn: LSN) -> &mut TransactionState": "get existing or create new transaction state",
    "mark_dirty(&mut self, page_id: PageId, lsn: LSN)": "mark page as dirty with earliest LSN",
    "min_recovery_lsn(&self) -> Option<LSN>": "get minimum LSN from dirty page table",
    "update_lsn(&mut self, lsn: LSN, prev_lsn: Option<LSN>)": "update transaction state with new LSN",
    "active_transactions(&self) -> impl Iterator": "iterator over active transactions",
    "committed_transactions(&self) -> impl Iterator": "iterator over committed transaction IDs",
    "create_checkpoint(&mut self, active_transactions: Vec<TransactionId>, dirty_pages: Vec<PageId>) -> WalResult<LSN>": "creates fuzzy checkpoint without blocking transactions",
    "truncate_log(&mut self) -> WalResult<()>": "safely removes old log segments after checkpoint",
    "update_master_record(&self, checkpoint_lsn: LSN) -> WalResult<()>": "atomically updates master record with checkpoint info",
    "load_master_record(&mut self) -> WalResult<Option<MasterRecord>>": "loads master record during initialization",
    "calculate_safe_truncation_lsn(&self, checkpoint_lsn: LSN, active_transactions: &[TransactionId]) -> LSN": "determines earliest LSN that must be retained",
    "begin_checkpoint(&self, checkpoint_lsn: LSN) -> bool": "starts checkpoint operation in state machine",
    "transition_to_writing(&self) -> Option<LSN>": "moves checkpoint to writing phase",
    "complete_checkpoint(&self) -> Option<LSN>": "marks checkpoint as complete",
    "validate(&self) -> WalResult<()>": "validates master record integrity",
    "calculate_checksum(&self) -> u32": "computes CRC32 for master record",
    "initialize() -> WalResult<()>": "run complete recovery before accepting transactions",
    "begin_transaction() -> WalResult<TransactionId>": "create new transaction and return ID",
    "execute_transaction(txn_id, operations) -> WalResult<()>": "complete write transaction flow with proper coordination",
    "abort_transaction(txn_id) -> WalResult<()>": "rollback transaction with compensation logging",
    "trigger_checkpoint() -> WalResult<LSN>": "coordinate checkpoint creation without blocking transactions",
    "shutdown() -> WalResult<()>": "graceful shutdown with active transaction completion",
    "write_and_force(record) -> WalResult<LSN>": "write log record with immediate fsync for durability",
    "recover() -> WalResult<()>": "perform complete ARIES recovery",
    "create_checkpoint(active_txns, dirty_pages) -> WalResult<LSN>": "create fuzzy checkpoint without blocking transactions",
    "add_crash_point(operation)": "add crash simulation point for testing",
    "set_current_operation(operation)": "check for crash simulation trigger",
    "check_space() -> WalResult<DiskSpaceStatus>": "check current disk space status against thresholds",
    "check_record(&mut self, record_data: &[u8]) -> WalResult<()>": "validate record integrity and update error counters",
    "scan_for_partial_writes(&mut self, log_reader: &mut LogReader) -> WalResult<Option<LSN>>": "detect incomplete log records and identify truncation point",
    "truncate_log_at_partial_write(&mut self, log_file: &mut File) -> WalResult<()>": "remove partial writes from log file",
    "handle_recovery_failure(&mut self, error: WalError, recovery_phase: &str) -> WalResult<bool>": "process recovery errors and determine retry strategy",
    "reset_to_previous_checkpoint(&mut self) -> WalResult<LSN>": "fallback to earlier checkpoint for recovery restart",
    "record_operation(&mut self, txn_id: TransactionId, page_id: PageId, offset: usize, data: Vec<u8>)": "record transaction operation in golden state",
    "commit_transaction(&mut self, txn_id: TransactionId, commit_lsn: LSN)": "commit transaction in golden state oracle",
    "verify_matches(&self, recovered_db: &MockDatabase) -> Result<(), String>": "compare golden state against recovered database",
    "parse_next_record() -> WalResult<Option<ParsedRecord>>": "parse next log record from file with validation",
    "analyze_log_file() -> WalResult<LogAnalysis>": "scan entire log file and build comprehensive analysis",
    "add_crash_point(operation: &str)": "add specific crash injection point for testing",
    "check_crash_trigger(operation: &str) -> bool": "check if crash should be triggered at current operation",
    "set_crash_probability(probability: f64)": "configure probabilistic crash injection",
    "process_log_record(record: &LogRecord) -> WalResult<()>": "process log record and update oracle state independently",
    "verify_recovery_correctness(recovered_db: &MockDatabase) -> WalResult<()>": "compare oracle state against recovered database",
    "generate_validation_report(recovered_db: &MockDatabase) -> ValidationReport": "generate comprehensive validation report",
    "commit_transaction(txn_id: TransactionId, commit_record: CommitRecord) -> WalResult<LSN>": "submit transaction for group commit batching",
    "compress_segment(segment: &LogSegment) -> WalResult<CompressedSegment>": "compress log segment when read-only",
    "decompress_segment(compressed: &CompressedSegment) -> WalResult<Vec<u8>>": "decompress segment data during recovery",
    "parallel_redo(redo_records: Vec<RedoRecord>) -> WalResult<()>": "execute redo operations in parallel"
  },
  "constants": {
    "DEFAULT_SEGMENT_SIZE": "64MB maximum size for log segments",
    "PAGE_SIZE": "4KB typical database page size",
    "RECORD_HEADER_SIZE": "21 bytes for log record headers",
    "CRC_SIZE": "4 bytes for CRC32 checksum",
    "SMALL_RECORD_SIZE": "256 bytes for small record buffers",
    "MEDIUM_RECORD_SIZE": "1KB for typical record buffers",
    "LARGE_RECORD_SIZE": "4KB for large record buffers",
    "MASTER_RECORD_MAGIC": "0xCHEC_KPNT magic number for master record validation",
    "MASTER_RECORD_VERSION": "1 current master record format version",
    "DEFAULT_CHECKPOINT_INTERVAL": "configurable time interval between automatic checkpoints"
  },
  "terms": {
    "Write-Ahead Logging": "technique where changes are logged before database updates",
    "ARIES": "Algorithm for Recovery and Isolation Exploiting Semantics",
    "durability": "ACID property ensuring committed changes survive crashes",
    "fsync": "system call to force buffered data to persistent storage",
    "fuzzy checkpoint": "checkpoint created without blocking concurrent transactions",
    "log sequence number": "monotonically increasing identifier for log records",
    "redo operation": "replaying committed changes during recovery",
    "undo operation": "rolling back uncommitted changes during recovery",
    "partial write": "incomplete write operation interrupted by crash",
    "group commit": "optimization batching multiple transaction commits to share fsync cost",
    "Log Sequence Number": "monotonically increasing identifier for log records",
    "after-image": "new data state after operation completes",
    "before-image": "original data state before operation",
    "append-only semantics": "principle that log records are always written to end of file and never modified",
    "log rotation": "process of creating new segment files when current segment reaches size limit",
    "force-write": "operation that writes data and forces sync before returning",
    "buffer management": "strategy for batching records in memory before writing to disk",
    "Compensation Log Record": "log record describing undo operations during recovery",
    "idempotency": "property that applying same operation multiple times produces same result",
    "transaction table": "data structure tracking transaction states during recovery",
    "dirty page table": "data structure tracking pages with uncommitted changes",
    "undo chain": "linked sequence of log records for backward transaction traversal",
    "master record": "bootstrap file pointing to most recent checkpoint",
    "log truncation": "safe removal of obsolete log segments after checkpoint",
    "checkpoint LSN": "logical timestamp serving as temporal boundary for checkpoint",
    "recovery waypoint": "checkpoint serving as starting point for crash recovery",
    "atomic rename": "filesystem operation guaranteeing all-or-nothing master record updates",
    "temporal consistency": "guarantee that checkpoint reflects system state at specific LSN",
    "truncation safety": "ensuring deleted log records are not needed for recovery",
    "corruption detection": "validation techniques to identify invalid or damaged log records",
    "torn page": "scenario where part of a write operation completed but other parts did not",
    "idempotent operations": "operations that produce same result when applied multiple times",
    "reserved space pool": "pre-allocated disk blocks for emergency operations when disk is full",
    "recovery atomicity": "principle that recovery either completes entirely or restarts from consistent state",
    "compensation log record": "log record describing undo operations during recovery",
    "graceful degradation": "strategy of reducing functionality while maintaining core consistency guarantees",
    "property-based testing": "testing approach using randomized inputs to verify invariant properties",
    "crash consistency": "guarantee that system state remains valid after unexpected crashes",
    "golden state oracle": "independent implementation used to verify correctness",
    "shrinking": "process of reducing failing test cases to minimal examples",
    "log compression": "reducing storage overhead by compressing log records",
    "parallel recovery": "accelerating crash recovery by processing multiple log records concurrently",
    "point-in-time recovery": "restoring database to any specific timestamp",
    "logical replication": "capturing database changes at logical level for cross-database replication",
    "distributed WAL": "extending logging system across multiple nodes",
    "multiple log files": "eliminating single-writer bottleneck by partitioning log records",
    "partitioned recovery": "parallelizing crash recovery by dividing database into independent domains",
    "asynchronous checkpointing": "eliminating performance impact of checkpoint operations with background processing",
    "streaming checkpoint": "continuously writing checkpoint data without periodic snapshots"
  }
}