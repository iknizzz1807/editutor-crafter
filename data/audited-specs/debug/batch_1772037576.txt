Now I understand the audit format. Let me analyze each project against the audit criteria:

1. **Technical Accuracy**: Are concepts correct?
2. **Measurability**: Can acceptance criteria be objectively tested?
3. **Progression**: Do milestones build logically?
4. **Completeness**: Any gaps in learning path?
5. **Realism**: Is scope appropriate for difficulty?
6. **Security**: Any security considerations missing?
7. **Performance**: Any performance considerations addressed?

Let me analyze each project:

---

## 1. platformer

**Analysis:**
- Technical Accuracy: Excellent - physics concepts (AABB collision, delta time, semi-implicit Euler) are correct
- Measurability: All criteria are testable (collision works, player moves, etc.)
- Progression: Good - builds from tile map → movement → jumping → animation → enemies
- Completeness: Covers all essential platformer mechanics
- Realism: 25-40 hours for intermediate seems reasonable
- Security: N/A for game dev project
- Performance: Already addresses overhead measurement and optimization

**Strengths:**
- Very detailed acceptance criteria with specific values (coyote time 80-150ms)
- Excellent pitfalls section covering real issues developers face
- Good prerequisite listing
- Resources are relevant

**Issues:**
None significant. This is a well-crafted spec.

---

## 2. process-spawner

**Analysis:**
- Technical Accuracy: All Unix process concepts are correct
- Measurability: Criteria are testable (fork works, pipes communicate, etc.)
- Progression: Good - single process → pipes → worker pool
- Completeness: Covers all essential process management concepts
- Realism: 12-18 hours seems appropriate for intermediate
- Security: Addresses some aspects (file descriptor cleanup)
- Performance: Mentions deadlock awareness

**Strengths:**
- Excellent technical depth (_exit vs exit, pipe deadlock)
- Very thorough pitfalls covering real edge cases
- Clear deliverables

**Issues:**
None significant. This is excellent for systems-level learning.

---

## 3. profiler-tool

**Analysis:**
- Technical Accuracy: Concepts are correct
- Measurability: Most criteria measurable
- Progression: CPU profiler → call graph → memory → async → export
- Completeness: Covers major profiling aspects
- Realism: 20-30 hours for intermediate seems tight but possible
- Security: Missing - no mention of security implications
- Performance: Already addresses overhead

**Strengths:**
- Good progression from CPU to memory to async
- Includes modern async profiling

**Issues:**
- **Minor security issue**: M2 mentions "HTTP endpoint for continuous profiling" but doesn't mention security considerations (authentication, access control) for exposing profiling data
- The 99Hz vs 100Hz prime frequency issue is mentioned but could be clearer on WHY

---

## 4. property-based-testing-framework

**Analysis:**
- Technical Accuracy: PBT concepts are correct
- Measurability: Criteria are testable
- Progression: Good - generators → runner → shrinking → stateful → integration
- Completeness: Covers core PBT features
- Realism: 40-55 hours for advanced is appropriate
- Security: N/A for testing framework
- Performance: Minimal overhead mentioned

**Strengths:**
- Comprehensive coverage of PBT concepts
- Good resource links to original papers

**Issues:**
None significant. Well-structured spec.

---

## 5. query-optimizer

**Analysis:**
- Technical Accuracy: Database optimization concepts are correct
- Measurability: Criteria are measurable and specific
- Progression: Plan representation → cost estimation → logical optimization → physical selection
- Completeness: Covers all major optimizer components
- Realism: 25-40 hours for advanced is reasonable
- Security: Missing - no mention of SQL injection or query security
- Performance: Core focus of project

**Strengths:**
- Very detailed acceptance criteria with specific algorithms (Selinger-style DP)
- Excellent prerequisites (requires sql-parser)
- Thorough coverage of optimization techniques

**Issues:**
- **Minor security gap**: No mention of security considerations even though query optimizers sit at the security-critical point of parsing user SQL (e.g., should the optimizer reject obviously malicious patterns, or is security handled at the parser level?)

---

## Summary

All 5 specs are high-quality (7-9/10 range). Only 2 have minor issues:

1. **profiler-tool**: HTTP endpoint security consideration missing
2. **query-optimizer**: No mention of where security concerns fit in the architecture

Let me provide the audit results in the required JSON format:

```json
[
  {
    "project_id": "platformer",
    "overall_score": 9,
    "verdict": "good",
    "issues": [],
    "strengths": [
      "Excellent technical depth with specific timing values (coyote time 80-150ms, jump buffer)",
      "Thorough pitfalls section covering real physics bugs (tunneling, corner-clipping, floating-point drift)",
      "Clear milestone progression from static tiles to full game loop",
      "Well-defined prerequisites and relevant learning resources"
    ],
    "critique": "An exceptionally well-crafted game physics spec with precise acceptance criteria and realistic scope for intermediate learners."
  },
  {
    "project_id": "process-spawner",
    "overall_score": 9,
    "verdict": "good",
    "issues": [],
    "strengths": [
      "Deep technical accuracy (_exit vs exit, pipe deadlock scenarios, SIGCHLD queuing)",
      "Comprehensive pitfalls covering dangerous race conditions and resource leaks",
      "Clear progression from single fork/exec to complex worker pools with signal handling",
      "Practical scope appropriate for Unix systems learning"
    ],
    "critique": "Outstanding systems programming spec with correct low-level details and excellent coverage of edge cases."
  },
  {
    "project_id": "profiler-tool",
    "overall_score": 8,
    "verdict": "needs_fix",
    "issues": [
      {
        "type": "security",
        "severity": "minor",
        "location": "milestone 5 acceptance criteria",
        "description": "HTTP endpoint for continuous profiling lacks security considerations",
        "suggestion": "Add acceptance criterion: 'HTTP endpoint requires authentication or documents that it should only bind to localhost' in M5"
      }
    ],
    "strengths": [
      "Good coverage of both CPU and memory profiling techniques",
      "Includes modern async/await profiling which is increasingly relevant",
      "Appropriate technical depth with overhead limits specified"
    ],
    "critique": "Strong profiling spec with minor gap around HTTP endpoint security."
  },
  {
    "project_id": "property-based-testing-framework",
    "overall_score": 9,
    "verdict": "good",
    "issues": [],
    "strengths": [
      "Comprehensive coverage of PBT from generators through stateful testing",
      "Excellent resource links to foundational papers (QuickCheck, Hughes)",
      "Clear progression with appropriate complexity for advanced level",
      "Includes shrinking which is often overlooked in PBT implementations"
    ],
    "critique": "Well-structured advanced spec covering all essential PBT concepts with good references."
  },
  {
    "project_id": "query-optimizer",
    "overall_score": 8,
    "verdict": "needs_fix",
    "issues": [
      {
        "type": "security",
        "severity": "minor",
        "location": "overall",
        "description": "No mention of security considerations despite being in the SQL query path",
        "suggestion": "Add note in prerequisites or essence clarifying that SQL injection/security is handled by the parser prerequisite, or add security considerations section"
      }
    ],
    "strengths": [
      "Highly detailed acceptance criteria with specific algorithms (Selinger-style DP)",
      "Excellent technical accuracy on cost estimation and histogram statistics",
      "Clear dependency on sql-parser prerequisite shows good project decomposition",
      "Appropriate scope for advanced database systems learning"
    ],
    "critique": "Technically excellent query optimizer spec with minor gap clarifying security boundaries."
  }
]
```

Since I need to provide fixed_yaml for the two "needs_fix" projects:

For **profiler-tool**, I need to add a security consideration to M5:

For **query-optimizer**, I need to add a note about security boundaries.

Let me construct the fixed YAML for these two projects:

```json
[
  {
    "project_id": "platformer",
    "overall_score": 9,
    "verdict": "good",
    "issues": [],
    "strengths": ["Excellent technical depth with specific timing values (coyote time 80-150ms, jump buffer)", "Thorough pitfalls section covering real physics bugs (tunneling, corner-clipping, floating-point drift)", "Clear milestone progression from static tiles to full game loop", "Well-defined prerequisites and relevant learning resources"],
    "critique": "An exceptionally well-crafted game physics spec with precise acceptance criteria and realistic scope for intermediate learners.",
    "fixed_yaml": null
  },
  {
    "project_id": "process-spawner",
    "overall_score": 9,
    "verdict": "good",
    "issues": [],
    "strengths": ["Deep technical accuracy (_exit vs exit, pipe deadlock scenarios, SIGCHLD queuing)", "Comprehensive pitfalls covering dangerous race conditions and resource leaks", "Clear progression from single fork/exec to complex worker pools with signal handling", "Practical scope appropriate for Unix systems learning"],
    "critique": "Outstanding systems programming spec with correct low-level details and excellent coverage of edge cases.",
    "fixed_yaml": null
  },
  {
    "project_id": "profiler-tool",
    "overall_score": 8,
    "verdict": "needs_fix",
    "issues": [{"type": "security", "severity": "minor", "location": "milestone 5 acceptance criteria", "description": "HTTP endpoint for continuous profiling lacks security considerations", "suggestion": "Add acceptance criterion: 'HTTP endpoint requires authentication or documents that it should only bind to localhost' in M5"}],
    "strengths": ["Good coverage of both CPU and memory profiling techniques", "Includes modern async/await profiling which is increasingly relevant", "Appropriate technical depth with overhead limits specified"],
    "critique": "Strong profiling spec with minor gap around HTTP endpoint security.",
    "fixed_yaml": "description: Build a sampling profiler that measures CPU usage, memory allocation,\n  and function timing for performance analysis of applications.\n\ndifficulty: intermediate\ndomain: performance-engineering\nessence: Statistical sampling of program execution to identify hot paths, memory\n  allocation tracking for leak detection, call graph construction for understanding\n  program behavior, and flame graph visualization for intuitive performance analysis.\n  \nestimated_hours: 20-30\nid: profiler-tool\nlanguages:\n  also_possible:\n  - Python\n  - C\n  recommended:\n  - Rust\n  - C++\n  - Go\nlearning_outcomes:\n- Implement sampling-based CPU profiling\n- Track memory allocations and detect leaks\n- Build call graph from stack samples\n- Generate flame graph visualization data\n- Handle profiling overhead minimization\n- Support async/await profiling patterns\n- Profile multi-threaded applications\n- Export profiles in standard formats (pprof)\nmilestones:\n- acceptance_criteria:\n  - Timer-based sampling at configurable frequency (e.g., 99Hz)\n  - Stack unwinding captures full call stack\n  - Samples aggregated by function/call path\n  - Overhead under 5% at 100Hz sampling\n  - Works with multi-threaded applications\n  - Handles dynamically loaded code (shared libraries)\n  concepts:\n  - Statistical sampling\n  - Stack unwinding\n  - Signal handling\n  - Sampling bias\n  deliverables:\n  - Sampling profiler core\n  - Stack unwinder\n  - Sample aggregation\n  - Multi-thread support\n  - Overhead measurement\n  description: Implement a statistical sampler that periodically captures stack traces\n    to identify CPU hot spots.\n\n  \n  estimated_hours: 8-10\n  id: profiler-m1\n  name: Sampling-Based CPU Profiler\n  pitfalls:\n  - Sampling frequency affects accuracy vs overhead\n  - Prime frequency (99Hz) avoids lock-step with application\n  - Stack unwinding fails without frame pointers\n  - Signal safety in signal handler\n  - Thread races during sample collection\n  skills:\n  - Timer signals\n  - Stack walking\n  - Thread handling\n  - Overhead management\n- acceptance_criteria:\n  - Call graph constructed from stack samples\n  - Each node shows function name and sample count\n  - Edges show caller-callee relationships\n  - Flame graph data in standard format (folded stacks)\n  - SVG or JSON output for visualization\n  - Supports filtering by function name\n  - Differential flame graphs (compare two profiles)\n  concepts:\n  - Call graph construction\n  - Stack folding\n  - Flame graph visualization\n  - Differential analysis\n  deliverables:\n  - Call graph builder\n  - Flame graph generator\n  - Stack folder\n  - SVG/JSON output\n  - Differential support\n  description: Build call graph from samples and generate flame graph visualization\n    data.\n\n  \n  estimated_hours: 6-8\n  id: profiler-m2\n  name: Call Graph & Flame Graph Generation\n  pitfalls:\n  - Recursive functions inflate counts\n  - Inlined functions may not appear\n  - Generic functions need monomorphization info\n  - Large call graphs hard to visualize\n  - Time ordering lost in aggregation\n  skills:\n  - Graph algorithms\n  - Data aggregation\n  - Visualization formats\n  - Comparative analysis\n- acceptance_criteria:\n  - Hook malloc/free or use LD_PRELOAD interceptor\n  - Track allocation size, site (call stack), and timestamp\n  - Detect memory leaks (allocated but not freed)\n  - Report allocation hot spots by call site\n  - Temporal analysis: allocations over time\n  - Memory usage high-water mark\n  - Low overhead (< 10%) for allocation-heavy code\n  concepts:\n  - Allocation interception\n  - Leak detection\n  - Call site attribution\n  - Memory profiling\n  deliverables:\n  - Allocation interceptor\n  - Leak detector\n  - Hot spot reporter\n  - Temporal analysis\n  - Memory high-water tracking\n  description: Track memory allocations to identify allocation hot spots and potential\n    memory leaks.\n\n  \n  estimated_hours: 6-8\n  id: profiler-m3\n  name: Memory Allocation Tracking\n  pitfalls:\n  - Interceptor overhead significant for alloc-heavy code\n  - Boehm effect: profiler changes allocation behavior\n  - Tracking every allocation uses lots of memory\n  - False positives from long-lived caches\n  - Free tracking may miss due to optimization\n  skills:\n  - Library interposition\n  - Allocation tracking\n  - Leak analysis\n  - Overhead minimization\n- acceptance_criteria:\n  - Async task identification (Future, coroutine, goroutine)\n  - Logical call stack reconstruction across await points\n  - Time attribution includes both CPU and await time\n  - Works with at least one async runtime (Tokio, async-std, goroutines)\n  - Visualizes await time separately from CPU time\n  - Handles task spawning and completion\n  concepts:\n  - Async execution model\n  - Logical vs physical stack\n  - Task identity\n  - Time attribution\n  deliverables:\n  - Async task tracker\n  - Logical stack builder\n  - CPU vs await time split\n  - Runtime integration\n  - Async-aware output\n  description: Extend profiler to handle async/await patterns where logical call\n    stacks span multiple physical stacks.\n\n  \n  estimated_hours: 6-8\n  id: profiler-m4\n  name: Async/Aware Profiling\n  pitfalls:\n  - Async runtimes vary in implementation\n  - Task identity tracking across migrations\n  - Await time includes I/O wait and scheduling delay\n  - Logical stack may be deeper than physical\n  - Wake-up attribution loses original spawn context\n  skills:\n  - Async runtime integration\n  - Task tracking\n  - Stack reconstruction\n  - Time categorization\n- acceptance_criteria:\n  - Export in pprof protobuf format\n  - Export in collapsed stack format (for flamegraph.pl)\n  - Import and merge multiple profile files\n  - Symbol resolution for stripped binaries\n  - Source code line attribution with debug info\n  - HTTP endpoint for continuous profiling binds to localhost only or requires\n    authentication\n  - Command-line interface for profiling\n  concepts:\n  - Profile formats\n  - Symbol resolution\n  - Debug information\n  - Continuous profiling\n  - Secure endpoint design\n  deliverables:\n  - pprof exporter\n  - Collapsed stack exporter\n  - Profile merger\n  - Symbol resolver\n  - CLI tool\n  - HTTP endpoint with localhost-only binding or auth\n  description: Export profiles in standard formats and integrate with visualization\n    tools.\n\n  \n  estimated_hours: 4-6\n  id: profiler-m5\n  name: Profile Export & Integration\n  pitfalls:\n  - pprof format has multiple versions\n  - Symbol resolution needs debug symbols\n  - Source lines may be inlined away\n  - Continuous profiling uses storage\n  - HTTP endpoint exposed without authentication can leak sensitive performance data\n  skills:\n  - Format implementation\n  - Binary analysis\n  - Tool integration\n  - CLI design\n  - Service security\nname: Profiler Tool\nprerequisites:\n- name: Understanding of stack frames and calling conventions\n  type: skill\n- name: Signal handling in Unix/Linux\n  type: skill\n- name: Basic statistics (sampling, confidence)\n  type: skill\n- name: Multi-threading concepts\n  type: skill\nresources:\n- name: Systems Performance (Brendan Gregg)\n  type: book\n  url: https://www.brendangregg.com/sysperfbook.html\n- name: Flame Graphs\n  type: article\n  url: https://www.brendangregg.com/flamegraphs.html\n- name: pprof Format\n  type: documentation\n  url: https://github.com/google/pprof/blob/main/proto/README.md\n- name: perf_event_open\n  type: documentation\n  url: https://man7.org/linux/man-pages/man2/perf_event_open.2.html\nskills:\n- Statistical Sampling\n- Stack Unwinding\n- Memory Tracking\n- Call Graph Analysis\n- Flame Graphs\n- Signal Handling\n- Multi-threading\n- Data Visualization\ntags:\n- intermediate\n- debugging\n- flame-graph\n- memory\n- performance\n- profiling\n- sampling\nwhy_important: Profiling is essential for performance optimization. Understanding\n  how profilers work enables you to build custom profiling tools and interpret profiling\n  data correctly. Valuable at $130K-220K+ for performance engineers and systems developers.\n  \n"
  },
  {
    "project_id": "property-based-testing-framework",
    "overall_score": 9,
    "verdict": "good",
    "issues": [],
    "strengths": ["Comprehensive coverage of PBT from generators through stateful testing", "Excellent resource links to foundational papers (QuickCheck, Hughes)", "Clear progression with appropriate complexity for advanced level", "Includes shrinking which is often overlooked in PBT implementations"],
    "critique": "Well-structured advanced spec covering all essential PBT concepts with good references.",
    "fixed_yaml": null
  },
  {
    "project_id": "query-optimizer",
    "overall_score": 8,
    "verdict": "needs_fix",
    "issues": [{"type": "security", "severity": "minor", "location": "overall", "description": "No mention of security considerations despite being in the SQL query path", "suggestion": "Add note in prerequisites or essence clarifying that SQL injection/security is handled by the parser prerequisite, or add security considerations section"}],
    "strengths": ["Highly detailed acceptance criteria with specific algorithms (Selinger-style DP)", "Excellent technical accuracy on cost estimation and histogram statistics", "Clear dependency on sql-parser prerequisite shows good project decomposition", "Appropriate scope for advanced database systems learning"],
    "critique": "Technically excellent query optimizer spec with minor gap clarifying security boundaries.",
    "fixed_yaml": "architecture_doc: architecture-docs/query-optimizer/index.md\ndescription: Build a cost-based query optimizer that transforms SQL query ASTs into efficient\n  physical execution plans using statistics-driven cost estimation, rule-based logical\n  rewrites, dynamic programming for join ordering, and physical operator selection.\ndifficulty: advanced\ndomain: data-storage\nessence: Cost-based query plan generation through statistics collection (histograms,\n  row counts), cardinality estimation, rule-based logical optimization (predicate\n  pushdown, projection pruning), dynamic programming for join ordering, and selection\n  between physical operators to minimize estimated execution cost.\n\n  Note: Security concerns such as SQL injection are handled by the prerequisite sql-parser\n  project; this optimizer operates on validated ASTs and focuses solely on performance.\nestimated_hours: 25-40\nid: query-optimizer\nlanguages:\n  also_possible:\n  - Rust\n  - C++\n  recommended:\n  - Python\n  - Java\n  - Go\nlearning_outcomes:\n- Implement tree-based query plan representations with logical and physical operators\n- Build a statistics collector (ANALYZE) that gathers histograms and row counts from\n  tables\n- Design cost models that estimate I/O and CPU usage using table statistics\n- Apply rule-based logical optimizations (predicate pushdown, projection pruning,\n  constant folding)\n- Build dynamic programming algorithms for optimal join ordering\n- Select between physical operators (sequential scan vs index scan, hash join vs merge\n  join vs nested loop)\n- Apply selectivity estimation using equi-depth histograms\n- Debug query performance by analyzing and comparing execution plans\nmilestones:\n- acceptance_criteria:\n  - Plan tree has operator nodes with children representing inputs; each node carries\n    estimated cost and output cardinality\n  - Logical operators include Scan, Filter, Project, Join (with join type), and Aggregate\n  - Physical operators include SeqScan, IndexScan, NestedLoopJoin, HashJoin, SortMergeJoin,\n    and Sort\n  - Each operator node is annotated with estimated row count (cardinality) and estimated\n    cost\n  - ANALYZE command scans a table and collects — total row count, per-column distinct\n    value count, per-column null fraction, and per-column equi-depth histogram with\n    configurable bucket count (default 100)\n  - Histograms store bucket boundaries and per-bucket row counts for selectivity estimation\n  - Statistics are stored in a catalog and can be retrieved by table name and column\n    name\n  - Plan tree pretty-printer outputs indented text showing operator type, estimated\n    cardinality, and cost at each node\n  concepts:\n  - Logical operators (Scan, Filter, Join, Project, Aggregate)\n  - Physical operators (SeqScan, IndexScan, HashJoin, MergeJoin, NLJoin)\n  - Plan tree structure with parent-child relationships\n  - Table statistics (row count, column distinct values, null fraction)\n  - Equi-depth histograms for value distribution\n  deliverables:\n  - Plan tree data structure with logical and physical operator node types\n  - Cost and cardinality annotations per node\n  - ANALYZE command collecting row counts, distinct values, null fractions, and histograms\n  - Statistics catalog storing per-table, per-column statistics\n  - Plan tree pretty-printer for debugging\n  description: Define query plan tree structure with logical and physical operator\n    nodes, and implement a statistics collector (ANALYZE) that gathers row counts,\n    distinct value counts, and histograms from table data.\n  estimated_hours: 5-7\n  id: query-optimizer-m1\n  name: Plan Representation & Statistics Collection\n  pitfalls:\n  - Logical and physical operators must be separate types; conflating them prevents\n    exploring alternative physical implementations of the same logical plan\n  - Histogram bucket count affects estimation accuracy — too few buckets miss value\n    distribution details; too many waste memory\n  - ANALYZE on a large table is expensive; support sampling (random N% of rows) for\n    approximate statistics\n  - Statistics become stale after data modifications; document that ANALYZE should\n    be re-run after significant data changes\n  skills:\n  - Tree data structure design\n  - Operator pattern with polymorphism\n  - Statistics collection algorithms\n  - Histogram construction\n- acceptance_criteria:\n  - Cost model estimates total cost as weighted sum of I/O cost (pages read/written)\n    and CPU cost (tuples processed); weights are configurable (default I/O weight\n    = 1.0, CPU weight = 0.01)\n  - Sequential scan cost = number_of_pages + (cpu_weight * number_of_rows)\n  - Index scan cost = (selectivity * number_of_pages) + (selectivity * number_of_rows\n    * cpu_weight) + index_lookup_cost\n  - Filter selectivity for equality predicates (col = value) is estimated using histogram\n    — bucket containing the value gives selectivity = bucket_frequency / total_rows\n  - Filter selectivity for range predicates (col > value) is estimated by summing\n    histogram buckets above the value with interpolation for partial buckets\n  - Compound predicates with AND use multiplicative selectivity (independence assumption);\n    OR uses additive minus intersection\n  - Join output cardinality is estimated as (rows_A * rows_B) / max(distinct_A, distinct_B)\n    for equi-joins\n  - Cost estimates are within 2x of actual row counts on test datasets with uniform\n    and skewed distributions (verified by running queries and counting actual rows)\n  concepts:\n  - Cost model (I/O pages + CPU tuples)\n  - Filter selectivity estimation from histograms\n  - Join cardinality estimation using min(distinct_A, distinct_B)\n  - Independence assumption for compound predicates\n  - Cost unit definition and weighting\n  deliverables:\n  - Cost model with configurable I/O and CPU weight parameters\n  - Sequential scan cost estimator\n  - Index scan cost estimator with selectivity-based I/O reduction\n  - Filter selectivity estimator using histogram lookup for equality and range predicates\n  - Compound predicate selectivity using independence assumption\n  - Join cardinality estimator using distinct value counts\n  - Accuracy test comparing estimated vs actual row counts\n  description: Implement cost models that estimate plan execution cost using table\n    statistics. Estimate filter selectivity using histograms and join output cardinality\n    using distinct value counts.\n  estimated_hours: 5-8\n  id: query-optimizer-m2\n  name: Cost Estimation & Selectivity\n  pitfalls:\n  - Independence assumption for AND predicates is often wrong (correlated columns);\n    acknowledge this as a known limitation\n  - Histogram interpolation within a bucket assumes uniform distribution within the\n    bucket; highly skewed data within buckets produces errors\n  - Join cardinality estimation assumes key uniqueness on at least one side; many-to-many\n    joins can produce much larger output than estimated\n  - Statistics staleness after INSERT/DELETE causes cost estimates to diverge from\n    reality; test with fresh statistics\n  - Cost model weights (I/O vs CPU) depend on hardware; provide defaults but make\n    them tunable\n  skills:\n  - Statistical selectivity estimation\n  - Probability-based cardinality prediction\n  - Cost function design with I/O and CPU components\n  - Histogram lookup and interpolation\n- acceptance_criteria:\n  - Predicate pushdown moves filter operators below join operators when the filter\n    references only one side of the join, reducing the number of rows entering the\n    join\n  - Predicate pushdown correctly handles filters that reference both sides of a join\n    by leaving them above the join as join conditions\n  - Projection pruning removes columns from intermediate results that are not referenced\n    by any upstream operator, reducing tuple width\n  - Constant folding evaluates expressions with only literal operands at optimization\n    time (e.g., WHERE 1 = 1 is eliminated, WHERE 1 = 0 prunes the entire branch)\n  - Redundant predicate elimination removes duplicate filter conditions (e.g., x >\n    5 AND x > 5 → x > 5)\n  - Rules are applied iteratively until no more transformations are possible (fixed-point\n    iteration)\n  - Each rule transformation preserves query semantics — the logical plan produces\n    the same result before and after transformation (verified by test)\n  - Predicate pushdown demonstrably reduces estimated plan cost on a 3-table join\n    query with selective filters\n  concepts:\n  - Predicate pushdown (push filters below joins)\n  - Projection pruning (remove unused columns early)\n  - Constant folding (evaluate constant expressions at compile time)\n  - Redundant predicate elimination\n  - Rule application ordering\n  deliverables:\n  - Predicate pushdown rule moving filters below joins where applicable\n  - Projection pruning rule removing unreferenced columns from intermediate plans\n  - Constant folding rule evaluating compile-time constant expressions\n  - Redundant predicate elimination rule\n  - Rule engine applying transformations to fixed point\n  - Test suite verifying semantic preservation and cost reduction\n  description: Implement rule-based logical rewrites that transform the logical plan\n    tree to reduce cost before physical operator selection. Rules include predicate\n    pushdown, projection pruning, and constant folding.\n  estimated_hours: 5-8\n  id: query-optimizer-m3\n  name: Rule-Based Logical Optimization\n  pitfalls:\n  - Predicate pushdown must check column provenance — pushing a filter that references\n    table B below a join with table A is incorrect\n  - Projection pruning must not remove columns needed by ORDER BY, GROUP BY, or HAVING\n    even if they don't appear in SELECT\n  - Rule ordering matters — predicate pushdown should run before join ordering so\n    that pushed-down predicates reduce cardinality estimates used in join cost calculation\n  - Fixed-point iteration can be infinite if rules are not monotonically reducing;\n    add a max iteration limit (e.g., 100) as a safety valve\n  skills:\n  - AST/plan tree transformation\n  - Pattern matching on tree nodes\n  - Rule engine design\n  - Algebraic equivalence rules\n- acceptance_criteria:\n  - Dynamic programming enumerates join subsets for multi-table queries; for N tables,\n    considers all 2^N - 1 non-empty subsets with memoization\n  - For each pair of subsets that combine to form a larger subset, the optimizer evaluates\n    the cost of joining them and retains only the cheapest plan per subset\n  - Both left-deep and bushy join tree shapes are considered (or configurable to restrict\n    to left-deep only for reduced search space)\n  - Cross products (joins without predicates) are penalized with a high cost multiplier\n    and avoided when predicate-connected alternatives exist\n  - Physical operator selection chooses between SeqScan and IndexScan based on filter\n    selectivity — IndexScan is preferred when selectivity < configurable threshold\n    (default 15%)\n  - Join algorithm selection chooses between NestedLoopJoin (small outer, indexed\n    inner), HashJoin (equi-join with large inputs), and SortMergeJoin (pre-sorted\n    inputs or when sort order is needed downstream)\n  - Final physical plan includes all operators selected with their cost estimates;\n    total plan cost is the sum of all operator costs\n  - For a 4-table join query, the optimizer produces a plan with demonstrably lower\n    cost than a naive left-to-right join order (verified by comparing plan costs)\n  - Optimization completes in under 1 second for queries with up to 8 tables; for\n    more tables, a greedy heuristic fallback is used\n  concepts:\n  - Dynamic programming for join ordering (Selinger-style)\n  - Left-deep vs bushy join trees\n  - Physical operator selection based on cost\n  - Access method selection (sequential scan vs index scan)\n  - Join algorithm selection (nested loop, hash join, sort-merge join)\n  - Sort order propagation (interesting orders)\n  deliverables:\n  - Dynamic programming join ordering with subset enumeration and memoization\n  - Left-deep and bushy join tree support\n  - Cross product detection and penalization\n  - Access method selector (SeqScan vs IndexScan based on selectivity)\n  - Join algorithm selector (NLJ, HashJoin, SortMergeJoin based on input sizes and\n    sort order)\n  - Final physical plan generator with total cost annotation\n  - Greedy heuristic fallback for queries exceeding DP table count limit\n  - Comparison test showing optimizer plan cost vs naive join order cost\n  description: Find the optimal join order for multi-table queries using dynamic programming,\n    and select physical operators (scan method, join algorithm) based on cost estimates\n    to produce the final execution plan.\n  estimated_hours: 7-12\n  id: query-optimizer-m4\n  name: Join Ordering & Physical Plan Selection\n  pitfalls:\n  - Join ordering is exponential (2^N subsets); for >10 tables, DP is infeasible —\n    implement a greedy or genetic algorithm fallback\n  - Cross products must still be considered when no predicate connects two subsets;\n    completely excluding them breaks queries with Cartesian products\n  - IndexScan selection at low selectivity (e.g., 50%) is worse than SeqScan due to\n    random I/O; the threshold depends on hardware\n  - SortMergeJoin is advantageous when one or both inputs are already sorted (e.g.,\n    from an index scan on the join key); detecting this requires tracking 'interesting\n    orders' through the plan\n  - Hash join requires the smaller input to fit in memory; for very large joins, partition-based\n    (grace) hash join is needed but is out of scope for this project\n  skills:\n  - Dynamic programming implementation\n  - Combinatorial optimization\n  - Physical operator cost comparison\n  - Memoization for subplan reuse\nname: Query Optimizer\nprerequisites:\n- name: sql-parser\n  type: project\n- name: Database fundamentals (tables, indexes, query execution)\n  type: skill\n- name: Algorithm complexity and dynamic programming\n  type: skill\nresources:\n- name: CMU Database Systems Course (15-445)\"\"\n  type: course\n  url: https://15445.courses.cs.cmu.edu/\n- name: Access Path Selection in a Relational Database Management System (Selinger)\"\"\n  type: paper\n  url: https://courses.cs.duke.edu/compsci516/cps216/spring03/papers/selinger-etal-1979.pdf\n- name: Query Optimization Survey\"\"\n  type: paper\n  url: https://www.vldb.org/pvldb/vol14/p3025-yang.pdf\nskills:\n- Cost-based Optimization\n- Dynamic Programming\n- Cardinality Estimation\n- Histogram Statistics\n- Query Plan Trees\n- Join Algorithms\n- Rule-based Rewriting\n- Performance Profiling\ntags:\n- advanced\n- cost-estimation\n- databases\n- join-ordering\n- optimization\n- statistics\nwhy_important: Building a query optimizer teaches you the algorithmic foundations\n  of modern databases and how to reason about computational complexity in systems\n  with multiple valid execution paths — skills critical for backend engineering and\n  data infrastructure roles.\n"
  }
]
```