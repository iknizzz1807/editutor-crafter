{"html":"<h1 id=\"background-job-processor-design-document\">Background Job Processor: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This document designs a distributed background job processing system similar to Celery/Sidekiq, enabling asynchronous execution of tasks with scheduling, retries, and monitoring. The key architectural challenge is ensuring reliable, ordered task execution across multiple worker processes while maintaining fault tolerance, performance, and observability in a distributed environment.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational understanding for the entire project, establishing the core problems and mental models that all subsequent milestones will address.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<p>At the heart of modern web applications lies a fundamental tension: user-facing requests must complete quickly (typically under 100-500 milliseconds) to provide responsive interfaces, while many necessary business operations are inherently slow or unpredictable. These operations—sending emails, processing uploaded images, generating reports, updating search indexes, synchronizing data with external APIs—can take seconds, minutes, or even hours to complete. Attempting to execute such work synchronously within request-response cycles creates intolerable user delays, timeout errors, and brittle systems that fail under load.</p>\n<p>This section establishes why we need a dedicated background job processing system, explores why naive approaches fail at scale, and examines the architectural patterns that successful systems like Celery and Sidekiq employ. We begin with an intuitive analogy to build a mental model, then formalize the technical problem, and finally survey existing approaches to understand the trade-offs our design must navigate.</p>\n<h3 id=\"mental-model-the-restaurant-kitchen-ticket-system\">Mental Model: The Restaurant Kitchen Ticket System</h3>\n<p>Imagine a busy restaurant during dinner service. Customers (web requests) arrive and place orders (tasks) with the server (web application). Some orders are simple and quick—a glass of water (simple database query). Others are complex and time-consuming—a well-done steak with customized sides, sauce modifications, and allergy considerations (resource-intensive background work).</p>\n<p><strong>Naive Approach (Synchronous Kitchen):</strong> The server takes the order, walks to the kitchen, stands by the grill, prepares the steak themselves, plates it, and then returns to the table 25 minutes later. During this time, the server cannot attend to other customers, the kitchen is cluttered with servers trying to cook, and customers wait endlessly. This is analogous to a web server process blocking on a long-running task within the HTTP request handler—terrible utilization of resources and terrible customer experience.</p>\n<p><strong>Background Job System (Kitchen Ticket System):</strong> Instead, the server writes the order on a ticket (job) and places it in a queue (the ticket rail). The kitchen staff (worker processes) continuously monitor the ticket rail, grab the next ticket (dequeue a job), prepare the order (execute the task), and place the finished dish in the pickup area (store results). Meanwhile, the server is free to attend to other customers immediately after submitting the ticket. The ticket system provides:</p>\n<ul>\n<li><strong>Decoupling:</strong> Servers and cooks work independently at their own paces.</li>\n<li><strong>Prioritization:</strong> Urgent orders (fire tables, VIP customers) can jump the queue.</li>\n<li><strong>Specialization:</strong> Different stations (grill, sauté, pastry) handle specific ticket types.</li>\n<li><strong>Resilience:</strong> If a cook gets sick (worker crash), another can pick up their tickets.</li>\n<li><strong>Visibility:</strong> The chef can monitor ticket volume (dashboard) and reassign staff dynamically.</li>\n</ul>\n<p>This mental model maps directly to our technical system:</p>\n<table>\n<thead>\n<tr>\n<th>Restaurant Concept</th>\n<th>Technical Equivalent</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Customer Order</td>\n<td>Task/Job to be performed</td>\n</tr>\n<tr>\n<td>Server</td>\n<td>Web application/API server</td>\n</tr>\n<tr>\n<td>Ticket</td>\n<td>Job definition (serialized payload)</td>\n</tr>\n<tr>\n<td>Ticket Rail</td>\n<td>Job queue (Redis list/stream)</td>\n</tr>\n<tr>\n<td>Kitchen Staff</td>\n<td>Worker processes</td>\n</tr>\n<tr>\n<td>Cook Stations</td>\n<td>Queue names/priorities</td>\n</tr>\n<tr>\n<td>Head Chef</td>\n<td>Monitoring dashboard</td>\n</tr>\n<tr>\n<td>&quot;86&quot; Board (out of stock)</td>\n<td>Dead letter queue</td>\n</tr>\n</tbody></table>\n<p>The critical insight is that <strong>asynchronous processing via queues decouples the act of requesting work from the act of executing it</strong>, enabling each component to operate at its optimal scale and failure independently.</p>\n<h3 id=\"the-asynchronous-task-execution-problem\">The Asynchronous Task Execution Problem</h3>\n<p>Formally, the asynchronous task execution problem consists of four interconnected challenges that must be solved simultaneously:</p>\n<ol>\n<li><p><strong>Reliable Work Dispatch:</strong> How do producers (web servers) reliably submit work units (jobs) such that no work is lost, even during system failures, while maintaining acceptable latency for the submission operation itself?</p>\n</li>\n<li><p><strong>Scalable Work Consumption:</strong> How do consumers (workers) efficiently retrieve and execute jobs with configurable concurrency, ensuring公平分配 across workers, handling varying workloads, and preventing single workers from becoming bottlenecks?</p>\n</li>\n<li><p><strong>Fault Tolerance and Recovery:</strong> When jobs fail (due to transient errors, resource constraints, or bugs), how does the system automatically retry them with intelligent backoff, and when retries are exhausted, how does it preserve failed jobs for inspection without blocking the queue?</p>\n</li>\n<li><p><strong>Observability and Control:</strong> How do operators monitor queue health, job progress, and system performance in real-time, and how can they intervene (pause, retry, cancel) when necessary?</p>\n</li>\n</ol>\n<p>Naive approaches fail systematically as scale increases:</p>\n<table>\n<thead>\n<tr>\n<th>Naive Approach</th>\n<th>Failure Mode</th>\n<th>Consequence</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>In-process threading</strong></td>\n<td>One failing job crashes the entire worker process. No persistence across restarts.</td>\n<td>Lost jobs, requires manual intervention.</td>\n</tr>\n<tr>\n<td><strong>Database-backed queues</strong> (simple <code>jobs</code> table)</td>\n<td>Polling causes high database load. Transactional locks create contention.</td>\n<td>Database becomes bottleneck, poor performance at scale.</td>\n</tr>\n<tr>\n<td><strong>Direct RPC/HTTP calls to workers</strong></td>\n<td>Tight coupling between producers and consumers. Worker failure causes producer failures.</td>\n<td>Cascading failures, requires complex circuit breakers.</td>\n</tr>\n<tr>\n<td><strong>Cron jobs for recurring tasks</strong></td>\n<td>No retry logic, poor error handling, duplicate execution if jobs run longer than interval.</td>\n<td>Missed jobs, duplicated work, no visibility into failures.</td>\n</tr>\n<tr>\n<td><strong>Local filesystem queues</strong></td>\n<td>Not accessible across multiple servers. Disk corruption loses all jobs.</td>\n<td>Cannot scale horizontally, single point of failure.</td>\n</tr>\n</tbody></table>\n<p>The core technical requirements that emerge are:</p>\n<ul>\n<li><strong>Durability:</strong> Jobs must survive process crashes and system restarts.</li>\n<li><strong>Atomicity:</strong> Job state transitions (enqueued → processing → completed) must occur atomically to prevent double-processing or lost jobs.</li>\n<li><strong>Scalability:</strong> The system must handle increasing job volumes by adding workers without reconfiguration.</li>\n<li><strong>Isolation:</strong> A misbehaving job should not affect other jobs or the system itself.</li>\n<li><strong>Observability:</strong> Every job&#39;s status, timing, and errors must be trackable.</li>\n<li><strong>Configurability:</strong> Retry policies, priorities, and scheduling must be adjustable per job type.</li>\n</ul>\n<p>Our design must address these requirements while remaining simple enough to implement, understand, and operate.</p>\n<h3 id=\"existing-approaches-and-trade-offs\">Existing Approaches and Trade-offs</h3>\n<p>Several established solutions and patterns exist, each with distinct architectural choices and trade-offs. Understanding these helps inform our design decisions.</p>\n<p><strong>1. Dedicated Job Queue Systems (Celery, Sidekiq, RQ)</strong></p>\n<p>These are full-featured frameworks that implement the complete pattern we&#39;re building.</p>\n<table>\n<thead>\n<tr>\n<th>System</th>\n<th>Language</th>\n<th>Broker</th>\n<th>Architecture</th>\n<th>Key Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Celery</strong></td>\n<td>Python</td>\n<td>Redis, RabbitMQ, etc.</td>\n<td>Distributed task queue with worker processes, beat scheduler</td>\n<td>Extremely flexible but complex configuration. &quot;Batteries included&quot; leads to large dependency footprint.</td>\n</tr>\n<tr>\n<td><strong>Sidekiq</strong></td>\n<td>Ruby</td>\n<td>Redis</td>\n<td>Multi-threaded workers within processes, emphasis on simplicity</td>\n<td>Ruby-centric, requires Redis. Excellent performance but less language-agnostic.</td>\n</tr>\n<tr>\n<td><strong>RQ (Redis Queue)</strong></td>\n<td>Python</td>\n<td>Redis</td>\n<td>Simple, minimal design. Single-threaded workers.</td>\n<td>Easy to understand but limited features. No native scheduling, simpler retry logic.</td>\n</tr>\n</tbody></table>\n<p><strong>Common Architectural Patterns:</strong></p>\n<ul>\n<li><strong>Broker-Centric Design:</strong> All components communicate through a central broker (Redis, RabbitMQ). This provides a clear decoupling point but makes the broker a potential single point of failure and performance bottleneck.</li>\n<li><strong>Process-per-Worker vs Thread-per-Worker:</strong> Celery uses prefork process pools for isolation; Sidekiq uses threads within processes for memory efficiency. This is a fundamental trade-off between isolation and resource usage.</li>\n<li><strong>Polling vs Event-Driven Workers:</strong> Most workers poll the broker for jobs (BRPOP, basic.get). Advanced systems use event-driven protocols (AMQP&#39;s basic.consume) for lower latency but increased complexity.</li>\n<li><strong>Embedded vs External Monitoring:</strong> Some systems include web dashboards; others rely on external monitoring (Prometheus, Grafana).</li>\n</ul>\n<p><strong>2. Database as Queue</strong></p>\n<p>Using a relational database table as a job queue is a common alternative, with two primary patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Implementation</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Poll-based</strong></td>\n<td>Workers SELECT FOR UPDATE SKIP LOCKED on a <code>jobs</code> table.</td>\n<td>Leverages existing infrastructure, ACID guarantees.</td>\n<td>High database load, scaling challenges, table bloat.</td>\n</tr>\n<tr>\n<td><strong>Notify-based</strong></td>\n<td>PostgreSQL LISTEN/NOTIFY with SKIP LOCKED.</td>\n<td>Reduced polling overhead.</td>\n<td>Database connection overhead, still consumes DB resources.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> While tempting for simplicity, database-backed queues typically become the primary scalability bottleneck in growing systems. They couple job processing throughput to database capacity, which is expensive and difficult to scale horizontally compared to purpose-built brokers like Redis.</p>\n</blockquote>\n<p><strong>3. Message Brokers (RabbitMQ, Kafka, AWS SQS)</strong></p>\n<p>These are general-purpose message brokers that can be used as job queues.</p>\n<table>\n<thead>\n<tr>\n<th>Broker</th>\n<th>Model</th>\n<th>Job Queue Suitability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>RabbitMQ</strong></td>\n<td>AMQP, exchanges/queues</td>\n<td>Excellent for job queues: persistence, acknowledgments, dead letter exchanges. Requires more operational knowledge.</td>\n</tr>\n<tr>\n<td><strong>Kafka</strong></td>\n<td>Log-based, partitioned streams</td>\n<td>Better for event streaming. Job processing possible but lacks built-in per-message retry. Overkill for simple tasks.</td>\n</tr>\n<tr>\n<td><strong>AWS SQS</strong></td>\n<td>Cloud queue service</td>\n<td>Fully managed, simple API. Limited features (max 15-minute visibility timeout, no native scheduling). Vendor lock-in.</td>\n</tr>\n</tbody></table>\n<p><strong>4. Cloud-Native Services (AWS Lambda, Google Cloud Tasks)</strong></p>\n<p>Serverless functions and managed task services represent a different architectural approach.</p>\n<table>\n<thead>\n<tr>\n<th>Service</th>\n<th>Model</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>AWS Lambda</strong></td>\n<td>Event-driven functions</td>\n<td>No infrastructure to manage, automatic scaling. Cold starts, limited execution time (15 min), vendor lock-in, debugging complexity.</td>\n</tr>\n<tr>\n<td><strong>Google Cloud Tasks</strong></td>\n<td>Managed task queue</td>\n<td>Fully managed, integrates with App Engine/Cloud Run. Vendor-specific, less control over underlying infrastructure.</td>\n</tr>\n</tbody></table>\n<p><strong>Key Architectural Decisions Our Design Must Address:</strong></p>\n<p>Based on these existing approaches, we identify several pivotal decisions:</p>\n<blockquote>\n<p><strong>Decision: Broker Selection</strong></p>\n<ul>\n<li><strong>Context:</strong> We need a durable, high-performance data store for job queues that supports multiple data structures (lists, sorted sets, hashes) and provides atomic operations.</li>\n<li><strong>Options Considered:</strong> <ol>\n<li><strong>Relational Database (PostgreSQL):</strong> Familiar, ACID guarantees, but poor performance under high throughput.</li>\n<li><strong>Redis:</strong> In-memory data structures with persistence options, extremely fast, supports needed operations (LPUSH, BRPOP, ZADD).</li>\n<li><strong>RabbitMQ:</strong> Purpose-built message broker with advanced routing, but additional operational complexity.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use Redis as the primary broker.</li>\n<li><strong>Rationale:</strong> Redis provides the necessary data structures with atomic operations, offers excellent performance, is widely available in cloud environments, and has a simple operational model. The project prerequisites already include Redis basics.</li>\n<li><strong>Consequences:</strong> System performance is tied to Redis availability and memory capacity. Requires Redis persistence configuration for durability.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Worker Concurrency Model</strong></p>\n<ul>\n<li><strong>Context:</strong> Workers must execute multiple jobs concurrently to utilize multi-core systems efficiently while maintaining fault isolation.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Process Pool (prefork):</strong> Each job runs in a separate process. Maximum isolation, but higher memory overhead.</li>\n<li><strong>Thread Pool:</strong> Each job runs in a thread within a single process. Lower memory overhead, but one misbehaving job can affect others.</li>\n<li><strong>Green Threads/Async:</strong> Single-threaded event loop with cooperative multitasking. Efficient for I/O-bound tasks but requires all job code to be async.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use a process pool for maximum isolation.</li>\n<li><strong>Rationale:</strong> Since jobs can be arbitrary Python code (possibly buggy or resource-intensive), process isolation prevents one job from crashing the entire worker. This matches Celery&#39;s approach and provides the cleanest failure boundaries.</li>\n<li><strong>Consequences:</strong> Higher memory usage, inter-process communication overhead for job state updates.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Job Persistence Format</strong></p>\n<ul>\n<li><strong>Context:</strong> Jobs must be serialized for storage in Redis and deserialized by workers.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>JSON:</strong> Human-readable, language-agnostic, but verbose and limited in data types.</li>\n<li><strong>MessagePack:</strong> Binary, compact, preserves some type information, faster than JSON.</li>\n<li><strong>Pickle:</strong> Python-specific, can serialize almost anything, but security risks with untrusted data.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use JSON for primary serialization with type-preserving extensions.</li>\n<li><strong>Rationale:</strong> JSON is universally supported, debuggable (can inspect in Redis CLI), and safe. We&#39;ll add custom encoding for Python-specific types (datetime, Decimal) as needed. This balances simplicity with practicality.</li>\n<li><strong>Consequences:</strong> Slightly larger payload size than binary formats, requires custom encoders/decoders for special types.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Comparison: Serialization Formats</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Format</td>\n</tr>\n<tr>\n<td>JSON</td>\n</tr>\n<tr>\n<td>MessagePack</td>\n</tr>\n<tr>\n<td>Pickle</td>\n</tr>\n</tbody></table>\n<p>These decisions shape the architecture we&#39;ll detail in subsequent sections. The system we&#39;re designing occupies a middle ground: more feature-complete than RQ but simpler than Celery, with explicit trade-offs chosen for educational clarity and practical utility.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<blockquote>\n<p><strong>Note:</strong> This section provides preliminary implementation guidance to set up the foundational concepts. Detailed code for each component will appear in subsequent sections.</p>\n</blockquote>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Broker</strong></td>\n<td>Redis (single instance)</td>\n<td>Redis Cluster for horizontal scaling</td>\n</tr>\n<tr>\n<td><strong>Serialization</strong></td>\n<td>JSON with <code>json</code> module</td>\n<td>MessagePack with <code>msgpack</code> package</td>\n</tr>\n<tr>\n<td><strong>Process Management</strong></td>\n<td><code>multiprocessing.Pool</code></td>\n<td><code>concurrent.futures.ProcessPoolExecutor</code></td>\n</tr>\n<tr>\n<td><strong>HTTP Dashboard</strong></td>\n<td>Flask + Jinja2 templates</td>\n<td>FastAPI + WebSocket for real-time updates</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td><code>unittest</code> with <code>fakeredis</code></td>\n<td><code>pytest</code> with Redis fixtures</td>\n</tr>\n<tr>\n<td><strong>Packaging</strong></td>\n<td>Single module</td>\n<td>Package with <code>setup.py</code>/<code>pyproject.toml</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background-job-processor/\n├── pyproject.toml              # Project dependencies and metadata\n├── README.md\n├── src/\n│   └── jobqueue/              # Main package\n│       ├── __init__.py\n│       ├── exceptions.py      # Custom exceptions (JobError, QueueFullError, etc.)\n│       ├── models.py          # Core data models (Job, Queue, Worker, etc.)\n│       ├── serialization.py   # JSON encoding/decoding with custom extensions\n│       ├── redis_client.py    # Redis connection pooling and wrapper\n│       ├── queue_manager.py   # Milestone 1: Enqueue, priority, queue management\n│       ├── worker.py          # Milestone 2: Worker main loop, job execution\n│       ├── retry_manager.py   # Milestone 3: Exponential backoff, dead letter queue\n│       ├── scheduler.py       # Milestone 4: Delayed jobs, cron scheduling\n│       ├── monitor.py         # Milestone 5: Metrics collection, dashboard backend\n│       └── web/\n│           ├── __init__.py\n│           ├── app.py         # Flask/FastAPI application\n│           ├── templates/     # Jinja2 templates for dashboard\n│           └── static/        # CSS, JavaScript\n├── tests/\n│   ├── __init__.py\n│   ├── test_models.py\n│   ├── test_queue_manager.py\n│   └── ...\n└── scripts/\n    ├── worker-cli.py          # CLI to start workers\n    └── enqueue-job.py         # CLI to enqueue jobs manually</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-redis-client-wrapper\">C. Infrastructure Starter Code: Redis Client Wrapper</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/jobqueue/redis_client.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Redis connection management with connection pooling and error handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Wrapper around Redis client with connection pooling and retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _instance: Optional[</span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _client: Optional[redis.Redis] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379/0\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize Redis connection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            url: Redis connection URL (redis://[:password]@host:port/db)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            **kwargs: Additional arguments passed to redis.Redis()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> RedisClient._instance </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Use RedisClient.get_instance() to get singleton\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_kwargs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> kwargs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._connect()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _connect</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Establish Redis connection with retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis.from_url(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.url,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Automatically decode bytes to str</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                socket_connect_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                socket_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                retry_on_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                **</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_kwargs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test connection</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.ping()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Connected to Redis at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.ConnectionError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to connect to Redis at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_instance</span><span style=\"color:#E1E4E8\">(cls, url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> </span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get singleton RedisClient instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> url </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                url </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379/0\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(url, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_client</span><span style=\"color:#E1E4E8\">(cls) -> redis.Redis:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the underlying Redis client instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            cls</span><span style=\"color:#E1E4E8\">.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            cls</span><span style=\"color:#E1E4E8\">._instance._connect()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance._client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute</span><span style=\"color:#E1E4E8\">(self, command: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a Redis command with error handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            command: Redis command name (e.g., 'lpush', 'zadd')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            *args: Command arguments</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            **kwargs: Additional options</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Command result</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            redis.RedisError: If Redis operation fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            method </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(client, command)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> method(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis command </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">command</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return a Redis pipeline for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client().pipeline()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Convenience function for easy access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_redis</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get the Redis client instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> RedisClient.get_client()</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-job-model\">D. Core Logic Skeleton: Job Model</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/jobqueue/models.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core data models for the job queue system.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, List, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Status of a job through its lifecycle.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACTIVE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retry_scheduled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEAD_LETTER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dead_letter\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Job</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Represents a unit of work to be processed asynchronously.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the core data structure that gets serialized and stored in Redis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        args: List[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kwargs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"default\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        created_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        **</span><span style=\"color:#E1E4E8\">extra_metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize a job.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_type: String identifier for the handler function/class</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            args: Positional arguments for the job handler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            kwargs: Keyword arguments for the job handler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique identifier (auto-generated if None)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue: Target queue name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            priority: Higher priority jobs are processed first</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_retries: Maximum number of retry attempts before giving up</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            timeout_seconds: Maximum execution time in seconds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            created_at: Job creation timestamp (auto-generated if None)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            **extra_metadata: Additional metadata stored with the job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_id </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4())</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_type</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.args </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> args </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.kwargs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> kwargs </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.priority </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> priority</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_retries</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timeout_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> timeout_seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.created_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> created_at </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> datetime.utcnow()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> extra_metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Runtime state (not serialized)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.result: Optional[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Convert job to dictionary for serialization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary representation suitable for JSON serialization</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'job_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.job_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'job_type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.job_type,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'args'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.args,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'kwargs'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.kwargs,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'queue'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.queue,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'priority'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.priority,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'max_retries'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_retries,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timeout_seconds'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.timeout_seconds,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'created_at'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.created_at.isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.created_at </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'metadata'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.metadata,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'attempts'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.attempts,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'errors'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.errors,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'status'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.status.value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Only include runtime fields if they exist</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.started_at:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data[</span><span style=\"color:#9ECBFF\">'started_at'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.started_at.isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.completed_at:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data[</span><span style=\"color:#9ECBFF\">'completed_at'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.completed_at.isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.result </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data[</span><span style=\"color:#9ECBFF\">'result'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Reconstruct a job from dictionary representation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: Dictionary from Redis storage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Reconstructed Job instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract core fields from data dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse datetime strings back to datetime objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Reconstruct job with all fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set runtime state fields (status, attempts, errors, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the reconstructed job instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use datetime.fromisoformat() for parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> serialize</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Serialize job to JSON string for storage in Redis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            JSON string representation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert job to dictionary using to_dict()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Serialize dictionary to JSON string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate that serialized size is under 1MB limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If over limit, raise PayloadTooLargeError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the JSON string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> deserialize</span><span style=\"color:#E1E4E8\">(cls, data: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Deserialize job from JSON string.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: JSON string from Redis</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Deserialized Job instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse JSON string to dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Reconstruct job using from_dict()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return the job instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_error</span><span style=\"color:#E1E4E8\">(self, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Record an error that occurred during job execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            error: Exception that was raised</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'type'</span><span style=\"color:#E1E4E8\">: error.</span><span style=\"color:#79B8FF\">__class__</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'message'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(error),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'attempt'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.attempts,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Optionally add stack trace in debug mode</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors.append(error_info)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_retry</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Determine if job should be retried based on attempts and max_retries.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if job should be retried, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if attempts &#x3C; max_retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return True if should retry, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-python\">E. Language-Specific Hints: Python</h4>\n<ul>\n<li><strong>Use <code>datetime.utcnow()</code></strong> instead of <code>datetime.now()</code> for timestamps to avoid timezone issues across servers.</li>\n<li><strong>JSON serialization of datetime:</strong> Use ISO format strings (<code>datetime.isoformat()</code>), which are both human-readable and parseable.</li>\n<li><strong>Redis connections:</strong> Always use <code>decode_responses=True</code> when creating Redis client to avoid byte-string handling.</li>\n<li><strong>Connection pooling:</strong> Let <code>redis.Redis</code> handle connection pooling internally; create one client instance and reuse it.</li>\n<li><strong>Process vs Thread:</strong> Use <code>multiprocessing</code> for worker pools to isolate job crashes, but be aware that objects passed to worker processes must be picklable.</li>\n<li><strong>Graceful shutdown:</strong> Catch <code>KeyboardInterrupt</code> and <code>SystemExit</code> in worker main loop, and use <code>signal.signal()</code> to handle SIGTERM.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint-after-section-completion\">F. Milestone Checkpoint: After Section Completion</h4>\n<p>After understanding this section (but before implementing Milestone 1), you should be able to:</p>\n<ol>\n<li><strong>Explain in your own words</strong> why background job processing is necessary and the failures of naive approaches.</li>\n<li><strong>Draw the restaurant ticket system analogy</strong> and map each component to the technical system.</li>\n<li><strong>Articulate the trade-offs</strong> between Redis, database-backed, and message broker queues.</li>\n<li><strong>Run the Redis starter code</strong> successfully:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start Redis (if not already running)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> 6379:6379</span><span style=\"color:#9ECBFF\"> redis:7-alpine</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test the Redis connection</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"from src.jobqueue.redis_client import RedisClient; r = RedisClient.get_instance(); print(r.execute('ping'))\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should print: True</span></span></code></pre></div>\n\n<ol start=\"5\">\n<li><strong>Create a Job instance</strong> and inspect its dictionary representation:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.jobqueue.models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"send_email\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"user@example.com\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Welcome!\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    kwargs</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"template\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"welcome\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    queue</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"emails\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    max_retries</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(job.job_id)  </span><span style=\"color:#6A737D\"># Should show a UUID</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(job.to_dict())  </span><span style=\"color:#6A737D\"># Should show dictionary with all fields</span></span></code></pre></div>\n\n<p><strong>Signs something is wrong:</strong></p>\n<ul>\n<li>Redis connection fails: Check if Redis is running and the URL is correct.</li>\n<li>Job serialization fails: Ensure all data types are JSON-serializable (no datetime objects in args/kwargs directly).</li>\n<li>Import errors: Verify your Python path includes the <code>src</code> directory or install the package in development mode with <code>pip install -e .</code>.</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;ModuleNotFoundError: No module named &#39;jobqueue&#39;&quot;</td>\n<td>Python cannot find the package</td>\n<td>Run <code>python -c &quot;import sys; print(sys.path)&quot;</code> to see Python path</td>\n<td>Install with <code>pip install -e .</code> or set <code>PYTHONPATH</code></td>\n</tr>\n<tr>\n<td>Redis connection timeout</td>\n<td>Redis not running or wrong port</td>\n<td>Run <code>redis-cli ping</code> from terminal</td>\n<td>Start Redis: <code>docker run -d -p 6379:6379 redis</code></td>\n</tr>\n<tr>\n<td>&quot;datetime is not JSON serializable&quot;</td>\n<td>Direct datetime objects in job data</td>\n<td>Check job args/kwargs for datetime objects</td>\n<td>Use ISO format strings or extend JSON encoder</td>\n</tr>\n<tr>\n<td>Job ID collisions</td>\n<td>Using simple incrementing IDs</td>\n<td>Check ID generation logic</td>\n<td>Use UUIDs (uuid4) or ULIDs for distributed uniqueness</td>\n</tr>\n<tr>\n<td>Cannot pickle local object</td>\n<td>Passing unpicklable objects to multiprocessing</td>\n<td>Check what&#39;s in job args/kwargs</td>\n<td>Ensure all arguments are picklable (no lambdas, local functions)</td>\n</tr>\n</tbody></table>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the fundamental requirements and boundaries that apply across all milestones, providing the foundation for architectural decisions made throughout the entire system design. These goals guide implementation priorities and trade-offs from Milestone 1 through Milestone 5.</p>\n</blockquote>\n<h3 id=\"functional-requirements-goals\">Functional Requirements (Goals)</h3>\n<p>Think of our job processor as an <strong>industrial factory</strong> that must reliably transform raw materials (job requests) into finished products (completed work). The factory must operate 24/7, handle different product lines with varying priorities, recover from equipment failures, and provide supervisors with real-time visibility into operations. These functional requirements represent the core capabilities our factory must deliver to be commercially viable.</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Requirement</th>\n<th>Description</th>\n<th>Implementation Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Job Management</strong></td>\n<td>Enqueue jobs with metadata</td>\n<td>Jobs must be enqueued with JSON-serialized payloads containing job type, arguments, priority, retry limits, and custom metadata. Each job receives a unique ID (UUID/ULID) for tracking.</td>\n<td>Foundation for all job operations; requires robust serialization and validation (Milestone 1).</td>\n</tr>\n<tr>\n<td></td>\n<td>Multiple priority queues</td>\n<td>Support at least 3-5 named queues with configurable priority weights. Higher-priority queues are polled more frequently by workers while maintaining fairness.</td>\n<td>Impacts worker polling algorithm and requires weighted queue selection logic (Milestone 1).</td>\n</tr>\n<tr>\n<td></td>\n<td>Queue inspection</td>\n<td>APIs to check queue length, peek at pending jobs, list all queues, and retrieve job status by ID. Essential for operational visibility.</td>\n<td>Adds monitoring endpoints and Redis query patterns (Milestones 1, 5).</td>\n</tr>\n<tr>\n<td><strong>Processing</strong></td>\n<td>FIFO job execution</td>\n<td>Within the same priority level, jobs execute in first-in-first-out order. Priority weights determine which queue&#39;s jobs are processed next.</td>\n<td>Guides Redis data structure selection (list vs. sorted set) and dequeue algorithms (Milestone 1).</td>\n</tr>\n<tr>\n<td></td>\n<td>Concurrent execution</td>\n<td>Single worker process must handle multiple jobs concurrently using thread/process pools. Configurable concurrency level (default: CPU count).</td>\n<td>Affects worker architecture, resource isolation, and coordination (Milestone 2).</td>\n</tr>\n<tr>\n<td></td>\n<td>Graceful shutdown</td>\n<td>Workers must complete current jobs before exiting when receiving SIGTERM/SIGINT. No partial state or orphaned jobs after shutdown.</td>\n<td>Requires signal handlers and job state persistence (Milestone 2).</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Automatic retries with backoff</td>\n<td>Failed jobs automatically retry with exponential backoff (1s, 2s, 4s... up to max retries). Configurable jitter prevents thundering herd.</td>\n<td>Requires retry scheduler, delayed execution mechanism, and attempt tracking (Milestone 3).</td>\n</tr>\n<tr>\n<td></td>\n<td>Dead letter queue</td>\n<td>Jobs exhausting max retries move to a dedicated dead letter queue with full error history. Can be manually retried or inspected.</td>\n<td>Creates new storage location and requires error serialization (Milestone 3).</td>\n</tr>\n<tr>\n<td></td>\n<td>Worker heartbeat &amp; failure detection</td>\n<td>Workers report liveness at regular intervals (e.g., every 30s). Stale workers (no heartbeat for 2x interval) are detected and their jobs returned to queue.</td>\n<td>Requires periodic background tasks and stale worker cleanup (Milestone 2).</td>\n</tr>\n<tr>\n<td><strong>Scheduling</strong></td>\n<td>Delayed execution</td>\n<td>Jobs can be scheduled for future execution at specific timestamps. Must remain invisible to workers until scheduled time arrives.</td>\n<td>Requires time-based data structure (Redis sorted set) and scheduler process (Milestone 4).</td>\n</tr>\n<tr>\n<td></td>\n<td>Recurring jobs (cron)</td>\n<td>Define jobs that run on schedule using standard cron syntax (5-field: minute, hour, day, month, weekday). Unique constraints prevent duplicate execution.</td>\n<td>Requires cron parser, schedule storage, and catch-up logic for missed runs (Milestone 4).</td>\n</tr>\n<tr>\n<td><strong>Monitoring</strong></td>\n<td>Real-time dashboard</td>\n<td>Web dashboard showing queue depths, active workers, processing rates, error rates, and recent job history. Updates without page refresh.</td>\n<td>Requires WebSocket/SSE, metrics aggregation, and UI components (Milestone 5).</td>\n</tr>\n<tr>\n<td></td>\n<td>Job search &amp; filtering</td>\n<td>Search jobs by ID, status, queue, time range, or job type. Paginated results for large datasets.</td>\n<td>Creates query interface over job history data (Milestone 5).</td>\n</tr>\n<tr>\n<td></td>\n<td>Alerting</td>\n<td>Configurable alerts when queue backlog exceeds threshold or error rate spikes above limit. Notify via webhook or email.</td>\n<td>Requires background monitoring and notification system (Milestone 5).</td>\n</tr>\n<tr>\n<td><strong>Operational</strong></td>\n<td>Payload validation</td>\n<td>Reject jobs with payloads &gt;1MB at enqueue time. Validate JSON structure and required fields.</td>\n<td>Prevents memory issues and malformed job processing (Milestone 1).</td>\n</tr>\n<tr>\n<td></td>\n<td>Job timeout enforcement</td>\n<td>Kill jobs exceeding their configured timeout (default: 30 minutes). Prevent worker starvation by long-running jobs.</td>\n<td>Requires subprocess management and timeout monitoring (Milestone 2).</td>\n</tr>\n<tr>\n<td></td>\n<td>Cross-language compatibility</td>\n<td>Job payloads serializable/deserializable across Python, Go, and Rust clients. Use standard formats (JSON/msgpack).</td>\n<td>Influences serialization format choice and type system design (Milestone 1).</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight:</strong> The progression from Milestone 1 to 5 represents a journey from core job mechanics (enqueue/dequeue) through reliability features (retries), time-based operations (scheduling), and finally operational excellence (monitoring). Each milestone builds on the previous one—you cannot implement retries without a working worker system, nor scheduling without reliable queue infrastructure.</p>\n</blockquote>\n<h4 id=\"job-lifecycle-requirements\">Job Lifecycle Requirements</h4>\n<p>The system must support the complete job lifecycle shown in the state machine diagram:</p>\n<p><img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fjob-state-machine.svg\" alt=\"Job State Machine\"></p>\n<ol>\n<li><strong>Creation → Pending:</strong> Jobs enter the system via <code>enqueue()</code> and await worker pickup.</li>\n<li><strong>Pending → Active:</strong> Worker claims job via atomic dequeue operation.</li>\n<li><strong>Active → Completed:</strong> Job executes successfully, result stored.</li>\n<li><strong>Active → Failed:</strong> Job execution throws exception, error recorded.</li>\n<li><strong>Failed → Retry Scheduled:</strong> If retries remain, job scheduled for future retry with backoff.</li>\n<li><strong>Retry Scheduled → Pending:</strong> Scheduler moves job back to queue after backoff delay.</li>\n<li><strong>Failed → Dead Letter:</strong> Max retries exhausted, job moved to dead letter queue.</li>\n<li><strong>Dead Letter → Pending (manual):</strong> Admin manually retries job from dashboard.</li>\n</ol>\n<p>Each transition must be atomic and leave the system in a consistent state, even during failures.</p>\n<h3 id=\"non-functional-requirements\">Non-Functional Requirements</h3>\n<p>Consider these as the <strong>factory&#39;s operating standards</strong>—not what it produces, but how well it operates. A factory might produce excellent widgets but if it consumes excessive power, breaks down frequently, or requires specialized operators, it&#39;s not commercially viable. These requirements ensure our job processor operates efficiently, reliably, and maintainably in production environments.</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Requirement</th>\n<th>Metric</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Performance</strong></td>\n<td>Enqueue latency</td>\n<td>≤10ms P99 for single job enqueue</td>\n<td>Producers shouldn&#39;t block waiting for job acceptance.</td>\n</tr>\n<tr>\n<td></td>\n<td>Dequeue latency</td>\n<td>≤50ms P99 for worker to get next job</td>\n<td>Workers should spend minimal time waiting for work.</td>\n</tr>\n<tr>\n<td></td>\n<td>Throughput</td>\n<td>≥1000 jobs/sec per worker process</td>\n<td>Handle moderate workloads on modest hardware.</td>\n</tr>\n<tr>\n<td></td>\n<td>Memory efficiency</td>\n<td>≤50MB baseline per worker process</td>\n<td>Enable running multiple workers on same machine.</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Durability</td>\n<td>Job persistence ≥99.9% after successful enqueue</td>\n<td>Jobs shouldn&#39;t disappear once accepted.</td>\n</tr>\n<tr>\n<td></td>\n<td>Availability</td>\n<td>System operational ≥99.5% of time</td>\n<td>Allow for maintenance and occasional failures.</td>\n</tr>\n<tr>\n<td></td>\n<td>Data consistency</td>\n<td>No job loss during graceful shutdown</td>\n<td>Critical for user trust in the system.</td>\n</tr>\n<tr>\n<td></td>\n<td>Failure recovery</td>\n<td>≤5 minutes to recover from Redis restart</td>\n<td>Quick recovery from infrastructure issues.</td>\n</tr>\n<tr>\n<td><strong>Scalability</strong></td>\n<td>Horizontal scaling</td>\n<td>Linear throughput increase with added workers</td>\n<td>Support growing workloads by adding resources.</td>\n</tr>\n<tr>\n<td></td>\n<td>Queue isolation</td>\n<td>One misbehaving queue doesn&#39;t block others</td>\n<td>Fault isolation between different job types.</td>\n</tr>\n<tr>\n<td></td>\n<td>Redis connection efficiency</td>\n<td>≤100 connections per worker pool</td>\n<td>Prevent overwhelming Redis with connections.</td>\n</tr>\n<tr>\n<td><strong>Operational</strong></td>\n<td>Monitoring granularity</td>\n<td>Metrics updated every 30 seconds</td>\n<td>Balance real-time visibility with system load.</td>\n</tr>\n<tr>\n<td></td>\n<td>Dashboard responsiveness</td>\n<td>≤2 second page load for 10k job history</td>\n<td>Operators need quick access to system state.</td>\n</tr>\n<tr>\n<td></td>\n<td>Configuration simplicity</td>\n<td>≤5 required configuration parameters</td>\n<td>Reduce deployment friction and errors.</td>\n</tr>\n<tr>\n<td></td>\n<td>Documentation completeness</td>\n<td>All public APIs documented with examples</td>\n<td>Enable adoption and reduce support burden.</td>\n</tr>\n<tr>\n<td><strong>Security</strong></td>\n<td>Input validation</td>\n<td>All user input validated before processing</td>\n<td>Prevent injection attacks and malformed data.</td>\n</tr>\n<tr>\n<td></td>\n<td>Dashboard authentication</td>\n<td>Optional basic auth for web dashboard</td>\n<td>Protect operational interface in production.</td>\n</tr>\n<tr>\n<td></td>\n<td>Job payload encryption</td>\n<td>Support for encrypted payloads (optional)</td>\n<td>Handle sensitive job data requirements.</td>\n</tr>\n<tr>\n<td></td>\n<td>Redis security</td>\n<td>Support Redis ACL and TLS connections</td>\n<td>Enterprise deployment requirements.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Principle:</strong> These non-functional requirements create concrete constraints for architectural decisions. For example, the ≤10ms enqueue latency requirement rules out complex validation that requires external service calls, while the ≥99.9% durability requirement necessitates synchronous Redis writes (or WAL) for critical operations.</p>\n</blockquote>\n<h4 id=\"performance-trade-off-decisions\">Performance Trade-off Decisions</h4>\n<p>The system makes deliberate trade-offs between competing non-functional requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Trade-off</th>\n<th>Choice</th>\n<th>Rationale</th>\n<th>Consequence</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Latency vs. Durability</td>\n<td><strong>Durability with fsync every 1s</strong></td>\n<td>Job loss is worse than slightly higher latency for background jobs.</td>\n<td>Enqueue operations may block during Redis persistence.</td>\n</tr>\n<tr>\n<td>Memory vs. Functionality</td>\n<td><strong>Keep 10k most recent job history</strong></td>\n<td>Most debugging uses recent jobs; older history can be archived.</td>\n<td>Dashboard job search limited to recent history without external storage.</td>\n</tr>\n<tr>\n<td>Real-time vs. Load</td>\n<td><strong>30-second metric aggregation</strong></td>\n<td>Real-time updates aren&#39;t critical for background job monitoring.</td>\n<td>Dashboard shows near-real-time but not exact current state.</td>\n</tr>\n<tr>\n<td>Simplicity vs. Features</td>\n<td><strong>Polling scheduler vs. event-based</strong></td>\n<td>Simpler to implement and debug; acceptable for scheduled jobs.</td>\n<td>Scheduler consumes CPU even when no jobs are due.</td>\n</tr>\n</tbody></table>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>Just as important as what we build is what we <strong>deliberately choose not to build</strong>. A factory producing automobiles shouldn&#39;t also try to refine gasoline—it should focus on its core competency and integrate with specialists. These non-goals prevent scope creep, keep the implementation focused, and acknowledge that some problems are better solved by other systems or future extensions.</p>\n<table>\n<thead>\n<tr>\n<th>Non-Goal</th>\n<th>Explanation</th>\n<th>Alternative/Workaround</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Job dependency graphs</strong></td>\n<td>No support for complex workflows where job B depends on job A&#39;s completion.</td>\n<td>Chain jobs by having job A enqueue job B upon completion, or use external workflow engine.</td>\n</tr>\n<tr>\n<td><strong>Exactly-once execution</strong></td>\n<td>Cannot guarantee jobs execute exactly once; at-least-once is the best guarantee.</td>\n<td>Design jobs to be idempotent; include idempotency keys in job metadata for deduplication.</td>\n</tr>\n<tr>\n<td><strong>Job migration during upgrades</strong></td>\n<td>No automatic migration of in-flight jobs during code deployments.</td>\n<td>Drain queues before deployment or use versioned job handlers that can process multiple formats.</td>\n</tr>\n<tr>\n<td><strong>Multi-broker support</strong></td>\n<td>Only Redis is supported as the broker; no pluggable broker architecture.</td>\n<td>Use Redis Sentinel/Cluster for HA; future extension could abstract broker interface.</td>\n</tr>\n<tr>\n<td><strong>Built-in rate limiting</strong></td>\n<td>No per-user or per-job-type rate limiting in the core.</td>\n<td>Implement in job handlers or use Redis-backed rate limiter before enqueueing jobs.</td>\n</tr>\n<tr>\n<td><strong>Job prioritization within queue</strong></td>\n<td>All jobs in same queue are FIFO; no sub-prioritization within queue.</td>\n<td>Use multiple queues with different priorities or add timestamp-based ordering within queue.</td>\n</tr>\n<tr>\n<td><strong>Long-term job archival</strong></td>\n<td>Job history older than configured retention is deleted, not archived.</td>\n<td>External system can consume job completion events and archive to cold storage.</td>\n</tr>\n<tr>\n<td><strong>Built-in user management</strong></td>\n<td>Dashboard has simple auth or none; no RBAC or multi-user permission system.</td>\n<td>Place dashboard behind reverse proxy with authentication for production use.</td>\n</tr>\n<tr>\n<td><strong>Geographic job routing</strong></td>\n<td>Jobs cannot be routed to workers in specific regions/datacenters.</td>\n<td>Use separate Redis instances per region or implement routing in producer logic.</td>\n</tr>\n<tr>\n<td><strong>Real-time job streaming</strong></td>\n<td>Cannot subscribe to job events via streaming APIs (though events are emitted).</td>\n<td>Use Redis pub/sub or add webhook callbacks as extension.</td>\n</tr>\n<tr>\n<td><strong>Job pausing/resuming</strong></td>\n<td>Cannot pause a specific job&#39;s execution once started.</td>\n<td>Implement checkpointing within job handler code for long-running jobs.</td>\n</tr>\n<tr>\n<td><strong>Automatic scaling of workers</strong></td>\n<td>No auto-scaling based on queue depth; manual worker management required.</td>\n<td>Use container orchestration (K8s HPA) that monitors queue metrics to scale workers.</td>\n</tr>\n<tr>\n<td><strong>Built-in job versioning</strong></td>\n<td>No automatic handling of breaking changes to job payload formats.</td>\n<td>Use version field in job metadata and handler version compatibility logic.</td>\n</tr>\n<tr>\n<td><strong>Transactional enqueue</strong></td>\n<td>Cannot atomically enqueue multiple jobs across queues.</td>\n<td>Use Redis pipelines for best-effort atomicity or implement compensating transactions.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architectural Philosophy:</strong> By explicitly stating these non-goals, we acknowledge common requests but defer them to keep the initial implementation manageable. The architecture should be extensible enough to add these features later without major redesign. For example, while we don&#39;t build job dependency graphs, the system&#39;s webhook events could feed into an external workflow orchestrator.</p>\n</blockquote>\n<h4 id=\"boundary-decisions-with-rationale\">Boundary Decisions with Rationale</h4>\n<p>For each major non-goal, we made explicit architectural decisions:</p>\n<p><strong>Decision: No Built-in Job Dependencies</strong></p>\n<ul>\n<li><strong>Context:</strong> Many background job systems need to chain jobs (A → B → C) or create complex workflows.</li>\n<li><strong>Options Considered:</strong> <ol>\n<li><strong>Built-in DAG engine:</strong> Complex, requires persistent graph storage and complex scheduling.</li>\n<li><strong>Simple chaining:</strong> Job A enqueues job B on completion—simple but creates tight coupling.</li>\n<li><strong>External workflow system:</strong> Keep job processor simple, integrate with specialized workflow engines.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Option 3—no built-in dependencies, but emit completion events for external integration.</li>\n<li><strong>Rationale:</strong> Job dependencies belong in a workflow layer, not the core job execution engine. Most users can implement simple chaining in job handlers, while complex workflows need specialized tools (Airflow, Temporal) that our system can feed into.</li>\n<li><strong>Consequences:</strong> Users must implement chaining manually or use external system; system remains simple and focused.</li>\n</ul>\n<p><strong>Decision: At-Least-Once Semantics Only</strong></p>\n<ul>\n<li><strong>Context:</strong> Exactly-once execution requires distributed transactions and persistent deduplication.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Exactly-once with idempotency keys:</strong> Complex, requires persistent store of processed IDs.</li>\n<li><strong>At-least-once with idempotent jobs:</strong> Simpler, pushes idempotency responsibility to job handlers.</li>\n<li><strong>At-most-once:</strong> Unacceptable for reliable job processing.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Option 2—guarantee jobs execute at least once, possibly multiple times during failures.</li>\n<li><strong>Rationale:</strong> Exactly-once semantics are extremely difficult in distributed systems and often require application-level idempotency anyway. By embracing at-least-once, we simplify the core system dramatically while providing the tools (idempotency keys, job metadata) for handlers to implement idempotency.</li>\n<li><strong>Consequences:</strong> Job handlers must be idempotent; system cannot guarantee no duplicates during network partitions.</li>\n</ul>\n<p><strong>Decision: Single Broker (Redis) Architecture</strong></p>\n<ul>\n<li><strong>Context:</strong> Production systems might want to use different brokers (RabbitMQ, Kafka, SQS) for different needs.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Pluggable broker interface:</strong> Abstract broker operations behind interface, support multiple backends.</li>\n<li><strong>Redis-only with extension points:</strong> Optimize for Redis, allow extensions for other brokers.</li>\n<li><strong>Multi-broker from day one:</strong> Support 2-3 popular brokers in initial implementation.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Option 2—Redis-only core with clear abstraction boundaries for future broker support.</li>\n<li><strong>Rationale:</strong> Redis provides all needed functionality with excellent performance and reliability. Supporting multiple brokers initially would triple implementation complexity. By keeping clean internal interfaces, we can add other brokers later without breaking existing Redis users.</li>\n<li><strong>Consequences:</strong> Users locked into Redis; migration to other brokers requires code changes later.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<blockquote>\n<p><strong>Implementation Note:</strong> While this section doesn&#39;t directly translate to code, the goals and non-goals defined here should guide every implementation decision. Before writing any code, ensure your design addresses each functional requirement while respecting the non-goals.</p>\n</blockquote>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Beginner-Friendly)</th>\n<th>Advanced Option (Production-Ready)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Serialization</strong></td>\n<td>JSON with Python&#39;s <code>json</code> module</td>\n<td>MessagePack (<code>msgpack</code>) for better performance and binary data</td>\n</tr>\n<tr>\n<td><strong>Redis Client</strong></td>\n<td><code>redis-py</code> with connection pooling</td>\n<td><code>redis-py</code> with client-side sharding for cluster support</td>\n</tr>\n<tr>\n<td><strong>Concurrency</strong></td>\n<td><code>concurrent.futures.ThreadPoolExecutor</code></td>\n<td><code>multiprocessing.Pool</code> with process isolation for CPU-bound jobs</td>\n</tr>\n<tr>\n<td><strong>Web Dashboard</strong></td>\n<td>Flask + Jinja2 templates with auto-refresh</td>\n<td>FastAPI + WebSocket for real-time updates + React frontend</td>\n</tr>\n<tr>\n<td><strong>Cron Parsing</strong></td>\n<td><code>croniter</code> library for schedule calculation</td>\n<td>Custom parser supporting extended cron syntax with timezones</td>\n</tr>\n<tr>\n<td><strong>Metrics</strong></td>\n<td>Custom Redis counters updated after job completion</td>\n<td>Prometheus client library + Grafana for advanced visualization</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-configuration-structure\">Recommended Configuration Structure</h4>\n<p>Create a configuration system that balances simplicity (≤5 required parameters) with flexibility:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># config.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Higher = more frequent polling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Optional queue size limit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WorkerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queues: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Queues this worker polls</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    concurrency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">  # Number of concurrent job executions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    heartbeat_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#6A737D\">  # Seconds between heartbeats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1800</span><span style=\"color:#6A737D\">  # Default job timeout in seconds (30 min)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379/0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_connections: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    socket_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_timeout: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SystemConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis: RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queues: List[QueueConfig]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    workers: List[WorkerConfig]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Non-functional defaults</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_payload_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\">  # 1MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_history_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#6A737D\">  # Keep last 10k jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_base_delay: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Base delay in seconds for exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">  # Default max retries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_env</span><span style=\"color:#E1E4E8\">(cls) -> </span><span style=\"color:#9ECBFF\">\"SystemConfig\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from environment variables with defaults.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            redis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RedisConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">\"REDIS_URL\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"redis://localhost:6379/0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            queues</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"high\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"low\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                WorkerConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    queues</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"high\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"low\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    concurrency</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">\"WORKER_CONCURRENCY\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"4\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<h4 id=\"milestone-goal-mapping\">Milestone Goal Mapping</h4>\n<p>As you implement each milestone, verify your work against these specific goal checkpoints:</p>\n<table>\n<thead>\n<tr>\n<th>Milestone</th>\n<th>Must-Have Goal</th>\n<th>How to Verify</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1</strong></td>\n<td>Jobs enqueue and dequeue in FIFO order</td>\n<td>Enqueue 10 jobs, dequeue 10 jobs - verify same order.</td>\n</tr>\n<tr>\n<td></td>\n<td>Payloads &gt;1MB rejected</td>\n<td>Try enqueuing 2MB payload - get validation error.</td>\n</tr>\n<tr>\n<td><strong>2</strong></td>\n<td>Worker processes jobs concurrently</td>\n<td>Enqueue 5 jobs that each sleep 1s - all complete in ~1s with concurrency=5.</td>\n</tr>\n<tr>\n<td></td>\n<td>Graceful shutdown on SIGTERM</td>\n<td>Send SIGTERM to worker during job execution - job completes before exit.</td>\n</tr>\n<tr>\n<td><strong>3</strong></td>\n<td>Exponential backoff with jitter</td>\n<td>Fail job 3 times - verify retry delays are ~1s, ~2s, ~4s ± jitter.</td>\n</tr>\n<tr>\n<td></td>\n<td>Dead letter queue after max retries</td>\n<td>Fail job beyond max retries - find it in dead letter queue.</td>\n</tr>\n<tr>\n<td><strong>4</strong></td>\n<td>Delayed job execution</td>\n<td>Schedule job for 5s future - verify it executes ~5s after enqueue.</td>\n</tr>\n<tr>\n<td></td>\n<td>Cron job re-enqueues</td>\n<td>Create cron job for every minute - see it enqueue automatically each minute.</td>\n</tr>\n<tr>\n<td><strong>5</strong></td>\n<td>Real-time dashboard updates</td>\n<td>Open dashboard, enqueue job - see queue depth update without refresh.</td>\n</tr>\n<tr>\n<td></td>\n<td>Job search by criteria</td>\n<td>Enqueue jobs with different metadata - search finds them by attributes.</td>\n</tr>\n</tbody></table>\n<h4 id=\"common-implementation-pitfalls-to-avoid\">Common Implementation Pitfalls to Avoid</h4>\n<p>While implementing to meet these goals, watch for these specific pitfalls:</p>\n<p>⚠️ <strong>Pitfall: Over-engineering for edge cases</strong></p>\n<ul>\n<li><strong>Description:</strong> Trying to handle every possible failure mode or feature request from the start.</li>\n<li><strong>Why it&#39;s wrong:</strong> Violates YAGNI principle, dramatically increases complexity, delays delivery of core functionality.</li>\n<li><strong>Fix:</strong> Implement only the requirements from the table above. Add extensibility points for future enhancements but don&#39;t implement them now.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring non-functional requirements</strong></p>\n<ul>\n<li><strong>Description:</strong> Building functionally correct system that&#39;s too slow, uses too much memory, or can&#39;t scale.</li>\n<li><strong>Why it&#39;s wrong:</strong> System may work in development but fail in production under load.</li>\n<li><strong>Fix:</strong> Measure performance early—profile enqueue/dequeue latency, memory usage of workers, Redis connection counts.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Violating explicit non-goals</strong></p>\n<ul>\n<li><strong>Description:</strong> Adding &quot;just one small feature&quot; that&#39;s explicitly listed as a non-goal.</li>\n<li><strong>Why it&#39;s wrong:</strong> Creates scope creep, introduces complexity for features users shouldn&#39;t depend on.</li>\n<li><strong>Fix:</strong> When tempted to add a feature, check if it&#39;s in the non-goals table. If yes, document it as a future extension but don&#39;t implement now.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Hardcoding instead of configuring</strong></p>\n<ul>\n<li><strong>Description:</strong> Embedding queue names, timeouts, retry counts in code instead of configuration.</li>\n<li><strong>Why it&#39;s wrong:</strong> Requires code changes for operational adjustments, limits deployment flexibility.</li>\n<li><strong>Fix:</strong> Use the configuration pattern shown above for all tunable parameters.</li>\n</ul>\n<hr>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides the architectural foundation for all five milestones, defining the core components, their responsibilities, and how they interact. It establishes the blueprint that each subsequent milestone&#39;s detailed component design will follow and extend.</p>\n</blockquote>\n<p>This section presents the <strong>bird&#39;s-eye view</strong> of the Background Job Processor system. Before diving into individual components like queues or workers, it&#39;s essential to understand the complete picture—how all parts fit together into a cohesive whole. The architecture is designed around a central <strong>broker</strong> (Redis) that acts as the system&#39;s nervous system, connecting all components while maintaining loose coupling and enabling horizontal scaling. We&#39;ll explore this through three lenses: the static view of components and their responsibilities, the dynamic view of how they communicate, and the structural view of how code should be organized.</p>\n<h3 id=\"system-components-and-responsibilities\">System Components and Responsibilities</h3>\n<p>Think of the system as a <strong>distributed factory</strong> with specialized departments. Each department has a specific role, operates independently, but communicates through a central dispatch office (Redis). This separation of concerns allows each component to be developed, deployed, and scaled independently.</p>\n<p><img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fsystem-component.svg\" alt=\"System Component Overview\"></p>\n<p>The system comprises seven core components, each with distinct responsibilities:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Key Data It Owns/Maintains</th>\n<th>Interaction Partners</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Producer</strong></td>\n<td>Creates and submits jobs for asynchronous execution</td>\n<td>Local job definitions and arguments (not persisted centrally)</td>\n<td>Queue Manager (via client library)</td>\n</tr>\n<tr>\n<td><strong>Queue Manager</strong></td>\n<td>Validates, prioritizes, and stores incoming jobs into appropriate queues</td>\n<td>Queue configurations, job serialization/deserialization logic</td>\n<td>Producer, Redis (broker), Worker</td>\n</tr>\n<tr>\n<td><strong>Worker</strong></td>\n<td>Fetches jobs from queues and executes the corresponding task logic</td>\n<td>Worker process state, job execution environment, thread/process pool</td>\n<td>Queue Manager (via Redis), Job Handlers, Monitoring</td>\n</tr>\n<tr>\n<td><strong>Scheduler</strong></td>\n<td>Manages delayed and recurring jobs based on cron expressions</td>\n<td>Schedule definitions, next execution timestamps, uniqueness constraints</td>\n<td>Redis (for scheduled sets), Queue Manager</td>\n</tr>\n<tr>\n<td><strong>Retry Manager</strong></td>\n<td>Handles failed jobs with exponential backoff and manages dead letter queue</td>\n<td>Retry policies, backoff calculations, error history</td>\n<td>Worker (on failure), Redis (for retry sets)</td>\n</tr>\n<tr>\n<td><strong>Monitor/Dashboard</strong></td>\n<td>Provides real-time visibility into system health and job status</td>\n<td>Aggregated metrics, job history, alert configurations</td>\n<td>All components (via Redis and direct reporting)</td>\n</tr>\n<tr>\n<td><strong>Redis (Broker)</strong></td>\n<td>Serves as the central data store for all job and system state</td>\n<td>Job queues, scheduled sets, retry sets, worker heartbeats, metrics</td>\n<td>All other components</td>\n</tr>\n</tbody></table>\n<p>Let&#39;s examine each component in detail:</p>\n<p><strong>Producer</strong> is any application code that needs to perform work asynchronously. It&#39;s not a separate service but a <strong>client library</strong> integrated into your application. The producer&#39;s sole responsibility is to define a job (type, arguments, metadata) and hand it off to the Queue Manager. It doesn&#39;t wait for job completion—this is the essence of asynchronous execution.</p>\n<p><strong>Queue Manager</strong> acts as the <strong>post office sorting facility</strong> for jobs. When a producer submits a job, the Queue Manager validates the payload size (rejecting oversized jobs), assigns a unique job ID (ULID for time-ordered uniqueness), serializes the job to JSON, and determines which queue it belongs to based on the job type and priority configuration. It then uses Redis list operations to atomically enqueue the job. The Queue Manager exposes a clean API (<code>enqueue</code>, <code>bulk_enqueue</code>, <code>queue_stats</code>) that producers call.</p>\n<p><strong>Worker</strong> is the <strong>factory assembly line</strong> that actually does the work. Each worker process runs an infinite loop that polls Redis for available jobs across multiple queues, respecting priority weights. When it obtains a job, it deserializes it, looks up the corresponding job handler (registered during worker startup), and executes it within a configurable timeout. The worker manages the job&#39;s lifecycle state transitions (PENDING → ACTIVE → COMPLETED/FAILED) and reports heartbeats to Redis so the monitoring system knows it&#39;s alive.</p>\n<p><strong>Scheduler</strong> functions like a <strong>calendar app with recurring events</strong>. It runs as a separate background process that continuously polls Redis sorted sets for jobs scheduled for future execution. For delayed jobs (run once at a specific time), it simply moves them to the appropriate work queue when their timestamp arrives. For recurring jobs defined by cron expressions, it calculates the next execution time after each run and re-enqueues the job. The scheduler also handles timezone conversions and ensures unique jobs aren&#39;t duplicated.</p>\n<p><strong>Retry Manager</strong> implements a <strong>customer service escalation process</strong>. When a job fails, the worker consults the Retry Manager to determine what should happen next. The Retry Manager examines the job&#39;s attempt count, applies an exponential backoff algorithm (with optional jitter) to calculate the next retry delay, and stores the job in a Redis sorted set keyed by its retry timestamp. A separate polling process (or the main scheduler) later moves retry jobs back to work queues. Jobs that exceed their maximum retry count are moved to the dead letter queue for manual inspection.</p>\n<p><strong>Monitor/Dashboard</strong> serves as the <strong>air traffic control tower</strong> for the entire system. It collects metrics from Redis (queue lengths, worker heartbeats) and from job completion events, aggregates them, and presents them in a real-time web dashboard. It can trigger alerts when queues exceed thresholds or error rates spike. The dashboard also provides administrative controls to manually retry failed jobs or inspect job details.</p>\n<p><strong>Redis</strong> is the <strong>central nervous system</strong> connecting all components. We use different Redis data structures for different purposes:</p>\n<ul>\n<li><strong>Lists</strong> for FIFO job queues (LPUSH/RPOP)</li>\n<li><strong>Sorted Sets</strong> for scheduled and retry jobs (ZADD/ZRANGEBYSCORE)</li>\n<li><strong>Hashes</strong> for job details and worker heartbeats (HSET/HGETALL)</li>\n<li><strong>Streams</strong> for job completion events and metrics (XADD/XREAD)</li>\n<li><strong>Sets</strong> for tracking active workers and queues (SADD/SMEMBERS)</li>\n</ul>\n<blockquote>\n<p><strong>Design Insight:</strong> Redis serves as both the queue broker AND the system of record for job state. This simplifies the architecture (no separate database needed) but requires careful Redis data design to avoid bottlenecks. All components are stateless except for Redis—this enables horizontal scaling of workers and producers.</p>\n</blockquote>\n<p>Now let&#39;s examine the key interfaces between these components:</p>\n<p><strong>Queue Manager Interface Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>enqueue</code></td>\n<td><code>job_type: str</code>, <code>args: List</code>, <code>kwargs: Dict</code>, <code>queue: str</code>, <code>priority: int</code>, <code>max_retries: int</code>, <code>timeout_seconds: int</code>, <code>metadata: Dict</code></td>\n<td><code>job_id: str</code></td>\n<td>Validates job payload, generates job ID, serializes job, and pushes to appropriate Redis queue</td>\n</tr>\n<tr>\n<td><code>bulk_enqueue</code></td>\n<td><code>jobs: List[Dict]</code> (each dict contains same fields as <code>enqueue</code> parameters)</td>\n<td><code>List[str]</code> (job IDs)</td>\n<td>Atomically enqueues multiple jobs in a single Redis transaction</td>\n</tr>\n<tr>\n<td><code>queue_stats</code></td>\n<td><code>queue_names: Optional[List[str]]</code></td>\n<td><code>Dict[str, Dict]</code> with keys: <code>pending_count</code>, <code>active_count</code>, <code>scheduled_count</code>, <code>retry_count</code></td>\n<td>Returns current metrics for specified queues (or all queues)</td>\n</tr>\n<tr>\n<td><code>peek</code></td>\n<td><code>queue: str</code>, <code>count: int</code></td>\n<td><code>List[Job]</code></td>\n<td>Returns next <code>count</code> jobs from queue without removing them (for inspection)</td>\n</tr>\n</tbody></table>\n<p><strong>Worker Interface Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>register_handler</code></td>\n<td><code>job_type: str</code>, <code>handler: Callable</code></td>\n<td><code>None</code></td>\n<td>Maps a job type string to a Python function that will execute jobs of that type</td>\n</tr>\n<tr>\n<td><code>start</code></td>\n<td><code>config: WorkerConfig</code></td>\n<td><code>None</code> (blocks until shutdown)</td>\n<td>Starts the worker main loop, polling queues and executing jobs</td>\n</tr>\n<tr>\n<td><code>stop</code></td>\n<td><code>graceful: bool = True</code></td>\n<td><code>None</code></td>\n<td>Signals the worker to stop (immediately if <code>graceful=False</code>, else after current job completes)</td>\n</tr>\n<tr>\n<td><code>pause</code></td>\n<td><code>queue: Optional[str]</code></td>\n<td><code>None</code></td>\n<td>Temporarily stops processing jobs from specified queue (or all queues)</td>\n</tr>\n<tr>\n<td><code>resume</code></td>\n<td><code>queue: Optional[str]</code></td>\n<td><code>None</code></td>\n<td>Resumes processing from a paused queue</td>\n</tr>\n</tbody></table>\n<p><strong>Scheduler Interface Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>schedule</code></td>\n<td><code>job: Job</code>, <code>run_at: datetime</code></td>\n<td><code>schedule_id: str</code></td>\n<td>Schedules a job for one-time execution at <code>run_at</code></td>\n</tr>\n<tr>\n<td><code>schedule_cron</code></td>\n<td><code>job: Job</code>, <code>cron_expression: str</code>, <code>timezone: Optional[str]</code></td>\n<td><code>schedule_id: str</code></td>\n<td>Schedules a recurring job using cron expression</td>\n</tr>\n<tr>\n<td><code>unschedule</code></td>\n<td><code>schedule_id: str</code></td>\n<td><code>bool</code> (success)</td>\n<td>Removes a scheduled job (both one-time and recurring)</td>\n</tr>\n<tr>\n<td><code>list_schedules</code></td>\n<td><code>job_type: Optional[str]</code></td>\n<td><code>List[Dict]</code></td>\n<td>Returns all scheduled jobs, optionally filtered by job type</td>\n</tr>\n</tbody></table>\n<p><strong>Retry Manager Interface Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>schedule_retry</code></td>\n<td><code>job: Job</code>, <code>error: Exception</code></td>\n<td><code>retry_at: datetime</code></td>\n<td>Calculates next retry time using exponential backoff, stores job in retry set, records error</td>\n</tr>\n<tr>\n<td><code>get_dead_letter_jobs</code></td>\n<td><code>queue: Optional[str]</code>, <code>limit: int</code></td>\n<td><code>List[Job]</code></td>\n<td>Retrieves jobs from dead letter queue for manual inspection/retry</td>\n</tr>\n<tr>\n<td><code>retry_dead_letter_job</code></td>\n<td><code>job_id: str</code></td>\n<td><code>bool</code> (success)</td>\n<td>Moves a job from dead letter queue back to work queue for immediate retry</td>\n</tr>\n<tr>\n<td><code>purge_dead_letter_jobs</code></td>\n<td><code>older_than: Optional[datetime]</code></td>\n<td><code>int</code> (count purged)</td>\n<td>Permanently removes jobs from dead letter queue, optionally filtered by age</td>\n</tr>\n</tbody></table>\n<p><strong>Monitor Interface Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>get_queue_metrics</code></td>\n<td><code>time_range: str = &quot;1h&quot;</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Returns queue depths, processing rates, error rates for all queues over time range</td>\n</tr>\n<tr>\n<td><code>get_worker_status</code></td>\n<td><code>worker_id: Optional[str]</code></td>\n<td><code>List[Dict]</code> or <code>Dict</code></td>\n<td>Returns status of all workers or specific worker (ID, current job, last heartbeat)</td>\n</tr>\n<tr>\n<td><code>search_jobs</code></td>\n<td><code>filters: Dict</code> (job_id, status, queue, time_range)</td>\n<td><code>List[Job]</code></td>\n<td>Searches job history with pagination and filtering</td>\n</tr>\n<tr>\n<td><code>trigger_alert</code></td>\n<td><code>alert_type: str</code>, <code>details: Dict</code></td>\n<td><code>None</code></td>\n<td>Triggers an alert (logs, webhook, etc.) when system conditions warrant</td>\n</tr>\n</tbody></table>\n<h3 id=\"communication-patterns-and-data-flow\">Communication Patterns and Data Flow</h3>\n<p>The components communicate through <strong>two primary patterns</strong>: <strong>broker-mediated messaging</strong> (via Redis) for job data flow, and <strong>direct API calls</strong> for management operations. This hybrid approach balances decoupling with efficiency.</p>\n<p><strong>Primary Communication Channels:</strong></p>\n<ol>\n<li><p><strong>Redis as Message Broker</strong>: All job-related data flows through Redis using atomic operations:</p>\n<ul>\n<li><strong>Producer → Queue Manager → Redis</strong>: Jobs are LPUSHed to list keys like <code>queue:{queue_name}</code></li>\n<li><strong>Redis → Worker</strong>: Workers BRPOP jobs from multiple queues</li>\n<li><strong>Worker → Retry Manager → Redis</strong>: Failed jobs are ZADDed to sorted set <code>retry:{queue_name}</code> with score = retry timestamp</li>\n<li><strong>Scheduler → Redis</strong>: Due jobs are moved from <code>scheduled</code> sorted sets to work queues</li>\n<li><strong>All → Monitor</strong>: Components write metrics/events to Redis streams/hashes for monitoring</li>\n</ul>\n</li>\n<li><p><strong>Direct HTTP/RPC for Management</strong>: The dashboard and administrative tools communicate directly with components via REST APIs or RPC:</p>\n<ul>\n<li><strong>Dashboard → Monitor API</strong>: Real-time metrics polling</li>\n<li><strong>Dashboard → Queue Manager</strong>: Manual job enqueueing/ inspection</li>\n<li><strong>Dashboard → Retry Manager</strong>: Dead letter queue operations</li>\n</ul>\n</li>\n</ol>\n<p><strong>Data Flow Through the System:</strong></p>\n<p>Let&#39;s trace a job through the complete system with three scenarios: immediate execution, scheduled execution, and failed execution with retries.</p>\n<p><strong>Scenario 1: Immediate Job Execution (Happy Path)</strong></p>\n<ol>\n<li><strong>Producer</strong> creates a <code>Job</code> object with <code>job_type=&quot;process_image&quot;</code>, <code>args=[&quot;image.jpg&quot;]</code>, <code>queue=&quot;high_priority&quot;</code>.</li>\n<li><strong>Producer</strong> calls <code>QueueManager.enqueue(job)</code>.</li>\n<li><strong>Queue Manager</strong> validates the job (size &lt; 1MB), generates a ULID job ID, serializes the job to JSON, and executes <code>LPUSH queue:high_priority &lt;serialized_job&gt;</code>.</li>\n<li><strong>Worker</strong> (continuously polling with <code>BRPOP queue:high_priority queue:low_priority ...</code>) receives the serialized job from Redis.</li>\n<li><strong>Worker</strong> deserializes the job, looks up the registered handler for <code>process_image</code>, and executes it within a timeout.</li>\n<li><strong>Worker</strong> on success: Updates job status to <code>COMPLETED</code>, stores result in Redis hash <code>job:{job_id}</code>, emits completion event to Redis stream <code>events:completed</code>.</li>\n<li><strong>Monitor</strong> reads from <code>events:completed</code> stream, updates real-time metrics, and makes job result available via dashboard.</li>\n</ol>\n<p><strong>Scenario 2: Scheduled Recurring Job</strong></p>\n<ol>\n<li><strong>Producer</strong> calls <code>Scheduler.schedule_cron(job, &quot;0 * * * *&quot;)</code> (hourly job).</li>\n<li><strong>Scheduler</strong> calculates next run time (e.g., 14:00 today), stores job in Redis sorted set <code>scheduled:cron</code> with score = 14:00 timestamp.</li>\n<li><strong>Scheduler Process</strong> (separate daemon) polls <code>scheduled:cron</code> every 30 seconds for jobs with score &lt;= current time.</li>\n<li>At 14:00, scheduler finds the job, moves it to work queue via <code>QueueManager.enqueue(job)</code>, calculates next run time (15:00), updates the score in <code>scheduled:cron</code>.</li>\n<li>Job flows through normal execution path (Scenario 1).</li>\n</ol>\n<p><strong>Scenario 3: Failed Job with Retries</strong></p>\n<ol>\n<li><strong>Worker</strong> executes a job, but it raises <code>ConnectionError</code>.</li>\n<li><strong>Worker</strong> catches the exception, calls <code>RetryManager.schedule_retry(job, error)</code>.</li>\n<li><strong>Retry Manager</strong> increments job&#39;s <code>attempts</code> count, calculates retry delay using exponential backoff (e.g., 2^attempt seconds), stores job in Redis sorted set <code>retry:{queue_name}</code> with score = current time + delay.</li>\n<li><strong>Retry Poller</strong> (could be same as scheduler) periodically checks retry sets for due jobs (score &lt;= current time), moves them back to work queues.</li>\n<li>If job fails again, process repeats with longer delay (4s, 8s, 16s...).</li>\n<li>After exceeding <code>max_retries</code> (default 5), job is moved to dead letter queue (Redis list <code>dead:{queue_name}</code>) with full error history.</li>\n</ol>\n<p><strong>Component Interaction Patterns Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Interaction</th>\n<th>Pattern</th>\n<th>Technology</th>\n<th>Data Format</th>\n<th>Reliability Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Enqueue</td>\n<td>Fire-and-forget</td>\n<td>Redis LPUSH</td>\n<td>JSON</td>\n<td>At-most-once (may be lost if Redis crashes before persistence)</td>\n</tr>\n<tr>\n<td>Job Dequeue</td>\n<td>Blocking poll</td>\n<td>Redis BRPOP</td>\n<td>JSON</td>\n<td>At-least-once (job may be processed multiple times if worker crashes after dequeue but before processing)</td>\n</tr>\n<tr>\n<td>Scheduled Jobs</td>\n<td>Periodic poll</td>\n<td>Redis ZRANGEBYSCORE</td>\n<td>JSON</td>\n<td>At-least-once (may execute multiple times if scheduler crashes after enqueue but before updating score)</td>\n</tr>\n<tr>\n<td>Worker Heartbeats</td>\n<td>Periodic write</td>\n<td>Redis HSET</td>\n<td>JSON</td>\n<td>Best-effort (stale workers detected via timeout)</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Write to stream</td>\n<td>Redis XADD</td>\n<td>JSON</td>\n<td>Best-effort (occasional loss acceptable)</td>\n</tr>\n<tr>\n<td>Dashboard Updates</td>\n<td>Polling + WebSocket</td>\n<td>HTTP + Redis pub/sub</td>\n<td>JSON</td>\n<td>Best-effort</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The system guarantees <strong>at-least-once delivery</strong> for jobs, which is appropriate for most background tasks (idempotent by design). For truly once-only processing, jobs must include their own idempotency logic using unique keys in the application domain.</p>\n</blockquote>\n<p><strong>Data Flow State Transitions:</strong></p>\n<p>A job moves through various Redis data structures during its lifecycle:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Job Created → queue:{name} LIST (PENDING)\n            → (Worker fetches) → job:{id} HASH (ACTIVE)\n            → (Success) → events:completed STREAM (COMPLETED)\n            → (Failure) → retry:{queue} ZSET (RETRY_SCHEDULED)\n            → (After backoff) → queue:{name} LIST again\n            → (Max retries) → dead:{queue} LIST (DEAD_LETTER)</code></pre></div>\n\n<p><strong>Concurrency and Scaling Considerations:</strong></p>\n<ul>\n<li><strong>Multiple Workers</strong> can poll the same queue simultaneously—Redis ensures each job goes to only one worker via atomic BRPOP.</li>\n<li><strong>Queue Priority</strong> is implemented by having workers poll multiple queues with weighted round-robin (more BRPOP calls on high-priority queues).</li>\n<li><strong>Horizontal Scaling</strong> is achieved by adding more worker processes or even worker hosts—all connect to the same Redis instance.</li>\n<li><strong>Redis Bottlenecks</strong> may occur at very high throughput (&gt;10K jobs/sec); solutions include Redis clustering or sharding queues across instances.</li>\n</ul>\n<p><strong>Failure Handling in Communication:</strong></p>\n<ul>\n<li><strong>Redis Connection Loss</strong>: All components implement exponential backoff reconnection logic with circuit breaker pattern.</li>\n<li><strong>Worker Crash During Job</strong>: Jobs remain in ACTIVE state; a cleanup process periodically moves stale active jobs back to PENDING queue.</li>\n<li><strong>Network Partition</strong>: Split-brain scenarios are mitigated by favoring availability—jobs continue processing on both sides of partition, requiring application-level idempotency.</li>\n</ul>\n<h3 id=\"recommended-file-and-module-structure\">Recommended File and Module Structure</h3>\n<p>Organizing code clearly from the start prevents &quot;big ball of mud&quot; architecture. The recommended structure follows <strong>clean architecture</strong> principles: core domain logic independent of frameworks, with clear separation between components.</p>\n<p><strong>Project Root Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background_job_processor/\n├── pyproject.toml                    # Python project config and dependencies\n├── README.md\n├── .env.example                      # Example environment variables\n├── docker-compose.yml                # For local Redis and dashboard\n├── scripts/                          # Utility scripts\n│   ├── start_worker.py\n│   ├── start_scheduler.py\n│   └── seed_test_jobs.py\n├── src/                              # Main source code (Python package)\n│   └── job_processor/\n│       ├── __init__.py\n│       ├── core/                     # Domain models and interfaces\n│       │   ├── __init__.py\n│       │   ├── job.py               # Job class and serialization\n│       │   ├── exceptions.py        # Custom exceptions\n│       │   └── types.py             # Type definitions and enums\n│       ├── redis_client/             # Redis abstraction layer\n│       │   ├── __init__.py\n│       │   ├── client.py            # RedisClient wrapper\n│       │   ├── connection_pool.py\n│       │   └── errors.py\n│       ├── queue/                    # Milestone 1: Queue Manager\n│       │   ├── __init__.py\n│       │   ├── manager.py           # QueueManager class\n│       │   ├── priority.py          # Priority calculation logic\n│       │   ├── serialization.py     # Job serialization/validation\n│       │   └── tests/\n│       ├── worker/                   # Milestone 2: Worker Process\n│       │   ├── __init__.py\n│       │   ├── worker.py            # Worker main class\n│       │   ├── handler_registry.py  # Job type to handler mapping\n│       │   ├── heartbeat.py         # Worker health reporting\n│       │   ├── pool.py              # Thread/process pool for concurrency\n│       │   └── tests/\n│       ├── retry/                    # Milestone 3: Retry System\n│       │   ├── __init__.py\n│       │   ├── manager.py           # RetryManager class\n│       │   ├── backoff.py           # Exponential backoff calculators\n│       │   ├── dead_letter.py       # Dead letter queue operations\n│       │   └── tests/\n│       ├── scheduler/                # Milestone 4: Scheduling\n│       │   ├── __init__.py\n│       │   ├── scheduler.py         # Scheduler main class\n│       │   ├── cron.py              # Cron expression parser\n│       │   ├── timezone.py          # Timezone handling utilities\n│       │   └── tests/\n│       ├── monitoring/               # Milestone 5: Monitoring\n│       │   ├── __init__.py\n│       │   ├── metrics.py           # Metrics collection and aggregation\n│       │   ├── dashboard/           # Web dashboard (FastAPI/Flask)\n│       │   │   ├── app.py\n│       │   │   ├── routes.py\n│       │   │   ├── static/\n│       │   │   └── templates/\n│       │   ├── alerts.py            # Alerting logic\n│       │   └── tests/\n│       ├── config/                   # Configuration management\n│       │   ├── __init__.py\n│       │   ├── settings.py          # SystemConfig and friends\n│       │   └── validation.py\n│       └── utils/                    # Shared utilities\n│           ├── __init__.py\n│           ├── ulid_generator.py\n│           ├── signal_handlers.py\n│           └── validation.py\n└── tests/                            # Integration and end-to-end tests\n    ├── conftest.py\n    ├── test_integration.py\n    └── fixtures/</code></pre></div>\n\n<p><strong>Module Dependencies Flow:</strong> </p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>core/ (no external dependencies)\n  ↓\nredis_client/ (depends on core, redis-py)\n  ↓\nqueue/ (depends on core, redis_client)\n  ↓\nworker/ (depends on core, redis_client, queue)\n  ↓\nretry/ (depends on core, redis_client, queue)\n  ↓\nscheduler/ (depends on core, redis_client, queue)\n  ↓\nmonitoring/ (depends on core, redis_client, all other modules)</code></pre></div>\n\n<p><strong>Key Design Decisions in Module Structure:</strong></p>\n<blockquote>\n<p><strong>Decision: Layered Architecture vs Vertical Slicing</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to organize code for a complex system with multiple interrelated components. Two common approaches are layered architecture (separating concerns like data access, business logic, presentation) and vertical slicing (organizing by feature/bounded context).</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Pure Layered Architecture</strong>: <code>data_access/</code>, <code>business_logic/</code>, <code>api/</code> layers</li>\n<li><strong>Vertical Slices by Component</strong>: <code>queue/</code>, <code>worker/</code>, <code>scheduler/</code> as self-contained modules</li>\n<li><strong>Hybrid Approach</strong>: Component-based with shared core layer</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach (option 3) as shown in the structure above</li>\n<li><strong>Rationale</strong>: Each component (queue, worker, scheduler) has clear boundaries and can be developed/tested independently (vertical slicing), while shared concerns like Redis client, configuration, and core domain models are extracted to common layers to avoid duplication. This balances modularity with DRY principles.</li>\n<li><strong>Consequences</strong>: Components are loosely coupled and can be optional (e.g., you could use just the queue and worker without scheduler). However, there&#39;s some complexity in managing cross-component dependencies.</li>\n</ul>\n</blockquote>\n<p><strong>Component Interface Contracts:</strong></p>\n<p>Each component module exposes a clean public API while hiding implementation details:</p>\n<ul>\n<li><strong><code>queue</code> module</strong>: Exports <code>QueueManager</code> class, <code>QueueConfig</code>, <code>enqueue_job</code> helper function</li>\n<li><strong><code>worker</code> module</strong>: Exports <code>Worker</code>, <code>WorkerConfig</code>, <code>register_job_handler</code> decorator</li>\n<li><strong><code>scheduler</code> module</strong>: Exports <code>Scheduler</code>, <code>CronSchedule</code>, <code>schedule_job</code> helper</li>\n<li><strong><code>retry</code> module</strong>: Exports <code>RetryManager</code>, <code>ExponentialBackoff</code>, <code>DeadLetterQueue</code></li>\n<li><strong><code>monitoring</code> module</strong>: Exports <code>MetricsCollector</code>, <code>DashboardApp</code>, <code>setup_monitoring</code></li>\n</ul>\n<p><strong>Configuration Management Strategy:</strong></p>\n<p>All components share a central <code>SystemConfig</code> object loaded from environment variables with sensible defaults. Each component section (queue, worker) has its own config class that can be overridden individually.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example configuration loading</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SystemConfig.from_env()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">queue_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QueueManager(config.redis, config.queues)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">worker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Worker(config.workers[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], config.redis)</span></span></code></pre></div>\n\n<p><strong>Deployment View:</strong></p>\n<ul>\n<li><strong>Development</strong>: Single Redis instance, workers and scheduler run as separate processes on same machine</li>\n<li><strong>Production</strong>: Redis cluster or managed Redis service, workers distributed across multiple machines/containers, scheduler as redundant active-passive pair</li>\n<li><strong>Container Orchestration</strong>: Each component (worker, scheduler, dashboard) in its own Docker container, scaled independently</li>\n</ul>\n<p><strong>Module Responsibility Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Module</th>\n<th>Primary Classes</th>\n<th>Key Responsibilities</th>\n<th>Depends On</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>core</code></td>\n<td><code>Job</code>, <code>JobStatus</code></td>\n<td>Define core domain models and interfaces</td>\n<td>None (pure Python)</td>\n</tr>\n<tr>\n<td><code>redis_client</code></td>\n<td><code>RedisClient</code></td>\n<td>Abstract Redis operations, connection pooling, error handling</td>\n<td><code>core</code>, <code>redis-py</code></td>\n</tr>\n<tr>\n<td><code>queue</code></td>\n<td><code>QueueManager</code></td>\n<td>Job validation, serialization, enqueueing with priority</td>\n<td><code>core</code>, <code>redis_client</code></td>\n</tr>\n<tr>\n<td><code>worker</code></td>\n<td><code>Worker</code>, <code>HandlerRegistry</code></td>\n<td>Polling queues, executing jobs, concurrency management</td>\n<td><code>core</code>, <code>redis_client</code>, <code>queue</code></td>\n</tr>\n<tr>\n<td><code>retry</code></td>\n<td><code>RetryManager</code>, <code>ExponentialBackoff</code></td>\n<td>Managing retry logic, backoff calculations, dead letter queue</td>\n<td><code>core</code>, <code>redis_client</code>, <code>queue</code></td>\n</tr>\n<tr>\n<td><code>scheduler</code></td>\n<td><code>Scheduler</code>, <code>CronParser</code></td>\n<td>Scheduling delayed/recurring jobs, timezone handling</td>\n<td><code>core</code>, <code>redis_client</code>, <code>queue</code></td>\n</tr>\n<tr>\n<td><code>monitoring</code></td>\n<td><code>MetricsCollector</code>, <code>DashboardApp</code></td>\n<td>Collecting metrics, providing web UI, alerting</td>\n<td>All other modules</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> This modular structure enables incremental implementation aligned with the project milestones. You can build and test the <code>queue</code> module (Milestone 1) independently before implementing <code>worker</code> (Milestone 2), with each milestone adding a new module to the system.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale for Choice</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis Client</strong></td>\n<td><code>redis-py</code> with connection pooling</td>\n<td><code>hiredis</code> parser for performance + <code>redis-py</code></td>\n<td><code>redis-py</code> is standard, well-maintained, and sufficient for most workloads</td>\n</tr>\n<tr>\n<td><strong>Web Dashboard</strong></td>\n<td>FastAPI + Jinja2 templates + HTMX for dynamic updates</td>\n<td>Separate React frontend with FastAPI backend</td>\n<td>FastAPI provides automatic OpenAPI docs, async support; HTMX keeps complexity low</td>\n</tr>\n<tr>\n<td><strong>Job Serialization</strong></td>\n<td>JSON with <code>orjson</code> for speed</td>\n<td>MessagePack for smaller payloads</td>\n<td>JSON is universal, debuggable; <code>orjson</code> is 2-3x faster than stdlib <code>json</code></td>\n</tr>\n<tr>\n<td><strong>Concurrency Model</strong></td>\n<td><code>concurrent.futures.ThreadPoolExecutor</code></td>\n<td><code>multiprocessing.Pool</code> for CPU-bound tasks</td>\n<td>Threads are fine for I/O-bound jobs; processes for CPU-bound but with serialization overhead</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>Python-dotenv + Pydantic models</td>\n<td>ConfigMap + environment variables (K8s)</td>\n<td>Pydantic provides validation with clear error messages</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td>pytest with <code>fakeredis</code> for unit tests</td>\n<td>Docker-based integration tests with real Redis</td>\n<td><code>fakeredis</code> is fast for unit tests; real Redis for integration</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure Implementation:</strong></p>\n<p>Here&#39;s the minimal structure to begin with (expanding as milestones progress):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> background_job_processor/src/job_processor/{core,redis_client,queue,worker,retry,scheduler,monitoring,config,utils}</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">touch</span><span style=\"color:#9ECBFF\"> background_job_processor/src/job_processor/__init__.py</span></span></code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<p>Create these foundational files first—they&#39;re prerequisites for all components:</p>\n<p><strong>1. Core Domain Models (<code>src/job_processor/core/job.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Core Job model and serialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumeration of possible job states.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACTIVE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retry_scheduled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEAD_LETTER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dead_letter\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Job</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a unit of work to be processed asynchronously.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args: List[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    kwargs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queue: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"default\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: JobStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result: Optional[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert job to dictionary for serialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asdict(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert datetime objects to ISO format strings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> date_field </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'created_at'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'started_at'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'completed_at'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> data[date_field]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                data[date_field] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data[date_field].isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data[date_field], datetime) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> data[date_field]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert status enum to string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data[</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status.value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reconstruct job from dictionary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert string dates back to datetime objects</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        date_fields </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'created_at'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'started_at'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'completed_at'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> field_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> date_fields:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> field_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> data[field_name] </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data[field_name], </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                data[field_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.fromisoformat(data[field_name])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> field_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> data[field_name]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                data[field_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert status string back to enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#9ECBFF\"> 'status'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data[</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data[</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus(data[</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> serialize</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize job to JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> json.dumps(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.to_dict(), </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> deserialize</span><span style=\"color:#E1E4E8\">(cls, data: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deserialize job from JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.from_dict(json.loads(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_error</span><span style=\"color:#E1E4E8\">(self, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record an error that occurred during job execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'type'</span><span style=\"color:#E1E4E8\">: error.</span><span style=\"color:#79B8FF\">__class__</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'message'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(error),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'traceback'</span><span style=\"color:#E1E4E8\">: traceback.format_exc(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'occurred_at'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'attempt'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.attempts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_retry</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if job should be retried.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.attempts </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">DEAD_LETTER</span></span></code></pre></div>\n\n<p><strong>2. Redis Client Abstraction (<code>src/job_processor/redis_client/client.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Redis client wrapper with connection pooling and error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> redis.exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Singleton Redis client wrapper with connection pooling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _instance: Optional[</span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _client: Optional[redis.Redis] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">connection_kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize Redis client (private, use get_instance).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_kwargs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_kwargs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connect()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> connect</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Establish connection to Redis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.from_url(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.url,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                **</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_kwargs,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#6A737D\">  # Auto-decode bytes to str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test connection</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.ping()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Connected to Redis at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to connect to Redis: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_instance</span><span style=\"color:#E1E4E8\">(cls, url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> </span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get singleton RedisClient instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> url </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                url </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379/0\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(url, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_client</span><span style=\"color:#E1E4E8\">(self) -> redis.Redis:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the underlying Redis client instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.connect()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute</span><span style=\"color:#E1E4E8\">(self, command: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a Redis command with error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            method </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(client, command)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> method(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis command failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">command</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#79B8FF\"> {</span><span style=\"color:#E1E4E8\">args</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Re-raise for caller to handle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline</span><span style=\"color:#E1E4E8\">(self) -> redis.Pipeline:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return a Redis pipeline for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client().pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> health_check</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if Redis is responsive.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client().ping()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> RedisError:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>3. Configuration Management (<code>src/job_processor/config/settings.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Configuration management with Pydantic models.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pydantic </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseModel, Field, validator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dotenv </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> load_dotenv</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">load_dotenv()  </span><span style=\"color:#6A737D\"># Load environment variables from .env file</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseModel</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Redis connection configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"redis://localhost:6379/0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_connections: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    socket_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_timeout: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseModel</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Queue configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @validator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> name_must_be_valid</span><span style=\"color:#E1E4E8\">(cls, v):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> v </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> ':'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> v:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'Queue name cannot be empty or contain colons'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WorkerConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseModel</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Worker process configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queues: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    concurrency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    heartbeat_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SystemConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseModel</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main system configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis: RedisConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RedisConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queues: List[QueueConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: [QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">)])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    workers: List[WorkerConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: [WorkerConfig()])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_payload_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1048576</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 1MB default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_history_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># How many completed jobs to keep</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_base_delay: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Base delay in seconds for exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ge</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_env</span><span style=\"color:#E1E4E8\">(cls) -> </span><span style=\"color:#9ECBFF\">'SystemConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from environment variables with defaults.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            redis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RedisConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">\"REDIS_URL\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"redis://localhost:6379/0\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_connections</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">\"REDIS_MAX_CONNECTIONS\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"10\"</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                socket_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">\"REDIS_SOCKET_TIMEOUT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"5\"</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                retry_on_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">\"REDIS_RETRY_ON_TIMEOUT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">).lower() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"true\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            max_payload_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">\"MAX_PAYLOAD_SIZE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1048576\"</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            job_history_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">\"JOB_HISTORY_SIZE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1000\"</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            retry_base_delay</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">\"RETRY_BASE_DELAY\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            retry_max_attempts</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">\"RETRY_MAX_ATTEMPTS\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"5\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton for Queue Manager (<code>src/job_processor/queue/manager.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Queue Manager - handles job enqueueing with validation and priority.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> uuid </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid4</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..core.job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job, JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..redis_client.client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..config.settings </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueConfig, SystemConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages job queues with validation, serialization, and priority handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: RedisClient, queue_configs: List[QueueConfig]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize QueueManager with Redis client and queue configurations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {q.name: q </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> q </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> queue_configs}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize any required Redis data structures on startup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that queue names don't conflict with Redis key patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> enqueue</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        args: Optional[List[Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kwargs: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"default\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Enqueue a job for asynchronous execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_type: String identifier for the job handler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            args: Positional arguments for the job handler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            kwargs: Keyword arguments for the job handler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue: Name of the queue to enqueue to</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            priority: Priority level (higher = more important)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_retries: Maximum number of retry attempts</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            timeout_seconds: Maximum execution time in seconds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            metadata: Additional metadata for the job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique identifier for the enqueued job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValueError: If queue doesn't exist or payload is too large</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            RedisError: If Redis operation fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate that the queue exists in configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Job object with generated ULID/UUID job_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate payload size (args + kwargs + metadata) doesn't exceed max_payload_size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Serialize job to JSON string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Use Redis pipeline for atomic operations:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - LPUSH to queue:queue_name with serialized job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - HSET to job:job_id with job metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - SADD to queues:active with queue name (for monitoring)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return job_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> bulk_enqueue</span><span style=\"color:#E1E4E8\">(self, jobs: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically enqueue multiple jobs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            jobs: List of job dictionaries (same format as enqueue parameters)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of job IDs in the same order as input jobs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate all jobs before any Redis operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create pipeline for atomic bulk insert</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each job: validate, create Job object, serialize, add to pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of job IDs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> queue_stats</span><span style=\"color:#E1E4E8\">(self, queue_names: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Get statistics for specified queues (or all queues).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns dictionary with queue names as keys and dicts containing:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - pending_count: Jobs waiting in queue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - active_count: Jobs currently being processed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - scheduled_count: Jobs scheduled for future execution</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - retry_count: Jobs waiting for retry</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - dead_letter_count: Jobs in dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If queue_names is None, get all configured queue names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each queue, use Redis LLEN for pending count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each queue, use Redis SCARD for active jobs (from job:active:{queue})</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each queue, use Redis ZCARD for scheduled and retry counts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each queue, use Redis LLEN for dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return aggregated dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self, queue: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> List[Job]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Peek at next jobs in queue without removing them.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue: Queue name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            count: Maximum number of jobs to return</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of Job objects (deserialized)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate queue exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use Redis LRANGE to get first 'count' items from queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Deserialize each item to Job object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return list of Jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints (Python):</strong></p>\n<ul>\n<li>Use <code>orjson</code> instead of standard <code>json</code> for 2-3x faster serialization (install via <code>pip install orjson</code>)</li>\n<li>For ULID generation, use <code>ulid-py</code> package: <code>from ulid import ULID; job_id = str(ULID())</code></li>\n<li>Use <code>redis-py</code> version 4.0+ for proper async support and connection management</li>\n<li>Implement connection pooling via <code>redis.ConnectionPool</code> to reuse connections across components</li>\n<li>Use Python&#39;s <code>signal</code> module for graceful shutdown: <code>signal.signal(signal.SIGTERM, shutdown_handler)</code></li>\n<li>For cron parsing, consider <code>croniter</code> library instead of implementing your own parser</li>\n</ul>\n<p><strong>Milestone Checkpoint for Architecture Foundation:</strong>\nAfter setting up the basic structure and core files, verify your foundation:</p>\n<ol>\n<li><strong>Run validation</strong>: <code>python -c &quot;from src.job_processor.core.job import Job; from src.job_processor.config.settings import SystemConfig; print(&#39;Core imports successful&#39;)&quot;</code></li>\n<li><strong>Test Redis connection</strong>: Create a simple test script that instantiates <code>RedisClient</code> and calls <code>health_check()</code></li>\n<li><strong>Verify serialization roundtrip</strong>: Create a test that creates a <code>Job</code>, serializes it, deserializes it, and compares fields</li>\n<li><strong>Expected outcome</strong>: All imports succeed, Redis connects (if running locally), and job serialization preserves all data types including datetimes.</li>\n</ol>\n<p><strong>Common Setup Issues:</strong></p>\n<ul>\n<li><strong>Redis not running</strong>: Install Redis via <code>brew install redis</code> (macOS) or <code>apt install redis-server</code> (Linux), then <code>redis-server</code></li>\n<li><strong>Python path issues</strong>: Install package in development mode: <code>pip install -e .</code> from project root</li>\n<li><strong>Missing dependencies</strong>: Create <code>requirements.txt</code> with: <code>redis&gt;=4.5, pydantic&gt;=2.0, python-dotenv&gt;=1.0, orjson&gt;=3.9</code></li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the foundational data structures for all five milestones. The data model defines how jobs are represented, stored, and transmitted between components, forming the backbone of the entire background job processing system.</p>\n</blockquote>\n<p>The data model is the DNA of our background job processor—it defines how information is structured, stored, and flows through the system. Think of it as the <strong>central filing system</strong> in a busy government office. Each job is like a case file that moves through different departments (queues, workers, retry systems). The filing system must be organized so any clerk (system component) can quickly find the current status of any case, understand its history, and know what to do next. A poorly designed filing system leads to lost cases, duplicated work, and confusion about responsibilities.</p>\n<p>This section defines three layers of data organization: the <strong>persistent storage</strong> in Redis (how data is stored on disk/network), the <strong>in-memory objects</strong> (how data is structured while being processed), and the <strong>serialization format</strong> (how data moves between these two layers). Each layer serves distinct purposes with different constraints—Redis storage prioritizes atomic operations and durability, in-memory objects prioritize type safety and business logic, and serialization prioritizes cross-language compatibility and space efficiency.</p>\n<h3 id=\"redis-data-structures-and-keys\">Redis Data Structures and Keys</h3>\n<p>Redis serves as our <strong>central bulletin board and filing cabinet</strong>—a shared space where all components post messages and look up information. Unlike a traditional relational database, Redis is a key-value store with specialized data structures optimized for different access patterns. Our design uses multiple Redis data structures, each chosen for specific operational characteristics that match how we need to access the data.</p>\n<p>The key naming convention follows a clear pattern: <code>{system}:{component}:{identifier}</code>. This colon-separated hierarchy enables Redis key scanning operations, logical grouping, and automatic key expiration management. All keys use lowercase with underscores for readability.</p>\n<h4 id=\"primary-job-storage\">Primary Job Storage</h4>\n<p>Jobs in the active processing pipeline are stored in Redis lists, which provide O(1) push/pop operations and maintain FIFO ordering:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Data Type</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_queue:{queue_name}</code></td>\n<td>List</td>\n<td>Primary job queue storing serialized <code>Job</code> objects in FIFO order. Workers pop from the left, enqueuers push to the right.</td>\n<td><code>job_queue:default</code>, <code>job_queue:emails</code></td>\n</tr>\n<tr>\n<td><code>job_active</code></td>\n<td>Set</td>\n<td>Set of job IDs currently being processed by workers. Used for crash recovery to detect orphaned jobs.</td>\n<td>Contains: <code>&quot;job_01HXYZ...&quot;, &quot;job_01HABC...&quot;</code></td>\n</tr>\n<tr>\n<td><code>job_retry_schedule</code></td>\n<td>Sorted Set</td>\n<td>Delayed retry queue with execution timestamp as score. Jobs are moved back to main queue when their scheduled time arrives.</td>\n<td>Score: 1678901234.56, Value: serialized job</td>\n</tr>\n</tbody></table>\n<p>Each queue list contains JSON-serialized <code>Job</code> objects. The list structure naturally supports FIFO semantics with <code>LPUSH</code> (add to end) and <code>BRPOP</code> (remove from beginning with blocking). The separate <code>job_active</code> set allows us to track which jobs are currently being processed without modifying the original queue entry, enabling crash recovery.</p>\n<h4 id=\"job-metadata-and-state-tracking\">Job Metadata and State Tracking</h4>\n<p>Beyond the queue itself, we maintain comprehensive metadata about each job&#39;s lifecycle:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Data Type</th>\n<th>Description</th>\n<th>Expiration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job:{job_id}</code></td>\n<td>Hash</td>\n<td>Complete job state including arguments, status, timestamps, and error history. Serves as the system of record.</td>\n<td>7 days after completion</td>\n</tr>\n<tr>\n<td><code>job_status:{job_id}</code></td>\n<td>String</td>\n<td>Current job status as <code>JobStatus</code> enum value. Optimized for quick status checks without loading full job.</td>\n<td>Same as parent job</td>\n</tr>\n<tr>\n<td><code>job_result:{job_id}</code></td>\n<td>String</td>\n<td>Serialized job result for completed jobs. Stored separately to avoid loading large results during status checks.</td>\n<td>7 days after completion</td>\n</tr>\n</tbody></table>\n<p>The <code>job:{job_id}</code> hash contains all mutable job state fields. Storing this separately from the queue payload allows us to update job status and metadata without modifying the serialized job in the queue (which is immutable once enqueued). The hash structure provides O(1) access to individual fields, which is efficient for partial updates.</p>\n<h4 id=\"system-management-and-monitoring\">System Management and Monitoring</h4>\n<p>Operational data enables queue management, worker coordination, and monitoring:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Data Type</th>\n<th>Description</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>worker:heartbeat:{worker_id}</code></td>\n<td>String</td>\n<td>Timestamp of last worker heartbeat. Used to detect dead workers.</td>\n<td>Worker liveness detection</td>\n</tr>\n<tr>\n<td><code>worker:current_job:{worker_id}</code></td>\n<td>String</td>\n<td>Job ID currently being processed by worker. Maps worker to active job.</td>\n<td>Crash recovery correlation</td>\n</tr>\n<tr>\n<td><code>queue:config:{queue_name}</code></td>\n<td>Hash</td>\n<td>Queue configuration including priority and maximum length.</td>\n<td>Runtime queue management</td>\n</tr>\n<tr>\n<td><code>schedule:recurring</code></td>\n<td>Hash</td>\n<td>Recurring job definitions with cron expressions and job templates.</td>\n<td>Scheduler configuration</td>\n</tr>\n<tr>\n<td><code>schedule:due</code></td>\n<td>Sorted Set</td>\n<td>Scheduled jobs with execution timestamp as score.</td>\n<td>Time-based scheduling</td>\n</tr>\n<tr>\n<td><code>dead_letter:{queue_name}</code></td>\n<td>List</td>\n<td>Jobs that exceeded maximum retry attempts. Requires manual intervention.</td>\n<td>Error analysis and manual retry</td>\n</tr>\n<tr>\n<td><code>metrics:queue:{queue_name}:depth</code></td>\n<td>String</td>\n<td>Current queue depth (updated periodically).</td>\n<td>Dashboard monitoring</td>\n</tr>\n<tr>\n<td><code>metrics:job:{job_type}:processed</code></td>\n<td>Counter</td>\n<td>Number of jobs processed (incremented atomically).</td>\n<td>Performance analytics</td>\n</tr>\n</tbody></table>\n<p>The heartbeat system uses simple string keys with timestamps. When a worker updates its heartbeat, it also sets an automatic expiration (TTL) slightly longer than the heartbeat interval. If the key expires, monitoring systems know the worker has died. This pattern avoids the need for a separate cleanup process.</p>\n<h4 id=\"indexes-and-reverse-lookups\">Indexes and Reverse Lookups</h4>\n<p>To support operational queries (&quot;find all jobs of type X&quot;, &quot;find all failed jobs in queue Y&quot;), we maintain secondary indexes:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Data Type</th>\n<th>Description</th>\n<th>Maintenance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>index:job_type:{job_type}:{status}</code></td>\n<td>Set</td>\n<td>Set of job IDs for a given job type and status. Enables quick filtering.</td>\n<td>Updated on status changes</td>\n</tr>\n<tr>\n<td><code>index:queue:{queue_name}:{status}</code></td>\n<td>Set</td>\n<td>Set of job IDs for a given queue and status.</td>\n<td>Updated on status changes</td>\n</tr>\n<tr>\n<td><code>index:time:enqueued:{date}</code></td>\n<td>Sorted Set</td>\n<td>Job IDs indexed by enqueue timestamp. Enables time-range queries.</td>\n<td>Score = timestamp</td>\n</tr>\n</tbody></table>\n<p>These indexes are updated atomically with the main job status changes using Redis transactions (<code>MULTI/EXEC</code>). While Redis doesn&#39;t have automatic indexing like relational databases, this manual indexing pattern enables the query capabilities needed for the monitoring dashboard without expensive full scans.</p>\n<blockquote>\n<p><strong>Design Insight:</strong> We separate <strong>job data</strong> (in hashes) from <strong>job placement</strong> (in lists/sorted sets). This is analogous to separating a library&#39;s book catalog (metadata about books) from the shelves where books are physically placed (ordered collection). The catalog contains complete information about each book, while the shelf position determines when it will be accessed. This separation allows us to update a job&#39;s metadata (like recording an error) without moving it between queues.</p>\n</blockquote>\n<h4 id=\"redis-key-design-adr\">Redis Key Design ADR</h4>\n<blockquote>\n<p><strong>Decision: Hierarchical Key Structure with Colon Separators</strong></p>\n<ul>\n<li><strong>Context</strong>: Redis has a flat key namespace, but we need to organize thousands of keys for different purposes (jobs, workers, queues, metrics). We need a structure that supports pattern matching for cleanup operations, logical grouping for debugging, and clear ownership.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Flat names with prefixes: <code>job_12345</code>, <code>worker_abc</code></li>\n<li>Hierarchical with colons: <code>job:12345</code>, <code>worker:abc:heartbeat</code></li>\n<li>Hierarchical with dots: <code>job.12345</code>, <code>worker.abc.heartbeat</code></li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use hierarchical colon-separated keys (<code>system:component:id:subcomponent</code>).</li>\n<li><strong>Rationale</strong>: Colons are Redis&#39; conventional hierarchy separator (used in Redis Cluster hash tags). The <code>KEYS</code> and <code>SCAN</code> commands work naturally with pattern matching (<code>job:*</code> finds all job keys). This structure also matches common Redis monitoring tools&#39; expectations. Dots are less conventional in Redis and might conflict with some Redis modules&#39; naming schemes.</li>\n<li><strong>Consequences</strong>: Keys are slightly longer but self-documenting. We can delete all keys for a component with <code>DEL job:*</code> (though in production we&#39;d use <code>SCAN</code> for large datasets). The hierarchy makes debugging easier when inspecting Redis with <code>redis-cli</code>.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Flat prefixes</td>\n<td>Shorter keys, slightly less memory</td>\n<td>No hierarchy, harder to pattern match, mixes different key types</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Colon hierarchy</td>\n<td>Standard Redis convention, excellent pattern matching, self-documenting</td>\n<td>Slightly longer keys</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Dot hierarchy</td>\n<td>Familiar from domain names, hierarchical</td>\n<td>Not standard in Redis, might conflict with modules</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"in-memory-object-types-and-schemas\">In-Memory Object Types and Schemas</h3>\n<p>While Redis stores serialized data, our application logic works with rich, typed objects in memory. These objects encapsulate business logic, validation rules, and behavioral methods. Think of them as the <strong>active case files</strong> on a clerk&#39;s desk—annotated, with sticky notes, calculations in the margins, and decision logic applied. The in-memory objects transform raw data into intelligent entities that know how to validate themselves, calculate retry delays, and transition between states.</p>\n<h4 id=\"core-job-object\">Core Job Object</h4>\n<p>The <code>Job</code> class is the central entity representing a unit of work. It follows a <strong>builder pattern</strong> where required fields are set at construction and optional metadata accumulates during execution:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Immutable?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_id</code></td>\n<td><code>str</code></td>\n<td>Unique identifier (ULID format). Provides natural time-based ordering.</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>job_type</code></td>\n<td><code>str</code></td>\n<td>Job handler type (e.g., &quot;send_email&quot;, &quot;process_image&quot;). Maps to handler class.</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>args</code></td>\n<td><code>List[Any]</code></td>\n<td>Positional arguments for the job handler.</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>kwargs</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Keyword arguments for the job handler.</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>queue</code></td>\n<td><code>str</code></td>\n<td>Target queue name (e.g., &quot;default&quot;, &quot;high_priority&quot;).</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>priority</code></td>\n<td><code>int</code></td>\n<td>Relative priority within queue (higher = more important). Default: 0.</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>max_retries</code></td>\n<td><code>int</code></td>\n<td>Maximum retry attempts before moving to dead letter queue. Default: 3.</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>timeout_seconds</code></td>\n<td><code>int</code></td>\n<td>Maximum execution time before job is forcibly terminated. Default: 1800.</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td><code>datetime</code></td>\n<td>Job creation timestamp (UTC).</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><code>metadata</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Custom key-value pairs for tracing, feature flags, etc.</td>\n<td>No</td>\n</tr>\n<tr>\n<td><code>status</code></td>\n<td><code>JobStatus</code></td>\n<td>Current lifecycle state (enum).</td>\n<td>No</td>\n</tr>\n<tr>\n<td><code>attempts</code></td>\n<td><code>int</code></td>\n<td>Number of execution attempts so far (starts at 0).</td>\n<td>No</td>\n</tr>\n<tr>\n<td><code>errors</code></td>\n<td><code>List[Dict]</code></td>\n<td>Error history with timestamps, exception details, and stack traces.</td>\n<td>No</td>\n</tr>\n<tr>\n<td><code>started_at</code></td>\n<td><code>Optional[datetime]</code></td>\n<td>When current execution attempt began. <code>None</code> if not started.</td>\n<td>No</td>\n</tr>\n<tr>\n<td><code>completed_at</code></td>\n<td><code>Optional[datetime]</code></td>\n<td>When job reached terminal state (completed/failed/dead). <code>None</code> if pending.</td>\n<td>No</td>\n</tr>\n<tr>\n<td><code>result</code></td>\n<td><code>Optional[Any]</code></td>\n<td>Job execution result (serializable). Only set for <code>COMPLETED</code> jobs.</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The <code>job_id</code> uses ULID (Universally Unique Lexicographically Sortable Identifier) which provides both uniqueness and time-based ordering. Unlike UUIDv4, ULIDs are sortable by creation time, making time-range queries efficient. The <code>metadata</code> field is a flexible bag for cross-cutting concerns like tracing IDs, tenant identifiers, or feature flags—it&#39;s mutable because these can be added during processing.</p>\n<h4 id=\"job-status-state-machine\">Job Status State Machine</h4>\n<p>The <code>JobStatus</code> enum defines the legal states a job can occupy, forming a strict state machine (illustrated in <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fjob-state-machine.svg\" alt=\"Job State Machine\">):</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PENDING</code></td>\n<td>Worker dequeues job</td>\n<td><code>ACTIVE</code></td>\n<td>Set <code>started_at</code> to current time, increment <code>attempts</code></td>\n</tr>\n<tr>\n<td><code>ACTIVE</code></td>\n<td>Job completes successfully</td>\n<td><code>COMPLETED</code></td>\n<td>Set <code>completed_at</code>, store result, clear <code>started_at</code></td>\n</tr>\n<tr>\n<td><code>ACTIVE</code></td>\n<td>Job fails with retries remaining</td>\n<td><code>RETRY_SCHEDULED</code></td>\n<td>Record error, calculate next retry time, schedule in sorted set</td>\n</tr>\n<tr>\n<td><code>ACTIVE</code></td>\n<td>Job fails with no retries left</td>\n<td><code>DEAD_LETTER</code></td>\n<td>Record error, move to dead letter queue, set <code>completed_at</code></td>\n</tr>\n<tr>\n<td><code>RETRY_SCHEDULED</code></td>\n<td>Retry timer expires</td>\n<td><code>PENDING</code></td>\n<td>Move from retry sorted set to main queue</td>\n</tr>\n<tr>\n<td><code>RETRY_SCHEDULED</code></td>\n<td>Manual retry requested</td>\n<td><code>PENDING</code></td>\n<td>Reset error count, move to main queue immediately</td>\n</tr>\n<tr>\n<td><code>DEAD_LETTER</code></td>\n<td>Manual retry requested</td>\n<td><code>PENDING</code></td>\n<td>Reset error count and attempts, move to main queue</td>\n</tr>\n<tr>\n<td>Any state</td>\n<td>Job cancelled</td>\n<td><code>FAILED</code></td>\n<td>Record cancellation error, set <code>completed_at</code></td>\n</tr>\n</tbody></table>\n<p>The state machine ensures jobs progress through well-defined paths. Terminal states (<code>COMPLETED</code>, <code>FAILED</code>, <code>DEAD_LETTER</code>) are final—jobs in these states are only kept for history and monitoring. The <code>FAILED</code> state is used for immediate failures (like validation errors) that shouldn&#39;t retry, while <code>DEAD_LETTER</code> is for exhausted retries.</p>\n<h4 id=\"configuration-objects\">Configuration Objects</h4>\n<p>System configuration is represented as typed objects rather than loose dictionaries, enabling validation and IDE support:</p>\n<p><strong>QueueConfig</strong> defines the behavior of a named queue:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Default</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>name</code></td>\n<td><code>str</code></td>\n<td>Queue identifier (must be alphanumeric with underscores).</td>\n<td>Required</td>\n</tr>\n<tr>\n<td><code>priority</code></td>\n<td><code>int</code></td>\n<td>Weight for weighted round-robin polling (higher = more frequent).</td>\n<td>1</td>\n</tr>\n<tr>\n<td><code>max_length</code></td>\n<td><code>Optional[int]</code></td>\n<td>Maximum jobs allowed in queue. <code>None</code> means unlimited.</td>\n<td><code>None</code></td>\n</tr>\n</tbody></table>\n<p><strong>WorkerConfig</strong> controls worker process behavior:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Default</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>queues</code></td>\n<td><code>List[str]</code></td>\n<td>List of queue names to poll (in priority order).</td>\n<td><code>[&quot;default&quot;]</code></td>\n</tr>\n<tr>\n<td><code>concurrency</code></td>\n<td><code>int</code></td>\n<td>Maximum parallel job executions (threads/processes).</td>\n<td>4</td>\n</tr>\n<tr>\n<td><code>heartbeat_interval</code></td>\n<td><code>int</code></td>\n<td>Seconds between heartbeat updates to Redis.</td>\n<td>30</td>\n</tr>\n<tr>\n<td><code>job_timeout</code></td>\n<td><code>int</code></td>\n<td>Default job timeout (seconds) if not specified per job.</td>\n<td>1800</td>\n</tr>\n</tbody></table>\n<p><strong>RedisConfig</strong> encapsulates Redis connection details:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Default</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>url</code></td>\n<td><code>str</code></td>\n<td>Redis connection URL (e.g., &quot;redis://localhost:6379/0&quot;).</td>\n<td>Required</td>\n</tr>\n<tr>\n<td><code>max_connections</code></td>\n<td><code>int</code></td>\n<td>Connection pool maximum size.</td>\n<td>20</td>\n</tr>\n<tr>\n<td><code>socket_timeout</code></td>\n<td><code>int</code></td>\n<td>Socket timeout in seconds.</td>\n<td>5</td>\n</tr>\n<tr>\n<td><code>retry_on_timeout</code></td>\n<td><code>bool</code></td>\n<td>Automatically retry on connection timeout.</td>\n<td><code>True</code></td>\n</tr>\n</tbody></table>\n<p><strong>SystemConfig</strong> is the root configuration container:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Default</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>redis</code></td>\n<td><code>RedisConfig</code></td>\n<td>Redis connection configuration.</td>\n<td>Required</td>\n</tr>\n<tr>\n<td><code>queues</code></td>\n<td><code>List[QueueConfig]</code></td>\n<td>All queue definitions in the system.</td>\n<td><code>[]</code></td>\n</tr>\n<tr>\n<td><code>workers</code></td>\n<td><code>List[WorkerConfig]</code></td>\n<td>Worker process configurations.</td>\n<td><code>[]</code></td>\n</tr>\n<tr>\n<td><code>max_payload_size</code></td>\n<td><code>int</code></td>\n<td>Maximum job payload size in bytes.</td>\n<td>1,048,576</td>\n</tr>\n<tr>\n<td><code>job_history_size</code></td>\n<td><code>int</code></td>\n<td>Number of completed jobs to retain in history.</td>\n<td>10,000</td>\n</tr>\n<tr>\n<td><code>retry_base_delay</code></td>\n<td><code>int</code></td>\n<td>Base delay in seconds for exponential backoff.</td>\n<td>1</td>\n</tr>\n<tr>\n<td><code>retry_max_attempts</code></td>\n<td><code>int</code></td>\n<td>Global default maximum retry attempts.</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<h4 id=\"redis-client-wrapper\">Redis Client Wrapper</h4>\n<p>The <code>RedisClient</code> class provides a thread-safe, connection-pooled interface to Redis with automatic error handling and retry logic:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>url</code></td>\n<td><code>str</code></td>\n<td>Redis connection URL (read-only).</td>\n</tr>\n<tr>\n<td><code>connection_kwargs</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Additional connection parameters (read-only).</td>\n</tr>\n<tr>\n<td><code>_client</code></td>\n<td><code>Optional[redis.Redis]</code></td>\n<td>Private Redis client instance (lazy-initialized).</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>get_instance</code></td>\n<td><code>url: Optional[str] = None, **kwargs</code></td>\n<td><code>RedisClient</code></td>\n<td>Singleton factory method ensuring single connection pool per configuration.</td>\n</tr>\n<tr>\n<td><code>get_client</code></td>\n<td>None</td>\n<td><code>redis.Redis</code></td>\n<td>Returns underlying Redis client (initializes if needed).</td>\n</tr>\n<tr>\n<td><code>execute</code></td>\n<td><code>command: str, *args, **kwargs</code></td>\n<td><code>Any</code></td>\n<td>Executes Redis command with automatic connection retry and error logging.</td>\n</tr>\n<tr>\n<td><code>pipeline</code></td>\n<td>None</td>\n<td><code>redis.Pipeline</code></td>\n<td>Returns Redis pipeline for atomic multi-command execution.</td>\n</tr>\n</tbody></table>\n<p>The singleton pattern ensures all parts of the application share the same connection pool, preventing connection exhaustion. The <code>execute</code> method wraps all Redis operations with consistent error handling—transient network errors trigger retries with exponential backoff, while logical errors (like wrong command syntax) raise immediate exceptions.</p>\n<h4 id=\"object-relationships\">Object Relationships</h4>\n<p>The data model relationships form a clear ownership hierarchy (as shown in <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fdata-model-relationships.svg\" alt=\"Data Model Relationships\">):</p>\n<ol>\n<li><strong>SystemConfig owns RedisConfig, multiple QueueConfigs, and multiple WorkerConfigs</strong></li>\n<li><strong>Each Job references a QueueConfig by name</strong></li>\n<li><strong>Each WorkerConfig references multiple QueueConfigs by name</strong></li>\n<li><strong>RedisClient is a singleton service used by all components</strong></li>\n<li><strong>Job objects are created by producers, stored in Redis, loaded by workers, and updated throughout lifecycle</strong></li>\n</ol>\n<p>This structure minimizes coupling—components only need references to what they directly use. For example, a worker only needs its <code>WorkerConfig</code> and a <code>RedisClient</code> instance; it doesn&#39;t need the full <code>SystemConfig</code>.</p>\n<h3 id=\"job-serialization-and-encoding\">Job Serialization and Encoding</h3>\n<p>Serialization is the <strong>language translator</strong> between our Python objects and Redis storage. It must preserve all data types, handle versioning, and be efficient for both small and large payloads. We face the classic tension between human readability (JSON) and efficiency (binary formats). Our design chooses JSON as the primary format with strict schema validation, augmented with custom encoders for special types like datetimes.</p>\n<h4 id=\"serialization-format-adr\">Serialization Format ADR</h4>\n<blockquote>\n<p><strong>Decision: JSON with Custom Type Encoders</strong></p>\n<ul>\n<li><strong>Context</strong>: Jobs contain diverse data types (strings, numbers, lists, dicts, datetimes, optional values). We need a serialization format that is human-readable for debugging, widely supported across languages (in case we add Go or Rust workers later), and efficient enough for typical job sizes (&lt; 1MB).</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>JSON</strong>: Human-readable, universal support, but limited type system (no datetime, no binary data)</li>\n<li><strong>MessagePack</strong>: Binary, compact, preserves some type hints, but less human-readable</li>\n<li><strong>Protocol Buffers</strong>: Strong typing, versioning, compact binary, but requires schema compilation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use JSON with custom encoders/decoders for extended types (datetime, bytes as base64).</li>\n<li><strong>Rationale</strong>: Debuggability is critical during development—engineers need to inspect queues with <code>redis-cli</code> and understand the data. JSON&#39;s ubiquity means any language can eventually consume our queues. The 1MB payload limit means space efficiency is secondary. We can always add MessagePack as an optimization later while keeping JSON as a fallback.</li>\n<li><strong>Consequences</strong>: Slightly larger payloads than binary formats. Datetimes are serialized to ISO 8601 strings. Binary data must be base64-encoded. We must implement robust custom encoders/decoders.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON</td>\n<td>Human-readable, universal, no schema needed, easy debugging</td>\n<td>Larger size, limited types, slower parsing than binary</td>\n<td><strong>Primary</strong></td>\n</tr>\n<tr>\n<td>MessagePack</td>\n<td>Compact, preserves some types, fast parsing</td>\n<td>Not human-readable, less universal support</td>\n<td>Fallback option</td>\n</tr>\n<tr>\n<td>Protocol Buffers</td>\n<td>Strong typing, versioning, compact</td>\n<td>Schema management overhead, compilation step</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h4 id=\"serialization-schema\">Serialization Schema</h4>\n<p>The JSON schema for serialized jobs follows a strict structure with versioning for future evolution:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"$schema\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"http://json-schema.org/draft-07/schema#\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"required\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"v\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"job_id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"job_type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"args\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"kwargs\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"queue\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"properties\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"v\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"integer\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"const\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"job_id\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"pattern\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"^[0-9A-Z]{26}$\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"job_type\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"maxLength\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"args\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"array\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"maxItems\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"kwargs\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"maxProperties\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"queue\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"maxLength\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"priority\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"integer\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"minimum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">-100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"maximum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"max_retries\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"integer\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"minimum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"maximum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"timeout_seconds\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"integer\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"minimum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"maximum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">86400</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"created_at\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"format\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"date-time\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"metadata\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"additionalProperties\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"status\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"enum\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"PENDING\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"ACTIVE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"COMPLETED\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"FAILED\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"RETRY_SCHEDULED\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"DEAD_LETTER\"</span><span style=\"color:#E1E4E8\">]},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"attempts\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"integer\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"minimum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"errors\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"array\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"items\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"required\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"timestamp\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"exception\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"message\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"properties\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"timestamp\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"format\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"date-time\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"exception\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"message\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"traceback\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"started_at\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null\"</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">\"format\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"date-time\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"completed_at\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null\"</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">\"format\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"date-time\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"result\"</span><span style=\"color:#E1E4E8\">: {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"additionalProperties\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>The <code>v</code> field (version) allows future schema migrations—if we need to change the format, we can detect version 1 and apply migration logic. All fields have reasonable size limits to prevent abuse (e.g., <code>args</code> limited to 100 items, <code>kwargs</code> to 50 properties).</p>\n<h4 id=\"type-encoding-rules\">Type Encoding Rules</h4>\n<p>Special Python types require custom encoding rules to survive JSON serialization:</p>\n<table>\n<thead>\n<tr>\n<th>Python Type</th>\n<th>JSON Representation</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>datetime</code></td>\n<td>ISO 8601 string with timezone (e.g., &quot;2023-01-15T12:30:45.123456Z&quot;)</td>\n<td>Always UTC, microsecond precision</td>\n</tr>\n<tr>\n<td><code>bytes</code></td>\n<td>Base64-encoded string</td>\n<td>Used sparingly; large binaries should be stored elsewhere</td>\n</tr>\n<tr>\n<td><code>Decimal</code></td>\n<td>String representation</td>\n<td>Preserves exact decimal values without float rounding</td>\n</tr>\n<tr>\n<td><code>set</code></td>\n<td>Array</td>\n<td>Order is not preserved (sets are unordered)</td>\n</tr>\n<tr>\n<td><code>frozenset</code></td>\n<td>Array</td>\n<td>Same as set</td>\n</tr>\n<tr>\n<td><code>tuple</code></td>\n<td>Array</td>\n<td>Preserved as list on deserialization (JSON has no tuple type)</td>\n</tr>\n<tr>\n<td><code>Enum</code></td>\n<td>String value (<code>.value</code> property)</td>\n<td>Requires Enum to have string values</td>\n</tr>\n<tr>\n<td><code>UUID</code></td>\n<td>Canonical string representation (8-4-4-4-12)</td>\n<td>Lowercase, no braces</td>\n</tr>\n<tr>\n<td><code>None</code></td>\n<td><code>null</code></td>\n<td>Standard JSON null</td>\n</tr>\n</tbody></table>\n<p>The serialization system uses a custom JSON encoder class that extends <code>json.JSONEncoder</code>, overriding the <code>default</code> method to handle these special types. During deserialization, we use a custom object hook that recognizes encoded types by structure (e.g., a dict with <code>{&quot;__type__&quot;: &quot;datetime&quot;, &quot;value&quot;: &quot;...&quot;}</code>) or by convention (ISO datetime strings can be auto-detected by regex).</p>\n<h4 id=\"serialization-methods\">Serialization Methods</h4>\n<p>The <code>Job</code> class provides symmetric serialization methods that handle the complete round-trip:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Algorithm</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>to_dict</code></td>\n<td>None</td>\n<td><code>Dict[str, Any]</code></td>\n<td>1. Create dict with all job fields 2. Convert special types using encoding rules 3. Add version field <code>&quot;v&quot;: 1</code></td>\n</tr>\n<tr>\n<td><code>from_dict</code></td>\n<td><code>data: Dict[str, Any]</code></td>\n<td><code>Job</code></td>\n<td>1. Validate required fields exist 2. Check version compatibility 3. Decode special types using decoding rules 4. Return new Job instance</td>\n</tr>\n<tr>\n<td><code>serialize</code></td>\n<td>None</td>\n<td><code>str</code></td>\n<td>1. Call <code>to_dict()</code> 2. Convert dict to JSON string with compact separation 3. Validate size against <code>max_payload_size</code></td>\n</tr>\n<tr>\n<td><code>deserialize</code></td>\n<td><code>data: str</code></td>\n<td><code>Job</code></td>\n<td>1. Parse JSON string to dict 2. Call <code>from_dict()</code> with parsed data</td>\n</tr>\n<tr>\n<td><code>record_error</code></td>\n<td><code>error: Exception</code></td>\n<td><code>None</code></td>\n<td>1. Create error dict with timestamp, exception class name, message, and traceback 2. Append to <code>errors</code> list 3. Update status if needed</td>\n</tr>\n<tr>\n<td><code>should_retry</code></td>\n<td>None</td>\n<td><code>bool</code></td>\n<td>1. Return <code>True</code> if status is <code>FAILED</code> and <code>attempts &lt; max_retries</code> 2. Return <code>False</code> otherwise</td>\n</tr>\n</tbody></table>\n<p>The <code>serialize</code> method includes size validation—if the serialized JSON exceeds <code>max_payload_size</code> (default 1MB), it raises a <code>JobTooLargeError</code> before attempting to store in Redis. This prevents queue clogging with oversized jobs.</p>\n<h4 id=\"common-pitfalls-in-data-modeling\">Common Pitfalls in Data Modeling</h4>\n<p>⚠️ <strong>Pitfall: Storing unserializable objects in job arguments</strong></p>\n<ul>\n<li><strong>Description</strong>: Passing database connections, file handles, or complex ORM objects in <code>args</code> or <code>kwargs</code>.</li>\n<li><strong>Why it&#39;s wrong</strong>: These objects can&#39;t be serialized to JSON. The job fails during enqueue with cryptic serialization errors, or worse, serializes to something meaningless that causes runtime failures.</li>\n<li><strong>Fix</strong>: Only pass primitive types, dicts, lists, or simple data objects. If you need database access, pass record IDs and fetch fresh connections in the job handler.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Modifying serialized job in queue</strong></p>\n<ul>\n<li><strong>Description</strong>: Trying to update a job&#39;s properties after it&#39;s been enqueued by modifying the JSON in the Redis list.</li>\n<li><strong>Why it&#39;s wrong</strong>: The job in the queue is immutable by design. Multiple workers might have already read the serialized data, and updates won&#39;t be reflected. The correct place for mutable state is the <code>job:{id}</code> hash.</li>\n<li><strong>Fix</strong>: Store mutable state in the job hash, not in the queue payload. Use atomic updates to the hash when job state changes.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Forgetting timezone awareness</strong></p>\n<ul>\n<li><strong>Description</strong>: Storing datetimes as naive (timezone-unaware) objects or using local system time.</li>\n<li><strong>Why it&#39;s wrong</strong>: Workers running in different timezones will interpret scheduled times differently. Daylight saving transitions cause duplicates or gaps.</li>\n<li><strong>Fix</strong>: Always use UTC for storage. Convert to local time only for display. Use <code>datetime.utcnow()</code> not <code>datetime.now()</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Inconsistent ULID generation</strong></p>\n<ul>\n<li><strong>Description</strong>: Generating ULIDs on different machines without synchronized clocks or using random components incorrectly.</li>\n<li><strong>Why it&#39;s wrong</strong>: ULIDs lose their time-sortable property if clocks are skewed. Jobs might appear out of order.</li>\n<li><strong>Fix</strong>: Use a ULID library that handles clock synchronization (monotonic counter for same-millisecond collisions). Ensure system clocks are synchronized with NTP.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not planning for schema evolution</strong></p>\n<ul>\n<li><strong>Description</strong>: Serializing jobs without version field, assuming the schema will never change.</li>\n<li><strong>Why it&#39;s wrong</strong>: When you need to add a new field or change encoding, existing jobs in queues become unreadable.</li>\n<li><strong>Fix</strong>: Include version field (<code>&quot;v&quot;: 1</code>) in all serialized jobs. Write migration code that can upgrade old versions on deserialization.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON Serialization</td>\n<td>Python&#39;s built-in <code>json</code> module with custom encoder</td>\n<td><code>orjson</code> for 10x faster serialization (C-optimized)</td>\n</tr>\n<tr>\n<td>ULID Generation</td>\n<td><code>python-ulid</code> library (pure Python)</td>\n<td><code>ulid-py</code> with C extensions for performance</td>\n</tr>\n<tr>\n<td>Redis Client</td>\n<td><code>redis-py</code> with connection pooling</td>\n<td><code>redis-py</code> with client-side caching enabled</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td><code>pydantic</code> for validation and type checking</td>\n<td>Custom YAML/TOML loader with environment variable override</td>\n</tr>\n<tr>\n<td>Date/Time</td>\n<td><code>pendulum</code> for timezone-aware operations</td>\n<td>Python 3.9+ <code>zoneinfo</code> with <code>pytz</code> compatibility layer</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background_jobs/\n├── __init__.py\n├── config/\n│   ├── __init__.py\n│   ├── models.py           # QueueConfig, WorkerConfig, RedisConfig, SystemConfig\n│   └── loader.py           # SystemConfig.from_env(), from_yaml()\n├── models/\n│   ├── __init__.py\n│   ├── job.py              # Job class with serialization methods\n│   ├── job_status.py       # JobStatus enum\n│   └── errors.py           # Custom exceptions (JobTooLargeError, etc.)\n├── storage/\n│   ├── __init__.py\n│   ├── redis_client.py     # RedisClient singleton wrapper\n│   ├── serialization.py    # Custom JSON encoder/decoder for special types\n│   └── keys.py             # Key generation utilities (e.g., job_key(), queue_key())\n└── utils/\n    ├── __init__.py\n    ├── ulid_generator.py   # ULID generation with monotonic counter\n    └── validation.py       # Payload size validation, queue name validation</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Redis Client Wrapper</strong> (<code>storage/redis_client.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Any, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> threading </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Lock</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe Redis client singleton with connection pooling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _instances: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _lock: Lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">connection_kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_kwargs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_kwargs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._client: Optional[redis.Redis] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_instance</span><span style=\"color:#E1E4E8\">(cls, url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> </span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get singleton instance for given URL and connection parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> url </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            url </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379/0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        instance_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(kwargs)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> instance_key </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instances:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                cls</span><span style=\"color:#E1E4E8\">._instances[instance_key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(url, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instances[instance_key]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_client</span><span style=\"color:#E1E4E8\">(self) -> redis.Redis:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Lazy initialization of Redis client with connection pool.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.from_url(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.url,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                **</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_kwargs,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Auto-decode bytes to str</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                health_check_interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test connection</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.ping()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Connected to Redis at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute</span><span style=\"color:#E1E4E8\">(self, command: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute Redis command with automatic retry on connection errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Get the method from client (e.g., client.get, client.set)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            method </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(client, command)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> method(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> (redis.ConnectionError, redis.TimeoutError) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis connection error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, retrying...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Reset client to force reconnection</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            method </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(client, command)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> method(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline</span><span style=\"color:#E1E4E8\">(self) -> redis.Pipeline:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return a Redis pipeline for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> client.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> close</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Close Redis connection (called during graceful shutdown).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.close()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<p><strong>Complete Custom JSON Encoder/Decoder</strong> (<code>storage/serialization.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> base64</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, date, time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> decimal </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Decimal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> uuid </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> UUID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobEncoder</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">json</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JSONEncoder</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Custom JSON encoder for Job serialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> default</span><span style=\"color:#E1E4E8\">(self, obj: Any) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle datetime objects (convert to ISO format with timezone)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, datetime):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Ensure datetime is timezone-aware (UTC)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> obj.tzinfo </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                obj </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> obj.replace(</span><span style=\"color:#FFAB70\">tzinfo</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timezone.utc)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> obj.isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle date objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, date):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> obj.isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle bytes (base64 encode)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"__type__\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"bytes\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"value\"</span><span style=\"color:#E1E4E8\">: base64.b64encode(obj).decode(</span><span style=\"color:#9ECBFF\">'ascii'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle Decimal (string representation)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, Decimal):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(obj)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle UUID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, </span><span style=\"color:#79B8FF\">UUID</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(obj)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle Enum (store value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, Enum):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> obj.value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Let base class handle other types (will raise TypeError)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> super</span><span style=\"color:#E1E4E8\">().default(obj)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> job_object_hook</span><span style=\"color:#E1E4E8\">(dct: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Custom object hook for JSON decoding to restore special types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for encoded bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#9ECBFF\"> \"__type__\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> dct </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> dct[</span><span style=\"color:#9ECBFF\">\"__type__\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"bytes\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> base64.b64decode(dct[</span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Auto-detect ISO datetime strings (naive or with timezone)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> dct.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(value, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Try to parse as datetime (ISO format)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Check if it looks like an ISO datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#9ECBFF\"> \"T\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"-\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> \":\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> value):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Parse with datetime.fromisoformat (Python 3.7+)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Note: This handles timezone-aware strings in Python 3.11+</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    dt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.fromisoformat(value.replace(</span><span style=\"color:#9ECBFF\">'Z'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'+00:00'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    dct[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">ValueError</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">AttributeError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> dct</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> serialize_job</span><span style=\"color:#E1E4E8\">(job_dict: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Serialize job dictionary to JSON string with custom encoding.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> json.dumps(job_dict, </span><span style=\"color:#FFAB70\">cls</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">JobEncoder, </span><span style=\"color:#FFAB70\">separators</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">','</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">':'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> deserialize_job</span><span style=\"color:#E1E4E8\">(json_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Deserialize JSON string to job dictionary with custom decoding.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> json.loads(json_str, </span><span style=\"color:#FFAB70\">object_hook</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_object_hook)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Job Class with Serialization Methods</strong> (<code>models/job.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .job_status </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> storage.serialization </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> serialize_job, deserialize_job</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Job</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Core job representation with serialization capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        args: List[Any],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kwargs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1800</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        created_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        status: JobStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errors: Optional[List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result: Optional[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate required fields (job_id, job_type, queue)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set created_at to UTC now if not provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize metadata as empty dict if None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize errors as empty list if None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store all parameters as instance attributes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert job to dictionary for serialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create dictionary with all instance attributes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert datetime objects using ISO format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add version field \"v\": 1 for schema evolution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle Optional fields (skip if None or use null)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return dictionary ready for JSON serialization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reconstruct job from dictionary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate required fields exist in data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check version compatibility (data.get(\"v\") == 1)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Convert string dates back to datetime objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create and return new Job instance with loaded data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle missing optional fields with defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> serialize</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize job to JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Call to_dict() to get dictionary representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use serialize_job() helper to convert to JSON string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate size against max_payload_size (1MB default)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Raise JobTooLargeError if payload exceeds limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the JSON string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> deserialize</span><span style=\"color:#E1E4E8\">(cls, data: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deserialize job from JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use deserialize_job() helper to parse JSON string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call from_dict() with the parsed dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return the reconstructed Job instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle JSON parsing errors with descriptive exceptions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_error</span><span style=\"color:#E1E4E8\">(self, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record an error that occurred during job execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create error dictionary with keys:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - timestamp: current UTC datetime</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - exception: error class name (type(error).__name__)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - message: str(error)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - traceback: formatted traceback string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Append error dictionary to self.errors list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update self.status based on retry logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Increment self.attempts counter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_retry</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if job should be retried.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if status is FAILED (not DEAD_LETTER)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare attempts to max_retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return True if attempts &#x3C; max_retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_next_retry</span><span style=\"color:#E1E4E8\">(self, base_delay: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> datetime:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate next retry time using exponential backoff.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute delay = base_delay * (2 ** (self.attempts - 1))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add optional jitter (±10% of delay)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return current time + delay as datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Configuration Models</strong> (<code>config/models.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for a named queue.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate queue name (alphanumeric + underscores)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate priority is non-negative integer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate max_length is positive if not None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WorkerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for a worker process.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queues: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    concurrency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    heartbeat_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1800</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate queues list is non-empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate concurrency is positive integer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate heartbeat_interval is between 5 and 300 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate job_timeout is positive integer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Redis connection configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    url: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_connections: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 20</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    socket_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_timeout: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate URL format (starts with redis://)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate max_connections is positive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate socket_timeout is positive</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SystemConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Root system configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis: RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queues: List[QueueConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    workers: List[WorkerConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_payload_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1_048_576</span><span style=\"color:#6A737D\">  # 1MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_history_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10_000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_base_delay: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_env</span><span style=\"color:#E1E4E8\">(cls) -> </span><span style=\"color:#9ECBFF\">'SystemConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from environment variables with defaults.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Read REDIS_URL from environment (default: redis://localhost:6379/0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse queue definitions from QUEUES environment variable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse worker configurations from WORKERS environment variable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create RedisConfig instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create QueueConfig instances from parsed data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create WorkerConfig instances from parsed data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return SystemConfig instance with all components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<ol>\n<li><strong>Use Python 3.8+</strong> for dataclasses with <code>field(default_factory=...)</code> support</li>\n<li><strong>For ULIDs</strong>, install <code>python-ulid</code> package: <code>pip install python-ulid</code></li>\n<li><strong>Use <code>datetime.timezone.utc</code></strong> for timezone-aware UTC timestamps (Python 3.2+)</li>\n<li><strong>For JSON serialization</strong>, <code>json.dumps()</code> with <code>default</code> parameter handles custom types</li>\n<li><strong>Use <code>__post_init__</code> method</strong> in dataclasses for validation logic</li>\n<li><strong>Implement <code>__hash__</code> and <code>__eq__</code></strong> for Job if you need to store in sets/dicts</li>\n<li><strong>Use <code>functools.lru_cache</code></strong> for expensive computations like cron expression parsing</li>\n<li><strong>For thread safety</strong>, use <code>threading.Lock</code> in singleton patterns</li>\n</ol>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the data model (Milestone 1 foundation), run this validation script:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test job serialization round-trip</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_job_serialization.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_job_to_dict_roundtrip ... PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_job_serialize_deserialize ... PASSED  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_job_with_special_types ... PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_job_size_validation ... PASSED</span></span></code></pre></div>\n\n<p>Manual verification steps:</p>\n<ol>\n<li>Create a Job with various data types (string, int, list, dict, datetime)</li>\n<li>Call <code>job.serialize()</code> - should return JSON string</li>\n<li>Call <code>Job.deserialize()</code> on that string - should return identical Job</li>\n<li>Check that <code>job.job_id</code> is a valid ULID (26 characters, alphanumeric)</li>\n<li>Verify that datetimes are stored in ISO format with timezone indicator</li>\n<li>Attempt to serialize a 2MB payload - should raise <code>JobTooLargeError</code></li>\n</ol>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON serialization fails with &quot;TypeError: Object of type datetime is not JSON serializable&quot;</td>\n<td>Forgot to use custom encoder or passed datetime directly to <code>json.dumps()</code></td>\n<td>Check if you&#39;re using <code>JobEncoder</code> or calling <code>serialize()</code> method</td>\n<td>Use <code>job.serialize()</code> instead of <code>json.dumps(job)</code></td>\n</tr>\n<tr>\n<td>Job appears in queue but workers can&#39;t process it</td>\n<td>Job schema changed or version mismatch</td>\n<td>Check Redis: <code>redis-cli LRANGE job_queue:default 0 0</code> to see raw JSON</td>\n<td>Ensure all components use same schema version</td>\n</tr>\n<tr>\n<td>ULIDs not sorting chronologically</td>\n<td>Clock skew between machines or incorrect ULID generation</td>\n<td>Compare ULID timestamps with system clock</td>\n<td>Use NTP for clock sync, ensure ULID lib uses monotonic counter</td>\n</tr>\n<tr>\n<td>Datetimes showing wrong timezone</td>\n<td>Naive datetime objects or mixing UTC/local time</td>\n<td>Check <code>created_at</code> field in serialized JSON for timezone indicator</td>\n<td>Always use <code>datetime.now(timezone.utc)</code> not <code>datetime.utcnow()</code></td>\n</tr>\n<tr>\n<td>Redis connections exhausted</td>\n<td>Creating new RedisClient instance per job instead of singleton</td>\n<td>Check Redis <code>CLIENT LIST</code> command - too many connections</td>\n<td>Use <code>RedisClient.get_instance()</code> singleton pattern</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-design-job-queue-core\">Component Design: Job Queue Core</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1: Job Queue Core. This section provides the complete design for the foundational queueing system that enables producers to enqueue jobs and workers to retrieve them. It establishes the core job representation, queue management, and priority handling that all other components build upon.</p>\n</blockquote>\n<h3 id=\"mental-model-post-office-sorting-facility\">Mental Model: Post Office Sorting Facility</h3>\n<p>Imagine a large, well-organized post office that handles packages (jobs) from senders (producers) to recipients (job handlers). This mental model helps visualize the core queueing concepts:</p>\n<ul>\n<li><p><strong>Mail Collection Point (Enqueue Interface):</strong> Senders bring packages to the counter. Each package has a destination address (queue name) and a priority sticker (priority). The clerk (Queue Manager) validates the package isn&#39;t too large or improperly addressed, then stamps it with a unique tracking number (job ID) and places it in the correct sorting bin.</p>\n</li>\n<li><p><strong>Sorting Bins (Queues in Redis):</strong> The post office has multiple wall-mounted bins, each labeled with a destination city (queue name). Some bins are marked &quot;Express&quot; (high priority) while others are &quot;Standard&quot; (normal priority). Within each bin, packages are stacked in order of arrival (FIFO). The facility has a policy that bins can only hold a certain number of packages to prevent overloading.</p>\n</li>\n<li><p><strong>Sorting Machine (Priority Algorithm):</strong> A sorting machine (worker polling algorithm) continuously checks bins for packages to process. It doesn&#39;t just check bins in random order—it has a weighted schedule: it checks the &quot;Express&quot; bins twice as often as &quot;Standard&quot; bins. This ensures urgent packages move faster without completely starving regular ones.</p>\n</li>\n<li><p><strong>Package Manifest (Job Serialization):</strong> Each package contains a detailed manifest (serialized job payload) listing the contents (arguments), handling instructions (job type), and special requirements (timeout, retries). The manifest uses a standardized format (JSON) that anyone in the postal system can read, ensuring packages can be processed by any worker station.</p>\n</li>\n</ul>\n<p>This mental model emphasizes several critical design aspects: <strong>validation at entry</strong> prevents problems downstream, <strong>standardized packaging</strong> enables interoperability, <strong>organized storage</strong> ensures efficient retrieval, and <strong>intelligent sorting</strong> respects priority without starvation. The post office doesn&#39;t process packages itself—it merely stores and routes them to available workers, just as our queue core doesn&#39;t execute jobs but manages their lifecycle until pickup.</p>\n<h3 id=\"queue-manager-interface\">Queue Manager Interface</h3>\n<p>The Queue Manager is the central orchestration component that mediates all job enqueue operations. It provides a clean, abstract interface for producers while handling the complex details of validation, serialization, and atomic Redis operations. Think of it as the &quot;front desk&quot; of our post office—all interactions with the queue system go through this interface.</p>\n<p>The Queue Manager&#39;s primary responsibilities are:</p>\n<ol>\n<li><strong>Job Validation:</strong> Ensuring jobs meet size and schema constraints before acceptance</li>\n<li><strong>Serialization:</strong> Converting in-memory <code>Job</code> objects to a storage-ready format</li>\n<li><strong>Atomic Enqueue:</strong> Safely adding jobs to Redis with proper priority handling</li>\n<li><strong>Queue Inspection:</strong> Providing visibility into queue state without modifying it</li>\n<li><strong>Configuration Management:</strong> Applying queue-specific rules (max length, priority)</li>\n</ol>\n<p>The complete interface is defined in the following table:</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>enqueue_job</code></td>\n<td><code>job: Job</code></td>\n<td><code>str</code> (job_id)</td>\n<td>Validates, serializes, and atomically adds a job to the appropriate Redis queue. Returns the unique job ID. Raises <code>ValidationError</code> for oversized/malformed jobs, <code>QueueFullError</code> if queue at capacity.</td>\n</tr>\n<tr>\n<td><code>bulk_enqueue</code></td>\n<td><code>jobs: List[Job]</code></td>\n<td><code>List[str]</code> (job_ids)</td>\n<td>Enqueues multiple jobs in a single atomic transaction using Redis pipeline. Returns list of job IDs in same order. If any job fails validation, the entire operation rolls back.</td>\n</tr>\n<tr>\n<td><code>get_queue_length</code></td>\n<td><code>queue_name: str</code></td>\n<td><code>int</code></td>\n<td>Returns the current number of pending jobs in the specified queue. Counts only jobs in the main pending queue (not active, retry, or dead letter).</td>\n</tr>\n<tr>\n<td><code>peek_queue</code></td>\n<td><code>queue_name: str</code>, <code>count: int = 10</code></td>\n<td><code>List[Job]</code></td>\n<td>Retrieves up to <code>count</code> jobs from the front of the queue without removing them. Useful for debugging and monitoring. Jobs are deserialized back to <code>Job</code> objects.</td>\n</tr>\n<tr>\n<td><code>list_queues</code></td>\n<td><code>include_internal: bool = False</code></td>\n<td><code>List[str]</code></td>\n<td>Returns names of all configured queues. When <code>include_internal</code> is True, also returns system queues (retry, dead letter, scheduler).</td>\n</tr>\n<tr>\n<td><code>delete_job</code></td>\n<td><code>job_id: str</code>, <code>queue_name: str</code></td>\n<td><code>bool</code></td>\n<td>Removes a specific job from a queue if it exists. Used for manual job management. Returns True if job was found and removed.</td>\n</tr>\n<tr>\n<td><code>get_job_by_id</code></td>\n<td><code>job_id: str</code></td>\n<td><code>Optional[Job]</code></td>\n<td>Retrieves a job from Redis by its ID, regardless of which queue or state it&#39;s in. Returns None if job doesn&#39;t exist.</td>\n</tr>\n<tr>\n<td><code>validate_job</code></td>\n<td><code>job: Job</code></td>\n<td><code>None</code></td>\n<td>Internal method that checks job size (against <code>max_payload_size</code>), required fields, and serializability. Raises <code>ValidationError</code> with specific reason.</td>\n</tr>\n</tbody></table>\n<p>The Queue Manager maintains an internal mapping of queue configurations (<code>QueueConfig</code> objects) that define each queue&#39;s behavior. When a job is enqueued, the manager:</p>\n<ol>\n<li>Validates the job payload size and structure</li>\n<li>Generates a unique job ID (ULID for time-ordered uniqueness)</li>\n<li>Sets the job&#39;s initial status to <code>PENDING</code> and timestamps</li>\n<li>Serializes the job to JSON format</li>\n<li>Atomically pushes the serialized job to the appropriate Redis list</li>\n<li>Updates queue metrics for monitoring</li>\n</ol>\n<p>This interface deliberately exposes only queue management operations—not worker operations like dequeue, which are handled by the Worker component. This separation of concerns ensures the queue manager remains focused on producer-side concerns while workers handle consumer-side logic.</p>\n<h3 id=\"enqueue-and-priority-algorithm\">Enqueue and Priority Algorithm</h3>\n<p>The algorithm for enqueuing jobs with priority support is more nuanced than a simple <code>LPUSH</code> to a Redis list. Our system supports multiple named queues with configurable priority weights, which means workers should poll higher-priority queues more frequently. However, we must maintain strict FIFO ordering <em>within</em> each queue. The algorithm achieves this through a combination of Redis operations and weighted polling logic in workers.</p>\n<h4 id=\"step-by-step-enqueue-algorithm\">Step-by-Step Enqueue Algorithm</h4>\n<p>When a producer calls <code>enqueue_job(job)</code>:</p>\n<ol>\n<li><p><strong>Validation Phase:</strong></p>\n<ul>\n<li>Check if job payload size exceeds <code>SystemConfig.max_payload_size</code> (default 1MB)</li>\n<li>Verify all required <code>Job</code> fields are present: <code>job_type</code>, <code>args</code>, <code>kwargs</code>, <code>queue</code></li>\n<li>Ensure the target queue exists in configuration (or create with default priority)</li>\n<li>If validation fails, raise <code>ValidationError</code> with specific message</li>\n</ul>\n</li>\n<li><p><strong>Job Preparation:</strong></p>\n<ul>\n<li>Generate job ID using ULID (Universally Unique Lexicographically Sortable Identifier)<ul>\n<li>ULID provides time-ordered uniqueness: <code>01J5XZR0W3F6Y2A4B8C9D10E11</code> (26 chars)</li>\n<li>First 10 characters are timestamp (milliseconds since epoch in base32)</li>\n<li>Remaining 16 characters are random bytes, ensuring uniqueness even at same millisecond</li>\n</ul>\n</li>\n<li>Set <code>job.job_id</code> to generated ULID</li>\n<li>Set <code>job.status</code> to <code>PENDING</code></li>\n<li>Set <code>job.created_at</code> to current UTC datetime</li>\n<li>Initialize <code>job.attempts = 0</code>, <code>job.errors = []</code></li>\n<li>Set <code>job.started_at</code>, <code>job.completed_at</code>, <code>job.result</code> to <code>None</code></li>\n</ul>\n</li>\n<li><p><strong>Serialization:</strong></p>\n<ul>\n<li>Convert <code>Job</code> object to dictionary via <code>Job.to_dict()</code></li>\n<li>Convert datetime objects to ISO 8601 strings for JSON compatibility</li>\n<li>Serialize dictionary to JSON string via <code>Job.serialize()</code></li>\n<li>Example serialized job:</li>\n</ul>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">     {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"job_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"01J5XZR0W3F6Y2A4B8C9D10E11\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"job_type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"process_image\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"args\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"photo.jpg\"</span><span style=\"color:#E1E4E8\">, {</span><span style=\"color:#79B8FF\">\"width\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">800</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"height\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">600</span><span style=\"color:#E1E4E8\">}],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"kwargs\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"quality\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">90</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"queue\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"image_processing\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"priority\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"max_retries\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"timeout_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"created_at\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"2024-01-15T10:30:00Z\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"metadata\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"user_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">12345</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"batch_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"batch-001\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"PENDING\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"attempts\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"errors\"</span><span style=\"color:#E1E4E8\">: [],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"started_at\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">null</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"completed_at\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">null</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       \"result\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">null</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">     }</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Atomic Enqueue to Redis:</strong><ul>\n<li>Use Redis pipeline for atomic operation sequence:<ol>\n<li><code>HSET job:{job_id} {serialized_job}</code> - Store job in Redis hash for later retrieval</li>\n<li><code>EXPIRE job:{job_id} {job_ttl}</code> - Set TTL (e.g., 7 days) to prevent memory bloat</li>\n<li><code>LPUSH queue:{queue_name} {job_id}</code> - Add job ID to the queue&#39;s list</li>\n<li><code>INCR queue:{queue_name}:count</code> - Increment queue length metric</li>\n</ol>\n</li>\n<li>If any step fails, the entire pipeline is rolled back</li>\n<li>Return the generated job ID to the producer</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"priority-weighted-polling-algorithm-worker-side\">Priority Weighted Polling Algorithm (Worker-Side)</h4>\n<p>While the enqueue algorithm handles producer-side operations, the priority system requires special consideration for how workers poll queues. The Queue Manager doesn&#39;t implement this directly, but it influences the algorithm through queue configuration. Here&#39;s how priority-weighted polling works:</p>\n<ol>\n<li><strong>Queue Configuration Example:</strong></li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">   queues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"critical\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">),    </span><span style=\"color:#6A737D\"># High priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">),     </span><span style=\"color:#6A737D\"># Medium priority  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"low\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),         </span><span style=\"color:#6A737D\"># Low priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"reports\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)      </span><span style=\"color:#6A737D\"># Also low priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   ]</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Worker Polling Logic:</strong><ul>\n<li>Workers maintain a list of queues sorted by priority weight</li>\n<li>They calculate polling frequency: <code>poll_count = priority_weight / total_weight</code></li>\n<li>For the above configuration:<ul>\n<li>Total weight = 3 + 2 + 1 + 1 = 7</li>\n<li>&quot;critical&quot; gets 3/7 ≈ 43% of polls</li>\n<li>&quot;default&quot; gets 2/7 ≈ 29% of polls</li>\n<li>&quot;low&quot; and &quot;reports&quot; each get 1/7 ≈ 14% of polls</li>\n</ul>\n</li>\n<li>Workers use Redis <code>BRPOP</code> with timeout, cycling through queues according to these weights</li>\n</ul>\n</li>\n</ol>\n<p>This approach ensures higher-priority queues are checked more frequently while maintaining FIFO within each queue. It&#39;s a trade-off: true priority queuing (where a high-priority job jumps ahead of all lower-priority jobs) would require a single sorted data structure, but that&#39;s more complex and reduces throughput. Our approach provides good enough priority handling for most use cases while maintaining simplicity and performance.</p>\n<blockquote>\n<p><strong>Key Insight:</strong> We separate job storage (Redis hash) from queue ordering (Redis list). The list contains only job IDs, not full job data. This reduces memory usage when moving jobs between queues (e.g., to retry queue) and allows efficient job lookup by ID. The trade-off is an extra Redis operation to fetch job data when a worker dequeues a job ID.</p>\n</blockquote>\n<h3 id=\"adr-job-storage-mechanism\">ADR: Job Storage Mechanism</h3>\n<blockquote>\n<p><strong>Decision: Separate Job Storage from Queue Ordering</strong></p>\n<p><strong>Context:</strong> We need to store complete job data (arguments, metadata, status history) while also maintaining queue ordering for efficient worker polling. Jobs may move between multiple queues during their lifecycle (pending → active → retry → dead letter), and we need efficient access to job data by ID for monitoring and management.</p>\n<p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Store full job data in queue lists:</strong> Each Redis list element contains the complete serialized job.</li>\n<li><strong>Separate storage with ID pointers:</strong> Store jobs in Redis hashes keyed by job ID, with queue lists containing only IDs.</li>\n<li><strong>Hybrid with metadata in lists:</strong> Store minimal metadata in lists with full data in separate storage.</li>\n</ol>\n<p><strong>Decision:</strong> Option 2 - Separate storage with ID pointers. Store complete job data in Redis hashes (<code>job:{job_id}</code>) and queue job IDs in Redis lists (<code>queue:{queue_name}</code>).</p>\n<p><strong>Rationale:</strong> This separation provides multiple benefits: (1) Moving jobs between queues is cheap—just move the ID, not the full payload; (2) Job lookup by ID is O(1) via hash get; (3) We can store additional job metadata in the hash without affecting queue operations; (4) Serialized job data is stored only once, reducing memory usage; (5) It aligns with Redis best practices for modeling relationships.</p>\n<p><strong>Consequences:</strong> Positive: Efficient queue operations and job management. Negative: Requires two Redis operations to dequeue (get ID from list, then get data from hash). This is acceptable because the extra operation&#39;s cost is minimal compared to job execution time, and we can mitigate it with pipelining.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full data in lists</td>\n<td>Single operation to dequeue, simpler implementation</td>\n<td>Moving jobs between queues requires re-serializing and copying data, memory inefficient with large payloads, no efficient job-by-ID lookup</td>\n<td>Too inefficient for job lifecycle management</td>\n</tr>\n<tr>\n<td><strong>Separate storage with IDs</strong></td>\n<td><strong>Efficient job movement, O(1) job lookup, memory efficient, flexible metadata</strong></td>\n<td><strong>Requires two operations to dequeue</strong></td>\n<td><strong>Chosen - benefits outweigh minor performance cost</strong></td>\n</tr>\n<tr>\n<td>Hybrid (metadata in lists)</td>\n<td>Balances lookup efficiency with queue performance</td>\n<td>Complex serialization/deserialization logic, still requires separate storage for full job data</td>\n<td>Adds complexity without clear benefits over option 2</td>\n</tr>\n</tbody></table>\n<h3 id=\"adr-queue-priority-strategy\">ADR: Queue Priority Strategy</h3>\n<blockquote>\n<p><strong>Decision: Weighted Round-Robin Polling</strong></p>\n<p><strong>Context:</strong> We need to support multiple queues with different priority levels. High-priority jobs should be processed sooner than low-priority ones, but we must avoid complete starvation of low-priority queues. The system should be predictable and configurable.</p>\n<p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Strict priority polling:</strong> Always check highest-priority queue first, only check lower queues when higher ones are empty.</li>\n<li><strong>Weighted random selection:</strong> Randomly select queue based on weights each poll.</li>\n<li><strong>Weighted round-robin:</strong> Systematically cycle through queues, visiting higher-priority queues more frequently.</li>\n<li><strong>Multiple Redis sorted sets:</strong> Single data structure with priority scores, requiring atomic ZADD and ZRANGE operations.</li>\n</ol>\n<p><strong>Decision:</strong> Option 3 - Weighted round-robin polling implemented in workers.</p>\n<p><strong>Rationale:</strong> Strict priority causes starvation. Weighted random lacks predictability. Multiple sorted sets add Redis complexity and atomicity challenges. Weighted round-robin provides predictable, configurable priority handling without starvation. It&#39;s simple to implement and understand: if queue A has priority 3 and queue B has priority 1, workers will check A three times for every one time they check B. This gives priority jobs 3x more polling attention while still guaranteeing B gets processed eventually.</p>\n<p><strong>Consequences:</strong> Positive: Predictable priority handling, no starvation, simple implementation. Negative: Not true priority queuing (a high-priority job enqueued after low-priority jobs in a different queue might still wait). This is acceptable for most background job scenarios where approximate priority is sufficient.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Strict priority polling</td>\n<td>Highest priority jobs always processed first</td>\n<td>Complete starvation of low-priority queues, poor resource utilization</td>\n<td>Unacceptable starvation risk</td>\n</tr>\n<tr>\n<td>Weighted random selection</td>\n<td>Simple to implement, probabilistic priority</td>\n<td>Unpredictable timing, can still starve low-priority queues by chance</td>\n<td>Lack of predictability problematic</td>\n</tr>\n<tr>\n<td><strong>Weighted round-robin</strong></td>\n<td><strong>Predictable, configurable, no starvation, simple</strong></td>\n<td><strong>Not true priority (FIFO within queue only)</strong></td>\n<td><strong>Chosen - best balance of fairness and priority</strong></td>\n</tr>\n<tr>\n<td>Multiple Redis sorted sets</td>\n<td>True priority across all jobs, single atomic operation</td>\n<td>Complex implementation, Redis contention, harder to debug</td>\n<td>Over-engineering for most use cases</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls-in-queue-implementation\">Common Pitfalls in Queue Implementation</h3>\n<p>⚠️ <strong>Pitfall: Not Validating Payload Size Before Redis Operations</strong></p>\n<p><strong>Description:</strong> Checking payload size only after serialization or during Redis operations. If a 5MB payload is serialized to JSON (making it even larger) and only rejected when Redis fails with &quot;OOM&quot; error or slow performance.</p>\n<p><strong>Why It&#39;s Wrong:</strong> Wastes CPU on serialization, creates unclear error messages, may cause partial failures in bulk operations, and could trigger Redis memory issues before validation catches it.</p>\n<p><strong>How to Fix:</strong> Validate payload size <em>before</em> any serialization. Calculate approximate JSON size by summing string lengths of arguments + fixed overhead. Implement in <code>QueueManager.validate_job()</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">estimated_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(json.dumps(job.args)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(json.dumps(job.kwargs)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 500</span><span style=\"color:#6A737D\">  # metadata overhead</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> estimated_size </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> config.max_payload_size:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    raise</span><span style=\"color:#E1E4E8\"> ValidationError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job payload exceeds </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">config.max_payload_size</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> bytes limit\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<hr>\n<p>⚠️ <strong>Pitfall: Forgetting TTL on Job Data</strong></p>\n<p><strong>Description:</strong> Storing jobs in Redis without expiration time, assuming they&#39;ll always be deleted after processing. However, jobs can get stuck (worker crashes, bugs) or monitoring queries may keep references, causing indefinite memory growth.</p>\n<p><strong>Why It&#39;s Wrong:</strong> Redis memory fills up over time, causing evictions or crashes. Production incidents often trace back to forgotten TTLs.</p>\n<p><strong>How to Fix:</strong> Always set TTL when storing jobs. Use a conservative value (e.g., 7 days) that exceeds maximum possible job lifetime. In Redis pipeline:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">pipeline.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">serialized_job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">pipeline.expire(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">604800</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 7 days in seconds</span></span></code></pre></div>\n\n<hr>\n<p>⚠️ <strong>Pitfall: Non-Atomic Multi-Step Queue Operations</strong></p>\n<p><strong>Description:</strong> Performing separate Redis commands for storing job data and adding to queue list. If system crashes between commands, job data may be stored but not enqueued (orphaned) or enqueued without data (worker will fail).</p>\n<p><strong>Why It&#39;s Wrong:</strong> Creates inconsistent state that&#39;s hard to detect and recover from. Orphaned jobs waste memory; missing data causes worker crashes.</p>\n<p><strong>How to Fix:</strong> Use Redis pipelines or transactions for all multi-step operations. The enqueue process should be atomic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">with</span><span style=\"color:#E1E4E8\"> redis.pipeline() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pipe:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipe.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">serialized_job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipe.expire(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, ttl)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipe.lpush(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"queue:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipe.execute()  </span><span style=\"color:#6A737D\"># All commands happen atomically</span></span></code></pre></div>\n\n<hr>\n<p>⚠️ <strong>Pitfall: Using Local Time Instead of UTC</strong></p>\n<p><strong>Description:</strong> Storing timestamps in local timezone or using <code>datetime.now()</code> without timezone. When deployed across regions or during daylight saving transitions, job ordering and scheduling become inconsistent.</p>\n<p><strong>Why It&#39;s Wrong:</strong> Causes incorrect job ordering (timezone differences), scheduling errors (DST shifts), and debugging nightmares when comparing timestamps from different servers.</p>\n<p><strong>How to Fix:</strong> Always use UTC for all timestamps. In Python:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timezone</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">job.created_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now(timezone.utc)</span></span></code></pre></div>\n<p>Serialize as ISO 8601 with Z suffix: <code>&quot;2024-01-15T10:30:00Z&quot;</code>.</p>\n<hr>\n<p>⚠️ <strong>Pitfall: Blocking Operations Without Timeouts</strong></p>\n<p><strong>Description:</strong> Using Redis <code>BLPOP</code> without timeout or with very long timeout in workers. This prevents graceful shutdown (worker hangs waiting for job when SIGTERM arrives) and hides connectivity issues.</p>\n<p><strong>Why It&#39;s Wrong:</strong> Graceful shutdown fails, forcing SIGKILL. Network partitions cause all workers to hang indefinitely. System becomes unresponsive.</p>\n<p><strong>How to Fix:</strong> Use reasonable timeouts (1-5 seconds) on blocking Redis operations. Implement heartbeat checking in worker main loop. For graceful shutdown, use interruptible waits or check shutdown flag between polls.</p>\n<hr>\n<p>⚠️ <strong>Pitfall: Serializing Non-Serializable Objects</strong></p>\n<p><strong>Description:</strong> Allowing job arguments to contain database connections, file handles, or custom objects without proper serialization. JSON serialization fails with cryptic errors.</p>\n<p><strong>Why It&#39;s Wrong:</strong> Jobs fail immediately on enqueue with unclear errors. Requires producers to understand serialization details. Limits what can be passed as job arguments.</p>\n<p><strong>How to Fix:</strong> Provide clear documentation and validation. Use JSON-serializable types only: strings, numbers, booleans, lists, dicts. For complex objects, pass identifiers (database IDs, file paths) and have the job handler reconstruct them. Implement validation in <code>Job.to_dict()</code> that checks each argument.</p>\n<h3 id=\"implementation-guidance-for-queue-core\">Implementation Guidance for Queue Core</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Client</td>\n<td><code>redis-py</code> (official client)</td>\n<td><code>hiredis</code> parser for performance</td>\n<td><code>redis-py</code> with connection pooling - simple, robust, well-supported</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON (built-in)</td>\n<td>MessagePack or CBOR</td>\n<td>JSON - human-readable, debuggable, good enough for most payloads</td>\n</tr>\n<tr>\n<td>ID Generation</td>\n<td>UUID4 (random)</td>\n<td>ULID (time-ordered)</td>\n<td><strong>ULID</strong> - provides time-based sorting, collision resistant, readable</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td>Manual size calculation</td>\n<td>Pydantic models with validation</td>\n<td>Manual + JSON schema check - keeps dependencies minimal</td>\n</tr>\n<tr>\n<td>Connection Handling</td>\n<td>Simple client per operation</td>\n<td>Connection pool with health checks</td>\n<td><strong>Connection pool</strong> - essential for production performance</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background_job_processor/\n├── pyproject.toml                    # Project dependencies and metadata\n├── README.md\n├── src/\n│   └── job_processor/\n│       ├── __init__.py\n│       ├── core/                      # Core queue infrastructure\n│       │   ├── __init__.py\n│       │   ├── job.py                 # Job class definition and serialization\n│       │   ├── redis_client.py        # RedisClient wrapper with pooling\n│       │   ├── queue_manager.py       # QueueManager implementation\n│       │   └── config.py              # SystemConfig, QueueConfig, etc.\n│       ├── utils/\n│       │   ├── __init__.py\n│       │   ├── ulid_generator.py      # ULID generation utility\n│       │   └── validation.py          # Payload validation helpers\n│       └── cli.py                     # Command-line interface for testing\n├── tests/\n│   ├── __init__.py\n│   └── test_queue_core.py             # Tests for Milestone 1\n└── examples/\n    └── basic_enqueue.py               # Example usage of queue system</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Complete Redis Client Wrapper (<code>redis_client.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Redis client wrapper with connection pooling and error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Any, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Singleton Redis client with connection pooling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This wrapper provides a consistent interface for Redis operations with</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    proper connection management, error handling, and logging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _instance: Optional[</span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _client: Optional[redis.Redis] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379/0\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">connection_kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize Redis client (private, use get_instance).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            url: Redis connection URL (redis://, rediss://, unix://)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            **connection_kwargs: Additional connection parameters passed to redis.Redis</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> RedisClient._instance </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Use RedisClient.get_instance() to get singleton\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_kwargs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_kwargs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_instance</span><span style=\"color:#E1E4E8\">(cls, url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> </span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get singleton RedisClient instance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            url: Optional Redis URL (uses default if None on first call)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            **kwargs: Connection parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Singleton RedisClient instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> url </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379/0\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(url, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            cls</span><span style=\"color:#E1E4E8\">._instance._connect()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _connect</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Establish Redis connection with pooling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Parse URL and set up connection pool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.ConnectionPool.from_url(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.url,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_connections</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_kwargs.get(</span><span style=\"color:#9ECBFF\">'max_connections'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                socket_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_kwargs.get(</span><span style=\"color:#9ECBFF\">'socket_timeout'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                retry_on_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_kwargs.get(</span><span style=\"color:#9ECBFF\">'retry_on_timeout'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#6A737D\">  # Automatically decode responses to strings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span><span style=\"color:#FFAB70\">connection_pool</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pool)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test connection</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.ping()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Connected to Redis at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to connect to Redis: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_client</span><span style=\"color:#E1E4E8\">(self) -> redis.Redis:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the underlying Redis client instance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            redis.Redis client instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            RuntimeError: If client is not connected</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Redis client not connected\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute</span><span style=\"color:#E1E4E8\">(self, command: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a Redis command with error handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            command: Redis command name (e.g., 'hset', 'lpush')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            *args: Command arguments</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            **kwargs: Additional options</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Command result</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            redis.RedisError: On Redis operation failure</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Get the method from redis.Redis instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            method </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(client, command.lower())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> method(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis command failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">command</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#79B8FF\"> {</span><span style=\"color:#E1E4E8\">args</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline</span><span style=\"color:#E1E4E8\">(self) -> redis.Pipeline:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return a Redis pipeline for atomic operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            redis.Pipeline instance for executing multiple commands atomically</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> client.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transaction</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for Redis transaction (MULTI/EXEC).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Usage:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            with redis_client.transaction() as pipe:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                pipe.set('key1', 'value1')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                pipe.incr('key2')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> pipe</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.execute()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.reset()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> close</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Close Redis connection pool.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.close()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#9ECBFF\">\"Redis connection closed\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Complete ULID Generator Utility (<code>utils/ulid_generator.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"ULID generation for time-ordered unique job IDs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timezone</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Base32 encoding alphabet (Crockford's)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">BASE32</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"0123456789ABCDEFGHJKMNPQRSTVWXYZ\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">BASE32_LEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">BASE32</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> ulid</span><span style=\"color:#E1E4E8\">() -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate a ULID string.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    ULID format: 01J5XZR0W3F6Y2A4B8C9D10E11</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - First 10 chars: Timestamp in milliseconds (base32 encoded)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Last 16 chars: Random bytes (base32 encoded)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        26-character ULID string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get current time in milliseconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(time.time() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Encode timestamp (48 bits -> 10 base32 chars)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp_chars </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp_chars.append(</span><span style=\"color:#79B8FF\">BASE32</span><span style=\"color:#E1E4E8\">[(timestamp </span><span style=\"color:#F97583\">>></span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">9</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> i))) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#F97583\"> 0x</span><span style=\"color:#79B8FF\">1F</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate random bytes (80 bits -> 16 base32 chars)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    random_bytes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> bytearray</span><span style=\"color:#E1E4E8\">(os.urandom(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Ensure randomness doesn't exceed 80 bits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    random_bytes[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x26;=</span><span style=\"color:#F97583\"> 0x</span><span style=\"color:#79B8FF\">FF</span><span style=\"color:#6A737D\">  # Clear top bits to keep within 80 bits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    random_chars </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Process 5-bit chunks from the 80 random bits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    random_bits </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">.from_bytes(random_bytes, </span><span style=\"color:#9ECBFF\">'big'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        random_chars.append(</span><span style=\"color:#79B8FF\">BASE32</span><span style=\"color:#E1E4E8\">[(random_bits </span><span style=\"color:#F97583\">>></span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">15</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> i))) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#F97583\"> 0x</span><span style=\"color:#79B8FF\">1F</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> ''</span><span style=\"color:#E1E4E8\">.join(timestamp_chars </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> random_chars)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> ulid_to_datetime</span><span style=\"color:#E1E4E8\">(ulid_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> datetime:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extract timestamp from ULID.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ulid_str: ULID string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        datetime object in UTC</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ulid_str) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 26</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid ULID length: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">ulid_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp_part </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ulid_str[:</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Decode base32 timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> timestamp_part:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> timestamp </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> BASE32</span><span style=\"color:#E1E4E8\">.index(char)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Convert milliseconds to seconds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> datetime.fromtimestamp(timestamp </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tz</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timezone.utc)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test the implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate and test a few ULIDs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        u </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ulid()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        dt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ulid_to_datetime(u)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"ULID: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">u</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> -> Time: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">dt.isoformat()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Job Class with Serialization (<code>core/job.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Job class definition with serialization methods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Job lifecycle status states.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span><span style=\"color:#6A737D\">          # Waiting in queue</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACTIVE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span><span style=\"color:#6A737D\">            # Currently being processed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span><span style=\"color:#6A737D\">      # Finished successfully</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span><span style=\"color:#6A737D\">            # Finished with error</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retry_scheduled\"</span><span style=\"color:#6A737D\">  # Failed and scheduled for retry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEAD_LETTER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dead_letter\"</span><span style=\"color:#6A737D\">  # Permanently failed after max retries</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Job</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a unit of work to be processed asynchronously.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the core data structure that flows through the entire system.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        args: Optional[List[Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kwargs: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"default\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        priority: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        created_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        status: JobStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errors: Optional[List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result: Optional[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize a new Job.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_type: Identifier for the handler function/class</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            args: Positional arguments for the job handler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            kwargs: Keyword arguments for the job handler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue: Target queue name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            priority: Queue priority weight (higher = more important)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_retries: Maximum retry attempts before giving up</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            timeout_seconds: Maximum execution time in seconds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            metadata: Arbitrary key-value data for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique identifier (auto-generated if None)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            created_at: Creation timestamp (auto-set if None)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            status: Current job status</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            attempts: Number of execution attempts so far</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            errors: List of error records from previous failures</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            started_at: When job execution began</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            completed_at: When job finished (success or failure)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            result: Job execution result (on success)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_id </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4())  </span><span style=\"color:#6A737D\"># Will replace with ULID later</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_type</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.args </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> args </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.kwargs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> kwargs </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.priority </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> priority</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_retries</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timeout_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> timeout_seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.created_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> created_at </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> datetime.now(timezone.utc)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> status</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attempts</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.started_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> started_at</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.completed_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> completed_at</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert job to dictionary for serialization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary representation of the job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert datetime objects to ISO format strings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use .isoformat() for datetime objects, check if value is datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert JobStatus enum to string value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Access .value property of the enum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure all values are JSON-serializable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Recursively check lists and dicts, raise TypeError for non-serializable values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return complete dictionary with all fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Structure should match the Job constructor parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reconstruct job from dictionary.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: Dictionary representation from to_dict()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Job instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse ISO format strings back to datetime objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: datetime.fromisoformat() for ISO strings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert status string back to JobStatus enum</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: JobStatus(value) to reconstruct</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle optional fields that might be missing in older serializations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use .get() with default values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create and return Job instance with parsed data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pass all fields to the constructor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> serialize</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize job to JSON string.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            JSON string representation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert to dictionary using to_dict()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Serialize to JSON with default=str for any non-serializable values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return the JSON string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> deserialize</span><span style=\"color:#E1E4E8\">(cls, data: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Job'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deserialize job from JSON string.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: JSON string from serialize()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Job instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse JSON string to dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert from dictionary using from_dict()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return the Job instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_error</span><span style=\"color:#E1E4E8\">(self, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record an error that occurred during job execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            error: Exception that was raised</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create error dictionary with keys:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - 'type': error class name (type(error).__name__)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - 'message': str(error)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - 'traceback': formatted traceback string (use traceback.format_exc())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - 'timestamp': current UTC time in ISO format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Append error dict to self.errors list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Increment self.attempts counter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_retry</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if job should be retried.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if job has retries remaining and hasn't exceeded max_retries</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if attempts &#x3C; max_retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return True if more retries available, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Queue Manager Implementation (<code>core/queue_manager.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Queue manager for enqueuing and managing jobs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timezone</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job, JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .redis_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueConfig, SystemConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> utils.ulid_generator </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ulid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when job validation fails.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueFullError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when queue is at maximum capacity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages job enqueue operations and queue inspection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: SystemConfig):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize QueueManager with configuration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            config: System configuration</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.url,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            max_connections</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.max_connections,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            socket_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.socket_timeout,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            retry_on_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.retry_on_timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Build queue config map for quick lookup</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queue_configs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, QueueConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            qc.name: qc </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> qc </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> config.queues</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> enqueue_job</span><span style=\"color:#E1E4E8\">(self, job: Job) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validates, serializes, and atomically adds a job to the appropriate Redis queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: Job instance to enqueue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique identifier for the enqueued job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValidationError: If job fails validation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            QueueFullError: If queue is at maximum capacity</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate the job using _validate_job() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate ULID for job_id if not already set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set job status to PENDING and created_at timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Get queue config for the target queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check if queue is at capacity (if max_length is set)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Serialize job to JSON using job.serialize()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Use Redis pipeline for atomic operations:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Store job data in hash: HSET job:{job_id} {serialized_job}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Set TTL on job hash: EXPIRE job:{job_id} {job_ttl}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Add job ID to queue list: LPUSH queue:{queue_name} {job_id}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   d. Increment queue length metric: INCR queue:{queue_name}:count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Log successful enqueue with job_id and queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Return job_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> bulk_enqueue</span><span style=\"color:#E1E4E8\">(self, jobs: List[Job]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Enqueues multiple jobs in a single atomic transaction.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            jobs: List of Job instances to enqueue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of job IDs in same order as input jobs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValidationError: If any job fails validation (entire batch rolls back)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate all jobs first (fail fast)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate job IDs for all jobs missing them</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Prepare all jobs (status, timestamps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Group jobs by queue for efficient Redis operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Use Redis pipeline for atomic batch:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   For each job:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     - HSET job:{job_id} {serialized_job}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     - EXPIRE job:{job_id} {job_ttl}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   For each queue with jobs:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     - LPUSH queue:{queue_name} [job_id1, job_id2, ...]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     - INCRBY queue:{queue_name}:count {count}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of job IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_queue_length</span><span style=\"color:#E1E4E8\">(self, queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns the current number of pending jobs in the specified queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue_name: Name of the queue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of pending jobs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use Redis LLEN command on queue:{queue_name} key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return the length as integer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek_queue</span><span style=\"color:#E1E4E8\">(self, queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> List[Job]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieves jobs from the front of the queue without removing them.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue_name: Name of the queue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            count: Maximum number of jobs to peek</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of Job instances (deserialized)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use Redis LRANGE to get job IDs from start of list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each job ID, fetch job data from hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Deserialize job data to Job objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return list of Job instances</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_queues</span><span style=\"color:#E1E4E8\">(self, include_internal: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns names of all configured queues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            include_internal: If True, include system queues</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of queue names</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get all queue names from self.queue_configs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If include_internal is True, add system queue names:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - retry:scheduled (sorted set)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - dead:letter (list)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - scheduler:queued (list)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return sorted list of queue names</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete_job</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Removes a specific job from a queue if it exists.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue_name: Queue to remove job from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if job was found and removed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use Redis LREM to remove job_id from queue:{queue_name} list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If removed, decrement queue:{queue_name}:count metric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return True if count > 0, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_job_by_id</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Job]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieves a job from Redis by its ID.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Job instance if found, None otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use Redis HGET to fetch job data from job:{job_id} hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If data exists, deserialize to Job object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return Job instance or None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_job</span><span style=\"color:#E1E4E8\">(self, job: Job) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Internal method to validate job before enqueue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: Job to validate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValidationError: If validation fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if job payload size exceeds max_payload_size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Estimate size using len(json.dumps(job.args)) + len(json.dumps(job.kwargs))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate required fields: job_type, queue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check that args and kwargs are JSON-serializable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Try json.dumps on each and catch TypeError</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure queue exists in config (or use default)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate priority is positive integer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<p><strong>Python-Specific Implementation Tips:</strong></p>\n<ol>\n<li><strong>JSON Serialization of Datetimes:</strong> Use a custom JSON encoder for datetime objects:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   class</span><span style=\"color:#B392F0\"> DateTimeEncoder</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">json</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JSONEncoder</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       def</span><span style=\"color:#B392F0\"> default</span><span style=\"color:#E1E4E8\">(self, obj):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, datetime):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">               return</span><span style=\"color:#E1E4E8\"> obj.isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           return</span><span style=\"color:#79B8FF\"> super</span><span style=\"color:#E1E4E8\">().default(obj)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   json_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.dumps(job_dict, </span><span style=\"color:#FFAB70\">cls</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">DateTimeEncoder)</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Redis Connection Pooling:</strong> Configure connection pooling properly:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">   pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.ConnectionPool.from_url(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       'redis://localhost:6379/0'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       max_connections</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       socket_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       socket_connect_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       retry_on_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       health_check_interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   )</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Atomic Operations with Pipelines:</strong> Always use pipelines for multi-step Redis operations:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   with</span><span style=\"color:#E1E4E8\"> redis_client.pipeline() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pipe:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       pipe.multi()  </span><span style=\"color:#6A737D\"># Start transaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       pipe.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       pipe.expire(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">604800</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       pipe.lpush(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"queue:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       pipe.execute()  </span><span style=\"color:#6A737D\"># Atomic execution</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Size Estimation Without Full Serialization:</strong> Estimate JSON size efficiently:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> estimate_json_size</span><span style=\"color:#E1E4E8\">(obj):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"\"\"Estimate JSON size without serializing entire object.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, (</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(obj)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, (</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           return</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#6A737D\">  # Approximate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, (</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           return</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(estimate_json_size(item) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> item </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> obj) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(obj, </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           size </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> obj.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               size </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> estimate_json_size(k) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> estimate_json_size(v) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           return</span><span style=\"color:#E1E4E8\"> size</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       return</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#6A737D\">  # Conservative estimate for unknown types</span></span></code></pre></div>\n\n<ol start=\"5\">\n<li><strong>ULID Implementation:</strong> Use the <code>ulid-py</code> library for production:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # pip install ulid-py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> ulid </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ULID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(ULID())  </span><span style=\"color:#6A737D\"># Generates time-ordered ULID</span></span></code></pre></div>\n\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the Job Queue Core (Milestone 1), verify your implementation with these checkpoints:</p>\n<p><strong>Checkpoint 1: Job Serialization Round-Trip</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_queue_core.py::test_job_serialization_roundtrip</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p><strong>Expected Output:</strong> Test passes with no errors. A job serialized and deserialized should have identical field values (timestamps within millisecond tolerance).</p>\n<p><strong>Checkpoint 2: Enqueue and Retrieve</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start Redis locally first: redis-server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/basic_enqueue.py</span></span></code></pre></div>\n<p><strong>Example script output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Enqueued job with ID: 01J5XZR0W3F6Y2A4B8C9D10E11\nQueue 'default' length: 1\nPeeked job: Job(job_id='01J5XZR0W3F6Y2A4B8C9D10E11', job_type='test_task', ...)</code></pre></div>\n\n<p><strong>Checkpoint 3: Payload Size Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_queue_core.py::test_oversized_payload_rejected</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p><strong>Expected:</strong> Test passes with <code>ValidationError</code> raised for payload &gt; 1MB.</p>\n<p><strong>Checkpoint 4: Queue Priority Configuration</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/queue_priority_test.py</span></span></code></pre></div>\n<p><strong>Verify:</strong> The script should show that high-priority queues receive more frequent polling attention (simulated by checking dequeue order over many iterations).</p>\n<p><strong>Signs Something Is Wrong:</strong></p>\n<ul>\n<li><strong>Redis connection errors:</strong> Check Redis is running and connection URL is correct</li>\n<li><strong>Serialization errors:</strong> Ensure all job arguments are JSON-serializable (no custom objects)</li>\n<li><strong>Missing jobs after enqueue:</strong> Verify pipeline operations are atomic and all execute successfully</li>\n<li><strong>ULIDs not time-ordered:</strong> Check ULID generation uses milliseconds, not seconds</li>\n<li><strong>Queue length mismatch:</strong> Ensure <code>LPUSH</code> and <code>INCR</code> operations happen in same pipeline</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job disappears after enqueue</td>\n<td>Pipeline not executing or Redis transaction rolled back</td>\n<td>Check Redis logs for errors, verify <code>pipe.execute()</code> is called</td>\n<td>Ensure all pipeline operations are followed by <code>execute()</code></td>\n</tr>\n<tr>\n<td>Job data exists but not in queue</td>\n<td>Orphaned job - stored in hash but not added to list</td>\n<td>Use <code>redis-cli KEYS &quot;job:*&quot;</code> to find jobs, check if ID in queue list</td>\n<td>Use atomic pipeline for both operations</td>\n</tr>\n<tr>\n<td>Queue length metric wrong</td>\n<td>Race condition in INCR/DECR operations</td>\n<td>Monitor <code>queue:{name}:count</code> vs actual LLEN</td>\n<td>Use Redis transactions or Lua scripts for atomic increments</td>\n</tr>\n<tr>\n<td>Serialization fails with TypeError</td>\n<td>Non-JSON-serializable arguments in job</td>\n<td>Add validation before serialization, log argument types</td>\n<td>Convert objects to serializable forms (IDs, paths, strings)</td>\n</tr>\n<tr>\n<td>ULIDs not monotonic</td>\n<td>Using UUID instead of time-based ID</td>\n<td>Check ULID generation uses current time in milliseconds</td>\n<td>Implement proper ULID or use <code>ulid-py</code> library</td>\n</tr>\n<tr>\n<td>Jobs enqueued to non-existent queue</td>\n<td>Queue config not loaded or mismatch</td>\n<td>Check <code>queue_configs</code> map in QueueManager</td>\n<td>Ensure queue exists in config or create default config</td>\n</tr>\n<tr>\n<td>Redis memory growing indefinitely</td>\n<td>Missing TTL on job hashes</td>\n<td>Check <code>job:{id}</code> keys with <code>TTL</code> command</td>\n<td>Always set TTL when storing jobs</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"component-design-worker-process\">Component Design: Worker Process</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2: Worker Process. This section provides the complete design for worker processes that fetch jobs from Redis queues, execute them with proper isolation, handle failures gracefully, and report their liveness for monitoring.</p>\n</blockquote>\n<h3 id=\"mental-model-factory-assembly-line-worker\">Mental Model: Factory Assembly Line Worker</h3>\n<p>Imagine a factory assembly line with multiple stations. Each <strong>worker</strong> is a skilled operator who:</p>\n<ol>\n<li><strong>Waits at a designated station</strong> (queue) for work items to arrive</li>\n<li><strong>Receives a work ticket</strong> (job) describing exactly what needs to be done</li>\n<li><strong>Performs the operation</strong> using the right tools (handler functions)</li>\n<li><strong>Marks the ticket</strong> as complete, moves it to the finished pile, or escalates it if something goes wrong</li>\n<li><strong>Reports their status</strong> regularly to the floor manager to confirm they&#39;re still operational</li>\n</ol>\n<p>Just like factory workers have different specializations (some handle painting, others assembly), our workers can be configured to monitor specific queues for different job types. They work concurrently within their station, can be told to finish their current task before going home (graceful shutdown), and have safety mechanisms to stop working if a task takes too long or becomes dangerous (timeouts and exception handling).</p>\n<p>This mental model emphasizes key worker characteristics: <strong>specialization by queue</strong>, <strong>reliable task execution</strong>, <strong>concurrent operation within limits</strong>, and <strong>constant status reporting</strong> for system health monitoring.</p>\n<h3 id=\"worker-interface-and-configuration\">Worker Interface and Configuration</h3>\n<p>The worker component exposes a clean interface for starting, stopping, and monitoring worker processes. Each worker instance is configured through a <code>WorkerConfig</code> object that determines its behavior.</p>\n<p><strong>Worker Configuration Data Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>queues</code></td>\n<td><code>List[str]</code></td>\n<td>List of queue names this worker should monitor (e.g., <code>[&quot;email&quot;, &quot;reports&quot;, &quot;default&quot;]</code>)</td>\n</tr>\n<tr>\n<td><code>concurrency</code></td>\n<td><code>int</code></td>\n<td>Maximum number of jobs this worker can process simultaneously (default: 1)</td>\n</tr>\n<tr>\n<td><code>heartbeat_interval</code></td>\n<td><code>int</code></td>\n<td>Seconds between heartbeats written to Redis (default: 30)</td>\n</tr>\n<tr>\n<td><code>job_timeout</code></td>\n<td><code>int</code></td>\n<td>Maximum seconds a job can run before being forcibly terminated (default: 300)</td>\n</tr>\n<tr>\n<td><code>worker_id</code></td>\n<td><code>str</code></td>\n<td>Unique identifier for this worker instance (auto-generated if not provided)</td>\n</tr>\n<tr>\n<td><code>hostname</code></td>\n<td><code>str</code></td>\n<td>Host machine name for monitoring (auto-detected)</td>\n</tr>\n<tr>\n<td><code>pid</code></td>\n<td><code>int</code></td>\n<td>Process ID for monitoring (auto-detected)</td>\n</tr>\n</tbody></table>\n<p><strong>Worker Public Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>start()</code></td>\n<td>None</td>\n<td>None</td>\n<td>Starts the worker main loop, begins polling queues and processing jobs</td>\n</tr>\n<tr>\n<td><code>stop(graceful=True)</code></td>\n<td><code>graceful: bool</code></td>\n<td>None</td>\n<td>Stops the worker; if <code>graceful=True</code>, completes current job before stopping</td>\n</tr>\n<tr>\n<td><code>pause()</code></td>\n<td>None</td>\n<td>None</td>\n<td>Temporarily stops fetching new jobs but continues processing current ones</td>\n</tr>\n<tr>\n<td><code>resume()</code></td>\n<td>None</td>\n<td>None</td>\n<td>Resumes fetching new jobs after being paused</td>\n</tr>\n<tr>\n<td><code>get_status()</code></td>\n<td>None</td>\n<td><code>Dict[str, Any]</code></td>\n<td>Returns current worker status: idle/active, current job, queue depths, etc.</td>\n</tr>\n<tr>\n<td><code>register_handler(job_type, handler_func)</code></td>\n<td><code>job_type: str</code>, <code>handler_func: Callable</code></td>\n<td>None</td>\n<td>Registers a function to handle a specific job type</td>\n</tr>\n</tbody></table>\n<p><strong>Worker Internal State Tracking:</strong></p>\n<p>Each worker maintains internal state that evolves throughout its lifecycle:</p>\n<table>\n<thead>\n<tr>\n<th>State</th>\n<th>Description</th>\n<th>Triggers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STARTING</code></td>\n<td>Worker is initializing connections and loading handlers</td>\n<td>Worker instantiation</td>\n</tr>\n<tr>\n<td><code>IDLE</code></td>\n<td>Worker is waiting for jobs (blocking on queue reads)</td>\n<td>After startup or job completion</td>\n</tr>\n<tr>\n<td><code>PROCESSING</code></td>\n<td>Worker is actively executing a job</td>\n<td>Job dequeued and execution started</td>\n</tr>\n<tr>\n<td><code>PAUSED</code></td>\n<td>Worker is not fetching new jobs but may complete current job</td>\n<td>Manual pause or system overload</td>\n</tr>\n<tr>\n<td><code>SHUTTING_DOWN</code></td>\n<td>Worker is finishing current job before exiting</td>\n<td>Stop signal received</td>\n</tr>\n<tr>\n<td><code>STOPPED</code></td>\n<td>Worker has completely terminated</td>\n<td>All cleanup complete</td>\n</tr>\n</tbody></table>\n<p>These states are captured in the worker state machine diagram:</p>\n<p><img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fworker-state-machine.svg\" alt=\"Worker Process State Machine\"></p>\n<h3 id=\"worker-main-loop-algorithm\">Worker Main Loop Algorithm</h3>\n<p>The worker&#39;s core logic follows a deterministic main loop that orchestrates job fetching, execution, and status reporting. Below is the step-by-step algorithm executed by each worker process:</p>\n<ol>\n<li><p><strong>Initialization Phase</strong></p>\n<ol>\n<li>Generate a unique worker ID using ULID format (timestamp + randomness for lexicographic sorting)</li>\n<li>Establish connection to Redis using the <code>RedisClient.get_instance()</code> singleton</li>\n<li>Load job handler registry from application code (functions decorated with <code>@job_handler</code>)</li>\n<li>Set up signal handlers for SIGTERM (graceful shutdown) and SIGINT (immediate shutdown)</li>\n<li>Initialize worker status in Redis with current timestamp and <code>STARTING</code> state</li>\n<li>Start heartbeat thread that updates Redis every <code>heartbeat_interval</code> seconds</li>\n</ol>\n</li>\n<li><p><strong>Main Processing Loop</strong></p>\n<ol>\n<li><strong>State Check</strong>: If worker state is <code>SHUTTING_DOWN</code>, break loop and proceed to shutdown sequence</li>\n<li><strong>Pause Check</strong>: If worker state is <code>PAUSED</code>, sleep for 1 second and continue loop</li>\n<li><strong>Queue Selection</strong>: Determine which queue to poll next using priority-weighted round-robin<ul>\n<li>Calculate total priority weight: sum of all configured queue priorities</li>\n<li>Generate random number between 0 and total weight</li>\n<li>Iterate through queues, subtracting each queue&#39;s priority until random number ≤ 0</li>\n<li>Select that queue for this polling cycle</li>\n</ul>\n</li>\n<li><strong>Job Fetching</strong>: Attempt to fetch a job from the selected queue using <code>BRPOPLPUSH</code><ul>\n<li>Use <code>BRPOPLPUSH source_queue processing_queue timeout=1</code></li>\n<li>If timeout occurs with no job, continue to next iteration</li>\n<li>If job retrieved, parse JSON into <code>Job</code> object using <code>Job.deserialize()</code></li>\n</ul>\n</li>\n<li><strong>Job Processing Setup</strong>:<ul>\n<li>Update job status to <code>ACTIVE</code> and set <code>started_at</code> timestamp in Redis</li>\n<li>Update worker status to <code>PROCESSING</code> with current job ID</li>\n<li>Start execution timeout timer based on <code>job.timeout_seconds</code></li>\n</ul>\n</li>\n<li><strong>Job Execution</strong>:<ul>\n<li>Look up handler function for <code>job.job_type</code> in registry</li>\n<li>If no handler found, mark job as failed with &quot;No handler registered&quot; error</li>\n<li>Execute handler with <code>job.args</code> and <code>job.kwargs</code> in isolated context (thread/process)</li>\n<li>Wait for completion or timeout</li>\n</ul>\n</li>\n<li><strong>Post-Execution Handling</strong>:<ul>\n<li><strong>If successful</strong>: Update job status to <code>COMPLETED</code>, store result, increment metrics</li>\n<li><strong>If failed with exception</strong>: Call <code>job.record_error()</code>, check <code>job.should_retry()</code><ul>\n<li>If retries remain: schedule retry with exponential backoff (Milestone 3)</li>\n<li>If no retries remain: move to dead letter queue</li>\n</ul>\n</li>\n<li><strong>If timeout</strong>: Record timeout error, treat as failure with retry logic</li>\n</ul>\n</li>\n<li><strong>Cleanup</strong>: Remove job from processing queue, update worker status to <code>IDLE</code></li>\n</ol>\n</li>\n<li><p><strong>Shutdown Sequence</strong></p>\n<ol>\n<li>Set worker state to <code>SHUTTING_DOWN</code> in Redis</li>\n<li>Wait for current job to complete (up to graceful shutdown timeout)</li>\n<li>If job doesn&#39;t complete in timeout, force termination and move job back to queue</li>\n<li>Close Redis connection</li>\n<li>Stop heartbeat thread</li>\n<li>Set final worker state to <code>STOPPED</code></li>\n</ol>\n</li>\n</ol>\n<p>The complete flow of a successful job execution is shown in the sequence diagram:</p>\n<p><img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fhappy-path-sequence.svg\" alt=\"Happy Path Job Execution Sequence\"></p>\n<h3 id=\"adr-concurrency-model-process-vs-thread\">ADR: Concurrency Model (Process vs Thread)</h3>\n<blockquote>\n<p><strong>Decision: Thread Pool with Process Isolation for CPU-bound Tasks</strong></p>\n<ul>\n<li><p><strong>Context</strong>: Workers need to execute multiple jobs concurrently to maximize throughput. The system must handle both I/O-bound tasks (API calls, database queries) and CPU-bound tasks (image processing, data analysis). We need isolation to prevent one failing job from crashing the entire worker.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Single-threaded worker</strong>: Simple but poor resource utilization</li>\n<li><strong>Thread pool</strong>: Good for I/O-bound tasks, shares memory space (risky)</li>\n<li><strong>Process pool</strong>: Full isolation, higher overhead for job data transfer</li>\n<li><strong>Hybrid approach</strong>: Thread pool for I/O tasks with optional process isolation for CPU tasks</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Implement thread pool for general execution with optional process-based isolation for CPU-bound job types.</p>\n</li>\n<li><p><strong>Rationale</strong>: Most background jobs in web applications are I/O-bound (sending emails, calling APIs, database operations). Threads provide excellent concurrency for these with minimal overhead. However, for CPU-intensive tasks or unreliable third-party code, process isolation prevents memory leaks and crashes from affecting other jobs. By making isolation configurable per job type, we get the best of both worlds.</p>\n</li>\n<li><p><strong>Consequences</strong>: </p>\n<ul>\n<li>Enables efficient handling of mixed workloads</li>\n<li>Adds complexity to job data serialization for process boundaries</li>\n<li>Requires careful management of process pool size to avoid resource exhaustion</li>\n<li>Provides safer execution environment for untrusted job code</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Concurrency Model Comparison:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single-threaded</td>\n<td>Simple, no race conditions</td>\n<td>Poor CPU utilization, blocks on I/O</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Thread pool</td>\n<td>Good I/O concurrency, shared memory</td>\n<td>One job can crash entire worker, GIL limits CPU tasks</td>\n<td><strong>Partially</strong> (default)</td>\n</tr>\n<tr>\n<td>Process pool</td>\n<td>Full isolation, bypasses GIL</td>\n<td>Higher memory, serialization overhead</td>\n<td><strong>Partially</strong> (optional)</td>\n</tr>\n<tr>\n<td>Hybrid approach</td>\n<td>Best of both worlds, flexible per job type</td>\n<td>Most complex implementation</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"adr-dequeue-strategy-and-reliability\">ADR: Dequeue Strategy and Reliability</h3>\n<blockquote>\n<p><strong>Decision: BRPOPLPUSH with Processing Queue for Reliable Dequeue</strong></p>\n<ul>\n<li><p><strong>Context</strong>: When a worker fetches a job from Redis, the job must be removed from the main queue to prevent duplicate processing. However, if the worker crashes during execution, the job should not be lost forever. We need atomic dequeue with crash recovery.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>RPOP then delete</strong>: Simple but can lose jobs if worker crashes between RPOP and processing</li>\n<li><strong>BRPOP with timeout</strong>: Blocking pop but job disappears from Redis during execution</li>\n<li><strong>BRPOPLPUSH</strong>: Atomically moves job to processing queue, visible during execution</li>\n<li><strong>Redis Streams with consumer groups</strong>: Modern approach with built-in acknowledgment</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Use <code>BRPOPLPUSH</code> to move jobs from main queue to a processing queue, with periodic health checks to recover orphaned jobs.</p>\n</li>\n<li><p><strong>Rationale</strong>: <code>BRPOPLPUSH</code> provides atomic move operation in Redis, ensuring the job is never in &quot;limbo&quot; between queues. While Redis Streams are more feature-rich, they require Redis 5.0+ and add complexity for our FIFO queue needs. The processing queue acts as a visibility mechanism: jobs being processed remain in Redis so we can detect crashed workers and recover their jobs.</p>\n</li>\n<li><p><strong>Consequences</strong>:</p>\n<ul>\n<li>Requires maintaining processing queues per worker or worker group</li>\n<li>Adds recovery logic to check for stale jobs in processing queues</li>\n<li>Provides at-least-once delivery semantics (jobs may be processed twice if worker crashes after processing but before cleanup)</li>\n<li>Compatible with older Redis versions</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Dequeue Strategy Comparison:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Reliability</th>\n<th>Complexity</th>\n<th>Performance</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>RPOP then delete</td>\n<td>Low (job loss on crash)</td>\n<td>Simple</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>BRPOP</td>\n<td>Medium (job invisible during processing)</td>\n<td>Simple</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>BRPOPLPUSH</td>\n<td>High (visible in processing queue)</td>\n<td>Moderate</td>\n<td>High</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Redis Streams</td>\n<td>High (built-in features)</td>\n<td>High</td>\n<td>Medium</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls-in-worker-implementation\">Common Pitfalls in Worker Implementation</h3>\n<p>⚠️ <strong>Pitfall: Blocking Forever on Queue Read</strong></p>\n<ul>\n<li><strong>Description</strong>: Using <code>BRPOP</code> with timeout=0 or <code>BLPOP</code> without timeout causes worker to block indefinitely, preventing graceful shutdown.</li>\n<li><strong>Why it&#39;s wrong</strong>: The worker cannot respond to shutdown signals while blocked in Redis I/O, requiring SIGKILL to terminate.</li>\n<li><strong>Fix</strong>: Use <code>BRPOPLPUSH</code> with a reasonable timeout (1-5 seconds) and check shutdown flags between polls.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Isoling Job Execution</strong></p>\n<ul>\n<li><strong>Description</strong>: Running job handlers directly in the worker&#39;s main thread allows a job exception to crash the entire worker process.</li>\n<li><strong>Why it&#39;s wrong</strong>: A bug in one job handler terminates all concurrent jobs and requires worker restart.</li>\n<li><strong>Fix</strong>: Execute each job in a separate thread or process with proper exception boundaries.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Forgetting Heartbeat Updates</strong></p>\n<ul>\n<li><strong>Description</strong>: Worker starts heartbeat thread but doesn&#39;t handle Redis connection failures, causing stale heartbeats.</li>\n<li><strong>Why it&#39;s wrong</strong>: Monitoring system marks active workers as dead, triggering unnecessary job recovery.</li>\n<li><strong>Fix</strong>: Implement retry logic in heartbeat updates and validate Redis connection before each update.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring Memory Leaks in Long-running Workers</strong></p>\n<ul>\n<li><strong>Description</strong>: Worker processes thousands of jobs without restarting, gradually accumulating memory from uncollectable Python references.</li>\n<li><strong>Why it&#39;s wrong</strong>: Worker memory grows unbounded, eventually causing OOM kills or performance degradation.</li>\n<li><strong>Fix</strong>: Implement soft memory limits that trigger graceful shutdown after processing N jobs or reaching memory threshold.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Incomplete Job State Transition</strong></p>\n<ul>\n<li><strong>Description</strong>: Worker updates job status to <code>ACTIVE</code> but crashes before setting <code>COMPLETED</code> or <code>FAILED</code>, leaving job in ambiguous state.</li>\n<li><strong>Why it&#39;s wrong</strong>: Recovery system cannot determine if job needs to be retried or was successful.</li>\n<li><strong>Fix</strong>: Use Redis transactions (MULTI/EXEC) to atomically update both job status and worker state.</li>\n</ul>\n<h3 id=\"implementation-guidance-for-worker\">Implementation Guidance for Worker</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrency</td>\n<td><code>concurrent.futures.ThreadPoolExecutor</code></td>\n<td><code>multiprocessing.Pool</code> with configurable isolation</td>\n</tr>\n<tr>\n<td>Signal Handling</td>\n<td><code>signal.signal()</code> for basic SIGTERM</td>\n<td><code>asyncio</code> event loop with signal integration</td>\n</tr>\n<tr>\n<td>Timeout Management</td>\n<td><code>threading.Timer</code> for job timeouts</td>\n<td>Resource-aware timeout with CPU time monitoring</td>\n</tr>\n<tr>\n<td>Job Isolation</td>\n<td>Basic try/except in thread</td>\n<td>Subprocess with resource limits (CPU, memory)</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background_job_processor/\n├── worker/\n│   ├── __init__.py\n│   ├── worker.py              # Main Worker class implementation\n│   ├── config.py              # WorkerConfig and related configuration\n│   ├── handlers.py            # Job handler registry and decorator\n│   ├── concurrency.py         # Thread/process pool management\n│   ├── heartbeat.py           # Heartbeat thread implementation\n│   └── signals.py             # Signal handling utilities\n├── models/\n│   └── job.py                 # Job class (from Milestone 1)\n├── redis_client.py            # RedisClient (from Milestone 1)\n└── config/\n    └── system_config.py       # SystemConfig (from Milestone 1)</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p><strong>Heartbeat Thread Implementation</strong> (complete working code):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># worker/heartbeat.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WorkerHeartbeat</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread that periodically updates worker liveness in Redis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, redis_client, interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.worker_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> worker_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._stop_event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Event()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._thread: Optional[threading.Thread] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._last_successful </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the heartbeat thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._thread </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._thread.is_alive():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.warning(</span><span style=\"color:#9ECBFF\">\"Heartbeat thread already running\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._stop_event.clear()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._run,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"heartbeat-</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Heartbeat started for worker </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop the heartbeat thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._stop_event.set()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Heartbeat stopped for worker </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _run</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main heartbeat loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._stop_event.is_set():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Update heartbeat in Redis with current timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                now </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.utcnow().isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"worker:heartbeat:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.redis_client.execute(</span><span style=\"color:#9ECBFF\">\"SET\"</span><span style=\"color:#E1E4E8\">, key, now, </span><span style=\"color:#9ECBFF\">\"EX\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._last_successful </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> now</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Also update worker status if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"worker:status:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                current_status </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis_client.execute(</span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, status_key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> current_status:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.redis_client.execute(</span><span style=\"color:#9ECBFF\">\"EXPIRE\"</span><span style=\"color:#E1E4E8\">, status_key, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Heartbeat update failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Sleep for interval, but check stop event more frequently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):  </span><span style=\"color:#6A737D\"># Check every 0.1 seconds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._stop_event.is_set():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                time.sleep(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_healthy</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if heartbeat is updating successfully.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._last_successful:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check if last successful update was within 2 intervals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            last_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.fromisoformat(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._last_successful)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cutoff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.utcnow().timestamp() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> last_time.timestamp() </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> cutoff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">ValueError</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">TypeError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<p><strong>Worker Main Loop Method</strong> (with TODOs matching algorithm steps):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># worker/worker.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> signal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Callable, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> models.job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job, JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> redis_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> worker.config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> WorkerConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> worker.heartbeat </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> WorkerHeartbeat</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> worker.concurrency </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobExecutor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Worker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main worker class that processes jobs from Redis queues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: WorkerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.worker_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.worker_id </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._generate_worker_id()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.handlers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.heartbeat: Optional[WorkerHeartbeat] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.executor: Optional[JobExecutor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"STARTING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_job: Optional[Job] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._shutdown_requested </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._paused </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the worker main loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set up signal handlers for SIGTERM and SIGINT</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - SIGTERM should trigger graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - SIGINT should trigger immediate shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize Redis connection using self.redis.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start heartbeat thread with self.config.heartbeat_interval</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize job executor (thread/process pool) with self.config.concurrency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set worker state to \"IDLE\" in Redis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Enter main processing loop (call self._run_loop())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: After loop exits, perform cleanup in self._shutdown()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _run_loop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main worker processing loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._shutdown_requested:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if worker is paused - if so, sleep briefly and continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Select queue to poll using priority-weighted algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Get queue priorities from configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Calculate weighted random selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Choose queue for this iteration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Attempt to fetch job from selected queue using BRPOPLPUSH</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Use timeout of 1 second to allow shutdown checking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - If no job, continue to next iteration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - If job received, parse JSON into Job object</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update job status to ACTIVE and record start time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Use Redis pipeline for atomic updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Set job.started_at = current time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Update worker state to PROCESSING with job ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Execute job using self.executor.submit()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Look up handler function for job.job_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Submit to executor with timeout from job.timeout_seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Handle case where no handler is registered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Wait for job completion with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - If successful: update job status to COMPLETED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - If failed: call job.record_error() and handle retry logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - If timeout: record timeout error and handle as failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Clean up processing queue entry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Remove job from processing queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Update worker state back to IDLE</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                #   - Increment success/failure metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.exception(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unexpected error in worker loop: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Brief sleep to prevent tight error loop</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                time.sleep(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_job_result</span><span style=\"color:#E1E4E8\">(self, job: Job, result, error: Optional[</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Process the result of a job execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If error is None (success), update job status to COMPLETED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Set job.completed_at = current time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Store result in job.result (if serializable)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Update success metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If error occurred, call job.record_error(error)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Check job.should_retry() to determine next action</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If retries remain: schedule retry with exponential backoff (Milestone 3)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If no retries: move to dead letter queue (Milestone 3)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update job in Redis with final status and any result/error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Use Redis pipeline for atomic updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Set appropriate TTL based on job status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self, graceful: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Request worker shutdown.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set self._shutdown_requested = True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If graceful=True, set state to SHUTTING_DOWN and wait for current job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Wait up to job.timeout_seconds for current job to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If timeout, force terminate and requeue job</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If graceful=False, cancel current job immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Use executor.shutdown(wait=False)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Move current job back to main queue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Stop heartbeat thread</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Close Redis connection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>Signal Handling</strong>: Use <code>signal.signal(signal.SIGTERM, handler)</code> to catch shutdown signals. Set a flag in the handler rather than exiting immediately.</li>\n<li><strong>Thread Safety</strong>: Use <code>threading.Lock</code> for shared state between main thread and heartbeat thread.</li>\n<li><strong>Job Timeouts</strong>: Use <code>concurrent.futures.TimeoutError</code> with <code>executor.submit()</code> and <code>future.result(timeout=...)</code>.</li>\n<li><strong>Graceful Shutdown</strong>: Implement <code>__enter__</code> and <code>__exit__</code> methods on Worker class to use as context manager.</li>\n<li><strong>Logging</strong>: Use structured logging with <code>logging.JSONFormatter</code> for production monitoring.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the Worker component, verify functionality with these steps:</p>\n<ol>\n<li><strong>Start a worker</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> worker </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Worker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> worker.config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> WorkerConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> WorkerConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       queues</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       concurrency</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       heartbeat_interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       job_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">300</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   worker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Worker(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   worker.start()  </span><span style=\"color:#6A737D\"># Runs in foreground, Ctrl+C to stop</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><p><strong>Test job execution</strong>:</p>\n<ul>\n<li>Enqueue a test job using QueueManager from Milestone 1</li>\n<li>Observe worker logs: should show &quot;Processing job X&quot;, &quot;Job completed successfully&quot;</li>\n<li>Check Redis: job status should transition PENDING → ACTIVE → COMPLETED</li>\n</ul>\n</li>\n<li><p><strong>Verify graceful shutdown</strong>:</p>\n<ul>\n<li>Send SIGTERM to worker process: <code>kill -TERM &lt;pid&gt;</code></li>\n<li>Worker should finish current job before exiting</li>\n<li>Confirm &quot;Shutting down gracefully&quot; appears in logs</li>\n</ul>\n</li>\n<li><p><strong>Check heartbeat</strong>:</p>\n<ul>\n<li>Monitor Redis key <code>worker:heartbeat:&lt;worker_id&gt;</code></li>\n<li>Value should update every <code>heartbeat_interval</code> seconds</li>\n<li>If worker dies, key should expire after 3×interval</li>\n</ul>\n</li>\n</ol>\n<p><strong>Expected Output Signs:</strong></p>\n<ul>\n<li>✅ Worker connects to Redis and starts heartbeat</li>\n<li>✅ Worker fetches jobs from configured queues</li>\n<li>✅ Jobs execute with registered handlers</li>\n<li>✅ Worker responds to SIGTERM by finishing current job</li>\n<li>❌ If jobs fail silently, check exception handling in job wrapper</li>\n<li>❌ If worker hangs on shutdown, check BRPOPLPUSH timeout value</li>\n</ul>\n<h2 id=\"component-design-retry-amp-error-handling\">Component Design: Retry &amp; Error Handling</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3: Retry &amp; Error Handling. This section provides the complete design for automatic retry mechanisms with exponential backoff and dead letter queue functionality, enabling resilient job processing in the face of transient failures.</p>\n</blockquote>\n<h3 id=\"mental-model-customer-service-escalation-process\">Mental Model: Customer Service Escalation Process</h3>\n<p>Think of the retry system as a customer service escalation process in a large organization. When a customer request (job) fails initially:</p>\n<ol>\n<li><strong>Initial attempt</strong> → Frontline support handles the request (first execution attempt)</li>\n<li><strong>Failure detection</strong> → The request is marked as requiring follow-up (error is recorded)</li>\n<li><strong>Escalation timing</strong> → The request is scheduled for a retry after a calculated delay (escalation timer)</li>\n<li><strong>Escalation levels</strong> → Each retry goes to a more experienced specialist (exponential backoff)</li>\n<li><strong>Maximum escalation</strong> → After exhausting all escalation levels, the case goes to a special investigations team (dead letter queue)</li>\n<li><strong>Case review</strong> → The investigations team can manually review, fix, and retry or archive the case (manual intervention)</li>\n</ol>\n<p>This mental model helps understand why we need progressive delays between retries (to allow temporary issues to resolve), why we track error history (to understand the failure pattern), and why we need a dead letter queue (for cases requiring human intervention). The system ensures transient failures (network blips, temporary resource constraints) are automatically resolved while permanent failures (bad data, broken integrations) are surfaced for manual handling.</p>\n<h3 id=\"retry-manager-interface\">Retry Manager Interface</h3>\n<p>The <code>RetryManager</code> orchestrates the entire retry lifecycle, from detecting job failures to scheduling retries and managing the dead letter queue. It works closely with the <code>Worker</code> component (which detects failures) and the <code>Scheduler</code> component (which polls for scheduled retries).</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Key Methods</th>\n<th>Collaborates With</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RetryManager</code></td>\n<td>Manages retry logic and state</td>\n<td><code>schedule_retry</code>, <code>handle_failure</code>, <code>move_to_dead_letter</code>, <code>retry_dead_letter_job</code></td>\n<td><code>Worker</code>, <code>QueueManager</code>, <code>Scheduler</code></td>\n</tr>\n<tr>\n<td><code>BackoffCalculator</code></td>\n<td>Computes retry delays</td>\n<td><code>calculate_delay</code>, <code>add_jitter</code></td>\n<td><code>RetryManager</code></td>\n</tr>\n<tr>\n<td><code>DeadLetterQueue</code></td>\n<td>Stores permanently failed jobs</td>\n<td><code>store</code>, <code>retrieve</code>, <code>list</code>, <code>delete</code></td>\n<td><code>RetryManager</code>, <code>QueueManager</code></td>\n</tr>\n</tbody></table>\n<p>The <code>RetryManager</code> provides these key interfaces:</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>handle_job_failure</code></td>\n<td><code>job: Job</code>, <code>error: Exception</code></td>\n<td><code>None</code></td>\n<td>Called by <code>Worker</code> when a job fails. Records the error, checks retry eligibility, and either schedules a retry or moves to dead letter queue.</td>\n</tr>\n<tr>\n<td><code>schedule_retry</code></td>\n<td><code>job: Job</code>, <code>delay_seconds: int</code></td>\n<td><code>str</code> (job ID)</td>\n<td>Schedules a job for retry after specified delay. Returns the job ID for tracking.</td>\n</tr>\n<tr>\n<td><code>move_to_dead_letter</code></td>\n<td><code>job: Job</code></td>\n<td><code>bool</code></td>\n<td>Moves a job that has exhausted retries to the dead letter queue. Returns success status.</td>\n</tr>\n<tr>\n<td><code>retry_dead_letter_job</code></td>\n<td><code>job_id: str</code></td>\n<td><code>Optional[Job]</code></td>\n<td>Removes a job from the dead letter queue and re-enqueues it for immediate retry. Returns the re-enqueued job or <code>None</code> if not found.</td>\n</tr>\n<tr>\n<td><code>get_retry_stats</code></td>\n<td><code>job_type: Optional[str]</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Returns retry statistics (success rate, average attempts, common errors) for all jobs or filtered by job type.</td>\n</tr>\n<tr>\n<td><code>cleanup_expired_retries</code></td>\n<td><code>max_age_hours: int</code></td>\n<td><code>int</code> (count removed)</td>\n<td>Removes old retry records from Redis to prevent memory bloat. Returns number of records removed.</td>\n</tr>\n</tbody></table>\n<p>The retry process integrates with the job state machine (referenced in <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fjob-state-machine.svg\" alt=\"Job State Machine\">):</p>\n<ol>\n<li><strong>Job execution fails</strong> → <code>Worker</code> calls <code>RetryManager.handle_job_failure(job, error)</code></li>\n<li><strong>Error recording</strong> → Job&#39;s <code>errors</code> list is updated with exception details</li>\n<li><strong>Retry check</strong> → If <code>job.attempts &lt; job.max_retries</code>, schedule retry; otherwise move to dead letter</li>\n<li><strong>Retry scheduling</strong> → Job status changes to <code>RETRY_SCHEDULED</code> and is added to Redis sorted set with execution timestamp</li>\n<li><strong>Retry execution</strong> → <code>Scheduler</code> polls sorted set, finds due retries, re-enqueues to appropriate queue</li>\n<li><strong>Dead letter handling</strong> → Jobs in <code>DEAD_LETTER</code> status can be manually inspected and retried via dashboard</li>\n</ol>\n<h3 id=\"retry-scheduling-and-backoff-algorithm\">Retry Scheduling and Backoff Algorithm</h3>\n<p>The retry scheduling algorithm ensures failed jobs are retried with increasing delays to avoid overwhelming recovering systems while maintaining fairness across different job types. Here&#39;s the complete algorithm executed by <code>RetryManager.handle_job_failure</code>:</p>\n<ol>\n<li><p><strong>Record error details</strong>:</p>\n<ul>\n<li>Extract exception class name, message, and formatted stack trace</li>\n<li>Create error dictionary with timestamp and attempt number</li>\n<li>Append to <code>job.errors</code> list (capped at last 10 errors to prevent unbounded growth)</li>\n</ul>\n</li>\n<li><p><strong>Increment attempt counter</strong>:</p>\n<ul>\n<li>Increment <code>job.attempts</code> by 1</li>\n<li>Update <code>job.started_at = None</code> and <code>job.completed_at = None</code> to reset execution timing</li>\n</ul>\n</li>\n<li><p><strong>Check retry eligibility</strong>:</p>\n<ul>\n<li>If <code>job.attempts &gt; job.max_retries</code>: proceed to dead letter queue</li>\n<li>If job has <code>no_retry</code> metadata flag: proceed to dead letter queue</li>\n<li>If error is in permanent failure list (configurable): proceed to dead letter queue</li>\n<li>Otherwise: schedule retry</li>\n</ul>\n</li>\n<li><p><strong>Calculate retry delay</strong> (exponential backoff with jitter):</p>\n<ul>\n<li>Base formula: <code>delay = base_delay * (2 ^ (attempt - 1))</code></li>\n<li>Apply optional jitter: <code>delay_with_jitter = delay * (1 ± random_jitter_factor)</code></li>\n<li>Enforce maximum delay cap: <code>final_delay = min(delay_with_jitter, max_delay)</code></li>\n<li>Add minimum floor: <code>final_delay = max(final_delay, min_delay)</code></li>\n</ul>\n</li>\n<li><p><strong>Schedule retry execution</strong>:</p>\n<ul>\n<li>Calculate retry timestamp: <code>retry_at = current_time + final_delay</code></li>\n<li>Set job status to <code>RETRY_SCHEDULED</code></li>\n<li>Store job in Redis sorted set with score = retry_at (Unix timestamp)</li>\n<li>Also store job in Redis hash for quick retrieval by job ID</li>\n</ul>\n</li>\n<li><p><strong>Move to dead letter queue</strong> (if retries exhausted):</p>\n<ul>\n<li>Set job status to <code>DEAD_LETTER</code></li>\n<li>Store job in dead letter Redis hash (keyed by job ID)</li>\n<li>Also add to dead letter list for chronological access</li>\n<li>Optionally send notification/alert about permanent failure</li>\n</ul>\n</li>\n</ol>\n<p>The retry scheduling uses Redis sorted sets for time-ordered execution:</p>\n<table>\n<thead>\n<tr>\n<th>Redis Key Pattern</th>\n<th>Data Type</th>\n<th>Purpose</th>\n<th>Example Key</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>retry:scheduled:{queue}</code></td>\n<td>Sorted Set (ZSET)</td>\n<td>Stores job IDs with retry timestamp as score</td>\n<td><code>retry:scheduled:email</code></td>\n</tr>\n<tr>\n<td><code>retry:jobs:{job_id}</code></td>\n<td>Hash (HASH)</td>\n<td>Stores full job data for scheduled retries</td>\n<td><code>retry:jobs:01H5XYZABCDEF</code></td>\n</tr>\n<tr>\n<td><code>dead:letter:{queue}</code></td>\n<td>List (LIST)</td>\n<td>Chronological list of dead letter job IDs</td>\n<td><code>dead:letter:email</code></td>\n</tr>\n<tr>\n<td><code>dead:jobs:{job_id}</code></td>\n<td>Hash (HASH)</td>\n<td>Stores full job data for dead letter jobs</td>\n<td><code>dead:jobs:01H5XYZABCDEF</code></td>\n</tr>\n</tbody></table>\n<p>Here&#39;s a concrete walk-through: Consider a <code>send_email</code> job with <code>max_retries=3</code> and <code>base_delay=1</code> second that fails due to a temporary SMTP connection error.</p>\n<ol>\n<li><strong>Attempt 1 fails at 10:00:00</strong> → Error recorded, attempts=1, delay = 1 * 2^(1-1) = 1 second</li>\n<li><strong>Scheduled retry at 10:00:01</strong> → Job added to sorted set with score=10:00:01 (Unix timestamp)</li>\n<li><strong>Attempt 2 fails at 10:00:01</strong> → Error recorded, attempts=2, delay = 1 * 2^(2-1) = 2 seconds  </li>\n<li><strong>Scheduled retry at 10:00:03</strong> → Job added to sorted set with score=10:00:03</li>\n<li><strong>Attempt 3 fails at 10:00:03</strong> → Error recorded, attempts=3, delay = 1 * 2^(3-1) = 4 seconds</li>\n<li><strong>Scheduled retry at 10:00:07</strong> → Job added to sorted set with score=10:00:07</li>\n<li><strong>Attempt 4 (final) fails at 10:00:07</strong> → attempts=4 &gt; max_retries=3 → Move to dead letter queue</li>\n</ol>\n<blockquote>\n<p><strong>Key Insight</strong>: The exponential backoff algorithm (<code>delay = base * 2^(attempt-1)</code>) creates a geometric progression that quickly increases delays while remaining computationally simple. Adding jitter (±10-20%) prevents retry stampedes where many jobs retry simultaneously after a system outage.</p>\n</blockquote>\n<h3 id=\"adr-backoff-algorithm-selection\">ADR: Backoff Algorithm Selection</h3>\n<blockquote>\n<p><strong>Decision: Exponential Backoff with Configurable Jitter</strong></p>\n</blockquote>\n<p><strong>Context</strong>: When jobs fail due to transient errors (network timeouts, temporary resource constraints), immediate retries can exacerbate the problem by creating retry storms that overwhelm recovering systems. We need a backoff algorithm that spaces retries further apart with each attempt while providing some randomness to avoid synchronization.</p>\n<p><strong>Options Considered</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Fixed delay</strong> (constant interval)</td>\n<td>Simple to implement and understand</td>\n<td>Doesn&#39;t adapt to failure severity; can cause persistent overload</td>\n</tr>\n<tr>\n<td><strong>Linear backoff</strong> (delay = base * attempt)</td>\n<td>Gradually increases delay</td>\n<td>May increase too slowly for serious outages; predictable pattern</td>\n</tr>\n<tr>\n<td><strong>Exponential backoff</strong> (delay = base * 2^(attempt-1))</td>\n<td>Quickly reduces retry pressure; standard in distributed systems</td>\n<td>Can create long waits for users; may exceed reasonable timeouts</td>\n</tr>\n<tr>\n<td><strong>Fibonacci backoff</strong> (delay = base * fib(attempt))</td>\n<td>More gradual than exponential; less extreme waits</td>\n<td>More complex; less standardized in industry</td>\n</tr>\n<tr>\n<td><strong>Randomized exponential</strong> (exponential + jitter)</td>\n<td>Prevents synchronization; standard AWS/Google approach</td>\n<td>Slightly more complex than pure exponential</td>\n</tr>\n</tbody></table>\n<p><strong>Decision</strong>: We selected <strong>Exponential backoff with configurable jitter</strong> as the default algorithm.</p>\n<p><strong>Rationale</strong>:</p>\n<ol>\n<li><strong>Industry standard</strong>: AWS, Google Cloud, and major message queues use exponential backoff, making it familiar to developers</li>\n<li><strong>Mathematical properties</strong>: Doubling delay each attempt provides O(2^n) growth which quickly reduces retry pressure during outages</li>\n<li><strong>Configurable</strong>: Base delay and jitter factor can be tuned per job type (sensitive jobs can use smaller base, batch jobs larger)</li>\n<li><strong>Jitter prevents synchronization</strong>: Without jitter, all jobs that failed at the same time retry simultaneously, potentially causing cascading failures. Adding ±10-25% random jitter spreads retries.</li>\n<li><strong>Predictable worst-case</strong>: Maximum wait time can be calculated as <code>sum(base * 2^i for i in 0..max_retries-1)</code> which helps set SLAs</li>\n</ol>\n<p><strong>Consequences</strong>:</p>\n<ul>\n<li><strong>Positive</strong>: System naturally backs off during outages, reducing load on recovering services</li>\n<li><strong>Positive</strong>: Jitter prevents thundering herd problems during recovery</li>\n<li><strong>Negative</strong>: Users of synchronous wrappers may experience long waits (mitigated by setting reasonable max_retries)</li>\n<li><strong>Negative</strong>: Jobs with tight deadlines may need custom retry strategies (supported via hooks)</li>\n</ul>\n<p>The algorithm is implemented in a <code>BackoffCalculator</code> class with these configurable parameters:</p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Default</th>\n<th>Description</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>base_delay_seconds</code></td>\n<td>1</td>\n<td>Initial delay for first retry</td>\n<td>1 second allows quick recovery from transient blips</td>\n</tr>\n<tr>\n<td><code>max_delay_seconds</code></td>\n<td>3600</td>\n<td>Maximum delay between retries</td>\n<td>1 hour prevents excessively long delays</td>\n</tr>\n<tr>\n<td><code>jitter_factor</code></td>\n<td>0.1</td>\n<td>Random variation (±10%)</td>\n<td>10% provides good desynchronization without excessive unpredictability</td>\n</tr>\n<tr>\n<td><code>min_delay_seconds</code></td>\n<td>0.1</td>\n<td>Minimum delay (floor)</td>\n<td>100ms prevents near-instant retries</td>\n</tr>\n</tbody></table>\n<h3 id=\"adr-dead-letter-queue-design\">ADR: Dead Letter Queue Design</h3>\n<blockquote>\n<p><strong>Decision: Dual Storage Dead Letter Queue with Manual Intervention</strong></p>\n</blockquote>\n<p><strong>Context</strong>: When jobs exhaust all retry attempts, we need to preserve them for investigation rather than silently discarding them. The dead letter queue (DLQ) must store failed jobs with complete error history while supporting manual review, retry, or deletion. The design must balance storage efficiency with queryability.</p>\n<p><strong>Options Considered</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis List only</strong></td>\n<td>Simple FIFO storage; easy to pop oldest jobs</td>\n<td>No random access by job ID; limited metadata querying</td>\n</tr>\n<tr>\n<td><strong>Redis Hash only</strong></td>\n<td>Fast job retrieval by ID; supports partial updates</td>\n<td>No chronological ordering; hard to list all jobs</td>\n</tr>\n<tr>\n<td><strong>Redis Sorted Set</strong></td>\n<td>Chronological ordering by failure time; range queries</td>\n<td>Duplicate storage of job data; more complex</td>\n</tr>\n<tr>\n<td><strong>Dual storage (List + Hash)</strong></td>\n<td>Chronological list + random access; best of both</td>\n<td>Double storage overhead; need to keep synchronized</td>\n</tr>\n<tr>\n<td><strong>External database</strong></td>\n<td>Unlimited storage; rich querying</td>\n<td>Additional infrastructure; operational complexity</td>\n</tr>\n</tbody></table>\n<p><strong>Decision</strong>: We selected <strong>Dual storage using Redis List and Hash</strong> for the dead letter queue.</p>\n<p><strong>Rationale</strong>:</p>\n<ol>\n<li><strong>Chronological access needed</strong>: Operators want to see &quot;most recent failures&quot; first, which Redis List provides via <code>LRANGE</code></li>\n<li><strong>Random access needed</strong>: When investigating a specific job ID from logs/alerts, Hash provides O(1) lookup</li>\n<li><strong>Redis already available</strong>: No new infrastructure dependencies</li>\n<li><strong>Controlled storage</strong>: We can implement automatic expiration/TTL to prevent unlimited growth</li>\n<li><strong>Atomic synchronization</strong>: Redis transactions can keep List and Hash in sync atomically</li>\n</ol>\n<p><strong>Consequences</strong>:</p>\n<ul>\n<li><strong>Positive</strong>: Both chronological browsing and direct job lookup are efficient</li>\n<li><strong>Positive</strong>: Leverages existing Redis infrastructure without new dependencies  </li>\n<li><strong>Negative</strong>: 2x storage overhead for DLQ jobs (mitigated by compression and TTL)</li>\n<li><strong>Negative</strong>: Manual cleanup required for List/Hash synchronization (handled in cleanup jobs)</li>\n</ul>\n<p>The dead letter queue implementation stores each job in two places:</p>\n<ol>\n<li><strong>Chronological list</strong>: <code>dead:letter:{queue}</code> (Redis List) contains job IDs in insertion order</li>\n<li><strong>Job data store</strong>: <code>dead:jobs:{job_id}</code> (Redis Hash) contains full serialized job with error history</li>\n</ol>\n<p>When moving a job to DLQ, this atomic operation occurs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">redis</span><pre class=\"arch-pre shiki-highlighted\"><code>MULTI\nHSET dead:jobs:{job_id} data {serialized_job} timestamp {failure_time}\nRPUSH dead:letter:{queue} {job_id}\nEXPIRE dead:jobs:{job_id} {dlq_ttl}\nEXPIRE dead:letter:{queue} {dlq_ttl}\nEXEC</code></pre></div>\n\n<blockquote>\n<p><strong>Design Principle</strong>: The dead letter queue is a diagnostic tool, not a permanent archive. Jobs automatically expire after a configurable TTL (default 30 days) to prevent unbounded Redis memory growth. Important failures should be alerted on and investigated promptly.</p>\n</blockquote>\n<h3 id=\"common-pitfalls-in-retry-implementation\">Common Pitfalls in Retry Implementation</h3>\n<p>⚠️ <strong>Pitfall: Infinite retry loops without limits</strong></p>\n<ul>\n<li><strong>Description</strong>: Forgetting to enforce <code>max_retries</code> or setting it too high (or infinite)</li>\n<li><strong>Why it&#39;s wrong</strong>: Jobs with permanent failures (bad data, broken integrations) retry forever, wasting resources and hiding real problems</li>\n<li><strong>How to fix</strong>: Always set a reasonable default <code>max_retries</code> (e.g., 3-5) and validate that <code>job.attempts &lt;= job.max_retries</code> before scheduling retry</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Retrying non-idempotent operations without safeguards</strong></p>\n<ul>\n<li><strong>Description</strong>: Automatically retrying jobs that have side effects (charging credit cards, sending emails) can cause duplicate charges/spam</li>\n<li><strong>Why it&#39;s wrong</strong>: Business logic executes multiple times for what should be a single operation</li>\n<li><strong>How to fix</strong>: Either (1) mark non-idempotent jobs with <code>no_retry</code> metadata flag, or (2) implement idempotency keys in job handlers that check for previous execution</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Losing error context between retries</strong></p>\n<ul>\n<li><strong>Description</strong>: Only storing the latest error instead of full error history</li>\n<li><strong>Why it&#39;s wrong</strong>: Operators can&#39;t see if the failure mode changed (network → database → validation), which is crucial for debugging</li>\n<li><strong>How to fix</strong>: Append each error with timestamp, exception class, message, and stack trace to <code>job.errors</code> list</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Blocking retries on Redis sorted set polling</strong></p>\n<ul>\n<li><strong>Description</strong>: Using <code>ZRANGEBYSCORE</code> without pagination on large retry sets can block Redis</li>\n<li><strong>Why it&#39;s wrong</strong>: Large numbers of scheduled retries cause long-running Redis commands that delay other operations</li>\n<li><strong>How to fix</strong>: Process retries in batches (e.g., 100 at a time) and use <code>ZRANGEBYSCORE ... LIMIT</code> for pagination</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not handling clock skew in distributed retry scheduling</strong></p>\n<ul>\n<li><strong>Description</strong>: Assuming all workers and scheduler have synchronized clocks when comparing retry timestamps</li>\n<li><strong>Why it&#39;s wrong</strong>: Jobs may retry too early or too late if system clocks differ</li>\n<li><strong>How to fix</strong>: Use Redis server time (<code>TIME</code> command) for retry scheduling, not local system time</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Dead letter queue without size limits or expiration</strong></p>\n<ul>\n<li><strong>Description</strong>: Storing all failed jobs indefinitely in Redis</li>\n<li><strong>Why it&#39;s wrong</strong>: Redis memory fills up, causing evictions or crashes</li>\n<li><strong>How to fix</strong>: Implement automatic cleanup based on age (TTL) and/or maximum queue size, with configurable limits</li>\n</ul>\n<h3 id=\"implementation-guidance-for-retry-system\">Implementation Guidance for Retry System</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Retry Storage</td>\n<td>Redis sorted sets + hashes</td>\n<td>Redis Streams with consumer groups</td>\n</tr>\n<tr>\n<td>Error Serialization</td>\n<td>JSON with Python&#39;s <code>traceback.format_exc()</code></td>\n<td>Structured logging with error codes</td>\n</tr>\n<tr>\n<td>Backoff Algorithm</td>\n<td>Exponential with fixed jitter</td>\n<td>Adaptive backoff based on error type</td>\n</tr>\n<tr>\n<td>Dead Letter Queue</td>\n<td>Redis List + Hash with TTL</td>\n<td>External database (PostgreSQL) for long-term storage</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background-job-processor/\n├── src/\n│   ├── job_processor/\n│   │   ├── __init__.py\n│   │   ├── job.py                    # Job class definition\n│   │   ├── redis_client.py           # RedisClient wrapper\n│   │   ├── queue_manager.py          # QueueManager implementation\n│   │   ├── worker.py                 # Worker implementation  \n│   │   ├── retry/                    # ← NEW: Retry subsystem\n│   │   │   ├── __init__.py\n│   │   │   ├── manager.py            # RetryManager class\n│   │   │   ├── backoff.py            # BackoffCalculator class\n│   │   │   ├── dead_letter.py        # DeadLetterQueue class\n│   │   │   └── errors.py             # Retry-specific exceptions\n│   │   ├── scheduler.py              # Scheduler (for retry polling)\n│   │   └── utils/\n│   │       ├── serialization.py\n│   │       └── time_utils.py\n├── tests/\n│   ├── test_retry_manager.py\n│   ├── test_backoff.py\n│   └── test_dead_letter.py\n└── pyproject.toml</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/job_processor/retry/backoff.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Exponential backoff calculator with jitter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BackoffConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for exponential backoff with jitter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    base_delay_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_delay_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3600.0</span><span style=\"color:#6A737D\">  # 1 hour</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    min_delay_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">     # 100ms minimum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    jitter_factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">         # ±10% randomness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BackoffCalculator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculates retry delays using exponential backoff with jitter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: Optional[BackoffConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> BackoffConfig()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_delay</span><span style=\"color:#E1E4E8\">(self, attempt: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate retry delay for given attempt number.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            attempt: Current attempt number (1-based, where 1 is first retry)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Delay in seconds before next retry attempt</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            attempt </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Exponential backoff: base * 2^(attempt-1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        exponential_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.base_delay_seconds </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> **</span><span style=\"color:#E1E4E8\"> (attempt </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply jitter: add random variation ±jitter_factor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.jitter_factor </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            jitter </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> random.uniform(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                -</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.jitter_factor, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.config.jitter_factor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exponential_delay </span><span style=\"color:#F97583\">*=</span><span style=\"color:#E1E4E8\"> jitter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Enforce minimum and maximum bounds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bounded_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.min_delay_seconds, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                           min</span><span style=\"color:#E1E4E8\">(exponential_delay, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.max_delay_seconds))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> bounded_delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_next_retry_time</span><span style=\"color:#E1E4E8\">(self, attempt: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate absolute timestamp for next retry attempt.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            attempt: Current attempt number</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Unix timestamp (seconds since epoch) for next retry</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.calculate_delay(attempt)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> delay</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/job_processor/retry/manager.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Retry manager for handling job failures and scheduling retries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job, JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..redis_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .backoff </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BackoffCalculator, BackoffConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .dead_letter </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DeadLetterQueue</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages retry logic for failed jobs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: RedisClient, config: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize retry manager.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            redis_client: Redis client for storing retry state</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            config: Optional configuration dictionary</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Configuration with defaults</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'base_delay'</span><span style=\"color:#E1E4E8\">: config.get(</span><span style=\"color:#9ECBFF\">'base_delay'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'max_delay'</span><span style=\"color:#E1E4E8\">: config.get(</span><span style=\"color:#9ECBFF\">'max_delay'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3600.0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'max_attempts'</span><span style=\"color:#E1E4E8\">: config.get(</span><span style=\"color:#9ECBFF\">'max_attempts'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'jitter_factor'</span><span style=\"color:#E1E4E8\">: config.get(</span><span style=\"color:#9ECBFF\">'jitter_factor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'retry_key_prefix'</span><span style=\"color:#E1E4E8\">: config.get(</span><span style=\"color:#9ECBFF\">'retry_key_prefix'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'retry:'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'dead_letter_key_prefix'</span><span style=\"color:#E1E4E8\">: config.get(</span><span style=\"color:#9ECBFF\">'dead_letter_key_prefix'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'dead:'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.backoff_calculator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BackoffCalculator(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            BackoffConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                base_delay_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config[</span><span style=\"color:#9ECBFF\">'base_delay'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_delay_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config[</span><span style=\"color:#9ECBFF\">'max_delay'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_attempts</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config[</span><span style=\"color:#9ECBFF\">'max_attempts'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                jitter_factor</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config[</span><span style=\"color:#9ECBFF\">'jitter_factor'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dead_letter_queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DeadLetterQueue(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            redis_client,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            key_prefix</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config[</span><span style=\"color:#9ECBFF\">'dead_letter_key_prefix'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_job_failure</span><span style=\"color:#E1E4E8\">(self, job: Job, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle a job failure by scheduling retry or moving to dead letter queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Called by Worker when a job execution fails.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: The failed job instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            error: Exception that caused the failure</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            None</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Record the error in job.errors list using job.record_error(error)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: job.record_error() should capture exception class, message, and stack trace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Increment job.attempts counter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if job should be retried (job.should_retry() method)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Consider: max_retries, no_retry metadata flag, permanent error types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If job should be retried:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Calculate retry delay using self.backoff_calculator.calculate_delay(job.attempts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call self.schedule_retry(job, delay_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Update job.status = JobStatus.RETRY_SCHEDULED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If job should NOT be retried (retries exhausted or permanent error):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call self.move_to_dead_letter(job)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Update job.status = JobStatus.DEAD_LETTER</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Log warning about permanent failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store updated job state in Redis (job.to_dict() and HSET)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use Redis pipeline for atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> schedule_retry</span><span style=\"color:#E1E4E8\">(self, job: Job, delay_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Schedule a job for retry after specified delay.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: Job to retry</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            delay_seconds: Delay before retry (in seconds)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Job ID of the scheduled retry</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate retry timestamp: current_time + delay_seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use time.time() for current Unix timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create retry key names:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   retry_scheduled_key = f\"{self.config['retry_key_prefix']}scheduled:{job.queue}\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   retry_job_key = f\"{self.config['retry_key_prefix']}jobs:{job.job_id}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use Redis pipeline for atomic operations:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - ZADD retry_scheduled_key {retry_timestamp} {job.job_id}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - HSET retry_job_key data {job.serialize()}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - EXPIRE retry_job_key {delay_seconds * 2} (TTL slightly longer than retry delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute pipeline and return job.job_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Log retry scheduling at INFO level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> move_to_dead_letter</span><span style=\"color:#E1E4E8\">(self, job: Job) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Move a job to the dead letter queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: Job that has exhausted retries</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if successful, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Call self.dead_letter_queue.store(job)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # The DeadLetterQueue class should handle dual storage (List + Hash)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Remove job from any retry scheduled sets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use ZREM on retry_scheduled_key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return success status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_due_retries</span><span style=\"color:#E1E4E8\">(self, queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> List[Job]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Retrieve jobs whose retry time has arrived.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Called by Scheduler component.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue_name: Queue to check for due retries</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_count: Maximum number of jobs to return at once</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of Job instances ready for retry</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current timestamp using time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Construct retry scheduled key: f\"{self.config['retry_key_prefix']}scheduled:{queue_name}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use ZRANGEBYSCORE with scores 0 to current_timestamp, LIMIT 0 max_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This gets job IDs whose retry time has arrived</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each job_id retrieved:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Get job data from HGET retry:jobs:{job_id}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Deserialize into Job object using Job.deserialize()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Remove from retry scheduled set (ZREM)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Delete retry job hash (HDEL)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use pipeline for atomic multi-get and cleanup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of Job objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ol>\n<li><strong>Error serialization</strong>: Use Python&#39;s <code>traceback.format_exception()</code> to capture full stack traces:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   error_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       'type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">(error).</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       'message'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(error),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       'traceback'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">.join(traceback.format_exception(</span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">(error), error, error.</span><span style=\"color:#79B8FF\">__traceback__</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Redis atomic operations</strong>: Always use pipelines for multi-step Redis operations:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">   pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.get_client().pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   pipe.zadd(</span><span style=\"color:#9ECBFF\">'retry:scheduled:email'</span><span style=\"color:#E1E4E8\">, {job_id: retry_timestamp})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   pipe.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'retry:jobs:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'data'</span><span style=\"color:#E1E4E8\">, job.serialize())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   pipe.expire(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'retry:jobs:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(delay_seconds </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   pipe.execute()  </span><span style=\"color:#6A737D\"># All commands execute atomically</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Configuration management</strong>: Use Python&#39;s <code>dataclasses</code> for type-safe configuration:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   @dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   class</span><span style=\"color:#B392F0\"> RetryConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       base_delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       max_delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3600.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       jitter_factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       per_job_type_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the retry system, verify functionality with these tests:</p>\n<ol>\n<li><strong>Basic retry flow</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Enqueue a job that will fail</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Job(</span><span style=\"color:#FFAB70\">job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'failing_task'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_retries</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_manager.enqueue_job(job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Worker processes and fails the job</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   worker.start()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Job should move to retry scheduled state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Check Redis for retry scheduling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retry_manager.get_due_retries(</span><span style=\"color:#9ECBFF\">'default'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Should be empty initially (not yet due)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Wait for retry delay, then check again</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   time.sleep(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Wait for first retry (1s base delay + jitter)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retry_manager.get_due_retries(</span><span style=\"color:#9ECBFF\">'default'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Should contain the job now</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Dead letter queue test</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Create job with max_retries=0 that fails</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Job(</span><span style=\"color:#FFAB70\">job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'permanent_failure'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_retries</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_manager.enqueue_job(job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Worker processes and fails</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   worker.process_one_job()  </span><span style=\"color:#6A737D\"># Should move directly to DLQ</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Check dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   dlq_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dead_letter_queue.list_jobs(</span><span style=\"color:#9ECBFF\">'default'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Should contain the failed job with DEAD_LETTER status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Manual retry from DLQ</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   retried_job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retry_manager.retry_dead_letter_job(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Should return Job object and remove from DLQ</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Error history verification</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Process a job that fails multiple times</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> get_job_from_redis(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">   print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Error count: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(job.errors)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should equal attempts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   for</span><span style=\"color:#E1E4E8\"> i, error </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(job.errors):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Attempt </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error[</span><span style=\"color:#9ECBFF\">'type'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error[</span><span style=\"color:#9ECBFF\">'message'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Should show full error history across all attempts</span></span></code></pre></div>\n\n<p><strong>Expected output signs</strong>:</p>\n<ul>\n<li>Jobs with transient failures automatically retry with increasing delays</li>\n<li>After final failure, jobs appear in dead letter queue with complete error history</li>\n<li>Redis memory usage remains bounded (TTL working)</li>\n<li>No infinite retry loops occur</li>\n</ul>\n<p><strong>Diagnose problems</strong>:</p>\n<ul>\n<li><strong>Symptom</strong>: Jobs retry immediately without delay → Check <code>BackoffCalculator.calculate_delay()</code> logic</li>\n<li><strong>Symptom</strong>: Jobs disappear after final failure (not in DLQ) → Check <code>move_to_dead_letter()</code> implementation</li>\n<li><strong>Symptom</strong>: Redis memory growing unbounded → Check TTL/EXPIRE commands on retry and DLQ keys</li>\n<li><strong>Symptom</strong>: Error history missing stack traces → Check <code>Job.record_error()</code> implementation</li>\n</ul>\n<p><strong>G. Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Jobs retry forever</td>\n<td><code>max_retries</code> not enforced or too high</td>\n<td>Check job.attempts vs job.max_retries in logs</td>\n<td>Implement retry limit check in <code>handle_job_failure()</code></td>\n</tr>\n<tr>\n<td>All retries happen simultaneously</td>\n<td>Missing jitter in backoff algorithm</td>\n<td>Check retry timestamps in Redis sorted set</td>\n<td>Add jitter factor to <code>BackoffCalculator</code></td>\n</tr>\n<tr>\n<td>Error history shows only last error</td>\n<td>Errors overwritten instead of appended</td>\n<td>Inspect <code>job.errors</code> list after multiple failures</td>\n<td>Ensure <code>job.errors.append()</code> not <code>job.errors = </code></td>\n</tr>\n<tr>\n<td>Dead letter queue grows indefinitely</td>\n<td>Missing TTL/expiration</td>\n<td>Check Redis keys with <code>TTL dead:jobs:*</code></td>\n<td>Add EXPIRE commands when storing in DLQ</td>\n</tr>\n<tr>\n<td>Retries happen at wrong times</td>\n<td>Clock skew between systems</td>\n<td>Compare Redis TIME with system time</td>\n<td>Use Redis server time for scheduling</td>\n</tr>\n<tr>\n<td>Manual retry from DLQ fails</td>\n<td>Job not removed from DLQ lists</td>\n<td>Check both List and Hash storage after retry</td>\n<td>Ensure atomic removal from both data structures</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-design-scheduling-amp-cron\">Component Design: Scheduling &amp; Cron</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4: Scheduling &amp; Cron. This section provides the complete design for delayed and recurring job scheduling using cron expressions, enabling jobs to be executed at specific future times or on recurring schedules with unique constraint enforcement and timezone handling.</p>\n</blockquote>\n<h3 id=\"mental-model-calendar-app-with-recurring-events\">Mental Model: Calendar App with Recurring Events</h3>\n<p>Think of the scheduler as a sophisticated calendar application with recurring event capabilities. Just as a calendar app stores events with specific dates, times, and recurrence rules (like &quot;every Monday at 9 AM&quot;), then notifies you when each event occurs, the scheduler stores jobs with execution times and recurrence patterns, then &quot;notifies&quot; the system by enqueuing them at the right moment.</p>\n<p>Imagine setting up a weekly team meeting in your calendar:</p>\n<ol>\n<li><strong>Initial Setup</strong>: You create an event titled &quot;Weekly Sync&quot; with start time &quot;next Monday 10:00 AM&quot; and recurrence &quot;weekly&quot;</li>\n<li><strong>Storage</strong>: The calendar stores this as a recurring event definition with the original start time and recurrence rule</li>\n<li><strong>Notification</strong>: Every week, your calendar checks which events are due and sends you a notification</li>\n<li><strong>Uniqueness</strong>: If you try to create the same event twice for the same time slot, the calendar prevents duplicates</li>\n<li><strong>Missed Events</strong>: If your phone was off during an event time, the calendar shows it as &quot;missed&quot; when you turn it back on</li>\n</ol>\n<p>The scheduler operates similarly:</p>\n<ul>\n<li><strong>Delayed Jobs</strong>: Like one-time calendar events (&quot;dentist appointment next Tuesday at 3 PM&quot;)</li>\n<li><strong>Recurring Jobs</strong>: Like repeating events (&quot;team meeting every Monday at 10 AM&quot;)</li>\n<li><strong>Notification</strong>: Instead of alerting you, it enqueues jobs into worker queues</li>\n<li><strong>Missed Jobs</strong>: If the scheduler is down, it detects missed schedules and enqueues overdue jobs on restart</li>\n</ul>\n<p>This mental model helps understand the core responsibilities: storing schedule definitions, calculating when jobs are due, ensuring uniqueness, and handling timezone complications.</p>\n<h3 id=\"scheduler-interface\">Scheduler Interface</h3>\n<p>The scheduler component provides a clean API for scheduling jobs for future execution, whether as one-time delayed jobs or recurring jobs based on cron expressions. It operates as a standalone process that periodically checks for due jobs and enqueues them into the appropriate worker queues.</p>\n<p><strong>Primary Responsibilities:</strong></p>\n<ol>\n<li>Store and manage schedule definitions for delayed and recurring jobs</li>\n<li>Periodically poll for jobs whose execution time has arrived</li>\n<li>Evaluate cron expressions to calculate next execution times</li>\n<li>Enforce uniqueness constraints to prevent duplicate job enqueueing</li>\n<li>Handle timezone conversions consistently</li>\n<li>Recover from scheduler downtime by catching up on missed jobs</li>\n</ol>\n<p><strong>Core Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Type Name</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Schedule</code></td>\n<td><code>schedule_id str</code>, <code>job_type str</code>, <code>args List[Any]</code>, <code>kwargs Dict[str, Any]</code>, <code>queue str</code>, <code>cron_expression Optional[str]</code>, <code>run_at Optional[datetime]</code>, <code>timezone str</code>, <code>enabled bool</code>, <code>unique_key Optional[str]</code>, <code>unique_window_seconds int</code>, <code>created_at datetime</code>, <code>last_enqueued_at Optional[datetime]</code>, <code>next_run_at Optional[datetime]</code>, <code>metadata Dict[str, Any]</code></td>\n<td>Defines a scheduled job, either one-time (<code>run_at</code>) or recurring (<code>cron_expression</code>). The <code>unique_key</code> prevents duplicate enqueueing within the <code>unique_window_seconds</code>.</td>\n</tr>\n<tr>\n<td><code>ScheduledJob</code></td>\n<td><code>schedule_id str</code>, <code>job_id str</code>, <code>scheduled_for datetime</code>, <code>enqueued_at Optional[datetime]</code>, <code>status ScheduleStatus</code></td>\n<td>Represents a specific scheduled execution instance, tracking when a job from a schedule should run and whether it has been enqueued.</td>\n</tr>\n<tr>\n<td><code>ScheduleStatus</code></td>\n<td><code>PENDING</code>, <code>ENQUEUED</code>, <code>SKIPPED</code>, <code>ERROR</code></td>\n<td>Status of individual scheduled job instances. <code>SKIPPED</code> occurs when uniqueness constraint prevents enqueueing.</td>\n</tr>\n</tbody></table>\n<p><strong>Scheduler Interface Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>schedule_delayed_job</code></td>\n<td><code>job Job</code>, <code>run_at datetime</code></td>\n<td><code>str</code> (schedule_id)</td>\n<td>Schedule a job for one-time execution at <code>run_at</code>. Validates that <code>run_at</code> is in the future.</td>\n</tr>\n<tr>\n<td><code>schedule_recurring_job</code></td>\n<td><code>job Job</code>, <code>cron_expression str</code>, <code>timezone str = &quot;UTC&quot;</code></td>\n<td><code>str</code> (schedule_id)</td>\n<td>Schedule a job for recurring execution based on cron expression. Validates cron syntax and calculates initial <code>next_run_at</code>.</td>\n</tr>\n<tr>\n<td><code>unschedule_job</code></td>\n<td><code>schedule_id str</code></td>\n<td><code>bool</code></td>\n<td>Remove a schedule by ID, preventing future executions. Returns <code>True</code> if schedule existed and was removed.</td>\n</tr>\n<tr>\n<td><code>pause_schedule</code></td>\n<td><code>schedule_id str</code></td>\n<td><code>bool</code></td>\n<td>Temporarily disable a schedule without removing it. Future polling will skip this schedule.</td>\n</tr>\n<tr>\n<td><code>resume_schedule</code></td>\n<td><code>schedule_id str</code></td>\n<td><code>bool</code></td>\n<td>Re-enable a paused schedule. Next run time is recalculated from current time.</td>\n</tr>\n<tr>\n<td><code>get_schedule</code></td>\n<td><code>schedule_id str</code></td>\n<td><code>Optional[Schedule]</code></td>\n<td>Retrieve schedule definition by ID, including current status and next run time.</td>\n</tr>\n<tr>\n<td><code>list_schedules</code></td>\n<td><code>enabled_only bool = False</code></td>\n<td><code>List[Schedule]</code></td>\n<td>List all schedules, optionally filtered to only enabled ones.</td>\n</tr>\n<tr>\n<td><code>update_schedule</code></td>\n<td><code>schedule_id str</code>, <code>updates Dict[str, Any]</code></td>\n<td><code>bool</code></td>\n<td>Update specific fields of a schedule (e.g., cron expression, timezone). Recalculates <code>next_run_at</code> if cron or timezone changes.</td>\n</tr>\n<tr>\n<td><code>enqueue_due_jobs</code></td>\n<td><code>max_jobs int = 100</code></td>\n<td><code>int</code> (jobs enqueued)</td>\n<td>Main polling method: finds due jobs, enqueues them to worker queues, updates schedule state. Returns count of jobs successfully enqueued.</td>\n</tr>\n</tbody></table>\n<p><strong>Schedule Storage Strategy:</strong></p>\n<p>The scheduler uses two primary Redis data structures:</p>\n<ol>\n<li><strong>Schedule Definitions Hash</strong>: <code>schedules:{schedule_id}</code> stores the full <code>Schedule</code> object as a hash for quick lookup and updates</li>\n<li><strong>Schedule Timeline Sorted Set</strong>: <code>schedules:timeline</code> uses <code>next_run_at</code> timestamps as scores and <code>schedule_id</code> as members for efficient polling of due schedules</li>\n</ol>\n<p><strong>Example Schedule Creation Flow:</strong></p>\n<ol>\n<li>Client calls <code>schedule_recurring_job(job, &quot;0 9 * * 1&quot;, &quot;America/New_York&quot;)</code> (every Monday at 9 AM Eastern)</li>\n<li>Scheduler validates cron expression, parses timezone, calculates next Monday 9 AM in that timezone</li>\n<li>Creates <code>Schedule</code> object with <code>schedule_id</code>, stores in Redis hash</li>\n<li>Adds <code>schedule_id</code> to <code>schedules:timeline</code> sorted set with score = next run timestamp</li>\n<li>Returns <code>schedule_id</code> to client for future reference</li>\n</ol>\n<h3 id=\"scheduler-polling-and-cron-evaluation\">Scheduler Polling and Cron Evaluation</h3>\n<p>The scheduler operates on a polling model where it periodically checks for due schedules and enqueues them. This approach balances simplicity with reliability, avoiding complex event-driven architectures while ensuring no schedules are missed during brief downtimes.</p>\n<p><strong>Polling Algorithm (Numbered Steps):</strong></p>\n<ol>\n<li><strong>Initialize Polling Loop</strong>: The scheduler starts with configuration defining <code>polling_interval_seconds</code> (default: 60 seconds) and <code>batch_size</code> (default: 100 jobs per poll)</li>\n<li><strong>Calculate Polling Window</strong>: Determine current time in UTC (<code>now_utc</code>) and calculate <code>poll_until = now_utc + polling_interval_seconds</code> to catch jobs due within the next polling interval</li>\n<li><strong>Retrieve Due Schedule IDs</strong>: Use Redis <code>ZRANGEBYSCORE</code> on <code>schedules:timeline</code> with score range <code>0</code> to <code>poll_until</code> to get schedule IDs due for execution</li>\n<li><strong>Process Each Due Schedule</strong>:<ol>\n<li>Fetch full schedule definition from Redis hash</li>\n<li>Validate schedule is still <code>enabled</code> (skip if paused)</li>\n<li>Apply uniqueness check using <code>unique_key</code> and <code>unique_window_seconds</code></li>\n<li>Create <code>Job</code> instance from schedule&#39;s <code>job_type</code>, <code>args</code>, <code>kwargs</code>, and <code>queue</code></li>\n<li>Generate <code>job_id</code> using ULID for chronological sorting</li>\n<li>Use <code>QueueManager.enqueue_job()</code> to enqueue job to appropriate worker queue</li>\n<li>Record <code>ScheduledJob</code> instance tracking this execution</li>\n<li>Update schedule&#39;s <code>last_enqueued_at</code> to current time</li>\n</ol>\n</li>\n<li><strong>Calculate Next Run for Recurring Jobs</strong>: For schedules with cron expressions, calculate next execution time using current time as reference, update <code>next_run_at</code>, and update score in timeline sorted set</li>\n<li><strong>Cleanup One-Time Delayed Jobs</strong>: For schedules with only <code>run_at</code> (no cron), remove from timeline and delete schedule definition after enqueueing</li>\n<li><strong>Handle Errors Gracefully</strong>: If any schedule fails during processing (e.g., queue full, Redis error), log error, increment error counter, but continue processing other schedules</li>\n<li><strong>Sleep Until Next Poll</strong>: Wait <code>polling_interval_seconds</code> before repeating, but adjust for processing time (e.g., if processing took 15 seconds, sleep only 45 seconds)</li>\n</ol>\n<p><strong>Cron Expression Evaluation:</strong></p>\n<p>The system uses standard 5-field cron expressions (minute, hour, day of month, month, day of week) with the following parsing and evaluation logic:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Allowed Values</th>\n<th>Special Characters</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Minute</td>\n<td>0-59</td>\n<td><code>*</code> , <code>-</code> , <code>/</code> , <code>,</code></td>\n<td><code>*/5</code> = every 5 minutes</td>\n</tr>\n<tr>\n<td>Hour</td>\n<td>0-23</td>\n<td><code>*</code> , <code>-</code> , <code>/</code> , <code>,</code></td>\n<td><code>0-8</code> = hours 0 through 8</td>\n</tr>\n<tr>\n<td>Day of Month</td>\n<td>1-31</td>\n<td><code>*</code> , <code>-</code> , <code>/</code> , <code>,</code> , <code>?</code> , <code>L</code></td>\n<td><code>L</code> = last day of month</td>\n</tr>\n<tr>\n<td>Month</td>\n<td>1-12 or JAN-DEC</td>\n<td><code>*</code> , <code>-</code> , <code>/</code> , <code>,</code></td>\n<td>Case-insensitive month names</td>\n</tr>\n<tr>\n<td>Day of Week</td>\n<td>0-7 or SUN-SAT</td>\n<td><code>*</code> , <code>-</code> , <code>/</code> , <code>,</code> , <code>?</code> , <code>L</code> , <code>#</code></td>\n<td>0 and 7 = Sunday, <code>2#3</code> = third Tuesday</td>\n</tr>\n</tbody></table>\n<p><strong>Next Run Calculation Algorithm:</strong></p>\n<ol>\n<li><strong>Start Reference Time</strong>: Use current time in schedule&#39;s timezone, or <code>last_enqueued_at</code> if calculating next run after successful enqueue</li>\n<li><strong>Increment to Next Candidate</strong>: Add 1 minute to reference time to ensure we don&#39;t re-enqueue for same minute</li>\n<li><strong>Iterate Through Fields</strong>: Check each cron field against candidate time:<ol>\n<li>If minute doesn&#39;t match, increment minutes until match found</li>\n<li>If hour doesn&#39;t match, increment hours (reset minutes to 0)</li>\n<li>If day of month doesn&#39;t match, increment days (reset hours/minutes)</li>\n<li>If month doesn&#39;t match, increment months (reset days/hours/minutes)</li>\n<li>If day of week doesn&#39;t match, increment days (could require checking multiple days)</li>\n</ol>\n</li>\n<li><strong>Handle Edge Cases</strong>: <ul>\n<li>31st day in months with only 30 days → roll to 1st of next month</li>\n<li>February 29th in non-leap years → roll to March 1st</li>\n<li>Last day of month (<code>L</code>) → calculate actual last day for that month/year</li>\n</ul>\n</li>\n<li><strong>Return Valid Time</strong>: First time that matches all cron fields, converted back to UTC for storage</li>\n</ol>\n<p><strong>Uniqueness Constraint Implementation:</strong></p>\n<p>To prevent duplicate enqueueing of the same recurring job (e.g., if scheduler polls twice in same minute), the system implements a window-based uniqueness check:</p>\n<ol>\n<li><strong>Generate Unique Key</strong>: Combine schedule ID with execution period identifier (e.g., <code>&quot;email_report:2024-03-25-09:00&quot;</code> for hourly job at 9 AM on March 25)</li>\n<li><strong>Check Redis Lock</strong>: Use Redis <code>SET key value NX EX window_seconds</code> as atomic check-and-set</li>\n<li><strong>If Key Exists</strong>: Job already enqueued within uniqueness window → skip enqueueing, mark as <code>SKIPPED</code></li>\n<li><strong>If Key Doesn&#39;t Exist</strong>: Proceed with enqueueing, set key with TTL = <code>unique_window_seconds</code></li>\n</ol>\n<p><strong>Example Polling Cycle:</strong></p>\n<ul>\n<li>Current time: 2024-03-25 09:01:30 UTC</li>\n<li>Polling interval: 60 seconds</li>\n<li>Due schedules in timeline with scores ≤ 2024-03-25 09:02:30 (now + 60s)</li>\n<li>Found: <code>schedule:report:daily</code> with cron <code>0 9 * * *</code> (9 AM daily)</li>\n<li>Schedule timezone: America/New_York (UTC-4 currently)</li>\n<li>Check: 9 AM Eastern = 13:00 UTC, not due yet → skip</li>\n<li>Found: <code>schedule:cleanup:hourly</code> with cron <code>0 * * * *</code> (top of every hour)</li>\n<li>Check: 09:00 UTC just passed, within polling window → enqueue job</li>\n<li>Calculate next run: next hour at 10:00 UTC</li>\n<li>Update timeline score to 10:00 UTC timestamp</li>\n</ul>\n<p><img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fscheduler-polling-flowchart.svg\" alt=\"Scheduler Polling Algorithm Flowchart\"></p>\n<h3 id=\"adr-scheduler-architecture-polling-vs-event\">ADR: Scheduler Architecture (Polling vs Event)</h3>\n<blockquote>\n<p><strong>Decision: Polling-Based Scheduler Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to execute jobs at specific future times or on recurring schedules. The scheduler must be reliable, handle missed schedules during downtime, and not overwhelm Redis with constant queries. We considered event-driven approaches using Redis keyspace notifications or separate timer services.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Polling Architecture</strong>: Scheduler process periodically queries Redis for due schedules</li>\n<li><strong>Event-Driven with Redis Keyspace Notifications</strong>: Use Redis <code>PSUBSCRIBE</code> to notifications when sorted set scores pass thresholds</li>\n<li><strong>External Timer Service</strong>: Use dedicated timer service like RabbitMQ&#39;s delayed message exchange or Redis Streams with consumer groups</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement polling architecture with configurable interval (default 60 seconds)</li>\n<li><strong>Rationale</strong>: <ol>\n<li><strong>Simplicity</strong>: Polling is straightforward to implement, debug, and reason about</li>\n<li><strong>Reliability</strong>: Missed polls (scheduler restart) are automatically caught on next poll using timeline scores</li>\n<li><strong>Resource Efficiency</strong>: 60-second intervals generate minimal Redis load (one ZRANGEBYSCORE per poll)</li>\n<li><strong>Precision Acceptable</strong>: Most scheduled jobs (daily reports, hourly cleanups) don&#39;t require sub-minute precision</li>\n<li><strong>Avoids Complex Dependencies</strong>: Redis keyspace notifications require additional configuration and have delivery guarantees that complicate reliability</li>\n</ol>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Simple implementation, easy to test, handles scheduler restarts gracefully</li>\n<li><strong>Negative</strong>: Jobs may be enqueued up to <code>polling_interval_seconds</code> late (worst-case)</li>\n<li><strong>Mitigation</strong>: Use shorter intervals (e.g., 10 seconds) for time-sensitive jobs, though this increases Redis load</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Comparison of Scheduler Architecture Options:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Polling</strong></td>\n<td>Simple implementation, self-contained, handles downtime naturally, tunable precision vs load trade-off</td>\n<td>Jobs can be delayed up to polling interval, constant Redis queries even when no jobs due</td>\n<td><strong>Chosen</strong> - Best balance of simplicity and reliability for this use case</td>\n</tr>\n<tr>\n<td><strong>Redis Keyspace Notifications</strong></td>\n<td>Event-driven, near-instantaneous triggering, reduces polling load</td>\n<td>Complex setup (notify-keyspace-events), notifications can be lost if client disconnected, requires persistent connection</td>\n<td>Risk of lost notifications undermines reliability; added complexity not justified</td>\n</tr>\n<tr>\n<td><strong>Redis Streams Consumer Groups</strong></td>\n<td>Built-in Redis reliability features, consumer tracking, message persistence</td>\n<td>Complex implementation, requires managing consumer groups, overkill for scheduling</td>\n<td>Scheduling doesn&#39;t need full message queue semantics; polling is simpler</td>\n</tr>\n<tr>\n<td><strong>External Timer Service</strong></td>\n<td>Potentially more precise, dedicated service for timing</td>\n<td>Additional dependency, operational complexity, network latency</td>\n<td>Violates YAGNI; polling meets requirements without new dependencies</td>\n</tr>\n</tbody></table>\n<p><strong>Polling Interval Trade-off Analysis:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Interval</th>\n<th>Precision</th>\n<th>Redis Load (QPS)</th>\n<th>Missed Schedule Risk</th>\n<th>Recommended For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>60 seconds</td>\n<td>±60 seconds</td>\n<td>0.017 queries/sec</td>\n<td>Very low</td>\n<td>Default: daily/hourly jobs, system maintenance</td>\n</tr>\n<tr>\n<td>30 seconds</td>\n<td>±30 seconds</td>\n<td>0.033 queries/sec</td>\n<td>Low</td>\n<td>Moderate precision needs: cache warming, periodic sync</td>\n</tr>\n<tr>\n<td>10 seconds</td>\n<td>±10 seconds</td>\n<td>0.1 queries/sec</td>\n<td>Moderate</td>\n<td>Higher precision: near-real-time batch processing</td>\n</tr>\n<tr>\n<td>1 second</td>\n<td>±1 second</td>\n<td>1 query/sec</td>\n<td>High</td>\n<td>Real-time scheduling (rarely needed)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The polling interval represents a classic trade-off between timeliness and resource usage. By making it configurable per scheduler instance, we allow different deployment profiles: a primary scheduler with 60-second intervals for most jobs, and a secondary scheduler with 10-second intervals for time-sensitive jobs, both reading from the same schedule store.</p>\n</blockquote>\n<h3 id=\"adr-timezone-handling-strategy\">ADR: Timezone Handling Strategy</h3>\n<blockquote>\n<p><strong>Decision: Store All Times in UTC, Evaluate Cron in Schedule Timezone</strong></p>\n<ul>\n<li><strong>Context</strong>: Scheduled jobs often need to run at specific local times (e.g., &quot;9 AM in New York&quot; for morning reports). We must handle timezones correctly, including daylight saving time transitions, without introducing ambiguity or complexity in storage and comparison.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>UTC-Only</strong>: Store all times in UTC, require users to convert cron expressions to UTC</li>\n<li><strong>Schedule Timezone with UTC Storage</strong>: Store next run time in UTC but calculate it using schedule&#39;s timezone</li>\n<li><strong>Local Time Storage</strong>: Store times in local timezone, convert during comparisons</li>\n<li><strong>Timezone-Aware Cron</strong>: Extend cron syntax with timezone field (e.g., <code>0 9 * * * America/New_York</code>)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Store <code>next_run_at</code> in UTC, but evaluate cron expressions in the schedule&#39;s configured timezone</li>\n<li><strong>Rationale</strong>:<ol>\n<li><strong>Comparison Simplicity</strong>: All timeline comparisons in Redis use UTC timestamps, avoiding timezone conversions during polling</li>\n<li><strong>Daylight Saving Handling</strong>: Using Python&#39;s <code>pytz</code> or <code>zoneinfo</code> libraries handles DST transitions automatically when calculating next run</li>\n<li><strong>Storage Consistency</strong>: UTC storage ensures consistent sorting and comparison in Redis sorted sets</li>\n<li><strong>User Expectation</strong>: Users specify schedules in local time (e.g., &quot;9 AM Eastern&quot;), not UTC equivalents which change with DST</li>\n</ol>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Clean separation of concerns, handles DST correctly, consistent storage format</li>\n<li><strong>Negative</strong>: Requires timezone database (pytz/zoneinfo), extra conversion step during cron evaluation</li>\n<li><strong>Edge Cases</strong>: Ambiguous times during DST fall-back (2 AM occurs twice) must be handled by choosing first occurrence</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Timezone Implementation Details:</strong></p>\n<ol>\n<li><strong>Schedule Configuration</strong>: Each <code>Schedule</code> has <code>timezone</code> field (string like &quot;America/New_York&quot;)</li>\n<li><strong>Cron Evaluation</strong>:<ul>\n<li>Load timezone object using <code>zoneinfo.ZoneInfo(timezone)</code> (Python 3.9+) or <code>pytz.timezone(timezone)</code></li>\n<li>Convert current reference time from UTC to schedule timezone</li>\n<li>Calculate next run in schedule timezone</li>\n<li>Convert next run back to UTC for storage</li>\n</ul>\n</li>\n<li><strong>UTC Storage</strong>: All Redis timeline scores and <code>run_at</code> fields are UNIX timestamps (seconds since epoch) or ISO 8601 strings ending with &quot;Z&quot;</li>\n<li><strong>Daylight Saving Transitions</strong>:<ul>\n<li>Spring forward (1 AM → 3 AM): Jobs scheduled for 2:30 AM will run at 3:30 AM on transition day</li>\n<li>Fall back (2 AM → 1 AM): Jobs scheduled for 1:30 AM will run twice (once before, once after transition)</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example: Daily Report at 9 AM Eastern Through DST Transition:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Date</th>\n<th>Eastern Time (EDT/EST)</th>\n<th>UTC Equivalent</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>March 10, 2024</td>\n<td>09:00 EDT</td>\n<td>13:00 UTC</td>\n<td>Before DST (Eastern Daylight Time)</td>\n</tr>\n<tr>\n<td>March 11, 2024</td>\n<td>09:00 EDT</td>\n<td>13:00 UTC</td>\n<td>DST begins at 2 AM → 3 AM</td>\n</tr>\n<tr>\n<td>November 3, 2024</td>\n<td>09:00 EST</td>\n<td>14:00 UTC</td>\n<td>DST ends at 2 AM → 1 AM</td>\n</tr>\n<tr>\n<td>November 4, 2024</td>\n<td>09:00 EST</td>\n<td>14:00 UTC</td>\n<td>After DST (Eastern Standard Time)</td>\n</tr>\n</tbody></table>\n<p>The scheduler correctly handles this by:</p>\n<ol>\n<li>Storing <code>next_run_at</code> as UTC timestamp (13:00 or 14:00 UTC)</li>\n<li>When calculating next run on Nov 3, 2024:<ul>\n<li>Current time: 2024-11-03 13:00 UTC (just ran)</li>\n<li>Convert to Eastern: 2024-11-03 09:00 EDT</li>\n<li>Add 1 day for daily schedule: 2024-11-04 09:00 EST (note timezone change)</li>\n<li>Convert to UTC: 2024-11-04 14:00 UTC</li>\n<li>Store <code>next_run_at</code> = 14:00 UTC</li>\n</ul>\n</li>\n</ol>\n<p><strong>Comparison of Timezone Strategies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Storage Format</th>\n<th>DST Handling</th>\n<th>User Experience</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>UTC-Only</strong></td>\n<td>All times UTC</td>\n<td>User must handle</td>\n<td>Poor: users calculate UTC offsets</td>\n<td>Simple but error-prone for users</td>\n</tr>\n<tr>\n<td><strong>UTC Storage, TZ Evaluation</strong></td>\n<td>Times UTC, TZ metadata</td>\n<td>Automatic via timezone libs</td>\n<td>Good: users specify local time</td>\n<td>Moderate: conversion logic</td>\n</tr>\n<tr>\n<td><strong>Local Time Storage</strong></td>\n<td>Times with timezone</td>\n<td>Complex comparisons</td>\n<td>Good: intuitive storage</td>\n<td>High: comparison logic complex</td>\n</tr>\n<tr>\n<td><strong>Timezone-Aware Cron</strong></td>\n<td>Extended cron syntax</td>\n<td>Built into syntax</td>\n<td>Best: explicit in expression</td>\n<td>High: custom parser needed</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: Timezone handling is a classic &quot;store in UTC, display in local&quot; pattern adapted for scheduling. By performing timezone conversion only during next-run calculation (not during storage or comparison), we maintain simple UTC-based polling while respecting local time requirements.</p>\n</blockquote>\n<h3 id=\"common-pitfalls-in-scheduler-implementation\">Common Pitfalls in Scheduler Implementation</h3>\n<p>⚠️ <strong>Pitfall: Naïve Cron Evaluation Without Timezone Consideration</strong></p>\n<ul>\n<li><strong>Description</strong>: Evaluating cron expressions using server local time instead of schedule&#39;s configured timezone, causing jobs to run at wrong times when server timezone differs from schedule timezone.</li>\n<li><strong>Why It&#39;s Wrong</strong>: A server in UTC running a &quot;9 AM Eastern&quot; report would enqueue it at 9 AM UTC (5 AM Eastern), four hours early. This violates user expectations and can break time-sensitive operations.</li>\n<li><strong>Fix</strong>: Always convert reference time to schedule&#39;s timezone before cron evaluation, then convert result back to UTC for storage.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Polling Without Catch-up Logic</strong></p>\n<ul>\n<li><strong>Description</strong>: Scheduler only checks for jobs with <code>next_run_at &lt;= now()</code>, missing schedules that were due during scheduler downtime.</li>\n<li><strong>Why It&#39;s Wrong</strong>: If scheduler restarts after being down for 2 hours, hourly jobs scheduled during that period would be skipped entirely, breaking recurring job guarantees.</li>\n<li><strong>Fix</strong>: Always poll with range <code>0</code> to <code>now() + polling_interval</code>, not just <code>now()</code>. For additional safety, implement a &quot;catch-up&quot; mode on scheduler startup that processes all schedules with <code>next_run_at &lt; now()</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Infinite Loop on Invalid Cron Expressions</strong></p>\n<ul>\n<li><strong>Description</strong>: When calculating next run time for invalid cron like <code>0 0 31 2 *</code> (February 31st), naïve algorithms may get stuck trying to find a matching date.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Scheduler process hangs indefinitely or crashes when encountering malformed schedules, affecting all scheduled jobs.</li>\n<li><strong>Fix</strong>: Implement maximum iteration limits (e.g., 100,000 minutes ≈ 70 days) when searching for next matching time. Validate cron expressions when schedules are created, rejecting obviously invalid ones.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Race Conditions in Uniqueness Checking</strong></p>\n<ul>\n<li><strong>Description</strong>: Two scheduler instances polling simultaneously might both pass uniqueness check and enqueue the same job, creating duplicates.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Duplicate jobs waste resources and may cause application-level issues if jobs aren&#39;t idempotent.</li>\n<li><strong>Fix</strong>: Use Redis <code>SET key value NX EX window</code> for atomic check-and-set. The first scheduler to set the key wins; others find key exists and skip enqueueing.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Handling DST Transition Ambiguity</strong></p>\n<ul>\n<li><strong>Description</strong>: During fall-back transition (2 AM → 1 AM), 1:30 AM occurs twice. Naïve scheduling might pick wrong occurrence or skip entirely.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Jobs might run at wrong time (1:30 AM before transition instead of after) or be skipped on transition day.</li>\n<li><strong>Fix</strong>: Use <code>fold</code> parameter in Python&#39;s <code>datetime</code> or <code>pytz</code> <code>localize</code> with <code>is_dst</code> parameter to handle ambiguous times consistently (typically choose first occurrence).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Stale Schedule Definitions</strong></p>\n<ul>\n<li><strong>Description</strong>: Schedules remain in Redis after being deleted or disabled, causing continuous errors during polling.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Polling loop constantly encounters missing schedule data, filling logs with errors and potentially affecting performance.</li>\n<li><strong>Fix</strong>: Implement stale schedule cleanup: when fetching schedule fails (missing hash), remove it from timeline sorted set. Add health check metric for orphaned timeline entries.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Blocking Polling Loop</strong></p>\n<ul>\n<li><strong>Description</strong>: Processing many due schedules synchronously causes polling loop to exceed interval, delaying subsequent polls.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Scheduler falls behind real time, causing increasing latencies for all scheduled jobs.</li>\n<li><strong>Fix</strong>: Process schedules in bounded batches (e.g., 100 per poll). Use async processing or worker pool for enqueue operations. Monitor average processing time vs polling interval.</li>\n</ul>\n<h3 id=\"implementation-guidance-for-scheduler\">Implementation Guidance for Scheduler</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Cron Parsing</strong></td>\n<td><code>croniter</code> library (pip install croniter)</td>\n<td>Custom parser using regex and datetime math</td>\n</tr>\n<tr>\n<td><strong>Timezone Handling</strong></td>\n<td>Python 3.9+ <code>zoneinfo</code> (standard library)</td>\n<td><code>pytz</code> library (more comprehensive historical data)</td>\n</tr>\n<tr>\n<td><strong>Polling Loop</strong></td>\n<td><code>while True</code> with <code>time.sleep()</code></td>\n<td><code>asyncio</code> with async Redis client for concurrent processing</td>\n</tr>\n<tr>\n<td><strong>Unique Key Generation</strong></td>\n<td>Redis <code>SET NX EX</code> atomic operations</td>\n<td>Redis Lua script for complex uniqueness logic</td>\n</tr>\n<tr>\n<td><strong>Schedule Storage</strong></td>\n<td>Redis Hashes for schedules, Sorted Set for timeline</td>\n<td>RedisJSON module for nested schedule objects</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background-job-processor/\n├── scheduler/                    # Scheduler component\n│   ├── __init__.py\n│   ├── scheduler.py             # Main Scheduler class\n│   ├── cron.py                  # Cron expression parser/evaluator\n│   ├── timezone.py              # Timezone handling utilities\n│   ├── models.py                # Schedule, ScheduledJob classes\n│   └── cli.py                   # Command-line interface\n├── core/                        # Shared core components\n│   ├── job.py                   # Job class (shared)\n│   ├── queue_manager.py         # QueueManager (shared)\n│   └── redis_client.py          # RedisClient (shared)\n└── config/\n    └── settings.py              # Configuration management</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (COMPLETE, ready to use):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># scheduler/timezone.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Timezone handling utilities for schedule evaluation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> zoneinfo </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ZoneInfo</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Fallback for Python &#x3C; 3.9</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    from</span><span style=\"color:#E1E4E8\"> zoneinfo </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ZoneInfo</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#79B8FF\"> ImportError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    from</span><span style=\"color:#E1E4E8\"> backports.zoneinfo </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ZoneInfo</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> ensure_timezone</span><span style=\"color:#E1E4E8\">(dt: datetime.datetime, timezone: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> datetime.datetime:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Ensure datetime has timezone, converting naive to specified timezone.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        dt: Datetime (naive or aware)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        timezone: IANA timezone string (e.g., \"America/New_York\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Timezone-aware datetime in specified timezone</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: If timezone string is invalid</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> dt.tzinfo </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Naive datetime, assume it's in the specified timezone</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tz </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ZoneInfo(timezone)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> dt.replace(</span><span style=\"color:#FFAB70\">tzinfo</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tz)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Already timezone-aware, convert to target timezone</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tz </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ZoneInfo(timezone)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> dt.astimezone(tz)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> to_utc</span><span style=\"color:#E1E4E8\">(dt: datetime.datetime) -> datetime.datetime:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Convert timezone-aware datetime to UTC.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        dt: Timezone-aware datetime</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        UTC datetime</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: If dt is naive (no timezone)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> dt.tzinfo </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Cannot convert naive datetime to UTC\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> dt.astimezone(datetime.timezone.utc)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> from_utc</span><span style=\"color:#E1E4E8\">(utc_dt: datetime.datetime, timezone: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> datetime.datetime:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Convert UTC datetime to specified timezone.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        utc_dt: UTC datetime (must be timezone-aware with UTC)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        timezone: Target IANA timezone string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Datetime in target timezone</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> utc_dt.tzinfo </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> datetime.timezone.utc:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected UTC datetime, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">utc_dt.tzinfo</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_tz </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ZoneInfo(timezone)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> utc_dt.astimezone(target_tz)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> next_dst_transition</span><span style=\"color:#E1E4E8\">(timezone: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, after_dt: Optional[datetime.datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Optional[datetime.datetime]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Find next DST transition after given datetime.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        timezone: IANA timezone string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        after_dt: Datetime to search after (defaults to now)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Datetime of next DST transition, or None if no upcoming transitions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tz </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ZoneInfo(timezone)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    after </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> after_dt </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> datetime.datetime.now(datetime.timezone.utc)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check transitions up to 2 years in future</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> after </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> datetime.timedelta(</span><span style=\"color:#FFAB70\">days</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">730</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get transitions from zoneinfo</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Note: This is a simplified implementation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Full implementation would use pytz for historical transitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transitions </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(tz, </span><span style=\"color:#9ECBFF\">'_transitions'</span><span style=\"color:#E1E4E8\">, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> trans_time, trans_info </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> transitions:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        trans_dt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.datetime.fromtimestamp(trans_time, </span><span style=\"color:#FFAB70\">tz</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.timezone.utc)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> after </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> trans_dt </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> end:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> trans_dt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># scheduler/models.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Data models for schedule definitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, List, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ScheduleStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Status of a scheduled job instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ENQUEUED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"enqueued\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span><span style=\"color:#6A737D\">  # Due to uniqueness constraint</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"error\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Schedule</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Definition of a scheduled job (recurring or one-time).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"schedule_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">uuid.uuid4().hex</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args: List[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    kwargs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queue: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"default\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Scheduling configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cron_expression: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # e.g., \"0 9 * * *\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_at: Optional[datetime.datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # For one-time delayed jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Timezone handling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timezone: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"UTC\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # State</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enabled: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    unique_key: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # For uniqueness constraint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    unique_window_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#6A737D\">  # 5 minutes default uniqueness window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime.datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.datetime.utcnow)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_enqueued_at: Optional[datetime.datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    next_run_at: Optional[datetime.datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert schedule to dictionary for serialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'schedule_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.schedule_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'job_type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.job_type,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'args'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.args,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'kwargs'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.kwargs,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'queue'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.queue,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'cron_expression'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cron_expression,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'run_at'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.run_at.isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.run_at </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timezone'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.timezone,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'enabled'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.enabled,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'unique_key'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.unique_key,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'unique_window_seconds'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.unique_window_seconds,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'created_at'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.created_at.isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'last_enqueued_at'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.last_enqueued_at.isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_enqueued_at </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'next_run_at'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.next_run_at.isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_run_at </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'metadata'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.metadata,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Schedule'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reconstruct schedule from dictionary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parse datetime fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        run_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data.get(</span><span style=\"color:#9ECBFF\">'run_at'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            run_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.datetime.fromisoformat(data[</span><span style=\"color:#9ECBFF\">'run_at'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_enqueued_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data.get(</span><span style=\"color:#9ECBFF\">'last_enqueued_at'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            last_enqueued_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.datetime.fromisoformat(data[</span><span style=\"color:#9ECBFF\">'last_enqueued_at'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next_run_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data.get(</span><span style=\"color:#9ECBFF\">'next_run_at'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            next_run_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.datetime.fromisoformat(data[</span><span style=\"color:#9ECBFF\">'next_run_at'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        created_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.datetime.fromisoformat(data[</span><span style=\"color:#9ECBFF\">'created_at'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            schedule_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'schedule_id'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'job_type'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'args'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            kwargs</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'kwargs'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            queue</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'queue'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cron_expression</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data.get(</span><span style=\"color:#9ECBFF\">'cron_expression'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            run_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">run_at,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timezone</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data.get(</span><span style=\"color:#9ECBFF\">'timezone'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'UTC'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            enabled</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data.get(</span><span style=\"color:#9ECBFF\">'enabled'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            unique_key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data.get(</span><span style=\"color:#9ECBFF\">'unique_key'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            unique_window_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data.get(</span><span style=\"color:#9ECBFF\">'unique_window_seconds'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            created_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">created_at,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            last_enqueued_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">last_enqueued_at,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            next_run_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">next_run_at,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data.get(</span><span style=\"color:#9ECBFF\">'metadata'</span><span style=\"color:#E1E4E8\">, {}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_job</span><span style=\"color:#E1E4E8\">(self) -> Job:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a Job instance from this schedule.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        from</span><span style=\"color:#E1E4E8\"> core.job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job, JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Generate job_id with ULID for chronological sorting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Using uuid as fallback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> ulid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(ulid.ULID())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            job_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.job_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            args</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.args,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            kwargs</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.kwargs,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            queue</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.queue,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Default priority</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            max_retries</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Default retries</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timeout_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Default timeout</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            created_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.datetime.utcnow(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                **</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.metadata,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'schedule_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.schedule_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'scheduled_for'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.next_run_at.isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_run_at </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            attempts</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            started_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            completed_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            result</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ScheduledJob</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Instance of a scheduled job execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scheduled_for: datetime.datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enqueued_at: Optional[datetime.datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: ScheduleStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ScheduleStatus.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert to dictionary for storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'schedule_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.schedule_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'job_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.job_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'scheduled_for'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.scheduled_for.isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'enqueued_at'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.enqueued_at.isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.enqueued_at </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'status'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.status.value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'ScheduledJob'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reconstruct from dictionary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scheduled_for </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.datetime.fromisoformat(data[</span><span style=\"color:#9ECBFF\">'scheduled_for'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        enqueued_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data.get(</span><span style=\"color:#9ECBFF\">'enqueued_at'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            enqueued_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.datetime.fromisoformat(data[</span><span style=\"color:#9ECBFF\">'enqueued_at'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ScheduleStatus(data[</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            schedule_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'schedule_id'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            job_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            scheduled_for</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">scheduled_for,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            enqueued_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">enqueued_at,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">status,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code (signature + TODOs only):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># scheduler/scheduler.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Main scheduler implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> croniter </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> croniter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.redis_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> scheduler.models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Schedule, ScheduledJob, ScheduleStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> scheduler.timezone </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ensure_timezone, to_utc, from_utc</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scheduler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main scheduler class for delayed and recurring job scheduling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: RedisClient, queue_manager: QueueManager,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 polling_interval_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#E1E4E8\">, batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize scheduler.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            redis_client: Redis client for schedule storage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue_manager: Queue manager for enqueuing jobs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            polling_interval_seconds: How often to poll for due schedules</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            batch_size: Maximum schedules to process per poll</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queue_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_manager</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.polling_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> polling_interval_seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.batch_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch_size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Redis key patterns</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.schedule_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"schedule:</span><span style=\"color:#79B8FF\">{schedule_id}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timeline_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"schedules:timeline\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.uniqueness_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"schedule:unique:</span><span style=\"color:#79B8FF\">{unique_key}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scheduled_jobs_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"schedule:jobs:</span><span style=\"color:#79B8FF\">{schedule_id}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> schedule_delayed_job</span><span style=\"color:#E1E4E8\">(self, job: Job, run_at: datetime.datetime) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Schedule a job for one-time delayed execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: Job to schedule</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            run_at: When to enqueue the job (must be in future)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Schedule ID for future reference</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValueError: If run_at is in the past</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate run_at is in the future (compare to UTC now)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Schedule object with run_at (no cron_expression)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate unique schedule_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate next_run_at (same as run_at)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store schedule in Redis hash (schedule_key)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add to timeline sorted set with score = next_run_at timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return schedule_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use to_utc() to convert run_at to UTC for storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use redis.pipeline() for atomic schedule creation + timeline addition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> schedule_recurring_job</span><span style=\"color:#E1E4E8\">(self, job: Job, cron_expression: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               timezone: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"UTC\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Schedule a job for recurring execution based on cron expression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: Job to schedule</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            cron_expression: Cron expression (e.g., \"0 9 * * *\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            timezone: IANA timezone for schedule evaluation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Schedule ID for future reference</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValueError: If cron expression is invalid</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate cron expression using croniter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Schedule object with cron_expression and timezone</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate unique schedule_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate initial next_run_at using calculate_next_run()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store schedule in Redis hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add to timeline sorted set with score = next_run_at timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return schedule_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use self._calculate_next_run() for initial calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Consider using current time in schedule's timezone as reference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_next_run</span><span style=\"color:#E1E4E8\">(self, schedule: Schedule,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           reference_time: Optional[datetime.datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> datetime.datetime:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate next run time for a schedule.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            schedule: Schedule definition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            reference_time: Time to calculate from (defaults to now)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Next run time in UTC</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValueError: If schedule has neither cron nor run_at</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If schedule has run_at (one-time), return run_at (already in UTC)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If schedule has cron_expression:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Determine reference time (use last_enqueued_at or now if None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Convert reference time to schedule's timezone</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Use croniter to get next matching time in schedule's timezone</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   d. Handle edge cases: maximum iterations, invalid dates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   e. Convert result back to UTC</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return UTC datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: For cron, add 1 minute to reference time to avoid re-enqueuing immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use croniter.croniter.get_next(datetime) for next run calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Wrap in try/except for croniter errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _enqueue_scheduled_job</span><span style=\"color:#E1E4E8\">(self, schedule: Schedule,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              scheduled_for: datetime.datetime) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Enqueue a job from a schedule.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            schedule: Schedule definition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            scheduled_for: When this job was scheduled to run</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (success, job_id or error message)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check uniqueness constraint if schedule.unique_key is set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If unique_key exists and within window, return (False, \"skipped\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create Job instance from schedule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Enqueue job using queue_manager.enqueue_job()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If successful:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Record ScheduledJob instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Update schedule.last_enqueued_at</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Set uniqueness key with TTL if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return (success, job_id or error)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: For uniqueness check: redis.set(key, 1, nx=True, ex=window)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use pipeline for atomic updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _poll_due_schedules</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Poll for and process due schedules.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of jobs successfully enqueued</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current UTC time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate cutoff = now + polling_interval (for catch-up)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use ZRANGEBYSCORE to get schedule_ids with score &#x3C;= cutoff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each schedule_id (up to batch_size):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Fetch schedule from Redis hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Skip if schedule is None or not enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Process schedule using _process_schedule()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return count of successfully enqueued jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use redis.pipeline() for batch operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Handle missing schedules (remove from timeline)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_schedule</span><span style=\"color:#E1E4E8\">(self, schedule: Schedule) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Process a single due schedule.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            schedule: Schedule to process</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if job was enqueued, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine scheduled_for time (from timeline score or schedule.next_run_at)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Attempt to enqueue job using _enqueue_scheduled_job()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If successful or skipped due to uniqueness:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Calculate next run time for recurring jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Update schedule.next_run_at</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Update timeline sorted set with new score</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If one-time job (no cron), remove from storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if job was enqueued</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: For next run calculation, use schedule's last_enqueued_at or now as reference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use _calculate_next_run() for recurring jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the scheduler polling loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set _running = True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Log scheduler startup with configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: While _running:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Record loop start time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Call _poll_due_schedules()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Calculate sleep time = polling_interval - processing_time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   d. Sleep max(0, sleep_time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log scheduler shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Use time.monotonic() for precise timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Catch KeyboardInterrupt for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop the scheduler polling loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set _running = False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Log scheduler stopping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_next_runs_for_all_schedules</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Recalculate next_run_at for all schedules (e.g., after timezone change).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of schedules updated</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get all schedule_ids from timeline or scan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each schedule:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Fetch schedule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Calculate new next_run_at</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Update timeline score</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return count updated</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # HINT: Useful for maintenance after DST rules change</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>Cron Parsing</strong>: Use <code>croniter</code> library: <code>pip install croniter</code>. It handles complex cron expressions including ranges, steps, and month/day names.</li>\n<li><strong>Timezone Handling</strong>: Python 3.9+ has <code>zoneinfo</code> in standard library. For older Python, use <code>backports.zoneinfo</code> or <code>pytz</code>. Avoid <code>datetime.replace(tzinfo=)</code> for pytz timezones - use <code>localize()</code> instead.</li>\n<li><strong>UTC Timestamps</strong>: Store all timestamps as UNIX timestamps (float) or ISO 8601 strings ending with &quot;Z&quot; in Redis. Use <code>datetime.datetime.now(datetime.timezone.utc)</code> for current UTC time.</li>\n<li><strong>Redis Sorted Sets</strong>: Use <code>ZADD</code>, <code>ZRANGEBYSCORE</code>, and <code>ZREM</code> for timeline management. Scores should be UNIX timestamps (seconds since epoch).</li>\n<li><strong>Atomic Operations</strong>: Use Redis pipelines or Lua scripts for operations requiring multiple commands (e.g., check uniqueness, enqueue job, update schedule).</li>\n<li><strong>ULID Generation</strong>: Use <code>ulid-py</code> library for chronological job IDs: <code>pip install ulid-py</code>. ULIDs are sortable by creation time unlike UUIDs.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the scheduler:</p>\n<ol>\n<li><strong>Test Basic Scheduling</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Start Redis</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   redis-server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # In Python shell</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   from scheduler.scheduler import Scheduler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   from core.job import Job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   from core.queue_manager import QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   from core.redis_client import RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   import datetime</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   redis = RedisClient.get_instance('redis://localhost:6379')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   queue_manager = QueueManager(SystemConfig.from_env())</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   scheduler = Scheduler(redis, queue_manager, polling_interval_seconds=10)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   # Schedule a job for 30 seconds from now</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   job = Job(job_type='test_task', args=['hello'])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   run_at = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(seconds=30)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   schedule_id = scheduler.schedule_delayed_job(job, run_at)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   print(f'Scheduled job with ID: {schedule_id}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   # Start scheduler in background thread</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   import threading</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   thread = threading.Thread(target=scheduler.start)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   # Wait 45 seconds, then check if job was enqueued</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   import time</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   time.sleep(45)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   queue_len = queue_manager.get_queue_length('default')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   print(f'Jobs in queue: {queue_len}')  # Should be 1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   scheduler.stop()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   thread.join()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   \"</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Test Recurring Jobs</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Schedule a job to run every minute</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   from scheduler.scheduler import Scheduler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   from core.job import Job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   # ... setup as above</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   job = Job(job_type='heartbeat', args=[])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   schedule_id = scheduler.schedule_recurring_job(job, '* * * * *', 'UTC')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   print(f'Recurring schedule ID: {schedule_id}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   # Start scheduler, wait 2.5 minutes</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   # Should see 2-3 jobs enqueued (depending on exact timing)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   \"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">G. Debugging Tips:</span><span style=\"color:#F97583\">**</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\"> Symptom</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> Likely</span><span style=\"color:#9ECBFF\"> Cause</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> How</span><span style=\"color:#9ECBFF\"> to</span><span style=\"color:#9ECBFF\"> Diagnose</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> Fix</span><span style=\"color:#F97583\"> |</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\">---------</span><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\">--------------</span><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\">-----------------</span><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\">-----</span><span style=\"color:#F97583\">|</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\"> **Scheduled</span><span style=\"color:#9ECBFF\"> jobs</span><span style=\"color:#9ECBFF\"> never</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#79B8FF\">**</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> Timeline</span><span style=\"color:#9ECBFF\"> sorted</span><span style=\"color:#9ECBFF\"> set</span><span style=\"color:#9ECBFF\"> empty</span><span style=\"color:#9ECBFF\"> or</span><span style=\"color:#9ECBFF\"> wrong</span><span style=\"color:#9ECBFF\"> scores</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> Check</span><span style=\"color:#9ECBFF\"> `</span><span style=\"color:#B392F0\">ZRANGE</span><span style=\"color:#9ECBFF\"> schedules:timeline </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#79B8FF\"> -1</span><span style=\"color:#9ECBFF\"> WITHSCORES`</span><span style=\"color:#E1E4E8\"> in Redis </span><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\"> Ensure</span><span style=\"color:#9ECBFF\"> `</span><span style=\"color:#B392F0\">next_run_at</span><span style=\"color:#9ECBFF\">`</span><span style=\"color:#B392F0\"> is</span><span style=\"color:#9ECBFF\"> in</span><span style=\"color:#9ECBFF\"> UTC</span><span style=\"color:#9ECBFF\"> when</span><span style=\"color:#9ECBFF\"> added</span><span style=\"color:#9ECBFF\"> to</span><span style=\"color:#9ECBFF\"> sorted</span><span style=\"color:#9ECBFF\"> set</span><span style=\"color:#F97583\"> |</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\"> **Jobs</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> at</span><span style=\"color:#9ECBFF\"> wrong</span><span style=\"color:#9ECBFF\"> time</span><span style=\"color:#79B8FF\">**</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> Timezone</span><span style=\"color:#9ECBFF\"> conversion</span><span style=\"color:#9ECBFF\"> error</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> Compare</span><span style=\"color:#9ECBFF\"> schedule's `timezone` field with actual calculation | Use `ensure_timezone()` and `to_utc()` consistently |</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">| **Duplicate jobs enqueued** | Uniqueness constraint not working | Check Redis for `schedule:unique:*` keys with `TTL` | Use `SET NX EX` atomically, not separate `GET` + `SET` |</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">| **Scheduler uses 100% CPU** | Polling loop with no sleep | Add logging to measure loop iteration time | Ensure `time.sleep()` accounts for processing time |</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">| **Missed jobs after restart** | Timeline scores in past not processed | Check if `ZRANGEBYSCORE` uses `now() + interval` not just `now()` | Always poll with range `0` to `now + interval` |</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">| **Cron job runs twice in same minute** | Uniqueness window too short | Check `unique_window_seconds` vs cron frequency | Increase window to cover entire polling interval |</span></span></code></pre></div>\n\n<hr>\n<h2 id=\"component-design-monitoring-amp-dashboard\">Component Design: Monitoring &amp; Dashboard</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 5: Monitoring &amp; Dashboard. This section provides the complete design for real-time monitoring, web dashboard, and alerting systems that provide observability into job processing, worker health, and system performance. This corresponds to the final milestone, building upon all previous components to create comprehensive operational visibility.</p>\n</blockquote>\n<h3 id=\"mental-model-air-traffic-control-dashboard\">Mental Model: Air Traffic Control Dashboard</h3>\n<p>Think of the monitoring system as an <strong>air traffic control dashboard</strong> for your distributed job processing system. Air traffic controllers monitor:</p>\n<ol>\n<li><strong>Active Flights (Jobs)</strong>: Where they are (which worker), their status (taking off, cruising, landing), and estimated completion time</li>\n<li><strong>Runway Capacity (Queues)</strong>: How many flights are waiting to take off, and which runways are most congested</li>\n<li><strong>Weather Conditions (System Health)</strong>: Any storms (errors) approaching, visibility issues (latency), or equipment problems</li>\n<li><strong>Historical Patterns</strong>: Which routes have the most turbulence (error-prone jobs), peak traffic hours, and operational bottlenecks</li>\n</ol>\n<p>Just as air traffic controllers need real-time visibility without overwhelming detail, our monitoring system must:</p>\n<ul>\n<li><strong>Aggregate</strong> high-level metrics while allowing drill-down to specific incidents</li>\n<li><strong>Alert</strong> on anomalies without generating noise</li>\n<li><strong>Preserve context</strong> so when something goes wrong, you can trace the entire journey of a job</li>\n<li><strong>Scale</strong> to handle thousands of jobs without impacting the performance of the actual job processing</li>\n</ul>\n<p>This mental model emphasizes that monitoring is not just passive observation but active system management—knowing when to re-route traffic (reprioritize queues), allocate more resources (scale workers), or investigate incidents (debug failing jobs).</p>\n<h3 id=\"monitoring-api-and-dashboard-interface\">Monitoring API and Dashboard Interface</h3>\n<p>The monitoring system consists of two primary interfaces: a <strong>programmatic API</strong> for automation and integration, and a <strong>web dashboard</strong> for human operators. Both share the same underlying data model but present it differently.</p>\n<h4 id=\"core-monitoring-data-types\">Core Monitoring Data Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type Name</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MetricPoint</code></td>\n<td><code>timestamp datetime</code>, <code>name str</code>, <code>value float</code>, <code>labels Dict[str, str]</code></td>\n<td>Single timestamped measurement with key-value labels for dimensionality</td>\n</tr>\n<tr>\n<td><code>JobSummary</code></td>\n<td><code>job_id str</code>, <code>job_type str</code>, <code>queue str</code>, <code>status JobStatus</code>, <code>enqueued_at datetime</code>, <code>started_at Optional[datetime]</code>, <code>completed_at Optional[datetime]</code>, <code>duration_seconds Optional[float]</code>, <code>attempts int</code>, <code>error_count int</code></td>\n<td>Aggregated job execution data for dashboard display</td>\n</tr>\n<tr>\n<td><code>WorkerStatus</code></td>\n<td><code>worker_id str</code>, <code>hostname str</code>, <code>pid int</code>, <code>state str</code>, <code>current_job Optional[JobSummary]</code>, <code>queues List[str]</code>, <code>last_heartbeat datetime</code>, <code>processed_count int</code>, <code>failed_count int</code>, <code>uptime_seconds float</code></td>\n<td>Current status of a worker process</td>\n</tr>\n<tr>\n<td><code>QueueMetrics</code></td>\n<td><code>queue_name str</code>, <code>pending_count int</code>, <code>active_count int</code>, <code>completed_count int</code>, <code>failed_count int</code>, <code>retry_scheduled_count int</code>, <code>dead_letter_count int</code>, <code>enqueue_rate float</code>, <code>process_rate float</code>, <code>error_rate float</code>, <code>avg_process_time float</code>, <code>oldest_job_age Optional[float]</code></td>\n<td>Comprehensive metrics for a single queue</td>\n</tr>\n<tr>\n<td><code>SystemAlert</code></td>\n<td><code>alert_id str</code>, <code>severity str</code> (CRITICAL, WARNING, INFO), <code>source str</code>, <code>message str</code>, <code>condition str</code>, <code>triggered_at datetime</code>, <code>acknowledged_at Optional[datetime]</code>, <code>resolved_at Optional[datetime]</code>, <code>metadata Dict[str, Any]</code></td>\n<td>Alert generated by monitoring rules</td>\n</tr>\n<tr>\n<td><code>DashboardConfig</code></td>\n<td><code>refresh_interval int</code>, <code>time_range str</code> (1h, 24h, 7d), <code>queue_filters List[str]</code>, <code>worker_filters List[str]</code>, <code>job_type_filters List[str]</code>, <code>metric_aggregation str</code> (avg, sum, p95, p99)</td>\n<td>User dashboard configuration</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fmonitoring-architecture.svg\" alt=\"Monitoring Data Collection Architecture\"></p>\n<h4 id=\"monitoring-api-endpoints\">Monitoring API Endpoints</h4>\n<p>The programmatic API provides RESTful endpoints for external monitoring systems (Prometheus, Datadog) and automation scripts:</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>HTTP Endpoint</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>get_system_metrics()</code></td>\n<td><code>GET /api/metrics/system</code></td>\n<td><code>time_range str = &quot;1h&quot;</code>, <code>aggregation str = &quot;avg&quot;</code></td>\n<td><code>Dict[str, List[MetricPoint]]</code></td>\n<td>System-wide metrics: memory usage, Redis connections, active workers</td>\n</tr>\n<tr>\n<td><code>get_queue_metrics()</code></td>\n<td><code>GET /api/metrics/queues</code></td>\n<td><code>queue_names Optional[List[str]]</code>, <code>time_range str = &quot;1h&quot;</code></td>\n<td><code>List[QueueMetrics]</code></td>\n<td>Current metrics for specified queues (all if none specified)</td>\n</tr>\n<tr>\n<td><code>get_worker_status()</code></td>\n<td><code>GET /api/workers</code></td>\n<td><code>active_only bool = True</code></td>\n<td><code>List[WorkerStatus]</code></td>\n<td>Status of all workers (including inactive if <code>active_only=False</code>)</td>\n</tr>\n<tr>\n<td><code>get_job_history()</code></td>\n<td><code>GET /api/jobs</code></td>\n<td><code>job_id Optional[str]</code>, <code>queue Optional[str]</code>, <code>status Optional[JobStatus]</code>, <code>job_type Optional[str]</code>, <code>limit int = 100</code>, <code>offset int = 0</code>, <code>time_range Optional[str]</code></td>\n<td><code>Dict[str, Any]</code> (with <code>jobs List[JobSummary]</code>, <code>total int</code>)</td>\n<td>Search jobs with filtering and pagination</td>\n</tr>\n<tr>\n<td><code>get_dead_letter_jobs()</code></td>\n<td><code>GET /api/dead-letter</code></td>\n<td><code>queue Optional[str]</code>, <code>job_type Optional[str]</code>, <code>limit int = 50</code></td>\n<td><code>List[Job]</code></td>\n<td>Jobs in dead letter queue for manual intervention</td>\n</tr>\n<tr>\n<td><code>retry_dead_letter_job()</code></td>\n<td><code>POST /api/dead-letter/{job_id}/retry</code></td>\n<td><code>job_id str</code> (path), <code>queue Optional[str]</code></td>\n<td><code>Dict[str, Any]</code> (with <code>success bool</code>, <code>new_job_id Optional[str]</code>)</td>\n<td>Retry a specific dead letter job in its original queue</td>\n</tr>\n<tr>\n<td><code>delete_dead_letter_job()</code></td>\n<td><code>DELETE /api/dead-letter/{job_id}</code></td>\n<td><code>job_id str</code> (path)</td>\n<td><code>Dict[str, Any]</code> (with <code>success bool</code>)</td>\n<td>Permanently remove a job from dead letter queue</td>\n</tr>\n<tr>\n<td><code>get_alerts()</code></td>\n<td><code>GET /api/alerts</code></td>\n<td><code>severity Optional[str]</code>, <code>acknowledged bool = False</code>, <code>limit int = 50</code></td>\n<td><code>List[SystemAlert]</code></td>\n<td>Get current active alerts</td>\n</tr>\n<tr>\n<td><code>acknowledge_alert()</code></td>\n<td><code>POST /api/alerts/{alert_id}/acknowledge</code></td>\n<td><code>alert_id str</code> (path)</td>\n<td><code>Dict[str, Any]</code> (with <code>success bool</code>)</td>\n<td>Mark an alert as acknowledged by an operator</td>\n</tr>\n<tr>\n<td><code>create_alert_rule()</code></td>\n<td><code>POST /api/alert-rules</code></td>\n<td><code>rule_config Dict[str, Any]</code></td>\n<td><code>Dict[str, Any]</code> (with <code>rule_id str</code>)</td>\n<td>Create a new alerting rule (admin only)</td>\n</tr>\n<tr>\n<td><code>get_metrics_stream()</code></td>\n<td><code>GET /api/metrics/stream</code></td>\n<td>-</td>\n<td><code>StreamingResponse</code> (Server-Sent Events)</td>\n<td>Real-time stream of metric updates for dashboard</td>\n</tr>\n</tbody></table>\n<h4 id=\"web-dashboard-interface\">Web Dashboard Interface</h4>\n<p>The web dashboard provides human-readable visualization through these key views:</p>\n<table>\n<thead>\n<tr>\n<th>View Name</th>\n<th>Purpose</th>\n<th>Key Visualizations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>System Overview</strong></td>\n<td>High-level system health</td>\n<td>- Queue depth gauges<br>- Worker status grid<br>- Error rate sparklines<br>- System metrics time-series</td>\n</tr>\n<tr>\n<td><strong>Queue Detail</strong></td>\n<td>Drill-down into specific queue</td>\n<td>- Job status distribution pie chart<br>- Processing rate over time<br>- Top error-producing job types<br>- Oldest pending jobs list</td>\n</tr>\n<tr>\n<td><strong>Worker Dashboard</strong></td>\n<td>Worker process monitoring</td>\n<td>- Worker state heatmap<br>- CPU/memory usage per worker<br>- Current job execution time<br>- Worker failure history</td>\n</tr>\n<tr>\n<td><strong>Job Explorer</strong></td>\n<td>Search and inspect jobs</td>\n<td>- Filterable job table with pagination<br>- Job execution timeline<br>- Error stack trace viewer<br>- Manual retry controls</td>\n</tr>\n<tr>\n<td><strong>Dead Letter Queue</strong></td>\n<td>Manage permanently failed jobs</td>\n<td>- DLQ job table with error details<br>- Bulk retry/delete operations<br>- Failure pattern analysis</td>\n</tr>\n<tr>\n<td><strong>Alert Center</strong></td>\n<td>Alert management</td>\n<td>- Active alert dashboard<br>- Alert history with filters<br>- Alert rule configuration</td>\n</tr>\n<tr>\n<td><strong>System Configuration</strong></td>\n<td>Runtime configuration view</td>\n<td>- Queue configuration table<br>- Worker pool settings<br>- Redis connection status</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The dashboard prioritizes <strong>operational clarity</strong> over completeness. Each view answers specific questions operators have: &quot;Why is my queue backing up?&quot;, &quot;Which worker is stuck?&quot;, &quot;What&#39;s failing and why?&quot; The API supports automation while the dashboard supports human intuition.</p>\n</blockquote>\n<h3 id=\"metrics-collection-and-aggregation\">Metrics Collection and Aggregation</h3>\n<p>Metrics flow through a three-stage pipeline: <strong>collection → aggregation → storage</strong>. This design ensures monitoring doesn&#39;t block job processing while providing accurate, real-time visibility.</p>\n<h4 id=\"collection-strategy\">Collection Strategy</h4>\n<p>Metrics are collected at these key points in the system:</p>\n<table>\n<thead>\n<tr>\n<th>Collection Point</th>\n<th>What&#39;s Collected</th>\n<th>How It&#39;s Collected</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Job Enqueue</strong></td>\n<td>Queue length increment, job type counter, payload size</td>\n<td><code>QueueManager.enqueue_job()</code> emits Redis Stream event</td>\n</tr>\n<tr>\n<td><strong>Job Start</strong></td>\n<td>Job transition to ACTIVE, worker assignment, queue length decrement</td>\n<td><code>Worker</code> emits event when beginning job execution</td>\n</tr>\n<tr>\n<td><strong>Job Completion</strong></td>\n<td>Success/failure status, execution duration, attempt count</td>\n<td><code>Worker</code> wraps job execution with timing and status tracking</td>\n</tr>\n<tr>\n<td><strong>Job Failure</strong></td>\n<td>Error details, retry scheduling, backoff delay</td>\n<td><code>RetryManager.handle_job_failure()</code> records error context</td>\n</tr>\n<tr>\n<td><strong>Worker Heartbeat</strong></td>\n<td>Worker liveness, current job, processed counts</td>\n<td><code>WorkerHeartbeat</code> periodically updates Redis hash</td>\n</tr>\n<tr>\n<td><strong>Scheduler Activity</strong></td>\n<td>Scheduled job enqueue, missed schedules, cron evaluation</td>\n<td><code>Scheduler._enqueue_scheduled_job()</code> emits events</td>\n</tr>\n<tr>\n<td><strong>System Resources</strong></td>\n<td>Redis memory usage, connection count, CPU via OS</td>\n<td>External agent collects via Redis INFO and psutil</td>\n</tr>\n</tbody></table>\n<h4 id=\"aggregation-algorithm\">Aggregation Algorithm</h4>\n<p>Raw events are aggregated into 10-second buckets to reduce storage and query load:</p>\n<ol>\n<li><strong>Event Emission</strong>: Components emit events to Redis Stream <code>monitoring:events</code> with JSON payload:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">   {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     \"timestamp\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"2024-01-15T10:30:45.123Z\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"job_completed\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     \"queue\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"emails\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     \"job_type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"send_welcome\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     \"worker_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"worker-abc123\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     \"duration_ms\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">125.5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     \"success\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Background Aggregator</strong>: A lightweight aggregator process runs every 10 seconds:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   1. Reads all events from `monitoring:events` stream since last run\n   2. Groups events by (metric_name, queue, job_type, worker_id) tuple\n   3. Calculates aggregates for each group:\n      - Count → sum of occurrences\n      - Duration → average, p95, p99, min, max\n      - Error rate → failed_count / total_count\n   4. Stores aggregated metrics to Redis Sorted Set with timestamp score\n   5. Truncates old raw events beyond retention window</code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Metric Retention</strong>:<ul>\n<li>Raw events: 1 hour retention (for debugging)</li>\n<li>10-second aggregates: 7 days retention</li>\n<li>1-minute aggregates (rolled up from 10-second): 30 days retention</li>\n<li>1-hour aggregates: 90 days retention</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"key-performance-indicators-kpis\">Key Performance Indicators (KPIs)</h4>\n<p>The system tracks these essential KPIs for operational health:</p>\n<table>\n<thead>\n<tr>\n<th>KPI Name</th>\n<th>Calculation</th>\n<th>Alert Threshold</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Queue Depth</strong></td>\n<td><code>LLEN queue:{name}</code></td>\n<td>&gt; 1000 pending jobs</td>\n<td>Detect processing backlog</td>\n</tr>\n<tr>\n<td><strong>Processing Rate</strong></td>\n<td>Jobs completed per second (10s window)</td>\n<td>&lt; 10% of expected rate</td>\n<td>Detect worker slowdown</td>\n</tr>\n<tr>\n<td><strong>Error Rate</strong></td>\n<td>Failed jobs / total jobs (5m window)</td>\n<td>&gt; 5%</td>\n<td>Detect systemic failures</td>\n</tr>\n<tr>\n<td><strong>Worker Health</strong></td>\n<td>Workers with heartbeat &lt; 60s ago / total workers</td>\n<td>&lt; 80% healthy</td>\n<td>Detect worker process failures</td>\n</tr>\n<tr>\n<td><strong>Job Duration P99</strong></td>\n<td>99th percentile job duration (by job_type)</td>\n<td>&gt; 2x historical baseline</td>\n<td>Detect performance degradation</td>\n</tr>\n<tr>\n<td><strong>Redis Memory Usage</strong></td>\n<td><code>used_memory</code> / <code>maxmemory</code></td>\n<td>&gt; 85%</td>\n<td>Prevent Redis OOM</td>\n</tr>\n<tr>\n<td><strong>Schedule Miss Rate</strong></td>\n<td>Missed schedules / total schedules (1h window)</td>\n<td>&gt; 1%</td>\n<td>Detect scheduler issues</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> Monitoring should be <strong>cheap enough to always be on</strong>. By aggregating metrics and using Redis&#39; efficient data structures, we avoid the common pitfall where monitoring itself becomes a performance bottleneck. The 10-second aggregation strikes a balance between real-time responsiveness and system load.</p>\n</blockquote>\n<h3 id=\"adr-real-time-update-strategy\">ADR: Real-time Update Strategy</h3>\n<blockquote>\n<p><strong>Decision: Server-Sent Events (SSE) over WebSockets for Real-time Dashboard Updates</strong></p>\n<ul>\n<li><p><strong>Context</strong>: The dashboard needs near real-time updates (queue depths, worker status, job completion) without requiring page refreshes. We need a lightweight, scalable solution that integrates well with our existing HTTP API and doesn&#39;t add significant complexity.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Polling (HTTP GET every few seconds)</strong>: Simple but inefficient, creates constant load regardless of change frequency</li>\n<li><strong>WebSockets</strong>: Full-duplex, low-latency, but requires connection management and protocol handling</li>\n<li><strong>Server-Sent Events (SSE)</strong>: HTTP-based, unidirectional from server to client, automatic reconnection, simple API</li>\n<li><strong>Redis Pub/Sub with bridge</strong>: Push updates via Redis publish, bridge to HTTP layer</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Use <strong>Server-Sent Events (SSE)</strong> for real-time dashboard updates.</p>\n</li>\n<li><p><strong>Rationale</strong>:</p>\n<ul>\n<li><strong>Simplicity</strong>: SSE works over standard HTTP, requires no special protocol handling on server or client</li>\n<li><strong>Automatic reconnection</strong>: Built-in reconnection with last-event-id tracking</li>\n<li><strong>Efficient</strong>: Server pushes updates only when metrics change, no constant polling</li>\n<li><strong>Compatibility</strong>: Works with existing authentication/authorization middleware</li>\n<li><strong>Resource friendly</strong>: Single HTTP connection per dashboard client vs. bidirectional WebSocket overhead</li>\n<li><strong>Graceful degradation</strong>: If SSE connection drops, dashboard falls back to periodic polling</li>\n</ul>\n</li>\n<li><p><strong>Consequences</strong>:</p>\n<ul>\n<li>✅ Dashboard gets sub-second updates without constant polling</li>\n<li>✅ Simple implementation using standard HTTP libraries</li>\n<li>✅ Automatic reconnection handles network issues gracefully</li>\n<li>✅ Works behind load balancers and proxies with standard HTTP</li>\n<li>❌ Unidirectional only (server → client), but dashboard doesn&#39;t need client→server updates</li>\n<li>❌ Browser limits of 6 concurrent HTTP connections per domain (mitigated by using single SSE stream for all metrics)</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Polling</strong></td>\n<td>- Extremely simple<br>- Works everywhere<br>- No connection state</td>\n<td>- High latency (poll interval)<br>- Constant load even when idle<br>- Wasted bandwidth</td>\n<td>Creates unnecessary load; poor real-time experience</td>\n</tr>\n<tr>\n<td><strong>WebSockets</strong></td>\n<td>- Full duplex<br>- Lowest latency<br>- Rich protocol options</td>\n<td>- Complex connection management<br>- Requires protocol upgrade<br>- Overkill for unidirectional updates</td>\n<td>Complexity outweighs benefits for our use case</td>\n</tr>\n<tr>\n<td><strong>Server-Sent Events</strong></td>\n<td>- HTTP-based, simple<br>- Automatic reconnection<br>- Efficient push model</td>\n<td>- Unidirectional only<br>- Less control than WebSockets</td>\n<td><strong>CHOSEN</strong>: Perfect fit for dashboard metrics streaming</td>\n</tr>\n<tr>\n<td><strong>Redis Pub/Sub Bridge</strong></td>\n<td>- Leverages existing Redis<br>- Decouples producers from consumers</td>\n<td>- Requires bridge process<br>- Additional moving parts</td>\n<td>Adds complexity without significant advantage</td>\n</tr>\n</tbody></table>\n<h3 id=\"adr-job-history-storage-strategy\">ADR: Job History Storage Strategy</h3>\n<blockquote>\n<p><strong>Decision: Two-tiered storage with Redis for recent history and optional archival to external storage</strong></p>\n<ul>\n<li><p><strong>Context</strong>: Job execution history is valuable for debugging and analytics but can grow unbounded. We need to store enough history for operational debugging (hours/days) while providing an optional path for longer-term retention without overwhelming Redis memory.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Redis-only with TTL</strong>: Store all job history in Redis with expiration (e.g., 7 days TTL)</li>\n<li><strong>External database primary</strong>: Write all job history directly to PostgreSQL/MySQL</li>\n<li><strong>Two-tiered with Redis cache</strong>: Recent history in Redis (fast access), older history in external DB</li>\n<li><strong>Hybrid with configurable backend</strong>: Plugable storage backends (Redis, SQL, filesystem)</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Implement <strong>two-tiered storage</strong> with Redis for recent history (configurable retention) and optional archival to external database for long-term retention.</p>\n</li>\n<li><p><strong>Rationale</strong>:</p>\n<ul>\n<li><strong>Performance</strong>: Redis provides sub-millisecond access to recent job history for dashboard</li>\n<li><strong>Operational simplicity</strong>: Most debugging needs recent history (last few hours/days)</li>\n<li><strong>Memory control</strong>: TTL prevents unbounded Redis growth</li>\n<li><strong>Flexibility</strong>: Optional external archival for compliance or analytics without impacting core system</li>\n<li><strong>Progressive enhancement</strong>: Start with Redis-only, add archival later if needed (YAGNI principle)</li>\n</ul>\n</li>\n<li><p><strong>Consequences</strong>:</p>\n<ul>\n<li>✅ Fast access to recent job history for dashboard and debugging</li>\n<li>✅ Configurable Redis memory usage via TTL settings</li>\n<li>✅ Optional external archival without core system dependency</li>\n<li>✅ Simple initial implementation (Redis only)</li>\n<li>❌ Historical queries beyond TTL require external archival setup</li>\n<li>❌ Two data stores to manage if archival enabled</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis-only with TTL</strong></td>\n<td>- Maximum performance<br>- Simple architecture<br>- No external dependencies</td>\n<td>- History limited by TTL<br>- Redis memory pressure<br>- No long-term analytics</td>\n<td><strong>CHOSEN</strong> as base: Simple and fast for operational needs</td>\n</tr>\n<tr>\n<td><strong>External DB primary</strong></td>\n<td>- Unlimited history<br>- Rich query capabilities<br>- Persistent storage</td>\n<td>- Slower access<br>- Additional dependency<br>- Operational complexity</td>\n<td>Overkill for core operational dashboard needs</td>\n</tr>\n<tr>\n<td><strong>Two-tiered with cache</strong></td>\n<td>- Best of both worlds<br>- Fast recent access + long-term storage</td>\n<td>- Complexity of synchronization<br>- Cache invalidation challenges</td>\n<td><strong>CHOSEN</strong> as extensible: Can add archival later</td>\n</tr>\n<tr>\n<td><strong>Plugable backends</strong></td>\n<td>- Maximum flexibility<br>- Choose storage per deployment</td>\n<td>- Highest complexity<br>- Interface abstraction overhead</td>\n<td>Premature optimization; start simple, extend if needed</td>\n</tr>\n</tbody></table>\n<h4 id=\"job-history-storage-implementation\">Job History Storage Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">plaintext</span><pre class=\"arch-pre shiki-highlighted\"><code>Redis Data Structures for Job History:\n\n1. `job:history:{job_id}` → Hash with complete job execution record\n   - Fields: job_data (JSON), status, started_at, completed_at, duration_ms, attempts, errors (JSON)\n   - TTL: 7 days (configurable)\n\n2. `job:history:index:by_queue:{queue_name}` → Sorted Set\n   - Score: completed_at timestamp (or enqueued_at if not completed)\n   - Member: job_id\n   - Used for: &quot;Show recent jobs in queue X&quot;\n   - TTL: 7 days (via periodic cleanup of expired jobs)\n\n3. `job:history:index:by_type:{job_type}` → Sorted Set\n   - Score: completed_at timestamp\n   - Member: job_id\n   - Used for: &quot;Show recent jobs of type Y&quot;\n   - TTL: 7 days\n\n4. `job:history:index:by_status:{status}` → Sorted Set\n   - Score: timestamp of status change\n   - Member: job_id\n   - Used for: &quot;Show recent failed jobs&quot;\n   - TTL: 7 days for COMPLETED, 30 days for FAILED/DEAD_LETTER\n\nArchival Process (optional):\n1. Background process scans jobs nearing TTL expiration\n2. Serializes job history to JSON\n3. Pushes to configurable sink (PostgreSQL, S3, Elasticsearch)\n4. Removes from Redis after successful archival</code></pre></div>\n\n<h3 id=\"common-pitfalls-in-monitoring-implementation\">Common Pitfalls in Monitoring Implementation</h3>\n<p>⚠️ <strong>Pitfall 1: Monitoring blocks job processing</strong></p>\n<ul>\n<li><strong>What happens</strong>: Adding metric collection directly in the hot path of job execution (e.g., synchronous writes to Redis during job start/complete)</li>\n<li><strong>Why it&#39;s wrong</strong>: Adds latency to job processing; under high load, monitoring can become the bottleneck</li>\n<li><strong>How to fix</strong>: Use asynchronous event emission (Redis Streams) or batch writes. Workers should emit events to a local buffer that&#39;s flushed periodically.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 2: Storing unlimited job history in Redis</strong></p>\n<ul>\n<li><strong>What happens</strong>: Every job execution creates permanent records in Redis, causing memory to grow without bound</li>\n<li><strong>Why it&#39;s wrong</strong>: Eventually Redis hits memory limits, causing eviction or crashes; performance degrades as datasets grow</li>\n<li><strong>How to fix</strong>: Implement TTL on all job history keys (7-30 days). For longer retention, add optional archival to external storage.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 3: Dashboard queries blocking worker operations</strong></p>\n<ul>\n<li><strong>What happens</strong>: Complex dashboard queries (e.g., &quot;show all jobs from last month&quot;) run expensive Redis operations that block worker access</li>\n<li><strong>Why it&#39;s wrong</strong>: Monitoring should be observational only; it should never impact system performance</li>\n<li><strong>How to fix</strong>: Use read replicas for dashboard queries, implement query timeouts, and denormalize data for common queries.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 4: Alert fatigue from noisy thresholds</strong></p>\n<ul>\n<li><strong>What happens</strong>: Setting alert thresholds too sensitively (e.g., alert on every single job failure) creates alert spam that operators ignore</li>\n<li><strong>Why it&#39;s wrong</strong>: Important alerts get lost in the noise; operators develop &quot;alert blindness&quot;</li>\n<li><strong>How to fix</strong>: Use rate-based alerting (error rate &gt; 5% over 5 minutes), implement alert deduplication, and tier alerts by severity.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 5: Exposing sensitive job data in dashboard</strong></p>\n<ul>\n<li><strong>What happens</strong>: Job arguments containing PII, passwords, or tokens are displayed in dashboard without sanitization</li>\n<li><strong>Why it&#39;s wrong</strong>: Security breach; sensitive data exposed to anyone with dashboard access</li>\n<li><strong>How to fix</strong>: Implement job argument sanitization filters, mask sensitive fields (e.g., <code>password: ***</code>), and control dashboard access with authentication.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 6: Hardcoded metric collection intervals</strong></p>\n<ul>\n<li><strong>What happens</strong>: Metric collection runs at fixed intervals (e.g., every 10 seconds) regardless of system load</li>\n<li><strong>Why it&#39;s wrong</strong>: Under low load, this wastes resources; under high load, it may not capture enough detail</li>\n<li><strong>How to fix</strong>: Implement adaptive sampling: increase frequency during high activity, decrease during quiet periods.</li>\n</ul>\n<h3 id=\"implementation-guidance-for-monitoring\">Implementation Guidance for Monitoring</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Web Framework</strong></td>\n<td>Flask + Flask-SSE</td>\n<td>FastAPI with async SSE support</td>\n</tr>\n<tr>\n<td><strong>Frontend Dashboard</strong></td>\n<td>Vanilla JavaScript + Chart.js</td>\n<td>React/Vue.js with D3.js for custom visualizations</td>\n</tr>\n<tr>\n<td><strong>Real-time Updates</strong></td>\n<td>Server-Sent Events (SSE)</td>\n<td>WebSockets with Socket.IO fallback</td>\n</tr>\n<tr>\n<td><strong>Metrics Storage</strong></td>\n<td>Redis TimeSeries module</td>\n<td>Redis + PostgreSQL for long-term archival</td>\n</tr>\n<tr>\n<td><strong>Alerting Engine</strong></td>\n<td>Custom rule evaluator</td>\n<td>Integration with Prometheus Alertmanager</td>\n</tr>\n<tr>\n<td><strong>Charting Library</strong></td>\n<td>Chart.js (simple, good defaults)</td>\n<td>Apache ECharts (more customizable)</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background-job-processor/\n├── src/\n│   ├── job_processor/\n│   │   ├── __init__.py\n│   │   ├── core/                   # Milestone 1-2\n│   │   │   ├── job.py\n│   │   │   ├── queue_manager.py\n│   │   │   └── worker.py\n│   │   ├── retry/                  # Milestone 3\n│   │   │   ├── retry_manager.py\n│   │   │   └── backoff_calculator.py\n│   │   ├── scheduler/              # Milestone 4\n│   │   │   ├── scheduler.py\n│   │   │   └── cron_parser.py\n│   │   ├── monitoring/             # Milestone 5 (THIS SECTION)\n│   │   │   ├── __init__.py\n│   │   │   ├── metrics.py          # Metric collection and aggregation\n│   │   │   ├── events.py           # Event emission and handling\n│   │   │   ├── dashboard_api.py    # REST API endpoints\n│   │   │   ├── dashboard_ui.py     # Web dashboard (HTML/JS)\n│   │   │   ├── alerting.py         # Alert rule evaluation\n│   │   │   ├── history_store.py    # Job history storage\n│   │   │   └── sse_stream.py       # Server-Sent Events streaming\n│   │   └── utils/\n│   │       ├── redis_client.py\n│   │       └── config.py\n├── tests/\n│   └── monitoring/\n│       ├── test_metrics.py\n│       ├── test_dashboard_api.py\n│       └── test_alerting.py\n├── static/                         # Dashboard static assets\n│   ├── css/\n│   ├── js/\n│   └── charts/\n├── templates/                      # Dashboard HTML templates\n│   └── dashboard.html\n└── requirements.txt</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Complete metrics aggregation service (<code>monitoring/aggregator.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricsAggregator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Background service that aggregates raw monitoring events into metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis, interval_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> interval_seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the aggregation thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._aggregation_loop, </span><span style=\"color:#FFAB70\">daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop the aggregation thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _aggregation_loop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main aggregation loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._run_aggregation()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Log but continue - monitoring should not crash the system</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Aggregation error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _run_aggregation</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a single aggregation cycle.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Get all events since last run</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_run_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"monitoring:aggregator:last_run\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_run </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.get(last_run_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> last_run.decode() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> last_run </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"0-0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Read events from stream</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stream_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"monitoring:events\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        events </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.xrange(stream_key, </span><span style=\"color:#FFAB70\">min</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">start_id, </span><span style=\"color:#FFAB70\">max</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> events:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Process events by type and labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics_by_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> event_id, event_data </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> events:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            event_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event_data.get(</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"unknown\"</span><span style=\"color:#E1E4E8\">).decode()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            labels_json </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event_data.get(</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"labels\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"{}\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            labels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(labels_json.decode())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(event_data.get(</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event_data.get(</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"timestamp\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">).decode()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Create metric key from labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            label_parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">k</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">v</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(labels.items())]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metric_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">':'</span><span style=\"color:#E1E4E8\">.join(label_parts)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metrics_by_key[metric_key].append((timestamp, value))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate aggregates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucket_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.utcnow().replace(</span><span style=\"color:#FFAB70\">second</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">microsecond</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucket_timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(bucket_time.timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> metric_key, values </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> metrics_by_key.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> values:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Calculate aggregates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values_only </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [v </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> values]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(values_only)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            avg_val </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(values_only) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> count </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> count </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sorted_vals </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(values_only)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            p95 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sorted_vals[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(count </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.95</span><span style=\"color:#E1E4E8\">)] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> count </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Store aggregated metric</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metric_hash </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(count),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"avg\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(avg_val),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"p95\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(p95),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"min\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(values_only)),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"max\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(values_only))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            agg_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"monitoring:metrics:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">metric_key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">bucket_timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis.hmset(agg_key, metric_hash)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis.expire(agg_key, </span><span style=\"color:#79B8FF\">604800</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 7 days TTL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update last run pointer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_event_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> events[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> events </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> start_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis.set(last_run_key, last_event_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clean up old raw events (keep last hour only)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        one_hour_ago </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">((datetime.utcnow() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> timedelta(</span><span style=\"color:#FFAB70\">hours</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)).timestamp() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis.xtrim(stream_key, </span><span style=\"color:#FFAB70\">minid</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">one_hour_ago)</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Dashboard API endpoint for real-time metrics stream (<code>monitoring/dashboard_api.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Flask, Response, request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">app </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Flask(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/api/metrics/stream'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> metrics_stream</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Server-Sent Events stream for real-time dashboard updates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_events</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get Redis client from application context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Implement connection pooling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get last event ID from request header for reconnection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.headers.get(</span><span style=\"color:#9ECBFF\">'Last-Event-ID'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'0-0'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Subscribe to monitoring events stream</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stream_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'monitoring:events'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Read new events since last_id (blocking with timeout)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Use redis.xread with count=10, block=5000ms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                events </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []  </span><span style=\"color:#6A737D\"># Implement stream reading</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> stream, event_list </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> events:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    for</span><span style=\"color:#E1E4E8\"> event_id, event_data </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> event_list:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Filter events based on dashboard filters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # Get filters from request query params</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        queue_filter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.args.get(</span><span style=\"color:#9ECBFF\">'queue'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        job_type_filter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.args.get(</span><span style=\"color:#9ECBFF\">'job_type'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Format event for SSE (event: type, data: JSON)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        event_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event_data.get(</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">'type'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">'metric'</span><span style=\"color:#E1E4E8\">).decode()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        event_data_json </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.dumps({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            'id'</span><span style=\"color:#E1E4E8\">: event_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            'type'</span><span style=\"color:#E1E4E8\">: event_type,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            'data'</span><span style=\"color:#E1E4E8\">: {k.decode(): v.decode() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> event_data.items()},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            'timestamp'</span><span style=\"color:#E1E4E8\">: time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Yield SSE formatted data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        yield</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"event: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event_type</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">data: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event_data_json</span><span style=\"color:#79B8FF\">}\\n\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # Update last_id for reconnection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        last_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Send heartbeat comment every 30 seconds to keep connection alive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # yield \": heartbeat\\n\\n\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Log error and yield error event</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                error_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.dumps({</span><span style=\"color:#9ECBFF\">'error'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                yield</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"event: error</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">data: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error_data</span><span style=\"color:#79B8FF\">}\\n\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return SSE response with appropriate headers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Response(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        generate_events(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        mimetype</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'text/event-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        headers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'Cache-Control'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'no-cache'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'Connection'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'keep-alive'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'X-Accel-Buffering'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'no'</span><span style=\"color:#6A737D\">  # Disable nginx buffering</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Implement other API endpoints from the API table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - GET /api/metrics/system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - GET /api/metrics/queues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - GET /api/workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - GET /api/jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - GET /api/dead-letter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - POST /api/dead-letter/{job_id}/retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - DELETE /api/dead-letter/{job_id}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - GET /api/alerts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - POST /api/alerts/{alert_id}/acknowledge</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - POST /api/alert-rules</span></span></code></pre></div>\n\n<p><strong>Alert rule evaluator (<code>monitoring/alerting.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AlertRule</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents an alert rule with condition and thresholds.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, rule_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rule_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rule_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Unnamed Rule'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.severity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'severity'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'WARNING'</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># CRITICAL, WARNING, INFO</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.condition </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'condition'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># e.g., \"error_rate > 0.05\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.window_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'window_seconds'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 5 minutes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cooldown_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'cooldown_seconds'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 5 minutes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enabled </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'enabled'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.labels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'labels'</span><span style=\"color:#E1E4E8\">, {})  </span><span style=\"color:#6A737D\"># e.g., {\"queue\": \"emails\", \"job_type\": \"send_welcome\"}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate</span><span style=\"color:#E1E4E8\">(self, metrics_store) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate rule against current metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse condition string into evaluable expression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Example: \"error_rate > 0.05 AND queue_depth > 1000\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Fetch relevant metrics for the time window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use metrics_store.get_metrics(window_seconds=self.window_seconds, labels=self.labels)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate aggregated values from metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # For rate-based conditions, calculate rate over window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Evaluate condition against calculated values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use safe_eval or ast parsing for condition evaluation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if condition is met</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_alert</span><span style=\"color:#E1E4E8\">(self, last_alert_time: Optional[datetime]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if alert should fire considering cooldown period.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.enabled:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> last_alert_time </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check cooldown period</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seconds_since_last </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (datetime.utcnow() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> last_alert_time).total_seconds()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> seconds_since_last </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cooldown_seconds</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AlertManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages alert rules and firing alerts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rules: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, AlertRule] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rule_key_prefix </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"monitoring:alert_rules:\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_rules</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load all alert rules from Redis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Scan for all rule keys with prefix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use redis.scan_iter(self.rule_key_prefix + \"*\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each key, load JSON config and create AlertRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store in self.rules dictionary</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_all_rules</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate all enabled alert rules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get last alert times for each rule from Redis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each rule, evaluate condition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If condition met and cooldown passed, fire alert</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create SystemAlert object and store to Redis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update last alert time for rule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fire_alert</span><span style=\"color:#E1E4E8\">(self, rule: AlertRule, condition_value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Fire an alert and store it.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate unique alert_id (ULID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create SystemAlert object with all fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store to Redis Sorted Set (by timestamp) and Hash (full data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Optionally send notification (email, webhook, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return alert_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"alert_123\"</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<p><strong>Python Implementation Tips:</strong></p>\n<ol>\n<li><p><strong>Use Flask for simple dashboard</strong>: Flask + Flask-SSE provides a straightforward SSE implementation. For production, use <code>gevent</code> or <code>gunicorn</code> with async workers.</p>\n</li>\n<li><p><strong>Redis connection management</strong>: Use connection pooling for dashboard queries to avoid overwhelming Redis with connections:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> redis.connection </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ConnectionPool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ConnectionPool.from_url(</span><span style=\"color:#9ECBFF\">'redis://localhost:6379'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_connections</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span><span style=\"color:#FFAB70\">connection_pool</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pool)</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>SSE implementation</strong>: Flask doesn&#39;t natively support SSE well; use a queue-based approach:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Response</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   class</span><span style=\"color:#B392F0\"> MessageAnnouncer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">           self</span><span style=\"color:#E1E4E8\">.listeners </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       def</span><span style=\"color:#B392F0\"> listen</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           q </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue.Queue(</span><span style=\"color:#FFAB70\">maxsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">           self</span><span style=\"color:#E1E4E8\">.listeners.append(q)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           return</span><span style=\"color:#E1E4E8\"> q</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       def</span><span style=\"color:#B392F0\"> announce</span><span style=\"color:#E1E4E8\">(self, msg):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> reversed</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.listeners))):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">               try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                   self</span><span style=\"color:#E1E4E8\">.listeners[i].put_nowait(msg)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">               except</span><span style=\"color:#E1E4E8\"> queue.Full:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                   del</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.listeners[i]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   announcer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MessageAnnouncer()</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><p><strong>Async metrics collection</strong>: Use background threads for aggregation, not async/await, since Redis operations are blocking.</p>\n</li>\n<li><p><strong>Job argument sanitization</strong>: Before displaying job arguments in dashboard, filter sensitive fields:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">   SENSITIVE_FIELDS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'password'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'token'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'secret'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'key'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'authorization'</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> sanitize_job_args</span><span style=\"color:#E1E4E8\">(args_dict):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       sanitized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> args_dict.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(sensitive </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> key.lower() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> sensitive </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SENSITIVE_FIELDS</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               sanitized[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> '***REDACTED***'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               sanitized[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       return</span><span style=\"color:#E1E4E8\"> sanitized</span></span></code></pre></div>\n\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p><strong>After implementing Monitoring &amp; Dashboard, verify:</strong></p>\n<ol>\n<li><p><strong>Dashboard loads</strong>: Start the web server and navigate to <code>http://localhost:8080/dashboard</code>. You should see:</p>\n<ul>\n<li>System overview with queue depths</li>\n<li>Worker status table</li>\n<li>Real-time metrics updating every few seconds</li>\n</ul>\n</li>\n<li><p><strong>API endpoints work</strong>: Test API with curl:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Get queue metrics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/metrics/queues</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Get worker status</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/workers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Stream real-time events (SSE)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -N</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/metrics/stream</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><p><strong>Metrics collection</strong>: Enqueue a few jobs and verify they appear in:</p>\n<ul>\n<li>Dashboard queue depth indicators</li>\n<li>Job history table (after completion)</li>\n<li>Real-time event stream (if connected via SSE)</li>\n</ul>\n</li>\n<li><p><strong>Alerting test</strong>: Set up a simple alert rule (error rate &gt; 0%) and trigger a job failure:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Create alert rule via API</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> requests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   rule </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Test Error Alert\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"severity\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"WARNING\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"condition\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"error_rate > 0\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"window_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"labels\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"queue\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   requests.post(</span><span style=\"color:#9ECBFF\">\"http://localhost:8080/api/alert-rules\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">json</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">rule)</span></span></code></pre></div>\n<p>   Check that alerts appear in <code>/api/alerts</code> endpoint after triggering a job failure.</p>\n<p><strong>Signs something is wrong:</strong></p>\n<ul>\n<li>Dashboard shows &quot;No data&quot; or &quot;Connection lost&quot; → Check Redis connection and event emission</li>\n<li>Real-time updates not working → Check SSE endpoint and browser console for errors</li>\n<li>High Redis memory usage → Verify TTL is set on history keys and aggregation is running</li>\n<li>Dashboard loads slowly → Check for expensive Redis queries (use <code>SLOWLOG</code> command)</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Dashboard shows &quot;Disconnected&quot;</strong></td>\n<td>SSE connection failing</td>\n<td>Check browser console for network errors; verify <code>/api/metrics/stream</code> endpoint returns proper SSE headers</td>\n<td>Ensure response has <code>text/event-stream</code> MIME type and proper headers</td>\n</tr>\n<tr>\n<td><strong>Real-time updates delayed by minutes</strong></td>\n<td>Event aggregation not running</td>\n<td>Check if <code>MetricsAggregator</code> process is running; look for events in <code>monitoring:events</code> stream</td>\n<td>Start aggregator service; check for exceptions in aggregator logs</td>\n</tr>\n<tr>\n<td><strong>Redis memory growing rapidly</strong></td>\n<td>Job history TTL not set</td>\n<td>Check Redis keys with <code>keys job:history:*</code>; verify TTL with <code>ttl keyname</code></td>\n<td>Ensure all history keys have TTL; implement periodic cleanup job</td>\n</tr>\n<tr>\n<td><strong>Alert rules not firing</strong></td>\n<td>Rule evaluation failing</td>\n<td>Check alert manager logs; test rule condition manually with current metrics</td>\n<td>Debug rule evaluation logic; verify metrics exist for rule labels</td>\n</tr>\n<tr>\n<td><strong>Dashboard queries timing out</strong></td>\n<td>Expensive Redis operations</td>\n<td>Use <code>redis-cli --stat</code> to see operations; check <code>SLOWLOG GET 10</code></td>\n<td>Add query limits; implement caching for expensive queries; use read replica</td>\n</tr>\n<tr>\n<td><strong>Job arguments visible in dashboard</strong></td>\n<td>Sanitization not applied</td>\n<td>Check dashboard display of job details; look for sensitive fields</td>\n<td>Implement argument sanitization in Job.to_dict() or dashboard serialization</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section integrates concepts and components from all five milestones, showing how they work together to handle key operational scenarios: normal job flow, error recovery, and scheduled job execution. Understanding these interactions is critical for implementing a cohesive system.</p>\n</blockquote>\n<p>This section details the end-to-end behavior of the Background Job Processor through three critical scenarios: the successful processing of a job from submission to completion, the recovery flow when a worker crashes mid-execution, and the lifecycle of a scheduled recurring job. These scenarios demonstrate how the system&#39;s components—<code>QueueManager</code>, <code>Worker</code>, <code>RetryManager</code>, <code>Scheduler</code>, and monitoring subsystems—orchestrate data flow and state transitions to deliver reliable asynchronous processing.</p>\n<h3 id=\"happy-path-job-submission-to-completion\">Happy Path: Job Submission to Completion</h3>\n<p><strong>Mental Model: Assembly Line in a Factory</strong>\nImagine a manufacturing plant. An order (job) is submitted to the front office (producer). The office validates the order, stamps it with a unique ID, and places it on the correct conveyor belt (queue) based on its type. A worker on the factory floor picks up the order, performs the required assembly steps (handler execution), marks it as completed, and places the finished product in the shipping area (result storage). The entire process is tracked in a central log (monitoring system), allowing managers to see the order&#39;s progress in real-time.</p>\n<p>This scenario begins with a producer submitting a job and ends with the job&#39;s successful completion, involving all core components in their normal operational states.</p>\n<h4 id=\"step-by-step-flow\">Step-by-Step Flow</h4>\n<p>The following numbered sequence details the journey of a single job, referencing the <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fsystem-component.svg\" alt=\"System Component Overview\"> and <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fhappy-path-sequence.svg\" alt=\"Happy Path Job Execution Sequence\"> diagrams.</p>\n<ol>\n<li><p><strong>Job Creation and Submission:</strong> The producer application creates a <code>Job</code> instance, populating its required fields (<code>job_type</code>, <code>args</code>, <code>queue</code>) and optional metadata. It then calls <code>QueueManager.enqueue_job(job)</code>.</p>\n</li>\n<li><p><strong>Validation and Serialization:</strong> Inside <code>QueueManager.enqueue_job</code>, the following occurs:</p>\n<ul>\n<li><code>QueueManager._validate_job(job)</code> is called. It checks that the payload size is under <code>SystemConfig.max_payload_size</code>, that the specified queue exists in <code>QueueManager.queue_configs</code>, and that the job&#39;s <code>job_type</code> is a non-empty string. If validation fails, a <code>ValidationError</code> is raised to the producer.</li>\n<li>The job&#39;s <code>status</code> is set to <code>PENDING</code>, <code>job_id</code> is assigned a ULID, and <code>created_at</code> is set to the current UTC timestamp.</li>\n<li><code>Job.serialize()</code> is invoked, converting the job&#39;s dictionary representation (from <code>Job.to_dict()</code>) into a JSON string.</li>\n</ul>\n</li>\n<li><p><strong>Atomic Enqueue to Redis:</strong> The <code>QueueManager</code> uses a Redis pipeline (via <code>RedisClient.pipeline()</code>) to execute the following commands atomically:</p>\n<ul>\n<li><code>HSET</code> to store the serialized job under a key like <code>job:{job_id}</code>. This creates a persistent record.</li>\n<li><code>EXPIRE</code> is set on the job key using a TTL (e.g., 7 days) to prevent indefinite memory bloat.</li>\n<li><code>LPUSH</code> to add the <code>job_id</code> to the appropriate Redis list key, e.g., <code>queue:{queue_name}</code>. This action makes the job visible to workers.</li>\n<li><code>INCR</code> on a metric key like <code>stats:queue:{queue_name}:enqueued</code> for monitoring.</li>\n<li>If the queue has a <code>QueueConfig.max_length</code>, the pipeline also checks <code>LLEN</code> and may raise a <code>QueueFullError</code> before the <code>LPUSH</code>.</li>\n<li>The pipeline is executed. Upon success, <code>enqueue_job</code> returns the generated <code>job_id</code> to the producer.</li>\n</ul>\n</li>\n<li><p><strong>Worker Polling and Dequeue:</strong> A <code>Worker</code> process, configured to listen on the relevant queue, is in its main loop. It uses the priority-weighted polling algorithm to select a queue and then calls <code>BRPOPLPUSH</code> (or a similar reliable dequeue pattern) with a timeout. This command atomically moves the <code>job_id</code> from the main <code>queue:{queue_name}</code> list to a temporary <code>queue:{queue_name}:processing</code> list, providing at-least-once delivery semantics. The worker receives the <code>job_id</code>.</p>\n</li>\n<li><p><strong>Job Materialization and Dispatch:</strong> The worker uses <code>QueueManager.get_job_by_id(job_id)</code> to fetch the full job data from the <code>job:{job_id}</code> hash. It then calls <code>Job.deserialize()</code> to recreate the <code>Job</code> object in memory. The worker updates the job&#39;s <code>status</code> to <code>ACTIVE</code> and <code>started_at</code> to the current time, persisting this change back to Redis. It then looks up the job&#39;s <code>job_type</code> in its internal <code>Worker.handlers</code> registry to find the appropriate handler function.</p>\n</li>\n<li><p><strong>Job Execution:</strong> The worker invokes the registered handler function, passing the job&#39;s <code>args</code> and <code>kwargs</code>. The handler executes the business logic (e.g., sending an email, processing an image). The worker&#39;s <code>WorkerHeartbeat</code> thread continues to periodically update a key like <code>worker:heartbeat:{worker_id}</code> in Redis, indicating liveness.</p>\n</li>\n<li><p><strong>Success Handling and Cleanup:</strong> Upon successful handler completion:</p>\n<ul>\n<li>The worker updates the job&#39;s <code>status</code> to <code>COMPLETED</code>, sets <code>completed_at</code>, and optionally stores the handler&#39;s return value in <code>result</code>.</li>\n<li>The job&#39;s updated state is persisted to the <code>job:{job_id}</code> hash in Redis.</li>\n<li>The worker removes the <code>job_id</code> from the <code>queue:{queue_name}:processing</code> list (e.g., via <code>LREM</code>), completing the reliable dequeue cycle.</li>\n<li>Metric keys are incremented (e.g., <code>stats:queue:{queue_name}:completed</code>).</li>\n</ul>\n</li>\n<li><p><strong>Monitoring Update:</strong> The <code>MetricsAggregator</code>, running periodically, observes the changes in job state and queue lengths. It updates aggregated metrics (like <code>QueueMetrics.process_rate</code>). If the dashboard is open, it may receive these updates via Server-Sent Events (SSE), refreshing the display to show the job&#39;s completion and updated queue statistics.</p>\n</li>\n</ol>\n<p>The following table summarizes the key Redis commands and state changes during this flow:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Step</th>\n<th align=\"left\">Primary Redis Command(s)</th>\n<th align=\"left\">Job State Transition</th>\n<th align=\"left\">Key Data Structures Updated</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1-3</td>\n<td align=\"left\"><code>HSET job:{id} ...</code>, <code>EXPIRE</code>, <code>LPUSH queue:{name} {id}</code></td>\n<td align=\"left\">None → <code>PENDING</code></td>\n<td align=\"left\"><code>job:{id}</code> (Hash), <code>queue:{name}</code> (List)</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\"><code>BRPOPLPUSH queue:{name} queue:{name}:processing</code></td>\n<td align=\"left\"><code>PENDING</code> → (Implicitly Active)</td>\n<td align=\"left\"><code>queue:{name}</code> (List), <code>queue:{name}:processing</code> (List)</td>\n</tr>\n<tr>\n<td align=\"left\">5-6</td>\n<td align=\"left\"><code>HGET job:{id}</code>, <code>HSET job:{id} status ACTIVE</code></td>\n<td align=\"left\"><code>PENDING</code> → <code>ACTIVE</code></td>\n<td align=\"left\"><code>job:{id}</code> (Hash), <code>worker:heartbeat:{id}</code> (String)</td>\n</tr>\n<tr>\n<td align=\"left\">7</td>\n<td align=\"left\"><code>HSET job:{id} status COMPLETED ...</code>, <code>LREM queue:{name}:processing {id}</code></td>\n<td align=\"left\"><code>ACTIVE</code> → <code>COMPLETED</code></td>\n<td align=\"left\"><code>job:{id}</code> (Hash), <code>queue:{name}:processing</code> (List)</td>\n</tr>\n</tbody></table>\n<h3 id=\"error-recovery-flow-worker-crash-and-retry\">Error Recovery Flow: Worker Crash and Retry</h3>\n<p><strong>Mental Model: Paramedic Response and Hospital Triage</strong>\nWhen a factory worker (the <code>Worker</code> process) suddenly faints (crashes), the system&#39;s safety protocols activate. First, a supervisor (the heartbeat monitor) notices the worker&#39;s missing check-ins and marks them as inactive. Another worker then safely retrieves the half-assembled product (the job) from the worker&#39;s station (the processing queue) and assesses the damage. If the product can be fixed, it&#39;s sent to a repair station (the retry queue) with a note indicating the problem and a scheduled repair time. If it&#39;s beyond repair after several attempts, it&#39;s sent to the quality control failure analysis bin (the dead letter queue) for engineering review.</p>\n<p>This scenario highlights the system&#39;s fault tolerance, demonstrating how jobs are not lost when a <code>Worker</code> fails unexpectedly and how the retry mechanism with exponential backoff manages transient failures.</p>\n<h4 id=\"step-by-step-flow\">Step-by-Step Flow</h4>\n<p>The sequence, also illustrated in <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fretry-flow-sequence.svg\" alt=\"Retry with Exponential Backoff Flow\">, begins after step 6 of the Happy Path, assuming the worker crashes during job execution.</p>\n<ol>\n<li><p><strong>Worker Crash:</strong> During the execution of a handler (step 6), the <code>Worker</code> process suffers a fatal error (e.g., a segmentation fault, <code>SIGKILL</code>, or host failure) and terminates immediately. It does not reach the success handling logic (step 7).</p>\n</li>\n<li><p><strong>Stale Heartbeat Detection:</strong> The worker&#39;s <code>WorkerHeartbeat</code> thread also stops, ceasing updates to the <code>worker:heartbeat:{worker_id}</code> key in Redis. The <code>WorkerHeartbeat</code> key has a TTL slightly longer than the heartbeat interval (e.g., interval=30s, TTL=45s). After the TTL expires, the key disappears. The monitoring system&#39;s <code>AlertManager</code> or dashboard, which periodically scans for worker keys, now identifies this worker as &quot;stale&quot; or &quot;dead.&quot;</p>\n</li>\n<li><p><strong>Job Re-queue via Processing Queue:</strong> Because the job was dequeued using <code>BRPOPLPUSH</code> (or a similar pattern), its <code>job_id</code> remains in the <code>queue:{queue_name}:processing</code> list. A separate &quot;janitor&quot; process (which can be part of the <code>Worker</code> startup routine or a dedicated maintenance script) periodically scans all <code>queue:*:processing</code> lists. For each <code>job_id</code> found, it checks if the associated worker (which can be tracked by storing the <code>worker_id</code> in the processing list or alongside the job) is still alive (by checking the heartbeat key). If the worker is dead, the janitor process moves the <code>job_id</code> back from the <code>queue:{queue_name}:processing</code> list to the main <code>queue:{queue_name}</code> list using <code>RPOPLPUSH</code> or a similar atomic operation. The job is now <code>PENDING</code> again and available for any healthy worker.</p>\n</li>\n<li><p><strong>Worker Retry and Failure:</strong> A healthy worker picks up the job (returning to step 4). It loads the job, sees its <code>status</code> is <code>PENDING</code> (or <code>ACTIVE</code> with a very old <code>started_at</code>), and begins execution. This time, the handler fails due to a transient error (e.g., a network timeout, a deadlock). The worker catches the exception.</p>\n</li>\n<li><p><strong>Error Recording and Retry Check:</strong> The worker calls <code>Job.record_error(error)</code>, which appends a structured error dictionary (containing exception class, message, and traceback) to <code>Job.errors</code> and increments <code>Job.attempts</code>. It then calls <code>Job.should_retry()</code> which compares <code>Job.attempts</code> against <code>Job.max_retries</code>. If retries are allowed, the worker proceeds to the retry flow.</p>\n</li>\n<li><p><strong>Retry Scheduling:</strong> The worker calls <code>RetryManager.handle_job_failure(job, error)</code>. This method:</p>\n<ul>\n<li>Calculates the next retry delay using <code>BackoffCalculator.calculate_delay(Job.attempts)</code>. For attempt 1, this might be 1 second; for attempt 2, 2 seconds, etc., plus optional jitter.</li>\n<li>Computes an absolute timestamp for the retry: <code>current_time + delay</code>.</li>\n<li>Updates the job&#39;s <code>status</code> to <code>RETRY_SCHEDULED</code>.</li>\n<li>Uses <code>ZADD</code> to add the <code>job_id</code> with a score equal to the retry timestamp to a Redis sorted set key, e.g., <code>retry_queue:{queue_name}</code>.</li>\n<li>Persists the updated job (with the new error and attempt count) to the <code>job:{job_id}</code> hash.</li>\n<li>Removes the <code>job_id</code> from the processing queue.</li>\n</ul>\n</li>\n<li><p><strong>Retry Polling and Re-enqueue:</strong> The <code>RetryManager</code> (or a dedicated thread within each <code>Worker</code>) periodically polls the retry sorted sets using <code>ZRANGEBYSCORE</code> with a score range of <code>0</code> to <code>current_timestamp</code>. For each due <code>job_id</code>, it retrieves the full job, changes its <code>status</code> back to <code>PENDING</code>, and calls <code>QueueManager.enqueue_job(job)</code> to place it back on its original main queue. The job ID is removed from the sorted set via <code>ZREM</code>.</p>\n</li>\n<li><p><strong>Eventual Success or Final Failure:</strong> The job is retried by workers (looping through steps 4-7). If it eventually succeeds, the happy path completes. If <code>Job.attempts</code> exceeds <code>Job.max_retries</code>, <code>Job.should_retry()</code> returns <code>False</code>.</p>\n</li>\n<li><p><strong>Dead Letter Queue (DLQ) Placement:</strong> In the case of final failure, <code>RetryManager.handle_job_failure</code> calls <code>RetryManager.move_to_dead_letter(job)</code>. This method:</p>\n<ul>\n<li>Sets the job&#39;s <code>status</code> to <code>DEAD_LETTER</code>.</li>\n<li>Persists the final state to <code>job:{job_id}</code>.</li>\n<li>Adds the <code>job_id</code> to a dedicated list or set key, e.g., <code>dead_letter_queue</code>.</li>\n<li>Triggers a high-priority alert (e.g., <code>CRITICAL</code> severity) via the <code>AlertManager</code>.</li>\n<li>The job is now out of the processing loop and awaits manual intervention via the dashboard&#39;s <code>retry_dead_letter_job</code> or <code>delete_dead_letter_job</code> APIs.</li>\n</ul>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Failure Scenario</th>\n<th align=\"left\">Detection Mechanism</th>\n<th align=\"left\">Automatic Recovery Action</th>\n<th align=\"left\">Manual Intervention Required?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Worker Process Crash</td>\n<td align=\"left\">Heartbeat TTL expiry</td>\n<td align=\"left\">Job moved from processing queue back to main queue via janitor process.</td>\n<td align=\"left\">No, unless the janitor process itself fails.</td>\n</tr>\n<tr>\n<td align=\"left\">Transient Handler Error (e.g., network timeout)</td>\n<td align=\"left\">Exception caught by worker wrapper.</td>\n<td align=\"left\">Job is scheduled for retry with exponential backoff.</td>\n<td align=\"left\">No.</td>\n</tr>\n<tr>\n<td align=\"left\">Persistent Handler Error (e.g., bug in code)</td>\n<td align=\"left\">Max retries exhausted.</td>\n<td align=\"left\">Job moved to Dead Letter Queue (DLQ).</td>\n<td align=\"left\"><strong>Yes.</strong> An operator must inspect the error and decide to retry or delete.</td>\n</tr>\n<tr>\n<td align=\"left\">Redis Connection Loss</td>\n<td align=\"left\">Redis client exception, heartbeat fails.</td>\n<td align=\"left\">Worker may crash/restart; jobs remain in processing queue until janitor cleans up.</td>\n<td align=\"left\">No, assuming worker process manager (e.g., systemd) restarts it.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The combination of a <strong>processing queue</strong> (for crash safety) and a <strong>retry sorted set</strong> (for scheduled retries) is crucial. The processing queue ensures no job is ever truly &quot;in flight&quot; without a paper trail in Redis, enabling recovery from worker crashes. The sorted set provides efficient scheduling without requiring a separate timer thread for each failed job.</p>\n</blockquote>\n<h4 id=\"common-pitfalls-in-error-recovery\">Common Pitfalls in Error Recovery</h4>\n<p>⚠️ <strong>Pitfall: Busy-Waiting on Dead Workers</strong>\n<strong>Mistake:</strong> The janitor process continuously polls the processing queues in a tight loop, consuming CPU and Redis resources even when no workers have crashed.\n<strong>Why it&#39;s wrong:</strong> This wastes resources and can exacerbate problems during system stress.\n<strong>Fix:</strong> Implement an exponential backoff in the janitor&#39;s polling loop (e.g., sleep for 5 seconds, then 10, up to a max of 60 seconds). Additionally, use Redis&#39;s <code>KEYS</code> or <code>SCAN</code> command sparingly to find processing queues, and consider publishing events to a Pub/Sub channel when a worker stops heartbeating to trigger immediate cleanup.</p>\n<p>⚠️ <strong>Pitfall: Losing Error Context on Retry</strong>\n<strong>Mistake:</strong> Overwriting the job&#39;s error field on each retry, so only the last error is preserved.\n<strong>Why it&#39;s wrong:</strong> Debugging becomes difficult because you cannot see the history of failures that led to the dead letter state.\n<strong>Fix:</strong> Use <code>Job.record_error(error)</code>, which <em>appends</em> to the <code>Job.errors</code> list. Ensure the serialization format (e.g., JSON) supports a list of complex error objects.</p>\n<p>⚠️ <strong>Pitfall: Retry Storm on Permanent Failure</strong>\n<strong>Mistake:</strong> A job with a permanent error (e.g., &quot;FileNotFound&quot;) is continuously retried, wasting resources and clogging the queue until <code>max_retries</code> is reached.\n<strong>Why it&#39;s wrong:</strong> It delays the processing of other jobs and creates unnecessary load.\n<strong>Fix:</strong> Implement <strong>retry filters</strong> or allow handlers to raise specific exception types (e.g., <code>ImmediateFailError</code>) that bypass the retry logic and go directly to the DLQ.</p>\n<h3 id=\"scheduled-job-cron-to-execution\">Scheduled Job: Cron to Execution</h3>\n<p><strong>Mental Model: Calendar App with Recurring Events</strong>\nThink of a sophisticated calendar application. You create a recurring event &quot;Team Stand-up&quot; scheduled for &quot;Every weekday at 9 AM UTC.&quot; The calendar service doesn&#39;t create all future events at once. Instead, it constantly looks at the current time and the rules for all recurring events. When it detects that the current time matches a rule&#39;s &quot;next scheduled time,&quot; it creates a new calendar entry (enqueues a job) for that specific occurrence. It then immediately calculates the <em>next</em> occurrence after this one and waits. If the service is down for maintenance at 9 AM, upon restart, it checks which events it missed and creates them immediately (catch-up logic).</p>\n<p>This scenario involves the <code>Scheduler</code> component, demonstrating how future-dated and recurring jobs are managed, enqueued, and eventually processed.</p>\n<h4 id=\"step-by-step-flow\">Step-by-Step Flow</h4>\n<ol>\n<li><p><strong>Schedule Definition:</strong> An administrator or application calls <code>schedule_recurring_job(job, cron_expression, timezone=&#39;UTC&#39;)</code>. This function creates a <code>Schedule</code> object, assigns a <code>schedule_id</code>, and calculates the initial <code>next_run_at</code> time using <code>_calculate_next_run(schedule, reference_time=now)</code>. The <code>Schedule</code> is persisted to a Redis hash (e.g., <code>schedule:{schedule_id}</code>). The <code>Schedule.status</code> is set to <code>PENDING</code> for its future runs.</p>\n</li>\n<li><p><strong>Scheduler Polling Loop:</strong> The <code>Scheduler</code> process runs <code>_poll_due_schedules()</code> in a loop, sleeping for <code>polling_interval_seconds</code> between iterations. During each poll:</p>\n<ul>\n<li>It fetches all <code>Schedule</code> objects from Redis where <code>enabled=True</code>.</li>\n<li>For each schedule, it checks if <code>Schedule.next_run_at &lt;= current_time</code>. If not, it skips.</li>\n<li>For due schedules, it calls <code>_process_schedule(schedule)</code>.</li>\n</ul>\n</li>\n<li><p><strong>Processing a Due Schedule:</strong> <code>_process_schedule</code> performs the following atomic operations (typically within a Redis pipeline/WATCH to handle concurrent schedulers):</p>\n<ul>\n<li><strong>Uniqueness Check:</strong> If <code>Schedule.unique_key</code> is set, it uses Redis&#39;s <code>SET key value NX EX window</code> command to claim a lock for the <code>unique_window_seconds</code>. If the lock cannot be acquired (because a duplicate was enqueued recently), the schedule&#39;s status for this run is marked as <code>SKIPPED</code>, <code>last_enqueued_at</code> is updated, and the next run time is recalculated.</li>\n<li><strong>Job Creation and Enqueue:</strong> If unique (or not required), it creates a concrete <code>Job</code> instance from the template in the <code>Schedule</code>. The job&#39;s <code>queue</code>, <code>job_type</code>, <code>args</code>, <code>kwargs</code>, and metadata are copied. The job&#39;s <code>metadata</code> field may be augmented with the <code>schedule_id</code> and the <code>scheduled_for</code> timestamp.</li>\n<li><code>_enqueue_scheduled_job</code> is called, which uses <code>QueueManager.enqueue_job</code> to push the job onto the designated worker queue.</li>\n<li><strong>Schedule Update:</strong> Upon successful enqueue, the schedule&#39;s <code>last_enqueued_at</code> is set to the current time, and <code>status</code> for this run is set to <code>ENQUEUED</code>. <code>Schedule.next_run_at</code> is recalculated based on the <code>cron_expression</code> and <code>timezone</code>, using <code>_calculate_next_run(schedule, reference_time=now)</code>.</li>\n</ul>\n</li>\n<li><p><strong>Catch-up Logic on Scheduler Start:</strong> When the <code>Scheduler</code> starts (e.g., after a deployment or crash), it runs <code>calculate_next_runs_for_all_schedules()</code>. This method ensures <code>next_run_at</code> is set correctly for all schedules. More importantly, if <code>Scheduler</code> detects that a schedule&#39;s <code>next_run_at</code> is in the past (meaning it missed runs while down), it will immediately process those missed schedules, enqueuing jobs for each missed occurrence up to a sensible limit (to prevent a thundering herd). This is a critical feature for reliability.</p>\n</li>\n<li><p><strong>Job Execution:</strong> From this point, the journey of the enqueued job is identical to the <strong>Happy Path</strong>. Workers pick it up (step 4 onward) and execute it. The job&#39;s metadata retains its origin as a scheduled job, which can be useful for monitoring and debugging.</p>\n</li>\n<li><p><strong>Timezone and Daylight Saving Time (DST) Handling:</strong> The <code>_calculate_next_run</code> function uses a timezone-aware datetime library (like <code>pytz</code> or <code>zoneinfo</code>). When calculating the next run for a schedule in a timezone that observes DST, the function correctly handles ambiguous or non-existent times (e.g., 2:30 AM during a &quot;spring forward&quot; transition). A common strategy is to default to the later valid time during an ambiguous period and to skip non-existent times by advancing to the next valid interval.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Schedule Event</th>\n<th align=\"left\">Scheduler Action</th>\n<th align=\"left\">Redis Command Example</th>\n<th align=\"left\">Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">New recurring schedule created.</td>\n<td align=\"left\">Calculate <code>next_run_at</code>, store schedule.</td>\n<td align=\"left\"><code>HSET schedule:{id} ...</code></td>\n<td align=\"left\">Schedule is dormant until its first due time.</td>\n</tr>\n<tr>\n<td align=\"left\">Poll finds due schedule.</td>\n<td align=\"left\">Check uniqueness, create job, enqueue, update schedule.</td>\n<td align=\"left\">Pipeline: <code>SET NX</code>, <code>HSET job:{jid}</code>, <code>LPUSH queue:...</code>, <code>HSET schedule:{sid}</code></td>\n<td align=\"left\">Job is placed on worker queue. Schedule is updated for next run.</td>\n</tr>\n<tr>\n<td align=\"left\">Scheduler restarts after crash.</td>\n<td align=\"left\">Recalculate next runs, process missed schedules.</td>\n<td align=\"left\"><code>ZSCAN</code> for past-due <code>next_run_at</code> scores, then process.</td>\n<td align=\"left\">Missed jobs are enqueued, ensuring no schedule is permanently skipped.</td>\n</tr>\n<tr>\n<td align=\"left\">Admin pauses a schedule.</td>\n<td align=\"left\">Set <code>Schedule.enabled = False</code>.</td>\n<td align=\"left\"><code>HSET schedule:{id} enabled 0</code></td>\n<td align=\"left\">Scheduler skips this schedule in future polls.</td>\n</tr>\n</tbody></table>\n<h4 id=\"architecture-decision-record-scheduler-catch-up-strategy\">Architecture Decision Record: Scheduler Catch-up Strategy</h4>\n<blockquote>\n<p><strong>Decision: Limited Catch-up with Skip Option</strong></p>\n<ul>\n<li><strong>Context:</strong> When the Scheduler process is down (for maintenance, crash, etc.), scheduled jobs will be missed. Upon restart, we must decide how to handle these missed occurrences.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Enqueue All Missed Jobs:</strong> Calculate every missed execution time since the last successful run and enqueue a job for each.</li>\n<li><strong>Enqueue Only the Most Recent Missed Job:</strong> Enqueue a single job for the most recent missed occurrence.</li>\n<li><strong>Skip All Missed Jobs:</strong> Do not enqueue any missed jobs; simply resume the schedule from the next future time.</li>\n<li><strong>Configurable, Limited Catch-up:</strong> Allow configuration to enqueue up to N missed jobs, with an option to skip the rest.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Option 4 (Configurable, Limited Catch-up).</li>\n<li><strong>Rationale:</strong> Option 1 can cause a dangerous &quot;thundering herd&quot; problem, instantly flooding the system with a backlog of jobs which may overwhelm workers and downstream services. Option 2 loses data (missed executions). Option 3 may be unacceptable for critical schedules (e.g., daily reports). Option 4 provides safety with a cap (e.g., max 10 catch-up jobs) and allows operators to tune based on the job&#39;s idempotency and importance.</li>\n<li><strong>Consequences:</strong> The system is protected from self-inflicted denial-of-service on restart. Operators must understand the catch-up limit for their critical schedules. Some missed jobs beyond the limit will be permanently skipped, which may require manual intervention for critical business processes.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Option</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Enqueue All</td>\n<td align=\"left\">No data loss, simple.</td>\n<td align=\"left\">Risk of thundering herd, system overload.</td>\n<td align=\"left\">Too dangerous for automated recovery.</td>\n</tr>\n<tr>\n<td align=\"left\">Enqueue Most Recent</td>\n<td align=\"left\">Handles short outages, simple.</td>\n<td align=\"left\">Loses history of multiple missed runs.</td>\n<td align=\"left\">Unacceptable for schedules with high frequency or long outages.</td>\n</tr>\n<tr>\n<td align=\"left\">Skip All</td>\n<td align=\"left\">Safe, no overload.</td>\n<td align=\"left\">Potentially unacceptable data loss for business.</td>\n<td align=\"left\">Lacks flexibility for important jobs.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Configurable, Limited Catch-up</strong></td>\n<td align=\"left\"><strong>Safe, flexible, tunable per schedule.</strong></td>\n<td align=\"left\"><strong>Adds configuration complexity.</strong></td>\n<td align=\"left\"><strong>Chosen for optimal safety and control.</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete code skeletons and structure to implement the interaction flows described above.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Interaction Scenario</th>\n<th align=\"left\">Simple Option</th>\n<th align=\"left\">Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Reliable Job Dequeue (Worker)</td>\n<td align=\"left\">Use <code>BRPOPLPUSH</code> with a processing queue.</td>\n<td align=\"left\">Use Redis Streams (<code>XREADGROUP</code>, <code>XACK</code>) for more sophisticated consumer groups and pending entry management.</td>\n</tr>\n<tr>\n<td align=\"left\">Retry Scheduling</td>\n<td align=\"left\">Use a Redis Sorted Set (<code>ZSET</code>) with retry timestamp as score.</td>\n<td align=\"left\">Use a dedicated time-series database or a priority queue service for extremely high-scale retry scheduling.</td>\n</tr>\n<tr>\n<td align=\"left\">Scheduler Polling</td>\n<td align=\"left\">Simple <code>time.sleep()</code> loop in the main thread.</td>\n<td align=\"left\">Use <code>asyncio</code> with async Redis client for non-blocking polling of multiple schedules.</td>\n</tr>\n<tr>\n<td align=\"left\">Janitor Process for Crashed Workers</td>\n<td align=\"left\">A separate script run by a cron job every minute.</td>\n<td align=\"left\">Integrate the cleanup logic into the <code>Worker</code>&#39;s startup routine and use Redis Pub/Sub to broadcast worker deaths for immediate cleanup.</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Add the following files to manage the interaction flows and recovery processes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background_job_processor/\n├── core/\n│   ├── __init__.py\n│   ├── job.py                  # Job class definition\n│   ├── queue_manager.py        # QueueManager class\n│   ├── worker.py               # Worker class, main loop\n│   ├── retry_manager.py        # RetryManager class\n│   ├── scheduler.py            # Scheduler class\n│   └── janitor.py              # Cleanup process for stalled jobs\n├── models/\n│   ├── __init__.py\n│   ├── schedule.py             # Schedule, ScheduledJob classes\n│   └── metrics.py              # MetricPoint, QueueMetrics, etc.\n├── web/\n│   ├── __init__.py\n│   └── dashboard.py            # API endpoints for dashboard\n├── utils/\n│   ├── __init__.py\n│   ├── redis_client.py         # RedisClient wrapper\n│   └── backoff.py              # BackoffCalculator\n└── scripts/\n    ├── run_worker.py           # Worker entry point\n    ├── run_scheduler.py        # Scheduler entry point\n    └── run_janitor.py          # Janitor script (can be cron)</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong><code>utils/redis_client.py</code> - A robust Redis client wrapper:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> functools </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> wraps</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Singleton wrapper for Redis client with connection pooling and error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _instance: Optional[</span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _client: Optional[redis.Redis] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">connection_kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # ... (initialization as per naming conventions)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_instance</span><span style=\"color:#E1E4E8\">(cls, url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> </span><span style=\"color:#9ECBFF\">'RedisClient'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> url </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"URL required for first RedisClient initialization\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            cls</span><span style=\"color:#E1E4E8\">._instance </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(url, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_client</span><span style=\"color:#E1E4E8\">(self) -> redis.Redis:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> or</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.ping():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._reconnect()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> ping</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client.ping()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> (redis.ConnectionError, redis.TimeoutError):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _reconnect</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # ... reconnection logic with retry backoff</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute</span><span style=\"color:#E1E4E8\">(self, command: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a Redis command with automatic retry on connection errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(max_retries):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                method </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(client, command.lower())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> method(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> (redis.ConnectionError, redis.TimeoutError) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> max_retries </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logging.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis connection error on </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">command</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, retry </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attempt</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._reconnect()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis error on </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">command</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline</span><span style=\"color:#E1E4E8\">(self) -> redis.Pipeline:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return a pipeline for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_client().pipeline()</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong><code>core/janitor.py</code> - Cleanup process for crashed workers:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .redis_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Janitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Cleans up jobs stuck in processing queues from dead workers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: RedisClient, scan_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scan_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scan_interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_stale_processing_queues</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find all queue:processing lists where the associated worker is dead.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use SCAN to find all keys matching pattern 'queue:*:processing'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each processing queue key, extract the worker_id (might be stored in a separate key or in the list's metadata)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if the worker's heartbeat key (worker:heartbeat:{id}) exists and hasn't expired</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If worker is dead, add the processing queue key to the result list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the list of stale processing queue keys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _requeue_stale_jobs</span><span style=\"color:#E1E4E8\">(self, processing_queue_key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Move all jobs from a stale processing queue back to its main queue.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract the main queue name from the processing queue key (e.g., 'queue:email:processing' -> 'queue:email')</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: In a loop, use RPOPLPUSH to move jobs from processing queue back to main queue atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Count the number of jobs moved</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log a warning for each job moved</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the count of requeued jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_once</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a single cleanup cycle. Returns number of jobs requeued.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_requeued </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stale_queues </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._get_stale_processing_queues()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> queue_key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> stale_queues:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._requeue_stale_jobs(queue_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            total_requeued </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> total_requeued</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the janitor's main loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logging.info(</span><span style=\"color:#9ECBFF\">\"Janitor started.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                requeued </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.run_once()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> requeued </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    logging.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Janitor requeued </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">requeued</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> stale jobs.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Janitor error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exc_info</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.scan_interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong><code>core/retry_manager.py</code> - Handling retry scheduling:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job, JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .redis_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> utils.backoff </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BackoffCalculator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_job_failure</span><span style=\"color:#E1E4E8\">(self, job: Job, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main entry point: decide retry or dead letter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Call job.record_error(error) to update error list and attempt count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call job.should_retry() to check if max_retries not exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If should_retry is True:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Calculate delay using self.backoff_calculator.calculate_delay(job.attempts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call self.schedule_retry(job, delay)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If should_retry is False:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call self.move_to_dead_letter(job)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Log a critical error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update the job's status in Redis to RETRY_SCHEDULED or DEAD_LETTER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> schedule_retry</span><span style=\"color:#E1E4E8\">(self, job: Job, delay_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Schedule a job for retry after a delay.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate the absolute retry timestamp: current_time + delay_seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use Redis ZADD to add job.job_id to sorted set key 'retry_queue:{job.queue}' with score = retry_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return the job_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_due_retries</span><span style=\"color:#E1E4E8\">(self, queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> List[Job]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get jobs whose retry time has arrived.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use ZRANGEBYSCORE with scores 0 to current_timestamp to get job_ids due for retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each job_id, fetch the full Job from Redis using QueueManager.get_job_by_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return the list of Job objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-python\">E. Language-Specific Hints (Python)</h4>\n<ul>\n<li><strong>Atomic Operations:</strong> Always use Redis pipelines (<code>self.redis.pipeline()</code>) for operations that must be atomic, like enqueuing a job (store + push) or moving a job between queues. Use <code>WATCH</code> for optimistic concurrency control when updating schedules.</li>\n<li><strong>Time and Timezones:</strong> Use <code>datetime.datetime.now(datetime.timezone.utc)</code> for all timestamps stored in Redis. For cron schedule evaluation with timezones, use the <code>pytz</code> or <code>zoneinfo</code> (Python 3.9+) libraries. Be mindful of ambiguous times during DST transitions.</li>\n<li><strong>Graceful Shutdown:</strong> Use <code>signal.signal(signal.SIGTERM, handler)</code> in your <code>Worker</code> and <code>Scheduler</code> main loops to catch termination signals. Set a flag (<code>_shutdown_requested</code>) and break out of the loop cleanly after finishing the current unit of work.</li>\n<li><strong>Connection Management:</strong> Use the provided <code>RedisClient</code> wrapper, which includes connection pooling and automatic retry on transient network errors. Configure <code>max_connections</code> in the pool to match your concurrency level.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint-for-interactions\">F. Milestone Checkpoint for Interactions</h4>\n<p>After implementing the core components (Milestones 1-4), you should be able to run the following integration test:</p>\n<ol>\n<li><strong>Start Infrastructure:</strong> Start Redis. Run one worker (<code>python scripts/run_worker.py --queues default</code>). Run the scheduler (<code>python scripts/run_scheduler.py</code>).</li>\n<li><strong>Test Happy Path:</strong> Use a Python shell to enqueue a simple job that prints a message. Verify the worker prints the message and the job status in Redis becomes <code>COMPLETED</code>.</li>\n<li><strong>Test Worker Crash Recovery:</strong> Enqueue a long-running job (e.g., <code>time.sleep(30)</code>). While it&#39;s running, kill the worker process (Ctrl+C or <code>kill -9</code>). Wait for the heartbeat TTL (e.g., 45 seconds). Start the worker again. Observe that the killed job is re-queued and eventually completes.</li>\n<li><strong>Test Retry Logic:</strong> Enqueue a job that fails with an exception and has <code>max_retries=3</code>. Watch the logs and Redis sorted set (<code>ZRANGE retry_queue:default 0 -1 WITHSCORES</code>). You should see the job ID scheduled, then retried, until it finally moves to the dead letter queue after 3 attempts.</li>\n<li><strong>Test Scheduled Job:</strong> Create a recurring schedule that runs every minute. Observe the scheduler logs and the main queue length. You should see a new job appear in the queue approximately every minute.</li>\n</ol>\n<p><strong>Signs of Success:</strong> Jobs complete reliably despite failures. The dashboard shows accurate queue depths, worker status, and job history. No jobs are lost when workers or the scheduler restart.</p>\n<p><strong>Common Debugging Issues:</strong></p>\n<ul>\n<li><strong>Jobs disappearing:</strong> Likely a bug in the reliable dequeue pattern (<code>BRPOPLPUSH</code>). Check that jobs are being moved to the processing queue and removed only after successful completion.</li>\n<li><strong>Retries not happening:</strong> Ensure the <code>RetryManager.get_due_retries</code> is being called periodically (e.g., by the worker before polling for new jobs) and that the retry timestamps in the sorted set are correct.</li>\n<li><strong>Scheduler not enqueuing jobs:</strong> Check the scheduler&#39;s polling loop logs. Verify cron expressions are valid and that <code>next_run_at</code> is being calculated correctly with the proper timezone.</li>\n</ul>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section spans all five milestones, addressing cross-cutting concerns of failure detection, recovery, and edge case management that are critical for building a production-ready distributed system.</p>\n</blockquote>\n<p>Distributed systems operate in an inherently unreliable environment where failures are not exceptional but expected. A background job processor must be designed with the fundamental assumption that network connections will drop, processes will crash, clocks will drift, and data will become corrupted. This section systematically catalogs the failure modes the system must handle, details how to detect them, outlines recovery strategies, and provides specific solutions for challenging edge cases. Unlike simpler applications where failures might be catastrophic, our design treats failures as normal operational states that trigger well-defined recovery paths, ensuring the system maintains job durability, processing guarantees, and operational visibility even under adverse conditions.</p>\n<h3 id=\"failure-modes-and-detection\">Failure Modes and Detection</h3>\n<blockquote>\n<p><strong>Mental Model: The Hospital Emergency Room Triage System</strong><br>Imagine an emergency room where patients arrive with various conditions. The triage nurse must quickly assess each patient&#39;s vital signs, classify the severity of their condition, and route them to the appropriate treatment area. Some issues are immediately life-threatening (cardiac arrest), some are urgent but not critical (broken bones), and some are minor (small cuts). Similarly, our system must continuously monitor its own &quot;vital signs&quot;—network connectivity, process health, data integrity—and classify failures by severity to apply the correct intervention. Detection mechanisms act as the triage nurses, constantly checking pulses and responding to alarms.</p>\n</blockquote>\n<p>The system must detect failures across multiple dimensions: infrastructure (Redis, network), processes (workers, scheduler), data (corruption, validation), and temporal (timeouts, clock skew). Each failure mode requires specific detection strategies that balance sensitivity (catching real failures) with specificity (avoiding false alarms). The following table categorizes the primary failure modes, their symptoms, and detection mechanisms:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Symptoms &amp; Indicators</th>\n<th>Detection Mechanism</th>\n<th>Severity Class</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis Connection Failure</strong></td>\n<td>TCP connection timeout, connection reset, &quot;Connection refused&quot; errors, inability to ping Redis</td>\n<td>Periodic health checks (e.g., <code>PING</code> command every 5 seconds), monitoring connection pool exhaustion, tracking consecutive command failures</td>\n<td>Critical (system cannot function without broker)</td>\n</tr>\n<tr>\n<td><strong>Network Partition</strong></td>\n<td>Workers can connect to Redis but cannot communicate with each other or external services; some commands succeed while others timeout</td>\n<td>Heartbeat cross-checks between workers, external service health checks integrated into job execution, monitoring for asymmetric failure patterns</td>\n<td>Critical (can lead to split-brain scenarios)</td>\n</tr>\n<tr>\n<td><strong>Worker Process Crash</strong></td>\n<td>Worker heartbeat stops updating, processing queue jobs remain stuck in <code>ACTIVE</code> state beyond timeout, OS process disappears</td>\n<td><code>WorkerHeartbeat</code> timestamp exceeds <code>heartbeat_interval * 3</code>, <code>Janitor</code> process scans for stale <code>processing</code> queues, monitoring process PID existence</td>\n<td>High (jobs become orphaned)</td>\n</tr>\n<tr>\n<td><strong>Worker Graceful Shutdown Stall</strong></td>\n<td>Worker receives SIGTERM but doesn&#39;t exit within graceful shutdown timeout, hangs during job cleanup</td>\n<td>Internal shutdown timer in <code>Worker.stop()</code> method, monitoring for <code>Worker.state</code> stuck in <code>SHUTTING_DOWN</code> beyond timeout</td>\n<td>Medium (blocks deployment rotations)</td>\n</tr>\n<tr>\n<td><strong>Job Handler Timeout</strong></td>\n<td>Job execution exceeds <code>Job.timeout_seconds</code>, worker becomes unresponsive to heartbeats during execution</td>\n<td>Per-job timeout thread/process monitoring, watchdog timer that sends SIGKILL to runaway job handlers</td>\n<td>Medium (resource leak, blocks queue)</td>\n</tr>\n<tr>\n<td><strong>Job Data Corruption</strong></td>\n<td><code>Job.deserialize()</code> raises decoding errors, missing required fields, invalid data types</td>\n<td>Validation in <code>QueueManager._validate_job()</code> during enqueue, checksum verification (CRC32) stored with serialized data, schema validation on deserialize</td>\n<td>High (job lost, cannot be processed)</td>\n</tr>\n<tr>\n<td><strong>Redis Memory Exhaustion</strong></td>\n<td>Redis returns OOM errors, <code>used_memory</code> approaches <code>maxmemory</code>, key evictions occur despite <code>noeviction</code> policy</td>\n<td>Monitoring Redis metrics via <code>INFO memory</code>, alerting on memory usage &gt;80%, tracking eviction count for <code>noeviction</code> policy</td>\n<td>Critical (data loss possible)</td>\n</tr>\n<tr>\n<td><strong>Clock Skew</strong></td>\n<td>Scheduled jobs fire at wrong times, retry delays miscalculated, heartbeat timestamps inconsistent across servers</td>\n<td>NTP monitoring, comparing system time to Redis server time (<code>TIME</code> command), detecting scheduler runs that are significantly early/late</td>\n<td>Medium (timing-sensitive operations fail)</td>\n</tr>\n<tr>\n<td><strong>Deadlock in Worker Concurrency</strong></td>\n<td>Worker&#39;s thread/process pool stops making progress, all workers idle but jobs remain in queue, CPU usage drops to zero</td>\n<td>Monitoring job completion rate dropping to zero while queue non-empty, watchdog thread that samples executor thread states</td>\n<td>High (silent failure, hard to detect)</td>\n</tr>\n<tr>\n<td><strong>Queue Overflow</strong></td>\n<td><code>QueueConfig.max_length</code> exceeded, <code>QueueFullError</code> exceptions, enqueue operations fail</td>\n<td>Monitoring queue lengths vs. configured limits, alerting on <code>QueueFullError</code> frequency, tracking producer backpressure</td>\n<td>Medium (job submission blocked)</td>\n</tr>\n<tr>\n<td><strong>Silent Data Corruption in Redis</strong></td>\n<td>Job data retrieved from Redis differs from what was stored, bit flips due to hardware issues</td>\n<td>Store CRC32 checksum with each serialized job, verify on retrieval, Redis <code>DEBUG DIGEST</code> command for entire dataset (expensive)</td>\n<td>Critical (data integrity compromised)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Multi-Layered Failure Detection</strong>\n<strong>Context</strong>: We need to detect failures promptly without overwhelming the system with monitoring overhead or generating excessive false positives. Different components have different failure characteristics requiring tailored detection approaches.\n<strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Centralized Health Service</strong>: A dedicated service periodically probes all components and aggregates status.</li>\n<li><strong>Decentralized Self-Reporting</strong>: Each component reports its own health via heartbeets and logs, with consumers interpreting.</li>\n<li><strong>Hybrid Approach</strong>: Components self-report basic health (heartbeats) while dedicated monitors perform deeper diagnostic checks.\n<strong>Decision</strong>: Hybrid approach with component heartbeats for liveness and dedicated <code>Janitor</code> and <code>MetricsAggregator</code> for deeper health checks.\n<strong>Rationale</strong>: Heartbeats provide cheap, continuous liveness signals with low latency. Dedicated diagnostic processes can perform more expensive checks (like stale job detection) without impacting primary data paths. This balances detection speed with system overhead.\n<strong>Consequences</strong>: Requires implementing both heartbeat mechanisms and background maintenance processes. Failure detection latency depends on configuration: heartbeat intervals for quick detection vs. janitor scan intervals for orphaned job cleanup.</li>\n</ol>\n</blockquote>\n<p>Detection mechanisms are implemented across multiple components:</p>\n<ul>\n<li><strong>Heartbeat System</strong>: <code>WorkerHeartbeat</code> updates a Redis key with current timestamp at <code>heartbeat_interval</code>. The dashboard and alerting systems monitor these keys for staleness (timestamp &gt; interval × 3).</li>\n<li><strong>Janitor Process</strong>: <code>Janitor</code> periodically scans all <code>processing</code> queues (e.g., <code>queue:processing:{worker_id}</code>) and checks if the associated worker&#39;s heartbeat is current. If not, it assumes the worker crashed and re-queues the jobs.</li>\n<li><strong>Metric-Based Alerts</strong>: <code>AlertManager</code> evaluates rules against metrics like queue growth rate, error rate, and worker count, firing alerts when thresholds are breached.</li>\n<li><strong>Timeout Watchdogs</strong>: Each <code>Worker</code> runs a watchdog timer per job that kills the job handler if it exceeds <code>Job.timeout_seconds</code>. The scheduler has its own timeout for the polling loop to detect stalls.</li>\n<li><strong>Connection Health Checks</strong>: <code>RedisClient</code> implements circuit breaker patterns, tracking consecutive failures and entering &quot;open&quot; state to prevent cascading failures during Redis outages.</li>\n</ul>\n<h3 id=\"recovery-and-compensation-strategies\">Recovery and Compensation Strategies</h3>\n<blockquote>\n<p><strong>Mental Model: The Automobile Insurance Claims Process</strong><br>When an accident occurs, insurance companies don&#39;t just fix the car—they follow a structured process: assess damage, determine fault, arrange repairs, provide rental cars, and handle medical bills. Each type of damage has a corresponding recovery procedure. Similarly, our system must have specific recovery strategies for each failure mode, not just restarting components but compensating for lost work, preserving data integrity, and restoring normal operation with minimal manual intervention.</p>\n</blockquote>\n<p>Once a failure is detected, the system must execute appropriate recovery actions. The strategy depends on the failure type, component affected, and current system state. Recovery aims to restore system functionality while preserving job durability guarantees (at-least-once or at-most-once semantics) and maintaining data consistency. The following table maps failure modes to recovery strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Immediate Recovery Action</th>\n<th>Compensation Strategy</th>\n<th>Data Consistency Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis Connection Failure</strong></td>\n<td>Exponential backoff reconnect, fail-fast on enqueue operations with clear error to producers</td>\n<td>Buffer jobs in memory (with size limit) for later retry, enter degraded mode (pause scheduling, slow heartbeats)</td>\n<td>Jobs may be lost if memory buffer overflows before Redis recovery. At-least-once semantics compromised during outage.</td>\n</tr>\n<tr>\n<td><strong>Worker Process Crash</strong></td>\n<td><code>Janitor</code> detects stale <code>processing</code> queue and moves jobs back to main queue with <code>Job.attempts</code> incremented</td>\n<td>Worker supervisor (systemd, kubernetes) restarts worker process, <code>WorkerHeartbeat</code> cleanup removes orphaned heartbeat key</td>\n<td>Job may be partially executed (non-idempotent operations require careful design). At-least-once semantics preserved.</td>\n</tr>\n<tr>\n<td><strong>Job Handler Timeout</strong></td>\n<td>Watchdog sends SIGKILL to job handler process/thread, job marked as <code>FAILED</code> and passed to <code>RetryManager</code></td>\n<td>Retry with exponential backoff (if job idempotent), alert for manual inspection if timeout persistent for job type</td>\n<td>If job not idempotent, duplicate side effects possible on retry. System maintains forward progress.</td>\n</tr>\n<tr>\n<td><strong>Job Data Corruption</strong></td>\n<td>Job moved to dead letter queue with <code>VALIDATION_ERROR</code> flag, error details stored</td>\n<td>Notification to operators via alert, manual inspection required to fix or discard job</td>\n<td>Single job lost, but system continues processing other jobs. Dead letter queue provides audit trail.</td>\n</tr>\n<tr>\n<td><strong>Redis Memory Exhaustion</strong></td>\n<td>Switch to <code>allkeys-lru</code> eviction policy temporarily, emergency purge of old job history, alert operators</td>\n<td>Scale Redis memory, restart Redis with larger <code>maxmemory</code>, drain queues to secondary storage</td>\n<td>Possible job loss if eviction occurs. System prioritizes recent jobs over history.</td>\n</tr>\n<tr>\n<td><strong>Clock Skew</strong></td>\n<td>Alert operators to fix NTP synchronization, scheduler uses Redis time (<code>TIME</code> command) as authoritative source</td>\n<td>Scheduler runs catch-up logic for missed scheduled jobs when clock corrects, recalculates retry timestamps</td>\n<td>Scheduled jobs may fire late or early. Recovery ensures all jobs eventually enqueued.</td>\n</tr>\n<tr>\n<td><strong>Deadlock in Worker Concurrency</strong></td>\n<td>Watchdog thread detects stall and kills entire worker process, triggering worker crash recovery</td>\n<td>Reduce concurrency level, analyze job handlers for locking issues, implement thread dumps for diagnosis</td>\n<td>All jobs currently processing are failed and retried. Temporary throughput reduction.</td>\n</tr>\n<tr>\n<td><strong>Queue Overflow</strong></td>\n<td>Reject new jobs with <code>QueueFullError</code>, trigger scale-up alerts for workers, increase <code>max_length</code> temporarily</td>\n<td>Auto-scale worker capacity, implement backpressure to producers, add overflow queue (secondary Redis instance)</td>\n<td>Producers must handle rejection gracefully. System protects itself from overload.</td>\n</tr>\n<tr>\n<td><strong>Network Partition</strong></td>\n<td>Continue processing jobs that don&#39;t require external services, pause enqueue of jobs requiring partitioned resources</td>\n<td>When partition heals, reconciliation process compares job states across partitions, manual intervention may be needed</td>\n<td>Split-brain may cause duplicate processing. Idempotency keys and deterministic job IDs reduce risk.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Principle: Graceful Degradation Over Catastrophic Failure</strong><br>When components fail, the system should degrade functionality predictably rather than collapsing entirely. For example, during Redis outages, workers can continue processing jobs already in memory while the queue manager buffers new jobs locally (up to a limit). The dashboard can display cached metrics when Redis is unavailable. This principle ensures the system remains partially operational during partial failures.</p>\n</blockquote>\n<p>Recovery strategies are implemented through coordinated actions across components:</p>\n<ol>\n<li><p><strong>Automatic Retry with Exponential Backoff</strong>: For transient failures (network timeouts, temporary resource unavailability), the <code>RetryManager</code> schedules retries with increasing delays using <code>BackoffCalculator.calculate_delay()</code>. This prevents overwhelming recovering systems with immediate retry storms.</p>\n</li>\n<li><p><strong>Orphaned Job Recovery via Janitor</strong>: The <code>Janitor</code> process runs periodically (e.g., every 30 seconds) and executes:</p>\n<ul>\n<li><code>Janitor._get_stale_processing_queues()</code>: Scans Redis for all <code>queue:processing:{worker_id}</code> keys</li>\n<li>Checks if the corresponding worker heartbeat key (<code>worker:heartbeat:{worker_id}</code>) has expired</li>\n<li>For each stale processing queue, calls <code>Janitor._requeue_stale_jobs()</code> to:<ol>\n<li>Atomically move all jobs from the processing queue back to the main queue using <code>RPOPLPUSH</code> in a pipeline</li>\n<li>Increment the <code>Job.attempts</code> count for each job</li>\n<li>Update <code>Job.status</code> from <code>ACTIVE</code> to <code>PENDING</code></li>\n<li>Log the recovery event for auditing</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p><strong>Circuit Breaker for External Dependencies</strong>: The <code>RedisClient</code> implements a circuit breaker pattern that tracks consecutive connection failures. After a threshold (e.g., 5 failures), the circuit opens and subsequent commands immediately fail fast without attempting network calls. After a reset timeout (e.g., 30 seconds), the circuit enters half-open state, allowing a test command through; if successful, the circuit closes and normal operation resumes.</p>\n</li>\n<li><p><strong>Catch-Up Logic for Scheduler</strong>: When the <code>Scheduler</code> starts or detects significant clock corrections, it calls <code>Scheduler.calculate_next_runs_for_all_schedules()</code> to recalculate <code>Schedule.next_run_at</code> for all schedules. It then runs <code>Scheduler.enqueue_due_jobs()</code> with an expanded time window (e.g., previous 24 hours) to enqueue any jobs missed during downtime. The uniqueness window (<code>Schedule.unique_window_seconds</code>) prevents duplicate enqueues for the same schedule period.</p>\n</li>\n<li><p><strong>Dead Letter Queue Manual Intervention</strong>: Jobs that exhaust retries or fail validation are moved to the dead letter queue (<code>dead:letter:{queue_name}</code>) where they persist for manual inspection. The dashboard provides operations to:</p>\n<ul>\n<li>View error details and stack traces</li>\n<li>Retry the job (moves back to main queue with reset attempts)</li>\n<li>Delete the job permanently</li>\n<li>Download job payload for debugging</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"specific-edge-cases-and-solutions\">Specific Edge Cases and Solutions</h3>\n<blockquote>\n<p><strong>Mental Model: The &quot;Corner Cases&quot; Playbook in Sports</strong><br>Professional sports teams prepare for unusual game situations: what if the ball hits a bird mid-flight? What if a player&#39;s jersey gets torn? They have predetermined rules and procedures. Similarly, our system must anticipate rare but possible edge cases and have explicit solutions codified rather than leaving behavior undefined.</p>\n</blockquote>\n<p>Edge cases are scenarios that occur infrequently but can cause system instability, data loss, or undefined behavior if not handled explicitly. These often involve timing issues, race conditions, or unexpected combinations of normal events. The following table documents specific edge cases and their solutions:</p>\n<table>\n<thead>\n<tr>\n<th>Edge Case Scenario</th>\n<th>Description</th>\n<th>Potential Impact</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Worker Crash During Job Acknowledgement</strong></td>\n<td>Worker successfully processes job but crashes after business logic completes but before removing job from processing queue.</td>\n<td>Job could be re-queued by <code>Janitor</code> and executed twice (duplicate processing).</td>\n<td>Use atomic &quot;process and acknowledge&quot; operations: store job result in Redis atomically with removal from processing queue via <code>RedisClient.pipeline()</code> multi-exec.</td>\n</tr>\n<tr>\n<td><strong>Clock Jump Forward/Backward</strong></td>\n<td>System time changes abruptly due to NTP correction or manual clock adjustment.</td>\n<td>Scheduled jobs may fire at wrong times, retry delays miscalculated, heartbeat comparisons invalid.</td>\n<td>Use Redis <code>TIME</code> command as monotonic clock source for all scheduling decisions. For heartbeats, use system uptime (<code>time.monotonic()</code> in Python) for intervals, wall clock for absolute times.</td>\n</tr>\n<tr>\n<td><strong>Simultaneous Worker Startup with Same ID</strong></td>\n<td>Two workers accidentally configured with same <code>worker_id</code> (e.g., duplicated container image).</td>\n<td>Both update same heartbeat key, making liveness detection unreliable; both might process same jobs.</td>\n<td>Generate unique <code>worker_id</code> automatically: <code>hostname:pid:timestamp</code> (e.g., <code>web01:12345:1678901234</code>). Include random component in heartbeat key to detect collisions.</td>\n</tr>\n<tr>\n<td><strong>Job Payload Contains Non-Serializable Objects</strong></td>\n<td>Producer passes a database connection, file handle, or lambda function in job arguments.</td>\n<td><code>Job.serialize()</code> fails or produces corrupted serialization that cannot be deserialized later.</td>\n<td><code>QueueManager._validate_job()</code> performs deep validation using <code>json.dumps()</code> test before enqueue. Reject with <code>ValidationError</code> describing the problematic field.</td>\n</tr>\n<tr>\n<td><strong>Redis Failover During Transaction</strong></td>\n<td>Redis primary fails after a multi-exec transaction starts but before it completes.</td>\n<td>Partial transaction applied, leaving data inconsistent (e.g., job enqueued but job metadata not stored).</td>\n<td>Use Redis Cluster with proper failover configuration (minimizes window). Implement idempotent operations where possible. Add reconciliation checks that scan for inconsistencies.</td>\n</tr>\n<tr>\n<td><strong>Queue Priority Starvation</strong></td>\n<td>High-priority queue constantly has jobs, preventing lower-priority queues from ever being polled.</td>\n<td>Low-priority jobs never get processed, effectively creating a denial of service for certain job types.</td>\n<td>Implement weighted round-robin with dynamic adjustment: track time since last poll per queue and temporarily boost priority of starved queues.</td>\n</tr>\n<tr>\n<td><strong>Zombie Jobs in Processing Queue</strong></td>\n<td>Job marked as <code>ACTIVE</code> but no worker is processing it (worker died without cleanup).</td>\n<td>Job stuck forever, consuming memory, not retried.</td>\n<td><code>Janitor</code> scans all processing queues, not just those with heartbeat keys. Any job in processing queue for longer than <code>job_timeout * 2</code> is automatically re-queued.</td>\n</tr>\n<tr>\n<td><strong>Cron Expression Matching Daylight Saving Transition</strong></td>\n<td>Cron schedule like &quot;30 2 * * *&quot; (2:30 AM daily) on a day when clocks spring forward from 2:00 to 3:00.</td>\n<td>Job may be skipped (no 2:30 AM exists) or run twice (when falling back).</td>\n<td>Use UTC for all cron evaluations, or use timezone-aware scheduling with <code>pytz</code> library that handles DST transitions correctly (skips or doubles as appropriate).</td>\n</tr>\n<tr>\n<td><strong>Retry Storm After Service Recovery</strong></td>\n<td>External service is down, causing many jobs to fail and schedule retries. When service recovers, all retries fire simultaneously.</td>\n<td>Thundering herd overwhelms recovering service, causing it to fail again.</td>\n<td>Add jitter to retry delays (<code>BackoffCalculator</code> with <code>jitter_factor</code>). Implement congestion control: gradually increase retry concurrency after service recovery detected.</td>\n</tr>\n<tr>\n<td><strong>Memory Bloat from Job History</strong></td>\n<td>Job history and metrics accumulate indefinitely in Redis, consuming all memory.</td>\n<td>Redis crashes or evicts current job data, causing system failure.</td>\n<td>Implement TTL on all job metadata keys (e.g., 7 days). <code>Janitor</code> process periodically purges old records. Move old data to cold storage (S3, database) before deletion.</td>\n</tr>\n<tr>\n<td><strong>Two Workers Simultaneously Poll Same Queue</strong></td>\n<td>Both workers call <code>BRPOPLPUSH</code> at same instant, both get different jobs (normal). But race condition could cause both to get same job in some edge cases.</td>\n<td>Same job processed twice.</td>\n<td>Redis <code>BRPOPLPUSH</code> is atomic by design, but to be extra safe, use <code>RPOPLPUSH</code> with transaction and check for duplicate job IDs in processing queue before execution.</td>\n</tr>\n<tr>\n<td><strong>Job Depends on Deleted External Resource</strong></td>\n<td>Job references a database record ID that gets deleted before job executes.</td>\n<td>Job fails with &quot;record not found&quot; error, retries won&#39;t help.</td>\n<td>Implement early validation in job handler: check resource existence, if missing, mark job as <code>COMPLETED</code> with warning (or move to dead letter with <code>SKIPPED</code> reason).</td>\n</tr>\n<tr>\n<td><strong>Scheduler Process Deadlock</strong></td>\n<td>Scheduler holds Redis connection while waiting for queue manager, queue manager waiting for scheduler (circular dependency).</td>\n<td>All scheduled jobs stop being enqueued, but system appears healthy (no crashes).</td>\n<td>Use connection pooling with timeout, implement watchdog thread that monitors scheduler loop progress, break circular dependencies with async patterns.</td>\n</tr>\n<tr>\n<td><strong>ULID Collision</strong></td>\n<td>Extremely rare case where two jobs generate same ULID (1 in 2^128 probability).</td>\n<td>One job overwrites another in Redis storage, causing data loss.</td>\n<td>Fall back to check-and-regenerate: after generating ULID, check <code>EXISTS</code> in Redis, if exists, append random suffix. Log collision as critical event.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight: Edge Cases Are Inevitable, Not Theoretical</strong><br>In distributed systems running at scale, statistically improbable events occur regularly. The &quot;one in a million&quot; chance happens daily when processing millions of jobs. Therefore, every edge case in this table must be treated as a mandatory requirement, not an optional enhancement.</p>\n</blockquote>\n<p>Several edge cases require particular attention in implementation:</p>\n<p><strong>Clock Skew and Time Handling</strong>: The system uses a hybrid time strategy:</p>\n<ul>\n<li><strong>Redis as Authoritative Clock</strong>: For all scheduled job times and retry timestamps, use <code>RedisClient.execute(&#39;TIME&#39;)</code> which returns server time unaffected by client clock skew.</li>\n<li><strong>Monotonic Clocks for Intervals</strong>: For heartbeat intervals and job timeouts, use <code>time.monotonic()</code> (Python) or <code>process.hrtime()</code> (Node.js) which are unaffected by system time adjustments.</li>\n<li><strong>Timezone-Aware Scheduling</strong>: Store <code>Schedule.timezone</code> as IANA timezone string (e.g., &quot;America/New_York&quot;) and use <code>pytz</code> or equivalent to convert UTC to local time for cron evaluation, handling DST transitions correctly.</li>\n</ul>\n<p><strong>Idempotency and Duplicate Processing</strong>: To handle potential duplicate job execution (from retries, worker crashes, or network partitions):</p>\n<ul>\n<li><strong>Idempotency Keys</strong>: Allow producers to pass an <code>idempotency_key</code> in <code>Job.metadata</code>. Store this in Redis with TTL (e.g., 24 hours). Before processing job, check if key exists; if yes, skip execution and return previous result.</li>\n<li><strong>Deterministic Job IDs</strong>: For recurring jobs, generate job ID from schedule ID plus execution timestamp (e.g., <code>{schedule_id}:{yyyymmddhhmm}</code>) to prevent duplicate enqueues for same schedule period.</li>\n<li><strong>Transactional State Updates</strong>: Use Redis pipelines to atomically update job status, store results, and remove from processing queue in a single operation.</li>\n</ul>\n<p><strong>Memory Management and Cleanup</strong>: To prevent Redis memory exhaustion:</p>\n<ul>\n<li><strong>TTL on All Keys</strong>: Set appropriate TTLs: job metadata (7 days), worker heartbeats (2×heartbeat_interval), metrics (30 days), processing queues (job_timeout×3).</li>\n<li><strong>Janitor Cleanup</strong>: <code>Janitor</code> also removes expired keys beyond their TTL using <code>SCAN</code> and <code>DEL</code> operations in batches to avoid blocking Redis.</li>\n<li><strong>Job History Archival</strong>: For jobs older than TTL, move to cold storage (compress and store in S3) before deletion, maintaining audit trail without bloating Redis.</li>\n</ul>\n<p><strong>Network Partition Handling (Split-Brain)</strong>: In a network partition where workers can&#39;t reach Redis but can reach each other:</p>\n<ul>\n<li><strong>Continue Processing Local Jobs</strong>: Workers can complete jobs already fetched into memory, storing results locally.</li>\n<li><strong>Buffer Results for Reconciliation</strong>: When partition heals, workers push buffered results to Redis with conflict resolution (last-write-wins with timestamp).</li>\n<li><strong>Fencing Tokens</strong>: Use Redis <code>INCR</code> on a fencing token key to detect stale workers after partition; workers with lower token values are considered stale and stop processing.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations for Error Handling</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Circuit Breaker</td>\n<td>Manual failure counting with simple timeout</td>\n<td><code>tenacity</code> library with retry patterns or <code>pybreaker</code> for state machine implementation</td>\n</tr>\n<tr>\n<td>Connection Pooling</td>\n<td><code>redis-py</code> default connection pool</td>\n<td><code>redis-py</code> with <code>ConnectionPool</code> and <code>HealthCheck</code> enabled, custom retry logic</td>\n</tr>\n<tr>\n<td>Watchdog Timer</td>\n<td><code>threading.Timer</code> per job execution</td>\n<td><code>concurrent.futures</code> with <code>as_completed</code> and timeout, process-based isolation with <code>multiprocessing</code></td>\n</tr>\n<tr>\n<td>Time Handling</td>\n<td><code>datetime.utcnow()</code> for timestamps, <code>time.monotonic()</code> for intervals</td>\n<td><code>pendulum</code> or <code>arrow</code> for timezone-aware operations, Redis <code>TIME</code> command for authoritative time</td>\n</tr>\n<tr>\n<td>Health Checks</td>\n<td>Simple endpoint returning 200 OK</td>\n<td>Comprehensive health check verifying Redis connectivity, queue depths, worker status</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background_job_processor/\n├── src/\n│   ├── job_processor/\n│   │   ├── __init__.py\n│   │   ├── exceptions.py           # Custom exceptions: ValidationError, QueueFullError, etc.\n│   │   ├── circuit_breaker.py      # Circuit breaker pattern implementation\n│   │   ├── janitor.py              # Janitor process for cleanup\n│   │   ├── time_utils.py           # Time handling utilities, monotonic clocks\n│   │   └── reconciliation.py       # Reconciliation logic for network partitions\n│   ├── workers/\n│   │   ├── __init__.py\n│   │   ├── watchdog.py             # Job timeout watchdog\n│   │   └── heartbeat.py            # WorkerHeartbeat implementation\n│   └── redis/\n│       ├── __init__.py\n│       ├── client.py               # RedisClient with circuit breaker\n│       └── health.py               # Redis health checks\n└── tests/\n    └── test_error_handling.py      # Tests for failure scenarios</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/job_processor/exceptions.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Custom exceptions for the job processor.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobProcessorError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all job processor errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">JobProcessorError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when job validation fails.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, field: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.field </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (field: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">field</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> field </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> message)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueFullError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">JobProcessorError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when queue is at maximum capacity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Queue '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' is full (max: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">max_length</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WorkerTimeoutError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">JobProcessorError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when a worker operation times out.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConnectionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">JobProcessorError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when Redis connection fails.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># src/job_processor/circuit_breaker.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Circuit breaker pattern implementation.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CLOSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"closed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"open\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HALF_OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"half_open\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Circuit breaker that tracks failures and opens circuit after threshold.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        failure_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recovery_timeout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_exceptions: </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">,)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> failure_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_timeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> recovery_timeout</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected_exceptions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_exceptions</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_state_change </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.monotonic()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> call</span><span style=\"color:#E1E4E8\">(self, func: Callable, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute function with circuit breaker protection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check if recovery timeout has elapsed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> time.monotonic() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_state_change </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.recovery_timeout:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Circuit </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> entering HALF_OPEN state\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.last_state_change </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.monotonic()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#E1E4E8\"> RedisConnectionError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Circuit </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> is OPEN\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> func(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Success - reset failure count if we were in HALF_OPEN</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Circuit </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> reset to CLOSED after successful call\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.expected_exceptions </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._record_failure()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _record_failure</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record a failure and update circuit state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.monotonic()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.failure_threshold):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Circuit </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> opening after </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.failure_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> failures\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.last_state_change </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.monotonic()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Circuit </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> re-opening after HALF_OPEN failure\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.last_state_change </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.monotonic()</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/job_processor/janitor.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Janitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Maintenance process that cleans up stale jobs and expired data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: RedisClient, scan_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scan_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scan_interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_stale_processing_queues</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Find all queue:processing lists where the associated worker is dead.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of Redis keys for stale processing queues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Use SCAN to find all keys matching pattern 'queue:processing:*'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: For each key, extract worker_id from the key name</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Check if worker heartbeat key exists and is recent</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: If heartbeat missing or stale, add processing queue key to results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Also check for jobs stuck in processing queue longer than 2×timeout</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _requeue_stale_jobs</span><span style=\"color:#E1E4E8\">(self, processing_queue_key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Move all jobs from a stale processing queue back to its main queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            processing_queue_key: Redis key of the stale processing queue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of jobs requeued.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Parse main queue name from processing_queue_key pattern</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Use Redis pipeline to atomically:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            a. Get all jobs from processing queue (LRANGE)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            b. Delete processing queue (DEL)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            c. For each job:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - Deserialize job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - Increment job.attempts</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - Update job.status to PENDING</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - Re-serialize job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - LPUSH to main queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Log requeue operation with job count and worker ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Return number of jobs requeued</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _cleanup_expired_keys</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Remove expired keys from Redis to prevent memory bloat.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of keys deleted.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Scan for job metadata keys (pattern 'job:*:metadata')</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Check TTL for each key, if TTL &#x3C; 0 (expired), add to delete list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Scan for old heartbeat keys (pattern 'worker:heartbeat:*')</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Check last update time, if older than 3×interval, add to delete list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Batch delete keys in groups of 100 to avoid blocking Redis</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_once</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a single cleanup cycle.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Total number of jobs requeued plus keys deleted.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Call _get_stale_processing_queues() to get stale queues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: For each stale queue, call _requeue_stale_jobs()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Call _cleanup_expired_keys()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Sum and return total operations performed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the janitor's main loop in a background thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set self.running = True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create threading.Thread target=_run_loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start thread, store in self._thread</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log janitor startup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop the janitor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set self.running = False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If thread exists, wait for it to join with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Log janitor shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _run_loop</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main loop that runs cleanup periodically.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: While self.running is True:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call self.run_once()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Sleep for self.scan_interval seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Catch and log exceptions (don't let loop die on error)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># src/redis/client.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Redis client wrapper with circuit breaker and error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute</span><span style=\"color:#E1E4E8\">(self, command: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a Redis command with error handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Get circuit breaker for this Redis instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Wrap _execute_command with circuit_breaker.call()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Handle specific Redis errors:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - ConnectionError: increment circuit breaker, raise RedisConnectionError</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - TimeoutError: increment circuit breaker, retry with backoff</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - ResponseError: log and raise as-is (application error)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: On success, return result</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'RedisPipeline'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Return a Redis pipeline for atomic operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Create pipeline from underlying redis client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Wrap pipeline execution with circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Implement context manager support (with statement)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Ensure pipeline executes atomically even with failures</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ol>\n<li><p><strong>Python&#39;s <code>time.monotonic()</code></strong> is crucial for intervals unaffected by system clock changes. Use it for heartbeat intervals and job timeouts.</p>\n</li>\n<li><p><strong>Use <code>contextlib</code> context managers</strong> for resource cleanup (Redis connections, file handles) to ensure resources are released even during exceptions.</p>\n</li>\n<li><p><strong><code>tenacity</code> library</strong> provides advanced retry patterns with exponential backoff, jitter, and conditional retrying that can replace manual retry logic.</p>\n</li>\n<li><p><strong><code>pytz</code> or <code>zoneinfo</code> (Python 3.9+)</strong> for timezone-aware scheduling. Always convert to UTC before storing in Redis.</p>\n</li>\n<li><p><strong><code>asyncio</code> timeout context</strong> for implementing watchdogs: <code>async with asyncio.timeout(seconds): await job_handler()</code>.</p>\n</li>\n<li><p><strong>Redis <code>SCRIPT EXISTS</code> and <code>EVALSHA</code></strong> for executing Lua scripts atomically, useful for complex recovery operations.</p>\n</li>\n</ol>\n<p><strong>F. Milestone Checkpoint for Error Handling</strong></p>\n<p>After implementing error handling mechanisms, verify the system behaves correctly under failure conditions:</p>\n<ol>\n<li><strong>Test Redis Connection Failure</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Start system normally, then stop Redis</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   $</span><span style=\"color:#9ECBFF\"> sudo</span><span style=\"color:#9ECBFF\"> systemctl</span><span style=\"color:#9ECBFF\"> stop</span><span style=\"color:#9ECBFF\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Observe circuit breaker opening after 5 failed commands</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Check logs for \"Circuit redis is OPEN\" messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Restart Redis and verify system automatically recovers</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Test Worker Crash Recovery</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Start worker processing jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Send SIGKILL to worker process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Wait 30 seconds for janitor scan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Verify orphaned jobs are re-queued with incremented attempt count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Check dashboard shows job as \"recovered\" not \"lost\"</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Test Clock Skew Handling</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Set system clock forward 1 hour</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   $</span><span style=\"color:#9ECBFF\"> sudo</span><span style=\"color:#9ECBFF\"> date</span><span style=\"color:#79B8FF\"> -s</span><span style=\"color:#9ECBFF\"> \"+1 hour\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Observe scheduler logs using Redis TIME command, not system time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Verify scheduled jobs still fire at correct UTC time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Reset clock and verify catch-up logic enqueues missed jobs</span></span></code></pre></div>\n\n<p><strong>G. Debugging Tips for Error Scenarios</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Jobs disappearing from queue</td>\n<td>Worker crash during processing, janitor not running</td>\n<td>Check <code>processing</code> queues in Redis (<code>KEYS queue:processing:*</code>). Look for stuck jobs. Verify janitor process is running and scanning.</td>\n<td>Start janitor, manually move jobs back with <code>RPOPLPUSH</code>.</td>\n</tr>\n<tr>\n<td>All workers stuck idle</td>\n<td>Deadlock in concurrency pool, all threads blocked</td>\n<td>Check worker logs for timeout messages. Use thread dump (<code>import sys; sys._current_frames()</code>). Monitor CPU usage.</td>\n<td>Reduce concurrency, implement watchdog that kills stuck workers, review job handlers for locking issues.</td>\n</tr>\n<tr>\n<td>Scheduled jobs never fire</td>\n<td>Scheduler process dead, cron expression error, timezone mismatch</td>\n<td>Check scheduler logs for errors. Verify <code>Schedule.next_run_at</code> in Redis. Test cron expression with online validator.</td>\n<td>Restart scheduler, fix cron expression, ensure timezone configuration matches schedule.</td>\n</tr>\n<tr>\n<td>Redis memory constantly increasing</td>\n<td>No TTL on keys, job history not cleaned, memory leak</td>\n<td>Run <code>redis-cli info memory</code>. Check key TTLs with <code>TTL keyname</code>. Look for keys without expiration.</td>\n<td>Implement TTL on all job keys, run janitor cleanup, increase Redis memory or implement LRU eviction.</td>\n</tr>\n<tr>\n<td>Duplicate job execution</td>\n<td>Worker crash after processing but before acknowledgment, network partition</td>\n<td>Check job attempt count in metadata. Look for same job ID in both results and queue. Check for split-brain scenario logs.</td>\n<td>Implement atomic processing+acknowledgment, use idempotency keys, ensure proper network partition handling.</td>\n</tr>\n<tr>\n<td>Exponential backoff not working</td>\n<td>Retry delay calculation error, jitter factor too large</td>\n<td>Check <code>RetryManager</code> logs for scheduled retry times. Verify <code>BackoffCalculator.calculate_delay()</code> output.</td>\n<td>Debug backoff calculation, ensure retry jobs are being enqueued to correct sorted set with proper score.</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section spans all five milestones, providing a comprehensive verification approach that ensures each component works correctly in isolation and as part of the integrated system.</p>\n</blockquote>\n<p>A robust testing strategy is the linchpin of reliable distributed systems. Unlike monolithic applications where failures manifest predictably, distributed job processing systems exhibit emergent behaviors: race conditions surface only under specific timing, retry mechanisms can amplify errors exponentially, and resource leaks accumulate invisibly until a catastrophic failure. This section establishes a systematic approach to verification that moves beyond naive unit testing to encompass stateful integration testing, fault injection, and real-world scenario simulation.</p>\n<h3 id=\"testing-philosophy-and-approach\">Testing Philosophy and Approach</h3>\n<p><strong>Mental Model: The Building Inspector&#39;s Checklist</strong> — Think of testing this system as inspecting a newly constructed building. Unit tests are like checking individual materials (bricks, beams, wiring) for quality. Integration tests verify that systems work together (plumbing connects to fixtures, electrical circuits power lights). End-to-end tests simulate actual occupancy (people living in the building, using all systems simultaneously). Stress tests are like inviting a thousand people to a party to see if the floor holds. Each inspection type catches different classes of defects that would otherwise remain hidden until catastrophic failure.</p>\n<p>Our testing philosophy embraces three core principles:</p>\n<ol>\n<li><p><strong>Determinism in a Non-Deterministic World</strong>: Distributed systems are inherently non-deterministic due to timing variations, network delays, and concurrent operations. Our tests must either eliminate non-determinism through careful isolation or embrace it through statistical verification of probabilistic guarantees.</p>\n</li>\n<li><p><strong>Stateful Verification Over Mock-Heavy Isolation</strong>: While mocking external dependencies (Redis) simplifies unit tests, it creates false confidence. Redis operations have subtle atomicity and persistence characteristics that mocks often overlook. We favor integration tests with a real Redis instance in a controlled environment, supplemented by unit tests for pure business logic.</p>\n</li>\n<li><p><strong>Fault Injection as First-Class Testing</strong>: The system&#39;s reliability emerges from how it handles failures, not how it behaves in perfect conditions. We systematically inject failures at component boundaries (network timeouts, Redis unavailability, process crashes) to verify recovery mechanisms work as designed.</p>\n</li>\n</ol>\n<p>The testing pyramid for this system has a unique shape due to its distributed nature:</p>\n<p><img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fsystem-component.svg\" alt=\"Testing Pyramid\"></p>\n<p><strong>Testing Layers and Their Purposes</strong></p>\n<table>\n<thead>\n<tr>\n<th>Layer</th>\n<th>Scope</th>\n<th>Key Techniques</th>\n<th>Test Environment</th>\n<th>Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Unit Tests</strong></td>\n<td>Individual functions, classes, algorithms</td>\n<td>Mock external dependencies, parameterized tests, property-based testing</td>\n<td>Local development machine, CI pipeline</td>\n<td>On every change</td>\n</tr>\n<tr>\n<td><strong>Integration Tests</strong></td>\n<td>Interactions between 2-3 components (e.g., QueueManager + Redis)</td>\n<td>Real Redis instance, containerized dependencies, transactional rollback</td>\n<td>CI pipeline with Docker Compose</td>\n<td>On every change</td>\n</tr>\n<tr>\n<td><strong>End-to-End Tests</strong></td>\n<td>Full system workflows across all components</td>\n<td>Multiple processes, real network communication, failure injection</td>\n<td>Dedicated test environment</td>\n<td>Daily or per milestone</td>\n</tr>\n<tr>\n<td><strong>Chaos Tests</strong></td>\n<td>System behavior under injected failures</td>\n<td>Network partitioning, process killing, Redis failure simulation</td>\n<td>Isolated test cluster</td>\n<td>Weekly or major releases</td>\n</tr>\n<tr>\n<td><strong>Performance Tests</strong></td>\n<td>System behavior under load</td>\n<td>Load generation, metric collection, scalability verification</td>\n<td>Production-like environment</td>\n<td>Monthly or architectural changes</td>\n</tr>\n</tbody></table>\n<p><strong>Test Environment Architecture</strong></p>\n<p>To support this multi-layered approach, we establish a standardized test environment with the following characteristics:</p>\n<ol>\n<li><p><strong>Redis Isolation</strong>: Each test class spins up a dedicated Redis instance on a unique port using Docker or Redis&#39;s built-in <code>--port</code> option. This prevents test interference and allows parallel test execution.</p>\n</li>\n<li><p><strong>Transactional Rollback</strong>: Integration tests wrap operations in Redis transactions or use the <code>FLUSHDB</code> command between tests to ensure clean state. For end-to-end tests, we use separate Redis databases selected via the <code>SELECT</code> command.</p>\n</li>\n<li><p><strong>Time Control</strong>: Scheduling and retry tests require precise control over time. We implement a mockable clock abstraction that allows tests to simulate time jumps, freeze time at specific moments, or accelerate time for testing long delays.</p>\n</li>\n<li><p><strong>Process Management</strong>: End-to-end tests that involve multiple worker processes use Python&#39;s <code>multiprocessing</code> module with careful cleanup to prevent orphaned processes. Each test starts fresh worker processes and verifies they terminate correctly.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Key Insight</strong>: The most valuable tests for distributed systems are <strong>integration tests that verify distributed invariants</strong> — properties that must hold true across multiple components despite concurrent operations. Examples include &quot;every job enqueued is eventually processed exactly once&quot; or &quot;the sum of jobs across all queues equals the total jobs in the system.&quot;</p>\n</blockquote>\n<p><strong>ADR: Testing Strategy for Redis Dependencies</strong></p>\n<blockquote>\n<p><strong>Decision: Real Redis in Integration Tests with Clean State Isolation</strong></p>\n<ul>\n<li><strong>Context</strong>: Redis operations have subtle behaviors (atomicity, expiration, transaction semantics) that are difficult to mock accurately. Mock-based tests provide false confidence, while requiring a Redis server adds complexity to the test environment.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Pure Mocking</strong>: Mock all Redis calls with an in-memory simulation</li>\n<li><strong>Fake Redis Server</strong>: Use a lightweight Redis-compatible server like <code>fakeredis</code></li>\n<li><strong>Real Redis with Isolation</strong>: Run actual Redis server with isolated databases/ports</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use real Redis server with isolated state for integration tests, supplemented by mocking for unit tests of non-Redis logic.</li>\n<li><strong>Rationale</strong>: Only real Redis guarantees correct behavior of atomic operations, Lua scripting, and persistence semantics. The risk of mock inaccuracy outweighs the setup complexity, especially with containerization making Redis trivial to run. We mitigate complexity through reusable test fixtures.</li>\n<li><strong>Consequences</strong>: Tests require Redis installation or Docker, increasing CI setup complexity slightly. Tests run slower than with mocks but catch subtle bugs that mocks would miss. We maintain a fast unit test suite for business logic to keep overall test time reasonable.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pure Mocking</td>\n<td>Fast, no external dependencies</td>\n<td>Misses Redis-specific behaviors, false confidence</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Fake Redis Server</td>\n<td>Redis-like API, faster than real Redis</td>\n<td>May differ from real Redis in edge cases</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Real Redis with Isolation</td>\n<td>Accurate behavior, tests real interactions</td>\n<td>Requires Redis installation, slower tests</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<p><strong>Common Testing Patterns</strong></p>\n<p>We employ several recurring patterns across test suites:</p>\n<ol>\n<li><p><strong>Round-Trip Verification</strong>: Enqueue a job, dequeue it, verify payload preservation — the most basic integration test that catches serialization bugs.</p>\n</li>\n<li><p><strong>Concurrent Assertion</strong>: Start multiple workers simultaneously, enqueue jobs, verify all jobs are processed exactly once despite concurrent access.</p>\n</li>\n<li><p><strong>State Machine Validation</strong>: For each job, verify it progresses through the correct states (<code>PENDING</code> → <code>ACTIVE</code> → <code>COMPLETED</code>) and that invalid transitions are prevented.</p>\n</li>\n<li><p><strong>Timeout and Cancellation</strong>: Verify that jobs timeout correctly, workers handle SIGTERM gracefully, and scheduled jobs can be canceled mid-execution.</p>\n</li>\n<li><p><strong>Recovery Simulation</strong>: Simulate worker crashes by killing processes mid-job, then verify the janitor process re-queues the orphaned jobs.</p>\n</li>\n</ol>\n<h3 id=\"milestone-implementation-checkpoints\">Milestone Implementation Checkpoints</h3>\n<p>Each milestone includes specific verification checkpoints that validate core functionality. These checkpoints serve as progress indicators and quality gates before proceeding to the next milestone.</p>\n<p><strong>Milestone 1: Job Queue Core</strong></p>\n<blockquote>\n<p><strong>Mental Model: The Conveyor Belt Factory Acceptance Test</strong> — Imagine testing a newly installed conveyor belt system. You&#39;d verify packages can be placed on the belt (enqueue), move along it in order (FIFO), be inspected without removal (peek), and be redirected to different lanes (multiple queues). You&#39;d also check safety mechanisms like weight limits (payload size validation) and emergency stops (queue deletion).</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Description</th>\n<th>Verification Method</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CP1.1</strong></td>\n<td>Job serialization round-trip</td>\n<td>Unit test: <code>Job.serialize()</code> → <code>Job.deserialize()</code></td>\n<td>Original job equals deserialized job with all fields preserved</td>\n</tr>\n<tr>\n<td><strong>CP1.2</strong></td>\n<td>Basic enqueue/dequeue FIFO order</td>\n<td>Integration test with Redis: Enqueue 3 jobs, dequeue 3 times</td>\n<td>Jobs retrieved in same order as enqueued</td>\n</tr>\n<tr>\n<td><strong>CP1.3</strong></td>\n<td>Multiple queue support</td>\n<td>Integration test: Create 2 queues, enqueue jobs to each</td>\n<td>Jobs only retrieved from queue they were enqueued to</td>\n</tr>\n<tr>\n<td><strong>CP1.4</strong></td>\n<td>Payload size validation</td>\n<td>Unit test: Attempt to enqueue job &gt; 1MB</td>\n<td><code>ValidationError</code> raised before Redis interaction</td>\n</tr>\n<tr>\n<td><strong>CP1.5</strong></td>\n<td>Queue inspection APIs</td>\n<td>Integration test: Enqueue jobs, call <code>QueueManager.peek_queue()</code> and <code>get_queue_length()</code></td>\n<td>Peek returns correct jobs without removal, length matches count</td>\n</tr>\n<tr>\n<td><strong>CP1.6</strong></td>\n<td>Job ID uniqueness</td>\n<td>Integration test: Enqueue 1000 jobs concurrently</td>\n<td>All job IDs are unique (no collisions)</td>\n</tr>\n<tr>\n<td><strong>CP1.7</strong></td>\n<td>Atomic bulk enqueue</td>\n<td>Integration test: Enqueue 10 jobs via <code>bulk_enqueue()</code>, kill Redis mid-operation</td>\n<td>Either all jobs enqueued or none (no partial state)</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Verification for Milestone 1</strong></p>\n<p>After implementing Milestone 1, run the following verification suite:</p>\n<ol>\n<li><p><strong>Unit Test Suite</strong>: Execute all unit tests for <code>Job</code> class serialization, validation logic, and <code>QueueManager</code> business logic (excluding Redis operations).</p>\n</li>\n<li><p><strong>Integration Test Suite</strong>: Run the Redis-dependent tests which require a running Redis instance. These tests should pass with a clean Redis installation.</p>\n</li>\n<li><p><strong>Manual Verification Script</strong>: Execute a simple end-to-end workflow:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Start Redis locally</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   redis-server</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 6379</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Run the verification script</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   python</span><span style=\"color:#9ECBFF\"> tests/milestone1_verification.py</span></span></code></pre></div>\n\n<p>The verification script should:</p>\n<ul>\n<li>Enqueue 100 jobs with varying payloads</li>\n<li>Verify all jobs are stored in Redis with correct metadata</li>\n<li>Peek at the queue to verify jobs are in FIFO order</li>\n<li>Attempt to enqueue an oversized payload and confirm it&#39;s rejected</li>\n<li>Display queue metrics showing correct job counts</li>\n</ul>\n<p><strong>Common Testing Pitfalls for Milestone 1</strong></p>\n<p>⚠️ <strong>Pitfall: Testing with Production Redis</strong></p>\n<ul>\n<li><strong>Description</strong>: Accidentally running tests against a production Redis instance, corrupting real job data.</li>\n<li><strong>Why Wrong</strong>: Tests should never affect production data. This can cause data loss and service disruption.</li>\n<li><strong>Fix</strong>: Always use isolated Redis instances (different port or database) with automatic cleanup. Implement safety checks that refuse to connect to production Redis based on hostname or explicit test flag.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Assuming Serialization is Trivial</strong></p>\n<ul>\n<li><strong>Description</strong>: Writing naive <code>json.dumps()</code>/<code>json.loads()</code> tests that miss edge cases like datetime objects or custom classes.</li>\n<li><strong>Why Wrong</strong>: Job payloads often contain complex Python types that don&#39;t serialize to JSON by default.</li>\n<li><strong>Fix</strong>: Use parameterized tests with diverse payload types: strings, numbers, lists, dicts, datetimes, Decimal, and custom objects with <code>to_dict()</code> methods. Verify round-trip equivalence, not just string equality.</li>\n</ul>\n<p><strong>Milestone 2: Worker Process</strong></p>\n<blockquote>\n<p><strong>Mental Model: The Restaurant Kitchen Staff Evaluation</strong> — When evaluating kitchen staff, you&#39;d verify each cook can: 1) Receive orders from the ticket rail (poll queues), 2) Prepare dishes according to recipes (job handlers), 3) Work on multiple orders simultaneously (concurrency), 4) Clean up properly when closing (graceful shutdown), and 5) Signal when overwhelmed (heartbeat failure).</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Description</th>\n<th>Verification Method</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CP2.1</strong></td>\n<td>Worker polling from multiple queues</td>\n<td>Integration test: Start worker, enqueue jobs to different priority queues</td>\n<td>Worker processes jobs from higher-priority queues first</td>\n</tr>\n<tr>\n<td><strong>CP2.2</strong></td>\n<td>Job handler dispatch</td>\n<td>Unit test: Register multiple handlers, dispatch jobs</td>\n<td>Correct handler called with correct arguments</td>\n</tr>\n<tr>\n<td><strong>CP2.3</strong></td>\n<td>Graceful shutdown</td>\n<td>Integration test: Send SIGTERM to worker while job is processing</td>\n<td>Worker completes current job before exiting, leaves other jobs in queue</td>\n</tr>\n<tr>\n<td><strong>CP2.4</strong></td>\n<td>Worker heartbeat</td>\n<td>Integration test: Start worker, kill process abruptly, wait</td>\n<td>Heartbeat key expires, stale worker detection triggers</td>\n</tr>\n<tr>\n<td><strong>CP2.5</strong></td>\n<td>Concurrent job processing</td>\n<td>Integration test: Worker with concurrency=3, enqueue 5 long-running jobs</td>\n<td>3 jobs process simultaneously, then remaining 2</td>\n</tr>\n<tr>\n<td><strong>CP2.6</strong></td>\n<td>Job timeout enforcement</td>\n<td>Integration test: Enqueue job with 1-second timeout that sleeps for 5 seconds</td>\n<td>Job killed after 1 second, marked as failed with timeout error</td>\n</tr>\n<tr>\n<td><strong>CP2.7</strong></td>\n<td>Worker pause/resume</td>\n<td>Integration test: Pause worker, enqueue jobs, resume worker</td>\n<td>No jobs processed while paused, all jobs processed after resume</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Verification for Milestone 2</strong></p>\n<p>After implementing Milestone 2, run the following verification:</p>\n<ol>\n<li><p><strong>Worker Lifecycle Test</strong>: Start a worker, enqueue jobs, verify processing, then stop the worker gracefully.</p>\n</li>\n<li><p><strong>Concurrency Stress Test</strong>: Start worker with concurrency=5, enqueue 100 quick jobs, verify all processed correctly with no race conditions.</p>\n</li>\n<li><p><strong>Failure Recovery Test</strong>: Start worker processing a long job, kill the worker process abruptly (SIGKILL), then start a janitor process to verify orphaned job detection and re-queuing.</p>\n</li>\n</ol>\n<p>The verification should produce metrics showing:</p>\n<ul>\n<li>All jobs processed exactly once</li>\n<li>No jobs stuck in processing state after worker shutdown</li>\n<li>Heartbeat updates occurring at correct intervals</li>\n<li>Timeout errors properly recorded for long-running jobs</li>\n</ul>\n<p><strong>Milestone 3: Retry &amp; Error Handling</strong></p>\n<blockquote>\n<p><strong>Mental Model: The Customer Service Escalation Protocol Test</strong> — Testing a customer service escalation system involves: 1) Verifying initial complaints are logged (job failure recording), 2) Ensuring follow-ups happen at increasing intervals (exponential backoff), 3) Confirming unresolved issues escalate to management (dead letter queue), and 4) Validating managers can review and re-open cases (manual retry).</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Description</th>\n<th>Verification Method</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CP3.1</strong></td>\n<td>Exponential backoff calculation</td>\n<td>Unit test: <code>BackoffCalculator.calculate_delay()</code> with various attempt numbers</td>\n<td>Delays follow 2^attempt pattern within configured bounds</td>\n</tr>\n<tr>\n<td><strong>CP3.2</strong></td>\n<td>Retry scheduling</td>\n<td>Integration test: Fail a job, verify it appears in Redis sorted set with correct score</td>\n<td>Job scheduled for retry at correct future timestamp</td>\n</tr>\n<tr>\n<td><strong>CP3.3</strong></td>\n<td>Retry limit enforcement</td>\n<td>Integration test: Job with max_retries=3 fails repeatedly</td>\n<td>After 3 failures, job moves to dead letter queue, not retried again</td>\n</tr>\n<tr>\n<td><strong>CP3.4</strong></td>\n<td>Error serialization</td>\n<td>Unit test: Job fails with exception, <code>Job.record_error()</code> called</td>\n<td>Error details (type, message, traceback) stored in job metadata</td>\n</tr>\n<tr>\n<td><strong>CP3.5</strong></td>\n<td>Dead letter queue storage</td>\n<td>Integration test: Move job to dead letter queue, retrieve via API</td>\n<td>Job retrievable with all error history intact</td>\n</tr>\n<tr>\n<td><strong>CP3.6</strong></td>\n<td>Manual retry from dead letter</td>\n<td>Integration test: Retry job from dead letter queue</td>\n<td>Job re-enqueued with fresh attempt counter, processes successfully</td>\n</tr>\n<tr>\n<td><strong>CP3.7</strong></td>\n<td>Jitter in backoff</td>\n<td>Statistical test: Calculate 1000 retry delays with jitter enabled</td>\n<td>Delays vary within ±jitter_factor range, not perfectly predictable</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Verification for Milestone 3</strong></p>\n<p>Retry system verification requires time manipulation to avoid actual waiting. We use a mock clock to accelerate time:</p>\n<ol>\n<li><p><strong>Retry Flow Test</strong>: Configure a job to fail, verify it&#39;s scheduled for retry, advance mock clock to retry time, verify job executes again.</p>\n</li>\n<li><p><strong>Dead Letter Integration Test</strong>: Exhaust retries for a job, verify it moves to dead letter queue, retrieve it via API, manually retry it, verify success.</p>\n</li>\n<li><p><strong>Error Context Preservation Test</strong>: Fail a job with a complex exception containing nested attributes, verify all error context is preserved through serialization and available in dead letter queue.</p>\n</li>\n</ol>\n<p>Key metrics to verify:</p>\n<ul>\n<li>Retry delays follow exponential sequence with jitter</li>\n<li>No jobs exceed their max_retries limit</li>\n<li>Error count increments correctly with each failure</li>\n<li>Dead letter queue grows only when retries exhausted</li>\n</ul>\n<p><strong>Milestone 4: Scheduling &amp; Cron</strong></p>\n<blockquote>\n<p><strong>Mental Model: The Calendar Alarm System Validation</strong> — Testing a calendar alarm system involves: 1) Setting one-time alarms (delayed jobs), 2) Configuring recurring meetings (cron jobs), 3) Ensuring timezone conversions work correctly, 4) Preventing duplicate alerts for the same event (uniqueness constraints), and 5) Verifying missed alarms fire on system restart (catch-up logic).</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Description</th>\n<th>Verification Method</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CP4.1</strong></td>\n<td>Delayed job execution</td>\n<td>Integration test with time mock: Schedule job for future, advance clock</td>\n<td>Job enqueues only when scheduled time arrives, not before</td>\n</tr>\n<tr>\n<td><strong>CP4.2</strong></td>\n<td>Cron expression parsing</td>\n<td>Unit test: Parse various cron strings, calculate next run times</td>\n<td>Next run times match expected schedule</td>\n</tr>\n<tr>\n<td><strong>CP4.3</strong></td>\n<td>Recurring job execution</td>\n<td>Integration test: Schedule cron job, advance clock through multiple periods</td>\n<td>Job enqueues at each scheduled interval</td>\n</tr>\n<tr>\n<td><strong>CP4.4</strong></td>\n<td>Timezone handling</td>\n<td>Integration test: Schedule job in non-UTC timezone, verify enqueue times</td>\n<td>Job enqueues according to specified timezone, not system time</td>\n</tr>\n<tr>\n<td><strong>CP4.5</strong></td>\n<td>Uniqueness constraints</td>\n<td>Integration test: Schedule unique job, attempt to enqueue duplicate within window</td>\n<td>Duplicate job skipped (status=SKIPPED), not enqueued</td>\n</tr>\n<tr>\n<td><strong>CP4.6</strong></td>\n<td>Scheduler catch-up</td>\n<td>Integration test: Disable scheduler, miss scheduled time, re-enable scheduler</td>\n<td>Missed job enqueued immediately on scheduler restart</td>\n</tr>\n<tr>\n<td><strong>CP4.7</strong></td>\n<td>Schedule pause/resume</td>\n<td>Integration test: Pause schedule, advance time, resume schedule</td>\n<td>No jobs enqueued while paused, resumes correctly</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Verification for Milestone 4</strong></p>\n<p>Scheduler verification requires careful time manipulation and Redis state inspection:</p>\n<ol>\n<li><p><strong>Cron Schedule Test</strong>: Schedule a job with cron expression &quot;*/2 * * * *&quot; (every 2 minutes), advance mock clock 10 minutes, verify job enqueues 5 times.</p>\n</li>\n<li><p><strong>Timezone Boundary Test</strong>: Schedule a job for 2:30 AM in &quot;America/New_York&quot; timezone, verify correct UTC conversion accounting for daylight saving time if applicable.</p>\n</li>\n<li><p><strong>Uniqueness Window Test</strong>: Schedule unique job with 60-second window, enqueue it, attempt immediate re-enqueue, verify second attempt is skipped.</p>\n</li>\n</ol>\n<p>The verification should confirm:</p>\n<ul>\n<li>Scheduled jobs appear in Redis sorted set with correct timestamps</li>\n<li>Cron expressions calculate next run times accurately</li>\n<li>Timezone conversions handle daylight saving transitions</li>\n<li>Unique constraints prevent duplicates within specified windows</li>\n</ul>\n<p><strong>Milestone 5: Monitoring &amp; Dashboard</strong></p>\n<blockquote>\n<p><strong>Mental Model: The Air Traffic Control System Certification</strong> — Certifying an air traffic control dashboard involves: 1) Verifying all aircraft appear on radar (real-time updates), 2) Ensuring alert thresholds trigger warnings (alerting), 3) Validating historical flight data is searchable (job history), 4) Confirming operators can intervene when needed (manual controls), and 5) Testing system doesn&#39;t overload under heavy traffic (dashboard performance).</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Description</th>\n<th>Verification Method</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CP5.1</strong></td>\n<td>Real-time metrics updates</td>\n<td>Integration test: Enqueue/process jobs while dashboard connected via WebSocket/SSE</td>\n<td>Dashboard updates within 1 second of state change</td>\n</tr>\n<tr>\n<td><strong>CP5.2</strong></td>\n<td>Queue metrics calculation</td>\n<td>Unit test: Simulate job flows, calculate <code>QueueMetrics</code></td>\n<td>All counts and rates computed correctly</td>\n</tr>\n<tr>\n<td><strong>CP5.3</strong></td>\n<td>Alert rule evaluation</td>\n<td>Integration test: Configure alert rule, trigger condition</td>\n<td>Alert fires when threshold exceeded, respects cooldown</td>\n</tr>\n<tr>\n<td><strong>CP5.4</strong></td>\n<td>Job history search</td>\n<td>Integration test: Create jobs with various attributes, search with filters</td>\n<td>Only matching jobs returned, pagination works</td>\n</tr>\n<tr>\n<td><strong>CP5.5</strong></td>\n<td>Dashboard API rate limiting</td>\n<td>Load test: Generate many concurrent dashboard requests</td>\n<td>Requests throttled, Redis not overwhelmed</td>\n</tr>\n<tr>\n<td><strong>CP5.6</strong></td>\n<td>Manual job controls</td>\n<td>Integration test: Retry/delete dead letter jobs via dashboard API</td>\n<td>Operations succeed, system state updates accordingly</td>\n</tr>\n<tr>\n<td><strong>CP5.7</strong></td>\n<td>Metrics aggregation</td>\n<td>Integration test: Run aggregator over sample data</td>\n<td>Historical metrics computed, stored efficiently</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Verification for Milestone 5</strong></p>\n<p>Dashboard verification involves both API testing and UI simulation:</p>\n<ol>\n<li><p><strong>Real-time Flow Test</strong>: Start dashboard, enqueue jobs via API, verify WebSocket/SSE stream shows updates within expected latency.</p>\n</li>\n<li><p><strong>Alerting Integration Test</strong>: Configure alert for queue depth &gt; 10, enqueue 11 jobs, verify alert triggers, acknowledge alert, verify alert state updates.</p>\n</li>\n<li><p><strong>Search Performance Test</strong>: Insert 10,000 jobs into history, search with complex filters, verify response time &lt; 500ms with pagination.</p>\n</li>\n</ol>\n<p>Verification should produce:</p>\n<ul>\n<li>Dashboard displaying correct queue depths and worker status</li>\n<li>Alert history showing triggered alerts with correct metadata</li>\n<li>Job search returning accurate results with pagination</li>\n<li>Manual retry operations successfully re-queuing dead letter jobs</li>\n</ul>\n<h3 id=\"integration-and-end-to-end-tests\">Integration and End-to-End Tests</h3>\n<p>Integration tests verify interactions between components, while end-to-end tests simulate real-world usage scenarios across the entire system. These tests catch emergent behaviors that unit tests cannot.</p>\n<p><strong>Integration Test Architecture</strong></p>\n<p>Our integration test suite follows a layered approach:</p>\n<ol>\n<li><p><strong>Component Integration Tests</strong>: Test interactions between 2-3 components (e.g., QueueManager + Worker + Redis).</p>\n</li>\n<li><p><strong>Workflow Integration Tests</strong>: Test complete workflows (enqueue → process → retry → dead letter) across multiple components.</p>\n</li>\n<li><p><strong>Failure Scenario Tests</strong>: Inject failures at specific points (network timeout, Redis crash, worker SIGKILL) and verify recovery.</p>\n</li>\n<li><p><strong>Concurrency Tests</strong>: Run multiple producers and consumers simultaneously to detect race conditions.</p>\n</li>\n</ol>\n<p><strong>Key Integration Test Scenarios</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Components Involved</th>\n<th>Key Verification</th>\n<th>Failure Injection Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Happy Path Flow</strong></td>\n<td>Producer → QueueManager → Redis → Worker → Result Store</td>\n<td>Job processed exactly once, result stored</td>\n<td>None (baseline verification)</td>\n</tr>\n<tr>\n<td><strong>Worker Crash Recovery</strong></td>\n<td>Producer → Worker (crashed) → Janitor → New Worker</td>\n<td>Orphaned job re-queued, processed exactly once</td>\n<td>Kill worker mid-job execution</td>\n</tr>\n<tr>\n<td><strong>Redis Connection Loss</strong></td>\n<td>All components with Redis circuit breaker</td>\n<td>Operations fail fast, resume after Redis recovery</td>\n<td>Simulate Redis network partition</td>\n</tr>\n<tr>\n<td><strong>Concurrent Priority Queues</strong></td>\n<td>Multiple producers → QueueManager → Multiple workers</td>\n<td>Higher-priority jobs processed first despite concurrency</td>\n<td>Vary job durations and arrival times</td>\n</tr>\n<tr>\n<td><strong>Scheduler Catch-up</strong></td>\n<td>Scheduler (disabled) → Time advance → Scheduler (enabled)</td>\n<td>Missed jobs enqueued immediately, future jobs scheduled correctly</td>\n<td>Disable scheduler during scheduled execution time</td>\n</tr>\n<tr>\n<td><strong>Dead Letter Manual Retry</strong></td>\n<td>Failed job → DeadLetterQueue → Dashboard API → QueueManager → Worker</td>\n<td>Manually retried job succeeds, removed from dead letter</td>\n<td>None (manual operation verification)</td>\n</tr>\n</tbody></table>\n<p><strong>End-to-End Test Environment</strong></p>\n<p>End-to-end tests require a complete running system. We use Docker Compose to orchestrate:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test-environment/docker-compose.yaml</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">version</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'3.8'</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">services</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  redis</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    image</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">redis:7-alpine</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"6380:6379\"</span><span style=\"color:#6A737D\">  # Different port to avoid conflict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  worker-1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    build</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">.</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    command</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"worker\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--queues\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"high,low\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--concurrency\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"2\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    depends_on</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">redis</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  worker-2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    build</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">.</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    command</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"worker\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--queues\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"medium\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--concurrency\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    depends_on</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">redis</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  scheduler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    build</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">.</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    command</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"scheduler\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--polling-interval\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"5\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    depends_on</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">redis</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  dashboard</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    build</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">.</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    command</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"dashboard\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--port\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"8080\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"8080:8080\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    depends_on</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">redis</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<p><strong>End-to-End Test Execution Pattern</strong></p>\n<p>Each end-to-end test follows this pattern:</p>\n<ol>\n<li><strong>Setup</strong>: Start all services via Docker Compose, wait for health checks</li>\n<li><strong>Execution</strong>: Perform test actions via API calls or CLI</li>\n<li><strong>Verification</strong>: Query dashboard APIs, inspect Redis state, verify outcomes</li>\n<li><strong>Teardown</strong>: Capture logs, stop services, clean up resources</li>\n</ol>\n<p><strong>Sample End-to-End Test: Scheduled Job with Retry</strong></p>\n<p>This test verifies the complete lifecycle of a scheduled job that fails and retries:</p>\n<ol>\n<li><strong>Schedule a recurring job</strong> via scheduler API to run every minute</li>\n<li><strong>Advance system time</strong> by 5 minutes (using time mock in test environment)</li>\n<li><strong>Verify</strong> 5 job instances were enqueued</li>\n<li><strong>Configure first 2 executions to fail</strong> by registering a failing handler</li>\n<li><strong>Verify</strong> failed jobs are scheduled for retry with exponential backoff</li>\n<li><strong>Advance time</strong> to retry times</li>\n<li><strong>Configure retries to succeed</strong> by changing handler</li>\n<li><strong>Verify</strong> all jobs eventually complete successfully</li>\n<li><strong>Check dashboard</strong> shows correct metrics: 5 jobs completed, 2 initial failures, 3 retries</li>\n</ol>\n<p><strong>Concurrency and Race Condition Testing</strong></p>\n<p>Distributed systems exhibit subtle race conditions. We employ specific techniques to detect them:</p>\n<table>\n<thead>\n<tr>\n<th>Technique</th>\n<th>Implementation</th>\n<th>What It Detects</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Deterministic Simulation</strong></td>\n<td>Use controlled thread scheduling with barriers</td>\n<td>Race conditions in multi-threaded code</td>\n</tr>\n<tr>\n<td><strong>Statistical Assertion</strong></td>\n<td>Run test 1000 times, verify invariant always holds</td>\n<td>Probabilistic race conditions</td>\n</tr>\n<tr>\n<td><strong>Stress Testing</strong></td>\n<td>Run system under heavy load with random delays</td>\n<td>Resource leaks, deadlocks</td>\n</tr>\n<tr>\n<td><strong>Linearizability Checking</strong></td>\n<td>Record operation history, verify appears sequential</td>\n<td>Concurrent data structure bugs</td>\n</tr>\n</tbody></table>\n<p><strong>Example: Testing Job Uniqueness Under Concurrency</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Pseudocode for testing uniqueness constraint race condition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unique_job_concurrent_enqueue</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Schedule a unique job with 60-second window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scheduler.schedule_recurring_job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"unique_task\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        cron_expression</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"* * * * *\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        unique_key</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"task_1\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        unique_window_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Simulate concurrent scheduler invocations (e.g., multiple scheduler processes)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> ThreadPoolExecutor(</span><span style=\"color:#FFAB70\">max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> executor:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        futures </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [executor.submit(scheduler._process_schedule, schedule) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                  for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [f.result() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> f </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> futures]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Only one should succeed in enqueuing the job</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> success, _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> success) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span></code></pre></div>\n\n<p><strong>Performance and Load Testing</strong></p>\n<p>While not part of the core milestones, performance testing validates the architecture scales as expected:</p>\n<ol>\n<li><strong>Throughput Test</strong>: Measure jobs processed per second with varying worker counts</li>\n<li><strong>Latency Test</strong>: Measure enqueue-to-completion time under different loads</li>\n<li><strong>Redis Memory Test</strong>: Monitor Redis memory growth with long-running job history</li>\n<li><strong>Dashboard Scalability Test</strong>: Verify dashboard remains responsive with 10k+ jobs in history</li>\n</ol>\n<p><strong>Continuous Integration Pipeline</strong></p>\n<p>The CI pipeline executes tests in this order:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">mermaid</span><pre class=\"arch-pre shiki-highlighted\"><code>graph LR\n    A[Code Commit] --&gt; B[Unit Tests&lt;br/&gt;Fast, no Redis]\n    B --&gt; C[Integration Tests&lt;br/&gt;With Redis, slower]\n    C --&gt; D[End-to-End Tests&lt;br/&gt;Docker Compose, slowest]\n    D --&gt; E[Performance Tests&lt;br/&gt;Optional, on schedule]\n    E --&gt; F[Release Candidate]</code></pre></div>\n\n<p><strong>Test Data Management</strong></p>\n<p>We maintain realistic test data for comprehensive testing:</p>\n<table>\n<thead>\n<tr>\n<th>Data Set</th>\n<th>Purpose</th>\n<th>Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Job Payload Variants</strong></td>\n<td>Test serialization edge cases</td>\n<td>Large payloads, nested structures, binary data, special characters</td>\n</tr>\n<tr>\n<td><strong>Cron Expression Suite</strong></td>\n<td>Test scheduler correctness</td>\n<td>Simple intervals, complex ranges, timezone transitions, invalid expressions</td>\n</tr>\n<tr>\n<td><strong>Error Type Catalog</strong></td>\n<td>Test error handling</td>\n<td>Network errors, application exceptions, timeout exceptions, memory errors</td>\n</tr>\n<tr>\n<td><strong>Load Profiles</strong></td>\n<td>Performance testing</td>\n<td>Steady load, burst load, increasing load, spike load</td>\n</tr>\n</tbody></table>\n<p><strong>Test Maintenance Strategy</strong></p>\n<p>Tests require maintenance as the system evolves:</p>\n<ol>\n<li><strong>Flaky Test Detection</strong>: CI tracks test failure rates, automatically quarantines flaky tests</li>\n<li><strong>Test Data Generation</strong>: Use property-based testing to generate edge cases automatically</li>\n<li><strong>Golden Master Testing</strong>: For complex outputs (dashboard UI), maintain &quot;golden master&quot; snapshots</li>\n<li><strong>Test Coverage Tracking</strong>: Monitor coverage of critical paths (error handling, recovery logic)</li>\n</ol>\n<blockquote>\n<p><strong>Key Principle</strong>: The most valuable tests are those that fail when the system behaves incorrectly but pass when it behaves correctly. This seems obvious but is often violated by tests that pass for the wrong reasons (e.g., mocks that don&#39;t simulate real behavior) or fail for irrelevant reasons (e.g., timing sensitivities in test environment).</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Testing Framework</td>\n<td><code>pytest</code> with <code>pytest-asyncio</code></td>\n<td><code>pytest</code> with custom distributed test runner</td>\n</tr>\n<tr>\n<td>Redis for Tests</td>\n<td>Docker Redis container</td>\n<td>Redis cluster for distributed testing</td>\n</tr>\n<tr>\n<td>Mock Clock</td>\n<td><code>freezegun</code> library</td>\n<td>Custom time abstraction with dependency injection</td>\n</tr>\n<tr>\n<td>Concurrency Testing</td>\n<td><code>threading</code>/<code>multiprocessing</code></td>\n<td><code>asyncio</code> with controlled event loop scheduling</td>\n</tr>\n<tr>\n<td>Dashboard Testing</td>\n<td><code>requests</code> for API testing</td>\n<td><code>playwright</code> for full browser automation</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Locust.io for HTTP load testing</td>\n<td>Custom job enqueue load generator</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n├── tests/                           # All test code\n│   ├── unit/                        # Unit tests (no external dependencies)\n│   │   ├── test_job.py              # Job class tests\n│   │   ├── test_backoff_calculator.py\n│   │   └── test_cron_parser.py\n│   ├── integration/                 # Integration tests (with Redis)\n│   │   ├── conftest.py              # pytest fixtures (Redis setup)\n│   │   ├── test_queue_manager.py    # QueueManager + Redis tests\n│   │   ├── test_worker_integration.py\n│   │   └── test_retry_manager.py\n│   ├── e2e/                         # End-to-end tests\n│   │   ├── docker-compose.yaml      # Test environment\n│   │   ├── test_happy_path.py       # Full system workflow\n│   │   ├── test_failure_recovery.py\n│   │   └── test_scheduler_e2e.py\n│   ├── fixtures/                    # Test data fixtures\n│   │   ├── job_payloads.json\n│   │   ├── cron_expressions.yaml\n│   │   └── error_catalog.py\n│   └── utils/                       # Test utilities\n│       ├── redis_helpers.py         # Redis test helpers\n│       ├── time_helpers.py          # Mock time utilities\n│       └── process_helpers.py       # Worker process management\n├── src/                             # Main source code\n└── scripts/\n    └── run_tests.sh                 # Test execution script</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here&#39;s a complete, reusable test infrastructure for Redis integration tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/utils/redis_helpers.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Redis test helpers for isolated, parallel test execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Generator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisTestContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages isolated Redis instances for parallel testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, port: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6379</span><span style=\"color:#E1E4E8\">, db: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.port </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> port</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._client: Optional[redis.Redis] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> client</span><span style=\"color:#E1E4E8\">(self) -> redis.Redis:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Lazy initialization of Redis client.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        host</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'localhost'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        port</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.port,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        db</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Ping to ensure connection works</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">._client.ping()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> flush</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clear all data from the current database.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.flushdb()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_unique_key</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"test\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate a unique Redis key for tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prefix</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">threading.get_ident()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">random.randint(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000000</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline</span><span style=\"color:#E1E4E8\">(self) -> Generator[redis.Pipeline, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for Redis pipeline with automatic execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> pipe</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.execute()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.reset()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> close</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Close Redis connection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._client:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client.close()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global test context (can be overridden in tests)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">_test_context: Optional[RedisTestContext] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_test_context</span><span style=\"color:#E1E4E8\">() -> RedisTestContext:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get or create the global test context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    global</span><span style=\"color:#E1E4E8\"> _test_context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _test_context </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use random DB to allow parallel test execution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        _test_context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisTestContext(</span><span style=\"color:#FFAB70\">db</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">random.randint(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">15</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> _test_context</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> fresh_redis_context</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager that provides a fresh Redis context and cleans up.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisTestContext(</span><span style=\"color:#FFAB70\">db</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">random.randint(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">200</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context.flush()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context.close()</span></span></code></pre></div>\n\n<p><strong>D. Core Test Skeleton Code</strong></p>\n<p>Here&#39;s a template for writing integration tests with proper setup/teardown:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/integration/test_queue_manager.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Integration tests for QueueManager with real Redis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job, JobStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tests.utils.redis_helpers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> fresh_redis_context</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestQueueManagerIntegration</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test QueueManager with Redis integration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> queue_manager</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a QueueManager with fresh Redis context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> fresh_redis_context() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> redis_context:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Create system config for testing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SystemConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                redis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RedisConfig(</span><span style=\"color:#FFAB70\">url</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"redis://localhost:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">redis_context.port</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">redis_context.db</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                queues</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[QueueConfig(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"test-queue\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_payload_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># 1MB</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                job_history_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                retry_base_delay</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                retry_max_attempts</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QueueManager(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Cleanup happens automatically via context manager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_enqueue_and_dequeue_fifo</span><span style=\"color:#E1E4E8\">(self, queue_manager):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test basic FIFO ordering of jobs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create 3 test jobs with sequential IDs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Enqueue all jobs using queue_manager.enqueue_job()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use Redis BRPOP directly to dequeue jobs in order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify jobs are returned in same order as enqueued</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check job metadata is preserved through round-trip</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_bulk_enqueue_atomicity</span><span style=\"color:#E1E4E8\">(self, queue_manager):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test that bulk enqueue is atomic (all or nothing).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create 10 test jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start a Redis transaction to monitor operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Call queue_manager.bulk_enqueue() with the jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Simulate Redis failure mid-operation (mock redis.execute)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify either all jobs enqueued or none (check queue length)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Clean up test jobs if any were enqueued</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_queue_priority_weighted_polling</span><span style=\"color:#E1E4E8\">(self, queue_manager):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test that workers poll higher-priority queues more frequently.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Configure multiple queues with different priorities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Enqueue equal numbers of jobs to each queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Simulate worker polling using queue_manager's internal method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record which queue is polled each time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify higher-priority queues are polled more often</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Statistical test: run 1000 polls, check distribution matches weights</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_oversized_payload_rejection</span><span style=\"color:#E1E4E8\">(self, queue_manager):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test that jobs larger than max_payload_size are rejected.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create a job with payload exceeding max_payload_size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Attempt to enqueue the job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify ValidationError is raised</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify no job was actually enqueued to Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test boundary case: payload exactly at limit should succeed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_queue_inspection_apis</span><span style=\"color:#E1E4E8\">(self, queue_manager):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test peek_queue and get_queue_length APIs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Enqueue 5 jobs to a test queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call queue_manager.get_queue_length() - should return 5</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Call queue_manager.peek_queue(count=3) - should return first 3 jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify peek doesn't remove jobs (call get_queue_length again)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Dequeue one job, verify length becomes 4</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Test edge cases: peek with count larger than queue, peek empty queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints for Python Testing</strong></p>\n<ol>\n<li><strong>Use <code>pytest</code> Fixtures for Dependency Injection</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   @pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> redis_client</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       yield</span><span style=\"color:#E1E4E8\"> client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       client.flushdb()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       client.close()</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Mock Time with <code>freezegun</code></strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> freezegun </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> freeze_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   @freeze_time</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"2024-01-01 12:00:00\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> test_scheduled_job</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # Time is frozen at Jan 1, 2024</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       schedule_job(</span><span style=\"color:#FFAB70\">run_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime(</span><span style=\"color:#79B8FF\">2024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # Advance time 5 minutes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       with</span><span style=\"color:#E1E4E8\"> freeze_time(</span><span style=\"color:#9ECBFF\">\"2024-01-01 12:05:00\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           process_due_jobs()  </span><span style=\"color:#6A737D\"># Job should execute</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Test Concurrent Operations with <code>threading</code></strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> test_concurrent_enqueue</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       def</span><span style=\"color:#B392F0\"> enqueue_job</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_manager.enqueue_job(job)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           with</span><span style=\"color:#E1E4E8\"> lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               results.append(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       threads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">enqueue_job) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> threads: t.start()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> threads: t.join()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">(results)) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#6A737D\">  # All job IDs unique</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Use <code>pytest.mark.parametrize</code> for Comprehensive Input Testing</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   @pytest.mark.parametrize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"cron_expression,expected_next_run\"</span><span style=\"color:#E1E4E8\">, [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       (</span><span style=\"color:#9ECBFF\">\"*/15 * * * *\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"2024-01-01 12:15:00\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       (</span><span style=\"color:#9ECBFF\">\"0 2 * * *\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"2024-01-02 02:00:00\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       (</span><span style=\"color:#9ECBFF\">\"0 0 1 * *\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"2024-02-01 00:00:00\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   ])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> test_cron_expressions</span><span style=\"color:#E1E4E8\">(cron_expression, expected_next_run):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # Test each cron expression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       pass</span></span></code></pre></div>\n\n<p><strong>F. Milestone Checkpoint Verification</strong></p>\n<p>After completing each milestone, run these verification commands:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Milestone 1: Job Queue Core</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_job.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_queue_manager.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/verify_milestone1.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 2: Worker Process  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_worker_integration.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/start_worker_test.py</span><span style=\"color:#79B8FF\"> --run-verification</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 3: Retry &#x26; Error Handling</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_backoff_calculator.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_retry_manager.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/test_retry_flow.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 4: Scheduling &#x26; Cron</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_cron_parser.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_scheduler.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/test_scheduled_jobs.py</span><span style=\"color:#79B8FF\"> --time-mock</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 5: Monitoring &#x26; Dashboard</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_dashboard_api.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/load_test_dashboard.py</span><span style=\"color:#79B8FF\"> --users</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#79B8FF\"> --duration</span><span style=\"color:#79B8FF\"> 30</span></span></code></pre></div>\n\n<p><strong>Expected Outcomes for Milestone Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Milestone</th>\n<th>Test Command</th>\n<th>Success Indicators</th>\n<th>Failure Symptoms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1</strong></td>\n<td><code>pytest tests/integration/test_queue_manager.py</code></td>\n<td>All tests pass, Redis keys created correctly</td>\n<td>Jobs lost, serialization errors, queue priority not working</td>\n</tr>\n<tr>\n<td><strong>2</strong></td>\n<td><code>python scripts/start_worker_test.py</code></td>\n<td>Worker processes jobs, heartbeats update, graceful shutdown works</td>\n<td>Jobs stuck in processing, orphaned workers, timeout not enforced</td>\n</tr>\n<tr>\n<td><strong>3</strong></td>\n<td><code>python scripts/test_retry_flow.py</code></td>\n<td>Failed jobs retry with increasing delays, dead letter queue receives exhausted jobs</td>\n<td>Infinite retry loops, error context lost, manual retry fails</td>\n</tr>\n<tr>\n<td><strong>4</strong></td>\n<td><code>python scripts/test_scheduled_jobs.py</code></td>\n<td>Jobs enqueue at correct times, cron expressions evaluated correctly, uniqueness constraints work</td>\n<td>Timezone issues, missed schedules, duplicate jobs enqueued</td>\n</tr>\n<tr>\n<td><strong>5</strong></td>\n<td><code>python scripts/load_test_dashboard.py</code></td>\n<td>Dashboard updates in real-time, alerts trigger correctly, search returns accurate results</td>\n<td>Dashboard sluggish under load, alerts fire incorrectly, memory leaks</td>\n</tr>\n</tbody></table>\n<p><strong>G. Debugging Tips for Common Test Failures</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tests pass locally but fail in CI</strong></td>\n<td>Redis version differences, timezone issues, or race conditions</td>\n<td>Check Redis logs in CI, add debug logging to tests, run tests with <code>--tb=short</code></td>\n<td>Use same Redis version in CI, fix timezone assumptions, add retries for timing-sensitive tests</td>\n</tr>\n<tr>\n<td><strong>Integration tests leave Redis data between tests</strong></td>\n<td>Improper test isolation, missing <code>flushdb()</code> in teardown</td>\n<td>Check Redis keys after test runs: <code>redis-cli keys &#39;*&#39;</code></td>\n<td>Use <code>fresh_redis_context()</code> fixture or ensure each test cleans up</td>\n</tr>\n<tr>\n<td><strong>Worker tests hang indefinitely</strong></td>\n<td>Deadlock in worker shutdown, missing timeout in test</td>\n<td>Add debug logging to worker state machine, use test timeout decorator</td>\n<td>Ensure worker threads/processes properly join, add test timeouts</td>\n</tr>\n<tr>\n<td><strong>Time-sensitive tests are flaky</strong></td>\n<td>System clock resolution, test execution time variability</td>\n<td>Log actual vs expected timestamps, measure test execution time</td>\n<td>Use mock time, add tolerance to timestamp comparisons</td>\n</tr>\n<tr>\n<td><strong>Concurrent tests fail randomly</strong></td>\n<td>Race condition not caught in single-threaded tests</td>\n<td>Run test 100 times, add <code>pytest-repeat</code>, use thread sanitizer</td>\n<td>Add proper synchronization, use atomic operations, fix race condition</td>\n</tr>\n</tbody></table>\n<p><strong>Test Coverage Goals</strong></p>\n<p>Aim for these coverage targets (measured via <code>pytest-cov</code>):</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Statement Coverage</th>\n<th>Branch Coverage</th>\n<th>Critical Path Coverage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Job</code> class</td>\n<td>100%</td>\n<td>100%</td>\n<td>100% (serialization round-trip)</td>\n</tr>\n<tr>\n<td><code>QueueManager</code></td>\n<td>95%</td>\n<td>90%</td>\n<td>100% (atomic operations)</td>\n</tr>\n<tr>\n<td><code>Worker</code></td>\n<td>90%</td>\n<td>85%</td>\n<td>100% (graceful shutdown)</td>\n</tr>\n<tr>\n<td><code>RetryManager</code></td>\n<td>95%</td>\n<td>90%</td>\n<td>100% (dead letter transition)</td>\n</tr>\n<tr>\n<td><code>Scheduler</code></td>\n<td>90%</td>\n<td>85%</td>\n<td>100% (cron evaluation)</td>\n</tr>\n<tr>\n<td><code>Dashboard API</code></td>\n<td>85%</td>\n<td>80%</td>\n<td>100% (alert triggering)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Final Testing Principle</strong>: Write tests that fail when the system is broken and pass when it&#39;s working. This seems obvious but requires careful thought about what &quot;broken&quot; means for each component. A test that passes because mocks are too permissive is worse than no test at all — it provides false confidence.</p>\n</blockquote>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section spans all five milestones, providing systematic diagnostic procedures for common implementation issues that arise during development and operation. The debugging guide transforms vague symptoms into specific causes and actionable fixes.</p>\n</blockquote>\n<p>Building a distributed background job processor involves coordinating multiple processes, network calls, and persistent state—a perfect environment for subtle bugs to emerge. When jobs aren&#39;t processing, workers are crashing, or retries aren&#39;t happening, systematic debugging is essential. This guide provides concrete procedures for identifying and resolving the most common issues, organized by symptom pattern and component.</p>\n<p>Think of debugging this system as being a <strong>detective investigating a crime scene</strong>. You arrive at the scene (a production issue), examine the evidence (logs, Redis data, metrics), look for patterns and anomalies, reconstruct timelines, and identify the perpetrator (the bug). Each piece of evidence tells part of the story, and your job is to connect them logically.</p>\n<h3 id=\"common-bugs-and-their-fixes\">Common Bugs and Their Fixes</h3>\n<p>The following table categorizes common implementation bugs by their observable symptoms, underlying causes, and specific fixes. Each entry follows the pattern: you observe a symptom, investigate potential causes, and apply targeted corrections.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Investigation Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Jobs stuck in queue, workers idle</strong></td>\n<td>1. Workers polling wrong queue names<br>2. Queue priority weights misconfigured<br>3. Redis connection failures in workers<br>4. Workers paused or in shutdown state</td>\n<td>1. Check worker logs for connection errors<br>2. Verify <code>WorkerConfig.queues</code> matches queue names<br>3. Use <code>redis-cli LLEN queue:{name}</code> to confirm jobs exist<br>4. Check worker heartbeat status in Redis</td>\n<td>1. Ensure queue names in config match those used for enqueue<br>2. Reconfigure workers with correct queue list<br>3. Implement connection retry logic in <code>RedisClient</code><br>4. Restart workers with <code>SIGTERM</code> then fresh start</td>\n</tr>\n<tr>\n<td><strong>Jobs disappearing without execution</strong></td>\n<td>1. Jobs moved to processing queue but worker crashed<br>2. <code>BRPOPLPUSH</code> without backup processing queue cleanup<br>3. Job validation failing silently during deserialization<br>4. Race condition in multi-worker dequeuing</td>\n<td>1. Check <code>queue:{name}:processing</code> lists in Redis<br>2. Verify Janitor process is running<br>3. Examine worker crash logs for segmentation faults<br>4. Check for <code>ValidationError</code> in enqueue logs</td>\n<td>1. Implement and run <code>Janitor</code> to requeue stale processing jobs<br>2. Use atomic operations for dequeue with proper error handling<br>3. Add detailed logging in <code>Job.deserialize()</code><br>4. Use Redis transactions with <code>WATCH</code> for critical operations</td>\n</tr>\n<tr>\n<td><strong>Exponential retries happening too fast or not at all</strong></td>\n<td>1. Incorrect <code>attempt</code> counter in retry logic<br>2. Base delay miscalculation (seconds vs milliseconds)<br>3. Retry sorted set score using wrong timestamp format<br>4. Retry scheduler not polling frequently enough</td>\n<td>1. Inspect <code>job:{id}:attempts</code> key in Redis<br>2. Check <code>retry:{queue}</code> sorted set scores with <code>ZRANGE</code><br>3. Verify <code>BackoffCalculator.calculate_delay()</code> implementation<br>4. Check scheduler polling interval vs retry delays</td>\n<td>1. Ensure <code>Job.attempts</code> increments atomically on each failure<br>2. Use Unix timestamps (seconds) for sorted set scores<br>3. Debug formula: <code>base_delay * (2 ** (attempt - 1))</code><br>4. Configure scheduler to poll more frequently than minimum retry delay</td>\n</tr>\n<tr>\n<td><strong>Scheduled jobs not enqueuing at expected times</strong></td>\n<td>1. Timezone mismatch between scheduler and cron expression<br>2. <code>next_run_at</code> calculation error for cron expressions<br>3. Uniqueness constraint preventing enqueue<br>4. Scheduler process stopped or crashing</td>\n<td>1. Compare scheduler timezone with schedule timezone field<br>2. Check <code>schedule:{id}</code> hash for <code>next_run_at</code> value<br>3. Look for <code>SKIPPED</code> status in schedule history<br>4. Verify scheduler heartbeat in Redis</td>\n<td>1. Use UTC consistently or store timezone with each schedule<br>2. Test cron parsing with <code>croniter</code> library<br>3. Adjust <code>unique_window_seconds</code> or disable uniqueness<br>4. Implement scheduler supervisor with automatic restart</td>\n</tr>\n<tr>\n<td><strong>Memory usage growing unbounded in Redis</strong></td>\n<td>1. Missing TTL on job data after completion<br>2. Dead letter queue never purged<br>3. Job history accumulating without archival<br>4. Large payloads stored without size limits</td>\n<td>1. Run <code>redis-cli INFO memory</code> to check memory usage<br>2. Scan for keys without TTL: `redis-cli --scan</td>\n<td>while read k; do echo &quot;$k: $(redis-cli TTL $k)&quot;; done<code>&lt;br&gt;3. Check </code>job:{id}:result<code>keys still present&lt;br&gt;4. Verify</code>max_payload_size` enforcement</td>\n</tr>\n<tr>\n<td><strong>Worker crashes without logging errors</strong></td>\n<td>1. Unhandled exception in job handler<br>2. Memory leak in long-running worker process<br>3. Signal handler conflicts with threading<br>4. Job timeout killing process instead of just thread</td>\n<td>1. Check system logs (<code>dmesg</code>, <code>journalctl</code>) for OOM killer<br>2. Add <code>try...except</code> with logging in worker main loop<br>3. Test with reduced concurrency<br>4. Monitor Python GC with <code>gc.get_count()</code></td>\n<td>1. Wrap job execution in comprehensive error handler<br>2. Implement process-based isolation for memory-intensive jobs<br>3. Use <code>signal.signal()</code> carefully, preserve original handlers<br>4. Use thread/process timeout rather than process signal</td>\n</tr>\n<tr>\n<td><strong>Dashboard shows stale or incorrect metrics</strong></td>\n<td>1. Metrics aggregation interval too long<br>2. Race condition between metric collection and job state updates<br>3. Redis memory eviction deleting metric keys<br>4. WebSocket/SSE connection dropping silently</td>\n<td>1. Compare dashboard metrics with direct Redis queries<br>2. Check <code>metrics:queue:{name}:depth</code> key updates<br>3. Verify <code>MetricsAggregator</code> thread is running<br>4. Inspect browser console for WebSocket errors</td>\n<td>1. Reduce aggregation interval for near-real-time updates<br>2. Use atomic Redis operations for metric updates<br>3. Configure Redis maxmemory policy to <code>volatile-lru</code><br>4. Implement WebSocket heartbeat and automatic reconnection</td>\n</tr>\n<tr>\n<td><strong>Duplicate execution of the same job</strong></td>\n<td>1. Worker crash after processing but before acknowledgment<br>2. Race condition in unique job constraint<br>3. Network partition causing split-brain worker behavior<br>4. Retry of non-idempotent operation without deduplication</td>\n<td>1. Check for duplicate job IDs in completed jobs<br>2. Examine processing queue for same job multiple times<br>3. Look for network outage timestamps in logs<br>4. Verify idempotency key usage</td>\n<td>1. Implement idempotency keys for critical operations<br>2. Use Redis <code>SETNX</code> for unique job locks<br>3. Add partition tolerance logic with fencing tokens<br>4. Ensure job handlers are idempotent when possible</td>\n</tr>\n<tr>\n<td><strong>Priority queue ordering not respected</strong></td>\n<td>1. Worker polling algorithm checking queues in fixed order<br>2. Priority weights incorrectly normalized<br>3. High-priority jobs stuck behind long-running low-priority jobs<br>4. Queue starvation due to constant high-priority influx</td>\n<td>1. Trace worker polling sequence in debug logs<br>2. Verify <code>QueueConfig.priority</code> values are positive integers<br>3. Check if worker concurrency is saturated<br>4. Monitor queue depths for imbalance</td>\n<td>1. Implement weighted random queue selection algorithm<br>2. Use priority values to calculate polling probabilities<br>3. Add preemption for critical high-priority jobs<br>4. Implement separate worker pools for different priority tiers</td>\n</tr>\n<tr>\n<td><strong>Graceful shutdown not working</strong></td>\n<td>1. Signal handler not properly registered<br>2. Worker waiting indefinitely on blocking Redis call<br>3. Current job not interruptible<br>4. Thread pool not shutting down correctly</td>\n<td>1. Test with <code>kill -TERM &lt;pid&gt;</code> and observe behavior<br>2. Check if worker is stuck in <code>BRPOP</code> with no timeout<br>3. Verify job timeout configuration<br>4. Examine thread states during shutdown</td>\n<td>1. Set timeout on blocking Redis operations<br>2. Implement cooperative cancellation for long-running jobs<br>3. Use <code>ThreadPoolExecutor</code> with <code>shutdown(wait=True)</code><br>4. Add shutdown flag checked in main loop</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Debugging Principle:</strong> Always start with the simplest explanation. When jobs aren&#39;t processing, check that workers are actually running and connected to Redis before investigating complex race conditions. Use the scientific method: form a hypothesis, test it with a specific investigation, then refine.</p>\n</blockquote>\n<h4 id=\"state-transition-anomalies\">State Transition Anomalies</h4>\n<p>Jobs and workers follow specific state machines (as shown in <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fjob-state-machine.svg\" alt=\"Job State Machine\"> and <img src=\"/api/project/background-job-processor/architecture-doc/asset?path=diagrams%2Fworker-state-machine.svg\" alt=\"Worker Process State Machine\">). When states don&#39;t transition as expected, use this diagnostic table:</p>\n<table>\n<thead>\n<tr>\n<th>Anomaly</th>\n<th>Expected Transition</th>\n<th>Actual State</th>\n<th>Investigation</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Job stuck in ACTIVE</strong></td>\n<td>ACTIVE → COMPLETED or FAILED</td>\n<td>Remains ACTIVE for hours</td>\n<td>1. Check worker processing the job<br>2. Look for timeout configuration<br>3. Check for deadlock in job handler</td>\n<td>1. Implement job timeout with forced termination<br>2. Add deadlock detection in worker<br>3. Manual move to FAILED via dashboard</td>\n</tr>\n<tr>\n<td><strong>Worker stuck in ShuttingDown</strong></td>\n<td>ShuttingDown → Stopped</td>\n<td>Worker hangs during shutdown</td>\n<td>1. Check if current job has cleanup phase<br>2. Look for thread/child process not exiting<br>3. Check for finally blocks with infinite loops</td>\n<td>1. Add shutdown timeout to force exit<br>2. Implement interrupt handling for job threads<br>3. Use process-based isolation for clean termination</td>\n</tr>\n<tr>\n<td><strong>Job jumps from PENDING to DEAD_LETTER</strong></td>\n<td>PENDING → ACTIVE → FAILED → RETRY_SCHEDULED → ... → DEAD_LETTER</td>\n<td>Direct transition</td>\n<td>1. Check job <code>max_retries = 0</code><br>2. Look for <code>RetryManager</code> bypass on specific errors<br>3. Check for manual move to dead letter</td>\n<td>1. Validate <code>max_retries</code> configuration<br>2. Review retry filter logic<br>3. Add audit log for manual operations</td>\n</tr>\n</tbody></table>\n<h3 id=\"diagnostic-tools-and-techniques\">Diagnostic Tools and Techniques</h3>\n<p>Effective debugging requires the right tools and methodologies. This section outlines both built-in diagnostic capabilities and external tools that reveal system internals.</p>\n<h4 id=\"built-in-diagnostic-endpoints\">Built-in Diagnostic Endpoints</h4>\n<p>The monitoring dashboard (Milestone 5) should include diagnostic endpoints that expose internal state without requiring direct Redis access. These endpoints are invaluable for production debugging.</p>\n<table>\n<thead>\n<tr>\n<th>Endpoint</th>\n<th>Purpose</th>\n<th>Data Format</th>\n<th>Investigation Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>GET /api/debug/queue/{name}/contents</code></td>\n<td>Inspect jobs in a queue (including processing)</td>\n<td><code>{ &quot;pending&quot;: [Job], &quot;processing&quot;: [Job], &quot;retry_scheduled&quot;: [Job] }</code></td>\n<td>Find specific stuck jobs, verify job ordering</td>\n</tr>\n<tr>\n<td><code>GET /api/debug/worker/{id}/current_state</code></td>\n<td>Detailed worker internal state</td>\n<td><code>Worker</code> object with private fields</td>\n<td>See what job a worker is executing, its thread states</td>\n</tr>\n<tr>\n<td><code>GET /api/debug/job/{id}/trace</code></td>\n<td>Complete job lifecycle trace</td>\n<td><code>{ &quot;events&quot;: [{&quot;timestamp&quot;: &quot;...&quot;, &quot;state&quot;: &quot;...&quot;, &quot;detail&quot;: &quot;...&quot;}] }</code></td>\n<td>Reconstruct job history across retries and workers</td>\n</tr>\n<tr>\n<td><code>POST /api/debug/job/{id}/inspect</code></td>\n<td>Force re-evaluation of job state</td>\n<td><code>{ &quot;current_redis_state&quot;: {...}, &quot;suggested_actions&quot;: [...] }</code></td>\n<td>Diagnose corrupted job state, suggest repair actions</td>\n</tr>\n<tr>\n<td><code>GET /api/debug/redis/keys</code></td>\n<td>Scan Redis keys with pattern</td>\n<td><code>{ &quot;pattern&quot;: &quot;...&quot;, &quot;keys&quot;: [...], &quot;ttls&quot;: {...} }</code></td>\n<td>Find orphaned keys, check TTL expiration</td>\n</tr>\n<tr>\n<td><code>GET /api/debug/metrics/raw</code></td>\n<td>Raw metrics before aggregation</td>\n<td><code>{ &quot;timestamp&quot;: &quot;...&quot;, &quot;counters&quot;: {...}, &quot;gauges&quot;: {...} }</code></td>\n<td>Verify metric collection vs aggregation discrepancies</td>\n</tr>\n</tbody></table>\n<h4 id=\"logging-strategy-for-debugging\">Logging Strategy for Debugging</h4>\n<p>Strategic logging transforms opaque failures into diagnosable events. Implement these logging practices:</p>\n<ol>\n<li><p><strong>Structured JSON Logging</strong>: Each log entry includes:</p>\n<ul>\n<li><code>component</code>: <code>&quot;queue_manager&quot;</code>, <code>&quot;worker&quot;</code>, <code>&quot;retry_manager&quot;</code>, etc.</li>\n<li><code>job_id</code>: When relevant, the job identifier</li>\n<li><code>worker_id</code>: When relevant, the worker identifier</li>\n<li><code>queue</code>: Queue name involved</li>\n<li><code>event</code>: Specific event type (e.g., <code>&quot;job_enqueued&quot;</code>, <code>&quot;job_failed&quot;</code>, <code>&quot;retry_scheduled&quot;</code>)</li>\n<li><code>timestamp</code>: High-resolution timestamp</li>\n<li><code>level</code>: Log level (DEBUG, INFO, WARNING, ERROR)</li>\n</ul>\n</li>\n<li><p><strong>Correlation IDs</strong>: Pass a <code>correlation_id</code> through the entire job lifecycle, from enqueue through all retries to completion. This allows tracing a job&#39;s journey across multiple log entries and components.</p>\n</li>\n<li><p><strong>Debug-Level Verbosity</strong>: Enable detailed debug logging for specific components without overwhelming production logs. Example debug events:</p>\n<ul>\n<li>Queue polling decisions (which queue was selected and why)</li>\n<li>Redis command execution and timing</li>\n<li>State machine transitions</li>\n<li>Lock acquisition and release</li>\n</ul>\n</li>\n<li><p><strong>Health Check Logging</strong>: Workers and schedulers should log periodic health status including:</p>\n<ul>\n<li>Memory usage (<code>psutil.Process().memory_info().rss</code>)</li>\n<li>Queue depths being monitored</li>\n<li>Uptime and processed job counts</li>\n<li>Redis connection latency</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"external-diagnostic-tools\">External Diagnostic Tools</h4>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>Purpose</th>\n<th>Command Examples</th>\n<th>Interpretation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>redis-cli</strong></td>\n<td>Direct Redis inspection</td>\n<td><code>redis-cli -h host -p port</code><br><code>INFO stats</code> for operations<br><code>SLOWLOG GET 10</code> for slow commands</td>\n<td>See command counts, connected clients, memory usage, slow operations</td>\n</tr>\n<tr>\n<td><strong>netstat/ss</strong></td>\n<td>Network connection diagnosis</td>\n<td><code>netstat -tulpn | grep 6379</code><br><code>ss -t state established &#39;( dport = :6379 )&#39;</code></td>\n<td>Verify worker connections to Redis, check for connection leaks</td>\n</tr>\n<tr>\n<td><strong>strace/dtrace</strong></td>\n<td>System call tracing</td>\n<td><code>strace -p &lt;worker_pid&gt; -f -e poll,select,read,write</code><br><code>sudo dtrace -n &#39;syscall::read:entry /pid == 12345/ { printf(&quot;%s&quot;, arg0); }&#39;</code></td>\n<td>See if process is blocked on I/O, identify hanging system calls</td>\n</tr>\n<tr>\n<td><strong>py-spy</strong> (Python)</td>\n<td>CPU profiling and sampling</td>\n<td><code>py-spy top --pid &lt;worker_pid&gt;</code><br><code>py-spy dump --pid &lt;worker_pid&gt;</code></td>\n<td>Identify CPU hotspots in worker code, view real-time stack traces</td>\n</tr>\n<tr>\n<td><strong>memory-profiler</strong> (Python)</td>\n<td>Memory usage tracking</td>\n<td>Add <code>@profile</code> decorator to functions, run with <code>mprof</code></td>\n<td>Find memory leaks in job handlers or queue processing</td>\n</tr>\n<tr>\n<td><strong>tcpdump</strong></td>\n<td>Network traffic analysis</td>\n<td><code>sudo tcpdump -i any port 6379 -w redis.pcap</code><br><code>tcpflow -c port 6379</code></td>\n<td>Debug Redis protocol issues, inspect actual commands sent</td>\n</tr>\n</tbody></table>\n<h4 id=\"interactive-debugging-session-protocol\">Interactive Debugging Session Protocol</h4>\n<p>When faced with a complex bug, follow this structured protocol:</p>\n<ol>\n<li><p><strong>Reproduce the Issue</strong>: </p>\n<ul>\n<li>Can you reproduce it consistently? If not, what are the conditions?</li>\n<li>Create a minimal test case that triggers the bug.</li>\n</ul>\n</li>\n<li><p><strong>Gather Baseline Information</strong>:</p>\n<ul>\n<li>System time synchronization (<code>ntpq -p</code> or <code>timedatectl</code>)</li>\n<li>Redis version and configuration (<code>redis-cli INFO server</code>)</li>\n<li>Worker configurations and versions</li>\n<li>Recent deployments or configuration changes</li>\n</ul>\n</li>\n<li><p><strong>Enable Enhanced Logging</strong>:</p>\n<ul>\n<li>Temporarily increase log level to DEBUG for affected components</li>\n<li>Add specific trace logs around the suspected area</li>\n<li>Capture Redis MONITOR output briefly (caution: performance impact)</li>\n</ul>\n</li>\n<li><p><strong>Isolate the Component</strong>:</p>\n<ul>\n<li>Does the issue occur with a single worker vs multiple?</li>\n<li>Does it affect all queues or just one?</li>\n<li>Is it time-dependent (only at certain times)?</li>\n</ul>\n</li>\n<li><p><strong>Check Data Consistency</strong>:</p>\n<ul>\n<li>Verify Redis data structures match expected schemas</li>\n<li>Check for orphaned keys without corresponding metadata</li>\n<li>Validate serialization/deserialization round-trips</li>\n</ul>\n</li>\n<li><p><strong>Form and Test Hypotheses</strong>:</p>\n<ul>\n<li>List possible causes in order of likelihood</li>\n<li>Design tests to eliminate each hypothesis</li>\n<li>Use the dashboard&#39;s debug endpoints to validate</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p><strong>Debugging Insight:</strong> Time-related bugs (scheduling, retries, timeouts) often stem from clock skew between systems. Always verify time synchronization between workers, schedulers, and Redis servers. Use NTP and consider using Redis time (<code>TIME</code> command) as a reference rather than system time.</p>\n</blockquote>\n<h3 id=\"redis-data-inspection-guide\">Redis Data Inspection Guide</h3>\n<p>Redis stores the entire system state. Knowing how to interpret this data is crucial for debugging. This section provides a comprehensive guide to Redis key patterns and how to inspect them.</p>\n<h4 id=\"key-naming-patterns\">Key Naming Patterns</h4>\n<p>The system uses consistent key naming patterns. Understanding these patterns allows you to reconstruct system state from raw Redis data.</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Data Type</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>queue:{name}</code></td>\n<td>List</td>\n<td>Primary job queue (FIFO)</td>\n<td><code>queue:default</code>, <code>queue:high_priority</code></td>\n</tr>\n<tr>\n<td><code>queue:{name}:processing</code></td>\n<td>List</td>\n<td>Jobs currently being processed (temporary)</td>\n<td><code>queue:default:processing</code></td>\n</tr>\n<tr>\n<td><code>job:{id}</code></td>\n<td>Hash</td>\n<td>Job metadata and payload</td>\n<td><code>job:01HMAK5V2ZJX4Q6W8P3B0D9F7G</code></td>\n</tr>\n<tr>\n<td><code>job:{id}:result</code></td>\n<td>String (JSON)</td>\n<td>Job execution result</td>\n<td><code>job:01HMAK5V2ZJX4Q6W8P3B0D9F7G:result</code></td>\n</tr>\n<tr>\n<td><code>job:{id}:errors</code></td>\n<td>List</td>\n<td>Error history for failed job attempts</td>\n<td><code>job:01HMAK5V2ZJX4Q6W8P3B0D9F7G:errors</code></td>\n</tr>\n<tr>\n<td><code>retry:{queue}</code></td>\n<td>Sorted Set</td>\n<td>Scheduled retries (score = execution timestamp)</td>\n<td><code>retry:default</code>, <code>retry:email</code></td>\n</tr>\n<tr>\n<td><code>dead_letter:{queue}</code></td>\n<td>List</td>\n<td>Permanently failed jobs</td>\n<td><code>dead_letter:default</code></td>\n</tr>\n<tr>\n<td><code>schedule:{id}</code></td>\n<td>Hash</td>\n<td>Schedule definition</td>\n<td><code>schedule:daily_report</code></td>\n</tr>\n<tr>\n<td><code>schedule:{id}:history</code></td>\n<td>List</td>\n<td>Schedule execution history</td>\n<td><code>schedule:daily_report:history</code></td>\n</tr>\n<tr>\n<td><code>worker:{id}</code></td>\n<td>Hash</td>\n<td>Worker heartbeat and status</td>\n<td><code>worker:host123-pid4567</code></td>\n</tr>\n<tr>\n<td><code>worker:{id}:jobs</code></td>\n<td>List</td>\n<td>Recent jobs processed by worker</td>\n<td><code>worker:host123-pid4567:jobs</code></td>\n</tr>\n<tr>\n<td><code>metrics:queue:{name}:depth</code></td>\n<td>String</td>\n<td>Current queue depth (updated periodically)</td>\n<td><code>metrics:queue:default:depth</code></td>\n</tr>\n<tr>\n<td><code>metrics:queue:{name}:throughput</code></td>\n<td>Sorted Set</td>\n<td>Timestamped throughput samples</td>\n<td><code>metrics:queue:default:throughput</code></td>\n</tr>\n<tr>\n<td><code>alert:{id}</code></td>\n<td>Hash</td>\n<td>Active alert information</td>\n<td><code>alert:queue_depth_high_01HMAK5V2Z</code></td>\n</tr>\n<tr>\n<td><code>unique:{key}</code></td>\n<td>String</td>\n<td>Unique job constraint lock</td>\n<td><code>unique:daily_report_2024_01_15</code></td>\n</tr>\n<tr>\n<td><code>config:{component}</code></td>\n<td>Hash</td>\n<td>Runtime configuration</td>\n<td><code>config:system</code>, <code>config:alert_rules</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"inspection-commands-and-scripts\">Inspection Commands and Scripts</h4>\n<p>Use these Redis commands to inspect system health and diagnose issues:</p>\n<p><strong>Basic Queue Inspection:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check queue lengths</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> LLEN</span><span style=\"color:#9ECBFF\"> queue:default</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> LLEN</span><span style=\"color:#9ECBFF\"> queue:high_priority</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Peek at next jobs in queue (without removing)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> LRANGE</span><span style=\"color:#9ECBFF\"> queue:default</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> while</span><span style=\"color:#79B8FF\"> read</span><span style=\"color:#9ECBFF\"> line</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#79B8FF\"> echo</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#E1E4E8\">$line</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> python3</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> json.tool</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check processing queue for stuck jobs</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> LLEN</span><span style=\"color:#9ECBFF\"> queue:default:processing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> LRANGE</span><span style=\"color:#9ECBFF\"> queue:default:processing</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#79B8FF\"> -1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check retry schedule</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> ZRANGE</span><span style=\"color:#9ECBFF\"> retry:default</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#79B8FF\"> -1</span><span style=\"color:#9ECBFF\"> WITHSCORES</span></span></code></pre></div>\n\n<p><strong>Job State Investigation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Find all jobs in a particular state</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#79B8FF\"> --scan</span><span style=\"color:#79B8FF\"> --pattern</span><span style=\"color:#9ECBFF\"> 'job:*'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> while</span><span style=\"color:#79B8FF\"> read</span><span style=\"color:#9ECBFF\"> key</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> HGET</span><span style=\"color:#E1E4E8\"> $key </span><span style=\"color:#9ECBFF\">status</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  if</span><span style=\"color:#E1E4E8\"> [ </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">$status</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '\"ACTIVE\"'</span><span style=\"color:#E1E4E8\"> ]; </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    echo</span><span style=\"color:#9ECBFF\"> \"Active job: </span><span style=\"color:#E1E4E8\">$key</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    redis-cli</span><span style=\"color:#9ECBFF\"> HGET</span><span style=\"color:#E1E4E8\"> $key </span><span style=\"color:#9ECBFF\">started_at</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  fi</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Get complete job details</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> HGETALL</span><span style=\"color:#9ECBFF\"> job:01HMAK5V2ZJX4Q6W8P3B0D9F7G</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check job errors</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> LRANGE</span><span style=\"color:#9ECBFF\"> job:01HMAK5V2ZJX4Q6W8P3B0D9F7G:errors</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#79B8FF\"> -1</span></span></code></pre></div>\n\n<p><strong>Worker Health Check:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># List all active workers</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#79B8FF\"> --scan</span><span style=\"color:#79B8FF\"> --pattern</span><span style=\"color:#9ECBFF\"> 'worker:*'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ':jobs$'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> while</span><span style=\"color:#79B8FF\"> read</span><span style=\"color:#9ECBFF\"> key</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  heartbeat</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> HGET</span><span style=\"color:#E1E4E8\"> $key </span><span style=\"color:#9ECBFF\">last_heartbeat</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  echo</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#E1E4E8\">$key</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#E1E4E8\">$heartbeat</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check for stale workers (heartbeat > 60 seconds ago)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">current_time</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">date</span><span style=\"color:#9ECBFF\"> +%s</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#79B8FF\"> --scan</span><span style=\"color:#79B8FF\"> --pattern</span><span style=\"color:#9ECBFF\"> 'worker:*'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ':jobs$'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> while</span><span style=\"color:#79B8FF\"> read</span><span style=\"color:#9ECBFF\"> key</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  heartbeat</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> HGET</span><span style=\"color:#E1E4E8\"> $key </span><span style=\"color:#9ECBFF\">last_heartbeat</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  if</span><span style=\"color:#E1E4E8\"> [ </span><span style=\"color:#F97583\">!</span><span style=\"color:#F97583\"> -z</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#E1E4E8\">$heartbeat</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\"> ]; </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    age</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$((</span><span style=\"color:#B392F0\">current_time</span><span style=\"color:#9ECBFF\"> -</span><span style=\"color:#9ECBFF\"> heartbeat</span><span style=\"color:#E1E4E8\">));</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> [ $age </span><span style=\"color:#F97583\">-gt</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#E1E4E8\"> ]; </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      echo</span><span style=\"color:#9ECBFF\"> \"STALE WORKER: </span><span style=\"color:#E1E4E8\">$key</span><span style=\"color:#9ECBFF\"> (age: ${</span><span style=\"color:#E1E4E8\">age</span><span style=\"color:#9ECBFF\">}s)\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    fi</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  fi</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span></code></pre></div>\n\n<p><strong>Scheduler State Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check all schedules</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#79B8FF\"> --scan</span><span style=\"color:#79B8FF\"> --pattern</span><span style=\"color:#9ECBFF\"> 'schedule:*'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ':history$'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> while</span><span style=\"color:#79B8FF\"> read</span><span style=\"color:#9ECBFF\"> key</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  next_run</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> HGET</span><span style=\"color:#E1E4E8\"> $key </span><span style=\"color:#9ECBFF\">next_run_at</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  echo</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#E1E4E8\">$key</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#E1E4E8\">$next_run</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check schedule history</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> LRANGE</span><span style=\"color:#9ECBFF\"> schedule:daily_report:history</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#79B8FF\"> -1</span></span></code></pre></div>\n\n<p><strong>Memory Usage Analysis:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check Redis memory info</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> INFO</span><span style=\"color:#9ECBFF\"> memory</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Find largest keys</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#79B8FF\"> --scan</span><span style=\"color:#79B8FF\"> --pattern</span><span style=\"color:#9ECBFF\"> '*'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> while</span><span style=\"color:#79B8FF\"> read</span><span style=\"color:#9ECBFF\"> key</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  size</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> MEMORY</span><span style=\"color:#9ECBFF\"> USAGE</span><span style=\"color:#E1E4E8\"> $key);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  echo</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#E1E4E8\">$size</span><span style=\"color:#E1E4E8\"> $key</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> sort</span><span style=\"color:#79B8FF\"> -n</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> tail</span><span style=\"color:#79B8FF\"> -20</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check key TTLs</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#79B8FF\"> --scan</span><span style=\"color:#79B8FF\"> --pattern</span><span style=\"color:#9ECBFF\"> 'job:*:result'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> while</span><span style=\"color:#79B8FF\"> read</span><span style=\"color:#9ECBFF\"> key</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  ttl</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">redis-cli</span><span style=\"color:#9ECBFF\"> TTL</span><span style=\"color:#E1E4E8\"> $key);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  if</span><span style=\"color:#E1E4E8\"> [ $ttl </span><span style=\"color:#F97583\">-eq</span><span style=\"color:#79B8FF\"> -1</span><span style=\"color:#E1E4E8\"> ]; </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    echo</span><span style=\"color:#9ECBFF\"> \"NO TTL: </span><span style=\"color:#E1E4E8\">$key</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  fi</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span></code></pre></div>\n\n<h4 id=\"common-redis-data-anomalies\">Common Redis Data Anomalies</h4>\n<table>\n<thead>\n<tr>\n<th>Anomaly</th>\n<th>Detection Method</th>\n<th>Root Cause</th>\n<th>Repair Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Orphaned processing jobs</strong></td>\n<td><code>LLEN queue:*:processing</code> &gt; 0 with no corresponding active worker</td>\n<td>Worker crash during job execution</td>\n<td>Run <code>Janitor.run_once()</code> or manually move jobs: <code>redis-cli RPOPLPUSH queue:default:processing queue:default</code></td>\n</tr>\n<tr>\n<td><strong>Retry scores in past</strong></td>\n<td><code>ZRANGE retry:default 0 0 WITHSCORES</code> shows score &lt; current time</td>\n<td>Scheduler not running or too slow</td>\n<td>Manually enqueue: <code>redis-cli ZRANGE retry:default 0 0</code> to get job ID, then requeue</td>\n</tr>\n<tr>\n<td><strong>Duplicate unique keys</strong></td>\n<td><code>redis-cli GET unique:{key}</code> exists but schedule not executed</td>\n<td>Race condition in uniqueness check</td>\n<td>Delete key: <code>redis-cli DEL unique:{key}</code> and manually enqueue job</td>\n</tr>\n<tr>\n<td><strong>Job hash missing fields</strong></td>\n<td><code>redis-cli HGETALL job:{id}</code> shows incomplete field set</td>\n<td>Serialization bug or partial update</td>\n<td>Reconstruct from backup or mark as failed: <code>redis-cli HSET job:{id} status &#39;&quot;FAILED&quot;&#39;</code></td>\n</tr>\n<tr>\n<td><strong>Metrics keys without TTL</strong></td>\n<td><code>redis-cli TTL metrics:queue:default:depth</code> returns -1</td>\n<td>Metrics aggregation not setting expiration</td>\n<td>Set TTL: <code>redis-cli EXPIRE metrics:queue:default:depth 3600</code></td>\n</tr>\n<tr>\n<td><strong>Zombie worker entries</strong></td>\n<td><code>worker:{id}</code> exists but process is dead</td>\n<td>Worker crash without cleanup</td>\n<td>Remove entry: <code>redis-cli DEL worker:{id} worker:{id}:jobs</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"redis-lua-scripts-for-complex-diagnostics\">Redis Lua Scripts for Complex Diagnostics</h4>\n<p>For complex diagnostics, use Redis Lua scripts to atomically check conditions:</p>\n<p><strong>Script to find jobs stuck in processing with dead workers:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- find_stale_processing.lua</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> processing_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'KEYS'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'queue:*:processing'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'TIME'</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _, key </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> ipairs</span><span style=\"color:#E1E4E8\">(processing_keys) </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  local</span><span style=\"color:#E1E4E8\"> queue_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> string.match</span><span style=\"color:#E1E4E8\">(key, </span><span style=\"color:#9ECBFF\">'queue:(.+):processing'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  local</span><span style=\"color:#E1E4E8\"> worker_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'worker:active:' </span><span style=\"color:#F97583\">..</span><span style=\"color:#E1E4E8\"> queue_name  </span><span style=\"color:#6A737D\">-- hypothetical worker mapping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">  -- Check if worker for this queue is alive</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  local</span><span style=\"color:#E1E4E8\"> worker_alive </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'EXISTS'</span><span style=\"color:#E1E4E8\">, worker_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  if</span><span style=\"color:#E1E4E8\"> worker_alive </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> then</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    local</span><span style=\"color:#E1E4E8\"> job_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'LLEN'</span><span style=\"color:#E1E4E8\">, key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> job_count </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> then</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      table.insert</span><span style=\"color:#E1E4E8\">(results, {queue_name, job_count, key})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">end</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> results</span></span></code></pre></div>\n\n<p><strong>Script to validate job state consistency:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- validate_job_state.lua</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ARGV[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> job_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'job:' </span><span style=\"color:#F97583\">..</span><span style=\"color:#E1E4E8\"> job_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> job_status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'HGET'</span><span style=\"color:#E1E4E8\">, job_key, </span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> inconsistencies </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Check if job is ACTIVE but not in any processing queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> job_status </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"ACTIVE\"' </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  local</span><span style=\"color:#E1E4E8\"> found </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  local</span><span style=\"color:#E1E4E8\"> processing_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'KEYS'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'queue:*:processing'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  for</span><span style=\"color:#E1E4E8\"> _, key </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> ipairs</span><span style=\"color:#E1E4E8\">(processing_keys) </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    local</span><span style=\"color:#E1E4E8\"> items </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'LRANGE'</span><span style=\"color:#E1E4E8\">, key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, item </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> ipairs</span><span style=\"color:#E1E4E8\">(items) </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      if</span><span style=\"color:#79B8FF\"> string.find</span><span style=\"color:#E1E4E8\">(item, job_id) </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        found </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        break</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> found </span><span style=\"color:#F97583\">then</span><span style=\"color:#F97583\"> break</span><span style=\"color:#F97583\"> end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  end</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> found </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    table.insert</span><span style=\"color:#E1E4E8\">(inconsistencies, </span><span style=\"color:#9ECBFF\">'Job marked ACTIVE but not in any processing queue'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">end</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Check if job is PENDING but not in any main queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> job_status </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"PENDING\"' </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">  -- Similar logic checking main queues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">end</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> inconsistencies</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>Redis Inspection Principle:</strong> Redis is the single source of truth for the system. When in doubt, examine Redis data directly. The dashboard and logs are derived views that might have their own bugs or lag. Always validate against Redis state.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete tools and code to implement the debugging capabilities described above.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Diagnostic Need</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log aggregation</td>\n<td>Structured JSON to files + <code>tail -f</code></td>\n<td>Elasticsearch + Logstash + Kibana (ELK)</td>\n</tr>\n<tr>\n<td>Metrics collection</td>\n<td>Redis time-series with periodic aggregation</td>\n<td>Prometheus + Grafana for real-time metrics</td>\n</tr>\n<tr>\n<td>Distributed tracing</td>\n<td>Correlation IDs in logs</td>\n<td>OpenTelemetry with Jaeger/Zipkin</td>\n</tr>\n<tr>\n<td>Interactive debugging</td>\n<td>Python debugger (pdb)</td>\n<td>Remote debugging with debugpy</td>\n</tr>\n<tr>\n<td>Performance profiling</td>\n<td>cProfile for CPU, memory-profiler for memory</td>\n<td>Py-Spy for sampling, Scalene for comprehensive</td>\n</tr>\n<tr>\n<td>Alerting</td>\n<td>Simple threshold checks in scheduler</td>\n<td>Prometheus Alertmanager with complex rules</td>\n</tr>\n</tbody></table>\n<h4 id=\"debug-endpoint-implementation\">Debug Endpoint Implementation</h4>\n<p>Create a dedicated debug module with these endpoints (protected in production):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debug_api.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> fastapi </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> APIRouter, HTTPException, BackgroundTasks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Job, JobStatus, Worker, QueueMetrics, SystemConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueueManager, WorkerConfig, RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .janitor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Janitor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">router </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> APIRouter(</span><span style=\"color:#FFAB70\">prefix</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"/api/debug\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tags</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"debug\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@router.get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/queue/</span><span style=\"color:#79B8FF\">{queue_name}</span><span style=\"color:#9ECBFF\">/contents\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get_queue_contents</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    include_processing: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    include_retry: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Inspect all jobs in a queue, including processing and retry queues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Useful for diagnosing stuck jobs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get pending jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pending_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"queue:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pending_jobs_raw </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.lrange(pending_key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, limit </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pending_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> job_data </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> pending_jobs_raw:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Job.deserialize(job_data.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pending_jobs.append(job.to_dict())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pending_jobs.append({</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e), </span><span style=\"color:#9ECBFF\">\"raw\"</span><span style=\"color:#E1E4E8\">: job_data[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"pending\"</span><span style=\"color:#E1E4E8\">: pending_jobs}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get processing jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> include_processing:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        processing_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"queue:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:processing\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        processing_jobs_raw </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.lrange(processing_key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, limit </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        processing_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> job_data </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> processing_jobs_raw:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Job.deserialize(job_data.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                processing_jobs.append(job.to_dict())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                processing_jobs.append({</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e), </span><span style=\"color:#9ECBFF\">\"raw\"</span><span style=\"color:#E1E4E8\">: job_data[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result[</span><span style=\"color:#9ECBFF\">\"processing\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> processing_jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get retry scheduled jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> include_retry:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        retry_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"retry:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        retry_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.zrange(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            retry_key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, limit </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">withscores</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        retry_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> job_data, score </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> retry_jobs:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Job.deserialize(job_data.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                retry_list.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"job\"</span><span style=\"color:#E1E4E8\">: job.to_dict(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"scheduled_for\"</span><span style=\"color:#E1E4E8\">: datetime.fromtimestamp(score)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                retry_list.append({</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e), </span><span style=\"color:#9ECBFF\">\"raw\"</span><span style=\"color:#E1E4E8\">: job_data[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result[</span><span style=\"color:#9ECBFF\">\"retry_scheduled\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retry_list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@router.get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/job/</span><span style=\"color:#79B8FF\">{job_id}</span><span style=\"color:#9ECBFF\">/trace\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get_job_trace</span><span style=\"color:#E1E4E8\">(job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Reconstruct the complete lifecycle of a job.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get job metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.hgetall(job_key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> job_data:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#E1E4E8\"> HTTPException(</span><span style=\"color:#FFAB70\">status_code</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">404</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">detail</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Job not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Decode byte strings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_data_decoded </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        k.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">): v.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> job_data.items()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get error history</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:errors\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.lrange(errors_key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> error_json </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> errors:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            error_history.append(json.loads(error_json.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">)))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            error_history.append({</span><span style=\"color:#9ECBFF\">\"raw\"</span><span style=\"color:#E1E4E8\">: error_json.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'replace'</span><span style=\"color:#E1E4E8\">)})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get result if exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:result\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.get(result_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> result:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(result.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"raw\"</span><span style=\"color:#E1E4E8\">: result.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'replace'</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Reconstruct timeline from metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeline </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Created event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#9ECBFF\"> 'created_at'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> job_data_decoded:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeline.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: job_data_decoded[</span><span style=\"color:#9ECBFF\">'created_at'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"event\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"created\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"queue\"</span><span style=\"color:#E1E4E8\">: job_data_decoded.get(</span><span style=\"color:#9ECBFF\">'queue'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Started event  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#9ECBFF\"> 'started_at'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> job_data_decoded:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeline.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: job_data_decoded[</span><span style=\"color:#9ECBFF\">'started_at'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"event\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"started\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ACTIVE\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Error events</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, error </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(error_history):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeline.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: error.get(</span><span style=\"color:#9ECBFF\">'timestamp'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"event\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"failed_attempt_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"error\"</span><span style=\"color:#E1E4E8\">: error.get(</span><span style=\"color:#9ECBFF\">'error_type'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Unknown'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"message\"</span><span style=\"color:#E1E4E8\">: error.get(</span><span style=\"color:#9ECBFF\">'message'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Completed event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#9ECBFF\"> 'completed_at'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> job_data_decoded:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeline.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: job_data_decoded[</span><span style=\"color:#9ECBFF\">'completed_at'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"event\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"completed\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"status\"</span><span style=\"color:#E1E4E8\">: job_data_decoded.get(</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sort timeline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeline.sort(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x.get(</span><span style=\"color:#9ECBFF\">'timestamp'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"job_id\"</span><span style=\"color:#E1E4E8\">: job_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"metadata\"</span><span style=\"color:#E1E4E8\">: job_data_decoded,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"error_history\"</span><span style=\"color:#E1E4E8\">: error_history,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"result\"</span><span style=\"color:#E1E4E8\">: result_data,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"timeline\"</span><span style=\"color:#E1E4E8\">: timeline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@router.post</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/janitor/run\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> run_janitor</span><span style=\"color:#E1E4E8\">(background_tasks: BackgroundTasks) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Manually trigger the janitor process to clean up stale jobs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns immediately, runs in background.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> cleanup_task</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        janitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Janitor(RedisClient.get_instance())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        requeued </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> janitor.run_once()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> requeued</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asyncio.create_task(cleanup_task())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    background_tasks.add_task(task)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"started\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Janitor cleanup running in background\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@router.get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/redis/keys\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> scan_redis_keys</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"*\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    with_ttl: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Scan Redis keys matching pattern. Use with caution in production.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Use SCAN instead of KEYS for production safety</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ttls </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor, batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.scan(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cursor</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">cursor, </span><span style=\"color:#FFAB70\">match</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pattern, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        keys.extend([k.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> batch])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> with_ttl:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> batch:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ttl </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client.ttl(key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ttls[key.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">)] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ttl</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cursor </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(keys) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> limit:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> keys[:limit]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"pattern\"</span><span style=\"color:#E1E4E8\">: pattern,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(keys),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"keys\"</span><span style=\"color:#E1E4E8\">: keys,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"ttls\"</span><span style=\"color:#E1E4E8\">: ttls </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> with_ttl </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@router.post</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/worker/</span><span style=\"color:#79B8FF\">{worker_id}</span><span style=\"color:#9ECBFF\">/signal\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> signal_worker</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    signal: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"status\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Send a signal to a worker (status, pause, resume, shutdown).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    In production, this would require authentication and authorization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> signal </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"pause\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"resume\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"shutdown\"</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#E1E4E8\"> HTTPException(</span><span style=\"color:#FFAB70\">status_code</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">400</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">detail</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Invalid signal\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Store signal in Redis for worker to poll</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    signal_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"worker:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:signal\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_client.setex(signal_key, </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">, signal)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"worker_id\"</span><span style=\"color:#E1E4E8\">: worker_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"signal\"</span><span style=\"color:#E1E4E8\">: signal,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"sent_at\"</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span></code></pre></div>\n\n<h4 id=\"diagnostic-worker-implementation\">Diagnostic Worker Implementation</h4>\n<p>Create a diagnostic worker that can be run alongside normal workers to monitor system health:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># diagnostic_worker.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SystemMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_percent: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_rss: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_memory_used: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_connected_clients: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queue_depths: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    worker_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DiagnosticWorker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Runs periodic system diagnostics and logs anomalies.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Not a job worker, but a monitoring agent.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis, interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> collect_metrics</span><span style=\"color:#E1E4E8\">(self) -> SystemMetrics:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Collect comprehensive system metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # System metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cpu_percent </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.cpu_percent(</span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process().memory_info()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Redis metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        redis_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.info()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Queue depths</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue_depths </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue_patterns </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.keys(</span><span style=\"color:#9ECBFF\">\"queue:*\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> queue_patterns:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">\":processing\"</span><span style=\"color:#F97583\"> not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">and</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">\":retry\"</span><span style=\"color:#F97583\"> not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> key:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                queue_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> key.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">).replace(</span><span style=\"color:#9ECBFF\">\"queue:\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                depth </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.llen(key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                queue_depths[queue_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> depth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Worker count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        worker_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.keys(</span><span style=\"color:#9ECBFF\">\"worker:*\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        worker_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">([k </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> worker_keys </span><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">\":jobs\"</span><span style=\"color:#F97583\"> not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> k])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Schedule count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        schedule_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.keys(</span><span style=\"color:#9ECBFF\">\"schedule:*\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        schedule_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">([k </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> schedule_keys </span><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">\":history\"</span><span style=\"color:#F97583\"> not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> k])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> SystemMetrics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_percent</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">cpu_percent,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_rss</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">memory_info.rss,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            redis_memory_used</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">redis_info.get(</span><span style=\"color:#9ECBFF\">'used_memory'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            redis_connected_clients</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">redis_info.get(</span><span style=\"color:#9ECBFF\">'connected_clients'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            queue_depths</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">queue_depths,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            worker_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">worker_count,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            schedule_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">schedule_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_anomalies</span><span style=\"color:#E1E4E8\">(self, metrics: SystemMetrics) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check for system anomalies based on metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        anomalies </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # High memory usage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> metrics.redis_memory_used </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 500</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># 500MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            anomalies.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"high_redis_memory\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"severity\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"warning\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"value\"</span><span style=\"color:#E1E4E8\">: metrics.redis_memory_used,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"threshold\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">500</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Queue depth anomalies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> queue_name, depth </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> metrics.queue_depths.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> depth </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                anomalies.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"high_queue_depth\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"severity\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"warning\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"queue\"</span><span style=\"color:#E1E4E8\">: queue_name,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"depth\"</span><span style=\"color:#E1E4E8\">: depth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # No workers running</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> metrics.worker_count </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            anomalies.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"no_workers\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"severity\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"critical\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"No active workers detected\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Redis connection issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> metrics.redis_connected_clients </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            anomalies.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"redis_no_clients\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"severity\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"critical\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Redis shows no connected clients\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> anomalies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_diagnostics</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run one diagnostic cycle.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.collect_metrics()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            anomalies </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.check_anomalies(metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Log metrics</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis.rpush(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"diagnostics:metrics\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                json.dumps({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"timestamp\"</span><span style=\"color:#E1E4E8\">: metrics.timestamp.isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"metrics\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"cpu_percent\"</span><span style=\"color:#E1E4E8\">: metrics.cpu_percent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"memory_rss\"</span><span style=\"color:#E1E4E8\">: metrics.memory_rss,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"redis_memory\"</span><span style=\"color:#E1E4E8\">: metrics.redis_memory_used,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"worker_count\"</span><span style=\"color:#E1E4E8\">: metrics.worker_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Log anomalies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> anomalies:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> anomaly </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> anomalies:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.redis.rpush(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"diagnostics:anomalies\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        json.dumps({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"timestamp\"</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"anomaly\"</span><span style=\"color:#E1E4E8\">: anomaly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Also log to regular logs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"ANOMALY: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">anomaly</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Set TTL on diagnostics keys to prevent unbounded growth</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis.expire(</span><span style=\"color:#9ECBFF\">\"diagnostics:metrics\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 7 days</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis.expire(</span><span style=\"color:#9ECBFF\">\"diagnostics:anomalies\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Diagnostic error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the diagnostic worker in a background thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._run_loop, </span><span style=\"color:#FFAB70\">daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _run_loop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main diagnostic loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.run_diagnostics()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop the diagnostic worker.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"debugging-helper-scripts\">Debugging Helper Scripts</h4>\n<p>Create command-line scripts for common debugging tasks:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">#!/usr/bin/env python3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># scripts/debug_queue.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Command-line tool to inspect queue state.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> argparse</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> job_processor.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> job_processor.redis_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisClient</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> inspect_queue</span><span style=\"color:#E1E4E8\">(queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Inspect jobs in a queue.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queue_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"queue:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"=== Queue: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ===\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Length: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">redis.llen(queue_key)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Next jobs:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.lrange(queue_key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, limit </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, job_data </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(jobs):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_json </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(job_data.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1}</span><span style=\"color:#9ECBFF\">. </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_json.get(</span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"   Type: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_json.get(</span><span style=\"color:#9ECBFF\">'job_type'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"   Args: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_json.get(</span><span style=\"color:#9ECBFF\">'args'</span><span style=\"color:#E1E4E8\">, [])[:</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(job_json.get(</span><span style=\"color:#9ECBFF\">'args'</span><span style=\"color:#E1E4E8\">, [])) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"   ... and </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(job_json[</span><span style=\"color:#9ECBFF\">'args'</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 3}</span><span style=\"color:#9ECBFF\"> more\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> json.JSONDecodeError:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1}</span><span style=\"color:#9ECBFF\">. INVALID JSON: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_data[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> find_stuck_jobs</span><span style=\"color:#E1E4E8\">(age_hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Find jobs that have been active for too long.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Scan for all job keys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stuck_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor, keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.scan(cursor, </span><span style=\"color:#FFAB70\">match</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"job:*\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> keys:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Skip subkeys like job:{id}:errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">\":\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> key.count(</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">\":\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.hgetall(key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> job_data:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Decode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            decoded </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> job_data.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    decoded[k.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">)] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    decoded[k.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'replace'</span><span style=\"color:#E1E4E8\">)] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(v)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check if ACTIVE for too long</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> decoded.get(</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"ACTIVE\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                started_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> decoded.get(</span><span style=\"color:#9ECBFF\">'started_at'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> started_at:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # Parse timestamp - might be string in JSON format</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        if</span><span style=\"color:#E1E4E8\"> started_at.startswith(</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            started_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> started_at[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse datetime and compare with age_hours</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # For now just report</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        stuck_jobs.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            'job_id'</span><span style=\"color:#E1E4E8\">: decoded.get(</span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            'started_at'</span><span style=\"color:#E1E4E8\">: started_at,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            'queue'</span><span style=\"color:#E1E4E8\">: decoded.get(</span><span style=\"color:#9ECBFF\">'queue'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cursor </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> stuck_jobs:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Found </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(stuck_jobs)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> potentially stuck jobs:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> job </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> stuck_jobs:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job[</span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job[</span><span style=\"color:#9ECBFF\">'queue'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> since </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job[</span><span style=\"color:#9ECBFF\">'started_at'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"No stuck jobs found.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cleanup_processing_queues</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Clean up jobs stuck in processing queues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisClient.get_instance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Find all processing queues</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processing_queues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor, keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.scan(cursor, </span><span style=\"color:#FFAB70\">match</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"queue:*:processing\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        processing_queues.extend([k.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> keys])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cursor </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_moved </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> queue_key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> processing_queues:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract main queue name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        main_queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_key.replace(</span><span style=\"color:#9ECBFF\">\":processing\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Move all jobs back to main queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.rpoplpush(queue_key, main_queue)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> job_data:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            total_moved </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cleaned </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Total jobs requeued: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">total_moved</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> argparse.ArgumentParser(</span><span style=\"color:#FFAB70\">description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Debug job queue system\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    subparsers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.add_subparsers(</span><span style=\"color:#FFAB70\">dest</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'command'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Command'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # inspect command</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inspect_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> subparsers.add_parser(</span><span style=\"color:#9ECBFF\">'inspect'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Inspect a queue'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inspect_parser.add_argument(</span><span style=\"color:#9ECBFF\">'queue'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Queue name'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inspect_parser.add_argument(</span><span style=\"color:#9ECBFF\">'--limit'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">type</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Number of jobs to show'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # stuck command</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stuck_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> subparsers.add_parser(</span><span style=\"color:#9ECBFF\">'stuck'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Find stuck jobs'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stuck_parser.add_argument(</span><span style=\"color:#9ECBFF\">'--age-hours'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">type</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Age threshold in hours'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # cleanup command</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> subparsers.add_parser(</span><span style=\"color:#9ECBFF\">'cleanup'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Cleanup processing queues'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse_args()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> args.command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'inspect'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inspect_queue(args.queue, args.limit)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> args.command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'stuck'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        find_stuck_jobs(args.age_hours)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> args.command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'cleanup'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cleanup_processing_queues()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser.print_help()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    main()</span></span></code></pre></div>\n\n<h4 id=\"language-specific-debugging-hints\">Language-Specific Debugging Hints</h4>\n<p><strong>Python-Specific Debugging:</strong></p>\n<ol>\n<li><p><strong>Async/Await Deadlocks</strong>: Use <code>asyncio.all_tasks()</code> to see all running tasks when debugging hangs in async workers.</p>\n</li>\n<li><p><strong>Memory Leaks</strong>: Use <code>tracemalloc</code> to track memory allocations:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> tracemalloc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   tracemalloc.start()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # ... run operations ...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   snapshot </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracemalloc.take_snapshot()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   top_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> snapshot.statistics(</span><span style=\"color:#9ECBFF\">'lineno'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   for</span><span style=\"color:#E1E4E8\"> stat </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> top_stats[:</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       print</span><span style=\"color:#E1E4E8\">(stat)</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Thread Dumps</strong>: For hung workers, send <code>SIGUSR1</code> to get a Python traceback of all threads:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> signal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> debug</span><span style=\"color:#E1E4E8\">(sig, frame):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"\"\"Interrupt running process and provide a python prompt for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       import</span><span style=\"color:#E1E4E8\"> pdb</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       pdb.Pdb().set_trace(frame)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   signal.signal(signal.</span><span style=\"color:#79B8FF\">SIGUSR1</span><span style=\"color:#E1E4E8\">, debug)  </span><span style=\"color:#6A737D\"># Register handler</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Pickle Serialization Issues</strong>: When debugging job serialization, test round-trips:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   import</span><span style=\"color:#E1E4E8\"> pickle</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   job_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job.to_dict()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   serialized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pickle.dumps(job_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   deserialized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pickle.loads(serialized)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   assert</span><span style=\"color:#E1E4E8\"> job_dict </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> deserialized  </span><span style=\"color:#6A737D\"># Verify no data loss</span></span></code></pre></div>\n\n<h4 id=\"milestone-debugging-checkpoints\">Milestone Debugging Checkpoints</h4>\n<p>After implementing each milestone, verify these debugging capabilities work:</p>\n<p><strong>Milestone 1 (Queue Core) Checkpoint:</strong></p>\n<ul>\n<li>Run <code>python scripts/debug_queue.py inspect default</code> - should show enqueued jobs</li>\n<li>Enqueue a job &gt;1MB - should get validation error in logs</li>\n<li>Kill Redis temporarily during enqueue - should see connection error handling</li>\n</ul>\n<p><strong>Milestone 2 (Worker) Checkpoint:</strong></p>\n<ul>\n<li>Start worker, send <code>SIGTERM</code> - should complete current job before exiting</li>\n<li>Check <code>redis-cli HGETALL worker:{id}</code> - should see heartbeat updating</li>\n<li>Kill worker mid-job - janitor should move job back to queue</li>\n</ul>\n<p><strong>Milestone 3 (Retry) Checkpoint:</strong></p>\n<ul>\n<li>Create job that always fails - watch retry count increment in job metadata</li>\n<li>Check <code>redis-cli ZRANGE retry:default 0 -1 WITHSCORES</code> - should see scheduled retries</li>\n<li>Exhaust retries - job should move to dead letter queue</li>\n</ul>\n<p><strong>Milestone 4 (Scheduler) Checkpoint:</strong></p>\n<ul>\n<li>Create schedule for 1 minute in future - watch it enqueue at correct time</li>\n<li>Check <code>redis-cli HGETALL schedule:{id}</code> - <code>next_run_at</code> should update</li>\n<li>Change system timezone - schedule should adjust correctly</li>\n</ul>\n<p><strong>Milestone 5 (Monitoring) Checkpoint:</strong></p>\n<ul>\n<li>Access <code>/api/debug/queue/default/contents</code> - should see queue contents</li>\n<li>Check WebSocket connection to metrics stream - should receive updates</li>\n<li>Trigger alert condition - should see alert in dashboard</li>\n</ul>\n<blockquote>\n<p><strong>Implementation Insight:</strong> Build debugging capabilities alongside features, not as an afterthought. Each component should expose its internal state for inspection. The time spent building debug tools pays back multifold when diagnosing production issues.</p>\n</blockquote>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section explores potential enhancements beyond the five core milestones. It demonstrates how the architecture&#39;s design decisions enable natural evolution while identifying boundaries where significant rework would be required.</p>\n</blockquote>\n<p>A well-architected system should be both complete in its current form and extensible for future needs. This section examines enhancements that could be built upon the current background job processor, categorized by the level of architectural change required. The core design—with its clear separation of concerns, well-defined interfaces, and Redis-based coordination—enables numerous extensions without fundamental rework, adhering to the <strong>YAGNI</strong> principle while maintaining forward compatibility.</p>\n<h3 id=\"architecture-enabled-extensions\">Architecture-Enabled Extensions</h3>\n<p>These enhancements leverage existing architectural patterns, data structures, and component boundaries. They require minimal to moderate development effort and don&#39;t necessitate breaking changes to core interfaces or data models.</p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Description</th>\n<th>Implementation Approach</th>\n<th>Architectural Enablers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Rate Limiting</strong></td>\n<td>Control the maximum number of jobs processed per unit time per queue, job type, or worker group to prevent resource exhaustion.</td>\n<td>Extend <code>QueueManager</code> with token bucket or sliding window counters stored in Redis. Workers check limits before processing jobs.</td>\n<td>Redis atomic operations for counters, existing queue isolation, configurable worker polling.</td>\n</tr>\n<tr>\n<td><strong>Job Dependencies</strong></td>\n<td>Allow jobs to specify prerequisite jobs that must complete successfully before dependent jobs can start.</td>\n<td>Add <code>dependencies</code> field to <code>Job</code> storing list of prerequisite job IDs. New <code>DependencyManager</code> component tracks completion and enqueues dependents.</td>\n<td>Job lifecycle tracking in Redis, existing event flow from completion to enqueue.</td>\n</tr>\n<tr>\n<td><strong>Multi-Language Worker Support</strong></td>\n<td>Enable workers written in languages other than Python (Go, Rust, JavaScript) to process jobs from the same queues.</td>\n<td>Standardize job serialization format (JSON/msgpack) and Redis key structures. Create client libraries in each language implementing the same protocol.</td>\n<td>Language-agnostic Redis data model, explicit job serialization format, documented key naming conventions.</td>\n</tr>\n<tr>\n<td><strong>Priority Queues with Preemption</strong></td>\n<td>Allow higher-priority jobs to interrupt lower-priority jobs already being processed.</td>\n<td>Extend <code>Worker</code> to support job suspension/resumption or termination with requeue. Add preemption signal handling in job execution wrapper.</td>\n<td>Job state machine with explicit <code>ACTIVE</code> state, worker heartbeat for liveness, configurable timeout handling.</td>\n</tr>\n<tr>\n<td><strong>Job Result Caching</strong></td>\n<td>Cache job results for idempotent operations to avoid redundant computation for identical inputs.</td>\n<td>Add optional <code>cache_key</code> and <code>cache_ttl</code> fields to <code>Job</code>. <code>QueueManager</code> checks Redis cache before enqueueing, returns cached result if present.</td>\n<td>Redis as shared cache, job serialization includes all input parameters, existing result storage mechanism.</td>\n</tr>\n<tr>\n<td><strong>Job Chaining &amp; Pipelines</strong></td>\n<td>Define sequences of jobs where each job&#39;s output becomes the next job&#39;s input, with error handling across the chain.</td>\n<td>New <code>JobChain</code> object with list of job definitions and error policy. <code>ChainManager</code> monitors execution and propagates data between jobs.</td>\n<td>Job metadata for correlation, error handling infrastructure, job registry for dynamic job type resolution.</td>\n</tr>\n<tr>\n<td><strong>Dynamic Queue Configuration</strong></td>\n<td>Add, remove, or modify queue configurations at runtime without restarting workers or scheduler.</td>\n<td>Extend <code>QueueManager</code> to watch Redis for configuration changes and adjust polling weights. Workers subscribe to configuration updates via Redis Pub/Sub.</td>\n<td>Configuration stored in Redis, worker polling strategy decoupled from static config, heartbeat mechanism for dynamic updates.</td>\n</tr>\n<tr>\n<td><strong>Job Tagging &amp; Metadata Search</strong></td>\n<td>Attach arbitrary tags to jobs and enable searching/filtering by tags in the dashboard and APIs.</td>\n<td>Add <code>tags: Dict[str, str]</code> field to <code>Job</code>. Index tags in Redis sorted sets for efficient querying. Extend dashboard filters.</td>\n<td>Flexible job metadata field, Redis sorted set pattern for indexing, dashboard filtering architecture.</td>\n</tr>\n<tr>\n<td><strong>Worker Resource Profiles</strong></td>\n<td>Assign workers to specific resource pools (CPU-intensive, memory-heavy, GPU) and route jobs accordingly.</td>\n<td>Add <code>resource_profile</code> field to <code>Job</code> and <code>Worker</code>. Extend queue selection logic to match profiles. New resource-specific queues.</td>\n<td>Multiple named queues with priorities, worker queue subscription model, configurable routing rules.</td>\n</tr>\n<tr>\n<td><strong>Delayed Job Cancellation</strong></td>\n<td>Allow producers to cancel or modify delayed/scheduled jobs before they execute.</td>\n<td>Store delayed jobs in Redis with cancellation tokens. Add <code>cancel_job</code> API that marks jobs as cancelled in sorted set.</td>\n<td>Delayed jobs stored in Redis sorted sets with job IDs, scheduler polling with batch retrieval.</td>\n</tr>\n</tbody></table>\n<h4 id=\"detailed-analysis-rate-limiting-extension\">Detailed Analysis: Rate Limiting Extension</h4>\n<blockquote>\n<p><strong>Decision: Token Bucket Rate Limiter per Queue</strong></p>\n<ul>\n<li><strong>Context</strong>: Certain job types (e.g., external API calls, database-intensive operations) must be throttled to prevent overwhelming downstream systems or exhausting shared resources.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Fixed Window Counter</strong>: Simple count per time window, but suffers from boundary effects allowing bursts.</li>\n<li><strong>Sliding Window Log</strong>: Accurate but memory-intensive, storing timestamps for each request.</li>\n<li><strong>Token Bucket</strong>: Smooths rate, allows bursting up to bucket capacity, computationally efficient.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement token bucket algorithm using Redis Lua scripts for atomic operations.</li>\n<li><strong>Rationale</strong>: Token bucket provides predictable smoothing with burst capacity, matches typical rate limiting requirements for external APIs, and Redis Lua scripts ensure atomicity across multiple workers checking limits concurrently.</li>\n<li><strong>Consequences</strong>: Adds small overhead to each job dequeue operation, requires Redis Lua support, provides configurable burst capacity and refill rate.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Rate Limiting Implementation Comparison</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Option</strong></td>\n</tr>\n<tr>\n<td>Fixed Window Counter</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n</tr>\n<tr>\n<td>Token Bucket</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Walkthrough</strong>:</p>\n<ol>\n<li><strong>Configuration Enhancement</strong>: Add <code>rate_limit</code> field to <code>QueueConfig</code> with <code>requests_per_second</code> and <code>burst_capacity</code> parameters.</li>\n<li><strong>Redis Storage</strong>: Use key pattern <code>ratelimit:{queue_name}</code> storing bucket state (tokens, last_update) in a Redis hash.</li>\n<li><strong>Atomic Check Script</strong>: Lua script that:<ol>\n<li>Calculates time elapsed since last update</li>\n<li>Adds tokens based on elapsed time × refill rate (capped at burst capacity)</li>\n<li>If tokens ≥ 1, decrements tokens by 1 and returns <code>ALLOWED</code></li>\n<li>Otherwise returns <code>DENIED</code> with time until next token available</li>\n</ol>\n</li>\n<li><strong>Worker Integration</strong>: Before processing a job from a rate-limited queue, worker executes the Lua script. If <code>DENIED</code>, worker sleeps for the suggested duration before polling again.</li>\n<li><strong>Dashboard Visibility</strong>: Add rate limit status to queue metrics showing current token count, limit status, and throttled worker count.</li>\n</ol>\n<p><strong>Concrete Example</strong>: A webhook queue limited to 10 requests/second with burst capacity of 20. When 25 jobs arrive simultaneously, the first 20 process immediately (burst), then subsequent jobs throttle to 10/second. The token bucket refills at 10 tokens/second, ensuring long-term average compliance.</p>\n<h4 id=\"detailed-analysis-job-dependencies-extension\">Detailed Analysis: Job Dependencies Extension</h4>\n<blockquote>\n<p><strong>Decision: Directed Acyclic Graph (DAG) Dependency Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Complex workflows require sequencing where job B cannot start until job A completes successfully, enabling multi-step data pipelines and business processes.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Linear Chains</strong>: Simple sequential dependencies only (A→B→C), no branching or parallelism.</li>\n<li><strong>DAG with Explicit Graph Definition</strong>: Full dependency graph defined upfront, allowing parallel execution of independent branches.</li>\n<li><strong>Dynamic Dependencies</strong>: Jobs can spawn new dependencies at runtime, more flexible but harder to reason about.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement DAG model with upfront graph definition stored as job metadata.</li>\n<li><strong>Rationale</strong>: DAGs capture most real-world workflows (branching, parallelism), allow static validation of cycles, and enable optimal scheduling of independent branches. Upfront definition simplifies implementation and debugging.</li>\n<li><strong>Consequences</strong>: Requires graph validation to prevent cycles, adds complexity to job completion handling, needs visualization in dashboard.</li>\n</ul>\n</blockquote>\n<p><strong>Implementation Walkthrough</strong>:</p>\n<ol>\n<li><strong>Job Enhancement</strong>: Add <code>dependencies: List[str]</code> field to <code>Job</code> storing prerequisite job IDs, and <code>dependent_jobs: List[str]</code> for reverse links.</li>\n<li><strong>Dependency Manager</strong>: New component that:<ul>\n<li>Validates job graphs are acyclic at enqueue time</li>\n<li>Maintains Redis sets tracking pending dependencies for each job</li>\n<li>Listens for job completion events via Redis Pub/Sub</li>\n<li>When job completes, removes it from dependency sets of dependent jobs</li>\n<li>When all dependencies satisfied for a job, enqueues it via <code>QueueManager</code></li>\n</ul>\n</li>\n<li><strong>Atomic Graph Operations</strong>: Use Redis transactions to atomically update dependency state and enqueue jobs when ready.</li>\n<li><strong>Failure Handling</strong>: If a job fails, dependent jobs can be configured to either (a) not run, (b) run with error placeholder, or (c) run after manual intervention.</li>\n<li><strong>Dashboard Visualization</strong>: Display dependency graphs with status color coding, allow drilling into specific job chains.</li>\n</ol>\n<p><strong>Concrete Example</strong>: Data pipeline with jobs: <code>fetch_data</code> → [<code>clean_data</code>, <code>validate_schema</code>] → <code>merge_results</code>. <code>clean_data</code> and <code>validate_schema</code> run in parallel after <code>fetch_data</code> completes, both must complete before <code>merge_results</code> starts. The dependency manager ensures this ordering automatically.</p>\n<h3 id=\"extensions-requiring-major-changes\">Extensions Requiring Major Changes</h3>\n<p>These enhancements would require significant architectural modifications, breaking changes to interfaces, or fundamental shifts in system assumptions. They represent directions for major version upgrades or entirely new system variants.</p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Description</th>\n<th>Required Architectural Changes</th>\n<th>Rationale for Major Change</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Distributed Transactions Across Jobs</strong></td>\n<td>Ensure atomicity across multiple job executions (all succeed or all roll back) with two-phase commit protocol.</td>\n<td>New transaction coordinator, WAL for transaction state, participant job interface for prepare/commit/rollback, recovery mechanisms for coordinator failures.</td>\n<td>Changes core job execution model from independent to coordinated, requires persistent transaction state, adds complexity to job handlers.</td>\n</tr>\n<tr>\n<td><strong>Multi-Broker Support</strong></td>\n<td>Support alternative brokers beyond Redis (RabbitMQ, Kafka, PostgreSQL) with pluggable backend adapters.</td>\n<td>Abstract broker interface, adapter pattern for each backend, migration of data models between backends, feature compatibility matrix.</td>\n<td>Current architecture deeply coupled to Redis semantics (sorted sets, blocking pops, Lua scripts). Abstracting requires rethinking atomic operations and data structures.</td>\n</tr>\n<tr>\n<td><strong>Exactly-Once Processing Guarantees</strong></td>\n<td>Ensure each job is processed exactly once despite worker failures, network partitions, or duplicate enqueues.</td>\n<td>Idempotency keys with distributed locking, deterministic job ID generation, transaction log for dequeue operations, coordinated checkpointing.</td>\n<td>Conflicts with current at-least-once semantics with retries, requires fundamental changes to dequeue/requeue logic and job deduplication at storage layer.</td>\n</tr>\n<tr>\n<td><strong>Geographic Distribution with Cross-Region Replication</strong></td>\n<td>Deploy job processors across multiple regions with automatic failover and geographic job routing.</td>\n<td>Active-active Redis replication with conflict resolution, latency-based routing, cross-region job migration, distributed lock coordination.</td>\n<td>Assumes single Redis instance; multi-region requires consensus on job ownership, clock synchronization, and replication lag handling.</td>\n</tr>\n<tr>\n<td><strong>Stream Processing Mode</strong></td>\n<td>Process continuous streams of data with windowing, aggregation, and real-time analytics alongside batch jobs.</td>\n<td>New stream processing engine, windowed state storage, watermarking for out-of-order events, integration with job scheduler for batch/stream hybrid.</td>\n<td>Changes from discrete job model to continuous processing model, requires different programming model, state management, and monitoring.</td>\n</tr>\n<tr>\n<td><strong>Machine Learning Model Serving Integration</strong></td>\n<td>Specialized support for ML model inference jobs with GPU allocation, model versioning, and inference batching.</td>\n<td>GPU-aware scheduling, model registry integration, adaptive batching based on queue depth, performance profiling and auto-scaling.</td>\n<td>Requires specialized hardware awareness, performance optimization trade-offs different from CPU jobs, tight coupling with ML infrastructure.</td>\n</tr>\n<tr>\n<td><strong>Workflow Engine with Human Tasks</strong></td>\n<td>Extend job dependencies to include human approval steps, forms, and long-running processes (days/weeks).</td>\n<td>Human task queue, user assignment and notification, form rendering, long-term state persistence, suspension/resumption with context.</td>\n<td>Time scale mismatch (seconds vs days), requires user interface integration, authentication/authorization, different state persistence requirements.</td>\n</tr>\n<tr>\n<td><strong>Serverless Execution Backend</strong></td>\n<td>Execute jobs on serverless platforms (AWS Lambda, Google Cloud Functions) instead of persistent worker processes.</td>\n<td>Job packaging for serverless environments, cold start optimization, cost-based scheduling, execution environment isolation.</td>\n<td>Fundamental shift from persistent workers to ephemeral execution, changes scaling model, monitoring, and error recovery approaches.</td>\n</tr>\n</tbody></table>\n<h4 id=\"detailed-analysis-exactly-once-processing-guarantees\">Detailed Analysis: Exactly-Once Processing Guarantees</h4>\n<blockquote>\n<p><strong>Decision: Distributed Idempotency with Deterministic Job Chaining</strong></p>\n<ul>\n<li><strong>Context</strong>: Financial and audit-sensitive workloads require stronger guarantees than at-least-once delivery, preventing duplicate processing that could cause double charges or inconsistent state.</li>\n<li><strong>Architectural Challenges</strong>:<ol>\n<li><strong>Dequeue Atomicity</strong>: Current <code>BRPOPLPUSH</code> moves job to processing queue but worker crash before completion leaves job in limbo (janitor recovers but could cause duplicate processing).</li>\n<li><strong>Retry Duplication</strong>: Exponential backoff retries create multiple execution attempts of the same logical job.</li>\n<li><strong>Producer Duplication</strong>: Network timeouts might cause producers to enqueue the same job twice.</li>\n</ol>\n</li>\n<li><strong>Required Changes</strong>:<ul>\n<li><strong>Idempotency Keys</strong>: Require producers to supply idempotency key with each job, stored and checked before processing.</li>\n<li><strong>Deterministic Job IDs</strong>: Generate job IDs as hash of job content + idempotency key to enable deduplication at storage layer.</li>\n<li><strong>Two-Phase Dequeue</strong>: Worker first acquires distributed lock on job ID, processes, marks as committed in Redis transaction, then releases lock.</li>\n<li><strong>Intent Logging</strong>: Write &quot;processing started&quot; record before execution begins, with recovery scanning for orphaned intents.</li>\n</ul>\n</li>\n<li><strong>Impact</strong>: Significant performance overhead (distributed locks, additional Redis writes), more complex failure recovery, incompatible with current simple FIFO processing model.</li>\n</ul>\n</blockquote>\n<p><strong>Migration Path Considerations</strong>:</p>\n<ol>\n<li><strong>Phase 1</strong>: Add optional idempotency key support alongside current semantics for gradual migration.</li>\n<li><strong>Phase 2</strong>: Implement exactly-once processing as separate queue type with different guarantees.</li>\n<li><strong>Phase 3</strong>: Deprecate at-least-once queues over multiple releases, providing migration tools.</li>\n</ol>\n<h4 id=\"detailed-analysis-multi-broker-support\">Detailed Analysis: Multi-Broker Support</h4>\n<blockquote>\n<p><strong>Decision: Pluggable Broker Adapter Interface</strong></p>\n<ul>\n<li><strong>Context</strong>: Organizations have existing infrastructure investments (Kafka for event streaming, PostgreSQL for transactional consistency) and want to leverage them as job brokers without maintaining separate Redis clusters.</li>\n<li><strong>Architectural Challenges</strong>:<ol>\n<li><strong>Redis-Centric Data Structures</strong>: Sorted sets for retries, lists for queues, pub/sub for signals—each has different semantics in other systems.</li>\n<li><strong>Atomic Operations</strong>: Redis Lua scripts and pipelines provide atomicity that must be reimplemented for each backend.</li>\n<li><strong>Performance Characteristics</strong>: Different latency, throughput, and consistency trade-offs affect system design.</li>\n</ol>\n</li>\n<li><strong>Required Changes</strong>:<ul>\n<li><strong>Broker Abstraction Layer</strong>: Define <code>Broker</code> interface with methods for enqueue, dequeue, sorted set operations, pub/sub, and atomic transactions.</li>\n<li><strong>Adapter Implementations</strong>: Create <code>RedisBroker</code>, <code>KafkaBroker</code>, <code>PostgreSQLBroker</code> each implementing the interface with backend-specific optimizations.</li>\n<li><strong>Feature Flagging</strong>: Some features (e.g., priority queue weighting) may not be available in all backends—requires graceful degradation.</li>\n<li><strong>Migration Tools</strong>: Utilities to transfer job state between brokers during migration.</li>\n</ul>\n</li>\n<li><strong>Impact</strong>: Doubles code complexity, requires maintaining multiple backend implementations, potentially limits use of advanced Redis-specific features.</li>\n</ul>\n</blockquote>\n<p><strong>Feature Compatibility Matrix</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Redis</th>\n<th>Kafka</th>\n<th>PostgreSQL</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>FIFO Queues</td>\n<td>✅ Lists</td>\n<td>✅ Topics (partition ordering)</td>\n<td>✅ Table with sequence</td>\n<td>Kafka requires single partition per queue for strict FIFO</td>\n</tr>\n<tr>\n<td>Delayed Jobs</td>\n<td>✅ Sorted Sets</td>\n<td>⚠️ Requires separate scheduler</td>\n<td>✅ UPDATE with WHERE scheduled_at</td>\n<td></td>\n</tr>\n<tr>\n<td>Priority Queues</td>\n<td>✅ Multiple lists with weighted polling</td>\n<td>❌ No native support</td>\n<td>✅ WITH RECURSIVE query</td>\n<td></td>\n</tr>\n<tr>\n<td>Exactly-Once Processing</td>\n<td>⚠️ With careful design</td>\n<td>✅ With transactional producer</td>\n<td>✅ Native transactions</td>\n<td></td>\n</tr>\n<tr>\n<td>Horizontal Scaling</td>\n<td>✅ Native</td>\n<td>✅ Native</td>\n<td>⚠️ Connection pooling limits</td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: The architecture&#39;s clear separation between job logic and queue management enables these extensions, but the choice of Redis as the broker creates both capabilities and constraints. Extensions that align with Redis&#39;s strengths (in-memory data structures, atomic operations) are natural; those requiring different consistency models or storage patterns demand architectural evolution.</p>\n</blockquote>\n<h3 id=\"common-pitfalls-in-extension-implementation\">Common Pitfalls in Extension Implementation</h3>\n<p>⚠️ <strong>Pitfall: Over-Engineering for Hypothetical Use Cases</strong></p>\n<ul>\n<li><strong>Description</strong>: Implementing complex extensions (like distributed transactions) before any concrete requirement exists, based on &quot;what if&quot; scenarios.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Increases system complexity, introduces bugs in unused code paths, delays delivery of core functionality, violates YAGNI principle.</li>\n<li><strong>Fix</strong>: Implement only extensions with proven user demand. Use the architecture&#39;s separation of concerns to keep extension points clear but unimplemented until needed.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Breaking Backward Compatibility</strong></p>\n<ul>\n<li><strong>Description</strong>: Adding new features that require existing job definitions or worker code to be modified, forcing coordinated deployments.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Causes production outages during rolling updates, requires complex migration procedures, frustrates users.</li>\n<li><strong>Fix</strong>: Design extensions with backward compatibility: additive fields only, default values for new requirements, feature flags to enable gradually, dual-read/write patterns during transitions.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Neglecting Performance Impact</strong></p>\n<ul>\n<li><strong>Description</strong>: Adding rate limiting or dependency tracking without considering the additional Redis operations per job, causing latency spikes under load.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Turns a high-performance job processor into a bottleneck, defeats purpose of async processing.</li>\n<li><strong>Fix</strong>: Profile extensions under load, use Redis pipelining, consider approximate algorithms (e.g., probabilistic rate limiting), provide benchmarks for each extension.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Creating Monolithic Components</strong></p>\n<ul>\n<li><strong>Description</strong>: Building extensions as tightly coupled additions to core components rather than separate services or plugins.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Makes system harder to understand, test, and maintain; increases risk of regressions.</li>\n<li><strong>Fix</strong>: Implement extensions as separate processes or threads with well-defined interfaces: dependency manager as separate microservice, rate limiter as Redis-side Lua scripts, dashboard extensions as plugin architecture.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While extensions are beyond the core milestones, their implementation builds upon the established patterns. Here&#39;s guidance for implementing the most valuable architecture-enabled extensions.</p>\n<h4 id=\"technology-recommendations-for-extensions\">Technology Recommendations for Extensions</h4>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate Limiting</td>\n<td>Redis token bucket with Lua script</td>\n<td>Distributed rate limiter with Redis Cluster support, adaptive limits based on downstream health</td>\n</tr>\n<tr>\n<td>Job Dependencies</td>\n<td>Linear chains with Redis sets for tracking</td>\n<td>Full DAG with graph database backend (Neo4j), visual workflow editor</td>\n</tr>\n<tr>\n<td>Multi-Language Support</td>\n<td>JSON serialization with language-specific client libraries</td>\n<td>gRPC-based job protocol with code generation, shared memory for job payloads</td>\n</tr>\n<tr>\n<td>Priority Preemption</td>\n<td>Job termination with requeue at front of queue</td>\n<td>Job suspension/resumption with checkpointing, container snapshotting for long jobs</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure-for-extensions\">Recommended File/Module Structure for Extensions</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>background_job_processor/\n├── core/                    # Existing core components\n│   ├── queue_manager.py\n│   ├── worker.py\n│   └── ...\n├── extensions/              # New extensions directory\n│   ├── rate_limiting/\n│   │   ├── __init__.py\n│   │   ├── token_bucket.py\n│   │   ├── lua_scripts/    # Redis Lua scripts\n│   │   └── tests/\n│   ├── dependencies/\n│   │   ├── __init__.py\n│   │   ├── dependency_manager.py\n│   │   ├── graph_validator.py\n│   │   └── tests/\n│   └── multi_lang/\n│       ├── __init__.py\n│       ├── protocol.py     # Serialization protocol spec\n│       ├── go_client/      # Go client library\n│       └── rust_client/    # Rust client library\n└── dashboard_extensions/   # UI extensions\n    ├── rate_limit_panel.py\n    └── dependency_graph.py</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code-rate-limiting-lua-script\">Infrastructure Starter Code: Rate Limiting Lua Script</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/rate_limiting/lua_scripts/token_bucket.lua</span></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#79B8FF\"> KEYS</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]: rate limit key (e.g., </span><span style=\"color:#9ECBFF\">\"ratelimit:webhooks\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#79B8FF\"> ARGV</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]: current time </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> milliseconds</span></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#79B8FF\"> ARGV</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]: refill rate (tokens per millisecond)</span></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#79B8FF\"> ARGV</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]: burst capacity (</span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\"> tokens)</span></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#E1E4E8\"> Returns: </span><span style=\"color:#79B8FF\">ALLOWED</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> DENIED</span><span style=\"color:#E1E4E8\">:wait_time_ms</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> KEYS</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local now </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tonumber(</span><span style=\"color:#79B8FF\">ARGV</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local refill_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tonumber(</span><span style=\"color:#79B8FF\">ARGV</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tonumber(</span><span style=\"color:#79B8FF\">ARGV</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.call(</span><span style=\"color:#9ECBFF\">'HMGET'</span><span style=\"color:#E1E4E8\">, key, </span><span style=\"color:#9ECBFF\">'tokens'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'last_update'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tonumber(bucket[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local last_update </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tonumber(bucket[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> now</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#E1E4E8\"> Calculate refill based on time elapsed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local time_passed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.max(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, now </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> last_update)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local refill_amount </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time_passed </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.min(capacity, tokens </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> refill_amount)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">local result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\"> then</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"ALLOWED\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">else</span></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">    --</span><span style=\"color:#E1E4E8\"> Calculate time until </span><span style=\"color:#79B8FF\">next</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    local wait_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.ceil((</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> tokens) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> refill_rate)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"DENIED:\"</span><span style=\"color:#E1E4E8\"> .. wait_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">end</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#E1E4E8\"> Update bucket state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">redis.call(</span><span style=\"color:#9ECBFF\">'HMSET'</span><span style=\"color:#E1E4E8\">, key, </span><span style=\"color:#9ECBFF\">'tokens'</span><span style=\"color:#E1E4E8\">, tokens, </span><span style=\"color:#9ECBFF\">'last_update'</span><span style=\"color:#E1E4E8\">, now)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">redis.call(</span><span style=\"color:#9ECBFF\">'EXPIRE'</span><span style=\"color:#E1E4E8\">, key, math.ceil(capacity </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> refill_rate </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#FDAEB7;font-style:italic\">--</span><span style=\"color:#E1E4E8\"> Auto</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">expire idle buckets</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> result</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/rate_limiting/token_bucket.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Token bucket rate limiter implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucketRateLimiter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Rate limiter using token bucket algorithm with Redis Lua script.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis, script_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"token_bucket.lua\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Load Lua script</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(script_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.lua_script </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.register_script(f.read())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_limit</span><span style=\"color:#E1E4E8\">(self, queue_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, requests_per_second: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   burst_capacity: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check if job can be processed based on rate limit.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue_name: Name of queue to check</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            requests_per_second: Refill rate in tokens per second</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            burst_capacity: Maximum tokens bucket can hold</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (allowed, wait_time_seconds). wait_time is None if allowed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"ratelimit:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queue_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        now_ms </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(time.time() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        refill_per_ms </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requests_per_second </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1000.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lua_script(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            keys</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[key],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[now_ms, refill_per_ms, burst_capacity]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> result.startswith(</span><span style=\"color:#9ECBFF\">\"ALLOWED\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Result format: \"DENIED:wait_time_ms\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            wait_time_ms </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(result.split(</span><span style=\"color:#9ECBFF\">\":\"</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, wait_time_ms </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1000.0</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-dependency-manager\">Core Logic Skeleton: Dependency Manager</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/dependencies/dependency_manager.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Manager for job dependencies and DAG execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Set, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.job </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Job</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DependencyManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages job dependencies using Redis for state tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client, queue_manager: QueueManager):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queue_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_manager</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pubsub </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.pubsub()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_job_with_dependencies</span><span style=\"color:#E1E4E8\">(self, job: Job, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                      dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Register a job that depends on other jobs completing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: Job to register (will not be enqueued until dependencies met)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            dependencies: List of job IDs that must complete first</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Job ID of registered job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValidationError: If dependency graph would create a cycle</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate job has unique ID (generate if not)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for cycles in dependency graph using Redis sets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store job in Redis with status PENDING_DEPENDENCIES</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each dependency, add this job to the dependent_jobs set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each dependency, check if already COMPLETED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If so, decrement pending count for this job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If all dependencies already satisfied, enqueue job immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Subscribe to job completion events for dependencies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_job_completion</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, status: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle completion of a job and notify dependents.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: ID of completed job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            status: COMPLETED or FAILED (affects dependent jobs)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve list of dependent job IDs from Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each dependent job:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Decrement pending dependency count in Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If count reaches zero and job is not CANCELLED:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #        - Retrieve job from Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #        - Enqueue via queue_manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #        - Update job status to PENDING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If status is FAILED and dependent jobs should not run:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Mark dependent jobs as CANCELLED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Optionally notify via error channel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_listening</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start listening for job completion events.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Subscribe to job completion channel via Redis Pub/Sub</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start thread to process messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each completion message, call _handle_job_completion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cancel_job_chain</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Cancel a job and all its dependent jobs recursively.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: ID of job to cancel</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Mark job as CANCELLED in Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get dependent jobs recursively</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Mark all as CANCELLED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clean up dependency tracking sets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints-for-extensions\">Language-Specific Hints for Extensions</h4>\n<p><strong>Python-Specific Tips</strong>:</p>\n<ul>\n<li>Use <code>asyncio</code> for dependency manager event listening to avoid blocking threads.</li>\n<li>For rate limiting Lua scripts, use <code>redis.register_script()</code> which automatically handles script caching.</li>\n<li>Implement extensions as context managers for proper resource cleanup: <code>with RateLimiter(queue) as limiter:</code>.</li>\n<li>Use Python&#39;s <code>typing</code> module extensively for extension interfaces to improve IDE support and catch errors early.</li>\n</ul>\n<p><strong>Multi-Language Protocol Considerations</strong>:</p>\n<ol>\n<li><strong>Serialization</strong>: Use JSON for simplicity or msgpack for efficiency. Ensure all languages handle the same data types consistently (especially datetime).</li>\n<li><strong>Error Handling</strong>: Define standard error response format with code, message, and optional details.</li>\n<li><strong>Versioning</strong>: Include protocol version in job payload to handle backward compatibility.</li>\n<li><strong>Connection Management</strong>: Each client library should implement connection pooling and automatic reconnection.</li>\n</ol>\n<h4 id=\"milestone-checkpoint-for-rate-limiting-extension\">Milestone Checkpoint for Rate Limiting Extension</h4>\n<p>After implementing rate limiting:</p>\n<ol>\n<li><strong>Test Command</strong>: <code>python -m pytest extensions/rate_limiting/tests/ -v</code></li>\n<li><strong>Expected Output</strong>: Tests showing token bucket refill, burst handling, and denial with wait times.</li>\n<li><strong>Manual Verification</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Start worker with rate-limited queue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   limiter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketRateLimiter(redis_client)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Enqueue 30 jobs to queue limited to 10/sec with burst 20</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # First 20 should process immediately, next 10 at ~1/second</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Monitor dashboard to see throttling in action</span></span></code></pre></div>\n<ol start=\"4\">\n<li><strong>Signs of Success</strong>: Dashboard shows queue processing rate capped at limit, worker logs show <code>Waiting 0.1s for rate limit</code> messages.</li>\n<li><strong>Troubleshooting</strong>:<ul>\n<li>If jobs process faster than limit: Check Lua script loading, ensure time is in milliseconds.</li>\n<li>If Redis CPU spikes: Rate limit checks are too frequent; increase worker polling interval for rate-limited queues.</li>\n<li>If bursts not working: Verify burst capacity parameter is being passed correctly to Lua script.</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"debugging-tips-for-dependency-extensions\">Debugging Tips for Dependency Extensions</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dependent jobs never run</td>\n<td>Parent job completion not triggering</td>\n<td>Check Redis Pub/Sub subscription, verify <code>_handle_job_completion</code> is called</td>\n<td>Ensure dependency manager is started, check message format in completion channel</td>\n</tr>\n<tr>\n<td>Circular dependency deadlock</td>\n<td>Graph validation missed a cycle</td>\n<td>Run <code>validate_dag()</code> on job registration, check Redis for cycles</td>\n<td>Implement Kahn&#39;s algorithm for cycle detection before accepting job</td>\n</tr>\n<tr>\n<td>Memory leak with many dependencies</td>\n<td>Redis sets growing unbounded</td>\n<td>Monitor Redis memory usage, check for orphaned dependency sets</td>\n<td>Implement janitor process to clean up completed dependency chains</td>\n</tr>\n<tr>\n<td>Race condition: job runs before dependencies met</td>\n<td>Concurrent completion events</td>\n<td>Use Redis transactions (<code>MULTI/EXEC</code>) for dependency count updates</td>\n<td>Implement optimistic locking with version stamps on dependency counts</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architectural Insight</strong>: The most valuable extensions are those that leverage the existing Redis infrastructure without changing core job execution semantics. Rate limiting and dependencies enhance the system&#39;s capabilities while maintaining its simplicity and performance characteristics. More radical extensions (exactly-once processing, multi-broker) essentially create a different system with different trade-offs—these might be better implemented as separate projects that reuse concepts but not code.</p>\n</blockquote>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section spans all five milestones, providing a definitive reference for the technical terms, domain vocabulary, and acronyms used throughout the design document. A shared vocabulary ensures consistent understanding across the team and reduces ambiguity in technical discussions.</p>\n</blockquote>\n<h3 id=\"terms-and-definitions\">Terms and Definitions</h3>\n<p>This glossary defines all key terms used in the Background Job Processor system, organized alphabetically. Each term includes a concise definition and its relevance to the system design.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in System</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Active Job</strong></td>\n<td>A job that has been dequeued by a worker and is currently being executed. Represented by the <code>JobStatus.ACTIVE</code> status.</td>\n<td>Workers transition jobs from <code>PENDING</code> to <code>ACTIVE</code> when they begin processing. Active jobs appear in the processing queue until completion.</td>\n</tr>\n<tr>\n<td><strong>Adaptive Sampling</strong></td>\n<td>Adjusting the frequency of metric collection based on system load to balance observability with performance overhead.</td>\n<td>The <code>MetricsAggregator</code> may increase sampling during high activity and decrease during idle periods to optimize Redis usage.</td>\n</tr>\n<tr>\n<td><strong>Alert Cooldown</strong></td>\n<td>A minimum time period that must pass between consecutive alerts for the same rule to prevent notification spam.</td>\n<td>Configured in <code>AlertRule.cooldown_seconds</code> to prevent <strong>alert fatigue</strong> when a condition remains unstable.</td>\n</tr>\n<tr>\n<td><strong>Alert Fatigue</strong></td>\n<td>The problem where operators begin ignoring alerts due to excessive volume or frequency, reducing the effectiveness of monitoring.</td>\n<td>Addressed through <strong>alert cooldown</strong> periods, severity-based filtering, and intelligent aggregation of similar alerts.</td>\n</tr>\n<tr>\n<td><strong>Architecture Decision Record (ADR)</strong></td>\n<td>A documented decision capturing the context, options considered, and rationale for a specific architectural choice.</td>\n<td>Used throughout this document to explain design choices like Redis data structures, concurrency models, and retry algorithms.</td>\n</tr>\n<tr>\n<td><strong>Atomic Pipeline</strong></td>\n<td>A Redis feature that allows multiple commands to be executed as a single atomic operation, ensuring consistency even with concurrent access.</td>\n<td>Used by <code>QueueManager.bulk_enqueue()</code> and <code>QueueManager.delete_job()</code> to prevent race conditions between workers.</td>\n</tr>\n<tr>\n<td><strong>Backoff Calculator</strong></td>\n<td>A component (<code>BackoffCalculator</code>) that computes the delay between retry attempts using an exponential backoff algorithm with optional jitter.</td>\n<td>Called by <code>RetryManager.handle_job_failure()</code> to determine when to retry failed jobs.</td>\n</tr>\n<tr>\n<td><strong>Blocking Queue Read</strong></td>\n<td>A Redis operation like <code>BRPOP</code> that waits (blocks) for items to become available in a queue rather than returning immediately.</td>\n<td>Used by workers in their main loop to avoid busy-waiting and reduce Redis load.</td>\n</tr>\n<tr>\n<td><strong>Broker</strong></td>\n<td>The central message store that connects job producers and consumers. In our design, Redis serves as the broker.</td>\n<td>All job data flows through the Redis broker, which provides persistence, atomic operations, and pub/sub capabilities.</td>\n</tr>\n<tr>\n<td><strong>Catch-Up Logic</strong></td>\n<td>A mechanism that processes jobs missed during scheduler downtime by detecting overdue scheduled jobs and enqueuing them immediately.</td>\n<td>Implemented in <code>Scheduler._poll_due_schedules()</code> which compares current time with <code>Schedule.next_run_at</code> to identify missed executions.</td>\n</tr>\n<tr>\n<td><strong>Chaos Testing</strong></td>\n<td>Testing system behavior under intentionally injected failures (network partitions, process crashes, etc.) to verify resilience.</td>\n<td>Recommended for validating the system&#39;s error handling and recovery mechanisms in production-like environments.</td>\n</tr>\n<tr>\n<td><strong>Circuit Breaker</strong></td>\n<td>A design pattern (<code>CircuitBreaker</code>) that fails fast when a service is unavailable, preventing cascading failures and allowing time for recovery.</td>\n<td>Can be applied to external API calls within job handlers to prevent workers from getting stuck on repeatedly failing operations.</td>\n</tr>\n<tr>\n<td><strong>Circuit State</strong></td>\n<td>The current state of a circuit breaker: <code>CLOSED</code> (normal operation), <code>OPEN</code> (failing fast), or <code>HALF_OPEN</code> (testing recovery).</td>\n<td>Tracked by the <code>CircuitBreaker.state</code> field and transitions based on failure thresholds and recovery timeouts.</td>\n</tr>\n<tr>\n<td><strong>Completed Job</strong></td>\n<td>A job that has finished execution successfully. Represented by the <code>JobStatus.COMPLETED</code> status.</td>\n<td>Moved from active to completed storage with execution metrics, available for dashboard queries until archived.</td>\n</tr>\n<tr>\n<td><strong>Concurrency</strong></td>\n<td>The number of jobs a worker can process simultaneously, configured via <code>WorkerConfig.concurrency</code>.</td>\n<td>Implemented using thread or process pools within a worker to handle multiple jobs in parallel without blocking the main loop.</td>\n</tr>\n<tr>\n<td><strong>Correlation ID</strong></td>\n<td>A unique identifier passed through the entire job lifecycle for tracing and debugging purposes.</td>\n<td>Can be stored in <code>Job.metadata</code> to link job execution logs across workers, retries, and external systems.</td>\n</tr>\n<tr>\n<td><strong>Cron Expression</strong></td>\n<td>A time-based schedule expression using five fields (minute, hour, day of month, month, day of week) to define recurring execution times.</td>\n<td>Parsed by the scheduler to calculate <code>Schedule.next_run_at</code> for recurring jobs. Supports standard Unix cron syntax.</td>\n</tr>\n<tr>\n<td><strong>DAG (Directed Acyclic Graph)</strong></td>\n<td>A dependency model without cycles, where jobs depend on the completion of other jobs in a directed, non-circular manner.</td>\n<td>Future extension: <code>DependencyManager</code> could use DAGs to manage complex job workflows with dependencies.</td>\n</tr>\n<tr>\n<td><strong>Dashboard Config</strong></td>\n<td>Configuration (<code>DashboardConfig</code>) for the web dashboard controlling display filters, refresh intervals, and metric aggregation settings.</td>\n<td>Used by the monitoring API to customize dashboard views without changing code.</td>\n</tr>\n<tr>\n<td><strong>Daylight Saving Time</strong></td>\n<td>Seasonal time adjustment where clocks are set forward or backward by one hour, affecting local time calculations.</td>\n<td>The scheduler handles DST transitions by evaluating cron expressions in the configured timezone using aware datetime objects.</td>\n</tr>\n<tr>\n<td><strong>Dead Letter Queue</strong></td>\n<td>Storage (<code>DeadLetterQueue</code>) for jobs that have exhausted all retry attempts, allowing manual inspection and potential retry.</td>\n<td>Implemented as a Redis sorted set keyed by job ID, with error history preserved for debugging.</td>\n</tr>\n<tr>\n<td><strong>Dead Letter Job</strong></td>\n<td>A job that has permanently failed after exceeding its maximum retry count. Represented by the <code>JobStatus.DEAD_LETTER</code> status.</td>\n<td>Moved to the dead letter queue by <code>RetryManager.move_to_dead_letter()</code> and accessible via the dashboard for manual handling.</td>\n</tr>\n<tr>\n<td><strong>Diagnostic Worker</strong></td>\n<td>A background process (<code>DiagnosticWorker</code>) that periodically collects system health metrics and checks for anomalies.</td>\n<td>Runs alongside the main components to provide proactive monitoring and early warning of system issues.</td>\n</tr>\n<tr>\n<td><strong>Directed Acyclic Graph</strong></td>\n<td>See <strong>DAG</strong>.</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>End-to-End Tests</strong></td>\n<td>Tests that simulate real user scenarios across the entire system, verifying integration between all components.</td>\n<td>Example: Enqueue a job → worker processes it → verify result storage → check dashboard metrics update.</td>\n</tr>\n<tr>\n<td><strong>Exponential Backoff</strong></td>\n<td>A retry algorithm that doubles the wait time between consecutive retry attempts to prevent overloading systems.</td>\n<td>Implemented in <code>BackoffCalculator.calculate_delay()</code> using formula: <code>base_delay * 2^(attempt-1) ± jitter</code>.</td>\n</tr>\n<tr>\n<td><strong>Failed Job</strong></td>\n<td>A job that has finished execution with an error but may still be retried. Represented by the <code>JobStatus.FAILED</code> status.</td>\n<td>After failure, the job is either scheduled for retry or moved to the dead letter queue based on retry count.</td>\n</tr>\n<tr>\n<td><strong>FIFO (First-In-First-Out)</strong></td>\n<td>An ordering principle where the first element added to a queue is the first to be removed.</td>\n<td>Default queue behavior implemented using Redis <code>LPUSH</code> (add to end) and <code>BRPOP</code> (remove from front).</td>\n</tr>\n<tr>\n<td><strong>Flaky Test</strong></td>\n<td>A test that passes and fails intermittently without code changes, often due to timing issues, race conditions, or external dependencies.</td>\n<td>Mitigated by using deterministic <code>mock clock</code> for time-dependent tests and proper isolation of Redis test instances.</td>\n</tr>\n<tr>\n<td><strong>Graceful Shutdown</strong></td>\n<td>A shutdown process that allows the current operation to complete before exiting, rather than terminating abruptly.</td>\n<td>Workers implement graceful shutdown by catching <code>SIGTERM</code>, setting <code>_shutdown_requested</code>, and finishing the current job before exiting.</td>\n</tr>\n<tr>\n<td><strong>Heartbeat</strong></td>\n<td>A regular status update published by workers to indicate they are alive and functioning.</td>\n<td>Implemented by <code>WorkerHeartbeat</code> which periodically writes worker status to Redis with a TTL; missing heartbeats indicate crashed workers.</td>\n</tr>\n<tr>\n<td><strong>Idempotent</strong></td>\n<td>A property where repeated execution of an operation produces the same result as a single execution.</td>\n<td>Critical for job handlers that may be retried; designers should ensure jobs are idempotent where possible.</td>\n</tr>\n<tr>\n<td><strong>Idempotency Key</strong></td>\n<td>A unique identifier used to prevent duplicate processing of the same logical operation.</td>\n<td>Future extension: Could be stored in <code>Job.metadata</code> to deduplicate identical jobs enqueued multiple times.</td>\n</tr>\n<tr>\n<td><strong>Integration Tests</strong></td>\n<td>Tests that verify interactions between multiple components with real dependencies (like Redis).</td>\n<td>Use <code>RedisTestContext</code> to spin up isolated Redis instances for testing queue operations, worker coordination, and retry logic.</td>\n</tr>\n<tr>\n<td><strong>Janitor Process</strong></td>\n<td>A maintenance process (<code>Janitor</code>) that cleans up stale jobs from processing queues when workers crash unexpectedly.</td>\n<td>Periodically scans for jobs in processing queues whose worker heartbeat has expired and requeues them.</td>\n</tr>\n<tr>\n<td><strong>Job</strong></td>\n<td>The unit of work to be processed asynchronously, represented by the <code>Job</code> class with fields for type, arguments, metadata, and status.</td>\n<td>The fundamental data entity that flows through the system from enqueue to completion or failure.</td>\n</tr>\n<tr>\n<td><strong>Job Handler</strong></td>\n<td>A function registered with a worker to execute the business logic for a specific job type.</td>\n<td>Registered via <code>Worker.register_handler()</code>; invoked by the worker when it dequeues a job of the matching type.</td>\n</tr>\n<tr>\n<td><strong>Job History Archival</strong></td>\n<td>The process of moving old job data from Redis to external storage (like a database) to prevent unbounded memory growth.</td>\n<td>Implemented by <code>Janitor._cleanup_expired_keys()</code> which removes completed jobs after a configurable retention period.</td>\n</tr>\n<tr>\n<td><strong>Job Status</strong></td>\n<td>The current state of a job, represented by the <code>JobStatus</code> enum: <code>PENDING</code>, <code>ACTIVE</code>, <code>COMPLETED</code>, <code>FAILED</code>, <code>RETRY_SCHEDULED</code>, <code>DEAD_LETTER</code>.</td>\n<td>Tracked throughout the job lifecycle and used for filtering in the dashboard and determining next actions.</td>\n</tr>\n<tr>\n<td><strong>Job Summary</strong></td>\n<td>A condensed view of job information (<code>JobSummary</code>) used for display in the dashboard, containing key fields without full payload.</td>\n<td>Created from <code>Job</code> instances for efficient dashboard queries that don&#39;t require the complete job serialization.</td>\n</tr>\n<tr>\n<td><strong>Jitter</strong></td>\n<td>Random variation added to retry delays to prevent synchronization of multiple retrying jobs (thundering herd).</td>\n<td>Applied by <code>BackoffCalculator</code> using a configurable <code>jitter_factor</code> that adds ± random percentage to calculated delays.</td>\n</tr>\n<tr>\n<td><strong>Lua Script</strong></td>\n<td>Redis server-side scripting language that allows atomic execution of multiple commands on the Redis server.</td>\n<td>Used for complex atomic operations like <code>TokenBucketRateLimiter.check_limit()</code> that require multiple Redis commands.</td>\n</tr>\n<tr>\n<td><strong>Metric Aggregation</strong></td>\n<td>The process of combining raw job events into statistical summaries (counts, rates, averages) for monitoring.</td>\n<td>Performed by <code>MetricsAggregator</code> which periodically reads Redis streams and computes aggregated metrics for the dashboard.</td>\n</tr>\n<tr>\n<td><strong>Metric Point</strong></td>\n<td>A single metric data point (<code>MetricPoint</code>) with timestamp, name, value, and labels for dimensional analysis.</td>\n<td>The basic unit of metric storage; aggregated into time series for dashboard charts and alert evaluation.</td>\n</tr>\n<tr>\n<td><strong>Mock Clock</strong></td>\n<td>A simulated time control used in tests to manipulate and advance time without waiting for real clock progression.</td>\n<td>Essential for testing retry delays, scheduler cron evaluation, and time-based job expiration without real-world delays.</td>\n</tr>\n<tr>\n<td><strong>Pending Job</strong></td>\n<td>A job that has been enqueued but not yet picked up by a worker. Represented by the <code>JobStatus.PENDING</code> status.</td>\n<td>Stored in Redis lists (queues) waiting for worker consumption. Dashboard shows pending count per queue.</td>\n</tr>\n<tr>\n<td><strong>Polling Interval</strong></td>\n<td>The time period between scheduler checks for due jobs, configured via <code>Scheduler.polling_interval_seconds</code>.</td>\n<td>Balances schedule accuracy with Redis load; shorter intervals increase timeliness but increase Redis queries.</td>\n</tr>\n<tr>\n<td><strong>Priority Weighted Polling</strong></td>\n<td>A worker polling strategy that checks higher-priority queues more frequently than lower-priority ones based on configured weights.</td>\n<td>Implemented by workers using algorithm that selects queues probabilistically based on <code>QueueConfig.priority</code> values.</td>\n</tr>\n<tr>\n<td><strong>Processing Queue</strong></td>\n<td>A temporary Redis list holding jobs currently being executed by a worker, used for reliability during worker crashes.</td>\n<td>When a worker dequeues a job, it moves it from the main queue to a processing queue; on completion, removes it.</td>\n</tr>\n<tr>\n<td><strong>Property-Based Testing</strong></td>\n<td>Testing by generating random inputs that must satisfy certain properties, rather than testing specific predefined cases.</td>\n<td>Useful for validating job serialization/deserialization invariants and queue operation correctness across diverse inputs.</td>\n</tr>\n<tr>\n<td><strong>Protocol Versioning</strong></td>\n<td>A technique for maintaining backward compatibility in APIs by including version identifiers in messages or endpoints.</td>\n<td>Future extension: Could be added to job serialization format to allow evolution of <code>Job</code> fields without breaking existing jobs.</td>\n</tr>\n<tr>\n<td><strong>Queue</strong></td>\n<td>A named channel for jobs with configurable priority, implemented as a Redis list with associated metadata.</td>\n<td>Defined by <code>QueueConfig</code>; workers subscribe to specific queues and poll them according to priority weights.</td>\n</tr>\n<tr>\n<td><strong>Queue Config</strong></td>\n<td>Configuration (<code>QueueConfig</code>) for a named queue including its priority and optional maximum length.</td>\n<td>Loaded into <code>QueueManager.queue_configs</code> to control queue behavior and worker polling strategy.</td>\n</tr>\n<tr>\n<td><strong>Queue Full Error</strong></td>\n<td>An exception (<code>QueueFullError</code>) raised when attempting to enqueue a job to a queue that has reached its maximum capacity.</td>\n<td>Prevented by <code>QueueManager._validate_job()</code> checking queue length against <code>QueueConfig.max_length</code> before enqueue.</td>\n</tr>\n<tr>\n<td><strong>Queue Manager</strong></td>\n<td>The component (<code>QueueManager</code>) responsible for validating, enqueuing, and managing jobs in Redis queues.</td>\n<td>Provides the primary API for producers to submit jobs and inspect queue state.</td>\n</tr>\n<tr>\n<td><strong>Queue Metrics</strong></td>\n<td>Performance metrics for a specific queue (<code>QueueMetrics</code>) including depth, processing rate, error rate, and age statistics.</td>\n<td>Computed by <code>MetricsAggregator</code> and displayed in the dashboard&#39;s queue overview section.</td>\n</tr>\n<tr>\n<td><strong>Race Condition</strong></td>\n<td>A bug where the outcome depends on the timing of concurrent operations, often leading to inconsistent state.</td>\n<td>Mitigated throughout the system using Redis atomic operations, pipelines, and Lua scripts for critical sections.</td>\n</tr>\n<tr>\n<td><strong>Real-Time Dashboard</strong></td>\n<td>A web interface that updates without page refresh, showing current system state using WebSocket or SSE connections.</td>\n<td>Implemented with Server-Sent Events (SSE) streaming metric updates to the browser as they&#39;re collected.</td>\n</tr>\n<tr>\n<td><strong>Redis Client</strong></td>\n<td>A wrapper (<code>RedisClient</code>) around the Redis connection that provides error handling, connection pooling, and command execution.</td>\n<td>Used by all components to interact with Redis; ensures consistent connection management and error recovery.</td>\n</tr>\n<tr>\n<td><strong>Redis Config</strong></td>\n<td>Configuration (<code>RedisConfig</code>) for Redis connection parameters: URL, timeouts, retry behavior, and connection pool size.</td>\n<td>Loaded into <code>SystemConfig.redis</code> and used to initialize <code>RedisClient</code> instances.</td>\n</tr>\n<tr>\n<td><strong>Redis Streams</strong></td>\n<td>A Redis data type that provides an append-only log structure with consumer groups for reliable message processing.</td>\n<td>Used for job event logging (completions, failures) that feed into the <code>MetricsAggregator</code> for real-time metrics.</td>\n</tr>\n<tr>\n<td><strong>Redis Test Context</strong></td>\n<td>A test utility (<code>RedisTestContext</code>) that manages isolated Redis instances for integration testing.</td>\n<td>Provides clean Redis databases for each test case and ensures proper cleanup after tests complete.</td>\n</tr>\n<tr>\n<td><strong>Retry Filter</strong></td>\n<td>Logic to determine if a specific error type should bypass retries and move directly to the dead letter queue.</td>\n<td>Future extension: Could be implemented as a configurable mapping of exception types to retry policies in <code>RetryManager</code>.</td>\n</tr>\n<tr>\n<td><strong>Retry Manager</strong></td>\n<td>The component (<code>RetryManager</code>) that handles job failures, schedules retries with exponential backoff, and manages the dead letter queue.</td>\n<td>Centralized error handling logic invoked by workers when jobs fail during execution.</td>\n</tr>\n<tr>\n<td><strong>Retry Scheduled Job</strong></td>\n<td>A job that has failed and been scheduled for retry at a future time. Represented by the <code>JobStatus.RETRY_SCHEDULED</code> status.</td>\n<td>Stored in Redis sorted set with execution timestamp as score; periodically moved back to main queue by scheduler.</td>\n</tr>\n<tr>\n<td><strong>Scheduler</strong></td>\n<td>The component (<code>Scheduler</code>) that enqueues jobs for future or recurring execution based on cron expressions or specific timestamps.</td>\n<td>Runs as a separate process that polls for due schedules and enqueues jobs into the appropriate worker queues.</td>\n</tr>\n<tr>\n<td><strong>Schedule Status</strong></td>\n<td>The current state of a scheduled job execution: <code>PENDING</code>, <code>ENQUEUED</code>, <code>SKIPPED</code>, or <code>ERROR</code>.</td>\n<td>Tracked in <code>ScheduledJob.status</code> to monitor scheduler operation and detect issues with schedule execution.</td>\n</tr>\n<tr>\n<td><strong>Sensitive Data Redaction</strong></td>\n<td>The process of masking confidential information (like passwords, tokens) in job logs and dashboard displays.</td>\n<td>Implemented in dashboard API by filtering sensitive fields from <code>Job.metadata</code> before returning to clients.</td>\n</tr>\n<tr>\n<td><strong>Server-Sent Events (SSE)</strong></td>\n<td>An HTTP-based technology for server-to-client real-time updates, simpler than WebSockets for unidirectional data flow.</td>\n<td>Used by the dashboard to stream real-time metric updates from server to browser without polling.</td>\n</tr>\n<tr>\n<td><strong>SIGTERM</strong></td>\n<td>A Unix signal sent to a process to request graceful termination (as opposed to SIGKILL which forces immediate termination).</td>\n<td>Workers catch SIGTERM to initiate graceful shutdown, allowing current job to complete before exiting.</td>\n</tr>\n<tr>\n<td><strong>Split-Brain</strong></td>\n<td>A scenario in distributed systems where a network partition causes components to operate independently, potentially leading to data inconsistency.</td>\n<td>Mitigated in our design by using Redis as a single source of truth; network partition would prevent all components from operating.</td>\n</tr>\n<tr>\n<td><strong>Statistical Assertion</strong></td>\n<td>A test verification that holds true over many random trials, rather than for a single specific execution.</td>\n<td>Used in property-based testing to validate system properties like &quot;no job is lost&quot; across many random job sequences.</td>\n</tr>\n<tr>\n<td><strong>System Alert</strong></td>\n<td>A notification of a system condition requiring attention (<code>SystemAlert</code>), with severity, message, and timestamps.</td>\n<td>Generated by <code>AlertManager</code> when alert rules evaluate to true, displayed in dashboard and potentially sent via external channels.</td>\n</tr>\n<tr>\n<td><strong>System Config</strong></td>\n<td>The root configuration object (<code>SystemConfig</code>) containing all subsystem configurations: Redis, queues, workers, and global settings.</td>\n<td>Loaded from environment variables via <code>SystemConfig.from_env()</code> and passed to all components during initialization.</td>\n</tr>\n<tr>\n<td><strong>System Metrics</strong></td>\n<td>Comprehensive system health metrics (<code>SystemMetrics</code>) including CPU, memory, Redis usage, queue depths, and worker counts.</td>\n<td>Collected by <code>DiagnosticWorker</code> for system-level monitoring beyond job-specific metrics.</td>\n</tr>\n<tr>\n<td><strong>Test Fixture</strong></td>\n<td>Reusable setup and teardown code for tests that establishes a consistent initial state for test execution.</td>\n<td>Provided by <code>RedisTestContext</code> which sets up and tears down Redis instances for integration tests.</td>\n</tr>\n<tr>\n<td><strong>Thundering Herd</strong></td>\n<td>A problem where many retried jobs simultaneously become ready and overwhelm the system when they retry at the same time.</td>\n<td>Prevented by adding <strong>jitter</strong> to retry delays, staggering retry times across different jobs.</td>\n</tr>\n<tr>\n<td><strong>Timezone</strong></td>\n<td>A geographical region&#39;s standard time, used for evaluating cron expressions in the correct local context.</td>\n<td>Configured per schedule via <code>Schedule.timezone</code>; cron expressions are evaluated in this timezone rather than UTC only.</td>\n</tr>\n<tr>\n<td><strong>Token Bucket</strong></td>\n<td>A rate limiting algorithm that allows bursts up to a capacity while maintaining a steady average rate over time.</td>\n<td>Implemented by <code>TokenBucketRateLimiter</code> for future extension to limit job enqueue rates per producer.</td>\n</tr>\n<tr>\n<td><strong>TTL (Time-To-Live)</strong></td>\n<td>An expiration time set on Redis keys to automatically remove data after a period, preventing memory bloat.</td>\n<td>Applied to worker heartbeats, metric data, and completed job records to ensure Redis memory doesn&#39;t grow unbounded.</td>\n</tr>\n<tr>\n<td><strong>Two-Tiered Storage</strong></td>\n<td>An architecture with fast cache (Redis) for recent data and slow storage (database) for historical data.</td>\n<td>Future extension: Older job history could be moved from Redis to a database while keeping recent data in Redis for fast access.</td>\n</tr>\n<tr>\n<td><strong>ULID (Universally Unique Lexicographically Sortable Identifier)</strong></td>\n<td>An identifier format that combines timestamp (lexicographically sortable) with randomness for uniqueness.</td>\n<td>Alternative to UUID for job IDs; provides natural chronological ordering when sorted lexicographically.</td>\n</tr>\n<tr>\n<td><strong>Uniqueness Window</strong></td>\n<td>A time period during which duplicate enqueueing of the same scheduled job is prevented by the scheduler.</td>\n<td>Configured via <code>Schedule.unique_window_seconds</code>; prevents multiple instances of the same recurring job within the window.</td>\n</tr>\n<tr>\n<td><strong>Validation Error</strong></td>\n<td>An exception (<code>ValidationError</code>) raised when job validation fails, such as payload exceeding size limits or missing required fields.</td>\n<td>Thrown by <code>QueueManager._validate_job()</code> before enqueue to prevent malformed jobs from entering the system.</td>\n</tr>\n<tr>\n<td><strong>Worker</strong></td>\n<td>A process (<code>Worker</code>) that fetches jobs from queues and executes them using registered handlers, with configurable concurrency.</td>\n<td>The core processing unit of the system; multiple workers can run simultaneously for horizontal scaling.</td>\n</tr>\n<tr>\n<td><strong>Worker Config</strong></td>\n<td>Configuration (<code>WorkerConfig</code>) for a worker process: subscribed queues, concurrency, heartbeat interval, and job timeout.</td>\n<td>Passed to <code>Worker</code> instance at startup; determines which queues the worker polls and how many jobs it processes concurrently.</td>\n</tr>\n<tr>\n<td><strong>Worker Heartbeat</strong></td>\n<td>A component (<code>WorkerHeartbeat</code>) that periodically updates worker liveness in Redis, allowing detection of crashed workers.</td>\n<td>Runs in a separate thread within each worker process; updates a Redis key with TTL; missing updates indicate worker failure.</td>\n</tr>\n<tr>\n<td><strong>Worker Status</strong></td>\n<td>Current status of a worker (<code>WorkerStatus</code>): active/inactive, current job, processed counts, and last heartbeat time.</td>\n<td>Computed by the monitoring API from worker heartbeats and active job tracking; displayed in the dashboard&#39;s worker view.</td>\n</tr>\n<tr>\n<td><strong>YAGNI (You Ain&#39;t Gonna Need It)</strong></td>\n<td>A principle of not implementing features until they are actually needed, to avoid unnecessary complexity.</td>\n<td>Applied throughout the design: features like job dependencies, rate limiting, and multi-language support are marked as future extensions.</td>\n</tr>\n<tr>\n<td><strong>Zombie Job</strong></td>\n<td>A job stuck in a processing queue with no active worker processing it, typically because the worker crashed.</td>\n<td>Cleaned up by the <strong>janitor process</strong> which detects stale processing queues and requeues their jobs.</td>\n</tr>\n</tbody></table>\n","toc":[{"level":1,"text":"Background Job Processor: Design Document","id":"background-job-processor-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Restaurant Kitchen Ticket System","id":"mental-model-the-restaurant-kitchen-ticket-system"},{"level":3,"text":"The Asynchronous Task Execution Problem","id":"the-asynchronous-task-execution-problem"},{"level":3,"text":"Existing Approaches and Trade-offs","id":"existing-approaches-and-trade-offs"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: Redis Client Wrapper","id":"c-infrastructure-starter-code-redis-client-wrapper"},{"level":4,"text":"D. Core Logic Skeleton: Job Model","id":"d-core-logic-skeleton-job-model"},{"level":4,"text":"E. Language-Specific Hints: Python","id":"e-language-specific-hints-python"},{"level":4,"text":"F. Milestone Checkpoint: After Section Completion","id":"f-milestone-checkpoint-after-section-completion"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Functional Requirements (Goals)","id":"functional-requirements-goals"},{"level":4,"text":"Job Lifecycle Requirements","id":"job-lifecycle-requirements"},{"level":3,"text":"Non-Functional Requirements","id":"non-functional-requirements"},{"level":4,"text":"Performance Trade-off Decisions","id":"performance-trade-off-decisions"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":4,"text":"Boundary Decisions with Rationale","id":"boundary-decisions-with-rationale"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Configuration Structure","id":"recommended-configuration-structure"},{"level":4,"text":"Milestone Goal Mapping","id":"milestone-goal-mapping"},{"level":4,"text":"Common Implementation Pitfalls to Avoid","id":"common-implementation-pitfalls-to-avoid"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"System Components and Responsibilities","id":"system-components-and-responsibilities"},{"level":3,"text":"Communication Patterns and Data Flow","id":"communication-patterns-and-data-flow"},{"level":3,"text":"Recommended File and Module Structure","id":"recommended-file-and-module-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Redis Data Structures and Keys","id":"redis-data-structures-and-keys"},{"level":4,"text":"Primary Job Storage","id":"primary-job-storage"},{"level":4,"text":"Job Metadata and State Tracking","id":"job-metadata-and-state-tracking"},{"level":4,"text":"System Management and Monitoring","id":"system-management-and-monitoring"},{"level":4,"text":"Indexes and Reverse Lookups","id":"indexes-and-reverse-lookups"},{"level":4,"text":"Redis Key Design ADR","id":"redis-key-design-adr"},{"level":3,"text":"In-Memory Object Types and Schemas","id":"in-memory-object-types-and-schemas"},{"level":4,"text":"Core Job Object","id":"core-job-object"},{"level":4,"text":"Job Status State Machine","id":"job-status-state-machine"},{"level":4,"text":"Configuration Objects","id":"configuration-objects"},{"level":4,"text":"Redis Client Wrapper","id":"redis-client-wrapper"},{"level":4,"text":"Object Relationships","id":"object-relationships"},{"level":3,"text":"Job Serialization and Encoding","id":"job-serialization-and-encoding"},{"level":4,"text":"Serialization Format ADR","id":"serialization-format-adr"},{"level":4,"text":"Serialization Schema","id":"serialization-schema"},{"level":4,"text":"Type Encoding Rules","id":"type-encoding-rules"},{"level":4,"text":"Serialization Methods","id":"serialization-methods"},{"level":4,"text":"Common Pitfalls in Data Modeling","id":"common-pitfalls-in-data-modeling"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Component Design: Job Queue Core","id":"component-design-job-queue-core"},{"level":3,"text":"Mental Model: Post Office Sorting Facility","id":"mental-model-post-office-sorting-facility"},{"level":3,"text":"Queue Manager Interface","id":"queue-manager-interface"},{"level":3,"text":"Enqueue and Priority Algorithm","id":"enqueue-and-priority-algorithm"},{"level":4,"text":"Step-by-Step Enqueue Algorithm","id":"step-by-step-enqueue-algorithm"},{"level":4,"text":"Priority Weighted Polling Algorithm (Worker-Side)","id":"priority-weighted-polling-algorithm-worker-side"},{"level":3,"text":"ADR: Job Storage Mechanism","id":"adr-job-storage-mechanism"},{"level":3,"text":"ADR: Queue Priority Strategy","id":"adr-queue-priority-strategy"},{"level":3,"text":"Common Pitfalls in Queue Implementation","id":"common-pitfalls-in-queue-implementation"},{"level":3,"text":"Implementation Guidance for Queue Core","id":"implementation-guidance-for-queue-core"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Component Design: Worker Process","id":"component-design-worker-process"},{"level":3,"text":"Mental Model: Factory Assembly Line Worker","id":"mental-model-factory-assembly-line-worker"},{"level":3,"text":"Worker Interface and Configuration","id":"worker-interface-and-configuration"},{"level":3,"text":"Worker Main Loop Algorithm","id":"worker-main-loop-algorithm"},{"level":3,"text":"ADR: Concurrency Model (Process vs Thread)","id":"adr-concurrency-model-process-vs-thread"},{"level":3,"text":"ADR: Dequeue Strategy and Reliability","id":"adr-dequeue-strategy-and-reliability"},{"level":3,"text":"Common Pitfalls in Worker Implementation","id":"common-pitfalls-in-worker-implementation"},{"level":3,"text":"Implementation Guidance for Worker","id":"implementation-guidance-for-worker"},{"level":2,"text":"Component Design: Retry &amp; Error Handling","id":"component-design-retry-amp-error-handling"},{"level":3,"text":"Mental Model: Customer Service Escalation Process","id":"mental-model-customer-service-escalation-process"},{"level":3,"text":"Retry Manager Interface","id":"retry-manager-interface"},{"level":3,"text":"Retry Scheduling and Backoff Algorithm","id":"retry-scheduling-and-backoff-algorithm"},{"level":3,"text":"ADR: Backoff Algorithm Selection","id":"adr-backoff-algorithm-selection"},{"level":3,"text":"ADR: Dead Letter Queue Design","id":"adr-dead-letter-queue-design"},{"level":3,"text":"Common Pitfalls in Retry Implementation","id":"common-pitfalls-in-retry-implementation"},{"level":3,"text":"Implementation Guidance for Retry System","id":"implementation-guidance-for-retry-system"},{"level":2,"text":"Component Design: Scheduling &amp; Cron","id":"component-design-scheduling-amp-cron"},{"level":3,"text":"Mental Model: Calendar App with Recurring Events","id":"mental-model-calendar-app-with-recurring-events"},{"level":3,"text":"Scheduler Interface","id":"scheduler-interface"},{"level":3,"text":"Scheduler Polling and Cron Evaluation","id":"scheduler-polling-and-cron-evaluation"},{"level":3,"text":"ADR: Scheduler Architecture (Polling vs Event)","id":"adr-scheduler-architecture-polling-vs-event"},{"level":3,"text":"ADR: Timezone Handling Strategy","id":"adr-timezone-handling-strategy"},{"level":3,"text":"Common Pitfalls in Scheduler Implementation","id":"common-pitfalls-in-scheduler-implementation"},{"level":3,"text":"Implementation Guidance for Scheduler","id":"implementation-guidance-for-scheduler"},{"level":2,"text":"Component Design: Monitoring &amp; Dashboard","id":"component-design-monitoring-amp-dashboard"},{"level":3,"text":"Mental Model: Air Traffic Control Dashboard","id":"mental-model-air-traffic-control-dashboard"},{"level":3,"text":"Monitoring API and Dashboard Interface","id":"monitoring-api-and-dashboard-interface"},{"level":4,"text":"Core Monitoring Data Types","id":"core-monitoring-data-types"},{"level":4,"text":"Monitoring API Endpoints","id":"monitoring-api-endpoints"},{"level":4,"text":"Web Dashboard Interface","id":"web-dashboard-interface"},{"level":3,"text":"Metrics Collection and Aggregation","id":"metrics-collection-and-aggregation"},{"level":4,"text":"Collection Strategy","id":"collection-strategy"},{"level":4,"text":"Aggregation Algorithm","id":"aggregation-algorithm"},{"level":4,"text":"Key Performance Indicators (KPIs)","id":"key-performance-indicators-kpis"},{"level":3,"text":"ADR: Real-time Update Strategy","id":"adr-real-time-update-strategy"},{"level":3,"text":"ADR: Job History Storage Strategy","id":"adr-job-history-storage-strategy"},{"level":4,"text":"Job History Storage Implementation","id":"job-history-storage-implementation"},{"level":3,"text":"Common Pitfalls in Monitoring Implementation","id":"common-pitfalls-in-monitoring-implementation"},{"level":3,"text":"Implementation Guidance for Monitoring","id":"implementation-guidance-for-monitoring"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Happy Path: Job Submission to Completion","id":"happy-path-job-submission-to-completion"},{"level":4,"text":"Step-by-Step Flow","id":"step-by-step-flow"},{"level":3,"text":"Error Recovery Flow: Worker Crash and Retry","id":"error-recovery-flow-worker-crash-and-retry"},{"level":4,"text":"Step-by-Step Flow","id":"step-by-step-flow"},{"level":4,"text":"Common Pitfalls in Error Recovery","id":"common-pitfalls-in-error-recovery"},{"level":3,"text":"Scheduled Job: Cron to Execution","id":"scheduled-job-cron-to-execution"},{"level":4,"text":"Step-by-Step Flow","id":"step-by-step-flow"},{"level":4,"text":"Architecture Decision Record: Scheduler Catch-up Strategy","id":"architecture-decision-record-scheduler-catch-up-strategy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints (Python)","id":"e-language-specific-hints-python"},{"level":4,"text":"F. Milestone Checkpoint for Interactions","id":"f-milestone-checkpoint-for-interactions"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Failure Modes and Detection","id":"failure-modes-and-detection"},{"level":3,"text":"Recovery and Compensation Strategies","id":"recovery-and-compensation-strategies"},{"level":3,"text":"Specific Edge Cases and Solutions","id":"specific-edge-cases-and-solutions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Testing Philosophy and Approach","id":"testing-philosophy-and-approach"},{"level":3,"text":"Milestone Implementation Checkpoints","id":"milestone-implementation-checkpoints"},{"level":3,"text":"Integration and End-to-End Tests","id":"integration-and-end-to-end-tests"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Common Bugs and Their Fixes","id":"common-bugs-and-their-fixes"},{"level":4,"text":"State Transition Anomalies","id":"state-transition-anomalies"},{"level":3,"text":"Diagnostic Tools and Techniques","id":"diagnostic-tools-and-techniques"},{"level":4,"text":"Built-in Diagnostic Endpoints","id":"built-in-diagnostic-endpoints"},{"level":4,"text":"Logging Strategy for Debugging","id":"logging-strategy-for-debugging"},{"level":4,"text":"External Diagnostic Tools","id":"external-diagnostic-tools"},{"level":4,"text":"Interactive Debugging Session Protocol","id":"interactive-debugging-session-protocol"},{"level":3,"text":"Redis Data Inspection Guide","id":"redis-data-inspection-guide"},{"level":4,"text":"Key Naming Patterns","id":"key-naming-patterns"},{"level":4,"text":"Inspection Commands and Scripts","id":"inspection-commands-and-scripts"},{"level":4,"text":"Common Redis Data Anomalies","id":"common-redis-data-anomalies"},{"level":4,"text":"Redis Lua Scripts for Complex Diagnostics","id":"redis-lua-scripts-for-complex-diagnostics"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Debug Endpoint Implementation","id":"debug-endpoint-implementation"},{"level":4,"text":"Diagnostic Worker Implementation","id":"diagnostic-worker-implementation"},{"level":4,"text":"Debugging Helper Scripts","id":"debugging-helper-scripts"},{"level":4,"text":"Language-Specific Debugging Hints","id":"language-specific-debugging-hints"},{"level":4,"text":"Milestone Debugging Checkpoints","id":"milestone-debugging-checkpoints"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Architecture-Enabled Extensions","id":"architecture-enabled-extensions"},{"level":4,"text":"Detailed Analysis: Rate Limiting Extension","id":"detailed-analysis-rate-limiting-extension"},{"level":4,"text":"Detailed Analysis: Job Dependencies Extension","id":"detailed-analysis-job-dependencies-extension"},{"level":3,"text":"Extensions Requiring Major Changes","id":"extensions-requiring-major-changes"},{"level":4,"text":"Detailed Analysis: Exactly-Once Processing Guarantees","id":"detailed-analysis-exactly-once-processing-guarantees"},{"level":4,"text":"Detailed Analysis: Multi-Broker Support","id":"detailed-analysis-multi-broker-support"},{"level":3,"text":"Common Pitfalls in Extension Implementation","id":"common-pitfalls-in-extension-implementation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations for Extensions","id":"technology-recommendations-for-extensions"},{"level":4,"text":"Recommended File/Module Structure for Extensions","id":"recommended-filemodule-structure-for-extensions"},{"level":4,"text":"Infrastructure Starter Code: Rate Limiting Lua Script","id":"infrastructure-starter-code-rate-limiting-lua-script"},{"level":4,"text":"Core Logic Skeleton: Dependency Manager","id":"core-logic-skeleton-dependency-manager"},{"level":4,"text":"Language-Specific Hints for Extensions","id":"language-specific-hints-for-extensions"},{"level":4,"text":"Milestone Checkpoint for Rate Limiting Extension","id":"milestone-checkpoint-for-rate-limiting-extension"},{"level":4,"text":"Debugging Tips for Dependency Extensions","id":"debugging-tips-for-dependency-extensions"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Terms and Definitions","id":"terms-and-definitions"}],"title":"Background Job Processor: Design Document","markdown":"# Background Job Processor: Design Document\n\n\n## Overview\n\nThis document designs a distributed background job processing system similar to Celery/Sidekiq, enabling asynchronous execution of tasks with scheduling, retries, and monitoring. The key architectural challenge is ensuring reliable, ordered task execution across multiple worker processes while maintaining fault tolerance, performance, and observability in a distributed environment.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n> **Milestone(s):** This section provides foundational understanding for the entire project, establishing the core problems and mental models that all subsequent milestones will address.\n\n## Context and Problem Statement\n\nAt the heart of modern web applications lies a fundamental tension: user-facing requests must complete quickly (typically under 100-500 milliseconds) to provide responsive interfaces, while many necessary business operations are inherently slow or unpredictable. These operations—sending emails, processing uploaded images, generating reports, updating search indexes, synchronizing data with external APIs—can take seconds, minutes, or even hours to complete. Attempting to execute such work synchronously within request-response cycles creates intolerable user delays, timeout errors, and brittle systems that fail under load.\n\nThis section establishes why we need a dedicated background job processing system, explores why naive approaches fail at scale, and examines the architectural patterns that successful systems like Celery and Sidekiq employ. We begin with an intuitive analogy to build a mental model, then formalize the technical problem, and finally survey existing approaches to understand the trade-offs our design must navigate.\n\n### Mental Model: The Restaurant Kitchen Ticket System\n\nImagine a busy restaurant during dinner service. Customers (web requests) arrive and place orders (tasks) with the server (web application). Some orders are simple and quick—a glass of water (simple database query). Others are complex and time-consuming—a well-done steak with customized sides, sauce modifications, and allergy considerations (resource-intensive background work).\n\n**Naive Approach (Synchronous Kitchen):** The server takes the order, walks to the kitchen, stands by the grill, prepares the steak themselves, plates it, and then returns to the table 25 minutes later. During this time, the server cannot attend to other customers, the kitchen is cluttered with servers trying to cook, and customers wait endlessly. This is analogous to a web server process blocking on a long-running task within the HTTP request handler—terrible utilization of resources and terrible customer experience.\n\n**Background Job System (Kitchen Ticket System):** Instead, the server writes the order on a ticket (job) and places it in a queue (the ticket rail). The kitchen staff (worker processes) continuously monitor the ticket rail, grab the next ticket (dequeue a job), prepare the order (execute the task), and place the finished dish in the pickup area (store results). Meanwhile, the server is free to attend to other customers immediately after submitting the ticket. The ticket system provides:\n\n- **Decoupling:** Servers and cooks work independently at their own paces.\n- **Prioritization:** Urgent orders (fire tables, VIP customers) can jump the queue.\n- **Specialization:** Different stations (grill, sauté, pastry) handle specific ticket types.\n- **Resilience:** If a cook gets sick (worker crash), another can pick up their tickets.\n- **Visibility:** The chef can monitor ticket volume (dashboard) and reassign staff dynamically.\n\nThis mental model maps directly to our technical system:\n\n| Restaurant Concept | Technical Equivalent |\n|-------------------|----------------------|\n| Customer Order | Task/Job to be performed |\n| Server | Web application/API server |\n| Ticket | Job definition (serialized payload) |\n| Ticket Rail | Job queue (Redis list/stream) |\n| Kitchen Staff | Worker processes |\n| Cook Stations | Queue names/priorities |\n| Head Chef | Monitoring dashboard |\n| \"86\" Board (out of stock) | Dead letter queue |\n\nThe critical insight is that **asynchronous processing via queues decouples the act of requesting work from the act of executing it**, enabling each component to operate at its optimal scale and failure independently.\n\n### The Asynchronous Task Execution Problem\n\nFormally, the asynchronous task execution problem consists of four interconnected challenges that must be solved simultaneously:\n\n1. **Reliable Work Dispatch:** How do producers (web servers) reliably submit work units (jobs) such that no work is lost, even during system failures, while maintaining acceptable latency for the submission operation itself?\n\n2. **Scalable Work Consumption:** How do consumers (workers) efficiently retrieve and execute jobs with configurable concurrency, ensuring公平分配 across workers, handling varying workloads, and preventing single workers from becoming bottlenecks?\n\n3. **Fault Tolerance and Recovery:** When jobs fail (due to transient errors, resource constraints, or bugs), how does the system automatically retry them with intelligent backoff, and when retries are exhausted, how does it preserve failed jobs for inspection without blocking the queue?\n\n4. **Observability and Control:** How do operators monitor queue health, job progress, and system performance in real-time, and how can they intervene (pause, retry, cancel) when necessary?\n\nNaive approaches fail systematically as scale increases:\n\n| Naive Approach | Failure Mode | Consequence |\n|----------------|--------------|-------------|\n| **In-process threading** | One failing job crashes the entire worker process. No persistence across restarts. | Lost jobs, requires manual intervention. |\n| **Database-backed queues** (simple `jobs` table) | Polling causes high database load. Transactional locks create contention. | Database becomes bottleneck, poor performance at scale. |\n| **Direct RPC/HTTP calls to workers** | Tight coupling between producers and consumers. Worker failure causes producer failures. | Cascading failures, requires complex circuit breakers. |\n| **Cron jobs for recurring tasks** | No retry logic, poor error handling, duplicate execution if jobs run longer than interval. | Missed jobs, duplicated work, no visibility into failures. |\n| **Local filesystem queues** | Not accessible across multiple servers. Disk corruption loses all jobs. | Cannot scale horizontally, single point of failure. |\n\nThe core technical requirements that emerge are:\n\n- **Durability:** Jobs must survive process crashes and system restarts.\n- **Atomicity:** Job state transitions (enqueued → processing → completed) must occur atomically to prevent double-processing or lost jobs.\n- **Scalability:** The system must handle increasing job volumes by adding workers without reconfiguration.\n- **Isolation:** A misbehaving job should not affect other jobs or the system itself.\n- **Observability:** Every job's status, timing, and errors must be trackable.\n- **Configurability:** Retry policies, priorities, and scheduling must be adjustable per job type.\n\nOur design must address these requirements while remaining simple enough to implement, understand, and operate.\n\n### Existing Approaches and Trade-offs\n\nSeveral established solutions and patterns exist, each with distinct architectural choices and trade-offs. Understanding these helps inform our design decisions.\n\n**1. Dedicated Job Queue Systems (Celery, Sidekiq, RQ)**\n\nThese are full-featured frameworks that implement the complete pattern we're building.\n\n| System | Language | Broker | Architecture | Key Trade-offs |\n|--------|----------|---------|--------------|----------------|\n| **Celery** | Python | Redis, RabbitMQ, etc. | Distributed task queue with worker processes, beat scheduler | Extremely flexible but complex configuration. \"Batteries included\" leads to large dependency footprint. |\n| **Sidekiq** | Ruby | Redis | Multi-threaded workers within processes, emphasis on simplicity | Ruby-centric, requires Redis. Excellent performance but less language-agnostic. |\n| **RQ (Redis Queue)** | Python | Redis | Simple, minimal design. Single-threaded workers. | Easy to understand but limited features. No native scheduling, simpler retry logic. |\n\n**Common Architectural Patterns:**\n\n- **Broker-Centric Design:** All components communicate through a central broker (Redis, RabbitMQ). This provides a clear decoupling point but makes the broker a potential single point of failure and performance bottleneck.\n- **Process-per-Worker vs Thread-per-Worker:** Celery uses prefork process pools for isolation; Sidekiq uses threads within processes for memory efficiency. This is a fundamental trade-off between isolation and resource usage.\n- **Polling vs Event-Driven Workers:** Most workers poll the broker for jobs (BRPOP, basic.get). Advanced systems use event-driven protocols (AMQP's basic.consume) for lower latency but increased complexity.\n- **Embedded vs External Monitoring:** Some systems include web dashboards; others rely on external monitoring (Prometheus, Grafana).\n\n**2. Database as Queue**\n\nUsing a relational database table as a job queue is a common alternative, with two primary patterns:\n\n| Pattern | Implementation | Pros | Cons |\n|---------|---------------|------|------|\n| **Poll-based** | Workers SELECT FOR UPDATE SKIP LOCKED on a `jobs` table. | Leverages existing infrastructure, ACID guarantees. | High database load, scaling challenges, table bloat. |\n| **Notify-based** | PostgreSQL LISTEN/NOTIFY with SKIP LOCKED. | Reduced polling overhead. | Database connection overhead, still consumes DB resources. |\n\n> **Design Insight:** While tempting for simplicity, database-backed queues typically become the primary scalability bottleneck in growing systems. They couple job processing throughput to database capacity, which is expensive and difficult to scale horizontally compared to purpose-built brokers like Redis.\n\n**3. Message Brokers (RabbitMQ, Kafka, AWS SQS)**\n\nThese are general-purpose message brokers that can be used as job queues.\n\n| Broker | Model | Job Queue Suitability |\n|--------|-------|------------------------|\n| **RabbitMQ** | AMQP, exchanges/queues | Excellent for job queues: persistence, acknowledgments, dead letter exchanges. Requires more operational knowledge. |\n| **Kafka** | Log-based, partitioned streams | Better for event streaming. Job processing possible but lacks built-in per-message retry. Overkill for simple tasks. |\n| **AWS SQS** | Cloud queue service | Fully managed, simple API. Limited features (max 15-minute visibility timeout, no native scheduling). Vendor lock-in. |\n\n**4. Cloud-Native Services (AWS Lambda, Google Cloud Tasks)**\n\nServerless functions and managed task services represent a different architectural approach.\n\n| Service | Model | Trade-offs |\n|---------|-------|------------|\n| **AWS Lambda** | Event-driven functions | No infrastructure to manage, automatic scaling. Cold starts, limited execution time (15 min), vendor lock-in, debugging complexity. |\n| **Google Cloud Tasks** | Managed task queue | Fully managed, integrates with App Engine/Cloud Run. Vendor-specific, less control over underlying infrastructure. |\n\n**Key Architectural Decisions Our Design Must Address:**\n\nBased on these existing approaches, we identify several pivotal decisions:\n\n> **Decision: Broker Selection**\n> - **Context:** We need a durable, high-performance data store for job queues that supports multiple data structures (lists, sorted sets, hashes) and provides atomic operations.\n> - **Options Considered:** \n>   1. **Relational Database (PostgreSQL):** Familiar, ACID guarantees, but poor performance under high throughput.\n>   2. **Redis:** In-memory data structures with persistence options, extremely fast, supports needed operations (LPUSH, BRPOP, ZADD).\n>   3. **RabbitMQ:** Purpose-built message broker with advanced routing, but additional operational complexity.\n> - **Decision:** Use Redis as the primary broker.\n> - **Rationale:** Redis provides the necessary data structures with atomic operations, offers excellent performance, is widely available in cloud environments, and has a simple operational model. The project prerequisites already include Redis basics.\n> - **Consequences:** System performance is tied to Redis availability and memory capacity. Requires Redis persistence configuration for durability.\n\n> **Decision: Worker Concurrency Model**\n> - **Context:** Workers must execute multiple jobs concurrently to utilize multi-core systems efficiently while maintaining fault isolation.\n> - **Options Considered:**\n>   1. **Process Pool (prefork):** Each job runs in a separate process. Maximum isolation, but higher memory overhead.\n>   2. **Thread Pool:** Each job runs in a thread within a single process. Lower memory overhead, but one misbehaving job can affect others.\n>   3. **Green Threads/Async:** Single-threaded event loop with cooperative multitasking. Efficient for I/O-bound tasks but requires all job code to be async.\n> - **Decision:** Use a process pool for maximum isolation.\n> - **Rationale:** Since jobs can be arbitrary Python code (possibly buggy or resource-intensive), process isolation prevents one job from crashing the entire worker. This matches Celery's approach and provides the cleanest failure boundaries.\n> - **Consequences:** Higher memory usage, inter-process communication overhead for job state updates.\n\n> **Decision: Job Persistence Format**\n> - **Context:** Jobs must be serialized for storage in Redis and deserialized by workers.\n> - **Options Considered:**\n>   1. **JSON:** Human-readable, language-agnostic, but verbose and limited in data types.\n>   2. **MessagePack:** Binary, compact, preserves some type information, faster than JSON.\n>   3. **Pickle:** Python-specific, can serialize almost anything, but security risks with untrusted data.\n> - **Decision:** Use JSON for primary serialization with type-preserving extensions.\n> - **Rationale:** JSON is universally supported, debuggable (can inspect in Redis CLI), and safe. We'll add custom encoding for Python-specific types (datetime, Decimal) as needed. This balances simplicity with practicality.\n> - **Consequences:** Slightly larger payload size than binary formats, requires custom encoders/decoders for special types.\n\n| Comparison: Serialization Formats |\n|-----------------------------------|\n| Format | Pros | Cons | Our Choice? |\n| JSON | Human-readable, standard, safe | No binary data support, verbose | ✅ Primary choice |\n| MessagePack | Compact, fast, binary-safe | Less human-readable, extra dependency | ❌ Optional optimization |\n| Pickle | Handles Python objects natively | Security risks, Python-only | ❌ Too dangerous |\n\nThese decisions shape the architecture we'll detail in subsequent sections. The system we're designing occupies a middle ground: more feature-complete than RQ but simpler than Celery, with explicit trade-offs chosen for educational clarity and practical utility.\n\n### Implementation Guidance\n\n> **Note:** This section provides preliminary implementation guidance to set up the foundational concepts. Detailed code for each component will appear in subsequent sections.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Broker** | Redis (single instance) | Redis Cluster for horizontal scaling |\n| **Serialization** | JSON with `json` module | MessagePack with `msgpack` package |\n| **Process Management** | `multiprocessing.Pool` | `concurrent.futures.ProcessPoolExecutor` |\n| **HTTP Dashboard** | Flask + Jinja2 templates | FastAPI + WebSocket for real-time updates |\n| **Testing** | `unittest` with `fakeredis` | `pytest` with Redis fixtures |\n| **Packaging** | Single module | Package with `setup.py`/`pyproject.toml` |\n\n#### B. Recommended File/Module Structure\n\n```\nbackground-job-processor/\n├── pyproject.toml              # Project dependencies and metadata\n├── README.md\n├── src/\n│   └── jobqueue/              # Main package\n│       ├── __init__.py\n│       ├── exceptions.py      # Custom exceptions (JobError, QueueFullError, etc.)\n│       ├── models.py          # Core data models (Job, Queue, Worker, etc.)\n│       ├── serialization.py   # JSON encoding/decoding with custom extensions\n│       ├── redis_client.py    # Redis connection pooling and wrapper\n│       ├── queue_manager.py   # Milestone 1: Enqueue, priority, queue management\n│       ├── worker.py          # Milestone 2: Worker main loop, job execution\n│       ├── retry_manager.py   # Milestone 3: Exponential backoff, dead letter queue\n│       ├── scheduler.py       # Milestone 4: Delayed jobs, cron scheduling\n│       ├── monitor.py         # Milestone 5: Metrics collection, dashboard backend\n│       └── web/\n│           ├── __init__.py\n│           ├── app.py         # Flask/FastAPI application\n│           ├── templates/     # Jinja2 templates for dashboard\n│           └── static/        # CSS, JavaScript\n├── tests/\n│   ├── __init__.py\n│   ├── test_models.py\n│   ├── test_queue_manager.py\n│   └── ...\n└── scripts/\n    ├── worker-cli.py          # CLI to start workers\n    └── enqueue-job.py         # CLI to enqueue jobs manually\n```\n\n#### C. Infrastructure Starter Code: Redis Client Wrapper\n\n```python\n# src/jobqueue/redis_client.py\n\"\"\"\nRedis connection management with connection pooling and error handling.\n\"\"\"\nimport redis\nfrom typing import Optional, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass RedisClient:\n    \"\"\"Wrapper around Redis client with connection pooling and retry logic.\"\"\"\n    \n    _instance: Optional['RedisClient'] = None\n    _client: Optional[redis.Redis] = None\n    \n    def __init__(self, url: str = \"redis://localhost:6379/0\", **kwargs):\n        \"\"\"\n        Initialize Redis connection.\n        \n        Args:\n            url: Redis connection URL (redis://[:password]@host:port/db)\n            **kwargs: Additional arguments passed to redis.Redis()\n        \"\"\"\n        if RedisClient._instance is not None:\n            raise RuntimeError(\"Use RedisClient.get_instance() to get singleton\")\n        \n        self.url = url\n        self.connection_kwargs = kwargs\n        self._connect()\n    \n    def _connect(self) -> None:\n        \"\"\"Establish Redis connection with retry logic.\"\"\"\n        try:\n            self._client = redis.Redis.from_url(\n                self.url,\n                decode_responses=True,  # Automatically decode bytes to str\n                socket_connect_timeout=5,\n                socket_timeout=5,\n                retry_on_timeout=True,\n                **self.connection_kwargs\n            )\n            # Test connection\n            self._client.ping()\n            logger.info(f\"Connected to Redis at {self.url}\")\n        except redis.ConnectionError as e:\n            logger.error(f\"Failed to connect to Redis at {self.url}: {e}\")\n            raise\n    \n    @classmethod\n    def get_instance(cls, url: Optional[str] = None, **kwargs) -> 'RedisClient':\n        \"\"\"Get singleton RedisClient instance.\"\"\"\n        if cls._instance is None:\n            if url is None:\n                url = \"redis://localhost:6379/0\"\n            cls._instance = cls(url, **kwargs)\n        return cls._instance\n    \n    @classmethod\n    def get_client(cls) -> redis.Redis:\n        \"\"\"Get the underlying Redis client instance.\"\"\"\n        if cls._instance is None:\n            cls.get_instance()\n        if cls._instance._client is None:\n            cls._instance._connect()\n        return cls._instance._client\n    \n    def execute(self, command: str, *args, **kwargs) -> Any:\n        \"\"\"\n        Execute a Redis command with error handling.\n        \n        Args:\n            command: Redis command name (e.g., 'lpush', 'zadd')\n            *args: Command arguments\n            **kwargs: Additional options\n            \n        Returns:\n            Command result\n            \n        Raises:\n            redis.RedisError: If Redis operation fails\n        \"\"\"\n        client = self.get_client()\n        try:\n            method = getattr(client, command)\n            return method(*args, **kwargs)\n        except redis.RedisError as e:\n            logger.error(f\"Redis command {command} failed: {e}\")\n            raise\n    \n    def pipeline(self):\n        \"\"\"Return a Redis pipeline for atomic operations.\"\"\"\n        return self.get_client().pipeline()\n\n# Convenience function for easy access\ndef get_redis():\n    \"\"\"Get the Redis client instance.\"\"\"\n    return RedisClient.get_client()\n```\n\n#### D. Core Logic Skeleton: Job Model\n\n```python\n# src/jobqueue/models.py\n\"\"\"\nCore data models for the job queue system.\n\"\"\"\nimport json\nimport uuid\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass, field, asdict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass JobStatus(Enum):\n    \"\"\"Status of a job through its lifecycle.\"\"\"\n    PENDING = \"pending\"\n    ACTIVE = \"active\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    RETRY_SCHEDULED = \"retry_scheduled\"\n    DEAD_LETTER = \"dead_letter\"\n\nclass Job:\n    \"\"\"\n    Represents a unit of work to be processed asynchronously.\n    \n    This is the core data structure that gets serialized and stored in Redis.\n    \"\"\"\n    \n    def __init__(\n        self,\n        job_type: str,\n        args: List[Any] = None,\n        kwargs: Dict[str, Any] = None,\n        job_id: Optional[str] = None,\n        queue: str = \"default\",\n        priority: int = 0,\n        max_retries: int = 3,\n        timeout_seconds: int = 300,\n        created_at: Optional[datetime] = None,\n        **extra_metadata\n    ):\n        \"\"\"\n        Initialize a job.\n        \n        Args:\n            job_type: String identifier for the handler function/class\n            args: Positional arguments for the job handler\n            kwargs: Keyword arguments for the job handler\n            job_id: Unique identifier (auto-generated if None)\n            queue: Target queue name\n            priority: Higher priority jobs are processed first\n            max_retries: Maximum number of retry attempts before giving up\n            timeout_seconds: Maximum execution time in seconds\n            created_at: Job creation timestamp (auto-generated if None)\n            **extra_metadata: Additional metadata stored with the job\n        \"\"\"\n        self.job_id = job_id or str(uuid.uuid4())\n        self.job_type = job_type\n        self.args = args or []\n        self.kwargs = kwargs or {}\n        self.queue = queue\n        self.priority = priority\n        self.max_retries = max_retries\n        self.timeout_seconds = timeout_seconds\n        self.created_at = created_at or datetime.utcnow()\n        self.metadata = extra_metadata\n        \n        # Runtime state (not serialized)\n        self.status = JobStatus.PENDING\n        self.attempts = 0\n        self.errors: List[Dict[str, Any]] = []\n        self.started_at: Optional[datetime] = None\n        self.completed_at: Optional[datetime] = None\n        self.result: Optional[Any] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Convert job to dictionary for serialization.\n        \n        Returns:\n            Dictionary representation suitable for JSON serialization\n        \"\"\"\n        data = {\n            'job_id': self.job_id,\n            'job_type': self.job_type,\n            'args': self.args,\n            'kwargs': self.kwargs,\n            'queue': self.queue,\n            'priority': self.priority,\n            'max_retries': self.max_retries,\n            'timeout_seconds': self.timeout_seconds,\n            'created_at': self.created_at.isoformat() if self.created_at else None,\n            'metadata': self.metadata,\n            'attempts': self.attempts,\n            'errors': self.errors,\n            'status': self.status.value,\n        }\n        # Only include runtime fields if they exist\n        if self.started_at:\n            data['started_at'] = self.started_at.isoformat()\n        if self.completed_at:\n            data['completed_at'] = self.completed_at.isoformat()\n        if self.result is not None:\n            data['result'] = self.result\n        return data\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Job':\n        \"\"\"\n        Reconstruct a job from dictionary representation.\n        \n        Args:\n            data: Dictionary from Redis storage\n            \n        Returns:\n            Reconstructed Job instance\n        \"\"\"\n        # TODO 1: Extract core fields from data dict\n        # TODO 2: Parse datetime strings back to datetime objects\n        # TODO 3: Reconstruct job with all fields\n        # TODO 4: Set runtime state fields (status, attempts, errors, etc.)\n        # TODO 5: Return the reconstructed job instance\n        # Hint: Use datetime.fromisoformat() for parsing\n        pass\n    \n    def serialize(self) -> str:\n        \"\"\"\n        Serialize job to JSON string for storage in Redis.\n        \n        Returns:\n            JSON string representation\n        \"\"\"\n        # TODO 1: Convert job to dictionary using to_dict()\n        # TODO 2: Serialize dictionary to JSON string\n        # TODO 3: Validate that serialized size is under 1MB limit\n        # TODO 4: If over limit, raise PayloadTooLargeError\n        # TODO 5: Return the JSON string\n        pass\n    \n    @classmethod\n    def deserialize(cls, data: str) -> 'Job':\n        \"\"\"\n        Deserialize job from JSON string.\n        \n        Args:\n            data: JSON string from Redis\n            \n        Returns:\n            Deserialized Job instance\n        \"\"\"\n        # TODO 1: Parse JSON string to dictionary\n        # TODO 2: Reconstruct job using from_dict()\n        # TODO 3: Return the job instance\n        pass\n    \n    def record_error(self, error: Exception) -> None:\n        \"\"\"\n        Record an error that occurred during job execution.\n        \n        Args:\n            error: Exception that was raised\n        \"\"\"\n        error_info = {\n            'type': error.__class__.__name__,\n            'message': str(error),\n            'timestamp': datetime.utcnow().isoformat(),\n            'attempt': self.attempts,\n        }\n        # TODO: Optionally add stack trace in debug mode\n        self.errors.append(error_info)\n    \n    def should_retry(self) -> bool:\n        \"\"\"\n        Determine if job should be retried based on attempts and max_retries.\n        \n        Returns:\n            True if job should be retried, False otherwise\n        \"\"\"\n        # TODO 1: Check if attempts < max_retries\n        # TODO 2: Return True if should retry, False otherwise\n        pass\n```\n\n#### E. Language-Specific Hints: Python\n\n- **Use `datetime.utcnow()`** instead of `datetime.now()` for timestamps to avoid timezone issues across servers.\n- **JSON serialization of datetime:** Use ISO format strings (`datetime.isoformat()`), which are both human-readable and parseable.\n- **Redis connections:** Always use `decode_responses=True` when creating Redis client to avoid byte-string handling.\n- **Connection pooling:** Let `redis.Redis` handle connection pooling internally; create one client instance and reuse it.\n- **Process vs Thread:** Use `multiprocessing` for worker pools to isolate job crashes, but be aware that objects passed to worker processes must be picklable.\n- **Graceful shutdown:** Catch `KeyboardInterrupt` and `SystemExit` in worker main loop, and use `signal.signal()` to handle SIGTERM.\n\n#### F. Milestone Checkpoint: After Section Completion\n\nAfter understanding this section (but before implementing Milestone 1), you should be able to:\n\n1. **Explain in your own words** why background job processing is necessary and the failures of naive approaches.\n2. **Draw the restaurant ticket system analogy** and map each component to the technical system.\n3. **Articulate the trade-offs** between Redis, database-backed, and message broker queues.\n4. **Run the Redis starter code** successfully:\n\n```bash\n# Start Redis (if not already running)\ndocker run -d -p 6379:6379 redis:7-alpine\n\n# Test the Redis connection\npython -c \"from src.jobqueue.redis_client import RedisClient; r = RedisClient.get_instance(); print(r.execute('ping'))\"\n# Should print: True\n```\n\n5. **Create a Job instance** and inspect its dictionary representation:\n\n```python\nfrom src.jobqueue.models import Job\n\njob = Job(\n    job_type=\"send_email\",\n    args=[\"user@example.com\", \"Welcome!\"],\n    kwargs={\"template\": \"welcome\"},\n    queue=\"emails\",\n    priority=1,\n    max_retries=3\n)\n\nprint(job.job_id)  # Should show a UUID\nprint(job.to_dict())  # Should show dictionary with all fields\n```\n\n**Signs something is wrong:**\n- Redis connection fails: Check if Redis is running and the URL is correct.\n- Job serialization fails: Ensure all data types are JSON-serializable (no datetime objects in args/kwargs directly).\n- Import errors: Verify your Python path includes the `src` directory or install the package in development mode with `pip install -e .`.\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"ModuleNotFoundError: No module named 'jobqueue'\" | Python cannot find the package | Run `python -c \"import sys; print(sys.path)\"` to see Python path | Install with `pip install -e .` or set `PYTHONPATH` |\n| Redis connection timeout | Redis not running or wrong port | Run `redis-cli ping` from terminal | Start Redis: `docker run -d -p 6379:6379 redis` |\n| \"datetime is not JSON serializable\" | Direct datetime objects in job data | Check job args/kwargs for datetime objects | Use ISO format strings or extend JSON encoder |\n| Job ID collisions | Using simple incrementing IDs | Check ID generation logic | Use UUIDs (uuid4) or ULIDs for distributed uniqueness |\n| Cannot pickle local object | Passing unpicklable objects to multiprocessing | Check what's in job args/kwargs | Ensure all arguments are picklable (no lambdas, local functions) |\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** This section establishes the fundamental requirements and boundaries that apply across all milestones, providing the foundation for architectural decisions made throughout the entire system design. These goals guide implementation priorities and trade-offs from Milestone 1 through Milestone 5.\n\n### Functional Requirements (Goals)\n\nThink of our job processor as an **industrial factory** that must reliably transform raw materials (job requests) into finished products (completed work). The factory must operate 24/7, handle different product lines with varying priorities, recover from equipment failures, and provide supervisors with real-time visibility into operations. These functional requirements represent the core capabilities our factory must deliver to be commercially viable.\n\n| Category | Requirement | Description | Implementation Impact |\n|----------|-------------|-------------|----------------------|\n| **Job Management** | Enqueue jobs with metadata | Jobs must be enqueued with JSON-serialized payloads containing job type, arguments, priority, retry limits, and custom metadata. Each job receives a unique ID (UUID/ULID) for tracking. | Foundation for all job operations; requires robust serialization and validation (Milestone 1). |\n| | Multiple priority queues | Support at least 3-5 named queues with configurable priority weights. Higher-priority queues are polled more frequently by workers while maintaining fairness. | Impacts worker polling algorithm and requires weighted queue selection logic (Milestone 1). |\n| | Queue inspection | APIs to check queue length, peek at pending jobs, list all queues, and retrieve job status by ID. Essential for operational visibility. | Adds monitoring endpoints and Redis query patterns (Milestones 1, 5). |\n| **Processing** | FIFO job execution | Within the same priority level, jobs execute in first-in-first-out order. Priority weights determine which queue's jobs are processed next. | Guides Redis data structure selection (list vs. sorted set) and dequeue algorithms (Milestone 1). |\n| | Concurrent execution | Single worker process must handle multiple jobs concurrently using thread/process pools. Configurable concurrency level (default: CPU count). | Affects worker architecture, resource isolation, and coordination (Milestone 2). |\n| | Graceful shutdown | Workers must complete current jobs before exiting when receiving SIGTERM/SIGINT. No partial state or orphaned jobs after shutdown. | Requires signal handlers and job state persistence (Milestone 2). |\n| **Reliability** | Automatic retries with backoff | Failed jobs automatically retry with exponential backoff (1s, 2s, 4s... up to max retries). Configurable jitter prevents thundering herd. | Requires retry scheduler, delayed execution mechanism, and attempt tracking (Milestone 3). |\n| | Dead letter queue | Jobs exhausting max retries move to a dedicated dead letter queue with full error history. Can be manually retried or inspected. | Creates new storage location and requires error serialization (Milestone 3). |\n| | Worker heartbeat & failure detection | Workers report liveness at regular intervals (e.g., every 30s). Stale workers (no heartbeat for 2x interval) are detected and their jobs returned to queue. | Requires periodic background tasks and stale worker cleanup (Milestone 2). |\n| **Scheduling** | Delayed execution | Jobs can be scheduled for future execution at specific timestamps. Must remain invisible to workers until scheduled time arrives. | Requires time-based data structure (Redis sorted set) and scheduler process (Milestone 4). |\n| | Recurring jobs (cron) | Define jobs that run on schedule using standard cron syntax (5-field: minute, hour, day, month, weekday). Unique constraints prevent duplicate execution. | Requires cron parser, schedule storage, and catch-up logic for missed runs (Milestone 4). |\n| **Monitoring** | Real-time dashboard | Web dashboard showing queue depths, active workers, processing rates, error rates, and recent job history. Updates without page refresh. | Requires WebSocket/SSE, metrics aggregation, and UI components (Milestone 5). |\n| | Job search & filtering | Search jobs by ID, status, queue, time range, or job type. Paginated results for large datasets. | Creates query interface over job history data (Milestone 5). |\n| | Alerting | Configurable alerts when queue backlog exceeds threshold or error rate spikes above limit. Notify via webhook or email. | Requires background monitoring and notification system (Milestone 5). |\n| **Operational** | Payload validation | Reject jobs with payloads >1MB at enqueue time. Validate JSON structure and required fields. | Prevents memory issues and malformed job processing (Milestone 1). |\n| | Job timeout enforcement | Kill jobs exceeding their configured timeout (default: 30 minutes). Prevent worker starvation by long-running jobs. | Requires subprocess management and timeout monitoring (Milestone 2). |\n| | Cross-language compatibility | Job payloads serializable/deserializable across Python, Go, and Rust clients. Use standard formats (JSON/msgpack). | Influences serialization format choice and type system design (Milestone 1). |\n\n> **Key Insight:** The progression from Milestone 1 to 5 represents a journey from core job mechanics (enqueue/dequeue) through reliability features (retries), time-based operations (scheduling), and finally operational excellence (monitoring). Each milestone builds on the previous one—you cannot implement retries without a working worker system, nor scheduling without reliable queue infrastructure.\n\n#### Job Lifecycle Requirements\n\nThe system must support the complete job lifecycle shown in the state machine diagram:\n\n![Job State Machine](./diagrams/job-state-machine.svg)\n\n1. **Creation → Pending:** Jobs enter the system via `enqueue()` and await worker pickup.\n2. **Pending → Active:** Worker claims job via atomic dequeue operation.\n3. **Active → Completed:** Job executes successfully, result stored.\n4. **Active → Failed:** Job execution throws exception, error recorded.\n5. **Failed → Retry Scheduled:** If retries remain, job scheduled for future retry with backoff.\n6. **Retry Scheduled → Pending:** Scheduler moves job back to queue after backoff delay.\n7. **Failed → Dead Letter:** Max retries exhausted, job moved to dead letter queue.\n8. **Dead Letter → Pending (manual):** Admin manually retries job from dashboard.\n\nEach transition must be atomic and leave the system in a consistent state, even during failures.\n\n### Non-Functional Requirements\n\nConsider these as the **factory's operating standards**—not what it produces, but how well it operates. A factory might produce excellent widgets but if it consumes excessive power, breaks down frequently, or requires specialized operators, it's not commercially viable. These requirements ensure our job processor operates efficiently, reliably, and maintainably in production environments.\n\n| Category | Requirement | Metric | Rationale |\n|----------|-------------|--------|-----------|\n| **Performance** | Enqueue latency | ≤10ms P99 for single job enqueue | Producers shouldn't block waiting for job acceptance. |\n| | Dequeue latency | ≤50ms P99 for worker to get next job | Workers should spend minimal time waiting for work. |\n| | Throughput | ≥1000 jobs/sec per worker process | Handle moderate workloads on modest hardware. |\n| | Memory efficiency | ≤50MB baseline per worker process | Enable running multiple workers on same machine. |\n| **Reliability** | Durability | Job persistence ≥99.9% after successful enqueue | Jobs shouldn't disappear once accepted. |\n| | Availability | System operational ≥99.5% of time | Allow for maintenance and occasional failures. |\n| | Data consistency | No job loss during graceful shutdown | Critical for user trust in the system. |\n| | Failure recovery | ≤5 minutes to recover from Redis restart | Quick recovery from infrastructure issues. |\n| **Scalability** | Horizontal scaling | Linear throughput increase with added workers | Support growing workloads by adding resources. |\n| | Queue isolation | One misbehaving queue doesn't block others | Fault isolation between different job types. |\n| | Redis connection efficiency | ≤100 connections per worker pool | Prevent overwhelming Redis with connections. |\n| **Operational** | Monitoring granularity | Metrics updated every 30 seconds | Balance real-time visibility with system load. |\n| | Dashboard responsiveness | ≤2 second page load for 10k job history | Operators need quick access to system state. |\n| | Configuration simplicity | ≤5 required configuration parameters | Reduce deployment friction and errors. |\n| | Documentation completeness | All public APIs documented with examples | Enable adoption and reduce support burden. |\n| **Security** | Input validation | All user input validated before processing | Prevent injection attacks and malformed data. |\n| | Dashboard authentication | Optional basic auth for web dashboard | Protect operational interface in production. |\n| | Job payload encryption | Support for encrypted payloads (optional) | Handle sensitive job data requirements. |\n| | Redis security | Support Redis ACL and TLS connections | Enterprise deployment requirements. |\n\n> **Design Principle:** These non-functional requirements create concrete constraints for architectural decisions. For example, the ≤10ms enqueue latency requirement rules out complex validation that requires external service calls, while the ≥99.9% durability requirement necessitates synchronous Redis writes (or WAL) for critical operations.\n\n#### Performance Trade-off Decisions\n\nThe system makes deliberate trade-offs between competing non-functional requirements:\n\n| Trade-off | Choice | Rationale | Consequence |\n|-----------|--------|-----------|-------------|\n| Latency vs. Durability | **Durability with fsync every 1s** | Job loss is worse than slightly higher latency for background jobs. | Enqueue operations may block during Redis persistence. |\n| Memory vs. Functionality | **Keep 10k most recent job history** | Most debugging uses recent jobs; older history can be archived. | Dashboard job search limited to recent history without external storage. |\n| Real-time vs. Load | **30-second metric aggregation** | Real-time updates aren't critical for background job monitoring. | Dashboard shows near-real-time but not exact current state. |\n| Simplicity vs. Features | **Polling scheduler vs. event-based** | Simpler to implement and debug; acceptable for scheduled jobs. | Scheduler consumes CPU even when no jobs are due. |\n\n### Explicit Non-Goals\n\nJust as important as what we build is what we **deliberately choose not to build**. A factory producing automobiles shouldn't also try to refine gasoline—it should focus on its core competency and integrate with specialists. These non-goals prevent scope creep, keep the implementation focused, and acknowledge that some problems are better solved by other systems or future extensions.\n\n| Non-Goal | Explanation | Alternative/Workaround |\n|----------|-------------|------------------------|\n| **Job dependency graphs** | No support for complex workflows where job B depends on job A's completion. | Chain jobs by having job A enqueue job B upon completion, or use external workflow engine. |\n| **Exactly-once execution** | Cannot guarantee jobs execute exactly once; at-least-once is the best guarantee. | Design jobs to be idempotent; include idempotency keys in job metadata for deduplication. |\n| **Job migration during upgrades** | No automatic migration of in-flight jobs during code deployments. | Drain queues before deployment or use versioned job handlers that can process multiple formats. |\n| **Multi-broker support** | Only Redis is supported as the broker; no pluggable broker architecture. | Use Redis Sentinel/Cluster for HA; future extension could abstract broker interface. |\n| **Built-in rate limiting** | No per-user or per-job-type rate limiting in the core. | Implement in job handlers or use Redis-backed rate limiter before enqueueing jobs. |\n| **Job prioritization within queue** | All jobs in same queue are FIFO; no sub-prioritization within queue. | Use multiple queues with different priorities or add timestamp-based ordering within queue. |\n| **Long-term job archival** | Job history older than configured retention is deleted, not archived. | External system can consume job completion events and archive to cold storage. |\n| **Built-in user management** | Dashboard has simple auth or none; no RBAC or multi-user permission system. | Place dashboard behind reverse proxy with authentication for production use. |\n| **Geographic job routing** | Jobs cannot be routed to workers in specific regions/datacenters. | Use separate Redis instances per region or implement routing in producer logic. |\n| **Real-time job streaming** | Cannot subscribe to job events via streaming APIs (though events are emitted). | Use Redis pub/sub or add webhook callbacks as extension. |\n| **Job pausing/resuming** | Cannot pause a specific job's execution once started. | Implement checkpointing within job handler code for long-running jobs. |\n| **Automatic scaling of workers** | No auto-scaling based on queue depth; manual worker management required. | Use container orchestration (K8s HPA) that monitors queue metrics to scale workers. |\n| **Built-in job versioning** | No automatic handling of breaking changes to job payload formats. | Use version field in job metadata and handler version compatibility logic. |\n| **Transactional enqueue** | Cannot atomically enqueue multiple jobs across queues. | Use Redis pipelines for best-effort atomicity or implement compensating transactions. |\n\n> **Architectural Philosophy:** By explicitly stating these non-goals, we acknowledge common requests but defer them to keep the initial implementation manageable. The architecture should be extensible enough to add these features later without major redesign. For example, while we don't build job dependency graphs, the system's webhook events could feed into an external workflow orchestrator.\n\n#### Boundary Decisions with Rationale\n\nFor each major non-goal, we made explicit architectural decisions:\n\n**Decision: No Built-in Job Dependencies**\n- **Context:** Many background job systems need to chain jobs (A → B → C) or create complex workflows.\n- **Options Considered:** \n  1. **Built-in DAG engine:** Complex, requires persistent graph storage and complex scheduling.\n  2. **Simple chaining:** Job A enqueues job B on completion—simple but creates tight coupling.\n  3. **External workflow system:** Keep job processor simple, integrate with specialized workflow engines.\n- **Decision:** Option 3—no built-in dependencies, but emit completion events for external integration.\n- **Rationale:** Job dependencies belong in a workflow layer, not the core job execution engine. Most users can implement simple chaining in job handlers, while complex workflows need specialized tools (Airflow, Temporal) that our system can feed into.\n- **Consequences:** Users must implement chaining manually or use external system; system remains simple and focused.\n\n**Decision: At-Least-Once Semantics Only**\n- **Context:** Exactly-once execution requires distributed transactions and persistent deduplication.\n- **Options Considered:**\n  1. **Exactly-once with idempotency keys:** Complex, requires persistent store of processed IDs.\n  2. **At-least-once with idempotent jobs:** Simpler, pushes idempotency responsibility to job handlers.\n  3. **At-most-once:** Unacceptable for reliable job processing.\n- **Decision:** Option 2—guarantee jobs execute at least once, possibly multiple times during failures.\n- **Rationale:** Exactly-once semantics are extremely difficult in distributed systems and often require application-level idempotency anyway. By embracing at-least-once, we simplify the core system dramatically while providing the tools (idempotency keys, job metadata) for handlers to implement idempotency.\n- **Consequences:** Job handlers must be idempotent; system cannot guarantee no duplicates during network partitions.\n\n**Decision: Single Broker (Redis) Architecture**\n- **Context:** Production systems might want to use different brokers (RabbitMQ, Kafka, SQS) for different needs.\n- **Options Considered:**\n  1. **Pluggable broker interface:** Abstract broker operations behind interface, support multiple backends.\n  2. **Redis-only with extension points:** Optimize for Redis, allow extensions for other brokers.\n  3. **Multi-broker from day one:** Support 2-3 popular brokers in initial implementation.\n- **Decision:** Option 2—Redis-only core with clear abstraction boundaries for future broker support.\n- **Rationale:** Redis provides all needed functionality with excellent performance and reliability. Supporting multiple brokers initially would triple implementation complexity. By keeping clean internal interfaces, we can add other brokers later without breaking existing Redis users.\n- **Consequences:** Users locked into Redis; migration to other brokers requires code changes later.\n\n### Implementation Guidance\n\n> **Implementation Note:** While this section doesn't directly translate to code, the goals and non-goals defined here should guide every implementation decision. Before writing any code, ensure your design addresses each functional requirement while respecting the non-goals.\n\n#### Technology Recommendations\n\n| Component | Simple Option (Beginner-Friendly) | Advanced Option (Production-Ready) |\n|-----------|-----------------------------------|-----------------------------------|\n| **Serialization** | JSON with Python's `json` module | MessagePack (`msgpack`) for better performance and binary data |\n| **Redis Client** | `redis-py` with connection pooling | `redis-py` with client-side sharding for cluster support |\n| **Concurrency** | `concurrent.futures.ThreadPoolExecutor` | `multiprocessing.Pool` with process isolation for CPU-bound jobs |\n| **Web Dashboard** | Flask + Jinja2 templates with auto-refresh | FastAPI + WebSocket for real-time updates + React frontend |\n| **Cron Parsing** | `croniter` library for schedule calculation | Custom parser supporting extended cron syntax with timezones |\n| **Metrics** | Custom Redis counters updated after job completion | Prometheus client library + Grafana for advanced visualization |\n\n#### Recommended Configuration Structure\n\nCreate a configuration system that balances simplicity (≤5 required parameters) with flexibility:\n\n```python\n# config.py\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, List\n\n@dataclass\nclass QueueConfig:\n    name: str\n    priority: int = 1  # Higher = more frequent polling\n    max_length: Optional[int] = None  # Optional queue size limit\n\n@dataclass\nclass WorkerConfig:\n    queues: List[str]  # Queues this worker polls\n    concurrency: int = 4  # Number of concurrent job executions\n    heartbeat_interval: int = 30  # Seconds between heartbeats\n    job_timeout: int = 1800  # Default job timeout in seconds (30 min)\n\n@dataclass\nclass RedisConfig:\n    url: str = \"redis://localhost:6379/0\"\n    max_connections: int = 100\n    socket_timeout: int = 5\n    retry_on_timeout: bool = True\n\n@dataclass\nclass SystemConfig:\n    redis: RedisConfig\n    queues: List[QueueConfig]\n    workers: List[WorkerConfig]\n    \n    # Non-functional defaults\n    max_payload_size: int = 1024 * 1024  # 1MB\n    job_history_size: int = 10000  # Keep last 10k jobs\n    retry_base_delay: int = 1  # Base delay in seconds for exponential backoff\n    retry_max_attempts: int = 5  # Default max retries\n    \n    @classmethod\n    def from_env(cls) -> \"SystemConfig\":\n        \"\"\"Load configuration from environment variables with defaults.\"\"\"\n        return cls(\n            redis=RedisConfig(\n                url=os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n            ),\n            queues=[\n                QueueConfig(name=\"default\", priority=1),\n                QueueConfig(name=\"high\", priority=3),\n                QueueConfig(name=\"low\", priority=1),\n            ],\n            workers=[\n                WorkerConfig(\n                    queues=[\"default\", \"high\", \"low\"],\n                    concurrency=int(os.getenv(\"WORKER_CONCURRENCY\", \"4\"))\n                )\n            ]\n        )\n```\n\n#### Milestone Goal Mapping\n\nAs you implement each milestone, verify your work against these specific goal checkpoints:\n\n| Milestone | Must-Have Goal | How to Verify |\n|-----------|----------------|---------------|\n| **1** | Jobs enqueue and dequeue in FIFO order | Enqueue 10 jobs, dequeue 10 jobs - verify same order. |\n| | Payloads >1MB rejected | Try enqueuing 2MB payload - get validation error. |\n| **2** | Worker processes jobs concurrently | Enqueue 5 jobs that each sleep 1s - all complete in ~1s with concurrency=5. |\n| | Graceful shutdown on SIGTERM | Send SIGTERM to worker during job execution - job completes before exit. |\n| **3** | Exponential backoff with jitter | Fail job 3 times - verify retry delays are ~1s, ~2s, ~4s ± jitter. |\n| | Dead letter queue after max retries | Fail job beyond max retries - find it in dead letter queue. |\n| **4** | Delayed job execution | Schedule job for 5s future - verify it executes ~5s after enqueue. |\n| | Cron job re-enqueues | Create cron job for every minute - see it enqueue automatically each minute. |\n| **5** | Real-time dashboard updates | Open dashboard, enqueue job - see queue depth update without refresh. |\n| | Job search by criteria | Enqueue jobs with different metadata - search finds them by attributes. |\n\n#### Common Implementation Pitfalls to Avoid\n\nWhile implementing to meet these goals, watch for these specific pitfalls:\n\n⚠️ **Pitfall: Over-engineering for edge cases**\n- **Description:** Trying to handle every possible failure mode or feature request from the start.\n- **Why it's wrong:** Violates YAGNI principle, dramatically increases complexity, delays delivery of core functionality.\n- **Fix:** Implement only the requirements from the table above. Add extensibility points for future enhancements but don't implement them now.\n\n⚠️ **Pitfall: Ignoring non-functional requirements**\n- **Description:** Building functionally correct system that's too slow, uses too much memory, or can't scale.\n- **Why it's wrong:** System may work in development but fail in production under load.\n- **Fix:** Measure performance early—profile enqueue/dequeue latency, memory usage of workers, Redis connection counts.\n\n⚠️ **Pitfall: Violating explicit non-goals**\n- **Description:** Adding \"just one small feature\" that's explicitly listed as a non-goal.\n- **Why it's wrong:** Creates scope creep, introduces complexity for features users shouldn't depend on.\n- **Fix:** When tempted to add a feature, check if it's in the non-goals table. If yes, document it as a future extension but don't implement now.\n\n⚠️ **Pitfall: Hardcoding instead of configuring**\n- **Description:** Embedding queue names, timeouts, retry counts in code instead of configuration.\n- **Why it's wrong:** Requires code changes for operational adjustments, limits deployment flexibility.\n- **Fix:** Use the configuration pattern shown above for all tunable parameters.\n\n---\n\n\n## High-Level Architecture\n\n> **Milestone(s):** This section provides the architectural foundation for all five milestones, defining the core components, their responsibilities, and how they interact. It establishes the blueprint that each subsequent milestone's detailed component design will follow and extend.\n\nThis section presents the **bird's-eye view** of the Background Job Processor system. Before diving into individual components like queues or workers, it's essential to understand the complete picture—how all parts fit together into a cohesive whole. The architecture is designed around a central **broker** (Redis) that acts as the system's nervous system, connecting all components while maintaining loose coupling and enabling horizontal scaling. We'll explore this through three lenses: the static view of components and their responsibilities, the dynamic view of how they communicate, and the structural view of how code should be organized.\n\n### System Components and Responsibilities\n\nThink of the system as a **distributed factory** with specialized departments. Each department has a specific role, operates independently, but communicates through a central dispatch office (Redis). This separation of concerns allows each component to be developed, deployed, and scaled independently.\n\n![System Component Overview](./diagrams/system-component.svg)\n\nThe system comprises seven core components, each with distinct responsibilities:\n\n| Component | Primary Responsibility | Key Data It Owns/Maintains | Interaction Partners |\n|-----------|------------------------|----------------------------|----------------------|\n| **Producer** | Creates and submits jobs for asynchronous execution | Local job definitions and arguments (not persisted centrally) | Queue Manager (via client library) |\n| **Queue Manager** | Validates, prioritizes, and stores incoming jobs into appropriate queues | Queue configurations, job serialization/deserialization logic | Producer, Redis (broker), Worker |\n| **Worker** | Fetches jobs from queues and executes the corresponding task logic | Worker process state, job execution environment, thread/process pool | Queue Manager (via Redis), Job Handlers, Monitoring |\n| **Scheduler** | Manages delayed and recurring jobs based on cron expressions | Schedule definitions, next execution timestamps, uniqueness constraints | Redis (for scheduled sets), Queue Manager |\n| **Retry Manager** | Handles failed jobs with exponential backoff and manages dead letter queue | Retry policies, backoff calculations, error history | Worker (on failure), Redis (for retry sets) |\n| **Monitor/Dashboard** | Provides real-time visibility into system health and job status | Aggregated metrics, job history, alert configurations | All components (via Redis and direct reporting) |\n| **Redis (Broker)** | Serves as the central data store for all job and system state | Job queues, scheduled sets, retry sets, worker heartbeats, metrics | All other components |\n\nLet's examine each component in detail:\n\n**Producer** is any application code that needs to perform work asynchronously. It's not a separate service but a **client library** integrated into your application. The producer's sole responsibility is to define a job (type, arguments, metadata) and hand it off to the Queue Manager. It doesn't wait for job completion—this is the essence of asynchronous execution.\n\n**Queue Manager** acts as the **post office sorting facility** for jobs. When a producer submits a job, the Queue Manager validates the payload size (rejecting oversized jobs), assigns a unique job ID (ULID for time-ordered uniqueness), serializes the job to JSON, and determines which queue it belongs to based on the job type and priority configuration. It then uses Redis list operations to atomically enqueue the job. The Queue Manager exposes a clean API (`enqueue`, `bulk_enqueue`, `queue_stats`) that producers call.\n\n**Worker** is the **factory assembly line** that actually does the work. Each worker process runs an infinite loop that polls Redis for available jobs across multiple queues, respecting priority weights. When it obtains a job, it deserializes it, looks up the corresponding job handler (registered during worker startup), and executes it within a configurable timeout. The worker manages the job's lifecycle state transitions (PENDING → ACTIVE → COMPLETED/FAILED) and reports heartbeats to Redis so the monitoring system knows it's alive.\n\n**Scheduler** functions like a **calendar app with recurring events**. It runs as a separate background process that continuously polls Redis sorted sets for jobs scheduled for future execution. For delayed jobs (run once at a specific time), it simply moves them to the appropriate work queue when their timestamp arrives. For recurring jobs defined by cron expressions, it calculates the next execution time after each run and re-enqueues the job. The scheduler also handles timezone conversions and ensures unique jobs aren't duplicated.\n\n**Retry Manager** implements a **customer service escalation process**. When a job fails, the worker consults the Retry Manager to determine what should happen next. The Retry Manager examines the job's attempt count, applies an exponential backoff algorithm (with optional jitter) to calculate the next retry delay, and stores the job in a Redis sorted set keyed by its retry timestamp. A separate polling process (or the main scheduler) later moves retry jobs back to work queues. Jobs that exceed their maximum retry count are moved to the dead letter queue for manual inspection.\n\n**Monitor/Dashboard** serves as the **air traffic control tower** for the entire system. It collects metrics from Redis (queue lengths, worker heartbeats) and from job completion events, aggregates them, and presents them in a real-time web dashboard. It can trigger alerts when queues exceed thresholds or error rates spike. The dashboard also provides administrative controls to manually retry failed jobs or inspect job details.\n\n**Redis** is the **central nervous system** connecting all components. We use different Redis data structures for different purposes:\n- **Lists** for FIFO job queues (LPUSH/RPOP)\n- **Sorted Sets** for scheduled and retry jobs (ZADD/ZRANGEBYSCORE)\n- **Hashes** for job details and worker heartbeats (HSET/HGETALL)\n- **Streams** for job completion events and metrics (XADD/XREAD)\n- **Sets** for tracking active workers and queues (SADD/SMEMBERS)\n\n> **Design Insight:** Redis serves as both the queue broker AND the system of record for job state. This simplifies the architecture (no separate database needed) but requires careful Redis data design to avoid bottlenecks. All components are stateless except for Redis—this enables horizontal scaling of workers and producers.\n\nNow let's examine the key interfaces between these components:\n\n**Queue Manager Interface Table:**\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `enqueue` | `job_type: str`, `args: List`, `kwargs: Dict`, `queue: str`, `priority: int`, `max_retries: int`, `timeout_seconds: int`, `metadata: Dict` | `job_id: str` | Validates job payload, generates job ID, serializes job, and pushes to appropriate Redis queue |\n| `bulk_enqueue` | `jobs: List[Dict]` (each dict contains same fields as `enqueue` parameters) | `List[str]` (job IDs) | Atomically enqueues multiple jobs in a single Redis transaction |\n| `queue_stats` | `queue_names: Optional[List[str]]` | `Dict[str, Dict]` with keys: `pending_count`, `active_count`, `scheduled_count`, `retry_count` | Returns current metrics for specified queues (or all queues) |\n| `peek` | `queue: str`, `count: int` | `List[Job]` | Returns next `count` jobs from queue without removing them (for inspection) |\n\n**Worker Interface Table:**\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `register_handler` | `job_type: str`, `handler: Callable` | `None` | Maps a job type string to a Python function that will execute jobs of that type |\n| `start` | `config: WorkerConfig` | `None` (blocks until shutdown) | Starts the worker main loop, polling queues and executing jobs |\n| `stop` | `graceful: bool = True` | `None` | Signals the worker to stop (immediately if `graceful=False`, else after current job completes) |\n| `pause` | `queue: Optional[str]` | `None` | Temporarily stops processing jobs from specified queue (or all queues) |\n| `resume` | `queue: Optional[str]` | `None` | Resumes processing from a paused queue |\n\n**Scheduler Interface Table:**\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `schedule` | `job: Job`, `run_at: datetime` | `schedule_id: str` | Schedules a job for one-time execution at `run_at` |\n| `schedule_cron` | `job: Job`, `cron_expression: str`, `timezone: Optional[str]` | `schedule_id: str` | Schedules a recurring job using cron expression |\n| `unschedule` | `schedule_id: str` | `bool` (success) | Removes a scheduled job (both one-time and recurring) |\n| `list_schedules` | `job_type: Optional[str]` | `List[Dict]` | Returns all scheduled jobs, optionally filtered by job type |\n\n**Retry Manager Interface Table:**\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `schedule_retry` | `job: Job`, `error: Exception` | `retry_at: datetime` | Calculates next retry time using exponential backoff, stores job in retry set, records error |\n| `get_dead_letter_jobs` | `queue: Optional[str]`, `limit: int` | `List[Job]` | Retrieves jobs from dead letter queue for manual inspection/retry |\n| `retry_dead_letter_job` | `job_id: str` | `bool` (success) | Moves a job from dead letter queue back to work queue for immediate retry |\n| `purge_dead_letter_jobs` | `older_than: Optional[datetime]` | `int` (count purged) | Permanently removes jobs from dead letter queue, optionally filtered by age |\n\n**Monitor Interface Table:**\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `get_queue_metrics` | `time_range: str = \"1h\"` | `Dict[str, Any]` | Returns queue depths, processing rates, error rates for all queues over time range |\n| `get_worker_status` | `worker_id: Optional[str]` | `List[Dict]` or `Dict` | Returns status of all workers or specific worker (ID, current job, last heartbeat) |\n| `search_jobs` | `filters: Dict` (job_id, status, queue, time_range) | `List[Job]` | Searches job history with pagination and filtering |\n| `trigger_alert` | `alert_type: str`, `details: Dict` | `None` | Triggers an alert (logs, webhook, etc.) when system conditions warrant |\n\n### Communication Patterns and Data Flow\n\nThe components communicate through **two primary patterns**: **broker-mediated messaging** (via Redis) for job data flow, and **direct API calls** for management operations. This hybrid approach balances decoupling with efficiency.\n\n**Primary Communication Channels:**\n\n1. **Redis as Message Broker**: All job-related data flows through Redis using atomic operations:\n   - **Producer → Queue Manager → Redis**: Jobs are LPUSHed to list keys like `queue:{queue_name}`\n   - **Redis → Worker**: Workers BRPOP jobs from multiple queues\n   - **Worker → Retry Manager → Redis**: Failed jobs are ZADDed to sorted set `retry:{queue_name}` with score = retry timestamp\n   - **Scheduler → Redis**: Due jobs are moved from `scheduled` sorted sets to work queues\n   - **All → Monitor**: Components write metrics/events to Redis streams/hashes for monitoring\n\n2. **Direct HTTP/RPC for Management**: The dashboard and administrative tools communicate directly with components via REST APIs or RPC:\n   - **Dashboard → Monitor API**: Real-time metrics polling\n   - **Dashboard → Queue Manager**: Manual job enqueueing/ inspection\n   - **Dashboard → Retry Manager**: Dead letter queue operations\n\n**Data Flow Through the System:**\n\nLet's trace a job through the complete system with three scenarios: immediate execution, scheduled execution, and failed execution with retries.\n\n**Scenario 1: Immediate Job Execution (Happy Path)**\n\n1. **Producer** creates a `Job` object with `job_type=\"process_image\"`, `args=[\"image.jpg\"]`, `queue=\"high_priority\"`.\n2. **Producer** calls `QueueManager.enqueue(job)`.\n3. **Queue Manager** validates the job (size < 1MB), generates a ULID job ID, serializes the job to JSON, and executes `LPUSH queue:high_priority <serialized_job>`.\n4. **Worker** (continuously polling with `BRPOP queue:high_priority queue:low_priority ...`) receives the serialized job from Redis.\n5. **Worker** deserializes the job, looks up the registered handler for `process_image`, and executes it within a timeout.\n6. **Worker** on success: Updates job status to `COMPLETED`, stores result in Redis hash `job:{job_id}`, emits completion event to Redis stream `events:completed`.\n7. **Monitor** reads from `events:completed` stream, updates real-time metrics, and makes job result available via dashboard.\n\n**Scenario 2: Scheduled Recurring Job**\n\n1. **Producer** calls `Scheduler.schedule_cron(job, \"0 * * * *\")` (hourly job).\n2. **Scheduler** calculates next run time (e.g., 14:00 today), stores job in Redis sorted set `scheduled:cron` with score = 14:00 timestamp.\n3. **Scheduler Process** (separate daemon) polls `scheduled:cron` every 30 seconds for jobs with score <= current time.\n4. At 14:00, scheduler finds the job, moves it to work queue via `QueueManager.enqueue(job)`, calculates next run time (15:00), updates the score in `scheduled:cron`.\n5. Job flows through normal execution path (Scenario 1).\n\n**Scenario 3: Failed Job with Retries**\n\n1. **Worker** executes a job, but it raises `ConnectionError`.\n2. **Worker** catches the exception, calls `RetryManager.schedule_retry(job, error)`.\n3. **Retry Manager** increments job's `attempts` count, calculates retry delay using exponential backoff (e.g., 2^attempt seconds), stores job in Redis sorted set `retry:{queue_name}` with score = current time + delay.\n4. **Retry Poller** (could be same as scheduler) periodically checks retry sets for due jobs (score <= current time), moves them back to work queues.\n5. If job fails again, process repeats with longer delay (4s, 8s, 16s...).\n6. After exceeding `max_retries` (default 5), job is moved to dead letter queue (Redis list `dead:{queue_name}`) with full error history.\n\n**Component Interaction Patterns Table:**\n| Interaction | Pattern | Technology | Data Format | Reliability Guarantee |\n|-------------|---------|------------|-------------|------------------------|\n| Job Enqueue | Fire-and-forget | Redis LPUSH | JSON | At-most-once (may be lost if Redis crashes before persistence) |\n| Job Dequeue | Blocking poll | Redis BRPOP | JSON | At-least-once (job may be processed multiple times if worker crashes after dequeue but before processing) |\n| Scheduled Jobs | Periodic poll | Redis ZRANGEBYSCORE | JSON | At-least-once (may execute multiple times if scheduler crashes after enqueue but before updating score) |\n| Worker Heartbeats | Periodic write | Redis HSET | JSON | Best-effort (stale workers detected via timeout) |\n| Metrics Collection | Write to stream | Redis XADD | JSON | Best-effort (occasional loss acceptable) |\n| Dashboard Updates | Polling + WebSocket | HTTP + Redis pub/sub | JSON | Best-effort |\n\n> **Design Insight:** The system guarantees **at-least-once delivery** for jobs, which is appropriate for most background tasks (idempotent by design). For truly once-only processing, jobs must include their own idempotency logic using unique keys in the application domain.\n\n**Data Flow State Transitions:**\n\nA job moves through various Redis data structures during its lifecycle:\n\n```\nJob Created → queue:{name} LIST (PENDING)\n            → (Worker fetches) → job:{id} HASH (ACTIVE)\n            → (Success) → events:completed STREAM (COMPLETED)\n            → (Failure) → retry:{queue} ZSET (RETRY_SCHEDULED)\n            → (After backoff) → queue:{name} LIST again\n            → (Max retries) → dead:{queue} LIST (DEAD_LETTER)\n```\n\n**Concurrency and Scaling Considerations:**\n\n- **Multiple Workers** can poll the same queue simultaneously—Redis ensures each job goes to only one worker via atomic BRPOP.\n- **Queue Priority** is implemented by having workers poll multiple queues with weighted round-robin (more BRPOP calls on high-priority queues).\n- **Horizontal Scaling** is achieved by adding more worker processes or even worker hosts—all connect to the same Redis instance.\n- **Redis Bottlenecks** may occur at very high throughput (>10K jobs/sec); solutions include Redis clustering or sharding queues across instances.\n\n**Failure Handling in Communication:**\n\n- **Redis Connection Loss**: All components implement exponential backoff reconnection logic with circuit breaker pattern.\n- **Worker Crash During Job**: Jobs remain in ACTIVE state; a cleanup process periodically moves stale active jobs back to PENDING queue.\n- **Network Partition**: Split-brain scenarios are mitigated by favoring availability—jobs continue processing on both sides of partition, requiring application-level idempotency.\n\n### Recommended File and Module Structure\n\nOrganizing code clearly from the start prevents \"big ball of mud\" architecture. The recommended structure follows **clean architecture** principles: core domain logic independent of frameworks, with clear separation between components.\n\n**Project Root Structure:**\n```\nbackground_job_processor/\n├── pyproject.toml                    # Python project config and dependencies\n├── README.md\n├── .env.example                      # Example environment variables\n├── docker-compose.yml                # For local Redis and dashboard\n├── scripts/                          # Utility scripts\n│   ├── start_worker.py\n│   ├── start_scheduler.py\n│   └── seed_test_jobs.py\n├── src/                              # Main source code (Python package)\n│   └── job_processor/\n│       ├── __init__.py\n│       ├── core/                     # Domain models and interfaces\n│       │   ├── __init__.py\n│       │   ├── job.py               # Job class and serialization\n│       │   ├── exceptions.py        # Custom exceptions\n│       │   └── types.py             # Type definitions and enums\n│       ├── redis_client/             # Redis abstraction layer\n│       │   ├── __init__.py\n│       │   ├── client.py            # RedisClient wrapper\n│       │   ├── connection_pool.py\n│       │   └── errors.py\n│       ├── queue/                    # Milestone 1: Queue Manager\n│       │   ├── __init__.py\n│       │   ├── manager.py           # QueueManager class\n│       │   ├── priority.py          # Priority calculation logic\n│       │   ├── serialization.py     # Job serialization/validation\n│       │   └── tests/\n│       ├── worker/                   # Milestone 2: Worker Process\n│       │   ├── __init__.py\n│       │   ├── worker.py            # Worker main class\n│       │   ├── handler_registry.py  # Job type to handler mapping\n│       │   ├── heartbeat.py         # Worker health reporting\n│       │   ├── pool.py              # Thread/process pool for concurrency\n│       │   └── tests/\n│       ├── retry/                    # Milestone 3: Retry System\n│       │   ├── __init__.py\n│       │   ├── manager.py           # RetryManager class\n│       │   ├── backoff.py           # Exponential backoff calculators\n│       │   ├── dead_letter.py       # Dead letter queue operations\n│       │   └── tests/\n│       ├── scheduler/                # Milestone 4: Scheduling\n│       │   ├── __init__.py\n│       │   ├── scheduler.py         # Scheduler main class\n│       │   ├── cron.py              # Cron expression parser\n│       │   ├── timezone.py          # Timezone handling utilities\n│       │   └── tests/\n│       ├── monitoring/               # Milestone 5: Monitoring\n│       │   ├── __init__.py\n│       │   ├── metrics.py           # Metrics collection and aggregation\n│       │   ├── dashboard/           # Web dashboard (FastAPI/Flask)\n│       │   │   ├── app.py\n│       │   │   ├── routes.py\n│       │   │   ├── static/\n│       │   │   └── templates/\n│       │   ├── alerts.py            # Alerting logic\n│       │   └── tests/\n│       ├── config/                   # Configuration management\n│       │   ├── __init__.py\n│       │   ├── settings.py          # SystemConfig and friends\n│       │   └── validation.py\n│       └── utils/                    # Shared utilities\n│           ├── __init__.py\n│           ├── ulid_generator.py\n│           ├── signal_handlers.py\n│           └── validation.py\n└── tests/                            # Integration and end-to-end tests\n    ├── conftest.py\n    ├── test_integration.py\n    └── fixtures/\n```\n\n**Module Dependencies Flow:** \n```\ncore/ (no external dependencies)\n  ↓\nredis_client/ (depends on core, redis-py)\n  ↓\nqueue/ (depends on core, redis_client)\n  ↓\nworker/ (depends on core, redis_client, queue)\n  ↓\nretry/ (depends on core, redis_client, queue)\n  ↓\nscheduler/ (depends on core, redis_client, queue)\n  ↓\nmonitoring/ (depends on core, redis_client, all other modules)\n```\n\n**Key Design Decisions in Module Structure:**\n\n> **Decision: Layered Architecture vs Vertical Slicing**\n> - **Context**: We need to organize code for a complex system with multiple interrelated components. Two common approaches are layered architecture (separating concerns like data access, business logic, presentation) and vertical slicing (organizing by feature/bounded context).\n> - **Options Considered**:\n>   1. **Pure Layered Architecture**: `data_access/`, `business_logic/`, `api/` layers\n>   2. **Vertical Slices by Component**: `queue/`, `worker/`, `scheduler/` as self-contained modules\n>   3. **Hybrid Approach**: Component-based with shared core layer\n> - **Decision**: Hybrid approach (option 3) as shown in the structure above\n> - **Rationale**: Each component (queue, worker, scheduler) has clear boundaries and can be developed/tested independently (vertical slicing), while shared concerns like Redis client, configuration, and core domain models are extracted to common layers to avoid duplication. This balances modularity with DRY principles.\n> - **Consequences**: Components are loosely coupled and can be optional (e.g., you could use just the queue and worker without scheduler). However, there's some complexity in managing cross-component dependencies.\n\n**Component Interface Contracts:**\n\nEach component module exposes a clean public API while hiding implementation details:\n\n- **`queue` module**: Exports `QueueManager` class, `QueueConfig`, `enqueue_job` helper function\n- **`worker` module**: Exports `Worker`, `WorkerConfig`, `register_job_handler` decorator\n- **`scheduler` module**: Exports `Scheduler`, `CronSchedule`, `schedule_job` helper\n- **`retry` module**: Exports `RetryManager`, `ExponentialBackoff`, `DeadLetterQueue`\n- **`monitoring` module**: Exports `MetricsCollector`, `DashboardApp`, `setup_monitoring`\n\n**Configuration Management Strategy:**\n\nAll components share a central `SystemConfig` object loaded from environment variables with sensible defaults. Each component section (queue, worker) has its own config class that can be overridden individually.\n\n```python\n# Example configuration loading\nconfig = SystemConfig.from_env()\nqueue_manager = QueueManager(config.redis, config.queues)\nworker = Worker(config.workers[0], config.redis)\n```\n\n**Deployment View:**\n\n- **Development**: Single Redis instance, workers and scheduler run as separate processes on same machine\n- **Production**: Redis cluster or managed Redis service, workers distributed across multiple machines/containers, scheduler as redundant active-passive pair\n- **Container Orchestration**: Each component (worker, scheduler, dashboard) in its own Docker container, scaled independently\n\n**Module Responsibility Table:**\n| Module | Primary Classes | Key Responsibilities | Depends On |\n|--------|----------------|---------------------|------------|\n| `core` | `Job`, `JobStatus` | Define core domain models and interfaces | None (pure Python) |\n| `redis_client` | `RedisClient` | Abstract Redis operations, connection pooling, error handling | `core`, `redis-py` |\n| `queue` | `QueueManager` | Job validation, serialization, enqueueing with priority | `core`, `redis_client` |\n| `worker` | `Worker`, `HandlerRegistry` | Polling queues, executing jobs, concurrency management | `core`, `redis_client`, `queue` |\n| `retry` | `RetryManager`, `ExponentialBackoff` | Managing retry logic, backoff calculations, dead letter queue | `core`, `redis_client`, `queue` |\n| `scheduler` | `Scheduler`, `CronParser` | Scheduling delayed/recurring jobs, timezone handling | `core`, `redis_client`, `queue` |\n| `monitoring` | `MetricsCollector`, `DashboardApp` | Collecting metrics, providing web UI, alerting | All other modules |\n\n> **Design Insight:** This modular structure enables incremental implementation aligned with the project milestones. You can build and test the `queue` module (Milestone 1) independently before implementing `worker` (Milestone 2), with each milestone adding a new module to the system.\n\n### Implementation Guidance\n\n**Technology Recommendations Table:**\n| Component | Simple Option | Advanced Option | Rationale for Choice |\n|-----------|---------------|-----------------|----------------------|\n| **Redis Client** | `redis-py` with connection pooling | `hiredis` parser for performance + `redis-py` | `redis-py` is standard, well-maintained, and sufficient for most workloads |\n| **Web Dashboard** | FastAPI + Jinja2 templates + HTMX for dynamic updates | Separate React frontend with FastAPI backend | FastAPI provides automatic OpenAPI docs, async support; HTMX keeps complexity low |\n| **Job Serialization** | JSON with `orjson` for speed | MessagePack for smaller payloads | JSON is universal, debuggable; `orjson` is 2-3x faster than stdlib `json` |\n| **Concurrency Model** | `concurrent.futures.ThreadPoolExecutor` | `multiprocessing.Pool` for CPU-bound tasks | Threads are fine for I/O-bound jobs; processes for CPU-bound but with serialization overhead |\n| **Configuration** | Python-dotenv + Pydantic models | ConfigMap + environment variables (K8s) | Pydantic provides validation with clear error messages |\n| **Testing** | pytest with `fakeredis` for unit tests | Docker-based integration tests with real Redis | `fakeredis` is fast for unit tests; real Redis for integration |\n\n**Recommended File Structure Implementation:**\n\nHere's the minimal structure to begin with (expanding as milestones progress):\n\n```bash\nmkdir -p background_job_processor/src/job_processor/{core,redis_client,queue,worker,retry,scheduler,monitoring,config,utils}\ntouch background_job_processor/src/job_processor/__init__.py\n```\n\n**Infrastructure Starter Code:**\n\nCreate these foundational files first—they're prerequisites for all components:\n\n**1. Core Domain Models (`src/job_processor/core/job.py`):**\n```python\n\"\"\"Core Job model and serialization.\"\"\"\nfrom dataclasses import dataclass, field, asdict\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\nimport json\n\n\nclass JobStatus(Enum):\n    \"\"\"Enumeration of possible job states.\"\"\"\n    PENDING = \"pending\"\n    ACTIVE = \"active\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    RETRY_SCHEDULED = \"retry_scheduled\"\n    DEAD_LETTER = \"dead_letter\"\n\n\n@dataclass\nclass Job:\n    \"\"\"Represents a unit of work to be processed asynchronously.\"\"\"\n    job_id: str\n    job_type: str\n    args: List[Any] = field(default_factory=list)\n    kwargs: Dict[str, Any] = field(default_factory=dict)\n    queue: str = \"default\"\n    priority: int = 0\n    max_retries: int = 3\n    timeout_seconds: int = 300\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    status: JobStatus = JobStatus.PENDING\n    attempts: int = 0\n    errors: List[Dict[str, Any]] = field(default_factory=list)\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    result: Optional[Any] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert job to dictionary for serialization.\"\"\"\n        data = asdict(self)\n        # Convert datetime objects to ISO format strings\n        for date_field in ['created_at', 'started_at', 'completed_at']:\n            if data[date_field]:\n                data[date_field] = data[date_field].isoformat() if isinstance(data[date_field], datetime) else data[date_field]\n        # Convert status enum to string\n        data['status'] = self.status.value\n        return data\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Job':\n        \"\"\"Reconstruct job from dictionary.\"\"\"\n        # Convert string dates back to datetime objects\n        date_fields = ['created_at', 'started_at', 'completed_at']\n        for field_name in date_fields:\n            if field_name in data and data[field_name] and isinstance(data[field_name], str):\n                data[field_name] = datetime.fromisoformat(data[field_name])\n            elif field_name in data and not data[field_name]:\n                data[field_name] = None\n        \n        # Convert status string back to enum\n        if 'status' in data and isinstance(data['status'], str):\n            data['status'] = JobStatus(data['status'])\n        \n        return cls(**data)\n    \n    def serialize(self) -> str:\n        \"\"\"Serialize job to JSON string.\"\"\"\n        return json.dumps(self.to_dict(), default=str)\n    \n    @classmethod\n    def deserialize(cls, data: str) -> 'Job':\n        \"\"\"Deserialize job from JSON string.\"\"\"\n        return cls.from_dict(json.loads(data))\n    \n    def record_error(self, error: Exception) -> None:\n        \"\"\"Record an error that occurred during job execution.\"\"\"\n        import traceback\n        self.errors.append({\n            'type': error.__class__.__name__,\n            'message': str(error),\n            'traceback': traceback.format_exc(),\n            'occurred_at': datetime.utcnow().isoformat(),\n            'attempt': self.attempts\n        })\n    \n    def should_retry(self) -> bool:\n        \"\"\"Determine if job should be retried.\"\"\"\n        return self.attempts < self.max_retries and self.status != JobStatus.DEAD_LETTER\n```\n\n**2. Redis Client Abstraction (`src/job_processor/redis_client/client.py`):**\n```python\n\"\"\"Redis client wrapper with connection pooling and error handling.\"\"\"\nimport logging\nfrom typing import Any, Dict, Optional\nimport redis\nfrom redis.exceptions import RedisError\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass RedisClient:\n    \"\"\"Singleton Redis client wrapper with connection pooling.\"\"\"\n    \n    _instance: Optional['RedisClient'] = None\n    _client: Optional[redis.Redis] = None\n    \n    def __init__(self, url: str, **connection_kwargs):\n        \"\"\"Initialize Redis client (private, use get_instance).\"\"\"\n        self.url = url\n        self.connection_kwargs = connection_kwargs\n        self.connect()\n    \n    def connect(self) -> None:\n        \"\"\"Establish connection to Redis.\"\"\"\n        try:\n            self._client = redis.from_url(\n                self.url,\n                **self.connection_kwargs,\n                decode_responses=True  # Auto-decode bytes to str\n            )\n            # Test connection\n            self._client.ping()\n            logger.info(f\"Connected to Redis at {self.url}\")\n        except RedisError as e:\n            logger.error(f\"Failed to connect to Redis: {e}\")\n            raise\n    \n    @classmethod\n    def get_instance(cls, url: Optional[str] = None, **kwargs) -> 'RedisClient':\n        \"\"\"Get singleton RedisClient instance.\"\"\"\n        if cls._instance is None:\n            if url is None:\n                url = \"redis://localhost:6379/0\"\n            cls._instance = cls(url, **kwargs)\n        return cls._instance\n    \n    def get_client(self) -> redis.Redis:\n        \"\"\"Get the underlying Redis client instance.\"\"\"\n        if self._client is None:\n            self.connect()\n        return self._client\n    \n    def execute(self, command: str, *args, **kwargs) -> Any:\n        \"\"\"Execute a Redis command with error handling.\"\"\"\n        try:\n            client = self.get_client()\n            method = getattr(client, command)\n            return method(*args, **kwargs)\n        except RedisError as e:\n            logger.error(f\"Redis command failed: {command} {args}: {e}\")\n            # Re-raise for caller to handle\n            raise\n    \n    def pipeline(self) -> redis.Pipeline:\n        \"\"\"Return a Redis pipeline for atomic operations.\"\"\"\n        return self.get_client().pipeline()\n    \n    def health_check(self) -> bool:\n        \"\"\"Check if Redis is responsive.\"\"\"\n        try:\n            return self.get_client().ping()\n        except RedisError:\n            return False\n```\n\n**3. Configuration Management (`src/job_processor/config/settings.py`):**\n```python\n\"\"\"Configuration management with Pydantic models.\"\"\"\nimport os\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field, validator\nfrom dotenv import load_dotenv\n\nload_dotenv()  # Load environment variables from .env file\n\n\nclass RedisConfig(BaseModel):\n    \"\"\"Redis connection configuration.\"\"\"\n    url: str = Field(default=\"redis://localhost:6379/0\")\n    max_connections: int = Field(default=10, ge=1)\n    socket_timeout: int = Field(default=5, ge=1)\n    retry_on_timeout: bool = Field(default=True)\n\n\nclass QueueConfig(BaseModel):\n    \"\"\"Queue configuration.\"\"\"\n    name: str\n    priority: int = Field(default=1, ge=0)\n    max_length: Optional[int] = Field(default=None, ge=1)\n    \n    @validator('name')\n    def name_must_be_valid(cls, v):\n        if not v or ':' in v:\n            raise ValueError('Queue name cannot be empty or contain colons')\n        return v\n\n\nclass WorkerConfig(BaseModel):\n    \"\"\"Worker process configuration.\"\"\"\n    queues: List[str] = Field(default_factory=lambda: [\"default\"])\n    concurrency: int = Field(default=4, ge=1)\n    heartbeat_interval: int = Field(default=30, ge=5)\n    job_timeout: int = Field(default=300, ge=1)\n\n\nclass SystemConfig(BaseModel):\n    \"\"\"Main system configuration.\"\"\"\n    redis: RedisConfig = Field(default_factory=RedisConfig)\n    queues: List[QueueConfig] = Field(default_factory=lambda: [QueueConfig(name=\"default\")])\n    workers: List[WorkerConfig] = Field(default_factory=lambda: [WorkerConfig()])\n    max_payload_size: int = Field(default=1048576, ge=1024)  # 1MB default\n    job_history_size: int = Field(default=1000, ge=0)  # How many completed jobs to keep\n    retry_base_delay: int = Field(default=1, ge=1)  # Base delay in seconds for exponential backoff\n    retry_max_attempts: int = Field(default=5, ge=0)\n    \n    @classmethod\n    def from_env(cls) -> 'SystemConfig':\n        \"\"\"Load configuration from environment variables with defaults.\"\"\"\n        return cls(\n            redis=RedisConfig(\n                url=os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\"),\n                max_connections=int(os.getenv(\"REDIS_MAX_CONNECTIONS\", \"10\")),\n                socket_timeout=int(os.getenv(\"REDIS_SOCKET_TIMEOUT\", \"5\")),\n                retry_on_timeout=os.getenv(\"REDIS_RETRY_ON_TIMEOUT\", \"true\").lower() == \"true\"\n            ),\n            max_payload_size=int(os.getenv(\"MAX_PAYLOAD_SIZE\", \"1048576\")),\n            job_history_size=int(os.getenv(\"JOB_HISTORY_SIZE\", \"1000\")),\n            retry_base_delay=int(os.getenv(\"RETRY_BASE_DELAY\", \"1\")),\n            retry_max_attempts=int(os.getenv(\"RETRY_MAX_ATTEMPTS\", \"5\"))\n        )\n```\n\n**Core Logic Skeleton for Queue Manager (`src/job_processor/queue/manager.py`):**\n```python\n\"\"\"Queue Manager - handles job enqueueing with validation and priority.\"\"\"\nimport logging\nfrom typing import Dict, List, Optional, Any\nfrom uuid import uuid4\n\nfrom ..core.job import Job, JobStatus\nfrom ..redis_client.client import RedisClient\nfrom ..config.settings import QueueConfig, SystemConfig\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass QueueManager:\n    \"\"\"Manages job queues with validation, serialization, and priority handling.\"\"\"\n    \n    def __init__(self, redis_client: RedisClient, queue_configs: List[QueueConfig]):\n        \"\"\"Initialize QueueManager with Redis client and queue configurations.\"\"\"\n        self.redis = redis_client\n        self.queues = {q.name: q for q in queue_configs}\n        # TODO: Initialize any required Redis data structures on startup\n        # TODO: Validate that queue names don't conflict with Redis key patterns\n    \n    def enqueue(\n        self,\n        job_type: str,\n        args: Optional[List[Any]] = None,\n        kwargs: Optional[Dict[str, Any]] = None,\n        queue: str = \"default\",\n        priority: int = 0,\n        max_retries: int = 3,\n        timeout_seconds: int = 300,\n        metadata: Optional[Dict[str, Any]] = None\n    ) -> str:\n        \"\"\"\n        Enqueue a job for asynchronous execution.\n        \n        Args:\n            job_type: String identifier for the job handler\n            args: Positional arguments for the job handler\n            kwargs: Keyword arguments for the job handler\n            queue: Name of the queue to enqueue to\n            priority: Priority level (higher = more important)\n            max_retries: Maximum number of retry attempts\n            timeout_seconds: Maximum execution time in seconds\n            metadata: Additional metadata for the job\n            \n        Returns:\n            job_id: Unique identifier for the enqueued job\n            \n        Raises:\n            ValueError: If queue doesn't exist or payload is too large\n            RedisError: If Redis operation fails\n        \"\"\"\n        # TODO 1: Validate that the queue exists in configuration\n        # TODO 2: Create Job object with generated ULID/UUID job_id\n        # TODO 3: Validate payload size (args + kwargs + metadata) doesn't exceed max_payload_size\n        # TODO 4: Serialize job to JSON string\n        # TODO 5: Use Redis pipeline for atomic operations:\n        #   - LPUSH to queue:queue_name with serialized job\n        #   - HSET to job:job_id with job metadata\n        #   - SADD to queues:active with queue name (for monitoring)\n        # TODO 6: Return job_id\n        pass\n    \n    def bulk_enqueue(self, jobs: List[Dict[str, Any]]) -> List[str]:\n        \"\"\"\n        Atomically enqueue multiple jobs.\n        \n        Args:\n            jobs: List of job dictionaries (same format as enqueue parameters)\n            \n        Returns:\n            List of job IDs in the same order as input jobs\n        \"\"\"\n        # TODO 1: Validate all jobs before any Redis operations\n        # TODO 2: Create pipeline for atomic bulk insert\n        # TODO 3: For each job: validate, create Job object, serialize, add to pipeline\n        # TODO 4: Execute pipeline\n        # TODO 5: Return list of job IDs\n        pass\n    \n    def queue_stats(self, queue_names: Optional[List[str]] = None) -> Dict[str, Dict]:\n        \"\"\"\n        Get statistics for specified queues (or all queues).\n        \n        Returns dictionary with queue names as keys and dicts containing:\n        - pending_count: Jobs waiting in queue\n        - active_count: Jobs currently being processed\n        - scheduled_count: Jobs scheduled for future execution\n        - retry_count: Jobs waiting for retry\n        - dead_letter_count: Jobs in dead letter queue\n        \"\"\"\n        # TODO 1: If queue_names is None, get all configured queue names\n        # TODO 2: For each queue, use Redis LLEN for pending count\n        # TODO 3: For each queue, use Redis SCARD for active jobs (from job:active:{queue})\n        # TODO 4: For each queue, use Redis ZCARD for scheduled and retry counts\n        # TODO 5: For each queue, use Redis LLEN for dead letter queue\n        # TODO 6: Return aggregated dictionary\n        pass\n    \n    def peek(self, queue: str, count: int = 10) -> List[Job]:\n        \"\"\"\n        Peek at next jobs in queue without removing them.\n        \n        Args:\n            queue: Queue name\n            count: Maximum number of jobs to return\n            \n        Returns:\n            List of Job objects (deserialized)\n        \"\"\"\n        # TODO 1: Validate queue exists\n        # TODO 2: Use Redis LRANGE to get first 'count' items from queue\n        # TODO 3: Deserialize each item to Job object\n        # TODO 4: Return list of Jobs\n        pass\n```\n\n**Language-Specific Hints (Python):**\n- Use `orjson` instead of standard `json` for 2-3x faster serialization (install via `pip install orjson`)\n- For ULID generation, use `ulid-py` package: `from ulid import ULID; job_id = str(ULID())`\n- Use `redis-py` version 4.0+ for proper async support and connection management\n- Implement connection pooling via `redis.ConnectionPool` to reuse connections across components\n- Use Python's `signal` module for graceful shutdown: `signal.signal(signal.SIGTERM, shutdown_handler)`\n- For cron parsing, consider `croniter` library instead of implementing your own parser\n\n**Milestone Checkpoint for Architecture Foundation:**\nAfter setting up the basic structure and core files, verify your foundation:\n1. **Run validation**: `python -c \"from src.job_processor.core.job import Job; from src.job_processor.config.settings import SystemConfig; print('Core imports successful')\"`\n2. **Test Redis connection**: Create a simple test script that instantiates `RedisClient` and calls `health_check()`\n3. **Verify serialization roundtrip**: Create a test that creates a `Job`, serializes it, deserializes it, and compares fields\n4. **Expected outcome**: All imports succeed, Redis connects (if running locally), and job serialization preserves all data types including datetimes.\n\n**Common Setup Issues:**\n- **Redis not running**: Install Redis via `brew install redis` (macOS) or `apt install redis-server` (Linux), then `redis-server`\n- **Python path issues**: Install package in development mode: `pip install -e .` from project root\n- **Missing dependencies**: Create `requirements.txt` with: `redis>=4.5, pydantic>=2.0, python-dotenv>=1.0, orjson>=3.9`\n\n\n## Data Model\n\n> **Milestone(s):** This section establishes the foundational data structures for all five milestones. The data model defines how jobs are represented, stored, and transmitted between components, forming the backbone of the entire background job processing system.\n\nThe data model is the DNA of our background job processor—it defines how information is structured, stored, and flows through the system. Think of it as the **central filing system** in a busy government office. Each job is like a case file that moves through different departments (queues, workers, retry systems). The filing system must be organized so any clerk (system component) can quickly find the current status of any case, understand its history, and know what to do next. A poorly designed filing system leads to lost cases, duplicated work, and confusion about responsibilities.\n\nThis section defines three layers of data organization: the **persistent storage** in Redis (how data is stored on disk/network), the **in-memory objects** (how data is structured while being processed), and the **serialization format** (how data moves between these two layers). Each layer serves distinct purposes with different constraints—Redis storage prioritizes atomic operations and durability, in-memory objects prioritize type safety and business logic, and serialization prioritizes cross-language compatibility and space efficiency.\n\n### Redis Data Structures and Keys\n\nRedis serves as our **central bulletin board and filing cabinet**—a shared space where all components post messages and look up information. Unlike a traditional relational database, Redis is a key-value store with specialized data structures optimized for different access patterns. Our design uses multiple Redis data structures, each chosen for specific operational characteristics that match how we need to access the data.\n\nThe key naming convention follows a clear pattern: `{system}:{component}:{identifier}`. This colon-separated hierarchy enables Redis key scanning operations, logical grouping, and automatic key expiration management. All keys use lowercase with underscores for readability.\n\n#### Primary Job Storage\n\nJobs in the active processing pipeline are stored in Redis lists, which provide O(1) push/pop operations and maintain FIFO ordering:\n\n| Key Pattern | Data Type | Description | Example |\n|-------------|-----------|-------------|---------|\n| `job_queue:{queue_name}` | List | Primary job queue storing serialized `Job` objects in FIFO order. Workers pop from the left, enqueuers push to the right. | `job_queue:default`, `job_queue:emails` |\n| `job_active` | Set | Set of job IDs currently being processed by workers. Used for crash recovery to detect orphaned jobs. | Contains: `\"job_01HXYZ...\", \"job_01HABC...\"` |\n| `job_retry_schedule` | Sorted Set | Delayed retry queue with execution timestamp as score. Jobs are moved back to main queue when their scheduled time arrives. | Score: 1678901234.56, Value: serialized job |\n\nEach queue list contains JSON-serialized `Job` objects. The list structure naturally supports FIFO semantics with `LPUSH` (add to end) and `BRPOP` (remove from beginning with blocking). The separate `job_active` set allows us to track which jobs are currently being processed without modifying the original queue entry, enabling crash recovery.\n\n#### Job Metadata and State Tracking\n\nBeyond the queue itself, we maintain comprehensive metadata about each job's lifecycle:\n\n| Key Pattern | Data Type | Description | Expiration |\n|-------------|-----------|-------------|------------|\n| `job:{job_id}` | Hash | Complete job state including arguments, status, timestamps, and error history. Serves as the system of record. | 7 days after completion |\n| `job_status:{job_id}` | String | Current job status as `JobStatus` enum value. Optimized for quick status checks without loading full job. | Same as parent job |\n| `job_result:{job_id}` | String | Serialized job result for completed jobs. Stored separately to avoid loading large results during status checks. | 7 days after completion |\n\nThe `job:{job_id}` hash contains all mutable job state fields. Storing this separately from the queue payload allows us to update job status and metadata without modifying the serialized job in the queue (which is immutable once enqueued). The hash structure provides O(1) access to individual fields, which is efficient for partial updates.\n\n#### System Management and Monitoring\n\nOperational data enables queue management, worker coordination, and monitoring:\n\n| Key Pattern | Data Type | Description | Purpose |\n|-------------|-----------|-------------|---------|\n| `worker:heartbeat:{worker_id}` | String | Timestamp of last worker heartbeat. Used to detect dead workers. | Worker liveness detection |\n| `worker:current_job:{worker_id}` | String | Job ID currently being processed by worker. Maps worker to active job. | Crash recovery correlation |\n| `queue:config:{queue_name}` | Hash | Queue configuration including priority and maximum length. | Runtime queue management |\n| `schedule:recurring` | Hash | Recurring job definitions with cron expressions and job templates. | Scheduler configuration |\n| `schedule:due` | Sorted Set | Scheduled jobs with execution timestamp as score. | Time-based scheduling |\n| `dead_letter:{queue_name}` | List | Jobs that exceeded maximum retry attempts. Requires manual intervention. | Error analysis and manual retry |\n| `metrics:queue:{queue_name}:depth` | String | Current queue depth (updated periodically). | Dashboard monitoring |\n| `metrics:job:{job_type}:processed` | Counter | Number of jobs processed (incremented atomically). | Performance analytics |\n\nThe heartbeat system uses simple string keys with timestamps. When a worker updates its heartbeat, it also sets an automatic expiration (TTL) slightly longer than the heartbeat interval. If the key expires, monitoring systems know the worker has died. This pattern avoids the need for a separate cleanup process.\n\n#### Indexes and Reverse Lookups\n\nTo support operational queries (\"find all jobs of type X\", \"find all failed jobs in queue Y\"), we maintain secondary indexes:\n\n| Key Pattern | Data Type | Description | Maintenance |\n|-------------|-----------|-------------|-------------|\n| `index:job_type:{job_type}:{status}` | Set | Set of job IDs for a given job type and status. Enables quick filtering. | Updated on status changes |\n| `index:queue:{queue_name}:{status}` | Set | Set of job IDs for a given queue and status. | Updated on status changes |\n| `index:time:enqueued:{date}` | Sorted Set | Job IDs indexed by enqueue timestamp. Enables time-range queries. | Score = timestamp |\n\nThese indexes are updated atomically with the main job status changes using Redis transactions (`MULTI/EXEC`). While Redis doesn't have automatic indexing like relational databases, this manual indexing pattern enables the query capabilities needed for the monitoring dashboard without expensive full scans.\n\n> **Design Insight:** We separate **job data** (in hashes) from **job placement** (in lists/sorted sets). This is analogous to separating a library's book catalog (metadata about books) from the shelves where books are physically placed (ordered collection). The catalog contains complete information about each book, while the shelf position determines when it will be accessed. This separation allows us to update a job's metadata (like recording an error) without moving it between queues.\n\n#### Redis Key Design ADR\n\n> **Decision: Hierarchical Key Structure with Colon Separators**\n> - **Context**: Redis has a flat key namespace, but we need to organize thousands of keys for different purposes (jobs, workers, queues, metrics). We need a structure that supports pattern matching for cleanup operations, logical grouping for debugging, and clear ownership.\n> - **Options Considered**:\n>   1. Flat names with prefixes: `job_12345`, `worker_abc`\n>   2. Hierarchical with colons: `job:12345`, `worker:abc:heartbeat`\n>   3. Hierarchical with dots: `job.12345`, `worker.abc.heartbeat`\n> - **Decision**: Use hierarchical colon-separated keys (`system:component:id:subcomponent`).\n> - **Rationale**: Colons are Redis' conventional hierarchy separator (used in Redis Cluster hash tags). The `KEYS` and `SCAN` commands work naturally with pattern matching (`job:*` finds all job keys). This structure also matches common Redis monitoring tools' expectations. Dots are less conventional in Redis and might conflict with some Redis modules' naming schemes.\n> - **Consequences**: Keys are slightly longer but self-documenting. We can delete all keys for a component with `DEL job:*` (though in production we'd use `SCAN` for large datasets). The hierarchy makes debugging easier when inspecting Redis with `redis-cli`.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Flat prefixes | Shorter keys, slightly less memory | No hierarchy, harder to pattern match, mixes different key types | No |\n| Colon hierarchy | Standard Redis convention, excellent pattern matching, self-documenting | Slightly longer keys | **Yes** |\n| Dot hierarchy | Familiar from domain names, hierarchical | Not standard in Redis, might conflict with modules | No |\n\n### In-Memory Object Types and Schemas\n\nWhile Redis stores serialized data, our application logic works with rich, typed objects in memory. These objects encapsulate business logic, validation rules, and behavioral methods. Think of them as the **active case files** on a clerk's desk—annotated, with sticky notes, calculations in the margins, and decision logic applied. The in-memory objects transform raw data into intelligent entities that know how to validate themselves, calculate retry delays, and transition between states.\n\n#### Core Job Object\n\nThe `Job` class is the central entity representing a unit of work. It follows a **builder pattern** where required fields are set at construction and optional metadata accumulates during execution:\n\n| Field | Type | Description | Immutable? |\n|-------|------|-------------|------------|\n| `job_id` | `str` | Unique identifier (ULID format). Provides natural time-based ordering. | Yes |\n| `job_type` | `str` | Job handler type (e.g., \"send_email\", \"process_image\"). Maps to handler class. | Yes |\n| `args` | `List[Any]` | Positional arguments for the job handler. | Yes |\n| `kwargs` | `Dict[str, Any]` | Keyword arguments for the job handler. | Yes |\n| `queue` | `str` | Target queue name (e.g., \"default\", \"high_priority\"). | Yes |\n| `priority` | `int` | Relative priority within queue (higher = more important). Default: 0. | Yes |\n| `max_retries` | `int` | Maximum retry attempts before moving to dead letter queue. Default: 3. | Yes |\n| `timeout_seconds` | `int` | Maximum execution time before job is forcibly terminated. Default: 1800. | Yes |\n| `created_at` | `datetime` | Job creation timestamp (UTC). | Yes |\n| `metadata` | `Dict[str, Any]` | Custom key-value pairs for tracing, feature flags, etc. | No |\n| `status` | `JobStatus` | Current lifecycle state (enum). | No |\n| `attempts` | `int` | Number of execution attempts so far (starts at 0). | No |\n| `errors` | `List[Dict]` | Error history with timestamps, exception details, and stack traces. | No |\n| `started_at` | `Optional[datetime]` | When current execution attempt began. `None` if not started. | No |\n| `completed_at` | `Optional[datetime]` | When job reached terminal state (completed/failed/dead). `None` if pending. | No |\n| `result` | `Optional[Any]` | Job execution result (serializable). Only set for `COMPLETED` jobs. | No |\n\nThe `job_id` uses ULID (Universally Unique Lexicographically Sortable Identifier) which provides both uniqueness and time-based ordering. Unlike UUIDv4, ULIDs are sortable by creation time, making time-range queries efficient. The `metadata` field is a flexible bag for cross-cutting concerns like tracing IDs, tenant identifiers, or feature flags—it's mutable because these can be added during processing.\n\n#### Job Status State Machine\n\nThe `JobStatus` enum defines the legal states a job can occupy, forming a strict state machine (illustrated in ![Job State Machine](./diagrams/job-state-machine.svg)):\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|-------|------------|---------------|\n| `PENDING` | Worker dequeues job | `ACTIVE` | Set `started_at` to current time, increment `attempts` |\n| `ACTIVE` | Job completes successfully | `COMPLETED` | Set `completed_at`, store result, clear `started_at` |\n| `ACTIVE` | Job fails with retries remaining | `RETRY_SCHEDULED` | Record error, calculate next retry time, schedule in sorted set |\n| `ACTIVE` | Job fails with no retries left | `DEAD_LETTER` | Record error, move to dead letter queue, set `completed_at` |\n| `RETRY_SCHEDULED` | Retry timer expires | `PENDING` | Move from retry sorted set to main queue |\n| `RETRY_SCHEDULED` | Manual retry requested | `PENDING` | Reset error count, move to main queue immediately |\n| `DEAD_LETTER` | Manual retry requested | `PENDING` | Reset error count and attempts, move to main queue |\n| Any state | Job cancelled | `FAILED` | Record cancellation error, set `completed_at` |\n\nThe state machine ensures jobs progress through well-defined paths. Terminal states (`COMPLETED`, `FAILED`, `DEAD_LETTER`) are final—jobs in these states are only kept for history and monitoring. The `FAILED` state is used for immediate failures (like validation errors) that shouldn't retry, while `DEAD_LETTER` is for exhausted retries.\n\n#### Configuration Objects\n\nSystem configuration is represented as typed objects rather than loose dictionaries, enabling validation and IDE support:\n\n**QueueConfig** defines the behavior of a named queue:\n| Field | Type | Description | Default |\n|-------|------|-------------|---------|\n| `name` | `str` | Queue identifier (must be alphanumeric with underscores). | Required |\n| `priority` | `int` | Weight for weighted round-robin polling (higher = more frequent). | 1 |\n| `max_length` | `Optional[int]` | Maximum jobs allowed in queue. `None` means unlimited. | `None` |\n\n**WorkerConfig** controls worker process behavior:\n| Field | Type | Description | Default |\n|-------|------|-------------|---------|\n| `queues` | `List[str]` | List of queue names to poll (in priority order). | `[\"default\"]` |\n| `concurrency` | `int` | Maximum parallel job executions (threads/processes). | 4 |\n| `heartbeat_interval` | `int` | Seconds between heartbeat updates to Redis. | 30 |\n| `job_timeout` | `int` | Default job timeout (seconds) if not specified per job. | 1800 |\n\n**RedisConfig** encapsulates Redis connection details:\n| Field | Type | Description | Default |\n|-------|------|-------------|---------|\n| `url` | `str` | Redis connection URL (e.g., \"redis://localhost:6379/0\"). | Required |\n| `max_connections` | `int` | Connection pool maximum size. | 20 |\n| `socket_timeout` | `int` | Socket timeout in seconds. | 5 |\n| `retry_on_timeout` | `bool` | Automatically retry on connection timeout. | `True` |\n\n**SystemConfig** is the root configuration container:\n| Field | Type | Description | Default |\n|-------|------|-------------|---------|\n| `redis` | `RedisConfig` | Redis connection configuration. | Required |\n| `queues` | `List[QueueConfig]` | All queue definitions in the system. | `[]` |\n| `workers` | `List[WorkerConfig]` | Worker process configurations. | `[]` |\n| `max_payload_size` | `int` | Maximum job payload size in bytes. | 1,048,576 |\n| `job_history_size` | `int` | Number of completed jobs to retain in history. | 10,000 |\n| `retry_base_delay` | `int` | Base delay in seconds for exponential backoff. | 1 |\n| `retry_max_attempts` | `int` | Global default maximum retry attempts. | 3 |\n\n#### Redis Client Wrapper\n\nThe `RedisClient` class provides a thread-safe, connection-pooled interface to Redis with automatic error handling and retry logic:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `url` | `str` | Redis connection URL (read-only). |\n| `connection_kwargs` | `Dict[str, Any]` | Additional connection parameters (read-only). |\n| `_client` | `Optional[redis.Redis]` | Private Redis client instance (lazy-initialized). |\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `get_instance` | `url: Optional[str] = None, **kwargs` | `RedisClient` | Singleton factory method ensuring single connection pool per configuration. |\n| `get_client` | None | `redis.Redis` | Returns underlying Redis client (initializes if needed). |\n| `execute` | `command: str, *args, **kwargs` | `Any` | Executes Redis command with automatic connection retry and error logging. |\n| `pipeline` | None | `redis.Pipeline` | Returns Redis pipeline for atomic multi-command execution. |\n\nThe singleton pattern ensures all parts of the application share the same connection pool, preventing connection exhaustion. The `execute` method wraps all Redis operations with consistent error handling—transient network errors trigger retries with exponential backoff, while logical errors (like wrong command syntax) raise immediate exceptions.\n\n#### Object Relationships\n\nThe data model relationships form a clear ownership hierarchy (as shown in ![Data Model Relationships](./diagrams/data-model-relationships.svg)):\n\n1. **SystemConfig owns RedisConfig, multiple QueueConfigs, and multiple WorkerConfigs**\n2. **Each Job references a QueueConfig by name**\n3. **Each WorkerConfig references multiple QueueConfigs by name**\n4. **RedisClient is a singleton service used by all components**\n5. **Job objects are created by producers, stored in Redis, loaded by workers, and updated throughout lifecycle**\n\nThis structure minimizes coupling—components only need references to what they directly use. For example, a worker only needs its `WorkerConfig` and a `RedisClient` instance; it doesn't need the full `SystemConfig`.\n\n### Job Serialization and Encoding\n\nSerialization is the **language translator** between our Python objects and Redis storage. It must preserve all data types, handle versioning, and be efficient for both small and large payloads. We face the classic tension between human readability (JSON) and efficiency (binary formats). Our design chooses JSON as the primary format with strict schema validation, augmented with custom encoders for special types like datetimes.\n\n#### Serialization Format ADR\n\n> **Decision: JSON with Custom Type Encoders**\n> - **Context**: Jobs contain diverse data types (strings, numbers, lists, dicts, datetimes, optional values). We need a serialization format that is human-readable for debugging, widely supported across languages (in case we add Go or Rust workers later), and efficient enough for typical job sizes (< 1MB).\n> - **Options Considered**:\n>   1. **JSON**: Human-readable, universal support, but limited type system (no datetime, no binary data)\n>   2. **MessagePack**: Binary, compact, preserves some type hints, but less human-readable\n>   3. **Protocol Buffers**: Strong typing, versioning, compact binary, but requires schema compilation\n> - **Decision**: Use JSON with custom encoders/decoders for extended types (datetime, bytes as base64).\n> - **Rationale**: Debuggability is critical during development—engineers need to inspect queues with `redis-cli` and understand the data. JSON's ubiquity means any language can eventually consume our queues. The 1MB payload limit means space efficiency is secondary. We can always add MessagePack as an optimization later while keeping JSON as a fallback.\n> - **Consequences**: Slightly larger payloads than binary formats. Datetimes are serialized to ISO 8601 strings. Binary data must be base64-encoded. We must implement robust custom encoders/decoders.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| JSON | Human-readable, universal, no schema needed, easy debugging | Larger size, limited types, slower parsing than binary | **Primary** |\n| MessagePack | Compact, preserves some types, fast parsing | Not human-readable, less universal support | Fallback option |\n| Protocol Buffers | Strong typing, versioning, compact | Schema management overhead, compilation step | No |\n\n#### Serialization Schema\n\nThe JSON schema for serialized jobs follows a strict structure with versioning for future evolution:\n\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"required\": [\"v\", \"job_id\", \"job_type\", \"args\", \"kwargs\", \"queue\"],\n  \"properties\": {\n    \"v\": {\"type\": \"integer\", \"const\": 1},\n    \"job_id\": {\"type\": \"string\", \"pattern\": \"^[0-9A-Z]{26}$\"},\n    \"job_type\": {\"type\": \"string\", \"maxLength\": 100},\n    \"args\": {\"type\": \"array\", \"maxItems\": 100},\n    \"kwargs\": {\"type\": \"object\", \"maxProperties\": 50},\n    \"queue\": {\"type\": \"string\", \"maxLength\": 50},\n    \"priority\": {\"type\": \"integer\", \"minimum\": -100, \"maximum\": 100},\n    \"max_retries\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100},\n    \"timeout_seconds\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 86400},\n    \"created_at\": {\"type\": \"string\", \"format\": \"date-time\"},\n    \"metadata\": {\"type\": \"object\", \"additionalProperties\": true},\n    \"status\": {\"enum\": [\"PENDING\", \"ACTIVE\", \"COMPLETED\", \"FAILED\", \"RETRY_SCHEDULED\", \"DEAD_LETTER\"]},\n    \"attempts\": {\"type\": \"integer\", \"minimum\": 0},\n    \"errors\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"required\": [\"timestamp\", \"exception\", \"message\"],\n        \"properties\": {\n          \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"},\n          \"exception\": {\"type\": \"string\"},\n          \"message\": {\"type\": \"string\"},\n          \"traceback\": {\"type\": \"string\"}\n        }\n      }\n    },\n    \"started_at\": {\"type\": [\"string\", \"null\"], \"format\": \"date-time\"},\n    \"completed_at\": {\"type\": [\"string\", \"null\"], \"format\": \"date-time\"},\n    \"result\": {}\n  },\n  \"additionalProperties\": false\n}\n```\n\nThe `v` field (version) allows future schema migrations—if we need to change the format, we can detect version 1 and apply migration logic. All fields have reasonable size limits to prevent abuse (e.g., `args` limited to 100 items, `kwargs` to 50 properties).\n\n#### Type Encoding Rules\n\nSpecial Python types require custom encoding rules to survive JSON serialization:\n\n| Python Type | JSON Representation | Notes |\n|-------------|---------------------|-------|\n| `datetime` | ISO 8601 string with timezone (e.g., \"2023-01-15T12:30:45.123456Z\") | Always UTC, microsecond precision |\n| `bytes` | Base64-encoded string | Used sparingly; large binaries should be stored elsewhere |\n| `Decimal` | String representation | Preserves exact decimal values without float rounding |\n| `set` | Array | Order is not preserved (sets are unordered) |\n| `frozenset` | Array | Same as set |\n| `tuple` | Array | Preserved as list on deserialization (JSON has no tuple type) |\n| `Enum` | String value (`.value` property) | Requires Enum to have string values |\n| `UUID` | Canonical string representation (8-4-4-4-12) | Lowercase, no braces |\n| `None` | `null` | Standard JSON null |\n\nThe serialization system uses a custom JSON encoder class that extends `json.JSONEncoder`, overriding the `default` method to handle these special types. During deserialization, we use a custom object hook that recognizes encoded types by structure (e.g., a dict with `{\"__type__\": \"datetime\", \"value\": \"...\"}`) or by convention (ISO datetime strings can be auto-detected by regex).\n\n#### Serialization Methods\n\nThe `Job` class provides symmetric serialization methods that handle the complete round-trip:\n\n| Method | Parameters | Returns | Algorithm |\n|--------|------------|---------|-----------|\n| `to_dict` | None | `Dict[str, Any]` | 1. Create dict with all job fields 2. Convert special types using encoding rules 3. Add version field `\"v\": 1` |\n| `from_dict` | `data: Dict[str, Any]` | `Job` | 1. Validate required fields exist 2. Check version compatibility 3. Decode special types using decoding rules 4. Return new Job instance |\n| `serialize` | None | `str` | 1. Call `to_dict()` 2. Convert dict to JSON string with compact separation 3. Validate size against `max_payload_size` |\n| `deserialize` | `data: str` | `Job` | 1. Parse JSON string to dict 2. Call `from_dict()` with parsed data |\n| `record_error` | `error: Exception` | `None` | 1. Create error dict with timestamp, exception class name, message, and traceback 2. Append to `errors` list 3. Update status if needed |\n| `should_retry` | None | `bool` | 1. Return `True` if status is `FAILED` and `attempts < max_retries` 2. Return `False` otherwise |\n\nThe `serialize` method includes size validation—if the serialized JSON exceeds `max_payload_size` (default 1MB), it raises a `JobTooLargeError` before attempting to store in Redis. This prevents queue clogging with oversized jobs.\n\n#### Common Pitfalls in Data Modeling\n\n⚠️ **Pitfall: Storing unserializable objects in job arguments**\n- **Description**: Passing database connections, file handles, or complex ORM objects in `args` or `kwargs`.\n- **Why it's wrong**: These objects can't be serialized to JSON. The job fails during enqueue with cryptic serialization errors, or worse, serializes to something meaningless that causes runtime failures.\n- **Fix**: Only pass primitive types, dicts, lists, or simple data objects. If you need database access, pass record IDs and fetch fresh connections in the job handler.\n\n⚠️ **Pitfall: Modifying serialized job in queue**\n- **Description**: Trying to update a job's properties after it's been enqueued by modifying the JSON in the Redis list.\n- **Why it's wrong**: The job in the queue is immutable by design. Multiple workers might have already read the serialized data, and updates won't be reflected. The correct place for mutable state is the `job:{id}` hash.\n- **Fix**: Store mutable state in the job hash, not in the queue payload. Use atomic updates to the hash when job state changes.\n\n⚠️ **Pitfall: Forgetting timezone awareness**\n- **Description**: Storing datetimes as naive (timezone-unaware) objects or using local system time.\n- **Why it's wrong**: Workers running in different timezones will interpret scheduled times differently. Daylight saving transitions cause duplicates or gaps.\n- **Fix**: Always use UTC for storage. Convert to local time only for display. Use `datetime.utcnow()` not `datetime.now()`.\n\n⚠️ **Pitfall: Inconsistent ULID generation**\n- **Description**: Generating ULIDs on different machines without synchronized clocks or using random components incorrectly.\n- **Why it's wrong**: ULIDs lose their time-sortable property if clocks are skewed. Jobs might appear out of order.\n- **Fix**: Use a ULID library that handles clock synchronization (monotonic counter for same-millisecond collisions). Ensure system clocks are synchronized with NTP.\n\n⚠️ **Pitfall: Not planning for schema evolution**\n- **Description**: Serializing jobs without version field, assuming the schema will never change.\n- **Why it's wrong**: When you need to add a new field or change encoding, existing jobs in queues become unreadable.\n- **Fix**: Include version field (`\"v\": 1`) in all serialized jobs. Write migration code that can upgrade old versions on deserialization.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| JSON Serialization | Python's built-in `json` module with custom encoder | `orjson` for 10x faster serialization (C-optimized) |\n| ULID Generation | `python-ulid` library (pure Python) | `ulid-py` with C extensions for performance |\n| Redis Client | `redis-py` with connection pooling | `redis-py` with client-side caching enabled |\n| Configuration | `pydantic` for validation and type checking | Custom YAML/TOML loader with environment variable override |\n| Date/Time | `pendulum` for timezone-aware operations | Python 3.9+ `zoneinfo` with `pytz` compatibility layer |\n\n#### Recommended File/Module Structure\n\n```\nbackground_jobs/\n├── __init__.py\n├── config/\n│   ├── __init__.py\n│   ├── models.py           # QueueConfig, WorkerConfig, RedisConfig, SystemConfig\n│   └── loader.py           # SystemConfig.from_env(), from_yaml()\n├── models/\n│   ├── __init__.py\n│   ├── job.py              # Job class with serialization methods\n│   ├── job_status.py       # JobStatus enum\n│   └── errors.py           # Custom exceptions (JobTooLargeError, etc.)\n├── storage/\n│   ├── __init__.py\n│   ├── redis_client.py     # RedisClient singleton wrapper\n│   ├── serialization.py    # Custom JSON encoder/decoder for special types\n│   └── keys.py             # Key generation utilities (e.g., job_key(), queue_key())\n└── utils/\n    ├── __init__.py\n    ├── ulid_generator.py   # ULID generation with monotonic counter\n    └── validation.py       # Payload size validation, queue name validation\n```\n\n#### Infrastructure Starter Code\n\n**Complete Redis Client Wrapper** (`storage/redis_client.py`):\n```python\nimport redis\nfrom typing import Optional, Any, Dict\nimport logging\nfrom threading import Lock\n\nlogger = logging.getLogger(__name__)\n\nclass RedisClient:\n    \"\"\"Thread-safe Redis client singleton with connection pooling.\"\"\"\n    \n    _instances: Dict[str, 'RedisClient'] = {}\n    _lock: Lock = Lock()\n    \n    def __init__(self, url: str, **connection_kwargs):\n        self.url = url\n        self.connection_kwargs = connection_kwargs\n        self._client: Optional[redis.Redis] = None\n        \n    @classmethod\n    def get_instance(cls, url: Optional[str] = None, **kwargs) -> 'RedisClient':\n        \"\"\"Get singleton instance for given URL and connection parameters.\"\"\"\n        if url is None:\n            url = \"redis://localhost:6379/0\"\n            \n        instance_key = f\"{url}:{str(kwargs)}\"\n        \n        with cls._lock:\n            if instance_key not in cls._instances:\n                cls._instances[instance_key] = cls(url, **kwargs)\n            return cls._instances[instance_key]\n    \n    def get_client(self) -> redis.Redis:\n        \"\"\"Lazy initialization of Redis client with connection pool.\"\"\"\n        if self._client is None:\n            self._client = redis.from_url(\n                self.url,\n                **self.connection_kwargs,\n                decode_responses=True,  # Auto-decode bytes to str\n                health_check_interval=30,\n            )\n            # Test connection\n            self._client.ping()\n            logger.info(f\"Connected to Redis at {self.url}\")\n        return self._client\n    \n    def execute(self, command: str, *args, **kwargs) -> Any:\n        \"\"\"Execute Redis command with automatic retry on connection errors.\"\"\"\n        client = self.get_client()\n        try:\n            # Get the method from client (e.g., client.get, client.set)\n            method = getattr(client, command)\n            return method(*args, **kwargs)\n        except (redis.ConnectionError, redis.TimeoutError) as e:\n            logger.warning(f\"Redis connection error: {e}, retrying...\")\n            # Reset client to force reconnection\n            self._client = None\n            client = self.get_client()\n            method = getattr(client, command)\n            return method(*args, **kwargs)\n    \n    def pipeline(self) -> redis.Pipeline:\n        \"\"\"Return a Redis pipeline for atomic operations.\"\"\"\n        client = self.get_client()\n        return client.pipeline()\n    \n    def close(self):\n        \"\"\"Close Redis connection (called during graceful shutdown).\"\"\"\n        if self._client:\n            self._client.close()\n            self._client = None\n```\n\n**Complete Custom JSON Encoder/Decoder** (`storage/serialization.py`):\n```python\nimport json\nimport base64\nfrom datetime import datetime, date, time\nfrom decimal import Decimal\nfrom uuid import UUID\nfrom enum import Enum\nfrom typing import Any, Dict\n\nclass JobEncoder(json.JSONEncoder):\n    \"\"\"Custom JSON encoder for Job serialization.\"\"\"\n    \n    def default(self, obj: Any) -> Any:\n        # Handle datetime objects (convert to ISO format with timezone)\n        if isinstance(obj, datetime):\n            # Ensure datetime is timezone-aware (UTC)\n            if obj.tzinfo is None:\n                obj = obj.replace(tzinfo=timezone.utc)\n            return obj.isoformat()\n        \n        # Handle date objects\n        if isinstance(obj, date):\n            return obj.isoformat()\n        \n        # Handle bytes (base64 encode)\n        if isinstance(obj, bytes):\n            return {\n                \"__type__\": \"bytes\",\n                \"value\": base64.b64encode(obj).decode('ascii')\n            }\n        \n        # Handle Decimal (string representation)\n        if isinstance(obj, Decimal):\n            return str(obj)\n        \n        # Handle UUID\n        if isinstance(obj, UUID):\n            return str(obj)\n        \n        # Handle Enum (store value)\n        if isinstance(obj, Enum):\n            return obj.value\n        \n        # Let base class handle other types (will raise TypeError)\n        return super().default(obj)\n\ndef job_object_hook(dct: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Custom object hook for JSON decoding to restore special types.\"\"\"\n    # Check for encoded bytes\n    if \"__type__\" in dct and dct[\"__type__\"] == \"bytes\":\n        return base64.b64decode(dct[\"value\"])\n    \n    # Auto-detect ISO datetime strings (naive or with timezone)\n    for key, value in dct.items():\n        if isinstance(value, str):\n            # Try to parse as datetime (ISO format)\n            try:\n                # Check if it looks like an ISO datetime\n                if \"T\" in value and (\"-\" in value or \":\" in value):\n                    # Parse with datetime.fromisoformat (Python 3.7+)\n                    # Note: This handles timezone-aware strings in Python 3.11+\n                    dt = datetime.fromisoformat(value.replace('Z', '+00:00'))\n                    dct[key] = dt\n            except (ValueError, AttributeError):\n                pass\n    \n    return dct\n\ndef serialize_job(job_dict: Dict[str, Any]) -> str:\n    \"\"\"Serialize job dictionary to JSON string with custom encoding.\"\"\"\n    return json.dumps(job_dict, cls=JobEncoder, separators=(',', ':'))\n\ndef deserialize_job(json_str: str) -> Dict[str, Any]:\n    \"\"\"Deserialize JSON string to job dictionary with custom decoding.\"\"\"\n    return json.loads(json_str, object_hook=job_object_hook)\n```\n\n#### Core Logic Skeleton Code\n\n**Job Class with Serialization Methods** (`models/job.py`):\n```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nimport uuid\nfrom enum import Enum\nfrom .job_status import JobStatus\nfrom storage.serialization import serialize_job, deserialize_job\n\nclass Job:\n    \"\"\"Core job representation with serialization capabilities.\"\"\"\n    \n    def __init__(\n        self,\n        job_id: str,\n        job_type: str,\n        args: List[Any],\n        kwargs: Dict[str, Any],\n        queue: str,\n        priority: int = 0,\n        max_retries: int = 3,\n        timeout_seconds: int = 1800,\n        created_at: Optional[datetime] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        status: JobStatus = JobStatus.PENDING,\n        attempts: int = 0,\n        errors: Optional[List[Dict[str, Any]]] = None,\n        started_at: Optional[datetime] = None,\n        completed_at: Optional[datetime] = None,\n        result: Optional[Any] = None,\n    ):\n        # TODO 1: Validate required fields (job_id, job_type, queue)\n        # TODO 2: Set created_at to UTC now if not provided\n        # TODO 3: Initialize metadata as empty dict if None\n        # TODO 4: Initialize errors as empty list if None\n        # TODO 5: Store all parameters as instance attributes\n        pass\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert job to dictionary for serialization.\"\"\"\n        # TODO 1: Create dictionary with all instance attributes\n        # TODO 2: Convert datetime objects using ISO format\n        # TODO 3: Add version field \"v\": 1 for schema evolution\n        # TODO 4: Handle Optional fields (skip if None or use null)\n        # TODO 5: Return dictionary ready for JSON serialization\n        pass\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Job':\n        \"\"\"Reconstruct job from dictionary.\"\"\"\n        # TODO 1: Validate required fields exist in data\n        # TODO 2: Check version compatibility (data.get(\"v\") == 1)\n        # TODO 3: Convert string dates back to datetime objects\n        # TODO 4: Create and return new Job instance with loaded data\n        # TODO 5: Handle missing optional fields with defaults\n        pass\n    \n    def serialize(self) -> str:\n        \"\"\"Serialize job to JSON string.\"\"\"\n        # TODO 1: Call to_dict() to get dictionary representation\n        # TODO 2: Use serialize_job() helper to convert to JSON string\n        # TODO 3: Validate size against max_payload_size (1MB default)\n        # TODO 4: Raise JobTooLargeError if payload exceeds limit\n        # TODO 5: Return the JSON string\n        pass\n    \n    @classmethod\n    def deserialize(cls, data: str) -> 'Job':\n        \"\"\"Deserialize job from JSON string.\"\"\"\n        # TODO 1: Use deserialize_job() helper to parse JSON string\n        # TODO 2: Call from_dict() with the parsed dictionary\n        # TODO 3: Return the reconstructed Job instance\n        # TODO 4: Handle JSON parsing errors with descriptive exceptions\n        pass\n    \n    def record_error(self, error: Exception) -> None:\n        \"\"\"Record an error that occurred during job execution.\"\"\"\n        # TODO 1: Create error dictionary with keys:\n        #   - timestamp: current UTC datetime\n        #   - exception: error class name (type(error).__name__)\n        #   - message: str(error)\n        #   - traceback: formatted traceback string\n        # TODO 2: Append error dictionary to self.errors list\n        # TODO 3: Update self.status based on retry logic\n        # TODO 4: Increment self.attempts counter\n        pass\n    \n    def should_retry(self) -> bool:\n        \"\"\"Determine if job should be retried.\"\"\"\n        # TODO 1: Check if status is FAILED (not DEAD_LETTER)\n        # TODO 2: Compare attempts to max_retries\n        # TODO 3: Return True if attempts < max_retries\n        # TODO 4: Return False otherwise\n        pass\n    \n    def calculate_next_retry(self, base_delay: int = 1) -> datetime:\n        \"\"\"Calculate next retry time using exponential backoff.\"\"\"\n        # TODO 1: Compute delay = base_delay * (2 ** (self.attempts - 1))\n        # TODO 2: Add optional jitter (±10% of delay)\n        # TODO 3: Return current time + delay as datetime\n        pass\n```\n\n**Configuration Models** (`config/models.py`):\n```python\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\n\n@dataclass\nclass QueueConfig:\n    \"\"\"Configuration for a named queue.\"\"\"\n    name: str\n    priority: int = 1\n    max_length: Optional[int] = None\n    \n    def __post_init__(self):\n        # TODO 1: Validate queue name (alphanumeric + underscores)\n        # TODO 2: Validate priority is non-negative integer\n        # TODO 3: Validate max_length is positive if not None\n        pass\n\n@dataclass\nclass WorkerConfig:\n    \"\"\"Configuration for a worker process.\"\"\"\n    queues: List[str] = field(default_factory=lambda: [\"default\"])\n    concurrency: int = 4\n    heartbeat_interval: int = 30\n    job_timeout: int = 1800\n    \n    def __post_init__(self):\n        # TODO 1: Validate queues list is non-empty\n        # TODO 2: Validate concurrency is positive integer\n        # TODO 3: Validate heartbeat_interval is between 5 and 300 seconds\n        # TODO 4: Validate job_timeout is positive integer\n        pass\n\n@dataclass  \nclass RedisConfig:\n    \"\"\"Redis connection configuration.\"\"\"\n    url: str\n    max_connections: int = 20\n    socket_timeout: int = 5\n    retry_on_timeout: bool = True\n    \n    def __post_init__(self):\n        # TODO 1: Validate URL format (starts with redis://)\n        # TODO 2: Validate max_connections is positive\n        # TODO 3: Validate socket_timeout is positive\n        pass\n\n@dataclass\nclass SystemConfig:\n    \"\"\"Root system configuration.\"\"\"\n    redis: RedisConfig\n    queues: List[QueueConfig] = field(default_factory=list)\n    workers: List[WorkerConfig] = field(default_factory=list)\n    max_payload_size: int = 1_048_576  # 1MB\n    job_history_size: int = 10_000\n    retry_base_delay: int = 1\n    retry_max_attempts: int = 3\n    \n    @classmethod\n    def from_env(cls) -> 'SystemConfig':\n        \"\"\"Load configuration from environment variables with defaults.\"\"\"\n        # TODO 1: Read REDIS_URL from environment (default: redis://localhost:6379/0)\n        # TODO 2: Parse queue definitions from QUEUES environment variable\n        # TODO 3: Parse worker configurations from WORKERS environment variable\n        # TODO 4: Create RedisConfig instance\n        # TODO 5: Create QueueConfig instances from parsed data\n        # TODO 6: Create WorkerConfig instances from parsed data\n        # TODO 7: Return SystemConfig instance with all components\n        pass\n```\n\n#### Language-Specific Hints\n\n1. **Use Python 3.8+** for dataclasses with `field(default_factory=...)` support\n2. **For ULIDs**, install `python-ulid` package: `pip install python-ulid`\n3. **Use `datetime.timezone.utc`** for timezone-aware UTC timestamps (Python 3.2+)\n4. **For JSON serialization**, `json.dumps()` with `default` parameter handles custom types\n5. **Use `__post_init__` method** in dataclasses for validation logic\n6. **Implement `__hash__` and `__eq__`** for Job if you need to store in sets/dicts\n7. **Use `functools.lru_cache`** for expensive computations like cron expression parsing\n8. **For thread safety**, use `threading.Lock` in singleton patterns\n\n#### Milestone Checkpoint\n\nAfter implementing the data model (Milestone 1 foundation), run this validation script:\n\n```bash\n# Test job serialization round-trip\npython -m pytest tests/test_job_serialization.py -v\n\n# Expected output:\n# test_job_to_dict_roundtrip ... PASSED\n# test_job_serialize_deserialize ... PASSED  \n# test_job_with_special_types ... PASSED\n# test_job_size_validation ... PASSED\n```\n\nManual verification steps:\n1. Create a Job with various data types (string, int, list, dict, datetime)\n2. Call `job.serialize()` - should return JSON string\n3. Call `Job.deserialize()` on that string - should return identical Job\n4. Check that `job.job_id` is a valid ULID (26 characters, alphanumeric)\n5. Verify that datetimes are stored in ISO format with timezone indicator\n6. Attempt to serialize a 2MB payload - should raise `JobTooLargeError`\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| JSON serialization fails with \"TypeError: Object of type datetime is not JSON serializable\" | Forgot to use custom encoder or passed datetime directly to `json.dumps()` | Check if you're using `JobEncoder` or calling `serialize()` method | Use `job.serialize()` instead of `json.dumps(job)` |\n| Job appears in queue but workers can't process it | Job schema changed or version mismatch | Check Redis: `redis-cli LRANGE job_queue:default 0 0` to see raw JSON | Ensure all components use same schema version |\n| ULIDs not sorting chronologically | Clock skew between machines or incorrect ULID generation | Compare ULID timestamps with system clock | Use NTP for clock sync, ensure ULID lib uses monotonic counter |\n| Datetimes showing wrong timezone | Naive datetime objects or mixing UTC/local time | Check `created_at` field in serialized JSON for timezone indicator | Always use `datetime.now(timezone.utc)` not `datetime.utcnow()` |\n| Redis connections exhausted | Creating new RedisClient instance per job instead of singleton | Check Redis `CLIENT LIST` command - too many connections | Use `RedisClient.get_instance()` singleton pattern |\n\n\n## Component Design: Job Queue Core\n\n> **Milestone(s):** Milestone 1: Job Queue Core. This section provides the complete design for the foundational queueing system that enables producers to enqueue jobs and workers to retrieve them. It establishes the core job representation, queue management, and priority handling that all other components build upon.\n\n### Mental Model: Post Office Sorting Facility\n\nImagine a large, well-organized post office that handles packages (jobs) from senders (producers) to recipients (job handlers). This mental model helps visualize the core queueing concepts:\n\n- **Mail Collection Point (Enqueue Interface):** Senders bring packages to the counter. Each package has a destination address (queue name) and a priority sticker (priority). The clerk (Queue Manager) validates the package isn't too large or improperly addressed, then stamps it with a unique tracking number (job ID) and places it in the correct sorting bin.\n\n- **Sorting Bins (Queues in Redis):** The post office has multiple wall-mounted bins, each labeled with a destination city (queue name). Some bins are marked \"Express\" (high priority) while others are \"Standard\" (normal priority). Within each bin, packages are stacked in order of arrival (FIFO). The facility has a policy that bins can only hold a certain number of packages to prevent overloading.\n\n- **Sorting Machine (Priority Algorithm):** A sorting machine (worker polling algorithm) continuously checks bins for packages to process. It doesn't just check bins in random order—it has a weighted schedule: it checks the \"Express\" bins twice as often as \"Standard\" bins. This ensures urgent packages move faster without completely starving regular ones.\n\n- **Package Manifest (Job Serialization):** Each package contains a detailed manifest (serialized job payload) listing the contents (arguments), handling instructions (job type), and special requirements (timeout, retries). The manifest uses a standardized format (JSON) that anyone in the postal system can read, ensuring packages can be processed by any worker station.\n\nThis mental model emphasizes several critical design aspects: **validation at entry** prevents problems downstream, **standardized packaging** enables interoperability, **organized storage** ensures efficient retrieval, and **intelligent sorting** respects priority without starvation. The post office doesn't process packages itself—it merely stores and routes them to available workers, just as our queue core doesn't execute jobs but manages their lifecycle until pickup.\n\n### Queue Manager Interface\n\nThe Queue Manager is the central orchestration component that mediates all job enqueue operations. It provides a clean, abstract interface for producers while handling the complex details of validation, serialization, and atomic Redis operations. Think of it as the \"front desk\" of our post office—all interactions with the queue system go through this interface.\n\nThe Queue Manager's primary responsibilities are:\n1. **Job Validation:** Ensuring jobs meet size and schema constraints before acceptance\n2. **Serialization:** Converting in-memory `Job` objects to a storage-ready format\n3. **Atomic Enqueue:** Safely adding jobs to Redis with proper priority handling\n4. **Queue Inspection:** Providing visibility into queue state without modifying it\n5. **Configuration Management:** Applying queue-specific rules (max length, priority)\n\nThe complete interface is defined in the following table:\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `enqueue_job` | `job: Job` | `str` (job_id) | Validates, serializes, and atomically adds a job to the appropriate Redis queue. Returns the unique job ID. Raises `ValidationError` for oversized/malformed jobs, `QueueFullError` if queue at capacity. |\n| `bulk_enqueue` | `jobs: List[Job]` | `List[str]` (job_ids) | Enqueues multiple jobs in a single atomic transaction using Redis pipeline. Returns list of job IDs in same order. If any job fails validation, the entire operation rolls back. |\n| `get_queue_length` | `queue_name: str` | `int` | Returns the current number of pending jobs in the specified queue. Counts only jobs in the main pending queue (not active, retry, or dead letter). |\n| `peek_queue` | `queue_name: str`, `count: int = 10` | `List[Job]` | Retrieves up to `count` jobs from the front of the queue without removing them. Useful for debugging and monitoring. Jobs are deserialized back to `Job` objects. |\n| `list_queues` | `include_internal: bool = False` | `List[str]` | Returns names of all configured queues. When `include_internal` is True, also returns system queues (retry, dead letter, scheduler). |\n| `delete_job` | `job_id: str`, `queue_name: str` | `bool` | Removes a specific job from a queue if it exists. Used for manual job management. Returns True if job was found and removed. |\n| `get_job_by_id` | `job_id: str` | `Optional[Job]` | Retrieves a job from Redis by its ID, regardless of which queue or state it's in. Returns None if job doesn't exist. |\n| `validate_job` | `job: Job` | `None` | Internal method that checks job size (against `max_payload_size`), required fields, and serializability. Raises `ValidationError` with specific reason. |\n\nThe Queue Manager maintains an internal mapping of queue configurations (`QueueConfig` objects) that define each queue's behavior. When a job is enqueued, the manager:\n\n1. Validates the job payload size and structure\n2. Generates a unique job ID (ULID for time-ordered uniqueness)\n3. Sets the job's initial status to `PENDING` and timestamps\n4. Serializes the job to JSON format\n5. Atomically pushes the serialized job to the appropriate Redis list\n6. Updates queue metrics for monitoring\n\nThis interface deliberately exposes only queue management operations—not worker operations like dequeue, which are handled by the Worker component. This separation of concerns ensures the queue manager remains focused on producer-side concerns while workers handle consumer-side logic.\n\n### Enqueue and Priority Algorithm\n\nThe algorithm for enqueuing jobs with priority support is more nuanced than a simple `LPUSH` to a Redis list. Our system supports multiple named queues with configurable priority weights, which means workers should poll higher-priority queues more frequently. However, we must maintain strict FIFO ordering *within* each queue. The algorithm achieves this through a combination of Redis operations and weighted polling logic in workers.\n\n#### Step-by-Step Enqueue Algorithm\n\nWhen a producer calls `enqueue_job(job)`:\n\n1. **Validation Phase:**\n   - Check if job payload size exceeds `SystemConfig.max_payload_size` (default 1MB)\n   - Verify all required `Job` fields are present: `job_type`, `args`, `kwargs`, `queue`\n   - Ensure the target queue exists in configuration (or create with default priority)\n   - If validation fails, raise `ValidationError` with specific message\n\n2. **Job Preparation:**\n   - Generate job ID using ULID (Universally Unique Lexicographically Sortable Identifier)\n     - ULID provides time-ordered uniqueness: `01J5XZR0W3F6Y2A4B8C9D10E11` (26 chars)\n     - First 10 characters are timestamp (milliseconds since epoch in base32)\n     - Remaining 16 characters are random bytes, ensuring uniqueness even at same millisecond\n   - Set `job.job_id` to generated ULID\n   - Set `job.status` to `PENDING`\n   - Set `job.created_at` to current UTC datetime\n   - Initialize `job.attempts = 0`, `job.errors = []`\n   - Set `job.started_at`, `job.completed_at`, `job.result` to `None`\n\n3. **Serialization:**\n   - Convert `Job` object to dictionary via `Job.to_dict()`\n   - Convert datetime objects to ISO 8601 strings for JSON compatibility\n   - Serialize dictionary to JSON string via `Job.serialize()`\n   - Example serialized job:\n     ```json\n     {\n       \"job_id\": \"01J5XZR0W3F6Y2A4B8C9D10E11\",\n       \"job_type\": \"process_image\",\n       \"args\": [\"photo.jpg\", {\"width\": 800, \"height\": 600}],\n       \"kwargs\": {\"quality\": 90},\n       \"queue\": \"image_processing\",\n       \"priority\": 2,\n       \"max_retries\": 3,\n       \"timeout_seconds\": 300,\n       \"created_at\": \"2024-01-15T10:30:00Z\",\n       \"metadata\": {\"user_id\": 12345, \"batch_id\": \"batch-001\"},\n       \"status\": \"PENDING\",\n       \"attempts\": 0,\n       \"errors\": [],\n       \"started_at\": null,\n       \"completed_at\": null,\n       \"result\": null\n     }\n     ```\n\n4. **Atomic Enqueue to Redis:**\n   - Use Redis pipeline for atomic operation sequence:\n     1. `HSET job:{job_id} {serialized_job}` - Store job in Redis hash for later retrieval\n     2. `EXPIRE job:{job_id} {job_ttl}` - Set TTL (e.g., 7 days) to prevent memory bloat\n     3. `LPUSH queue:{queue_name} {job_id}` - Add job ID to the queue's list\n     4. `INCR queue:{queue_name}:count` - Increment queue length metric\n   - If any step fails, the entire pipeline is rolled back\n   - Return the generated job ID to the producer\n\n#### Priority Weighted Polling Algorithm (Worker-Side)\n\nWhile the enqueue algorithm handles producer-side operations, the priority system requires special consideration for how workers poll queues. The Queue Manager doesn't implement this directly, but it influences the algorithm through queue configuration. Here's how priority-weighted polling works:\n\n1. **Queue Configuration Example:**\n   ```python\n   queues = [\n       QueueConfig(name=\"critical\", priority=3),    # High priority\n       QueueConfig(name=\"default\", priority=2),     # Medium priority  \n       QueueConfig(name=\"low\", priority=1),         # Low priority\n       QueueConfig(name=\"reports\", priority=1)      # Also low priority\n   ]\n   ```\n\n2. **Worker Polling Logic:**\n   - Workers maintain a list of queues sorted by priority weight\n   - They calculate polling frequency: `poll_count = priority_weight / total_weight`\n   - For the above configuration:\n     - Total weight = 3 + 2 + 1 + 1 = 7\n     - \"critical\" gets 3/7 ≈ 43% of polls\n     - \"default\" gets 2/7 ≈ 29% of polls\n     - \"low\" and \"reports\" each get 1/7 ≈ 14% of polls\n   - Workers use Redis `BRPOP` with timeout, cycling through queues according to these weights\n\nThis approach ensures higher-priority queues are checked more frequently while maintaining FIFO within each queue. It's a trade-off: true priority queuing (where a high-priority job jumps ahead of all lower-priority jobs) would require a single sorted data structure, but that's more complex and reduces throughput. Our approach provides good enough priority handling for most use cases while maintaining simplicity and performance.\n\n> **Key Insight:** We separate job storage (Redis hash) from queue ordering (Redis list). The list contains only job IDs, not full job data. This reduces memory usage when moving jobs between queues (e.g., to retry queue) and allows efficient job lookup by ID. The trade-off is an extra Redis operation to fetch job data when a worker dequeues a job ID.\n\n### ADR: Job Storage Mechanism\n\n> **Decision: Separate Job Storage from Queue Ordering**\n>\n> **Context:** We need to store complete job data (arguments, metadata, status history) while also maintaining queue ordering for efficient worker polling. Jobs may move between multiple queues during their lifecycle (pending → active → retry → dead letter), and we need efficient access to job data by ID for monitoring and management.\n>\n> **Options Considered:**\n> 1. **Store full job data in queue lists:** Each Redis list element contains the complete serialized job.\n> 2. **Separate storage with ID pointers:** Store jobs in Redis hashes keyed by job ID, with queue lists containing only IDs.\n> 3. **Hybrid with metadata in lists:** Store minimal metadata in lists with full data in separate storage.\n>\n> **Decision:** Option 2 - Separate storage with ID pointers. Store complete job data in Redis hashes (`job:{job_id}`) and queue job IDs in Redis lists (`queue:{queue_name}`).\n>\n> **Rationale:** This separation provides multiple benefits: (1) Moving jobs between queues is cheap—just move the ID, not the full payload; (2) Job lookup by ID is O(1) via hash get; (3) We can store additional job metadata in the hash without affecting queue operations; (4) Serialized job data is stored only once, reducing memory usage; (5) It aligns with Redis best practices for modeling relationships.\n>\n> **Consequences:** Positive: Efficient queue operations and job management. Negative: Requires two Redis operations to dequeue (get ID from list, then get data from hash). This is acceptable because the extra operation's cost is minimal compared to job execution time, and we can mitigate it with pipelining.\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Full data in lists | Single operation to dequeue, simpler implementation | Moving jobs between queues requires re-serializing and copying data, memory inefficient with large payloads, no efficient job-by-ID lookup | Too inefficient for job lifecycle management |\n| **Separate storage with IDs** | **Efficient job movement, O(1) job lookup, memory efficient, flexible metadata** | **Requires two operations to dequeue** | **Chosen - benefits outweigh minor performance cost** |\n| Hybrid (metadata in lists) | Balances lookup efficiency with queue performance | Complex serialization/deserialization logic, still requires separate storage for full job data | Adds complexity without clear benefits over option 2 |\n\n### ADR: Queue Priority Strategy\n\n> **Decision: Weighted Round-Robin Polling**\n>\n> **Context:** We need to support multiple queues with different priority levels. High-priority jobs should be processed sooner than low-priority ones, but we must avoid complete starvation of low-priority queues. The system should be predictable and configurable.\n>\n> **Options Considered:**\n> 1. **Strict priority polling:** Always check highest-priority queue first, only check lower queues when higher ones are empty.\n> 2. **Weighted random selection:** Randomly select queue based on weights each poll.\n> 3. **Weighted round-robin:** Systematically cycle through queues, visiting higher-priority queues more frequently.\n> 4. **Multiple Redis sorted sets:** Single data structure with priority scores, requiring atomic ZADD and ZRANGE operations.\n>\n> **Decision:** Option 3 - Weighted round-robin polling implemented in workers.\n>\n> **Rationale:** Strict priority causes starvation. Weighted random lacks predictability. Multiple sorted sets add Redis complexity and atomicity challenges. Weighted round-robin provides predictable, configurable priority handling without starvation. It's simple to implement and understand: if queue A has priority 3 and queue B has priority 1, workers will check A three times for every one time they check B. This gives priority jobs 3x more polling attention while still guaranteeing B gets processed eventually.\n>\n> **Consequences:** Positive: Predictable priority handling, no starvation, simple implementation. Negative: Not true priority queuing (a high-priority job enqueued after low-priority jobs in a different queue might still wait). This is acceptable for most background job scenarios where approximate priority is sufficient.\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Strict priority polling | Highest priority jobs always processed first | Complete starvation of low-priority queues, poor resource utilization | Unacceptable starvation risk |\n| Weighted random selection | Simple to implement, probabilistic priority | Unpredictable timing, can still starve low-priority queues by chance | Lack of predictability problematic |\n| **Weighted round-robin** | **Predictable, configurable, no starvation, simple** | **Not true priority (FIFO within queue only)** | **Chosen - best balance of fairness and priority** |\n| Multiple Redis sorted sets | True priority across all jobs, single atomic operation | Complex implementation, Redis contention, harder to debug | Over-engineering for most use cases |\n\n### Common Pitfalls in Queue Implementation\n\n⚠️ **Pitfall: Not Validating Payload Size Before Redis Operations**\n\n**Description:** Checking payload size only after serialization or during Redis operations. If a 5MB payload is serialized to JSON (making it even larger) and only rejected when Redis fails with \"OOM\" error or slow performance.\n\n**Why It's Wrong:** Wastes CPU on serialization, creates unclear error messages, may cause partial failures in bulk operations, and could trigger Redis memory issues before validation catches it.\n\n**How to Fix:** Validate payload size *before* any serialization. Calculate approximate JSON size by summing string lengths of arguments + fixed overhead. Implement in `QueueManager.validate_job()`:\n```python\nestimated_size = len(json.dumps(job.args)) + len(json.dumps(job.kwargs)) + 500  # metadata overhead\nif estimated_size > config.max_payload_size:\n    raise ValidationError(f\"Job payload exceeds {config.max_payload_size} bytes limit\")\n```\n\n---\n\n⚠️ **Pitfall: Forgetting TTL on Job Data**\n\n**Description:** Storing jobs in Redis without expiration time, assuming they'll always be deleted after processing. However, jobs can get stuck (worker crashes, bugs) or monitoring queries may keep references, causing indefinite memory growth.\n\n**Why It's Wrong:** Redis memory fills up over time, causing evictions or crashes. Production incidents often trace back to forgotten TTLs.\n\n**How to Fix:** Always set TTL when storing jobs. Use a conservative value (e.g., 7 days) that exceeds maximum possible job lifetime. In Redis pipeline:\n```python\npipeline.hset(f\"job:{job_id}\", mapping=serialized_job)\npipeline.expire(f\"job:{job_id}\", 604800)  # 7 days in seconds\n```\n\n---\n\n⚠️ **Pitfall: Non-Atomic Multi-Step Queue Operations**\n\n**Description:** Performing separate Redis commands for storing job data and adding to queue list. If system crashes between commands, job data may be stored but not enqueued (orphaned) or enqueued without data (worker will fail).\n\n**Why It's Wrong:** Creates inconsistent state that's hard to detect and recover from. Orphaned jobs waste memory; missing data causes worker crashes.\n\n**How to Fix:** Use Redis pipelines or transactions for all multi-step operations. The enqueue process should be atomic:\n```python\nwith redis.pipeline() as pipe:\n    pipe.hset(f\"job:{job_id}\", mapping=serialized_job)\n    pipe.expire(f\"job:{job_id}\", ttl)\n    pipe.lpush(f\"queue:{queue_name}\", job_id)\n    pipe.execute()  # All commands happen atomically\n```\n\n---\n\n⚠️ **Pitfall: Using Local Time Instead of UTC**\n\n**Description:** Storing timestamps in local timezone or using `datetime.now()` without timezone. When deployed across regions or during daylight saving transitions, job ordering and scheduling become inconsistent.\n\n**Why It's Wrong:** Causes incorrect job ordering (timezone differences), scheduling errors (DST shifts), and debugging nightmares when comparing timestamps from different servers.\n\n**How to Fix:** Always use UTC for all timestamps. In Python:\n```python\nfrom datetime import datetime, timezone\njob.created_at = datetime.now(timezone.utc)\n```\nSerialize as ISO 8601 with Z suffix: `\"2024-01-15T10:30:00Z\"`.\n\n---\n\n⚠️ **Pitfall: Blocking Operations Without Timeouts**\n\n**Description:** Using Redis `BLPOP` without timeout or with very long timeout in workers. This prevents graceful shutdown (worker hangs waiting for job when SIGTERM arrives) and hides connectivity issues.\n\n**Why It's Wrong:** Graceful shutdown fails, forcing SIGKILL. Network partitions cause all workers to hang indefinitely. System becomes unresponsive.\n\n**How to Fix:** Use reasonable timeouts (1-5 seconds) on blocking Redis operations. Implement heartbeat checking in worker main loop. For graceful shutdown, use interruptible waits or check shutdown flag between polls.\n\n---\n\n⚠️ **Pitfall: Serializing Non-Serializable Objects**\n\n**Description:** Allowing job arguments to contain database connections, file handles, or custom objects without proper serialization. JSON serialization fails with cryptic errors.\n\n**Why It's Wrong:** Jobs fail immediately on enqueue with unclear errors. Requires producers to understand serialization details. Limits what can be passed as job arguments.\n\n**How to Fix:** Provide clear documentation and validation. Use JSON-serializable types only: strings, numbers, booleans, lists, dicts. For complex objects, pass identifiers (database IDs, file paths) and have the job handler reconstruct them. Implement validation in `Job.to_dict()` that checks each argument.\n\n### Implementation Guidance for Queue Core\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option | Recommendation |\n|-----------|---------------|-----------------|----------------|\n| Redis Client | `redis-py` (official client) | `hiredis` parser for performance | `redis-py` with connection pooling - simple, robust, well-supported |\n| Serialization | JSON (built-in) | MessagePack or CBOR | JSON - human-readable, debuggable, good enough for most payloads |\n| ID Generation | UUID4 (random) | ULID (time-ordered) | **ULID** - provides time-based sorting, collision resistant, readable |\n| Validation | Manual size calculation | Pydantic models with validation | Manual + JSON schema check - keeps dependencies minimal |\n| Connection Handling | Simple client per operation | Connection pool with health checks | **Connection pool** - essential for production performance |\n\n#### B. Recommended File/Module Structure\n\n```\nbackground_job_processor/\n├── pyproject.toml                    # Project dependencies and metadata\n├── README.md\n├── src/\n│   └── job_processor/\n│       ├── __init__.py\n│       ├── core/                      # Core queue infrastructure\n│       │   ├── __init__.py\n│       │   ├── job.py                 # Job class definition and serialization\n│       │   ├── redis_client.py        # RedisClient wrapper with pooling\n│       │   ├── queue_manager.py       # QueueManager implementation\n│       │   └── config.py              # SystemConfig, QueueConfig, etc.\n│       ├── utils/\n│       │   ├── __init__.py\n│       │   ├── ulid_generator.py      # ULID generation utility\n│       │   └── validation.py          # Payload validation helpers\n│       └── cli.py                     # Command-line interface for testing\n├── tests/\n│   ├── __init__.py\n│   └── test_queue_core.py             # Tests for Milestone 1\n└── examples/\n    └── basic_enqueue.py               # Example usage of queue system\n```\n\n#### C. Infrastructure Starter Code\n\n**Complete Redis Client Wrapper (`redis_client.py`):**\n```python\n\"\"\"Redis client wrapper with connection pooling and error handling.\"\"\"\nimport redis\nfrom typing import Optional, Any, Dict\nimport logging\nfrom contextlib import contextmanager\n\nlogger = logging.getLogger(__name__)\n\n\nclass RedisClient:\n    \"\"\"Singleton Redis client with connection pooling.\n    \n    This wrapper provides a consistent interface for Redis operations with\n    proper connection management, error handling, and logging.\n    \"\"\"\n    \n    _instance: Optional['RedisClient'] = None\n    _client: Optional[redis.Redis] = None\n    \n    def __init__(self, url: str = \"redis://localhost:6379/0\", **connection_kwargs):\n        \"\"\"Initialize Redis client (private, use get_instance).\n        \n        Args:\n            url: Redis connection URL (redis://, rediss://, unix://)\n            **connection_kwargs: Additional connection parameters passed to redis.Redis\n        \"\"\"\n        if RedisClient._instance is not None:\n            raise RuntimeError(\"Use RedisClient.get_instance() to get singleton\")\n        \n        self.url = url\n        self.connection_kwargs = connection_kwargs\n        self._client = None\n        \n    @classmethod\n    def get_instance(cls, url: Optional[str] = None, **kwargs) -> 'RedisClient':\n        \"\"\"Get singleton RedisClient instance.\n        \n        Args:\n            url: Optional Redis URL (uses default if None on first call)\n            **kwargs: Connection parameters\n        \n        Returns:\n            Singleton RedisClient instance\n        \"\"\"\n        if cls._instance is None:\n            url = url or \"redis://localhost:6379/0\"\n            cls._instance = cls(url, **kwargs)\n            cls._instance._connect()\n        return cls._instance\n    \n    def _connect(self) -> None:\n        \"\"\"Establish Redis connection with pooling.\"\"\"\n        try:\n            # Parse URL and set up connection pool\n            pool = redis.ConnectionPool.from_url(\n                self.url,\n                max_connections=self.connection_kwargs.get('max_connections', 10),\n                socket_timeout=self.connection_kwargs.get('socket_timeout', 5),\n                retry_on_timeout=self.connection_kwargs.get('retry_on_timeout', True),\n                decode_responses=True  # Automatically decode responses to strings\n            )\n            self._client = redis.Redis(connection_pool=pool)\n            \n            # Test connection\n            self._client.ping()\n            logger.info(f\"Connected to Redis at {self.url}\")\n            \n        except redis.RedisError as e:\n            logger.error(f\"Failed to connect to Redis: {e}\")\n            raise\n    \n    def get_client(self) -> redis.Redis:\n        \"\"\"Get the underlying Redis client instance.\n        \n        Returns:\n            redis.Redis client instance\n        \n        Raises:\n            RuntimeError: If client is not connected\n        \"\"\"\n        if self._client is None:\n            raise RuntimeError(\"Redis client not connected\")\n        return self._client\n    \n    def execute(self, command: str, *args, **kwargs) -> Any:\n        \"\"\"Execute a Redis command with error handling.\n        \n        Args:\n            command: Redis command name (e.g., 'hset', 'lpush')\n            *args: Command arguments\n            **kwargs: Additional options\n        \n        Returns:\n            Command result\n        \n        Raises:\n            redis.RedisError: On Redis operation failure\n        \"\"\"\n        client = self.get_client()\n        try:\n            # Get the method from redis.Redis instance\n            method = getattr(client, command.lower())\n            return method(*args, **kwargs)\n        except redis.RedisError as e:\n            logger.error(f\"Redis command failed: {command} {args}: {e}\")\n            raise\n    \n    def pipeline(self) -> redis.Pipeline:\n        \"\"\"Return a Redis pipeline for atomic operations.\n        \n        Returns:\n            redis.Pipeline instance for executing multiple commands atomically\n        \"\"\"\n        client = self.get_client()\n        return client.pipeline()\n    \n    @contextmanager\n    def transaction(self):\n        \"\"\"Context manager for Redis transaction (MULTI/EXEC).\n        \n        Usage:\n            with redis_client.transaction() as pipe:\n                pipe.set('key1', 'value1')\n                pipe.incr('key2')\n        \"\"\"\n        pipe = self.pipeline()\n        try:\n            yield pipe\n            pipe.execute()\n        except redis.RedisError as e:\n            pipe.reset()\n            raise\n    \n    def close(self) -> None:\n        \"\"\"Close Redis connection pool.\"\"\"\n        if self._client:\n            self._client.close()\n            self._client = None\n            logger.info(\"Redis connection closed\")\n```\n\n**Complete ULID Generator Utility (`utils/ulid_generator.py`):**\n```python\n\"\"\"ULID generation for time-ordered unique job IDs.\"\"\"\nimport time\nimport os\nimport random\nfrom datetime import datetime, timezone\n\n# Base32 encoding alphabet (Crockford's)\nBASE32 = \"0123456789ABCDEFGHJKMNPQRSTVWXYZ\"\nBASE32_LEN = len(BASE32)\n\n\ndef ulid() -> str:\n    \"\"\"Generate a ULID string.\n    \n    ULID format: 01J5XZR0W3F6Y2A4B8C9D10E11\n    - First 10 chars: Timestamp in milliseconds (base32 encoded)\n    - Last 16 chars: Random bytes (base32 encoded)\n    \n    Returns:\n        26-character ULID string\n    \"\"\"\n    # Get current time in milliseconds\n    timestamp = int(time.time() * 1000)\n    \n    # Encode timestamp (48 bits -> 10 base32 chars)\n    timestamp_chars = []\n    for i in range(10):\n        timestamp_chars.append(BASE32[(timestamp >> (5 * (9 - i))) & 0x1F])\n    \n    # Generate random bytes (80 bits -> 16 base32 chars)\n    random_bytes = bytearray(os.urandom(10))\n    \n    # Ensure randomness doesn't exceed 80 bits\n    random_bytes[0] &= 0xFF  # Clear top bits to keep within 80 bits\n    \n    random_chars = []\n    # Process 5-bit chunks from the 80 random bits\n    random_bits = int.from_bytes(random_bytes, 'big')\n    for i in range(16):\n        random_chars.append(BASE32[(random_bits >> (5 * (15 - i))) & 0x1F])\n    \n    return ''.join(timestamp_chars + random_chars)\n\n\ndef ulid_to_datetime(ulid_str: str) -> datetime:\n    \"\"\"Extract timestamp from ULID.\n    \n    Args:\n        ulid_str: ULID string\n    \n    Returns:\n        datetime object in UTC\n    \"\"\"\n    if len(ulid_str) != 26:\n        raise ValueError(f\"Invalid ULID length: {ulid_str}\")\n    \n    timestamp_part = ulid_str[:10]\n    timestamp = 0\n    \n    # Decode base32 timestamp\n    for char in timestamp_part:\n        timestamp = timestamp * 32 + BASE32.index(char)\n    \n    # Convert milliseconds to seconds\n    return datetime.fromtimestamp(timestamp / 1000, tz=timezone.utc)\n\n\n# Test the implementation\nif __name__ == \"__main__\":\n    # Generate and test a few ULIDs\n    for _ in range(5):\n        u = ulid()\n        dt = ulid_to_datetime(u)\n        print(f\"ULID: {u} -> Time: {dt.isoformat()}\")\n```\n\n#### D. Core Logic Skeleton Code\n\n**Job Class with Serialization (`core/job.py`):**\n```python\n\"\"\"Job class definition with serialization methods.\"\"\"\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\nimport json\nimport uuid\n\n\nclass JobStatus(Enum):\n    \"\"\"Job lifecycle status states.\"\"\"\n    PENDING = \"pending\"          # Waiting in queue\n    ACTIVE = \"active\"            # Currently being processed\n    COMPLETED = \"completed\"      # Finished successfully\n    FAILED = \"failed\"            # Finished with error\n    RETRY_SCHEDULED = \"retry_scheduled\"  # Failed and scheduled for retry\n    DEAD_LETTER = \"dead_letter\"  # Permanently failed after max retries\n\n\nclass Job:\n    \"\"\"Represents a unit of work to be processed asynchronously.\n    \n    This is the core data structure that flows through the entire system.\n    \"\"\"\n    \n    def __init__(\n        self,\n        job_type: str,\n        args: Optional[List[Any]] = None,\n        kwargs: Optional[Dict[str, Any]] = None,\n        queue: str = \"default\",\n        priority: int = 1,\n        max_retries: int = 3,\n        timeout_seconds: int = 300,\n        metadata: Optional[Dict[str, Any]] = None,\n        job_id: Optional[str] = None,\n        created_at: Optional[datetime] = None,\n        status: JobStatus = JobStatus.PENDING,\n        attempts: int = 0,\n        errors: Optional[List[Dict[str, Any]]] = None,\n        started_at: Optional[datetime] = None,\n        completed_at: Optional[datetime] = None,\n        result: Optional[Any] = None,\n    ):\n        \"\"\"Initialize a new Job.\n        \n        Args:\n            job_type: Identifier for the handler function/class\n            args: Positional arguments for the job handler\n            kwargs: Keyword arguments for the job handler\n            queue: Target queue name\n            priority: Queue priority weight (higher = more important)\n            max_retries: Maximum retry attempts before giving up\n            timeout_seconds: Maximum execution time in seconds\n            metadata: Arbitrary key-value data for monitoring\n            job_id: Unique identifier (auto-generated if None)\n            created_at: Creation timestamp (auto-set if None)\n            status: Current job status\n            attempts: Number of execution attempts so far\n            errors: List of error records from previous failures\n            started_at: When job execution began\n            completed_at: When job finished (success or failure)\n            result: Job execution result (on success)\n        \"\"\"\n        self.job_id = job_id or str(uuid.uuid4())  # Will replace with ULID later\n        self.job_type = job_type\n        self.args = args or []\n        self.kwargs = kwargs or {}\n        self.queue = queue\n        self.priority = priority\n        self.max_retries = max_retries\n        self.timeout_seconds = timeout_seconds\n        self.metadata = metadata or {}\n        self.created_at = created_at or datetime.now(timezone.utc)\n        self.status = status\n        self.attempts = attempts\n        self.errors = errors or []\n        self.started_at = started_at\n        self.completed_at = completed_at\n        self.result = result\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert job to dictionary for serialization.\n        \n        Returns:\n            Dictionary representation of the job\n        \"\"\"\n        # TODO 1: Convert datetime objects to ISO format strings\n        # Hint: Use .isoformat() for datetime objects, check if value is datetime\n        \n        # TODO 2: Convert JobStatus enum to string value\n        # Hint: Access .value property of the enum\n        \n        # TODO 3: Ensure all values are JSON-serializable\n        # Hint: Recursively check lists and dicts, raise TypeError for non-serializable values\n        \n        # TODO 4: Return complete dictionary with all fields\n        # Structure should match the Job constructor parameters\n        \n        pass\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Job':\n        \"\"\"Reconstruct job from dictionary.\n        \n        Args:\n            data: Dictionary representation from to_dict()\n        \n        Returns:\n            Job instance\n        \"\"\"\n        # TODO 1: Parse ISO format strings back to datetime objects\n        # Hint: datetime.fromisoformat() for ISO strings\n        \n        # TODO 2: Convert status string back to JobStatus enum\n        # Hint: JobStatus(value) to reconstruct\n        \n        # TODO 3: Handle optional fields that might be missing in older serializations\n        # Hint: Use .get() with default values\n        \n        # TODO 4: Create and return Job instance with parsed data\n        # Pass all fields to the constructor\n        \n        pass\n    \n    def serialize(self) -> str:\n        \"\"\"Serialize job to JSON string.\n        \n        Returns:\n            JSON string representation\n        \"\"\"\n        # TODO 1: Convert to dictionary using to_dict()\n        # TODO 2: Serialize to JSON with default=str for any non-serializable values\n        # TODO 3: Return the JSON string\n        pass\n    \n    @classmethod\n    def deserialize(cls, data: str) -> 'Job':\n        \"\"\"Deserialize job from JSON string.\n        \n        Args:\n            data: JSON string from serialize()\n        \n        Returns:\n            Job instance\n        \"\"\"\n        # TODO 1: Parse JSON string to dictionary\n        # TODO 2: Convert from dictionary using from_dict()\n        # TODO 3: Return the Job instance\n        pass\n    \n    def record_error(self, error: Exception) -> None:\n        \"\"\"Record an error that occurred during job execution.\n        \n        Args:\n            error: Exception that was raised\n        \"\"\"\n        # TODO 1: Create error dictionary with keys:\n        # - 'type': error class name (type(error).__name__)\n        # - 'message': str(error)\n        # - 'traceback': formatted traceback string (use traceback.format_exc())\n        # - 'timestamp': current UTC time in ISO format\n        \n        # TODO 2: Append error dict to self.errors list\n        \n        # TODO 3: Increment self.attempts counter\n        \n        pass\n    \n    def should_retry(self) -> bool:\n        \"\"\"Determine if job should be retried.\n        \n        Returns:\n            True if job has retries remaining and hasn't exceeded max_retries\n        \"\"\"\n        # TODO 1: Check if attempts < max_retries\n        # TODO 2: Return True if more retries available, False otherwise\n        pass\n```\n\n**Queue Manager Implementation (`core/queue_manager.py`):**\n```python\n\"\"\"Queue manager for enqueuing and managing jobs.\"\"\"\nimport json\nimport logging\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime, timezone\n\nfrom .job import Job, JobStatus\nfrom .redis_client import RedisClient\nfrom .config import QueueConfig, SystemConfig\nfrom utils.ulid_generator import ulid\n\nlogger = logging.getLogger(__name__)\n\n\nclass ValidationError(Exception):\n    \"\"\"Raised when job validation fails.\"\"\"\n    pass\n\n\nclass QueueFullError(Exception):\n    \"\"\"Raised when queue is at maximum capacity.\"\"\"\n    pass\n\n\nclass QueueManager:\n    \"\"\"Manages job enqueue operations and queue inspection.\"\"\"\n    \n    def __init__(self, config: SystemConfig):\n        \"\"\"Initialize QueueManager with configuration.\n        \n        Args:\n            config: System configuration\n        \"\"\"\n        self.config = config\n        self.redis = RedisClient.get_instance(\n            url=config.redis.url,\n            max_connections=config.redis.max_connections,\n            socket_timeout=config.redis.socket_timeout,\n            retry_on_timeout=config.redis.retry_on_timeout\n        )\n        \n        # Build queue config map for quick lookup\n        self.queue_configs: Dict[str, QueueConfig] = {\n            qc.name: qc for qc in config.queues\n        }\n    \n    def enqueue_job(self, job: Job) -> str:\n        \"\"\"Validates, serializes, and atomically adds a job to the appropriate Redis queue.\n        \n        Args:\n            job: Job instance to enqueue\n        \n        Returns:\n            job_id: Unique identifier for the enqueued job\n        \n        Raises:\n            ValidationError: If job fails validation\n            QueueFullError: If queue is at maximum capacity\n        \"\"\"\n        # TODO 1: Validate the job using _validate_job() method\n        # TODO 2: Generate ULID for job_id if not already set\n        # TODO 3: Set job status to PENDING and created_at timestamp\n        # TODO 4: Get queue config for the target queue\n        # TODO 5: Check if queue is at capacity (if max_length is set)\n        # TODO 6: Serialize job to JSON using job.serialize()\n        # TODO 7: Use Redis pipeline for atomic operations:\n        #   a. Store job data in hash: HSET job:{job_id} {serialized_job}\n        #   b. Set TTL on job hash: EXPIRE job:{job_id} {job_ttl}\n        #   c. Add job ID to queue list: LPUSH queue:{queue_name} {job_id}\n        #   d. Increment queue length metric: INCR queue:{queue_name}:count\n        # TODO 8: Log successful enqueue with job_id and queue\n        # TODO 9: Return job_id\n        \n        pass\n    \n    def bulk_enqueue(self, jobs: List[Job]) -> List[str]:\n        \"\"\"Enqueues multiple jobs in a single atomic transaction.\n        \n        Args:\n            jobs: List of Job instances to enqueue\n        \n        Returns:\n            List of job IDs in same order as input jobs\n        \n        Raises:\n            ValidationError: If any job fails validation (entire batch rolls back)\n        \"\"\"\n        # TODO 1: Validate all jobs first (fail fast)\n        # TODO 2: Generate job IDs for all jobs missing them\n        # TODO 3: Prepare all jobs (status, timestamps)\n        # TODO 4: Group jobs by queue for efficient Redis operations\n        # TODO 5: Use Redis pipeline for atomic batch:\n        #   For each job:\n        #     - HSET job:{job_id} {serialized_job}\n        #     - EXPIRE job:{job_id} {job_ttl}\n        #   For each queue with jobs:\n        #     - LPUSH queue:{queue_name} [job_id1, job_id2, ...]\n        #     - INCRBY queue:{queue_name}:count {count}\n        # TODO 6: Return list of job IDs\n        \n        pass\n    \n    def get_queue_length(self, queue_name: str) -> int:\n        \"\"\"Returns the current number of pending jobs in the specified queue.\n        \n        Args:\n            queue_name: Name of the queue\n        \n        Returns:\n            Number of pending jobs\n        \"\"\"\n        # TODO 1: Use Redis LLEN command on queue:{queue_name} key\n        # TODO 2: Return the length as integer\n        \n        pass\n    \n    def peek_queue(self, queue_name: str, count: int = 10) -> List[Job]:\n        \"\"\"Retrieves jobs from the front of the queue without removing them.\n        \n        Args:\n            queue_name: Name of the queue\n            count: Maximum number of jobs to peek\n        \n        Returns:\n            List of Job instances (deserialized)\n        \"\"\"\n        # TODO 1: Use Redis LRANGE to get job IDs from start of list\n        # TODO 2: For each job ID, fetch job data from hash\n        # TODO 3: Deserialize job data to Job objects\n        # TODO 4: Return list of Job instances\n        \n        pass\n    \n    def list_queues(self, include_internal: bool = False) -> List[str]:\n        \"\"\"Returns names of all configured queues.\n        \n        Args:\n            include_internal: If True, include system queues\n        \n        Returns:\n            List of queue names\n        \"\"\"\n        # TODO 1: Get all queue names from self.queue_configs\n        # TODO 2: If include_internal is True, add system queue names:\n        #   - retry:scheduled (sorted set)\n        #   - dead:letter (list)\n        #   - scheduler:queued (list)\n        # TODO 3: Return sorted list of queue names\n        \n        pass\n    \n    def delete_job(self, job_id: str, queue_name: str) -> bool:\n        \"\"\"Removes a specific job from a queue if it exists.\n        \n        Args:\n            job_id: Job identifier\n            queue_name: Queue to remove job from\n        \n        Returns:\n            True if job was found and removed\n        \"\"\"\n        # TODO 1: Use Redis LREM to remove job_id from queue:{queue_name} list\n        # TODO 2: If removed, decrement queue:{queue_name}:count metric\n        # TODO 3: Return True if count > 0, False otherwise\n        \n        pass\n    \n    def get_job_by_id(self, job_id: str) -> Optional[Job]:\n        \"\"\"Retrieves a job from Redis by its ID.\n        \n        Args:\n            job_id: Job identifier\n        \n        Returns:\n            Job instance if found, None otherwise\n        \"\"\"\n        # TODO 1: Use Redis HGET to fetch job data from job:{job_id} hash\n        # TODO 2: If data exists, deserialize to Job object\n        # TODO 3: Return Job instance or None\n        \n        pass\n    \n    def _validate_job(self, job: Job) -> None:\n        \"\"\"Internal method to validate job before enqueue.\n        \n        Args:\n            job: Job to validate\n        \n        Raises:\n            ValidationError: If validation fails\n        \"\"\"\n        # TODO 1: Check if job payload size exceeds max_payload_size\n        # Hint: Estimate size using len(json.dumps(job.args)) + len(json.dumps(job.kwargs))\n        \n        # TODO 2: Validate required fields: job_type, queue\n        \n        # TODO 3: Check that args and kwargs are JSON-serializable\n        # Hint: Try json.dumps on each and catch TypeError\n        \n        # TODO 4: Ensure queue exists in config (or use default)\n        \n        # TODO 5: Validate priority is positive integer\n        \n        pass\n```\n\n#### E. Language-Specific Hints\n\n**Python-Specific Implementation Tips:**\n\n1. **JSON Serialization of Datetimes:** Use a custom JSON encoder for datetime objects:\n   ```python\n   import json\n   from datetime import datetime\n   \n   class DateTimeEncoder(json.JSONEncoder):\n       def default(self, obj):\n           if isinstance(obj, datetime):\n               return obj.isoformat()\n           return super().default(obj)\n   \n   # Usage\n   json_str = json.dumps(job_dict, cls=DateTimeEncoder)\n   ```\n\n2. **Redis Connection Pooling:** Configure connection pooling properly:\n   ```python\n   pool = redis.ConnectionPool.from_url(\n       'redis://localhost:6379/0',\n       max_connections=10,\n       socket_timeout=5,\n       socket_connect_timeout=5,\n       retry_on_timeout=True,\n       health_check_interval=30\n   )\n   ```\n\n3. **Atomic Operations with Pipelines:** Always use pipelines for multi-step Redis operations:\n   ```python\n   with redis_client.pipeline() as pipe:\n       pipe.multi()  # Start transaction\n       pipe.hset(f\"job:{job_id}\", mapping=job_data)\n       pipe.expire(f\"job:{job_id}\", 604800)\n       pipe.lpush(f\"queue:{queue_name}\", job_id)\n       pipe.execute()  # Atomic execution\n   ```\n\n4. **Size Estimation Without Full Serialization:** Estimate JSON size efficiently:\n   ```python\n   def estimate_json_size(obj):\n       \"\"\"Estimate JSON size without serializing entire object.\"\"\"\n       if isinstance(obj, (str, bytes)):\n           return len(obj)\n       elif isinstance(obj, (int, float)):\n           return 10  # Approximate\n       elif isinstance(obj, (list, tuple)):\n           return sum(estimate_json_size(item) for item in obj) + 2\n       elif isinstance(obj, dict):\n           size = 2\n           for k, v in obj.items():\n               size += estimate_json_size(k) + estimate_json_size(v) + 1\n           return size\n       return 50  # Conservative estimate for unknown types\n   ```\n\n5. **ULID Implementation:** Use the `ulid-py` library for production:\n   ```python\n   # pip install ulid-py\n   from ulid import ULID\n   job_id = str(ULID())  # Generates time-ordered ULID\n   ```\n\n#### F. Milestone Checkpoint\n\nAfter implementing the Job Queue Core (Milestone 1), verify your implementation with these checkpoints:\n\n**Checkpoint 1: Job Serialization Round-Trip**\n```bash\npython -m pytest tests/test_queue_core.py::test_job_serialization_roundtrip -v\n```\n**Expected Output:** Test passes with no errors. A job serialized and deserialized should have identical field values (timestamps within millisecond tolerance).\n\n**Checkpoint 2: Enqueue and Retrieve**\n```bash\n# Start Redis locally first: redis-server\npython examples/basic_enqueue.py\n```\n**Example script output:**\n```\nEnqueued job with ID: 01J5XZR0W3F6Y2A4B8C9D10E11\nQueue 'default' length: 1\nPeeked job: Job(job_id='01J5XZR0W3F6Y2A4B8C9D10E11', job_type='test_task', ...)\n```\n\n**Checkpoint 3: Payload Size Validation**\n```bash\npython -m pytest tests/test_queue_core.py::test_oversized_payload_rejected -v\n```\n**Expected:** Test passes with `ValidationError` raised for payload > 1MB.\n\n**Checkpoint 4: Queue Priority Configuration**\n```bash\npython examples/queue_priority_test.py\n```\n**Verify:** The script should show that high-priority queues receive more frequent polling attention (simulated by checking dequeue order over many iterations).\n\n**Signs Something Is Wrong:**\n- **Redis connection errors:** Check Redis is running and connection URL is correct\n- **Serialization errors:** Ensure all job arguments are JSON-serializable (no custom objects)\n- **Missing jobs after enqueue:** Verify pipeline operations are atomic and all execute successfully\n- **ULIDs not time-ordered:** Check ULID generation uses milliseconds, not seconds\n- **Queue length mismatch:** Ensure `LPUSH` and `INCR` operations happen in same pipeline\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Job disappears after enqueue | Pipeline not executing or Redis transaction rolled back | Check Redis logs for errors, verify `pipe.execute()` is called | Ensure all pipeline operations are followed by `execute()` |\n| Job data exists but not in queue | Orphaned job - stored in hash but not added to list | Use `redis-cli KEYS \"job:*\"` to find jobs, check if ID in queue list | Use atomic pipeline for both operations |\n| Queue length metric wrong | Race condition in INCR/DECR operations | Monitor `queue:{name}:count` vs actual LLEN | Use Redis transactions or Lua scripts for atomic increments |\n| Serialization fails with TypeError | Non-JSON-serializable arguments in job | Add validation before serialization, log argument types | Convert objects to serializable forms (IDs, paths, strings) |\n| ULIDs not monotonic | Using UUID instead of time-based ID | Check ULID generation uses current time in milliseconds | Implement proper ULID or use `ulid-py` library |\n| Jobs enqueued to non-existent queue | Queue config not loaded or mismatch | Check `queue_configs` map in QueueManager | Ensure queue exists in config or create default config |\n| Redis memory growing indefinitely | Missing TTL on job hashes | Check `job:{id}` keys with `TTL` command | Always set TTL when storing jobs |\n\n---\n\n\n## Component Design: Worker Process\n\n> **Milestone(s):** Milestone 2: Worker Process. This section provides the complete design for worker processes that fetch jobs from Redis queues, execute them with proper isolation, handle failures gracefully, and report their liveness for monitoring.\n\n### Mental Model: Factory Assembly Line Worker\n\nImagine a factory assembly line with multiple stations. Each **worker** is a skilled operator who:\n1. **Waits at a designated station** (queue) for work items to arrive\n2. **Receives a work ticket** (job) describing exactly what needs to be done\n3. **Performs the operation** using the right tools (handler functions)\n4. **Marks the ticket** as complete, moves it to the finished pile, or escalates it if something goes wrong\n5. **Reports their status** regularly to the floor manager to confirm they're still operational\n\nJust like factory workers have different specializations (some handle painting, others assembly), our workers can be configured to monitor specific queues for different job types. They work concurrently within their station, can be told to finish their current task before going home (graceful shutdown), and have safety mechanisms to stop working if a task takes too long or becomes dangerous (timeouts and exception handling).\n\nThis mental model emphasizes key worker characteristics: **specialization by queue**, **reliable task execution**, **concurrent operation within limits**, and **constant status reporting** for system health monitoring.\n\n### Worker Interface and Configuration\n\nThe worker component exposes a clean interface for starting, stopping, and monitoring worker processes. Each worker instance is configured through a `WorkerConfig` object that determines its behavior.\n\n**Worker Configuration Data Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `queues` | `List[str]` | List of queue names this worker should monitor (e.g., `[\"email\", \"reports\", \"default\"]`) |\n| `concurrency` | `int` | Maximum number of jobs this worker can process simultaneously (default: 1) |\n| `heartbeat_interval` | `int` | Seconds between heartbeats written to Redis (default: 30) |\n| `job_timeout` | `int` | Maximum seconds a job can run before being forcibly terminated (default: 300) |\n| `worker_id` | `str` | Unique identifier for this worker instance (auto-generated if not provided) |\n| `hostname` | `str` | Host machine name for monitoring (auto-detected) |\n| `pid` | `int` | Process ID for monitoring (auto-detected) |\n\n**Worker Public Interface:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `start()` | None | None | Starts the worker main loop, begins polling queues and processing jobs |\n| `stop(graceful=True)` | `graceful: bool` | None | Stops the worker; if `graceful=True`, completes current job before stopping |\n| `pause()` | None | None | Temporarily stops fetching new jobs but continues processing current ones |\n| `resume()` | None | None | Resumes fetching new jobs after being paused |\n| `get_status()` | None | `Dict[str, Any]` | Returns current worker status: idle/active, current job, queue depths, etc. |\n| `register_handler(job_type, handler_func)` | `job_type: str`, `handler_func: Callable` | None | Registers a function to handle a specific job type |\n\n**Worker Internal State Tracking:**\n\nEach worker maintains internal state that evolves throughout its lifecycle:\n\n| State | Description | Triggers |\n|-------|-------------|----------|\n| `STARTING` | Worker is initializing connections and loading handlers | Worker instantiation |\n| `IDLE` | Worker is waiting for jobs (blocking on queue reads) | After startup or job completion |\n| `PROCESSING` | Worker is actively executing a job | Job dequeued and execution started |\n| `PAUSED` | Worker is not fetching new jobs but may complete current job | Manual pause or system overload |\n| `SHUTTING_DOWN` | Worker is finishing current job before exiting | Stop signal received |\n| `STOPPED` | Worker has completely terminated | All cleanup complete |\n\nThese states are captured in the worker state machine diagram:\n\n![Worker Process State Machine](./diagrams/worker-state-machine.svg)\n\n### Worker Main Loop Algorithm\n\nThe worker's core logic follows a deterministic main loop that orchestrates job fetching, execution, and status reporting. Below is the step-by-step algorithm executed by each worker process:\n\n1. **Initialization Phase**\n   1. Generate a unique worker ID using ULID format (timestamp + randomness for lexicographic sorting)\n   2. Establish connection to Redis using the `RedisClient.get_instance()` singleton\n   3. Load job handler registry from application code (functions decorated with `@job_handler`)\n   4. Set up signal handlers for SIGTERM (graceful shutdown) and SIGINT (immediate shutdown)\n   5. Initialize worker status in Redis with current timestamp and `STARTING` state\n   6. Start heartbeat thread that updates Redis every `heartbeat_interval` seconds\n\n2. **Main Processing Loop**\n   1. **State Check**: If worker state is `SHUTTING_DOWN`, break loop and proceed to shutdown sequence\n   2. **Pause Check**: If worker state is `PAUSED`, sleep for 1 second and continue loop\n   3. **Queue Selection**: Determine which queue to poll next using priority-weighted round-robin\n      - Calculate total priority weight: sum of all configured queue priorities\n      - Generate random number between 0 and total weight\n      - Iterate through queues, subtracting each queue's priority until random number ≤ 0\n      - Select that queue for this polling cycle\n   4. **Job Fetching**: Attempt to fetch a job from the selected queue using `BRPOPLPUSH`\n      - Use `BRPOPLPUSH source_queue processing_queue timeout=1`\n      - If timeout occurs with no job, continue to next iteration\n      - If job retrieved, parse JSON into `Job` object using `Job.deserialize()`\n   5. **Job Processing Setup**:\n      - Update job status to `ACTIVE` and set `started_at` timestamp in Redis\n      - Update worker status to `PROCESSING` with current job ID\n      - Start execution timeout timer based on `job.timeout_seconds`\n   6. **Job Execution**:\n      - Look up handler function for `job.job_type` in registry\n      - If no handler found, mark job as failed with \"No handler registered\" error\n      - Execute handler with `job.args` and `job.kwargs` in isolated context (thread/process)\n      - Wait for completion or timeout\n   7. **Post-Execution Handling**:\n      - **If successful**: Update job status to `COMPLETED`, store result, increment metrics\n      - **If failed with exception**: Call `job.record_error()`, check `job.should_retry()`\n        - If retries remain: schedule retry with exponential backoff (Milestone 3)\n        - If no retries remain: move to dead letter queue\n      - **If timeout**: Record timeout error, treat as failure with retry logic\n   8. **Cleanup**: Remove job from processing queue, update worker status to `IDLE`\n\n3. **Shutdown Sequence**\n   1. Set worker state to `SHUTTING_DOWN` in Redis\n   2. Wait for current job to complete (up to graceful shutdown timeout)\n   3. If job doesn't complete in timeout, force termination and move job back to queue\n   4. Close Redis connection\n   5. Stop heartbeat thread\n   6. Set final worker state to `STOPPED`\n\nThe complete flow of a successful job execution is shown in the sequence diagram:\n\n![Happy Path Job Execution Sequence](./diagrams/happy-path-sequence.svg)\n\n### ADR: Concurrency Model (Process vs Thread)\n\n> **Decision: Thread Pool with Process Isolation for CPU-bound Tasks**\n>\n> - **Context**: Workers need to execute multiple jobs concurrently to maximize throughput. The system must handle both I/O-bound tasks (API calls, database queries) and CPU-bound tasks (image processing, data analysis). We need isolation to prevent one failing job from crashing the entire worker.\n>\n> - **Options Considered**:\n>   1. **Single-threaded worker**: Simple but poor resource utilization\n>   2. **Thread pool**: Good for I/O-bound tasks, shares memory space (risky)\n>   3. **Process pool**: Full isolation, higher overhead for job data transfer\n>   4. **Hybrid approach**: Thread pool for I/O tasks with optional process isolation for CPU tasks\n>\n> - **Decision**: Implement thread pool for general execution with optional process-based isolation for CPU-bound job types.\n>\n> - **Rationale**: Most background jobs in web applications are I/O-bound (sending emails, calling APIs, database operations). Threads provide excellent concurrency for these with minimal overhead. However, for CPU-intensive tasks or unreliable third-party code, process isolation prevents memory leaks and crashes from affecting other jobs. By making isolation configurable per job type, we get the best of both worlds.\n>\n> - **Consequences**: \n>   - Enables efficient handling of mixed workloads\n>   - Adds complexity to job data serialization for process boundaries\n>   - Requires careful management of process pool size to avoid resource exhaustion\n>   - Provides safer execution environment for untrusted job code\n\n**Concurrency Model Comparison:**\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Single-threaded | Simple, no race conditions | Poor CPU utilization, blocks on I/O | No |\n| Thread pool | Good I/O concurrency, shared memory | One job can crash entire worker, GIL limits CPU tasks | **Partially** (default) |\n| Process pool | Full isolation, bypasses GIL | Higher memory, serialization overhead | **Partially** (optional) |\n| Hybrid approach | Best of both worlds, flexible per job type | Most complex implementation | **Yes** |\n\n### ADR: Dequeue Strategy and Reliability\n\n> **Decision: BRPOPLPUSH with Processing Queue for Reliable Dequeue**\n>\n> - **Context**: When a worker fetches a job from Redis, the job must be removed from the main queue to prevent duplicate processing. However, if the worker crashes during execution, the job should not be lost forever. We need atomic dequeue with crash recovery.\n>\n> - **Options Considered**:\n>   1. **RPOP then delete**: Simple but can lose jobs if worker crashes between RPOP and processing\n>   2. **BRPOP with timeout**: Blocking pop but job disappears from Redis during execution\n>   3. **BRPOPLPUSH**: Atomically moves job to processing queue, visible during execution\n>   4. **Redis Streams with consumer groups**: Modern approach with built-in acknowledgment\n>\n> - **Decision**: Use `BRPOPLPUSH` to move jobs from main queue to a processing queue, with periodic health checks to recover orphaned jobs.\n>\n> - **Rationale**: `BRPOPLPUSH` provides atomic move operation in Redis, ensuring the job is never in \"limbo\" between queues. While Redis Streams are more feature-rich, they require Redis 5.0+ and add complexity for our FIFO queue needs. The processing queue acts as a visibility mechanism: jobs being processed remain in Redis so we can detect crashed workers and recover their jobs.\n>\n> - **Consequences**:\n>   - Requires maintaining processing queues per worker or worker group\n>   - Adds recovery logic to check for stale jobs in processing queues\n>   - Provides at-least-once delivery semantics (jobs may be processed twice if worker crashes after processing but before cleanup)\n>   - Compatible with older Redis versions\n\n**Dequeue Strategy Comparison:**\n\n| Option | Reliability | Complexity | Performance | Chosen? |\n|--------|-------------|------------|-------------|---------|\n| RPOP then delete | Low (job loss on crash) | Simple | High | No |\n| BRPOP | Medium (job invisible during processing) | Simple | High | No |\n| BRPOPLPUSH | High (visible in processing queue) | Moderate | High | **Yes** |\n| Redis Streams | High (built-in features) | High | Medium | No |\n\n### Common Pitfalls in Worker Implementation\n\n⚠️ **Pitfall: Blocking Forever on Queue Read**\n- **Description**: Using `BRPOP` with timeout=0 or `BLPOP` without timeout causes worker to block indefinitely, preventing graceful shutdown.\n- **Why it's wrong**: The worker cannot respond to shutdown signals while blocked in Redis I/O, requiring SIGKILL to terminate.\n- **Fix**: Use `BRPOPLPUSH` with a reasonable timeout (1-5 seconds) and check shutdown flags between polls.\n\n⚠️ **Pitfall: Not Isoling Job Execution**\n- **Description**: Running job handlers directly in the worker's main thread allows a job exception to crash the entire worker process.\n- **Why it's wrong**: A bug in one job handler terminates all concurrent jobs and requires worker restart.\n- **Fix**: Execute each job in a separate thread or process with proper exception boundaries.\n\n⚠️ **Pitfall: Forgetting Heartbeat Updates**\n- **Description**: Worker starts heartbeat thread but doesn't handle Redis connection failures, causing stale heartbeats.\n- **Why it's wrong**: Monitoring system marks active workers as dead, triggering unnecessary job recovery.\n- **Fix**: Implement retry logic in heartbeat updates and validate Redis connection before each update.\n\n⚠️ **Pitfall: Ignoring Memory Leaks in Long-running Workers**\n- **Description**: Worker processes thousands of jobs without restarting, gradually accumulating memory from uncollectable Python references.\n- **Why it's wrong**: Worker memory grows unbounded, eventually causing OOM kills or performance degradation.\n- **Fix**: Implement soft memory limits that trigger graceful shutdown after processing N jobs or reaching memory threshold.\n\n⚠️ **Pitfall: Incomplete Job State Transition**\n- **Description**: Worker updates job status to `ACTIVE` but crashes before setting `COMPLETED` or `FAILED`, leaving job in ambiguous state.\n- **Why it's wrong**: Recovery system cannot determine if job needs to be retried or was successful.\n- **Fix**: Use Redis transactions (MULTI/EXEC) to atomically update both job status and worker state.\n\n### Implementation Guidance for Worker\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Concurrency | `concurrent.futures.ThreadPoolExecutor` | `multiprocessing.Pool` with configurable isolation |\n| Signal Handling | `signal.signal()` for basic SIGTERM | `asyncio` event loop with signal integration |\n| Timeout Management | `threading.Timer` for job timeouts | Resource-aware timeout with CPU time monitoring |\n| Job Isolation | Basic try/except in thread | Subprocess with resource limits (CPU, memory) |\n\n**B. Recommended File/Module Structure:**\n\n```\nbackground_job_processor/\n├── worker/\n│   ├── __init__.py\n│   ├── worker.py              # Main Worker class implementation\n│   ├── config.py              # WorkerConfig and related configuration\n│   ├── handlers.py            # Job handler registry and decorator\n│   ├── concurrency.py         # Thread/process pool management\n│   ├── heartbeat.py           # Heartbeat thread implementation\n│   └── signals.py             # Signal handling utilities\n├── models/\n│   └── job.py                 # Job class (from Milestone 1)\n├── redis_client.py            # RedisClient (from Milestone 1)\n└── config/\n    └── system_config.py       # SystemConfig (from Milestone 1)\n```\n\n**C. Infrastructure Starter Code:**\n\n**Heartbeat Thread Implementation** (complete working code):\n\n```python\n# worker/heartbeat.py\nimport threading\nimport time\nimport logging\nfrom typing import Optional\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass WorkerHeartbeat:\n    \"\"\"Thread that periodically updates worker liveness in Redis.\"\"\"\n    \n    def __init__(self, worker_id: str, redis_client, interval: int = 30):\n        self.worker_id = worker_id\n        self.redis_client = redis_client\n        self.interval = interval\n        self._stop_event = threading.Event()\n        self._thread: Optional[threading.Thread] = None\n        self._last_successful = None\n        \n    def start(self):\n        \"\"\"Start the heartbeat thread.\"\"\"\n        if self._thread and self._thread.is_alive():\n            logger.warning(\"Heartbeat thread already running\")\n            return\n            \n        self._stop_event.clear()\n        self._thread = threading.Thread(\n            target=self._run,\n            name=f\"heartbeat-{self.worker_id}\",\n            daemon=True\n        )\n        self._thread.start()\n        logger.info(f\"Heartbeat started for worker {self.worker_id}\")\n        \n    def stop(self):\n        \"\"\"Stop the heartbeat thread.\"\"\"\n        self._stop_event.set()\n        if self._thread:\n            self._thread.join(timeout=5)\n            logger.info(f\"Heartbeat stopped for worker {self.worker_id}\")\n            \n    def _run(self):\n        \"\"\"Main heartbeat loop.\"\"\"\n        while not self._stop_event.is_set():\n            try:\n                # Update heartbeat in Redis with current timestamp\n                now = datetime.utcnow().isoformat()\n                key = f\"worker:heartbeat:{self.worker_id}\"\n                self.redis_client.execute(\"SET\", key, now, \"EX\", self.interval * 3)\n                self._last_successful = now\n                \n                # Also update worker status if available\n                status_key = f\"worker:status:{self.worker_id}\"\n                current_status = self.redis_client.execute(\"GET\", status_key)\n                if current_status:\n                    self.redis_client.execute(\"EXPIRE\", status_key, self.interval * 3)\n                    \n            except Exception as e:\n                logger.error(f\"Heartbeat update failed: {e}\")\n                \n            # Sleep for interval, but check stop event more frequently\n            for _ in range(self.interval * 10):  # Check every 0.1 seconds\n                if self._stop_event.is_set():\n                    break\n                time.sleep(0.1)\n                \n    def is_healthy(self) -> bool:\n        \"\"\"Check if heartbeat is updating successfully.\"\"\"\n        if not self._last_successful:\n            return False\n            \n        # Check if last successful update was within 2 intervals\n        try:\n            last_time = datetime.fromisoformat(self._last_successful)\n            cutoff = datetime.utcnow().timestamp() - (self.interval * 2)\n            return last_time.timestamp() > cutoff\n        except (ValueError, TypeError):\n            return False\n```\n\n**D. Core Logic Skeleton Code:**\n\n**Worker Main Loop Method** (with TODOs matching algorithm steps):\n\n```python\n# worker/worker.py\nimport signal\nimport time\nimport json\nimport logging\nfrom typing import Dict, Callable, Optional, List\nfrom datetime import datetime\n\nfrom models.job import Job, JobStatus\nfrom redis_client import RedisClient\nfrom worker.config import WorkerConfig\nfrom worker.heartbeat import WorkerHeartbeat\nfrom worker.concurrency import JobExecutor\n\nlogger = logging.getLogger(__name__)\n\nclass Worker:\n    \"\"\"Main worker class that processes jobs from Redis queues.\"\"\"\n    \n    def __init__(self, config: WorkerConfig):\n        self.config = config\n        self.worker_id = config.worker_id or self._generate_worker_id()\n        self.redis = RedisClient.get_instance()\n        self.handlers: Dict[str, Callable] = {}\n        self.heartbeat: Optional[WorkerHeartbeat] = None\n        self.executor: Optional[JobExecutor] = None\n        self.state = \"STARTING\"\n        self.current_job: Optional[Job] = None\n        self._shutdown_requested = False\n        self._paused = False\n        \n    def start(self):\n        \"\"\"Start the worker main loop.\"\"\"\n        # TODO 1: Set up signal handlers for SIGTERM and SIGINT\n        #   - SIGTERM should trigger graceful shutdown\n        #   - SIGINT should trigger immediate shutdown\n        \n        # TODO 2: Initialize Redis connection using self.redis.get_client()\n        \n        # TODO 3: Start heartbeat thread with self.config.heartbeat_interval\n        \n        # TODO 4: Initialize job executor (thread/process pool) with self.config.concurrency\n        \n        # TODO 5: Set worker state to \"IDLE\" in Redis\n        \n        # TODO 6: Enter main processing loop (call self._run_loop())\n        \n        # TODO 7: After loop exits, perform cleanup in self._shutdown()\n        \n        pass\n        \n    def _run_loop(self):\n        \"\"\"Main worker processing loop.\"\"\"\n        while not self._shutdown_requested:\n            try:\n                # TODO 1: Check if worker is paused - if so, sleep briefly and continue\n                \n                # TODO 2: Select queue to poll using priority-weighted algorithm\n                #   - Get queue priorities from configuration\n                #   - Calculate weighted random selection\n                #   - Choose queue for this iteration\n                \n                # TODO 3: Attempt to fetch job from selected queue using BRPOPLPUSH\n                #   - Use timeout of 1 second to allow shutdown checking\n                #   - If no job, continue to next iteration\n                #   - If job received, parse JSON into Job object\n                \n                # TODO 4: Update job status to ACTIVE and record start time\n                #   - Use Redis pipeline for atomic updates\n                #   - Set job.started_at = current time\n                #   - Update worker state to PROCESSING with job ID\n                \n                # TODO 5: Execute job using self.executor.submit()\n                #   - Look up handler function for job.job_type\n                #   - Submit to executor with timeout from job.timeout_seconds\n                #   - Handle case where no handler is registered\n                \n                # TODO 6: Wait for job completion with timeout\n                #   - If successful: update job status to COMPLETED\n                #   - If failed: call job.record_error() and handle retry logic\n                #   - If timeout: record timeout error and handle as failure\n                \n                # TODO 7: Clean up processing queue entry\n                #   - Remove job from processing queue\n                #   - Update worker state back to IDLE\n                #   - Increment success/failure metrics\n                \n            except Exception as e:\n                logger.exception(f\"Unexpected error in worker loop: {e}\")\n                # Brief sleep to prevent tight error loop\n                time.sleep(1)\n                \n    def _handle_job_result(self, job: Job, result, error: Optional[Exception]):\n        \"\"\"Process the result of a job execution.\"\"\"\n        # TODO 1: If error is None (success), update job status to COMPLETED\n        #   - Set job.completed_at = current time\n        #   - Store result in job.result (if serializable)\n        #   - Update success metrics\n        \n        # TODO 2: If error occurred, call job.record_error(error)\n        #   - Check job.should_retry() to determine next action\n        #   - If retries remain: schedule retry with exponential backoff (Milestone 3)\n        #   - If no retries: move to dead letter queue (Milestone 3)\n        \n        # TODO 3: Update job in Redis with final status and any result/error\n        #   - Use Redis pipeline for atomic updates\n        #   - Set appropriate TTL based on job status\n        \n        pass\n        \n    def stop(self, graceful: bool = True):\n        \"\"\"Request worker shutdown.\"\"\"\n        # TODO 1: Set self._shutdown_requested = True\n        \n        # TODO 2: If graceful=True, set state to SHUTTING_DOWN and wait for current job\n        #   - Wait up to job.timeout_seconds for current job to complete\n        #   - If timeout, force terminate and requeue job\n        \n        # TODO 3: If graceful=False, cancel current job immediately\n        #   - Use executor.shutdown(wait=False)\n        #   - Move current job back to main queue\n        \n        # TODO 4: Stop heartbeat thread\n        \n        # TODO 5: Close Redis connection\n        \n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- **Signal Handling**: Use `signal.signal(signal.SIGTERM, handler)` to catch shutdown signals. Set a flag in the handler rather than exiting immediately.\n- **Thread Safety**: Use `threading.Lock` for shared state between main thread and heartbeat thread.\n- **Job Timeouts**: Use `concurrent.futures.TimeoutError` with `executor.submit()` and `future.result(timeout=...)`.\n- **Graceful Shutdown**: Implement `__enter__` and `__exit__` methods on Worker class to use as context manager.\n- **Logging**: Use structured logging with `logging.JSONFormatter` for production monitoring.\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the Worker component, verify functionality with these steps:\n\n1. **Start a worker**:\n   ```python\n   from worker import Worker\n   from worker.config import WorkerConfig\n   \n   config = WorkerConfig(\n       queues=[\"default\"],\n       concurrency=2,\n       heartbeat_interval=30,\n       job_timeout=300\n   )\n   \n   worker = Worker(config)\n   worker.start()  # Runs in foreground, Ctrl+C to stop\n   ```\n\n2. **Test job execution**:\n   - Enqueue a test job using QueueManager from Milestone 1\n   - Observe worker logs: should show \"Processing job X\", \"Job completed successfully\"\n   - Check Redis: job status should transition PENDING → ACTIVE → COMPLETED\n\n3. **Verify graceful shutdown**:\n   - Send SIGTERM to worker process: `kill -TERM <pid>`\n   - Worker should finish current job before exiting\n   - Confirm \"Shutting down gracefully\" appears in logs\n\n4. **Check heartbeat**:\n   - Monitor Redis key `worker:heartbeat:<worker_id>`\n   - Value should update every `heartbeat_interval` seconds\n   - If worker dies, key should expire after 3×interval\n\n**Expected Output Signs:**\n- ✅ Worker connects to Redis and starts heartbeat\n- ✅ Worker fetches jobs from configured queues\n- ✅ Jobs execute with registered handlers\n- ✅ Worker responds to SIGTERM by finishing current job\n- ❌ If jobs fail silently, check exception handling in job wrapper\n- ❌ If worker hangs on shutdown, check BRPOPLPUSH timeout value\n\n\n## Component Design: Retry & Error Handling\n\n> **Milestone(s):** Milestone 3: Retry & Error Handling. This section provides the complete design for automatic retry mechanisms with exponential backoff and dead letter queue functionality, enabling resilient job processing in the face of transient failures.\n\n### Mental Model: Customer Service Escalation Process\n\nThink of the retry system as a customer service escalation process in a large organization. When a customer request (job) fails initially:\n\n1. **Initial attempt** → Frontline support handles the request (first execution attempt)\n2. **Failure detection** → The request is marked as requiring follow-up (error is recorded)\n3. **Escalation timing** → The request is scheduled for a retry after a calculated delay (escalation timer)\n4. **Escalation levels** → Each retry goes to a more experienced specialist (exponential backoff)\n5. **Maximum escalation** → After exhausting all escalation levels, the case goes to a special investigations team (dead letter queue)\n6. **Case review** → The investigations team can manually review, fix, and retry or archive the case (manual intervention)\n\nThis mental model helps understand why we need progressive delays between retries (to allow temporary issues to resolve), why we track error history (to understand the failure pattern), and why we need a dead letter queue (for cases requiring human intervention). The system ensures transient failures (network blips, temporary resource constraints) are automatically resolved while permanent failures (bad data, broken integrations) are surfaced for manual handling.\n\n### Retry Manager Interface\n\nThe `RetryManager` orchestrates the entire retry lifecycle, from detecting job failures to scheduling retries and managing the dead letter queue. It works closely with the `Worker` component (which detects failures) and the `Scheduler` component (which polls for scheduled retries).\n\n| Component | Responsibility | Key Methods | Collaborates With |\n|-----------|----------------|-------------|-------------------|\n| `RetryManager` | Manages retry logic and state | `schedule_retry`, `handle_failure`, `move_to_dead_letter`, `retry_dead_letter_job` | `Worker`, `QueueManager`, `Scheduler` |\n| `BackoffCalculator` | Computes retry delays | `calculate_delay`, `add_jitter` | `RetryManager` |\n| `DeadLetterQueue` | Stores permanently failed jobs | `store`, `retrieve`, `list`, `delete` | `RetryManager`, `QueueManager` |\n\nThe `RetryManager` provides these key interfaces:\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `handle_job_failure` | `job: Job`, `error: Exception` | `None` | Called by `Worker` when a job fails. Records the error, checks retry eligibility, and either schedules a retry or moves to dead letter queue. |\n| `schedule_retry` | `job: Job`, `delay_seconds: int` | `str` (job ID) | Schedules a job for retry after specified delay. Returns the job ID for tracking. |\n| `move_to_dead_letter` | `job: Job` | `bool` | Moves a job that has exhausted retries to the dead letter queue. Returns success status. |\n| `retry_dead_letter_job` | `job_id: str` | `Optional[Job]` | Removes a job from the dead letter queue and re-enqueues it for immediate retry. Returns the re-enqueued job or `None` if not found. |\n| `get_retry_stats` | `job_type: Optional[str]` | `Dict[str, Any]` | Returns retry statistics (success rate, average attempts, common errors) for all jobs or filtered by job type. |\n| `cleanup_expired_retries` | `max_age_hours: int` | `int` (count removed) | Removes old retry records from Redis to prevent memory bloat. Returns number of records removed. |\n\nThe retry process integrates with the job state machine (referenced in ![Job State Machine](./diagrams/job-state-machine.svg)):\n\n1. **Job execution fails** → `Worker` calls `RetryManager.handle_job_failure(job, error)`\n2. **Error recording** → Job's `errors` list is updated with exception details\n3. **Retry check** → If `job.attempts < job.max_retries`, schedule retry; otherwise move to dead letter\n4. **Retry scheduling** → Job status changes to `RETRY_SCHEDULED` and is added to Redis sorted set with execution timestamp\n5. **Retry execution** → `Scheduler` polls sorted set, finds due retries, re-enqueues to appropriate queue\n6. **Dead letter handling** → Jobs in `DEAD_LETTER` status can be manually inspected and retried via dashboard\n\n### Retry Scheduling and Backoff Algorithm\n\nThe retry scheduling algorithm ensures failed jobs are retried with increasing delays to avoid overwhelming recovering systems while maintaining fairness across different job types. Here's the complete algorithm executed by `RetryManager.handle_job_failure`:\n\n1. **Record error details**:\n   - Extract exception class name, message, and formatted stack trace\n   - Create error dictionary with timestamp and attempt number\n   - Append to `job.errors` list (capped at last 10 errors to prevent unbounded growth)\n\n2. **Increment attempt counter**:\n   - Increment `job.attempts` by 1\n   - Update `job.started_at = None` and `job.completed_at = None` to reset execution timing\n\n3. **Check retry eligibility**:\n   - If `job.attempts > job.max_retries`: proceed to dead letter queue\n   - If job has `no_retry` metadata flag: proceed to dead letter queue\n   - If error is in permanent failure list (configurable): proceed to dead letter queue\n   - Otherwise: schedule retry\n\n4. **Calculate retry delay** (exponential backoff with jitter):\n   - Base formula: `delay = base_delay * (2 ^ (attempt - 1))`\n   - Apply optional jitter: `delay_with_jitter = delay * (1 ± random_jitter_factor)`\n   - Enforce maximum delay cap: `final_delay = min(delay_with_jitter, max_delay)`\n   - Add minimum floor: `final_delay = max(final_delay, min_delay)`\n\n5. **Schedule retry execution**:\n   - Calculate retry timestamp: `retry_at = current_time + final_delay`\n   - Set job status to `RETRY_SCHEDULED`\n   - Store job in Redis sorted set with score = retry_at (Unix timestamp)\n   - Also store job in Redis hash for quick retrieval by job ID\n\n6. **Move to dead letter queue** (if retries exhausted):\n   - Set job status to `DEAD_LETTER`\n   - Store job in dead letter Redis hash (keyed by job ID)\n   - Also add to dead letter list for chronological access\n   - Optionally send notification/alert about permanent failure\n\nThe retry scheduling uses Redis sorted sets for time-ordered execution:\n\n| Redis Key Pattern | Data Type | Purpose | Example Key |\n|-------------------|-----------|---------|-------------|\n| `retry:scheduled:{queue}` | Sorted Set (ZSET) | Stores job IDs with retry timestamp as score | `retry:scheduled:email` |\n| `retry:jobs:{job_id}` | Hash (HASH) | Stores full job data for scheduled retries | `retry:jobs:01H5XYZABCDEF` |\n| `dead:letter:{queue}` | List (LIST) | Chronological list of dead letter job IDs | `dead:letter:email` |\n| `dead:jobs:{job_id}` | Hash (HASH) | Stores full job data for dead letter jobs | `dead:jobs:01H5XYZABCDEF` |\n\nHere's a concrete walk-through: Consider a `send_email` job with `max_retries=3` and `base_delay=1` second that fails due to a temporary SMTP connection error.\n\n1. **Attempt 1 fails at 10:00:00** → Error recorded, attempts=1, delay = 1 * 2^(1-1) = 1 second\n2. **Scheduled retry at 10:00:01** → Job added to sorted set with score=10:00:01 (Unix timestamp)\n3. **Attempt 2 fails at 10:00:01** → Error recorded, attempts=2, delay = 1 * 2^(2-1) = 2 seconds  \n4. **Scheduled retry at 10:00:03** → Job added to sorted set with score=10:00:03\n5. **Attempt 3 fails at 10:00:03** → Error recorded, attempts=3, delay = 1 * 2^(3-1) = 4 seconds\n6. **Scheduled retry at 10:00:07** → Job added to sorted set with score=10:00:07\n7. **Attempt 4 (final) fails at 10:00:07** → attempts=4 > max_retries=3 → Move to dead letter queue\n\n> **Key Insight**: The exponential backoff algorithm (`delay = base * 2^(attempt-1)`) creates a geometric progression that quickly increases delays while remaining computationally simple. Adding jitter (±10-20%) prevents retry stampedes where many jobs retry simultaneously after a system outage.\n\n### ADR: Backoff Algorithm Selection\n\n> **Decision: Exponential Backoff with Configurable Jitter**\n\n**Context**: When jobs fail due to transient errors (network timeouts, temporary resource constraints), immediate retries can exacerbate the problem by creating retry storms that overwhelm recovering systems. We need a backoff algorithm that spaces retries further apart with each attempt while providing some randomness to avoid synchronization.\n\n**Options Considered**:\n\n| Option | Pros | Cons |\n|--------|------|------|\n| **Fixed delay** (constant interval) | Simple to implement and understand | Doesn't adapt to failure severity; can cause persistent overload |\n| **Linear backoff** (delay = base * attempt) | Gradually increases delay | May increase too slowly for serious outages; predictable pattern |\n| **Exponential backoff** (delay = base * 2^(attempt-1)) | Quickly reduces retry pressure; standard in distributed systems | Can create long waits for users; may exceed reasonable timeouts |\n| **Fibonacci backoff** (delay = base * fib(attempt)) | More gradual than exponential; less extreme waits | More complex; less standardized in industry |\n| **Randomized exponential** (exponential + jitter) | Prevents synchronization; standard AWS/Google approach | Slightly more complex than pure exponential |\n\n**Decision**: We selected **Exponential backoff with configurable jitter** as the default algorithm.\n\n**Rationale**:\n1. **Industry standard**: AWS, Google Cloud, and major message queues use exponential backoff, making it familiar to developers\n2. **Mathematical properties**: Doubling delay each attempt provides O(2^n) growth which quickly reduces retry pressure during outages\n3. **Configurable**: Base delay and jitter factor can be tuned per job type (sensitive jobs can use smaller base, batch jobs larger)\n4. **Jitter prevents synchronization**: Without jitter, all jobs that failed at the same time retry simultaneously, potentially causing cascading failures. Adding ±10-25% random jitter spreads retries.\n5. **Predictable worst-case**: Maximum wait time can be calculated as `sum(base * 2^i for i in 0..max_retries-1)` which helps set SLAs\n\n**Consequences**:\n- **Positive**: System naturally backs off during outages, reducing load on recovering services\n- **Positive**: Jitter prevents thundering herd problems during recovery\n- **Negative**: Users of synchronous wrappers may experience long waits (mitigated by setting reasonable max_retries)\n- **Negative**: Jobs with tight deadlines may need custom retry strategies (supported via hooks)\n\nThe algorithm is implemented in a `BackoffCalculator` class with these configurable parameters:\n\n| Parameter | Default | Description | Rationale |\n|-----------|---------|-------------|-----------|\n| `base_delay_seconds` | 1 | Initial delay for first retry | 1 second allows quick recovery from transient blips |\n| `max_delay_seconds` | 3600 | Maximum delay between retries | 1 hour prevents excessively long delays |\n| `jitter_factor` | 0.1 | Random variation (±10%) | 10% provides good desynchronization without excessive unpredictability |\n| `min_delay_seconds` | 0.1 | Minimum delay (floor) | 100ms prevents near-instant retries |\n\n### ADR: Dead Letter Queue Design\n\n> **Decision: Dual Storage Dead Letter Queue with Manual Intervention**\n\n**Context**: When jobs exhaust all retry attempts, we need to preserve them for investigation rather than silently discarding them. The dead letter queue (DLQ) must store failed jobs with complete error history while supporting manual review, retry, or deletion. The design must balance storage efficiency with queryability.\n\n**Options Considered**:\n\n| Option | Pros | Cons |\n|--------|------|------|\n| **Redis List only** | Simple FIFO storage; easy to pop oldest jobs | No random access by job ID; limited metadata querying |\n| **Redis Hash only** | Fast job retrieval by ID; supports partial updates | No chronological ordering; hard to list all jobs |\n| **Redis Sorted Set** | Chronological ordering by failure time; range queries | Duplicate storage of job data; more complex |\n| **Dual storage (List + Hash)** | Chronological list + random access; best of both | Double storage overhead; need to keep synchronized |\n| **External database** | Unlimited storage; rich querying | Additional infrastructure; operational complexity |\n\n**Decision**: We selected **Dual storage using Redis List and Hash** for the dead letter queue.\n\n**Rationale**:\n1. **Chronological access needed**: Operators want to see \"most recent failures\" first, which Redis List provides via `LRANGE`\n2. **Random access needed**: When investigating a specific job ID from logs/alerts, Hash provides O(1) lookup\n3. **Redis already available**: No new infrastructure dependencies\n4. **Controlled storage**: We can implement automatic expiration/TTL to prevent unlimited growth\n5. **Atomic synchronization**: Redis transactions can keep List and Hash in sync atomically\n\n**Consequences**:\n- **Positive**: Both chronological browsing and direct job lookup are efficient\n- **Positive**: Leverages existing Redis infrastructure without new dependencies  \n- **Negative**: 2x storage overhead for DLQ jobs (mitigated by compression and TTL)\n- **Negative**: Manual cleanup required for List/Hash synchronization (handled in cleanup jobs)\n\nThe dead letter queue implementation stores each job in two places:\n\n1. **Chronological list**: `dead:letter:{queue}` (Redis List) contains job IDs in insertion order\n2. **Job data store**: `dead:jobs:{job_id}` (Redis Hash) contains full serialized job with error history\n\nWhen moving a job to DLQ, this atomic operation occurs:\n```redis\nMULTI\nHSET dead:jobs:{job_id} data {serialized_job} timestamp {failure_time}\nRPUSH dead:letter:{queue} {job_id}\nEXPIRE dead:jobs:{job_id} {dlq_ttl}\nEXPIRE dead:letter:{queue} {dlq_ttl}\nEXEC\n```\n\n> **Design Principle**: The dead letter queue is a diagnostic tool, not a permanent archive. Jobs automatically expire after a configurable TTL (default 30 days) to prevent unbounded Redis memory growth. Important failures should be alerted on and investigated promptly.\n\n### Common Pitfalls in Retry Implementation\n\n⚠️ **Pitfall: Infinite retry loops without limits**\n- **Description**: Forgetting to enforce `max_retries` or setting it too high (or infinite)\n- **Why it's wrong**: Jobs with permanent failures (bad data, broken integrations) retry forever, wasting resources and hiding real problems\n- **How to fix**: Always set a reasonable default `max_retries` (e.g., 3-5) and validate that `job.attempts <= job.max_retries` before scheduling retry\n\n⚠️ **Pitfall: Retrying non-idempotent operations without safeguards**\n- **Description**: Automatically retrying jobs that have side effects (charging credit cards, sending emails) can cause duplicate charges/spam\n- **Why it's wrong**: Business logic executes multiple times for what should be a single operation\n- **How to fix**: Either (1) mark non-idempotent jobs with `no_retry` metadata flag, or (2) implement idempotency keys in job handlers that check for previous execution\n\n⚠️ **Pitfall: Losing error context between retries**\n- **Description**: Only storing the latest error instead of full error history\n- **Why it's wrong**: Operators can't see if the failure mode changed (network → database → validation), which is crucial for debugging\n- **How to fix**: Append each error with timestamp, exception class, message, and stack trace to `job.errors` list\n\n⚠️ **Pitfall: Blocking retries on Redis sorted set polling**\n- **Description**: Using `ZRANGEBYSCORE` without pagination on large retry sets can block Redis\n- **Why it's wrong**: Large numbers of scheduled retries cause long-running Redis commands that delay other operations\n- **How to fix**: Process retries in batches (e.g., 100 at a time) and use `ZRANGEBYSCORE ... LIMIT` for pagination\n\n⚠️ **Pitfall: Not handling clock skew in distributed retry scheduling**\n- **Description**: Assuming all workers and scheduler have synchronized clocks when comparing retry timestamps\n- **Why it's wrong**: Jobs may retry too early or too late if system clocks differ\n- **How to fix**: Use Redis server time (`TIME` command) for retry scheduling, not local system time\n\n⚠️ **Pitfall: Dead letter queue without size limits or expiration**\n- **Description**: Storing all failed jobs indefinitely in Redis\n- **Why it's wrong**: Redis memory fills up, causing evictions or crashes\n- **How to fix**: Implement automatic cleanup based on age (TTL) and/or maximum queue size, with configurable limits\n\n### Implementation Guidance for Retry System\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Retry Storage | Redis sorted sets + hashes | Redis Streams with consumer groups |\n| Error Serialization | JSON with Python's `traceback.format_exc()` | Structured logging with error codes |\n| Backoff Algorithm | Exponential with fixed jitter | Adaptive backoff based on error type |\n| Dead Letter Queue | Redis List + Hash with TTL | External database (PostgreSQL) for long-term storage |\n\n**B. Recommended File/Module Structure**\n\n```\nbackground-job-processor/\n├── src/\n│   ├── job_processor/\n│   │   ├── __init__.py\n│   │   ├── job.py                    # Job class definition\n│   │   ├── redis_client.py           # RedisClient wrapper\n│   │   ├── queue_manager.py          # QueueManager implementation\n│   │   ├── worker.py                 # Worker implementation  \n│   │   ├── retry/                    # ← NEW: Retry subsystem\n│   │   │   ├── __init__.py\n│   │   │   ├── manager.py            # RetryManager class\n│   │   │   ├── backoff.py            # BackoffCalculator class\n│   │   │   ├── dead_letter.py        # DeadLetterQueue class\n│   │   │   └── errors.py             # Retry-specific exceptions\n│   │   ├── scheduler.py              # Scheduler (for retry polling)\n│   │   └── utils/\n│   │       ├── serialization.py\n│   │       └── time_utils.py\n├── tests/\n│   ├── test_retry_manager.py\n│   ├── test_backoff.py\n│   └── test_dead_letter.py\n└── pyproject.toml\n```\n\n**C. Infrastructure Starter Code**\n\n```python\n# src/job_processor/retry/backoff.py\n\"\"\"Exponential backoff calculator with jitter.\"\"\"\nimport random\nimport time\nfrom typing import Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass BackoffConfig:\n    \"\"\"Configuration for exponential backoff with jitter.\"\"\"\n    base_delay_seconds: float = 1.0\n    max_delay_seconds: float = 3600.0  # 1 hour\n    min_delay_seconds: float = 0.1     # 100ms minimum\n    jitter_factor: float = 0.1         # ±10% randomness\n    max_attempts: int = 5\n\n\nclass BackoffCalculator:\n    \"\"\"Calculates retry delays using exponential backoff with jitter.\"\"\"\n    \n    def __init__(self, config: Optional[BackoffConfig] = None):\n        self.config = config or BackoffConfig()\n    \n    def calculate_delay(self, attempt: int) -> float:\n        \"\"\"\n        Calculate retry delay for given attempt number.\n        \n        Args:\n            attempt: Current attempt number (1-based, where 1 is first retry)\n            \n        Returns:\n            Delay in seconds before next retry attempt\n        \"\"\"\n        if attempt < 1:\n            attempt = 1\n            \n        # Exponential backoff: base * 2^(attempt-1)\n        exponential_delay = self.config.base_delay_seconds * (2 ** (attempt - 1))\n        \n        # Apply jitter: add random variation ±jitter_factor\n        if self.config.jitter_factor > 0:\n            jitter = 1 + random.uniform(\n                -self.config.jitter_factor, \n                self.config.jitter_factor\n            )\n            exponential_delay *= jitter\n        \n        # Enforce minimum and maximum bounds\n        bounded_delay = max(self.config.min_delay_seconds, \n                           min(exponential_delay, self.config.max_delay_seconds))\n        \n        return bounded_delay\n    \n    def calculate_next_retry_time(self, attempt: int) -> float:\n        \"\"\"\n        Calculate absolute timestamp for next retry attempt.\n        \n        Args:\n            attempt: Current attempt number\n            \n        Returns:\n            Unix timestamp (seconds since epoch) for next retry\n        \"\"\"\n        delay = self.calculate_delay(attempt)\n        return time.time() + delay\n```\n\n**D. Core Logic Skeleton Code**\n\n```python\n# src/job_processor/retry/manager.py\n\"\"\"Retry manager for handling job failures and scheduling retries.\"\"\"\nimport logging\nimport time\nfrom typing import Dict, Any, Optional, List\nfrom datetime import datetime\n\nfrom ..job import Job, JobStatus\nfrom ..redis_client import RedisClient\nfrom .backoff import BackoffCalculator, BackoffConfig\nfrom .dead_letter import DeadLetterQueue\n\n\nclass RetryManager:\n    \"\"\"Manages retry logic for failed jobs.\"\"\"\n    \n    def __init__(self, redis_client: RedisClient, config: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Initialize retry manager.\n        \n        Args:\n            redis_client: Redis client for storing retry state\n            config: Optional configuration dictionary\n        \"\"\"\n        self.redis = redis_client\n        self.logger = logging.getLogger(__name__)\n        \n        # Configuration with defaults\n        self.config = {\n            'base_delay': config.get('base_delay', 1.0),\n            'max_delay': config.get('max_delay', 3600.0),\n            'max_attempts': config.get('max_attempts', 5),\n            'jitter_factor': config.get('jitter_factor', 0.1),\n            'retry_key_prefix': config.get('retry_key_prefix', 'retry:'),\n            'dead_letter_key_prefix': config.get('dead_letter_key_prefix', 'dead:'),\n        }\n        \n        self.backoff_calculator = BackoffCalculator(\n            BackoffConfig(\n                base_delay_seconds=self.config['base_delay'],\n                max_delay_seconds=self.config['max_delay'],\n                max_attempts=self.config['max_attempts'],\n                jitter_factor=self.config['jitter_factor']\n            )\n        )\n        \n        self.dead_letter_queue = DeadLetterQueue(\n            redis_client,\n            key_prefix=self.config['dead_letter_key_prefix']\n        )\n    \n    def handle_job_failure(self, job: Job, error: Exception) -> None:\n        \"\"\"\n        Handle a job failure by scheduling retry or moving to dead letter queue.\n        \n        Called by Worker when a job execution fails.\n        \n        Args:\n            job: The failed job instance\n            error: Exception that caused the failure\n            \n        Returns:\n            None\n        \"\"\"\n        # TODO 1: Record the error in job.errors list using job.record_error(error)\n        # Hint: job.record_error() should capture exception class, message, and stack trace\n        \n        # TODO 2: Increment job.attempts counter\n        \n        # TODO 3: Check if job should be retried (job.should_retry() method)\n        # Consider: max_retries, no_retry metadata flag, permanent error types\n        \n        # TODO 4: If job should be retried:\n        #   - Calculate retry delay using self.backoff_calculator.calculate_delay(job.attempts)\n        #   - Call self.schedule_retry(job, delay_seconds)\n        #   - Update job.status = JobStatus.RETRY_SCHEDULED\n        \n        # TODO 5: If job should NOT be retried (retries exhausted or permanent error):\n        #   - Call self.move_to_dead_letter(job)\n        #   - Update job.status = JobStatus.DEAD_LETTER\n        #   - Log warning about permanent failure\n        \n        # TODO 6: Store updated job state in Redis (job.to_dict() and HSET)\n        # Use Redis pipeline for atomic operations\n        \n        pass\n    \n    def schedule_retry(self, job: Job, delay_seconds: float) -> str:\n        \"\"\"\n        Schedule a job for retry after specified delay.\n        \n        Args:\n            job: Job to retry\n            delay_seconds: Delay before retry (in seconds)\n            \n        Returns:\n            Job ID of the scheduled retry\n        \"\"\"\n        # TODO 1: Calculate retry timestamp: current_time + delay_seconds\n        # Use time.time() for current Unix timestamp\n        \n        # TODO 2: Create retry key names:\n        #   retry_scheduled_key = f\"{self.config['retry_key_prefix']}scheduled:{job.queue}\"\n        #   retry_job_key = f\"{self.config['retry_key_prefix']}jobs:{job.job_id}\"\n        \n        # TODO 3: Use Redis pipeline for atomic operations:\n        #   - ZADD retry_scheduled_key {retry_timestamp} {job.job_id}\n        #   - HSET retry_job_key data {job.serialize()}\n        #   - EXPIRE retry_job_key {delay_seconds * 2} (TTL slightly longer than retry delay)\n        \n        # TODO 4: Execute pipeline and return job.job_id\n        \n        # TODO 5: Log retry scheduling at INFO level\n        pass\n    \n    def move_to_dead_letter(self, job: Job) -> bool:\n        \"\"\"\n        Move a job to the dead letter queue.\n        \n        Args:\n            job: Job that has exhausted retries\n            \n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        # TODO 1: Call self.dead_letter_queue.store(job)\n        # The DeadLetterQueue class should handle dual storage (List + Hash)\n        \n        # TODO 2: Remove job from any retry scheduled sets\n        # Use ZREM on retry_scheduled_key\n        \n        # TODO 3: Return success status\n        pass\n    \n    def get_due_retries(self, queue_name: str, max_count: int = 100) -> List[Job]:\n        \"\"\"\n        Retrieve jobs whose retry time has arrived.\n        \n        Called by Scheduler component.\n        \n        Args:\n            queue_name: Queue to check for due retries\n            max_count: Maximum number of jobs to return at once\n            \n        Returns:\n            List of Job instances ready for retry\n        \"\"\"\n        # TODO 1: Get current timestamp using time.time()\n        \n        # TODO 2: Construct retry scheduled key: f\"{self.config['retry_key_prefix']}scheduled:{queue_name}\"\n        \n        # TODO 3: Use ZRANGEBYSCORE with scores 0 to current_timestamp, LIMIT 0 max_count\n        # This gets job IDs whose retry time has arrived\n        \n        # TODO 4: For each job_id retrieved:\n        #   - Get job data from HGET retry:jobs:{job_id}\n        #   - Deserialize into Job object using Job.deserialize()\n        #   - Remove from retry scheduled set (ZREM)\n        #   - Delete retry job hash (HDEL)\n        # Use pipeline for atomic multi-get and cleanup\n        \n        # TODO 5: Return list of Job objects\n        pass\n```\n\n**E. Language-Specific Hints**\n\n1. **Error serialization**: Use Python's `traceback.format_exception()` to capture full stack traces:\n   ```python\n   import traceback\n   error_info = {\n       'type': type(error).__name__,\n       'message': str(error),\n       'traceback': ''.join(traceback.format_exception(type(error), error, error.__traceback__)),\n       'timestamp': datetime.utcnow().isoformat()\n   }\n   ```\n\n2. **Redis atomic operations**: Always use pipelines for multi-step Redis operations:\n   ```python\n   pipe = self.redis.get_client().pipeline()\n   pipe.zadd('retry:scheduled:email', {job_id: retry_timestamp})\n   pipe.hset(f'retry:jobs:{job_id}', 'data', job.serialize())\n   pipe.expire(f'retry:jobs:{job_id}', int(delay_seconds * 2))\n   pipe.execute()  # All commands execute atomically\n   ```\n\n3. **Configuration management**: Use Python's `dataclasses` for type-safe configuration:\n   ```python\n   from dataclasses import dataclass, field\n   from typing import Dict, Any\n   \n   @dataclass\n   class RetryConfig:\n       base_delay: float = 1.0\n       max_delay: float = 3600.0\n       max_attempts: int = 5\n       jitter_factor: float = 0.1\n       per_job_type_config: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n   ```\n\n**F. Milestone Checkpoint**\n\nAfter implementing the retry system, verify functionality with these tests:\n\n1. **Basic retry flow**: \n   ```python\n   # Enqueue a job that will fail\n   job = Job(job_type='failing_task', max_retries=3)\n   job_id = queue_manager.enqueue_job(job)\n   \n   # Worker processes and fails the job\n   worker.start()\n   # Job should move to retry scheduled state\n   \n   # Check Redis for retry scheduling\n   retries = retry_manager.get_due_retries('default')\n   # Should be empty initially (not yet due)\n   \n   # Wait for retry delay, then check again\n   time.sleep(2)  # Wait for first retry (1s base delay + jitter)\n   retries = retry_manager.get_due_retries('default')\n   # Should contain the job now\n   ```\n\n2. **Dead letter queue test**:\n   ```python\n   # Create job with max_retries=0 that fails\n   job = Job(job_type='permanent_failure', max_retries=0)\n   job_id = queue_manager.enqueue_job(job)\n   \n   # Worker processes and fails\n   worker.process_one_job()  # Should move directly to DLQ\n   \n   # Check dead letter queue\n   dlq_jobs = dead_letter_queue.list_jobs('default', limit=10)\n   # Should contain the failed job with DEAD_LETTER status\n   \n   # Manual retry from DLQ\n   retried_job = retry_manager.retry_dead_letter_job(job_id)\n   # Should return Job object and remove from DLQ\n   ```\n\n3. **Error history verification**:\n   ```python\n   # Process a job that fails multiple times\n   job = get_job_from_redis(job_id)\n   print(f\"Error count: {len(job.errors)}\")  # Should equal attempts\n   for i, error in enumerate(job.errors):\n       print(f\"Attempt {i+1}: {error['type']} - {error['message']}\")\n   # Should show full error history across all attempts\n   ```\n\n**Expected output signs**:\n- Jobs with transient failures automatically retry with increasing delays\n- After final failure, jobs appear in dead letter queue with complete error history\n- Redis memory usage remains bounded (TTL working)\n- No infinite retry loops occur\n\n**Diagnose problems**:\n- **Symptom**: Jobs retry immediately without delay → Check `BackoffCalculator.calculate_delay()` logic\n- **Symptom**: Jobs disappear after final failure (not in DLQ) → Check `move_to_dead_letter()` implementation\n- **Symptom**: Redis memory growing unbounded → Check TTL/EXPIRE commands on retry and DLQ keys\n- **Symptom**: Error history missing stack traces → Check `Job.record_error()` implementation\n\n**G. Debugging Tips**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Jobs retry forever | `max_retries` not enforced or too high | Check job.attempts vs job.max_retries in logs | Implement retry limit check in `handle_job_failure()` |\n| All retries happen simultaneously | Missing jitter in backoff algorithm | Check retry timestamps in Redis sorted set | Add jitter factor to `BackoffCalculator` |\n| Error history shows only last error | Errors overwritten instead of appended | Inspect `job.errors` list after multiple failures | Ensure `job.errors.append()` not `job.errors = ` |\n| Dead letter queue grows indefinitely | Missing TTL/expiration | Check Redis keys with `TTL dead:jobs:*` | Add EXPIRE commands when storing in DLQ |\n| Retries happen at wrong times | Clock skew between systems | Compare Redis TIME with system time | Use Redis server time for scheduling |\n| Manual retry from DLQ fails | Job not removed from DLQ lists | Check both List and Hash storage after retry | Ensure atomic removal from both data structures |\n\n\n## Component Design: Scheduling & Cron\n\n> **Milestone(s):** Milestone 4: Scheduling & Cron. This section provides the complete design for delayed and recurring job scheduling using cron expressions, enabling jobs to be executed at specific future times or on recurring schedules with unique constraint enforcement and timezone handling.\n\n### Mental Model: Calendar App with Recurring Events\n\nThink of the scheduler as a sophisticated calendar application with recurring event capabilities. Just as a calendar app stores events with specific dates, times, and recurrence rules (like \"every Monday at 9 AM\"), then notifies you when each event occurs, the scheduler stores jobs with execution times and recurrence patterns, then \"notifies\" the system by enqueuing them at the right moment.\n\nImagine setting up a weekly team meeting in your calendar:\n1. **Initial Setup**: You create an event titled \"Weekly Sync\" with start time \"next Monday 10:00 AM\" and recurrence \"weekly\"\n2. **Storage**: The calendar stores this as a recurring event definition with the original start time and recurrence rule\n3. **Notification**: Every week, your calendar checks which events are due and sends you a notification\n4. **Uniqueness**: If you try to create the same event twice for the same time slot, the calendar prevents duplicates\n5. **Missed Events**: If your phone was off during an event time, the calendar shows it as \"missed\" when you turn it back on\n\nThe scheduler operates similarly:\n- **Delayed Jobs**: Like one-time calendar events (\"dentist appointment next Tuesday at 3 PM\")\n- **Recurring Jobs**: Like repeating events (\"team meeting every Monday at 10 AM\")\n- **Notification**: Instead of alerting you, it enqueues jobs into worker queues\n- **Missed Jobs**: If the scheduler is down, it detects missed schedules and enqueues overdue jobs on restart\n\nThis mental model helps understand the core responsibilities: storing schedule definitions, calculating when jobs are due, ensuring uniqueness, and handling timezone complications.\n\n### Scheduler Interface\n\nThe scheduler component provides a clean API for scheduling jobs for future execution, whether as one-time delayed jobs or recurring jobs based on cron expressions. It operates as a standalone process that periodically checks for due jobs and enqueues them into the appropriate worker queues.\n\n**Primary Responsibilities:**\n1. Store and manage schedule definitions for delayed and recurring jobs\n2. Periodically poll for jobs whose execution time has arrived\n3. Evaluate cron expressions to calculate next execution times\n4. Enforce uniqueness constraints to prevent duplicate job enqueueing\n5. Handle timezone conversions consistently\n6. Recover from scheduler downtime by catching up on missed jobs\n\n**Core Data Structures:**\n\n| Type Name | Fields | Description |\n|-----------|--------|-------------|\n| `Schedule` | `schedule_id str`, `job_type str`, `args List[Any]`, `kwargs Dict[str, Any]`, `queue str`, `cron_expression Optional[str]`, `run_at Optional[datetime]`, `timezone str`, `enabled bool`, `unique_key Optional[str]`, `unique_window_seconds int`, `created_at datetime`, `last_enqueued_at Optional[datetime]`, `next_run_at Optional[datetime]`, `metadata Dict[str, Any]` | Defines a scheduled job, either one-time (`run_at`) or recurring (`cron_expression`). The `unique_key` prevents duplicate enqueueing within the `unique_window_seconds`. |\n| `ScheduledJob` | `schedule_id str`, `job_id str`, `scheduled_for datetime`, `enqueued_at Optional[datetime]`, `status ScheduleStatus` | Represents a specific scheduled execution instance, tracking when a job from a schedule should run and whether it has been enqueued. |\n| `ScheduleStatus` | `PENDING`, `ENQUEUED`, `SKIPPED`, `ERROR` | Status of individual scheduled job instances. `SKIPPED` occurs when uniqueness constraint prevents enqueueing. |\n\n**Scheduler Interface Methods:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `schedule_delayed_job` | `job Job`, `run_at datetime` | `str` (schedule_id) | Schedule a job for one-time execution at `run_at`. Validates that `run_at` is in the future. |\n| `schedule_recurring_job` | `job Job`, `cron_expression str`, `timezone str = \"UTC\"` | `str` (schedule_id) | Schedule a job for recurring execution based on cron expression. Validates cron syntax and calculates initial `next_run_at`. |\n| `unschedule_job` | `schedule_id str` | `bool` | Remove a schedule by ID, preventing future executions. Returns `True` if schedule existed and was removed. |\n| `pause_schedule` | `schedule_id str` | `bool` | Temporarily disable a schedule without removing it. Future polling will skip this schedule. |\n| `resume_schedule` | `schedule_id str` | `bool` | Re-enable a paused schedule. Next run time is recalculated from current time. |\n| `get_schedule` | `schedule_id str` | `Optional[Schedule]` | Retrieve schedule definition by ID, including current status and next run time. |\n| `list_schedules` | `enabled_only bool = False` | `List[Schedule]` | List all schedules, optionally filtered to only enabled ones. |\n| `update_schedule` | `schedule_id str`, `updates Dict[str, Any]` | `bool` | Update specific fields of a schedule (e.g., cron expression, timezone). Recalculates `next_run_at` if cron or timezone changes. |\n| `enqueue_due_jobs` | `max_jobs int = 100` | `int` (jobs enqueued) | Main polling method: finds due jobs, enqueues them to worker queues, updates schedule state. Returns count of jobs successfully enqueued. |\n\n**Schedule Storage Strategy:**\n\nThe scheduler uses two primary Redis data structures:\n1. **Schedule Definitions Hash**: `schedules:{schedule_id}` stores the full `Schedule` object as a hash for quick lookup and updates\n2. **Schedule Timeline Sorted Set**: `schedules:timeline` uses `next_run_at` timestamps as scores and `schedule_id` as members for efficient polling of due schedules\n\n**Example Schedule Creation Flow:**\n1. Client calls `schedule_recurring_job(job, \"0 9 * * 1\", \"America/New_York\")` (every Monday at 9 AM Eastern)\n2. Scheduler validates cron expression, parses timezone, calculates next Monday 9 AM in that timezone\n3. Creates `Schedule` object with `schedule_id`, stores in Redis hash\n4. Adds `schedule_id` to `schedules:timeline` sorted set with score = next run timestamp\n5. Returns `schedule_id` to client for future reference\n\n### Scheduler Polling and Cron Evaluation\n\nThe scheduler operates on a polling model where it periodically checks for due schedules and enqueues them. This approach balances simplicity with reliability, avoiding complex event-driven architectures while ensuring no schedules are missed during brief downtimes.\n\n**Polling Algorithm (Numbered Steps):**\n1. **Initialize Polling Loop**: The scheduler starts with configuration defining `polling_interval_seconds` (default: 60 seconds) and `batch_size` (default: 100 jobs per poll)\n2. **Calculate Polling Window**: Determine current time in UTC (`now_utc`) and calculate `poll_until = now_utc + polling_interval_seconds` to catch jobs due within the next polling interval\n3. **Retrieve Due Schedule IDs**: Use Redis `ZRANGEBYSCORE` on `schedules:timeline` with score range `0` to `poll_until` to get schedule IDs due for execution\n4. **Process Each Due Schedule**:\n   1. Fetch full schedule definition from Redis hash\n   2. Validate schedule is still `enabled` (skip if paused)\n   3. Apply uniqueness check using `unique_key` and `unique_window_seconds`\n   4. Create `Job` instance from schedule's `job_type`, `args`, `kwargs`, and `queue`\n   5. Generate `job_id` using ULID for chronological sorting\n   6. Use `QueueManager.enqueue_job()` to enqueue job to appropriate worker queue\n   7. Record `ScheduledJob` instance tracking this execution\n   8. Update schedule's `last_enqueued_at` to current time\n5. **Calculate Next Run for Recurring Jobs**: For schedules with cron expressions, calculate next execution time using current time as reference, update `next_run_at`, and update score in timeline sorted set\n6. **Cleanup One-Time Delayed Jobs**: For schedules with only `run_at` (no cron), remove from timeline and delete schedule definition after enqueueing\n7. **Handle Errors Gracefully**: If any schedule fails during processing (e.g., queue full, Redis error), log error, increment error counter, but continue processing other schedules\n8. **Sleep Until Next Poll**: Wait `polling_interval_seconds` before repeating, but adjust for processing time (e.g., if processing took 15 seconds, sleep only 45 seconds)\n\n**Cron Expression Evaluation:**\n\nThe system uses standard 5-field cron expressions (minute, hour, day of month, month, day of week) with the following parsing and evaluation logic:\n\n| Field | Allowed Values | Special Characters | Notes |\n|-------|----------------|-------------------|-------|\n| Minute | 0-59 | `*` , `-` , `/` , `,` | `*/5` = every 5 minutes |\n| Hour | 0-23 | `*` , `-` , `/` , `,` | `0-8` = hours 0 through 8 |\n| Day of Month | 1-31 | `*` , `-` , `/` , `,` , `?` , `L` | `L` = last day of month |\n| Month | 1-12 or JAN-DEC | `*` , `-` , `/` , `,` | Case-insensitive month names |\n| Day of Week | 0-7 or SUN-SAT | `*` , `-` , `/` , `,` , `?` , `L` , `#` | 0 and 7 = Sunday, `2#3` = third Tuesday |\n\n**Next Run Calculation Algorithm:**\n1. **Start Reference Time**: Use current time in schedule's timezone, or `last_enqueued_at` if calculating next run after successful enqueue\n2. **Increment to Next Candidate**: Add 1 minute to reference time to ensure we don't re-enqueue for same minute\n3. **Iterate Through Fields**: Check each cron field against candidate time:\n   1. If minute doesn't match, increment minutes until match found\n   2. If hour doesn't match, increment hours (reset minutes to 0)\n   3. If day of month doesn't match, increment days (reset hours/minutes)\n   4. If month doesn't match, increment months (reset days/hours/minutes)\n   5. If day of week doesn't match, increment days (could require checking multiple days)\n4. **Handle Edge Cases**: \n   - 31st day in months with only 30 days → roll to 1st of next month\n   - February 29th in non-leap years → roll to March 1st\n   - Last day of month (`L`) → calculate actual last day for that month/year\n5. **Return Valid Time**: First time that matches all cron fields, converted back to UTC for storage\n\n**Uniqueness Constraint Implementation:**\n\nTo prevent duplicate enqueueing of the same recurring job (e.g., if scheduler polls twice in same minute), the system implements a window-based uniqueness check:\n\n1. **Generate Unique Key**: Combine schedule ID with execution period identifier (e.g., `\"email_report:2024-03-25-09:00\"` for hourly job at 9 AM on March 25)\n2. **Check Redis Lock**: Use Redis `SET key value NX EX window_seconds` as atomic check-and-set\n3. **If Key Exists**: Job already enqueued within uniqueness window → skip enqueueing, mark as `SKIPPED`\n4. **If Key Doesn't Exist**: Proceed with enqueueing, set key with TTL = `unique_window_seconds`\n\n**Example Polling Cycle:**\n- Current time: 2024-03-25 09:01:30 UTC\n- Polling interval: 60 seconds\n- Due schedules in timeline with scores ≤ 2024-03-25 09:02:30 (now + 60s)\n- Found: `schedule:report:daily` with cron `0 9 * * *` (9 AM daily)\n- Schedule timezone: America/New_York (UTC-4 currently)\n- Check: 9 AM Eastern = 13:00 UTC, not due yet → skip\n- Found: `schedule:cleanup:hourly` with cron `0 * * * *` (top of every hour)\n- Check: 09:00 UTC just passed, within polling window → enqueue job\n- Calculate next run: next hour at 10:00 UTC\n- Update timeline score to 10:00 UTC timestamp\n\n![Scheduler Polling Algorithm Flowchart](./diagrams/scheduler-polling-flowchart.svg)\n\n### ADR: Scheduler Architecture (Polling vs Event)\n\n> **Decision: Polling-Based Scheduler Architecture**\n> - **Context**: We need to execute jobs at specific future times or on recurring schedules. The scheduler must be reliable, handle missed schedules during downtime, and not overwhelm Redis with constant queries. We considered event-driven approaches using Redis keyspace notifications or separate timer services.\n> - **Options Considered**:\n>   1. **Polling Architecture**: Scheduler process periodically queries Redis for due schedules\n>   2. **Event-Driven with Redis Keyspace Notifications**: Use Redis `PSUBSCRIBE` to notifications when sorted set scores pass thresholds\n>   3. **External Timer Service**: Use dedicated timer service like RabbitMQ's delayed message exchange or Redis Streams with consumer groups\n> - **Decision**: Implement polling architecture with configurable interval (default 60 seconds)\n> - **Rationale**: \n>   1. **Simplicity**: Polling is straightforward to implement, debug, and reason about\n>   2. **Reliability**: Missed polls (scheduler restart) are automatically caught on next poll using timeline scores\n>   3. **Resource Efficiency**: 60-second intervals generate minimal Redis load (one ZRANGEBYSCORE per poll)\n>   4. **Precision Acceptable**: Most scheduled jobs (daily reports, hourly cleanups) don't require sub-minute precision\n>   5. **Avoids Complex Dependencies**: Redis keyspace notifications require additional configuration and have delivery guarantees that complicate reliability\n> - **Consequences**:\n>   - **Positive**: Simple implementation, easy to test, handles scheduler restarts gracefully\n>   - **Negative**: Jobs may be enqueued up to `polling_interval_seconds` late (worst-case)\n>   - **Mitigation**: Use shorter intervals (e.g., 10 seconds) for time-sensitive jobs, though this increases Redis load\n\n**Comparison of Scheduler Architecture Options:**\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| **Polling** | Simple implementation, self-contained, handles downtime naturally, tunable precision vs load trade-off | Jobs can be delayed up to polling interval, constant Redis queries even when no jobs due | **Chosen** - Best balance of simplicity and reliability for this use case |\n| **Redis Keyspace Notifications** | Event-driven, near-instantaneous triggering, reduces polling load | Complex setup (notify-keyspace-events), notifications can be lost if client disconnected, requires persistent connection | Risk of lost notifications undermines reliability; added complexity not justified |\n| **Redis Streams Consumer Groups** | Built-in Redis reliability features, consumer tracking, message persistence | Complex implementation, requires managing consumer groups, overkill for scheduling | Scheduling doesn't need full message queue semantics; polling is simpler |\n| **External Timer Service** | Potentially more precise, dedicated service for timing | Additional dependency, operational complexity, network latency | Violates YAGNI; polling meets requirements without new dependencies |\n\n**Polling Interval Trade-off Analysis:**\n\n| Interval | Precision | Redis Load (QPS) | Missed Schedule Risk | Recommended For |\n|----------|-----------|------------------|----------------------|-----------------|\n| 60 seconds | ±60 seconds | 0.017 queries/sec | Very low | Default: daily/hourly jobs, system maintenance |\n| 30 seconds | ±30 seconds | 0.033 queries/sec | Low | Moderate precision needs: cache warming, periodic sync |\n| 10 seconds | ±10 seconds | 0.1 queries/sec | Moderate | Higher precision: near-real-time batch processing |\n| 1 second | ±1 second | 1 query/sec | High | Real-time scheduling (rarely needed) |\n\n> **Design Insight**: The polling interval represents a classic trade-off between timeliness and resource usage. By making it configurable per scheduler instance, we allow different deployment profiles: a primary scheduler with 60-second intervals for most jobs, and a secondary scheduler with 10-second intervals for time-sensitive jobs, both reading from the same schedule store.\n\n### ADR: Timezone Handling Strategy\n\n> **Decision: Store All Times in UTC, Evaluate Cron in Schedule Timezone**\n> - **Context**: Scheduled jobs often need to run at specific local times (e.g., \"9 AM in New York\" for morning reports). We must handle timezones correctly, including daylight saving time transitions, without introducing ambiguity or complexity in storage and comparison.\n> - **Options Considered**:\n>   1. **UTC-Only**: Store all times in UTC, require users to convert cron expressions to UTC\n>   2. **Schedule Timezone with UTC Storage**: Store next run time in UTC but calculate it using schedule's timezone\n>   3. **Local Time Storage**: Store times in local timezone, convert during comparisons\n>   4. **Timezone-Aware Cron**: Extend cron syntax with timezone field (e.g., `0 9 * * * America/New_York`)\n> - **Decision**: Store `next_run_at` in UTC, but evaluate cron expressions in the schedule's configured timezone\n> - **Rationale**:\n>   1. **Comparison Simplicity**: All timeline comparisons in Redis use UTC timestamps, avoiding timezone conversions during polling\n>   2. **Daylight Saving Handling**: Using Python's `pytz` or `zoneinfo` libraries handles DST transitions automatically when calculating next run\n>   3. **Storage Consistency**: UTC storage ensures consistent sorting and comparison in Redis sorted sets\n>   4. **User Expectation**: Users specify schedules in local time (e.g., \"9 AM Eastern\"), not UTC equivalents which change with DST\n> - **Consequences**:\n>   - **Positive**: Clean separation of concerns, handles DST correctly, consistent storage format\n>   - **Negative**: Requires timezone database (pytz/zoneinfo), extra conversion step during cron evaluation\n>   - **Edge Cases**: Ambiguous times during DST fall-back (2 AM occurs twice) must be handled by choosing first occurrence\n\n**Timezone Implementation Details:**\n\n1. **Schedule Configuration**: Each `Schedule` has `timezone` field (string like \"America/New_York\")\n2. **Cron Evaluation**:\n   - Load timezone object using `zoneinfo.ZoneInfo(timezone)` (Python 3.9+) or `pytz.timezone(timezone)`\n   - Convert current reference time from UTC to schedule timezone\n   - Calculate next run in schedule timezone\n   - Convert next run back to UTC for storage\n3. **UTC Storage**: All Redis timeline scores and `run_at` fields are UNIX timestamps (seconds since epoch) or ISO 8601 strings ending with \"Z\"\n4. **Daylight Saving Transitions**:\n   - Spring forward (1 AM → 3 AM): Jobs scheduled for 2:30 AM will run at 3:30 AM on transition day\n   - Fall back (2 AM → 1 AM): Jobs scheduled for 1:30 AM will run twice (once before, once after transition)\n\n**Example: Daily Report at 9 AM Eastern Through DST Transition:**\n\n| Date | Eastern Time (EDT/EST) | UTC Equivalent | Notes |\n|------|------------------------|----------------|-------|\n| March 10, 2024 | 09:00 EDT | 13:00 UTC | Before DST (Eastern Daylight Time) |\n| March 11, 2024 | 09:00 EDT | 13:00 UTC | DST begins at 2 AM → 3 AM |\n| November 3, 2024 | 09:00 EST | 14:00 UTC | DST ends at 2 AM → 1 AM |\n| November 4, 2024 | 09:00 EST | 14:00 UTC | After DST (Eastern Standard Time) |\n\nThe scheduler correctly handles this by:\n1. Storing `next_run_at` as UTC timestamp (13:00 or 14:00 UTC)\n2. When calculating next run on Nov 3, 2024:\n   - Current time: 2024-11-03 13:00 UTC (just ran)\n   - Convert to Eastern: 2024-11-03 09:00 EDT\n   - Add 1 day for daily schedule: 2024-11-04 09:00 EST (note timezone change)\n   - Convert to UTC: 2024-11-04 14:00 UTC\n   - Store `next_run_at` = 14:00 UTC\n\n**Comparison of Timezone Strategies:**\n\n| Strategy | Storage Format | DST Handling | User Experience | Implementation Complexity |\n|----------|----------------|--------------|-----------------|---------------------------|\n| **UTC-Only** | All times UTC | User must handle | Poor: users calculate UTC offsets | Simple but error-prone for users |\n| **UTC Storage, TZ Evaluation** | Times UTC, TZ metadata | Automatic via timezone libs | Good: users specify local time | Moderate: conversion logic |\n| **Local Time Storage** | Times with timezone | Complex comparisons | Good: intuitive storage | High: comparison logic complex |\n| **Timezone-Aware Cron** | Extended cron syntax | Built into syntax | Best: explicit in expression | High: custom parser needed |\n\n> **Design Insight**: Timezone handling is a classic \"store in UTC, display in local\" pattern adapted for scheduling. By performing timezone conversion only during next-run calculation (not during storage or comparison), we maintain simple UTC-based polling while respecting local time requirements.\n\n### Common Pitfalls in Scheduler Implementation\n\n⚠️ **Pitfall: Naïve Cron Evaluation Without Timezone Consideration**\n- **Description**: Evaluating cron expressions using server local time instead of schedule's configured timezone, causing jobs to run at wrong times when server timezone differs from schedule timezone.\n- **Why It's Wrong**: A server in UTC running a \"9 AM Eastern\" report would enqueue it at 9 AM UTC (5 AM Eastern), four hours early. This violates user expectations and can break time-sensitive operations.\n- **Fix**: Always convert reference time to schedule's timezone before cron evaluation, then convert result back to UTC for storage.\n\n⚠️ **Pitfall: Polling Without Catch-up Logic**\n- **Description**: Scheduler only checks for jobs with `next_run_at <= now()`, missing schedules that were due during scheduler downtime.\n- **Why It's Wrong**: If scheduler restarts after being down for 2 hours, hourly jobs scheduled during that period would be skipped entirely, breaking recurring job guarantees.\n- **Fix**: Always poll with range `0` to `now() + polling_interval`, not just `now()`. For additional safety, implement a \"catch-up\" mode on scheduler startup that processes all schedules with `next_run_at < now()`.\n\n⚠️ **Pitfall: Infinite Loop on Invalid Cron Expressions**\n- **Description**: When calculating next run time for invalid cron like `0 0 31 2 *` (February 31st), naïve algorithms may get stuck trying to find a matching date.\n- **Why It's Wrong**: Scheduler process hangs indefinitely or crashes when encountering malformed schedules, affecting all scheduled jobs.\n- **Fix**: Implement maximum iteration limits (e.g., 100,000 minutes ≈ 70 days) when searching for next matching time. Validate cron expressions when schedules are created, rejecting obviously invalid ones.\n\n⚠️ **Pitfall: Race Conditions in Uniqueness Checking**\n- **Description**: Two scheduler instances polling simultaneously might both pass uniqueness check and enqueue the same job, creating duplicates.\n- **Why It's Wrong**: Duplicate jobs waste resources and may cause application-level issues if jobs aren't idempotent.\n- **Fix**: Use Redis `SET key value NX EX window` for atomic check-and-set. The first scheduler to set the key wins; others find key exists and skip enqueueing.\n\n⚠️ **Pitfall: Not Handling DST Transition Ambiguity**\n- **Description**: During fall-back transition (2 AM → 1 AM), 1:30 AM occurs twice. Naïve scheduling might pick wrong occurrence or skip entirely.\n- **Why It's Wrong**: Jobs might run at wrong time (1:30 AM before transition instead of after) or be skipped on transition day.\n- **Fix**: Use `fold` parameter in Python's `datetime` or `pytz` `localize` with `is_dst` parameter to handle ambiguous times consistently (typically choose first occurrence).\n\n⚠️ **Pitfall: Stale Schedule Definitions**\n- **Description**: Schedules remain in Redis after being deleted or disabled, causing continuous errors during polling.\n- **Why It's Wrong**: Polling loop constantly encounters missing schedule data, filling logs with errors and potentially affecting performance.\n- **Fix**: Implement stale schedule cleanup: when fetching schedule fails (missing hash), remove it from timeline sorted set. Add health check metric for orphaned timeline entries.\n\n⚠️ **Pitfall: Blocking Polling Loop**\n- **Description**: Processing many due schedules synchronously causes polling loop to exceed interval, delaying subsequent polls.\n- **Why It's Wrong**: Scheduler falls behind real time, causing increasing latencies for all scheduled jobs.\n- **Fix**: Process schedules in bounded batches (e.g., 100 per poll). Use async processing or worker pool for enqueue operations. Monitor average processing time vs polling interval.\n\n### Implementation Guidance for Scheduler\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Cron Parsing** | `croniter` library (pip install croniter) | Custom parser using regex and datetime math |\n| **Timezone Handling** | Python 3.9+ `zoneinfo` (standard library) | `pytz` library (more comprehensive historical data) |\n| **Polling Loop** | `while True` with `time.sleep()` | `asyncio` with async Redis client for concurrent processing |\n| **Unique Key Generation** | Redis `SET NX EX` atomic operations | Redis Lua script for complex uniqueness logic |\n| **Schedule Storage** | Redis Hashes for schedules, Sorted Set for timeline | RedisJSON module for nested schedule objects |\n\n**B. Recommended File/Module Structure:**\n\n```\nbackground-job-processor/\n├── scheduler/                    # Scheduler component\n│   ├── __init__.py\n│   ├── scheduler.py             # Main Scheduler class\n│   ├── cron.py                  # Cron expression parser/evaluator\n│   ├── timezone.py              # Timezone handling utilities\n│   ├── models.py                # Schedule, ScheduledJob classes\n│   └── cli.py                   # Command-line interface\n├── core/                        # Shared core components\n│   ├── job.py                   # Job class (shared)\n│   ├── queue_manager.py         # QueueManager (shared)\n│   └── redis_client.py          # RedisClient (shared)\n└── config/\n    └── settings.py              # Configuration management\n```\n\n**C. Infrastructure Starter Code (COMPLETE, ready to use):**\n\n```python\n# scheduler/timezone.py\n\"\"\"Timezone handling utilities for schedule evaluation.\"\"\"\nimport datetime\nfrom typing import Optional\nfrom zoneinfo import ZoneInfo\n\n# Fallback for Python < 3.9\ntry:\n    from zoneinfo import ZoneInfo\nexcept ImportError:\n    from backports.zoneinfo import ZoneInfo\n\n\ndef ensure_timezone(dt: datetime.datetime, timezone: str) -> datetime.datetime:\n    \"\"\"Ensure datetime has timezone, converting naive to specified timezone.\n    \n    Args:\n        dt: Datetime (naive or aware)\n        timezone: IANA timezone string (e.g., \"America/New_York\")\n    \n    Returns:\n        Timezone-aware datetime in specified timezone\n    \n    Raises:\n        ValueError: If timezone string is invalid\n    \"\"\"\n    if dt.tzinfo is None:\n        # Naive datetime, assume it's in the specified timezone\n        tz = ZoneInfo(timezone)\n        return dt.replace(tzinfo=tz)\n    else:\n        # Already timezone-aware, convert to target timezone\n        tz = ZoneInfo(timezone)\n        return dt.astimezone(tz)\n\n\ndef to_utc(dt: datetime.datetime) -> datetime.datetime:\n    \"\"\"Convert timezone-aware datetime to UTC.\n    \n    Args:\n        dt: Timezone-aware datetime\n    \n    Returns:\n        UTC datetime\n    \n    Raises:\n        ValueError: If dt is naive (no timezone)\n    \"\"\"\n    if dt.tzinfo is None:\n        raise ValueError(\"Cannot convert naive datetime to UTC\")\n    return dt.astimezone(datetime.timezone.utc)\n\n\ndef from_utc(utc_dt: datetime.datetime, timezone: str) -> datetime.datetime:\n    \"\"\"Convert UTC datetime to specified timezone.\n    \n    Args:\n        utc_dt: UTC datetime (must be timezone-aware with UTC)\n        timezone: Target IANA timezone string\n    \n    Returns:\n        Datetime in target timezone\n    \"\"\"\n    if utc_dt.tzinfo != datetime.timezone.utc:\n        raise ValueError(f\"Expected UTC datetime, got {utc_dt.tzinfo}\")\n    \n    target_tz = ZoneInfo(timezone)\n    return utc_dt.astimezone(target_tz)\n\n\ndef next_dst_transition(timezone: str, after_dt: Optional[datetime.datetime] = None) -> Optional[datetime.datetime]:\n    \"\"\"Find next DST transition after given datetime.\n    \n    Args:\n        timezone: IANA timezone string\n        after_dt: Datetime to search after (defaults to now)\n    \n    Returns:\n        Datetime of next DST transition, or None if no upcoming transitions\n    \"\"\"\n    tz = ZoneInfo(timezone)\n    after = after_dt or datetime.datetime.now(datetime.timezone.utc)\n    \n    # Check transitions up to 2 years in future\n    end = after + datetime.timedelta(days=730)\n    \n    # Get transitions from zoneinfo\n    # Note: This is a simplified implementation\n    # Full implementation would use pytz for historical transitions\n    transitions = getattr(tz, '_transitions', [])\n    \n    for trans_time, trans_info in transitions:\n        trans_dt = datetime.datetime.fromtimestamp(trans_time, tz=datetime.timezone.utc)\n        if after < trans_dt < end:\n            return trans_dt\n    \n    return None\n```\n\n```python\n# scheduler/models.py\n\"\"\"Data models for schedule definitions.\"\"\"\nimport datetime\nimport uuid\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom core.job import Job\n\n\nclass ScheduleStatus(Enum):\n    \"\"\"Status of a scheduled job instance.\"\"\"\n    PENDING = \"pending\"\n    ENQUEUED = \"enqueued\"\n    SKIPPED = \"skipped\"  # Due to uniqueness constraint\n    ERROR = \"error\"\n\n\n@dataclass\nclass Schedule:\n    \"\"\"Definition of a scheduled job (recurring or one-time).\"\"\"\n    schedule_id: str = field(default_factory=lambda: f\"schedule_{uuid.uuid4().hex}\")\n    job_type: str = \"\"\n    args: List[Any] = field(default_factory=list)\n    kwargs: Dict[str, Any] = field(default_factory=dict)\n    queue: str = \"default\"\n    \n    # Scheduling configuration\n    cron_expression: Optional[str] = None  # e.g., \"0 9 * * *\"\n    run_at: Optional[datetime.datetime] = None  # For one-time delayed jobs\n    \n    # Timezone handling\n    timezone: str = \"UTC\"\n    \n    # State\n    enabled: bool = True\n    unique_key: Optional[str] = None  # For uniqueness constraint\n    unique_window_seconds: int = 300  # 5 minutes default uniqueness window\n    \n    # Metadata\n    created_at: datetime.datetime = field(default_factory=datetime.datetime.utcnow)\n    last_enqueued_at: Optional[datetime.datetime] = None\n    next_run_at: Optional[datetime.datetime] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert schedule to dictionary for serialization.\"\"\"\n        return {\n            'schedule_id': self.schedule_id,\n            'job_type': self.job_type,\n            'args': self.args,\n            'kwargs': self.kwargs,\n            'queue': self.queue,\n            'cron_expression': self.cron_expression,\n            'run_at': self.run_at.isoformat() if self.run_at else None,\n            'timezone': self.timezone,\n            'enabled': self.enabled,\n            'unique_key': self.unique_key,\n            'unique_window_seconds': self.unique_window_seconds,\n            'created_at': self.created_at.isoformat(),\n            'last_enqueued_at': self.last_enqueued_at.isoformat() if self.last_enqueued_at else None,\n            'next_run_at': self.next_run_at.isoformat() if self.next_run_at else None,\n            'metadata': self.metadata,\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Schedule':\n        \"\"\"Reconstruct schedule from dictionary.\"\"\"\n        # Parse datetime fields\n        run_at = None\n        if data.get('run_at'):\n            run_at = datetime.datetime.fromisoformat(data['run_at'])\n        \n        last_enqueued_at = None\n        if data.get('last_enqueued_at'):\n            last_enqueued_at = datetime.datetime.fromisoformat(data['last_enqueued_at'])\n        \n        next_run_at = None\n        if data.get('next_run_at'):\n            next_run_at = datetime.datetime.fromisoformat(data['next_run_at'])\n        \n        created_at = datetime.datetime.fromisoformat(data['created_at'])\n        \n        return cls(\n            schedule_id=data['schedule_id'],\n            job_type=data['job_type'],\n            args=data['args'],\n            kwargs=data['kwargs'],\n            queue=data['queue'],\n            cron_expression=data.get('cron_expression'),\n            run_at=run_at,\n            timezone=data.get('timezone', 'UTC'),\n            enabled=data.get('enabled', True),\n            unique_key=data.get('unique_key'),\n            unique_window_seconds=data.get('unique_window_seconds', 300),\n            created_at=created_at,\n            last_enqueued_at=last_enqueued_at,\n            next_run_at=next_run_at,\n            metadata=data.get('metadata', {}),\n        )\n    \n    def create_job(self) -> Job:\n        \"\"\"Create a Job instance from this schedule.\"\"\"\n        from core.job import Job, JobStatus\n        \n        # Generate job_id with ULID for chronological sorting\n        # Using uuid as fallback\n        import ulid\n        job_id = str(ulid.ULID())\n        \n        return Job(\n            job_id=job_id,\n            job_type=self.job_type,\n            args=self.args,\n            kwargs=self.kwargs,\n            queue=self.queue,\n            priority=0,  # Default priority\n            max_retries=3,  # Default retries\n            timeout_seconds=300,  # Default timeout\n            created_at=datetime.datetime.utcnow(),\n            metadata={\n                **self.metadata,\n                'schedule_id': self.schedule_id,\n                'scheduled_for': self.next_run_at.isoformat() if self.next_run_at else None,\n            },\n            status=JobStatus.PENDING,\n            attempts=0,\n            errors=[],\n            started_at=None,\n            completed_at=None,\n            result=None,\n        )\n\n\n@dataclass\nclass ScheduledJob:\n    \"\"\"Instance of a scheduled job execution.\"\"\"\n    schedule_id: str\n    job_id: str\n    scheduled_for: datetime.datetime\n    enqueued_at: Optional[datetime.datetime] = None\n    status: ScheduleStatus = ScheduleStatus.PENDING\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for storage.\"\"\"\n        return {\n            'schedule_id': self.schedule_id,\n            'job_id': self.job_id,\n            'scheduled_for': self.scheduled_for.isoformat(),\n            'enqueued_at': self.enqueued_at.isoformat() if self.enqueued_at else None,\n            'status': self.status.value,\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'ScheduledJob':\n        \"\"\"Reconstruct from dictionary.\"\"\"\n        scheduled_for = datetime.datetime.fromisoformat(data['scheduled_for'])\n        \n        enqueued_at = None\n        if data.get('enqueued_at'):\n            enqueued_at = datetime.datetime.fromisoformat(data['enqueued_at'])\n        \n        status = ScheduleStatus(data['status'])\n        \n        return cls(\n            schedule_id=data['schedule_id'],\n            job_id=data['job_id'],\n            scheduled_for=scheduled_for,\n            enqueued_at=enqueued_at,\n            status=status,\n        )\n```\n\n**D. Core Logic Skeleton Code (signature + TODOs only):**\n\n```python\n# scheduler/scheduler.py\n\"\"\"Main scheduler implementation.\"\"\"\nimport datetime\nimport logging\nimport time\nfrom typing import List, Optional, Tuple\n\nfrom croniter import croniter\n\nfrom core.job import Job\nfrom core.queue_manager import QueueManager\nfrom core.redis_client import RedisClient\nfrom scheduler.models import Schedule, ScheduledJob, ScheduleStatus\nfrom scheduler.timezone import ensure_timezone, to_utc, from_utc\n\n\nclass Scheduler:\n    \"\"\"Main scheduler class for delayed and recurring job scheduling.\"\"\"\n    \n    def __init__(self, redis_client: RedisClient, queue_manager: QueueManager,\n                 polling_interval_seconds: int = 60, batch_size: int = 100):\n        \"\"\"Initialize scheduler.\n        \n        Args:\n            redis_client: Redis client for schedule storage\n            queue_manager: Queue manager for enqueuing jobs\n            polling_interval_seconds: How often to poll for due schedules\n            batch_size: Maximum schedules to process per poll\n        \"\"\"\n        self.redis = redis_client\n        self.queue_manager = queue_manager\n        self.polling_interval = polling_interval_seconds\n        self.batch_size = batch_size\n        self.logger = logging.getLogger(__name__)\n        self._running = False\n        \n        # Redis key patterns\n        self.schedule_key = \"schedule:{schedule_id}\"\n        self.timeline_key = \"schedules:timeline\"\n        self.uniqueness_key = \"schedule:unique:{unique_key}\"\n        self.scheduled_jobs_key = \"schedule:jobs:{schedule_id}\"\n    \n    def schedule_delayed_job(self, job: Job, run_at: datetime.datetime) -> str:\n        \"\"\"Schedule a job for one-time delayed execution.\n        \n        Args:\n            job: Job to schedule\n            run_at: When to enqueue the job (must be in future)\n        \n        Returns:\n            Schedule ID for future reference\n        \n        Raises:\n            ValueError: If run_at is in the past\n        \"\"\"\n        # TODO 1: Validate run_at is in the future (compare to UTC now)\n        # TODO 2: Create Schedule object with run_at (no cron_expression)\n        # TODO 3: Generate unique schedule_id\n        # TODO 4: Calculate next_run_at (same as run_at)\n        # TODO 5: Store schedule in Redis hash (schedule_key)\n        # TODO 6: Add to timeline sorted set with score = next_run_at timestamp\n        # TODO 7: Return schedule_id\n        \n        # HINT: Use to_utc() to convert run_at to UTC for storage\n        # HINT: Use redis.pipeline() for atomic schedule creation + timeline addition\n    \n    def schedule_recurring_job(self, job: Job, cron_expression: str,\n                               timezone: str = \"UTC\") -> str:\n        \"\"\"Schedule a job for recurring execution based on cron expression.\n        \n        Args:\n            job: Job to schedule\n            cron_expression: Cron expression (e.g., \"0 9 * * *\")\n            timezone: IANA timezone for schedule evaluation\n        \n        Returns:\n            Schedule ID for future reference\n        \n        Raises:\n            ValueError: If cron expression is invalid\n        \"\"\"\n        # TODO 1: Validate cron expression using croniter\n        # TODO 2: Create Schedule object with cron_expression and timezone\n        # TODO 3: Generate unique schedule_id\n        # TODO 4: Calculate initial next_run_at using calculate_next_run()\n        # TODO 5: Store schedule in Redis hash\n        # TODO 6: Add to timeline sorted set with score = next_run_at timestamp\n        # TODO 7: Return schedule_id\n        \n        # HINT: Use self._calculate_next_run() for initial calculation\n        # HINT: Consider using current time in schedule's timezone as reference\n    \n    def _calculate_next_run(self, schedule: Schedule,\n                           reference_time: Optional[datetime.datetime] = None) -> datetime.datetime:\n        \"\"\"Calculate next run time for a schedule.\n        \n        Args:\n            schedule: Schedule definition\n            reference_time: Time to calculate from (defaults to now)\n        \n        Returns:\n            Next run time in UTC\n        \n        Raises:\n            ValueError: If schedule has neither cron nor run_at\n        \"\"\"\n        # TODO 1: If schedule has run_at (one-time), return run_at (already in UTC)\n        # TODO 2: If schedule has cron_expression:\n        #   a. Determine reference time (use last_enqueued_at or now if None)\n        #   b. Convert reference time to schedule's timezone\n        #   c. Use croniter to get next matching time in schedule's timezone\n        #   d. Handle edge cases: maximum iterations, invalid dates\n        #   e. Convert result back to UTC\n        # TODO 3: Return UTC datetime\n        \n        # HINT: For cron, add 1 minute to reference time to avoid re-enqueuing immediately\n        # HINT: Use croniter.croniter.get_next(datetime) for next run calculation\n        # HINT: Wrap in try/except for croniter errors\n    \n    def _enqueue_scheduled_job(self, schedule: Schedule,\n                              scheduled_for: datetime.datetime) -> Tuple[bool, Optional[str]]:\n        \"\"\"Enqueue a job from a schedule.\n        \n        Args:\n            schedule: Schedule definition\n            scheduled_for: When this job was scheduled to run\n        \n        Returns:\n            Tuple of (success, job_id or error message)\n        \"\"\"\n        # TODO 1: Check uniqueness constraint if schedule.unique_key is set\n        # TODO 2: If unique_key exists and within window, return (False, \"skipped\")\n        # TODO 3: Create Job instance from schedule\n        # TODO 4: Enqueue job using queue_manager.enqueue_job()\n        # TODO 5: If successful:\n        #   a. Record ScheduledJob instance\n        #   b. Update schedule.last_enqueued_at\n        #   c. Set uniqueness key with TTL if needed\n        # TODO 6: Return (success, job_id or error)\n        \n        # HINT: For uniqueness check: redis.set(key, 1, nx=True, ex=window)\n        # HINT: Use pipeline for atomic updates\n    \n    def _poll_due_schedules(self) -> int:\n        \"\"\"Poll for and process due schedules.\n        \n        Returns:\n            Number of jobs successfully enqueued\n        \"\"\"\n        # TODO 1: Get current UTC time\n        # TODO 2: Calculate cutoff = now + polling_interval (for catch-up)\n        # TODO 3: Use ZRANGEBYSCORE to get schedule_ids with score <= cutoff\n        # TODO 4: For each schedule_id (up to batch_size):\n        #   a. Fetch schedule from Redis hash\n        #   b. Skip if schedule is None or not enabled\n        #   c. Process schedule using _process_schedule()\n        # TODO 5: Return count of successfully enqueued jobs\n        \n        # HINT: Use redis.pipeline() for batch operations\n        # HINT: Handle missing schedules (remove from timeline)\n    \n    def _process_schedule(self, schedule: Schedule) -> bool:\n        \"\"\"Process a single due schedule.\n        \n        Args:\n            schedule: Schedule to process\n        \n        Returns:\n            True if job was enqueued, False otherwise\n        \"\"\"\n        # TODO 1: Determine scheduled_for time (from timeline score or schedule.next_run_at)\n        # TODO 2: Attempt to enqueue job using _enqueue_scheduled_job()\n        # TODO 3: If successful or skipped due to uniqueness:\n        #   a. Calculate next run time for recurring jobs\n        #   b. Update schedule.next_run_at\n        #   c. Update timeline sorted set with new score\n        # TODO 4: If one-time job (no cron), remove from storage\n        # TODO 5: Return True if job was enqueued\n        \n        # HINT: For next run calculation, use schedule's last_enqueued_at or now as reference\n        # HINT: Use _calculate_next_run() for recurring jobs\n    \n    def start(self) -> None:\n        \"\"\"Start the scheduler polling loop.\"\"\"\n        # TODO 1: Set _running = True\n        # TODO 2: Log scheduler startup with configuration\n        # TODO 3: While _running:\n        #   a. Record loop start time\n        #   b. Call _poll_due_schedules()\n        #   c. Calculate sleep time = polling_interval - processing_time\n        #   d. Sleep max(0, sleep_time)\n        # TODO 4: Log scheduler shutdown\n        \n        # HINT: Use time.monotonic() for precise timing\n        # HINT: Catch KeyboardInterrupt for graceful shutdown\n    \n    def stop(self) -> None:\n        \"\"\"Stop the scheduler polling loop.\"\"\"\n        # TODO 1: Set _running = False\n        # TODO 2: Log scheduler stopping\n    \n    def calculate_next_runs_for_all_schedules(self) -> int:\n        \"\"\"Recalculate next_run_at for all schedules (e.g., after timezone change).\n        \n        Returns:\n            Number of schedules updated\n        \"\"\"\n        # TODO 1: Get all schedule_ids from timeline or scan\n        # TODO 2: For each schedule:\n        #   a. Fetch schedule\n        #   b. Calculate new next_run_at\n        #   c. Update timeline score\n        # TODO 3: Return count updated\n        \n        # HINT: Useful for maintenance after DST rules change\n```\n\n**E. Language-Specific Hints:**\n\n- **Cron Parsing**: Use `croniter` library: `pip install croniter`. It handles complex cron expressions including ranges, steps, and month/day names.\n- **Timezone Handling**: Python 3.9+ has `zoneinfo` in standard library. For older Python, use `backports.zoneinfo` or `pytz`. Avoid `datetime.replace(tzinfo=)` for pytz timezones - use `localize()` instead.\n- **UTC Timestamps**: Store all timestamps as UNIX timestamps (float) or ISO 8601 strings ending with \"Z\" in Redis. Use `datetime.datetime.now(datetime.timezone.utc)` for current UTC time.\n- **Redis Sorted Sets**: Use `ZADD`, `ZRANGEBYSCORE`, and `ZREM` for timeline management. Scores should be UNIX timestamps (seconds since epoch).\n- **Atomic Operations**: Use Redis pipelines or Lua scripts for operations requiring multiple commands (e.g., check uniqueness, enqueue job, update schedule).\n- **ULID Generation**: Use `ulid-py` library for chronological job IDs: `pip install ulid-py`. ULIDs are sortable by creation time unlike UUIDs.\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the scheduler:\n\n1. **Test Basic Scheduling**:\n   ```bash\n   # Start Redis\n   redis-server\n   \n   # In Python shell\n   python -c \"\n   from scheduler.scheduler import Scheduler\n   from core.job import Job\n   from core.queue_manager import QueueManager\n   from core.redis_client import RedisClient\n   import datetime\n   \n   redis = RedisClient.get_instance('redis://localhost:6379')\n   queue_manager = QueueManager(SystemConfig.from_env())\n   scheduler = Scheduler(redis, queue_manager, polling_interval_seconds=10)\n   \n   # Schedule a job for 30 seconds from now\n   job = Job(job_type='test_task', args=['hello'])\n   run_at = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(seconds=30)\n   schedule_id = scheduler.schedule_delayed_job(job, run_at)\n   print(f'Scheduled job with ID: {schedule_id}')\n   \n   # Start scheduler in background thread\n   import threading\n   thread = threading.Thread(target=scheduler.start)\n   thread.start()\n   \n   # Wait 45 seconds, then check if job was enqueued\n   import time\n   time.sleep(45)\n   queue_len = queue_manager.get_queue_length('default')\n   print(f'Jobs in queue: {queue_len}')  # Should be 1\n   \n   scheduler.stop()\n   thread.join()\n   \"\n   ```\n\n2. **Test Recurring Jobs**:\n   ```bash\n   # Schedule a job to run every minute\n   python -c \"\n   from scheduler.scheduler import Scheduler\n   from core.job import Job\n   # ... setup as above\n   \n   job = Job(job_type='heartbeat', args=[])\n   schedule_id = scheduler.schedule_recurring_job(job, '* * * * *', 'UTC')\n   print(f'Recurring schedule ID: {schedule_id}')\n   \n   # Start scheduler, wait 2.5 minutes\n   # Should see 2-3 jobs enqueued (depending on exact timing)\n   \"\n\n**G. Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Scheduled jobs never run** | Timeline sorted set empty or wrong scores | Check `ZRANGE schedules:timeline 0 -1 WITHSCORES` in Redis | Ensure `next_run_at` is in UTC when added to sorted set |\n| **Jobs run at wrong time** | Timezone conversion error | Compare schedule's `timezone` field with actual calculation | Use `ensure_timezone()` and `to_utc()` consistently |\n| **Duplicate jobs enqueued** | Uniqueness constraint not working | Check Redis for `schedule:unique:*` keys with `TTL` | Use `SET NX EX` atomically, not separate `GET` + `SET` |\n| **Scheduler uses 100% CPU** | Polling loop with no sleep | Add logging to measure loop iteration time | Ensure `time.sleep()` accounts for processing time |\n| **Missed jobs after restart** | Timeline scores in past not processed | Check if `ZRANGEBYSCORE` uses `now() + interval` not just `now()` | Always poll with range `0` to `now + interval` |\n| **Cron job runs twice in same minute** | Uniqueness window too short | Check `unique_window_seconds` vs cron frequency | Increase window to cover entire polling interval |\n\n```\n\n---\n\n\n## Component Design: Monitoring & Dashboard\n> **Milestone(s):** Milestone 5: Monitoring & Dashboard. This section provides the complete design for real-time monitoring, web dashboard, and alerting systems that provide observability into job processing, worker health, and system performance. This corresponds to the final milestone, building upon all previous components to create comprehensive operational visibility.\n\n### Mental Model: Air Traffic Control Dashboard\n\nThink of the monitoring system as an **air traffic control dashboard** for your distributed job processing system. Air traffic controllers monitor:\n\n1. **Active Flights (Jobs)**: Where they are (which worker), their status (taking off, cruising, landing), and estimated completion time\n2. **Runway Capacity (Queues)**: How many flights are waiting to take off, and which runways are most congested\n3. **Weather Conditions (System Health)**: Any storms (errors) approaching, visibility issues (latency), or equipment problems\n4. **Historical Patterns**: Which routes have the most turbulence (error-prone jobs), peak traffic hours, and operational bottlenecks\n\nJust as air traffic controllers need real-time visibility without overwhelming detail, our monitoring system must:\n- **Aggregate** high-level metrics while allowing drill-down to specific incidents\n- **Alert** on anomalies without generating noise\n- **Preserve context** so when something goes wrong, you can trace the entire journey of a job\n- **Scale** to handle thousands of jobs without impacting the performance of the actual job processing\n\nThis mental model emphasizes that monitoring is not just passive observation but active system management—knowing when to re-route traffic (reprioritize queues), allocate more resources (scale workers), or investigate incidents (debug failing jobs).\n\n### Monitoring API and Dashboard Interface\n\nThe monitoring system consists of two primary interfaces: a **programmatic API** for automation and integration, and a **web dashboard** for human operators. Both share the same underlying data model but present it differently.\n\n#### Core Monitoring Data Types\n\n| Type Name | Fields | Description |\n|-----------|--------|-------------|\n| `MetricPoint` | `timestamp datetime`, `name str`, `value float`, `labels Dict[str, str]` | Single timestamped measurement with key-value labels for dimensionality |\n| `JobSummary` | `job_id str`, `job_type str`, `queue str`, `status JobStatus`, `enqueued_at datetime`, `started_at Optional[datetime]`, `completed_at Optional[datetime]`, `duration_seconds Optional[float]`, `attempts int`, `error_count int` | Aggregated job execution data for dashboard display |\n| `WorkerStatus` | `worker_id str`, `hostname str`, `pid int`, `state str`, `current_job Optional[JobSummary]`, `queues List[str]`, `last_heartbeat datetime`, `processed_count int`, `failed_count int`, `uptime_seconds float` | Current status of a worker process |\n| `QueueMetrics` | `queue_name str`, `pending_count int`, `active_count int`, `completed_count int`, `failed_count int`, `retry_scheduled_count int`, `dead_letter_count int`, `enqueue_rate float`, `process_rate float`, `error_rate float`, `avg_process_time float`, `oldest_job_age Optional[float]` | Comprehensive metrics for a single queue |\n| `SystemAlert` | `alert_id str`, `severity str` (CRITICAL, WARNING, INFO), `source str`, `message str`, `condition str`, `triggered_at datetime`, `acknowledged_at Optional[datetime]`, `resolved_at Optional[datetime]`, `metadata Dict[str, Any]` | Alert generated by monitoring rules |\n| `DashboardConfig` | `refresh_interval int`, `time_range str` (1h, 24h, 7d), `queue_filters List[str]`, `worker_filters List[str]`, `job_type_filters List[str]`, `metric_aggregation str` (avg, sum, p95, p99) | User dashboard configuration |\n\n![Monitoring Data Collection Architecture](./diagrams/monitoring-architecture.svg)\n\n#### Monitoring API Endpoints\n\nThe programmatic API provides RESTful endpoints for external monitoring systems (Prometheus, Datadog) and automation scripts:\n\n| Method Name | HTTP Endpoint | Parameters | Returns | Description |\n|-------------|---------------|------------|---------|-------------|\n| `get_system_metrics()` | `GET /api/metrics/system` | `time_range str = \"1h\"`, `aggregation str = \"avg\"` | `Dict[str, List[MetricPoint]]` | System-wide metrics: memory usage, Redis connections, active workers |\n| `get_queue_metrics()` | `GET /api/metrics/queues` | `queue_names Optional[List[str]]`, `time_range str = \"1h\"` | `List[QueueMetrics]` | Current metrics for specified queues (all if none specified) |\n| `get_worker_status()` | `GET /api/workers` | `active_only bool = True` | `List[WorkerStatus]` | Status of all workers (including inactive if `active_only=False`) |\n| `get_job_history()` | `GET /api/jobs` | `job_id Optional[str]`, `queue Optional[str]`, `status Optional[JobStatus]`, `job_type Optional[str]`, `limit int = 100`, `offset int = 0`, `time_range Optional[str]` | `Dict[str, Any]` (with `jobs List[JobSummary]`, `total int`) | Search jobs with filtering and pagination |\n| `get_dead_letter_jobs()` | `GET /api/dead-letter` | `queue Optional[str]`, `job_type Optional[str]`, `limit int = 50` | `List[Job]` | Jobs in dead letter queue for manual intervention |\n| `retry_dead_letter_job()` | `POST /api/dead-letter/{job_id}/retry` | `job_id str` (path), `queue Optional[str]` | `Dict[str, Any]` (with `success bool`, `new_job_id Optional[str]`) | Retry a specific dead letter job in its original queue |\n| `delete_dead_letter_job()` | `DELETE /api/dead-letter/{job_id}` | `job_id str` (path) | `Dict[str, Any]` (with `success bool`) | Permanently remove a job from dead letter queue |\n| `get_alerts()` | `GET /api/alerts` | `severity Optional[str]`, `acknowledged bool = False`, `limit int = 50` | `List[SystemAlert]` | Get current active alerts |\n| `acknowledge_alert()` | `POST /api/alerts/{alert_id}/acknowledge` | `alert_id str` (path) | `Dict[str, Any]` (with `success bool`) | Mark an alert as acknowledged by an operator |\n| `create_alert_rule()` | `POST /api/alert-rules` | `rule_config Dict[str, Any]` | `Dict[str, Any]` (with `rule_id str`) | Create a new alerting rule (admin only) |\n| `get_metrics_stream()` | `GET /api/metrics/stream` | - | `StreamingResponse` (Server-Sent Events) | Real-time stream of metric updates for dashboard |\n\n#### Web Dashboard Interface\n\nThe web dashboard provides human-readable visualization through these key views:\n\n| View Name | Purpose | Key Visualizations |\n|-----------|---------|-------------------|\n| **System Overview** | High-level system health | - Queue depth gauges<br>- Worker status grid<br>- Error rate sparklines<br>- System metrics time-series |\n| **Queue Detail** | Drill-down into specific queue | - Job status distribution pie chart<br>- Processing rate over time<br>- Top error-producing job types<br>- Oldest pending jobs list |\n| **Worker Dashboard** | Worker process monitoring | - Worker state heatmap<br>- CPU/memory usage per worker<br>- Current job execution time<br>- Worker failure history |\n| **Job Explorer** | Search and inspect jobs | - Filterable job table with pagination<br>- Job execution timeline<br>- Error stack trace viewer<br>- Manual retry controls |\n| **Dead Letter Queue** | Manage permanently failed jobs | - DLQ job table with error details<br>- Bulk retry/delete operations<br>- Failure pattern analysis |\n| **Alert Center** | Alert management | - Active alert dashboard<br>- Alert history with filters<br>- Alert rule configuration |\n| **System Configuration** | Runtime configuration view | - Queue configuration table<br>- Worker pool settings<br>- Redis connection status |\n\n> **Design Insight:** The dashboard prioritizes **operational clarity** over completeness. Each view answers specific questions operators have: \"Why is my queue backing up?\", \"Which worker is stuck?\", \"What's failing and why?\" The API supports automation while the dashboard supports human intuition.\n\n### Metrics Collection and Aggregation\n\nMetrics flow through a three-stage pipeline: **collection → aggregation → storage**. This design ensures monitoring doesn't block job processing while providing accurate, real-time visibility.\n\n#### Collection Strategy\n\nMetrics are collected at these key points in the system:\n\n| Collection Point | What's Collected | How It's Collected |\n|------------------|------------------|-------------------|\n| **Job Enqueue** | Queue length increment, job type counter, payload size | `QueueManager.enqueue_job()` emits Redis Stream event |\n| **Job Start** | Job transition to ACTIVE, worker assignment, queue length decrement | `Worker` emits event when beginning job execution |\n| **Job Completion** | Success/failure status, execution duration, attempt count | `Worker` wraps job execution with timing and status tracking |\n| **Job Failure** | Error details, retry scheduling, backoff delay | `RetryManager.handle_job_failure()` records error context |\n| **Worker Heartbeat** | Worker liveness, current job, processed counts | `WorkerHeartbeat` periodically updates Redis hash |\n| **Scheduler Activity** | Scheduled job enqueue, missed schedules, cron evaluation | `Scheduler._enqueue_scheduled_job()` emits events |\n| **System Resources** | Redis memory usage, connection count, CPU via OS | External agent collects via Redis INFO and psutil |\n\n#### Aggregation Algorithm\n\nRaw events are aggregated into 10-second buckets to reduce storage and query load:\n\n1. **Event Emission**: Components emit events to Redis Stream `monitoring:events` with JSON payload:\n   ```json\n   {\n     \"timestamp\": \"2024-01-15T10:30:45.123Z\",\n     \"type\": \"job_completed\",\n     \"queue\": \"emails\",\n     \"job_type\": \"send_welcome\",\n     \"worker_id\": \"worker-abc123\",\n     \"duration_ms\": 125.5,\n     \"success\": true\n   }\n   ```\n\n2. **Background Aggregator**: A lightweight aggregator process runs every 10 seconds:\n   ```\n   1. Reads all events from `monitoring:events` stream since last run\n   2. Groups events by (metric_name, queue, job_type, worker_id) tuple\n   3. Calculates aggregates for each group:\n      - Count → sum of occurrences\n      - Duration → average, p95, p99, min, max\n      - Error rate → failed_count / total_count\n   4. Stores aggregated metrics to Redis Sorted Set with timestamp score\n   5. Truncates old raw events beyond retention window\n   ```\n\n3. **Metric Retention**:\n   - Raw events: 1 hour retention (for debugging)\n   - 10-second aggregates: 7 days retention\n   - 1-minute aggregates (rolled up from 10-second): 30 days retention\n   - 1-hour aggregates: 90 days retention\n\n#### Key Performance Indicators (KPIs)\n\nThe system tracks these essential KPIs for operational health:\n\n| KPI Name | Calculation | Alert Threshold | Purpose |\n|----------|-------------|-----------------|---------|\n| **Queue Depth** | `LLEN queue:{name}` | > 1000 pending jobs | Detect processing backlog |\n| **Processing Rate** | Jobs completed per second (10s window) | < 10% of expected rate | Detect worker slowdown |\n| **Error Rate** | Failed jobs / total jobs (5m window) | > 5% | Detect systemic failures |\n| **Worker Health** | Workers with heartbeat < 60s ago / total workers | < 80% healthy | Detect worker process failures |\n| **Job Duration P99** | 99th percentile job duration (by job_type) | > 2x historical baseline | Detect performance degradation |\n| **Redis Memory Usage** | `used_memory` / `maxmemory` | > 85% | Prevent Redis OOM |\n| **Schedule Miss Rate** | Missed schedules / total schedules (1h window) | > 1% | Detect scheduler issues |\n\n> **Design Insight:** Monitoring should be **cheap enough to always be on**. By aggregating metrics and using Redis' efficient data structures, we avoid the common pitfall where monitoring itself becomes a performance bottleneck. The 10-second aggregation strikes a balance between real-time responsiveness and system load.\n\n### ADR: Real-time Update Strategy\n\n> **Decision: Server-Sent Events (SSE) over WebSockets for Real-time Dashboard Updates**\n> \n> - **Context**: The dashboard needs near real-time updates (queue depths, worker status, job completion) without requiring page refreshes. We need a lightweight, scalable solution that integrates well with our existing HTTP API and doesn't add significant complexity.\n> \n> - **Options Considered**:\n>   1. **Polling (HTTP GET every few seconds)**: Simple but inefficient, creates constant load regardless of change frequency\n>   2. **WebSockets**: Full-duplex, low-latency, but requires connection management and protocol handling\n>   3. **Server-Sent Events (SSE)**: HTTP-based, unidirectional from server to client, automatic reconnection, simple API\n>   4. **Redis Pub/Sub with bridge**: Push updates via Redis publish, bridge to HTTP layer\n> \n> - **Decision**: Use **Server-Sent Events (SSE)** for real-time dashboard updates.\n> \n> - **Rationale**:\n>   - **Simplicity**: SSE works over standard HTTP, requires no special protocol handling on server or client\n>   - **Automatic reconnection**: Built-in reconnection with last-event-id tracking\n>   - **Efficient**: Server pushes updates only when metrics change, no constant polling\n>   - **Compatibility**: Works with existing authentication/authorization middleware\n>   - **Resource friendly**: Single HTTP connection per dashboard client vs. bidirectional WebSocket overhead\n>   - **Graceful degradation**: If SSE connection drops, dashboard falls back to periodic polling\n> \n> - **Consequences**:\n>   - ✅ Dashboard gets sub-second updates without constant polling\n>   - ✅ Simple implementation using standard HTTP libraries\n>   - ✅ Automatic reconnection handles network issues gracefully\n>   - ✅ Works behind load balancers and proxies with standard HTTP\n>   - ❌ Unidirectional only (server → client), but dashboard doesn't need client→server updates\n>   - ❌ Browser limits of 6 concurrent HTTP connections per domain (mitigated by using single SSE stream for all metrics)\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| **Polling** | - Extremely simple<br>- Works everywhere<br>- No connection state | - High latency (poll interval)<br>- Constant load even when idle<br>- Wasted bandwidth | Creates unnecessary load; poor real-time experience |\n| **WebSockets** | - Full duplex<br>- Lowest latency<br>- Rich protocol options | - Complex connection management<br>- Requires protocol upgrade<br>- Overkill for unidirectional updates | Complexity outweighs benefits for our use case |\n| **Server-Sent Events** | - HTTP-based, simple<br>- Automatic reconnection<br>- Efficient push model | - Unidirectional only<br>- Less control than WebSockets | **CHOSEN**: Perfect fit for dashboard metrics streaming |\n| **Redis Pub/Sub Bridge** | - Leverages existing Redis<br>- Decouples producers from consumers | - Requires bridge process<br>- Additional moving parts | Adds complexity without significant advantage |\n\n### ADR: Job History Storage Strategy\n\n> **Decision: Two-tiered storage with Redis for recent history and optional archival to external storage**\n> \n> - **Context**: Job execution history is valuable for debugging and analytics but can grow unbounded. We need to store enough history for operational debugging (hours/days) while providing an optional path for longer-term retention without overwhelming Redis memory.\n> \n> - **Options Considered**:\n>   1. **Redis-only with TTL**: Store all job history in Redis with expiration (e.g., 7 days TTL)\n>   2. **External database primary**: Write all job history directly to PostgreSQL/MySQL\n>   3. **Two-tiered with Redis cache**: Recent history in Redis (fast access), older history in external DB\n>   4. **Hybrid with configurable backend**: Plugable storage backends (Redis, SQL, filesystem)\n> \n> - **Decision**: Implement **two-tiered storage** with Redis for recent history (configurable retention) and optional archival to external database for long-term retention.\n> \n> - **Rationale**:\n>   - **Performance**: Redis provides sub-millisecond access to recent job history for dashboard\n>   - **Operational simplicity**: Most debugging needs recent history (last few hours/days)\n>   - **Memory control**: TTL prevents unbounded Redis growth\n>   - **Flexibility**: Optional external archival for compliance or analytics without impacting core system\n>   - **Progressive enhancement**: Start with Redis-only, add archival later if needed (YAGNI principle)\n> \n> - **Consequences**:\n>   - ✅ Fast access to recent job history for dashboard and debugging\n>   - ✅ Configurable Redis memory usage via TTL settings\n>   - ✅ Optional external archival without core system dependency\n>   - ✅ Simple initial implementation (Redis only)\n>   - ❌ Historical queries beyond TTL require external archival setup\n>   - ❌ Two data stores to manage if archival enabled\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| **Redis-only with TTL** | - Maximum performance<br>- Simple architecture<br>- No external dependencies | - History limited by TTL<br>- Redis memory pressure<br>- No long-term analytics | **CHOSEN** as base: Simple and fast for operational needs |\n| **External DB primary** | - Unlimited history<br>- Rich query capabilities<br>- Persistent storage | - Slower access<br>- Additional dependency<br>- Operational complexity | Overkill for core operational dashboard needs |\n| **Two-tiered with cache** | - Best of both worlds<br>- Fast recent access + long-term storage | - Complexity of synchronization<br>- Cache invalidation challenges | **CHOSEN** as extensible: Can add archival later |\n| **Plugable backends** | - Maximum flexibility<br>- Choose storage per deployment | - Highest complexity<br>- Interface abstraction overhead | Premature optimization; start simple, extend if needed |\n\n#### Job History Storage Implementation\n\n```plaintext\nRedis Data Structures for Job History:\n\n1. `job:history:{job_id}` → Hash with complete job execution record\n   - Fields: job_data (JSON), status, started_at, completed_at, duration_ms, attempts, errors (JSON)\n   - TTL: 7 days (configurable)\n\n2. `job:history:index:by_queue:{queue_name}` → Sorted Set\n   - Score: completed_at timestamp (or enqueued_at if not completed)\n   - Member: job_id\n   - Used for: \"Show recent jobs in queue X\"\n   - TTL: 7 days (via periodic cleanup of expired jobs)\n\n3. `job:history:index:by_type:{job_type}` → Sorted Set\n   - Score: completed_at timestamp\n   - Member: job_id\n   - Used for: \"Show recent jobs of type Y\"\n   - TTL: 7 days\n\n4. `job:history:index:by_status:{status}` → Sorted Set\n   - Score: timestamp of status change\n   - Member: job_id\n   - Used for: \"Show recent failed jobs\"\n   - TTL: 7 days for COMPLETED, 30 days for FAILED/DEAD_LETTER\n\nArchival Process (optional):\n1. Background process scans jobs nearing TTL expiration\n2. Serializes job history to JSON\n3. Pushes to configurable sink (PostgreSQL, S3, Elasticsearch)\n4. Removes from Redis after successful archival\n```\n\n### Common Pitfalls in Monitoring Implementation\n\n⚠️ **Pitfall 1: Monitoring blocks job processing**\n- **What happens**: Adding metric collection directly in the hot path of job execution (e.g., synchronous writes to Redis during job start/complete)\n- **Why it's wrong**: Adds latency to job processing; under high load, monitoring can become the bottleneck\n- **How to fix**: Use asynchronous event emission (Redis Streams) or batch writes. Workers should emit events to a local buffer that's flushed periodically.\n\n⚠️ **Pitfall 2: Storing unlimited job history in Redis**\n- **What happens**: Every job execution creates permanent records in Redis, causing memory to grow without bound\n- **Why it's wrong**: Eventually Redis hits memory limits, causing eviction or crashes; performance degrades as datasets grow\n- **How to fix**: Implement TTL on all job history keys (7-30 days). For longer retention, add optional archival to external storage.\n\n⚠️ **Pitfall 3: Dashboard queries blocking worker operations**\n- **What happens**: Complex dashboard queries (e.g., \"show all jobs from last month\") run expensive Redis operations that block worker access\n- **Why it's wrong**: Monitoring should be observational only; it should never impact system performance\n- **How to fix**: Use read replicas for dashboard queries, implement query timeouts, and denormalize data for common queries.\n\n⚠️ **Pitfall 4: Alert fatigue from noisy thresholds**\n- **What happens**: Setting alert thresholds too sensitively (e.g., alert on every single job failure) creates alert spam that operators ignore\n- **Why it's wrong**: Important alerts get lost in the noise; operators develop \"alert blindness\"\n- **How to fix**: Use rate-based alerting (error rate > 5% over 5 minutes), implement alert deduplication, and tier alerts by severity.\n\n⚠️ **Pitfall 5: Exposing sensitive job data in dashboard**\n- **What happens**: Job arguments containing PII, passwords, or tokens are displayed in dashboard without sanitization\n- **Why it's wrong**: Security breach; sensitive data exposed to anyone with dashboard access\n- **How to fix**: Implement job argument sanitization filters, mask sensitive fields (e.g., `password: ***`), and control dashboard access with authentication.\n\n⚠️ **Pitfall 6: Hardcoded metric collection intervals**\n- **What happens**: Metric collection runs at fixed intervals (e.g., every 10 seconds) regardless of system load\n- **Why it's wrong**: Under low load, this wastes resources; under high load, it may not capture enough detail\n- **How to fix**: Implement adaptive sampling: increase frequency during high activity, decrease during quiet periods.\n\n### Implementation Guidance for Monitoring\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Web Framework** | Flask + Flask-SSE | FastAPI with async SSE support |\n| **Frontend Dashboard** | Vanilla JavaScript + Chart.js | React/Vue.js with D3.js for custom visualizations |\n| **Real-time Updates** | Server-Sent Events (SSE) | WebSockets with Socket.IO fallback |\n| **Metrics Storage** | Redis TimeSeries module | Redis + PostgreSQL for long-term archival |\n| **Alerting Engine** | Custom rule evaluator | Integration with Prometheus Alertmanager |\n| **Charting Library** | Chart.js (simple, good defaults) | Apache ECharts (more customizable) |\n\n#### B. Recommended File/Module Structure\n\n```\nbackground-job-processor/\n├── src/\n│   ├── job_processor/\n│   │   ├── __init__.py\n│   │   ├── core/                   # Milestone 1-2\n│   │   │   ├── job.py\n│   │   │   ├── queue_manager.py\n│   │   │   └── worker.py\n│   │   ├── retry/                  # Milestone 3\n│   │   │   ├── retry_manager.py\n│   │   │   └── backoff_calculator.py\n│   │   ├── scheduler/              # Milestone 4\n│   │   │   ├── scheduler.py\n│   │   │   └── cron_parser.py\n│   │   ├── monitoring/             # Milestone 5 (THIS SECTION)\n│   │   │   ├── __init__.py\n│   │   │   ├── metrics.py          # Metric collection and aggregation\n│   │   │   ├── events.py           # Event emission and handling\n│   │   │   ├── dashboard_api.py    # REST API endpoints\n│   │   │   ├── dashboard_ui.py     # Web dashboard (HTML/JS)\n│   │   │   ├── alerting.py         # Alert rule evaluation\n│   │   │   ├── history_store.py    # Job history storage\n│   │   │   └── sse_stream.py       # Server-Sent Events streaming\n│   │   └── utils/\n│   │       ├── redis_client.py\n│   │       └── config.py\n├── tests/\n│   └── monitoring/\n│       ├── test_metrics.py\n│       ├── test_dashboard_api.py\n│       └── test_alerting.py\n├── static/                         # Dashboard static assets\n│   ├── css/\n│   ├── js/\n│   └── charts/\n├── templates/                      # Dashboard HTML templates\n│   └── dashboard.html\n└── requirements.txt\n```\n\n#### C. Infrastructure Starter Code\n\n**Complete metrics aggregation service (`monitoring/aggregator.py`):**\n\n```python\nimport json\nimport time\nimport threading\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any\nfrom collections import defaultdict\nimport redis\n\nclass MetricsAggregator:\n    \"\"\"Background service that aggregates raw monitoring events into metrics.\"\"\"\n    \n    def __init__(self, redis_client: redis.Redis, interval_seconds: int = 10):\n        self.redis = redis_client\n        self.interval = interval_seconds\n        self._running = False\n        self._thread = None\n        \n    def start(self):\n        \"\"\"Start the aggregation thread.\"\"\"\n        self._running = True\n        self._thread = threading.Thread(target=self._aggregation_loop, daemon=True)\n        self._thread.start()\n        \n    def stop(self):\n        \"\"\"Stop the aggregation thread.\"\"\"\n        self._running = False\n        if self._thread:\n            self._thread.join(timeout=self.interval * 2)\n            \n    def _aggregation_loop(self):\n        \"\"\"Main aggregation loop.\"\"\"\n        while self._running:\n            try:\n                self._run_aggregation()\n            except Exception as e:\n                # Log but continue - monitoring should not crash the system\n                print(f\"Aggregation error: {e}\")\n            time.sleep(self.interval)\n            \n    def _run_aggregation(self):\n        \"\"\"Execute a single aggregation cycle.\"\"\"\n        # Get all events since last run\n        last_run_key = \"monitoring:aggregator:last_run\"\n        last_run = self.redis.get(last_run_key)\n        start_id = last_run.decode() if last_run else \"0-0\"\n        \n        # Read events from stream\n        stream_key = \"monitoring:events\"\n        events = self.redis.xrange(stream_key, min=start_id, max=\"+\", count=1000)\n        \n        if not events:\n            return\n            \n        # Process events by type and labels\n        metrics_by_key = defaultdict(list)\n        for event_id, event_data in events:\n            event_type = event_data.get(b\"type\", b\"unknown\").decode()\n            labels_json = event_data.get(b\"labels\", b\"{}\")\n            labels = json.loads(labels_json.decode())\n            value = float(event_data.get(b\"value\", b\"0\"))\n            timestamp = event_data.get(b\"timestamp\", b\"\").decode()\n            \n            # Create metric key from labels\n            label_parts = [f\"{k}={v}\" for k, v in sorted(labels.items())]\n            metric_key = f\"{event_type}:{':'.join(label_parts)}\"\n            metrics_by_key[metric_key].append((timestamp, value))\n            \n        # Calculate aggregates\n        bucket_time = datetime.utcnow().replace(second=0, microsecond=0)\n        bucket_timestamp = int(bucket_time.timestamp())\n        \n        for metric_key, values in metrics_by_key.items():\n            if not values:\n                continue\n                \n            # Calculate aggregates\n            values_only = [v for _, v in values]\n            count = len(values_only)\n            avg_val = sum(values_only) / count if count > 0 else 0\n            sorted_vals = sorted(values_only)\n            p95 = sorted_vals[int(count * 0.95)] if count > 0 else 0\n            \n            # Store aggregated metric\n            metric_hash = {\n                \"count\": str(count),\n                \"avg\": str(avg_val),\n                \"p95\": str(p95),\n                \"min\": str(min(values_only)),\n                \"max\": str(max(values_only))\n            }\n            \n            agg_key = f\"monitoring:metrics:{metric_key}:{bucket_timestamp}\"\n            self.redis.hmset(agg_key, metric_hash)\n            self.redis.expire(agg_key, 604800)  # 7 days TTL\n            \n        # Update last run pointer\n        last_event_id = events[-1][0] if events else start_id\n        self.redis.set(last_run_key, last_event_id)\n        \n        # Clean up old raw events (keep last hour only)\n        one_hour_ago = int((datetime.utcnow() - timedelta(hours=1)).timestamp() * 1000)\n        self.redis.xtrim(stream_key, minid=one_hour_ago)\n```\n\n#### D. Core Logic Skeleton Code\n\n**Dashboard API endpoint for real-time metrics stream (`monitoring/dashboard_api.py`):**\n\n```python\nimport json\nimport time\nfrom flask import Flask, Response, request\nimport redis\n\napp = Flask(__name__)\n\n@app.route('/api/metrics/stream')\ndef metrics_stream():\n    \"\"\"Server-Sent Events stream for real-time dashboard updates.\"\"\"\n    \n    def generate_events():\n        # TODO 1: Get Redis client from application context\n        redis_client = None  # Implement connection pooling\n        \n        # TODO 2: Get last event ID from request header for reconnection\n        last_id = request.headers.get('Last-Event-ID', '0-0')\n        \n        # TODO 3: Subscribe to monitoring events stream\n        stream_key = 'monitoring:events'\n        \n        while True:\n            try:\n                # TODO 4: Read new events since last_id (blocking with timeout)\n                # Use redis.xread with count=10, block=5000ms\n                events = []  # Implement stream reading\n                \n                for stream, event_list in events:\n                    for event_id, event_data in event_list:\n                        # TODO 5: Filter events based on dashboard filters\n                        # Get filters from request query params\n                        queue_filter = request.args.get('queue')\n                        job_type_filter = request.args.get('job_type')\n                        \n                        # TODO 6: Format event for SSE (event: type, data: JSON)\n                        event_type = event_data.get(b'type', b'metric').decode()\n                        event_data_json = json.dumps({\n                            'id': event_id,\n                            'type': event_type,\n                            'data': {k.decode(): v.decode() for k, v in event_data.items()},\n                            'timestamp': time.time()\n                        })\n                        \n                        # TODO 7: Yield SSE formatted data\n                        yield f\"event: {event_type}\\ndata: {event_data_json}\\n\\n\"\n                        \n                        # Update last_id for reconnection\n                        last_id = event_id\n                \n                # TODO 8: Send heartbeat comment every 30 seconds to keep connection alive\n                # yield \": heartbeat\\n\\n\"\n                \n            except Exception as e:\n                # TODO 9: Log error and yield error event\n                error_data = json.dumps({'error': str(e)})\n                yield f\"event: error\\ndata: {error_data}\\n\\n\"\n                break\n    \n    # TODO 10: Return SSE response with appropriate headers\n    return Response(\n        generate_events(),\n        mimetype='text/event-stream',\n        headers={\n            'Cache-Control': 'no-cache',\n            'Connection': 'keep-alive',\n            'X-Accel-Buffering': 'no'  # Disable nginx buffering\n        }\n    )\n\n# TODO 11: Implement other API endpoints from the API table\n# - GET /api/metrics/system\n# - GET /api/metrics/queues\n# - GET /api/workers\n# - GET /api/jobs\n# - GET /api/dead-letter\n# - POST /api/dead-letter/{job_id}/retry\n# - DELETE /api/dead-letter/{job_id}\n# - GET /api/alerts\n# - POST /api/alerts/{alert_id}/acknowledge\n# - POST /api/alert-rules\n```\n\n**Alert rule evaluator (`monitoring/alerting.py`):**\n\n```python\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nimport redis\n\nclass AlertRule:\n    \"\"\"Represents an alert rule with condition and thresholds.\"\"\"\n    \n    def __init__(self, rule_id: str, config: Dict[str, Any]):\n        self.rule_id = rule_id\n        self.name = config.get('name', 'Unnamed Rule')\n        self.severity = config.get('severity', 'WARNING')  # CRITICAL, WARNING, INFO\n        self.condition = config.get('condition', '')  # e.g., \"error_rate > 0.05\"\n        self.window_seconds = config.get('window_seconds', 300)  # 5 minutes\n        self.cooldown_seconds = config.get('cooldown_seconds', 300)  # 5 minutes\n        self.enabled = config.get('enabled', True)\n        self.labels = config.get('labels', {})  # e.g., {\"queue\": \"emails\", \"job_type\": \"send_welcome\"}\n        \n    def evaluate(self, metrics_store) -> bool:\n        \"\"\"Evaluate rule against current metrics.\"\"\"\n        # TODO 1: Parse condition string into evaluable expression\n        # Example: \"error_rate > 0.05 AND queue_depth > 1000\"\n        \n        # TODO 2: Fetch relevant metrics for the time window\n        # Use metrics_store.get_metrics(window_seconds=self.window_seconds, labels=self.labels)\n        \n        # TODO 3: Calculate aggregated values from metrics\n        # For rate-based conditions, calculate rate over window\n        \n        # TODO 4: Evaluate condition against calculated values\n        # Use safe_eval or ast parsing for condition evaluation\n        \n        # TODO 5: Return True if condition is met\n        return False\n        \n    def should_alert(self, last_alert_time: Optional[datetime]) -> bool:\n        \"\"\"Check if alert should fire considering cooldown period.\"\"\"\n        if not self.enabled:\n            return False\n            \n        if last_alert_time is None:\n            return True\n            \n        # Check cooldown period\n        seconds_since_last = (datetime.utcnow() - last_alert_time).total_seconds()\n        return seconds_since_last >= self.cooldown_seconds\n\nclass AlertManager:\n    \"\"\"Manages alert rules and firing alerts.\"\"\"\n    \n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n        self.rules: Dict[str, AlertRule] = {}\n        self.rule_key_prefix = \"monitoring:alert_rules:\"\n        \n    def load_rules(self):\n        \"\"\"Load all alert rules from Redis.\"\"\"\n        # TODO 1: Scan for all rule keys with prefix\n        # Use redis.scan_iter(self.rule_key_prefix + \"*\")\n        \n        # TODO 2: For each key, load JSON config and create AlertRule\n        \n        # TODO 3: Store in self.rules dictionary\n        \n    def evaluate_all_rules(self):\n        \"\"\"Evaluate all enabled alert rules.\"\"\"\n        # TODO 1: Get last alert times for each rule from Redis\n        \n        # TODO 2: For each rule, evaluate condition\n        \n        # TODO 3: If condition met and cooldown passed, fire alert\n        \n        # TODO 4: Create SystemAlert object and store to Redis\n        \n        # TODO 5: Update last alert time for rule\n        \n    def fire_alert(self, rule: AlertRule, condition_value: float):\n        \"\"\"Fire an alert and store it.\"\"\"\n        # TODO 1: Generate unique alert_id (ULID)\n        \n        # TODO 2: Create SystemAlert object with all fields\n        \n        # TODO 3: Store to Redis Sorted Set (by timestamp) and Hash (full data)\n        \n        # TODO 4: Optionally send notification (email, webhook, etc.)\n        \n        # TODO 5: Return alert_id\n        return \"alert_123\"\n```\n\n#### E. Language-Specific Hints\n\n**Python Implementation Tips:**\n\n1. **Use Flask for simple dashboard**: Flask + Flask-SSE provides a straightforward SSE implementation. For production, use `gevent` or `gunicorn` with async workers.\n\n2. **Redis connection management**: Use connection pooling for dashboard queries to avoid overwhelming Redis with connections:\n   ```python\n   import redis\n   from redis.connection import ConnectionPool\n   \n   pool = ConnectionPool.from_url('redis://localhost:6379', max_connections=20)\n   redis_client = redis.Redis(connection_pool=pool)\n   ```\n\n3. **SSE implementation**: Flask doesn't natively support SSE well; use a queue-based approach:\n   ```python\n   from flask import Response\n   import queue\n   import json\n   \n   class MessageAnnouncer:\n       def __init__(self):\n           self.listeners = []\n       \n       def listen(self):\n           q = queue.Queue(maxsize=5)\n           self.listeners.append(q)\n           return q\n       \n       def announce(self, msg):\n           for i in reversed(range(len(self.listeners))):\n               try:\n                   self.listeners[i].put_nowait(msg)\n               except queue.Full:\n                   del self.listeners[i]\n   \n   announcer = MessageAnnouncer()\n   ```\n\n4. **Async metrics collection**: Use background threads for aggregation, not async/await, since Redis operations are blocking.\n\n5. **Job argument sanitization**: Before displaying job arguments in dashboard, filter sensitive fields:\n   ```python\n   SENSITIVE_FIELDS = {'password', 'token', 'secret', 'key', 'authorization'}\n   \n   def sanitize_job_args(args_dict):\n       sanitized = {}\n       for key, value in args_dict.items():\n           if any(sensitive in key.lower() for sensitive in SENSITIVE_FIELDS):\n               sanitized[key] = '***REDACTED***'\n           else:\n               sanitized[key] = value\n       return sanitized\n   ```\n\n#### F. Milestone Checkpoint\n\n**After implementing Monitoring & Dashboard, verify:**\n\n1. **Dashboard loads**: Start the web server and navigate to `http://localhost:8080/dashboard`. You should see:\n   - System overview with queue depths\n   - Worker status table\n   - Real-time metrics updating every few seconds\n\n2. **API endpoints work**: Test API with curl:\n   ```bash\n   # Get queue metrics\n   curl http://localhost:8080/api/metrics/queues\n   \n   # Get worker status\n   curl http://localhost:8080/api/workers\n   \n   # Stream real-time events (SSE)\n   curl -N http://localhost:8080/api/metrics/stream\n   ```\n\n3. **Metrics collection**: Enqueue a few jobs and verify they appear in:\n   - Dashboard queue depth indicators\n   - Job history table (after completion)\n   - Real-time event stream (if connected via SSE)\n\n4. **Alerting test**: Set up a simple alert rule (error rate > 0%) and trigger a job failure:\n   ```python\n   # Create alert rule via API\n   import requests\n   rule = {\n       \"name\": \"Test Error Alert\",\n       \"severity\": \"WARNING\",\n       \"condition\": \"error_rate > 0\",\n       \"window_seconds\": 60,\n       \"labels\": {\"queue\": \"default\"}\n   }\n   requests.post(\"http://localhost:8080/api/alert-rules\", json=rule)\n   ```\n   \n   Check that alerts appear in `/api/alerts` endpoint after triggering a job failure.\n\n**Signs something is wrong:**\n- Dashboard shows \"No data\" or \"Connection lost\" → Check Redis connection and event emission\n- Real-time updates not working → Check SSE endpoint and browser console for errors\n- High Redis memory usage → Verify TTL is set on history keys and aggregation is running\n- Dashboard loads slowly → Check for expensive Redis queries (use `SLOWLOG` command)\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Dashboard shows \"Disconnected\"** | SSE connection failing | Check browser console for network errors; verify `/api/metrics/stream` endpoint returns proper SSE headers | Ensure response has `text/event-stream` MIME type and proper headers |\n| **Real-time updates delayed by minutes** | Event aggregation not running | Check if `MetricsAggregator` process is running; look for events in `monitoring:events` stream | Start aggregator service; check for exceptions in aggregator logs |\n| **Redis memory growing rapidly** | Job history TTL not set | Check Redis keys with `keys job:history:*`; verify TTL with `ttl keyname` | Ensure all history keys have TTL; implement periodic cleanup job |\n| **Alert rules not firing** | Rule evaluation failing | Check alert manager logs; test rule condition manually with current metrics | Debug rule evaluation logic; verify metrics exist for rule labels |\n| **Dashboard queries timing out** | Expensive Redis operations | Use `redis-cli --stat` to see operations; check `SLOWLOG GET 10` | Add query limits; implement caching for expensive queries; use read replica |\n| **Job arguments visible in dashboard** | Sanitization not applied | Check dashboard display of job details; look for sensitive fields | Implement argument sanitization in Job.to_dict() or dashboard serialization |\n\n---\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** This section integrates concepts and components from all five milestones, showing how they work together to handle key operational scenarios: normal job flow, error recovery, and scheduled job execution. Understanding these interactions is critical for implementing a cohesive system.\n\nThis section details the end-to-end behavior of the Background Job Processor through three critical scenarios: the successful processing of a job from submission to completion, the recovery flow when a worker crashes mid-execution, and the lifecycle of a scheduled recurring job. These scenarios demonstrate how the system's components—`QueueManager`, `Worker`, `RetryManager`, `Scheduler`, and monitoring subsystems—orchestrate data flow and state transitions to deliver reliable asynchronous processing.\n\n### Happy Path: Job Submission to Completion\n\n**Mental Model: Assembly Line in a Factory**\nImagine a manufacturing plant. An order (job) is submitted to the front office (producer). The office validates the order, stamps it with a unique ID, and places it on the correct conveyor belt (queue) based on its type. A worker on the factory floor picks up the order, performs the required assembly steps (handler execution), marks it as completed, and places the finished product in the shipping area (result storage). The entire process is tracked in a central log (monitoring system), allowing managers to see the order's progress in real-time.\n\nThis scenario begins with a producer submitting a job and ends with the job's successful completion, involving all core components in their normal operational states.\n\n#### Step-by-Step Flow\n\nThe following numbered sequence details the journey of a single job, referencing the ![System Component Overview](./diagrams/system-component.svg) and ![Happy Path Job Execution Sequence](./diagrams/happy-path-sequence.svg) diagrams.\n\n1.  **Job Creation and Submission:** The producer application creates a `Job` instance, populating its required fields (`job_type`, `args`, `queue`) and optional metadata. It then calls `QueueManager.enqueue_job(job)`.\n\n2.  **Validation and Serialization:** Inside `QueueManager.enqueue_job`, the following occurs:\n    *   `QueueManager._validate_job(job)` is called. It checks that the payload size is under `SystemConfig.max_payload_size`, that the specified queue exists in `QueueManager.queue_configs`, and that the job's `job_type` is a non-empty string. If validation fails, a `ValidationError` is raised to the producer.\n    *   The job's `status` is set to `PENDING`, `job_id` is assigned a ULID, and `created_at` is set to the current UTC timestamp.\n    *   `Job.serialize()` is invoked, converting the job's dictionary representation (from `Job.to_dict()`) into a JSON string.\n\n3.  **Atomic Enqueue to Redis:** The `QueueManager` uses a Redis pipeline (via `RedisClient.pipeline()`) to execute the following commands atomically:\n    *   `HSET` to store the serialized job under a key like `job:{job_id}`. This creates a persistent record.\n    *   `EXPIRE` is set on the job key using a TTL (e.g., 7 days) to prevent indefinite memory bloat.\n    *   `LPUSH` to add the `job_id` to the appropriate Redis list key, e.g., `queue:{queue_name}`. This action makes the job visible to workers.\n    *   `INCR` on a metric key like `stats:queue:{queue_name}:enqueued` for monitoring.\n    *   If the queue has a `QueueConfig.max_length`, the pipeline also checks `LLEN` and may raise a `QueueFullError` before the `LPUSH`.\n    *   The pipeline is executed. Upon success, `enqueue_job` returns the generated `job_id` to the producer.\n\n4.  **Worker Polling and Dequeue:** A `Worker` process, configured to listen on the relevant queue, is in its main loop. It uses the priority-weighted polling algorithm to select a queue and then calls `BRPOPLPUSH` (or a similar reliable dequeue pattern) with a timeout. This command atomically moves the `job_id` from the main `queue:{queue_name}` list to a temporary `queue:{queue_name}:processing` list, providing at-least-once delivery semantics. The worker receives the `job_id`.\n\n5.  **Job Materialization and Dispatch:** The worker uses `QueueManager.get_job_by_id(job_id)` to fetch the full job data from the `job:{job_id}` hash. It then calls `Job.deserialize()` to recreate the `Job` object in memory. The worker updates the job's `status` to `ACTIVE` and `started_at` to the current time, persisting this change back to Redis. It then looks up the job's `job_type` in its internal `Worker.handlers` registry to find the appropriate handler function.\n\n6.  **Job Execution:** The worker invokes the registered handler function, passing the job's `args` and `kwargs`. The handler executes the business logic (e.g., sending an email, processing an image). The worker's `WorkerHeartbeat` thread continues to periodically update a key like `worker:heartbeat:{worker_id}` in Redis, indicating liveness.\n\n7.  **Success Handling and Cleanup:** Upon successful handler completion:\n    *   The worker updates the job's `status` to `COMPLETED`, sets `completed_at`, and optionally stores the handler's return value in `result`.\n    *   The job's updated state is persisted to the `job:{job_id}` hash in Redis.\n    *   The worker removes the `job_id` from the `queue:{queue_name}:processing` list (e.g., via `LREM`), completing the reliable dequeue cycle.\n    *   Metric keys are incremented (e.g., `stats:queue:{queue_name}:completed`).\n\n8.  **Monitoring Update:** The `MetricsAggregator`, running periodically, observes the changes in job state and queue lengths. It updates aggregated metrics (like `QueueMetrics.process_rate`). If the dashboard is open, it may receive these updates via Server-Sent Events (SSE), refreshing the display to show the job's completion and updated queue statistics.\n\nThe following table summarizes the key Redis commands and state changes during this flow:\n\n| Step | Primary Redis Command(s) | Job State Transition | Key Data Structures Updated |\n| :--- | :--- | :--- | :--- |\n| 1-3 | `HSET job:{id} ...`, `EXPIRE`, `LPUSH queue:{name} {id}` | None → `PENDING` | `job:{id}` (Hash), `queue:{name}` (List) |\n| 4 | `BRPOPLPUSH queue:{name} queue:{name}:processing` | `PENDING` → (Implicitly Active) | `queue:{name}` (List), `queue:{name}:processing` (List) |\n| 5-6 | `HGET job:{id}`, `HSET job:{id} status ACTIVE` | `PENDING` → `ACTIVE` | `job:{id}` (Hash), `worker:heartbeat:{id}` (String) |\n| 7 | `HSET job:{id} status COMPLETED ...`, `LREM queue:{name}:processing {id}` | `ACTIVE` → `COMPLETED` | `job:{id}` (Hash), `queue:{name}:processing` (List) |\n\n### Error Recovery Flow: Worker Crash and Retry\n\n**Mental Model: Paramedic Response and Hospital Triage**\nWhen a factory worker (the `Worker` process) suddenly faints (crashes), the system's safety protocols activate. First, a supervisor (the heartbeat monitor) notices the worker's missing check-ins and marks them as inactive. Another worker then safely retrieves the half-assembled product (the job) from the worker's station (the processing queue) and assesses the damage. If the product can be fixed, it's sent to a repair station (the retry queue) with a note indicating the problem and a scheduled repair time. If it's beyond repair after several attempts, it's sent to the quality control failure analysis bin (the dead letter queue) for engineering review.\n\nThis scenario highlights the system's fault tolerance, demonstrating how jobs are not lost when a `Worker` fails unexpectedly and how the retry mechanism with exponential backoff manages transient failures.\n\n#### Step-by-Step Flow\n\nThe sequence, also illustrated in ![Retry with Exponential Backoff Flow](./diagrams/retry-flow-sequence.svg), begins after step 6 of the Happy Path, assuming the worker crashes during job execution.\n\n1.  **Worker Crash:** During the execution of a handler (step 6), the `Worker` process suffers a fatal error (e.g., a segmentation fault, `SIGKILL`, or host failure) and terminates immediately. It does not reach the success handling logic (step 7).\n\n2.  **Stale Heartbeat Detection:** The worker's `WorkerHeartbeat` thread also stops, ceasing updates to the `worker:heartbeat:{worker_id}` key in Redis. The `WorkerHeartbeat` key has a TTL slightly longer than the heartbeat interval (e.g., interval=30s, TTL=45s). After the TTL expires, the key disappears. The monitoring system's `AlertManager` or dashboard, which periodically scans for worker keys, now identifies this worker as \"stale\" or \"dead.\"\n\n3.  **Job Re-queue via Processing Queue:** Because the job was dequeued using `BRPOPLPUSH` (or a similar pattern), its `job_id` remains in the `queue:{queue_name}:processing` list. A separate \"janitor\" process (which can be part of the `Worker` startup routine or a dedicated maintenance script) periodically scans all `queue:*:processing` lists. For each `job_id` found, it checks if the associated worker (which can be tracked by storing the `worker_id` in the processing list or alongside the job) is still alive (by checking the heartbeat key). If the worker is dead, the janitor process moves the `job_id` back from the `queue:{queue_name}:processing` list to the main `queue:{queue_name}` list using `RPOPLPUSH` or a similar atomic operation. The job is now `PENDING` again and available for any healthy worker.\n\n4.  **Worker Retry and Failure:** A healthy worker picks up the job (returning to step 4). It loads the job, sees its `status` is `PENDING` (or `ACTIVE` with a very old `started_at`), and begins execution. This time, the handler fails due to a transient error (e.g., a network timeout, a deadlock). The worker catches the exception.\n\n5.  **Error Recording and Retry Check:** The worker calls `Job.record_error(error)`, which appends a structured error dictionary (containing exception class, message, and traceback) to `Job.errors` and increments `Job.attempts`. It then calls `Job.should_retry()` which compares `Job.attempts` against `Job.max_retries`. If retries are allowed, the worker proceeds to the retry flow.\n\n6.  **Retry Scheduling:** The worker calls `RetryManager.handle_job_failure(job, error)`. This method:\n    *   Calculates the next retry delay using `BackoffCalculator.calculate_delay(Job.attempts)`. For attempt 1, this might be 1 second; for attempt 2, 2 seconds, etc., plus optional jitter.\n    *   Computes an absolute timestamp for the retry: `current_time + delay`.\n    *   Updates the job's `status` to `RETRY_SCHEDULED`.\n    *   Uses `ZADD` to add the `job_id` with a score equal to the retry timestamp to a Redis sorted set key, e.g., `retry_queue:{queue_name}`.\n    *   Persists the updated job (with the new error and attempt count) to the `job:{job_id}` hash.\n    *   Removes the `job_id` from the processing queue.\n\n7.  **Retry Polling and Re-enqueue:** The `RetryManager` (or a dedicated thread within each `Worker`) periodically polls the retry sorted sets using `ZRANGEBYSCORE` with a score range of `0` to `current_timestamp`. For each due `job_id`, it retrieves the full job, changes its `status` back to `PENDING`, and calls `QueueManager.enqueue_job(job)` to place it back on its original main queue. The job ID is removed from the sorted set via `ZREM`.\n\n8.  **Eventual Success or Final Failure:** The job is retried by workers (looping through steps 4-7). If it eventually succeeds, the happy path completes. If `Job.attempts` exceeds `Job.max_retries`, `Job.should_retry()` returns `False`.\n\n9.  **Dead Letter Queue (DLQ) Placement:** In the case of final failure, `RetryManager.handle_job_failure` calls `RetryManager.move_to_dead_letter(job)`. This method:\n    *   Sets the job's `status` to `DEAD_LETTER`.\n    *   Persists the final state to `job:{job_id}`.\n    *   Adds the `job_id` to a dedicated list or set key, e.g., `dead_letter_queue`.\n    *   Triggers a high-priority alert (e.g., `CRITICAL` severity) via the `AlertManager`.\n    *   The job is now out of the processing loop and awaits manual intervention via the dashboard's `retry_dead_letter_job` or `delete_dead_letter_job` APIs.\n\n| Failure Scenario | Detection Mechanism | Automatic Recovery Action | Manual Intervention Required? |\n| :--- | :--- | :--- | :--- |\n| Worker Process Crash | Heartbeat TTL expiry | Job moved from processing queue back to main queue via janitor process. | No, unless the janitor process itself fails. |\n| Transient Handler Error (e.g., network timeout) | Exception caught by worker wrapper. | Job is scheduled for retry with exponential backoff. | No. |\n| Persistent Handler Error (e.g., bug in code) | Max retries exhausted. | Job moved to Dead Letter Queue (DLQ). | **Yes.** An operator must inspect the error and decide to retry or delete. |\n| Redis Connection Loss | Redis client exception, heartbeat fails. | Worker may crash/restart; jobs remain in processing queue until janitor cleans up. | No, assuming worker process manager (e.g., systemd) restarts it. |\n\n> **Design Insight:** The combination of a **processing queue** (for crash safety) and a **retry sorted set** (for scheduled retries) is crucial. The processing queue ensures no job is ever truly \"in flight\" without a paper trail in Redis, enabling recovery from worker crashes. The sorted set provides efficient scheduling without requiring a separate timer thread for each failed job.\n\n#### Common Pitfalls in Error Recovery\n\n⚠️ **Pitfall: Busy-Waiting on Dead Workers**\n**Mistake:** The janitor process continuously polls the processing queues in a tight loop, consuming CPU and Redis resources even when no workers have crashed.\n**Why it's wrong:** This wastes resources and can exacerbate problems during system stress.\n**Fix:** Implement an exponential backoff in the janitor's polling loop (e.g., sleep for 5 seconds, then 10, up to a max of 60 seconds). Additionally, use Redis's `KEYS` or `SCAN` command sparingly to find processing queues, and consider publishing events to a Pub/Sub channel when a worker stops heartbeating to trigger immediate cleanup.\n\n⚠️ **Pitfall: Losing Error Context on Retry**\n**Mistake:** Overwriting the job's error field on each retry, so only the last error is preserved.\n**Why it's wrong:** Debugging becomes difficult because you cannot see the history of failures that led to the dead letter state.\n**Fix:** Use `Job.record_error(error)`, which *appends* to the `Job.errors` list. Ensure the serialization format (e.g., JSON) supports a list of complex error objects.\n\n⚠️ **Pitfall: Retry Storm on Permanent Failure**\n**Mistake:** A job with a permanent error (e.g., \"FileNotFound\") is continuously retried, wasting resources and clogging the queue until `max_retries` is reached.\n**Why it's wrong:** It delays the processing of other jobs and creates unnecessary load.\n**Fix:** Implement **retry filters** or allow handlers to raise specific exception types (e.g., `ImmediateFailError`) that bypass the retry logic and go directly to the DLQ.\n\n### Scheduled Job: Cron to Execution\n\n**Mental Model: Calendar App with Recurring Events**\nThink of a sophisticated calendar application. You create a recurring event \"Team Stand-up\" scheduled for \"Every weekday at 9 AM UTC.\" The calendar service doesn't create all future events at once. Instead, it constantly looks at the current time and the rules for all recurring events. When it detects that the current time matches a rule's \"next scheduled time,\" it creates a new calendar entry (enqueues a job) for that specific occurrence. It then immediately calculates the *next* occurrence after this one and waits. If the service is down for maintenance at 9 AM, upon restart, it checks which events it missed and creates them immediately (catch-up logic).\n\nThis scenario involves the `Scheduler` component, demonstrating how future-dated and recurring jobs are managed, enqueued, and eventually processed.\n\n#### Step-by-Step Flow\n\n1.  **Schedule Definition:** An administrator or application calls `schedule_recurring_job(job, cron_expression, timezone='UTC')`. This function creates a `Schedule` object, assigns a `schedule_id`, and calculates the initial `next_run_at` time using `_calculate_next_run(schedule, reference_time=now)`. The `Schedule` is persisted to a Redis hash (e.g., `schedule:{schedule_id}`). The `Schedule.status` is set to `PENDING` for its future runs.\n\n2.  **Scheduler Polling Loop:** The `Scheduler` process runs `_poll_due_schedules()` in a loop, sleeping for `polling_interval_seconds` between iterations. During each poll:\n    *   It fetches all `Schedule` objects from Redis where `enabled=True`.\n    *   For each schedule, it checks if `Schedule.next_run_at <= current_time`. If not, it skips.\n    *   For due schedules, it calls `_process_schedule(schedule)`.\n\n3.  **Processing a Due Schedule:** `_process_schedule` performs the following atomic operations (typically within a Redis pipeline/WATCH to handle concurrent schedulers):\n    *   **Uniqueness Check:** If `Schedule.unique_key` is set, it uses Redis's `SET key value NX EX window` command to claim a lock for the `unique_window_seconds`. If the lock cannot be acquired (because a duplicate was enqueued recently), the schedule's status for this run is marked as `SKIPPED`, `last_enqueued_at` is updated, and the next run time is recalculated.\n    *   **Job Creation and Enqueue:** If unique (or not required), it creates a concrete `Job` instance from the template in the `Schedule`. The job's `queue`, `job_type`, `args`, `kwargs`, and metadata are copied. The job's `metadata` field may be augmented with the `schedule_id` and the `scheduled_for` timestamp.\n    *   `_enqueue_scheduled_job` is called, which uses `QueueManager.enqueue_job` to push the job onto the designated worker queue.\n    *   **Schedule Update:** Upon successful enqueue, the schedule's `last_enqueued_at` is set to the current time, and `status` for this run is set to `ENQUEUED`. `Schedule.next_run_at` is recalculated based on the `cron_expression` and `timezone`, using `_calculate_next_run(schedule, reference_time=now)`.\n\n4.  **Catch-up Logic on Scheduler Start:** When the `Scheduler` starts (e.g., after a deployment or crash), it runs `calculate_next_runs_for_all_schedules()`. This method ensures `next_run_at` is set correctly for all schedules. More importantly, if `Scheduler` detects that a schedule's `next_run_at` is in the past (meaning it missed runs while down), it will immediately process those missed schedules, enqueuing jobs for each missed occurrence up to a sensible limit (to prevent a thundering herd). This is a critical feature for reliability.\n\n5.  **Job Execution:** From this point, the journey of the enqueued job is identical to the **Happy Path**. Workers pick it up (step 4 onward) and execute it. The job's metadata retains its origin as a scheduled job, which can be useful for monitoring and debugging.\n\n6.  **Timezone and Daylight Saving Time (DST) Handling:** The `_calculate_next_run` function uses a timezone-aware datetime library (like `pytz` or `zoneinfo`). When calculating the next run for a schedule in a timezone that observes DST, the function correctly handles ambiguous or non-existent times (e.g., 2:30 AM during a \"spring forward\" transition). A common strategy is to default to the later valid time during an ambiguous period and to skip non-existent times by advancing to the next valid interval.\n\n| Schedule Event | Scheduler Action | Redis Command Example | Outcome |\n| :--- | :--- | :--- | :--- |\n| New recurring schedule created. | Calculate `next_run_at`, store schedule. | `HSET schedule:{id} ...` | Schedule is dormant until its first due time. |\n| Poll finds due schedule. | Check uniqueness, create job, enqueue, update schedule. | Pipeline: `SET NX`, `HSET job:{jid}`, `LPUSH queue:...`, `HSET schedule:{sid}` | Job is placed on worker queue. Schedule is updated for next run. |\n| Scheduler restarts after crash. | Recalculate next runs, process missed schedules. | `ZSCAN` for past-due `next_run_at` scores, then process. | Missed jobs are enqueued, ensuring no schedule is permanently skipped. |\n| Admin pauses a schedule. | Set `Schedule.enabled = False`. | `HSET schedule:{id} enabled 0` | Scheduler skips this schedule in future polls. |\n\n#### Architecture Decision Record: Scheduler Catch-up Strategy\n\n> **Decision: Limited Catch-up with Skip Option**\n> - **Context:** When the Scheduler process is down (for maintenance, crash, etc.), scheduled jobs will be missed. Upon restart, we must decide how to handle these missed occurrences.\n> - **Options Considered:**\n>     1.  **Enqueue All Missed Jobs:** Calculate every missed execution time since the last successful run and enqueue a job for each.\n>     2.  **Enqueue Only the Most Recent Missed Job:** Enqueue a single job for the most recent missed occurrence.\n>     3.  **Skip All Missed Jobs:** Do not enqueue any missed jobs; simply resume the schedule from the next future time.\n>     4.  **Configurable, Limited Catch-up:** Allow configuration to enqueue up to N missed jobs, with an option to skip the rest.\n> - **Decision:** Option 4 (Configurable, Limited Catch-up).\n> - **Rationale:** Option 1 can cause a dangerous \"thundering herd\" problem, instantly flooding the system with a backlog of jobs which may overwhelm workers and downstream services. Option 2 loses data (missed executions). Option 3 may be unacceptable for critical schedules (e.g., daily reports). Option 4 provides safety with a cap (e.g., max 10 catch-up jobs) and allows operators to tune based on the job's idempotency and importance.\n> - **Consequences:** The system is protected from self-inflicted denial-of-service on restart. Operators must understand the catch-up limit for their critical schedules. Some missed jobs beyond the limit will be permanently skipped, which may require manual intervention for critical business processes.\n\n| Option | Pros | Cons | Why Not Chosen |\n| :--- | :--- | :--- | :--- |\n| Enqueue All | No data loss, simple. | Risk of thundering herd, system overload. | Too dangerous for automated recovery. |\n| Enqueue Most Recent | Handles short outages, simple. | Loses history of multiple missed runs. | Unacceptable for schedules with high frequency or long outages. |\n| Skip All | Safe, no overload. | Potentially unacceptable data loss for business. | Lacks flexibility for important jobs. |\n| **Configurable, Limited Catch-up** | **Safe, flexible, tunable per schedule.** | **Adds configuration complexity.** | **Chosen for optimal safety and control.** |\n\n### Implementation Guidance\n\nThis section provides concrete code skeletons and structure to implement the interaction flows described above.\n\n#### A. Technology Recommendations Table\n\n| Interaction Scenario | Simple Option | Advanced Option |\n| :--- | :--- | :--- |\n| Reliable Job Dequeue (Worker) | Use `BRPOPLPUSH` with a processing queue. | Use Redis Streams (`XREADGROUP`, `XACK`) for more sophisticated consumer groups and pending entry management. |\n| Retry Scheduling | Use a Redis Sorted Set (`ZSET`) with retry timestamp as score. | Use a dedicated time-series database or a priority queue service for extremely high-scale retry scheduling. |\n| Scheduler Polling | Simple `time.sleep()` loop in the main thread. | Use `asyncio` with async Redis client for non-blocking polling of multiple schedules. |\n| Janitor Process for Crashed Workers | A separate script run by a cron job every minute. | Integrate the cleanup logic into the `Worker`'s startup routine and use Redis Pub/Sub to broadcast worker deaths for immediate cleanup. |\n\n#### B. Recommended File/Module Structure\n\nAdd the following files to manage the interaction flows and recovery processes:\n\n```\nbackground_job_processor/\n├── core/\n│   ├── __init__.py\n│   ├── job.py                  # Job class definition\n│   ├── queue_manager.py        # QueueManager class\n│   ├── worker.py               # Worker class, main loop\n│   ├── retry_manager.py        # RetryManager class\n│   ├── scheduler.py            # Scheduler class\n│   └── janitor.py              # Cleanup process for stalled jobs\n├── models/\n│   ├── __init__.py\n│   ├── schedule.py             # Schedule, ScheduledJob classes\n│   └── metrics.py              # MetricPoint, QueueMetrics, etc.\n├── web/\n│   ├── __init__.py\n│   └── dashboard.py            # API endpoints for dashboard\n├── utils/\n│   ├── __init__.py\n│   ├── redis_client.py         # RedisClient wrapper\n│   └── backoff.py              # BackoffCalculator\n└── scripts/\n    ├── run_worker.py           # Worker entry point\n    ├── run_scheduler.py        # Scheduler entry point\n    └── run_janitor.py          # Janitor script (can be cron)\n```\n\n#### C. Infrastructure Starter Code\n\n**`utils/redis_client.py` - A robust Redis client wrapper:**\n```python\nimport redis\nimport logging\nfrom typing import Optional, Any\nfrom functools import wraps\n\nclass RedisClient:\n    \"\"\"Singleton wrapper for Redis client with connection pooling and error handling.\"\"\"\n    \n    _instance: Optional['RedisClient'] = None\n    _client: Optional[redis.Redis] = None\n    \n    def __init__(self, url: str, **connection_kwargs):\n        # ... (initialization as per naming conventions)\n    \n    @classmethod\n    def get_instance(cls, url: Optional[str] = None, **kwargs) -> 'RedisClient':\n        if cls._instance is None:\n            if url is None:\n                raise ValueError(\"URL required for first RedisClient initialization\")\n            cls._instance = cls(url, **kwargs)\n        return cls._instance\n    \n    def get_client(self) -> redis.Redis:\n        if self._client is None or not self.ping():\n            self._reconnect()\n        return self._client\n    \n    def ping(self) -> bool:\n        try:\n            return self._client is not None and self._client.ping()\n        except (redis.ConnectionError, redis.TimeoutError):\n            return False\n    \n    def _reconnect(self):\n        # ... reconnection logic with retry backoff\n    \n    def execute(self, command: str, *args, **kwargs) -> Any:\n        \"\"\"Execute a Redis command with automatic retry on connection errors.\"\"\"\n        max_retries = 3\n        for attempt in range(max_retries):\n            try:\n                client = self.get_client()\n                method = getattr(client, command.lower())\n                return method(*args, **kwargs)\n            except (redis.ConnectionError, redis.TimeoutError) as e:\n                if attempt == max_retries - 1:\n                    raise\n                logging.warning(f\"Redis connection error on {command}, retry {attempt+1}: {e}\")\n                self._reconnect()\n            except redis.RedisError as e:\n                logging.error(f\"Redis error on {command}: {e}\")\n                raise\n    \n    def pipeline(self) -> redis.Pipeline:\n        \"\"\"Return a pipeline for atomic operations.\"\"\"\n        return self.get_client().pipeline()\n```\n\n#### D. Core Logic Skeleton Code\n\n**`core/janitor.py` - Cleanup process for crashed workers:**\n```python\nimport time\nimport logging\nfrom typing import List\nfrom .redis_client import RedisClient\nfrom core.queue_manager import QueueManager\n\nclass Janitor:\n    \"\"\"Cleans up jobs stuck in processing queues from dead workers.\"\"\"\n    \n    def __init__(self, redis_client: RedisClient, scan_interval: int = 30):\n        self.redis = redis_client\n        self.scan_interval = scan_interval\n        self.running = False\n        \n    def _get_stale_processing_queues(self) -> List[str]:\n        \"\"\"Find all queue:processing lists where the associated worker is dead.\"\"\"\n        # TODO 1: Use SCAN to find all keys matching pattern 'queue:*:processing'\n        # TODO 2: For each processing queue key, extract the worker_id (might be stored in a separate key or in the list's metadata)\n        # TODO 3: Check if the worker's heartbeat key (worker:heartbeat:{id}) exists and hasn't expired\n        # TODO 4: If worker is dead, add the processing queue key to the result list\n        # TODO 5: Return the list of stale processing queue keys\n        pass\n    \n    def _requeue_stale_jobs(self, processing_queue_key: str) -> int:\n        \"\"\"Move all jobs from a stale processing queue back to its main queue.\"\"\"\n        # TODO 1: Extract the main queue name from the processing queue key (e.g., 'queue:email:processing' -> 'queue:email')\n        # TODO 2: In a loop, use RPOPLPUSH to move jobs from processing queue back to main queue atomically\n        # TODO 3: Count the number of jobs moved\n        # TODO 4: Log a warning for each job moved\n        # TODO 5: Return the count of requeued jobs\n        pass\n    \n    def run_once(self) -> int:\n        \"\"\"Execute a single cleanup cycle. Returns number of jobs requeued.\"\"\"\n        total_requeued = 0\n        stale_queues = self._get_stale_processing_queues()\n        for queue_key in stale_queues:\n            count = self._requeue_stale_jobs(queue_key)\n            total_requeued += count\n        return total_requeued\n    \n    def start(self):\n        \"\"\"Start the janitor's main loop.\"\"\"\n        self.running = True\n        logging.info(\"Janitor started.\")\n        while self.running:\n            try:\n                requeued = self.run_once()\n                if requeued > 0:\n                    logging.warning(f\"Janitor requeued {requeued} stale jobs.\")\n            except Exception as e:\n                logging.error(f\"Janitor error: {e}\", exc_info=True)\n            time.sleep(self.scan_interval)\n    \n    def stop(self):\n        self.running = False\n```\n\n**`core/retry_manager.py` - Handling retry scheduling:**\n```python\nimport time\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom .job import Job, JobStatus\nfrom .redis_client import RedisClient\nfrom utils.backoff import BackoffCalculator\n\nclass RetryManager:\n    \n    def handle_job_failure(self, job: Job, error: Exception) -> None:\n        \"\"\"Main entry point: decide retry or dead letter.\"\"\"\n        # TODO 1: Call job.record_error(error) to update error list and attempt count\n        # TODO 2: Call job.should_retry() to check if max_retries not exceeded\n        # TODO 3: If should_retry is True:\n        #   - Calculate delay using self.backoff_calculator.calculate_delay(job.attempts)\n        #   - Call self.schedule_retry(job, delay)\n        # TODO 4: If should_retry is False:\n        #   - Call self.move_to_dead_letter(job)\n        #   - Log a critical error\n        # TODO 5: Update the job's status in Redis to RETRY_SCHEDULED or DEAD_LETTER\n        pass\n    \n    def schedule_retry(self, job: Job, delay_seconds: float) -> str:\n        \"\"\"Schedule a job for retry after a delay.\"\"\"\n        # TODO 1: Calculate the absolute retry timestamp: current_time + delay_seconds\n        # TODO 2: Use Redis ZADD to add job.job_id to sorted set key 'retry_queue:{job.queue}' with score = retry_timestamp\n        # TODO 3: Return the job_id\n        pass\n    \n    def get_due_retries(self, queue_name: str, max_count: int = 100) -> List[Job]:\n        \"\"\"Get jobs whose retry time has arrived.\"\"\"\n        # TODO 1: Get current timestamp\n        # TODO 2: Use ZRANGEBYSCORE with scores 0 to current_timestamp to get job_ids due for retry\n        # TODO 3: For each job_id, fetch the full Job from Redis using QueueManager.get_job_by_id\n        # TODO 4: Return the list of Job objects\n        pass\n```\n\n#### E. Language-Specific Hints (Python)\n\n*   **Atomic Operations:** Always use Redis pipelines (`self.redis.pipeline()`) for operations that must be atomic, like enqueuing a job (store + push) or moving a job between queues. Use `WATCH` for optimistic concurrency control when updating schedules.\n*   **Time and Timezones:** Use `datetime.datetime.now(datetime.timezone.utc)` for all timestamps stored in Redis. For cron schedule evaluation with timezones, use the `pytz` or `zoneinfo` (Python 3.9+) libraries. Be mindful of ambiguous times during DST transitions.\n*   **Graceful Shutdown:** Use `signal.signal(signal.SIGTERM, handler)` in your `Worker` and `Scheduler` main loops to catch termination signals. Set a flag (`_shutdown_requested`) and break out of the loop cleanly after finishing the current unit of work.\n*   **Connection Management:** Use the provided `RedisClient` wrapper, which includes connection pooling and automatic retry on transient network errors. Configure `max_connections` in the pool to match your concurrency level.\n\n#### F. Milestone Checkpoint for Interactions\n\nAfter implementing the core components (Milestones 1-4), you should be able to run the following integration test:\n\n1.  **Start Infrastructure:** Start Redis. Run one worker (`python scripts/run_worker.py --queues default`). Run the scheduler (`python scripts/run_scheduler.py`).\n2.  **Test Happy Path:** Use a Python shell to enqueue a simple job that prints a message. Verify the worker prints the message and the job status in Redis becomes `COMPLETED`.\n3.  **Test Worker Crash Recovery:** Enqueue a long-running job (e.g., `time.sleep(30)`). While it's running, kill the worker process (Ctrl+C or `kill -9`). Wait for the heartbeat TTL (e.g., 45 seconds). Start the worker again. Observe that the killed job is re-queued and eventually completes.\n4.  **Test Retry Logic:** Enqueue a job that fails with an exception and has `max_retries=3`. Watch the logs and Redis sorted set (`ZRANGE retry_queue:default 0 -1 WITHSCORES`). You should see the job ID scheduled, then retried, until it finally moves to the dead letter queue after 3 attempts.\n5.  **Test Scheduled Job:** Create a recurring schedule that runs every minute. Observe the scheduler logs and the main queue length. You should see a new job appear in the queue approximately every minute.\n\n**Signs of Success:** Jobs complete reliably despite failures. The dashboard shows accurate queue depths, worker status, and job history. No jobs are lost when workers or the scheduler restart.\n\n**Common Debugging Issues:**\n*   **Jobs disappearing:** Likely a bug in the reliable dequeue pattern (`BRPOPLPUSH`). Check that jobs are being moved to the processing queue and removed only after successful completion.\n*   **Retries not happening:** Ensure the `RetryManager.get_due_retries` is being called periodically (e.g., by the worker before polling for new jobs) and that the retry timestamps in the sorted set are correct.\n*   **Scheduler not enqueuing jobs:** Check the scheduler's polling loop logs. Verify cron expressions are valid and that `next_run_at` is being calculated correctly with the proper timezone.\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** This section spans all five milestones, addressing cross-cutting concerns of failure detection, recovery, and edge case management that are critical for building a production-ready distributed system.\n\nDistributed systems operate in an inherently unreliable environment where failures are not exceptional but expected. A background job processor must be designed with the fundamental assumption that network connections will drop, processes will crash, clocks will drift, and data will become corrupted. This section systematically catalogs the failure modes the system must handle, details how to detect them, outlines recovery strategies, and provides specific solutions for challenging edge cases. Unlike simpler applications where failures might be catastrophic, our design treats failures as normal operational states that trigger well-defined recovery paths, ensuring the system maintains job durability, processing guarantees, and operational visibility even under adverse conditions.\n\n### Failure Modes and Detection\n\n> **Mental Model: The Hospital Emergency Room Triage System**  \n> Imagine an emergency room where patients arrive with various conditions. The triage nurse must quickly assess each patient's vital signs, classify the severity of their condition, and route them to the appropriate treatment area. Some issues are immediately life-threatening (cardiac arrest), some are urgent but not critical (broken bones), and some are minor (small cuts). Similarly, our system must continuously monitor its own \"vital signs\"—network connectivity, process health, data integrity—and classify failures by severity to apply the correct intervention. Detection mechanisms act as the triage nurses, constantly checking pulses and responding to alarms.\n\nThe system must detect failures across multiple dimensions: infrastructure (Redis, network), processes (workers, scheduler), data (corruption, validation), and temporal (timeouts, clock skew). Each failure mode requires specific detection strategies that balance sensitivity (catching real failures) with specificity (avoiding false alarms). The following table categorizes the primary failure modes, their symptoms, and detection mechanisms:\n\n| Failure Mode | Symptoms & Indicators | Detection Mechanism | Severity Class |\n|--------------|-----------------------|---------------------|----------------|\n| **Redis Connection Failure** | TCP connection timeout, connection reset, \"Connection refused\" errors, inability to ping Redis | Periodic health checks (e.g., `PING` command every 5 seconds), monitoring connection pool exhaustion, tracking consecutive command failures | Critical (system cannot function without broker) |\n| **Network Partition** | Workers can connect to Redis but cannot communicate with each other or external services; some commands succeed while others timeout | Heartbeat cross-checks between workers, external service health checks integrated into job execution, monitoring for asymmetric failure patterns | Critical (can lead to split-brain scenarios) |\n| **Worker Process Crash** | Worker heartbeat stops updating, processing queue jobs remain stuck in `ACTIVE` state beyond timeout, OS process disappears | `WorkerHeartbeat` timestamp exceeds `heartbeat_interval * 3`, `Janitor` process scans for stale `processing` queues, monitoring process PID existence | High (jobs become orphaned) |\n| **Worker Graceful Shutdown Stall** | Worker receives SIGTERM but doesn't exit within graceful shutdown timeout, hangs during job cleanup | Internal shutdown timer in `Worker.stop()` method, monitoring for `Worker.state` stuck in `SHUTTING_DOWN` beyond timeout | Medium (blocks deployment rotations) |\n| **Job Handler Timeout** | Job execution exceeds `Job.timeout_seconds`, worker becomes unresponsive to heartbeats during execution | Per-job timeout thread/process monitoring, watchdog timer that sends SIGKILL to runaway job handlers | Medium (resource leak, blocks queue) |\n| **Job Data Corruption** | `Job.deserialize()` raises decoding errors, missing required fields, invalid data types | Validation in `QueueManager._validate_job()` during enqueue, checksum verification (CRC32) stored with serialized data, schema validation on deserialize | High (job lost, cannot be processed) |\n| **Redis Memory Exhaustion** | Redis returns OOM errors, `used_memory` approaches `maxmemory`, key evictions occur despite `noeviction` policy | Monitoring Redis metrics via `INFO memory`, alerting on memory usage >80%, tracking eviction count for `noeviction` policy | Critical (data loss possible) |\n| **Clock Skew** | Scheduled jobs fire at wrong times, retry delays miscalculated, heartbeat timestamps inconsistent across servers | NTP monitoring, comparing system time to Redis server time (`TIME` command), detecting scheduler runs that are significantly early/late | Medium (timing-sensitive operations fail) |\n| **Deadlock in Worker Concurrency** | Worker's thread/process pool stops making progress, all workers idle but jobs remain in queue, CPU usage drops to zero | Monitoring job completion rate dropping to zero while queue non-empty, watchdog thread that samples executor thread states | High (silent failure, hard to detect) |\n| **Queue Overflow** | `QueueConfig.max_length` exceeded, `QueueFullError` exceptions, enqueue operations fail | Monitoring queue lengths vs. configured limits, alerting on `QueueFullError` frequency, tracking producer backpressure | Medium (job submission blocked) |\n| **Silent Data Corruption in Redis** | Job data retrieved from Redis differs from what was stored, bit flips due to hardware issues | Store CRC32 checksum with each serialized job, verify on retrieval, Redis `DEBUG DIGEST` command for entire dataset (expensive) | Critical (data integrity compromised) |\n\n> **Architecture Decision: Multi-Layered Failure Detection**\n> **Context**: We need to detect failures promptly without overwhelming the system with monitoring overhead or generating excessive false positives. Different components have different failure characteristics requiring tailored detection approaches.\n> **Options Considered**:\n> 1. **Centralized Health Service**: A dedicated service periodically probes all components and aggregates status.\n> 2. **Decentralized Self-Reporting**: Each component reports its own health via heartbeets and logs, with consumers interpreting.\n> 3. **Hybrid Approach**: Components self-report basic health (heartbeats) while dedicated monitors perform deeper diagnostic checks.\n> **Decision**: Hybrid approach with component heartbeats for liveness and dedicated `Janitor` and `MetricsAggregator` for deeper health checks.\n> **Rationale**: Heartbeats provide cheap, continuous liveness signals with low latency. Dedicated diagnostic processes can perform more expensive checks (like stale job detection) without impacting primary data paths. This balances detection speed with system overhead.\n> **Consequences**: Requires implementing both heartbeat mechanisms and background maintenance processes. Failure detection latency depends on configuration: heartbeat intervals for quick detection vs. janitor scan intervals for orphaned job cleanup.\n\nDetection mechanisms are implemented across multiple components:\n- **Heartbeat System**: `WorkerHeartbeat` updates a Redis key with current timestamp at `heartbeat_interval`. The dashboard and alerting systems monitor these keys for staleness (timestamp > interval × 3).\n- **Janitor Process**: `Janitor` periodically scans all `processing` queues (e.g., `queue:processing:{worker_id}`) and checks if the associated worker's heartbeat is current. If not, it assumes the worker crashed and re-queues the jobs.\n- **Metric-Based Alerts**: `AlertManager` evaluates rules against metrics like queue growth rate, error rate, and worker count, firing alerts when thresholds are breached.\n- **Timeout Watchdogs**: Each `Worker` runs a watchdog timer per job that kills the job handler if it exceeds `Job.timeout_seconds`. The scheduler has its own timeout for the polling loop to detect stalls.\n- **Connection Health Checks**: `RedisClient` implements circuit breaker patterns, tracking consecutive failures and entering \"open\" state to prevent cascading failures during Redis outages.\n\n### Recovery and Compensation Strategies\n\n> **Mental Model: The Automobile Insurance Claims Process**  \n> When an accident occurs, insurance companies don't just fix the car—they follow a structured process: assess damage, determine fault, arrange repairs, provide rental cars, and handle medical bills. Each type of damage has a corresponding recovery procedure. Similarly, our system must have specific recovery strategies for each failure mode, not just restarting components but compensating for lost work, preserving data integrity, and restoring normal operation with minimal manual intervention.\n\nOnce a failure is detected, the system must execute appropriate recovery actions. The strategy depends on the failure type, component affected, and current system state. Recovery aims to restore system functionality while preserving job durability guarantees (at-least-once or at-most-once semantics) and maintaining data consistency. The following table maps failure modes to recovery strategies:\n\n| Failure Mode | Immediate Recovery Action | Compensation Strategy | Data Consistency Impact |\n|--------------|---------------------------|----------------------|-------------------------|\n| **Redis Connection Failure** | Exponential backoff reconnect, fail-fast on enqueue operations with clear error to producers | Buffer jobs in memory (with size limit) for later retry, enter degraded mode (pause scheduling, slow heartbeats) | Jobs may be lost if memory buffer overflows before Redis recovery. At-least-once semantics compromised during outage. |\n| **Worker Process Crash** | `Janitor` detects stale `processing` queue and moves jobs back to main queue with `Job.attempts` incremented | Worker supervisor (systemd, kubernetes) restarts worker process, `WorkerHeartbeat` cleanup removes orphaned heartbeat key | Job may be partially executed (non-idempotent operations require careful design). At-least-once semantics preserved. |\n| **Job Handler Timeout** | Watchdog sends SIGKILL to job handler process/thread, job marked as `FAILED` and passed to `RetryManager` | Retry with exponential backoff (if job idempotent), alert for manual inspection if timeout persistent for job type | If job not idempotent, duplicate side effects possible on retry. System maintains forward progress. |\n| **Job Data Corruption** | Job moved to dead letter queue with `VALIDATION_ERROR` flag, error details stored | Notification to operators via alert, manual inspection required to fix or discard job | Single job lost, but system continues processing other jobs. Dead letter queue provides audit trail. |\n| **Redis Memory Exhaustion** | Switch to `allkeys-lru` eviction policy temporarily, emergency purge of old job history, alert operators | Scale Redis memory, restart Redis with larger `maxmemory`, drain queues to secondary storage | Possible job loss if eviction occurs. System prioritizes recent jobs over history. |\n| **Clock Skew** | Alert operators to fix NTP synchronization, scheduler uses Redis time (`TIME` command) as authoritative source | Scheduler runs catch-up logic for missed scheduled jobs when clock corrects, recalculates retry timestamps | Scheduled jobs may fire late or early. Recovery ensures all jobs eventually enqueued. |\n| **Deadlock in Worker Concurrency** | Watchdog thread detects stall and kills entire worker process, triggering worker crash recovery | Reduce concurrency level, analyze job handlers for locking issues, implement thread dumps for diagnosis | All jobs currently processing are failed and retried. Temporary throughput reduction. |\n| **Queue Overflow** | Reject new jobs with `QueueFullError`, trigger scale-up alerts for workers, increase `max_length` temporarily | Auto-scale worker capacity, implement backpressure to producers, add overflow queue (secondary Redis instance) | Producers must handle rejection gracefully. System protects itself from overload. |\n| **Network Partition** | Continue processing jobs that don't require external services, pause enqueue of jobs requiring partitioned resources | When partition heals, reconciliation process compares job states across partitions, manual intervention may be needed | Split-brain may cause duplicate processing. Idempotency keys and deterministic job IDs reduce risk. |\n\n> **Design Principle: Graceful Degradation Over Catastrophic Failure**  \n> When components fail, the system should degrade functionality predictably rather than collapsing entirely. For example, during Redis outages, workers can continue processing jobs already in memory while the queue manager buffers new jobs locally (up to a limit). The dashboard can display cached metrics when Redis is unavailable. This principle ensures the system remains partially operational during partial failures.\n\nRecovery strategies are implemented through coordinated actions across components:\n\n1. **Automatic Retry with Exponential Backoff**: For transient failures (network timeouts, temporary resource unavailability), the `RetryManager` schedules retries with increasing delays using `BackoffCalculator.calculate_delay()`. This prevents overwhelming recovering systems with immediate retry storms.\n\n2. **Orphaned Job Recovery via Janitor**: The `Janitor` process runs periodically (e.g., every 30 seconds) and executes:\n   - `Janitor._get_stale_processing_queues()`: Scans Redis for all `queue:processing:{worker_id}` keys\n   - Checks if the corresponding worker heartbeat key (`worker:heartbeat:{worker_id}`) has expired\n   - For each stale processing queue, calls `Janitor._requeue_stale_jobs()` to:\n     1. Atomically move all jobs from the processing queue back to the main queue using `RPOPLPUSH` in a pipeline\n     2. Increment the `Job.attempts` count for each job\n     3. Update `Job.status` from `ACTIVE` to `PENDING`\n     4. Log the recovery event for auditing\n\n3. **Circuit Breaker for External Dependencies**: The `RedisClient` implements a circuit breaker pattern that tracks consecutive connection failures. After a threshold (e.g., 5 failures), the circuit opens and subsequent commands immediately fail fast without attempting network calls. After a reset timeout (e.g., 30 seconds), the circuit enters half-open state, allowing a test command through; if successful, the circuit closes and normal operation resumes.\n\n4. **Catch-Up Logic for Scheduler**: When the `Scheduler` starts or detects significant clock corrections, it calls `Scheduler.calculate_next_runs_for_all_schedules()` to recalculate `Schedule.next_run_at` for all schedules. It then runs `Scheduler.enqueue_due_jobs()` with an expanded time window (e.g., previous 24 hours) to enqueue any jobs missed during downtime. The uniqueness window (`Schedule.unique_window_seconds`) prevents duplicate enqueues for the same schedule period.\n\n5. **Dead Letter Queue Manual Intervention**: Jobs that exhaust retries or fail validation are moved to the dead letter queue (`dead:letter:{queue_name}`) where they persist for manual inspection. The dashboard provides operations to:\n   - View error details and stack traces\n   - Retry the job (moves back to main queue with reset attempts)\n   - Delete the job permanently\n   - Download job payload for debugging\n\n### Specific Edge Cases and Solutions\n\n> **Mental Model: The \"Corner Cases\" Playbook in Sports**  \n> Professional sports teams prepare for unusual game situations: what if the ball hits a bird mid-flight? What if a player's jersey gets torn? They have predetermined rules and procedures. Similarly, our system must anticipate rare but possible edge cases and have explicit solutions codified rather than leaving behavior undefined.\n\nEdge cases are scenarios that occur infrequently but can cause system instability, data loss, or undefined behavior if not handled explicitly. These often involve timing issues, race conditions, or unexpected combinations of normal events. The following table documents specific edge cases and their solutions:\n\n| Edge Case Scenario | Description | Potential Impact | Solution |\n|-------------------|-------------|------------------|----------|\n| **Worker Crash During Job Acknowledgement** | Worker successfully processes job but crashes after business logic completes but before removing job from processing queue. | Job could be re-queued by `Janitor` and executed twice (duplicate processing). | Use atomic \"process and acknowledge\" operations: store job result in Redis atomically with removal from processing queue via `RedisClient.pipeline()` multi-exec. |\n| **Clock Jump Forward/Backward** | System time changes abruptly due to NTP correction or manual clock adjustment. | Scheduled jobs may fire at wrong times, retry delays miscalculated, heartbeat comparisons invalid. | Use Redis `TIME` command as monotonic clock source for all scheduling decisions. For heartbeats, use system uptime (`time.monotonic()` in Python) for intervals, wall clock for absolute times. |\n| **Simultaneous Worker Startup with Same ID** | Two workers accidentally configured with same `worker_id` (e.g., duplicated container image). | Both update same heartbeat key, making liveness detection unreliable; both might process same jobs. | Generate unique `worker_id` automatically: `hostname:pid:timestamp` (e.g., `web01:12345:1678901234`). Include random component in heartbeat key to detect collisions. |\n| **Job Payload Contains Non-Serializable Objects** | Producer passes a database connection, file handle, or lambda function in job arguments. | `Job.serialize()` fails or produces corrupted serialization that cannot be deserialized later. | `QueueManager._validate_job()` performs deep validation using `json.dumps()` test before enqueue. Reject with `ValidationError` describing the problematic field. |\n| **Redis Failover During Transaction** | Redis primary fails after a multi-exec transaction starts but before it completes. | Partial transaction applied, leaving data inconsistent (e.g., job enqueued but job metadata not stored). | Use Redis Cluster with proper failover configuration (minimizes window). Implement idempotent operations where possible. Add reconciliation checks that scan for inconsistencies. |\n| **Queue Priority Starvation** | High-priority queue constantly has jobs, preventing lower-priority queues from ever being polled. | Low-priority jobs never get processed, effectively creating a denial of service for certain job types. | Implement weighted round-robin with dynamic adjustment: track time since last poll per queue and temporarily boost priority of starved queues. |\n| **Zombie Jobs in Processing Queue** | Job marked as `ACTIVE` but no worker is processing it (worker died without cleanup). | Job stuck forever, consuming memory, not retried. | `Janitor` scans all processing queues, not just those with heartbeat keys. Any job in processing queue for longer than `job_timeout * 2` is automatically re-queued. |\n| **Cron Expression Matching Daylight Saving Transition** | Cron schedule like \"30 2 * * *\" (2:30 AM daily) on a day when clocks spring forward from 2:00 to 3:00. | Job may be skipped (no 2:30 AM exists) or run twice (when falling back). | Use UTC for all cron evaluations, or use timezone-aware scheduling with `pytz` library that handles DST transitions correctly (skips or doubles as appropriate). |\n| **Retry Storm After Service Recovery** | External service is down, causing many jobs to fail and schedule retries. When service recovers, all retries fire simultaneously. | Thundering herd overwhelms recovering service, causing it to fail again. | Add jitter to retry delays (`BackoffCalculator` with `jitter_factor`). Implement congestion control: gradually increase retry concurrency after service recovery detected. |\n| **Memory Bloat from Job History** | Job history and metrics accumulate indefinitely in Redis, consuming all memory. | Redis crashes or evicts current job data, causing system failure. | Implement TTL on all job metadata keys (e.g., 7 days). `Janitor` process periodically purges old records. Move old data to cold storage (S3, database) before deletion. |\n| **Two Workers Simultaneously Poll Same Queue** | Both workers call `BRPOPLPUSH` at same instant, both get different jobs (normal). But race condition could cause both to get same job in some edge cases. | Same job processed twice. | Redis `BRPOPLPUSH` is atomic by design, but to be extra safe, use `RPOPLPUSH` with transaction and check for duplicate job IDs in processing queue before execution. |\n| **Job Depends on Deleted External Resource** | Job references a database record ID that gets deleted before job executes. | Job fails with \"record not found\" error, retries won't help. | Implement early validation in job handler: check resource existence, if missing, mark job as `COMPLETED` with warning (or move to dead letter with `SKIPPED` reason). |\n| **Scheduler Process Deadlock** | Scheduler holds Redis connection while waiting for queue manager, queue manager waiting for scheduler (circular dependency). | All scheduled jobs stop being enqueued, but system appears healthy (no crashes). | Use connection pooling with timeout, implement watchdog thread that monitors scheduler loop progress, break circular dependencies with async patterns. |\n| **ULID Collision** | Extremely rare case where two jobs generate same ULID (1 in 2^128 probability). | One job overwrites another in Redis storage, causing data loss. | Fall back to check-and-regenerate: after generating ULID, check `EXISTS` in Redis, if exists, append random suffix. Log collision as critical event. |\n\n> **Key Insight: Edge Cases Are Inevitable, Not Theoretical**  \n> In distributed systems running at scale, statistically improbable events occur regularly. The \"one in a million\" chance happens daily when processing millions of jobs. Therefore, every edge case in this table must be treated as a mandatory requirement, not an optional enhancement.\n\nSeveral edge cases require particular attention in implementation:\n\n**Clock Skew and Time Handling**: The system uses a hybrid time strategy:\n- **Redis as Authoritative Clock**: For all scheduled job times and retry timestamps, use `RedisClient.execute('TIME')` which returns server time unaffected by client clock skew.\n- **Monotonic Clocks for Intervals**: For heartbeat intervals and job timeouts, use `time.monotonic()` (Python) or `process.hrtime()` (Node.js) which are unaffected by system time adjustments.\n- **Timezone-Aware Scheduling**: Store `Schedule.timezone` as IANA timezone string (e.g., \"America/New_York\") and use `pytz` or equivalent to convert UTC to local time for cron evaluation, handling DST transitions correctly.\n\n**Idempotency and Duplicate Processing**: To handle potential duplicate job execution (from retries, worker crashes, or network partitions):\n- **Idempotency Keys**: Allow producers to pass an `idempotency_key` in `Job.metadata`. Store this in Redis with TTL (e.g., 24 hours). Before processing job, check if key exists; if yes, skip execution and return previous result.\n- **Deterministic Job IDs**: For recurring jobs, generate job ID from schedule ID plus execution timestamp (e.g., `{schedule_id}:{yyyymmddhhmm}`) to prevent duplicate enqueues for same schedule period.\n- **Transactional State Updates**: Use Redis pipelines to atomically update job status, store results, and remove from processing queue in a single operation.\n\n**Memory Management and Cleanup**: To prevent Redis memory exhaustion:\n- **TTL on All Keys**: Set appropriate TTLs: job metadata (7 days), worker heartbeats (2×heartbeat_interval), metrics (30 days), processing queues (job_timeout×3).\n- **Janitor Cleanup**: `Janitor` also removes expired keys beyond their TTL using `SCAN` and `DEL` operations in batches to avoid blocking Redis.\n- **Job History Archival**: For jobs older than TTL, move to cold storage (compress and store in S3) before deletion, maintaining audit trail without bloating Redis.\n\n**Network Partition Handling (Split-Brain)**: In a network partition where workers can't reach Redis but can reach each other:\n- **Continue Processing Local Jobs**: Workers can complete jobs already fetched into memory, storing results locally.\n- **Buffer Results for Reconciliation**: When partition heals, workers push buffered results to Redis with conflict resolution (last-write-wins with timestamp).\n- **Fencing Tokens**: Use Redis `INCR` on a fencing token key to detect stale workers after partition; workers with lower token values are considered stale and stop processing.\n\n### Implementation Guidance\n\n**A. Technology Recommendations for Error Handling**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Circuit Breaker | Manual failure counting with simple timeout | `tenacity` library with retry patterns or `pybreaker` for state machine implementation |\n| Connection Pooling | `redis-py` default connection pool | `redis-py` with `ConnectionPool` and `HealthCheck` enabled, custom retry logic |\n| Watchdog Timer | `threading.Timer` per job execution | `concurrent.futures` with `as_completed` and timeout, process-based isolation with `multiprocessing` |\n| Time Handling | `datetime.utcnow()` for timestamps, `time.monotonic()` for intervals | `pendulum` or `arrow` for timezone-aware operations, Redis `TIME` command for authoritative time |\n| Health Checks | Simple endpoint returning 200 OK | Comprehensive health check verifying Redis connectivity, queue depths, worker status |\n\n**B. Recommended File/Module Structure**\n\n```\nbackground_job_processor/\n├── src/\n│   ├── job_processor/\n│   │   ├── __init__.py\n│   │   ├── exceptions.py           # Custom exceptions: ValidationError, QueueFullError, etc.\n│   │   ├── circuit_breaker.py      # Circuit breaker pattern implementation\n│   │   ├── janitor.py              # Janitor process for cleanup\n│   │   ├── time_utils.py           # Time handling utilities, monotonic clocks\n│   │   └── reconciliation.py       # Reconciliation logic for network partitions\n│   ├── workers/\n│   │   ├── __init__.py\n│   │   ├── watchdog.py             # Job timeout watchdog\n│   │   └── heartbeat.py            # WorkerHeartbeat implementation\n│   └── redis/\n│       ├── __init__.py\n│       ├── client.py               # RedisClient with circuit breaker\n│       └── health.py               # Redis health checks\n└── tests/\n    └── test_error_handling.py      # Tests for failure scenarios\n```\n\n**C. Infrastructure Starter Code**\n\n```python\n# src/job_processor/exceptions.py\n\"\"\"Custom exceptions for the job processor.\"\"\"\n\nclass JobProcessorError(Exception):\n    \"\"\"Base exception for all job processor errors.\"\"\"\n    pass\n\nclass ValidationError(JobProcessorError):\n    \"\"\"Raised when job validation fails.\"\"\"\n    def __init__(self, message: str, field: str = None):\n        self.field = field\n        super().__init__(f\"{message} (field: {field})\" if field else message)\n\nclass QueueFullError(JobProcessorError):\n    \"\"\"Raised when queue is at maximum capacity.\"\"\"\n    def __init__(self, queue_name: str, max_length: int):\n        super().__init__(f\"Queue '{queue_name}' is full (max: {max_length})\")\n\nclass WorkerTimeoutError(JobProcessorError):\n    \"\"\"Raised when a worker operation times out.\"\"\"\n    pass\n\nclass RedisConnectionError(JobProcessorError):\n    \"\"\"Raised when Redis connection fails.\"\"\"\n    pass\n\n# src/job_processor/circuit_breaker.py\n\"\"\"Circuit breaker pattern implementation.\"\"\"\n\nimport time\nfrom enum import Enum\nfrom typing import Callable, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"\n    OPEN = \"open\"\n    HALF_OPEN = \"half_open\"\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker that tracks failures and opens circuit after threshold.\"\"\"\n    \n    def __init__(\n        self,\n        name: str,\n        failure_threshold: int = 5,\n        recovery_timeout: float = 30.0,\n        expected_exceptions: tuple = (Exception,)\n    ):\n        self.name = name\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.expected_exceptions = expected_exceptions\n        self.state = CircuitState.CLOSED\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.last_state_change = time.monotonic()\n    \n    def call(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        if self.state == CircuitState.OPEN:\n            # Check if recovery timeout has elapsed\n            if time.monotonic() - self.last_state_change > self.recovery_timeout:\n                logger.info(f\"Circuit {self.name} entering HALF_OPEN state\")\n                self.state = CircuitState.HALF_OPEN\n                self.last_state_change = time.monotonic()\n            else:\n                raise RedisConnectionError(f\"Circuit {self.name} is OPEN\")\n        \n        try:\n            result = func(*args, **kwargs)\n            # Success - reset failure count if we were in HALF_OPEN\n            if self.state == CircuitState.HALF_OPEN:\n                logger.info(f\"Circuit {self.name} reset to CLOSED after successful call\")\n                self.state = CircuitState.CLOSED\n                self.failure_count = 0\n            return result\n        except self.expected_exceptions as e:\n            self._record_failure()\n            raise\n    \n    def _record_failure(self):\n        \"\"\"Record a failure and update circuit state.\"\"\"\n        self.failure_count += 1\n        self.last_failure_time = time.monotonic()\n        \n        if (self.state == CircuitState.CLOSED and \n            self.failure_count >= self.failure_threshold):\n            logger.warning(f\"Circuit {self.name} opening after {self.failure_count} failures\")\n            self.state = CircuitState.OPEN\n            self.last_state_change = time.monotonic()\n        elif self.state == CircuitState.HALF_OPEN:\n            logger.warning(f\"Circuit {self.name} re-opening after HALF_OPEN failure\")\n            self.state = CircuitState.OPEN\n            self.last_state_change = time.monotonic()\n```\n\n**D. Core Logic Skeleton Code**\n\n```python\n# src/job_processor/janitor.py\nclass Janitor:\n    \"\"\"Maintenance process that cleans up stale jobs and expired data.\"\"\"\n    \n    def __init__(self, redis_client: RedisClient, scan_interval: int = 30):\n        self.redis = redis_client\n        self.scan_interval = scan_interval\n        self.running = False\n        self._thread = None\n    \n    def _get_stale_processing_queues(self) -> List[str]:\n        \"\"\"\n        Find all queue:processing lists where the associated worker is dead.\n        \n        Returns:\n            List of Redis keys for stale processing queues.\n        \n        TODO 1: Use SCAN to find all keys matching pattern 'queue:processing:*'\n        TODO 2: For each key, extract worker_id from the key name\n        TODO 3: Check if worker heartbeat key exists and is recent\n        TODO 4: If heartbeat missing or stale, add processing queue key to results\n        TODO 5: Also check for jobs stuck in processing queue longer than 2×timeout\n        \"\"\"\n        pass\n    \n    def _requeue_stale_jobs(self, processing_queue_key: str) -> int:\n        \"\"\"\n        Move all jobs from a stale processing queue back to its main queue.\n        \n        Args:\n            processing_queue_key: Redis key of the stale processing queue\n            \n        Returns:\n            Number of jobs requeued.\n        \n        TODO 1: Parse main queue name from processing_queue_key pattern\n        TODO 2: Use Redis pipeline to atomically:\n            a. Get all jobs from processing queue (LRANGE)\n            b. Delete processing queue (DEL)\n            c. For each job:\n                - Deserialize job\n                - Increment job.attempts\n                - Update job.status to PENDING\n                - Re-serialize job\n                - LPUSH to main queue\n        TODO 3: Log requeue operation with job count and worker ID\n        TODO 4: Return number of jobs requeued\n        \"\"\"\n        pass\n    \n    def _cleanup_expired_keys(self) -> int:\n        \"\"\"\n        Remove expired keys from Redis to prevent memory bloat.\n        \n        Returns:\n            Number of keys deleted.\n        \n        TODO 1: Scan for job metadata keys (pattern 'job:*:metadata')\n        TODO 2: Check TTL for each key, if TTL < 0 (expired), add to delete list\n        TODO 3: Scan for old heartbeat keys (pattern 'worker:heartbeat:*')\n        TODO 4: Check last update time, if older than 3×interval, add to delete list\n        TODO 5: Batch delete keys in groups of 100 to avoid blocking Redis\n        \"\"\"\n        pass\n    \n    def run_once(self) -> int:\n        \"\"\"\n        Execute a single cleanup cycle.\n        \n        Returns:\n            Total number of jobs requeued plus keys deleted.\n        \n        TODO 1: Call _get_stale_processing_queues() to get stale queues\n        TODO 2: For each stale queue, call _requeue_stale_jobs()\n        TODO 3: Call _cleanup_expired_keys()\n        TODO 4: Sum and return total operations performed\n        \"\"\"\n        pass\n    \n    def start(self) -> None:\n        \"\"\"Start the janitor's main loop in a background thread.\"\"\"\n        # TODO 1: Set self.running = True\n        # TODO 2: Create threading.Thread target=_run_loop\n        # TODO 3: Start thread, store in self._thread\n        # TODO 4: Log janitor startup\n        pass\n    \n    def stop(self) -> None:\n        \"\"\"Stop the janitor.\"\"\"\n        # TODO 1: Set self.running = False\n        # TODO 2: If thread exists, wait for it to join with timeout\n        # TODO 3: Log janitor shutdown\n        pass\n    \n    def _run_loop(self) -> None:\n        \"\"\"Main loop that runs cleanup periodically.\"\"\"\n        # TODO 1: While self.running is True:\n        # TODO 2: Call self.run_once()\n        # TODO 3: Sleep for self.scan_interval seconds\n        # TODO 4: Catch and log exceptions (don't let loop die on error)\n        pass\n\n# src/redis/client.py\nclass RedisClient:\n    \"\"\"Redis client wrapper with circuit breaker and error handling.\"\"\"\n    \n    def execute(self, command: str, *args, **kwargs) -> Any:\n        \"\"\"\n        Execute a Redis command with error handling.\n        \n        TODO 1: Get circuit breaker for this Redis instance\n        TODO 2: Wrap _execute_command with circuit_breaker.call()\n        TODO 3: Handle specific Redis errors:\n            - ConnectionError: increment circuit breaker, raise RedisConnectionError\n            - TimeoutError: increment circuit breaker, retry with backoff\n            - ResponseError: log and raise as-is (application error)\n        TODO 4: On success, return result\n        \"\"\"\n        pass\n    \n    def pipeline(self) -> 'RedisPipeline':\n        \"\"\"\n        Return a Redis pipeline for atomic operations.\n        \n        TODO 1: Create pipeline from underlying redis client\n        TODO 2: Wrap pipeline execution with circuit breaker\n        TODO 3: Implement context manager support (with statement)\n        TODO 4: Ensure pipeline executes atomically even with failures\n        \"\"\"\n        pass\n```\n\n**E. Language-Specific Hints**\n\n1. **Python's `time.monotonic()`** is crucial for intervals unaffected by system clock changes. Use it for heartbeat intervals and job timeouts.\n\n2. **Use `contextlib` context managers** for resource cleanup (Redis connections, file handles) to ensure resources are released even during exceptions.\n\n3. **`tenacity` library** provides advanced retry patterns with exponential backoff, jitter, and conditional retrying that can replace manual retry logic.\n\n4. **`pytz` or `zoneinfo` (Python 3.9+)** for timezone-aware scheduling. Always convert to UTC before storing in Redis.\n\n5. **`asyncio` timeout context** for implementing watchdogs: `async with asyncio.timeout(seconds): await job_handler()`.\n\n6. **Redis `SCRIPT EXISTS` and `EVALSHA`** for executing Lua scripts atomically, useful for complex recovery operations.\n\n**F. Milestone Checkpoint for Error Handling**\n\nAfter implementing error handling mechanisms, verify the system behaves correctly under failure conditions:\n\n1. **Test Redis Connection Failure**:\n   ```bash\n   # Start system normally, then stop Redis\n   $ sudo systemctl stop redis\n   # Observe circuit breaker opening after 5 failed commands\n   # Check logs for \"Circuit redis is OPEN\" messages\n   # Restart Redis and verify system automatically recovers\n   ```\n\n2. **Test Worker Crash Recovery**:\n   ```bash\n   # Start worker processing jobs\n   # Send SIGKILL to worker process\n   # Wait 30 seconds for janitor scan\n   # Verify orphaned jobs are re-queued with incremented attempt count\n   # Check dashboard shows job as \"recovered\" not \"lost\"\n   ```\n\n3. **Test Clock Skew Handling**:\n   ```bash\n   # Set system clock forward 1 hour\n   $ sudo date -s \"+1 hour\"\n   # Observe scheduler logs using Redis TIME command, not system time\n   # Verify scheduled jobs still fire at correct UTC time\n   # Reset clock and verify catch-up logic enqueues missed jobs\n   ```\n\n**G. Debugging Tips for Error Scenarios**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Jobs disappearing from queue | Worker crash during processing, janitor not running | Check `processing` queues in Redis (`KEYS queue:processing:*`). Look for stuck jobs. Verify janitor process is running and scanning. | Start janitor, manually move jobs back with `RPOPLPUSH`. |\n| All workers stuck idle | Deadlock in concurrency pool, all threads blocked | Check worker logs for timeout messages. Use thread dump (`import sys; sys._current_frames()`). Monitor CPU usage. | Reduce concurrency, implement watchdog that kills stuck workers, review job handlers for locking issues. |\n| Scheduled jobs never fire | Scheduler process dead, cron expression error, timezone mismatch | Check scheduler logs for errors. Verify `Schedule.next_run_at` in Redis. Test cron expression with online validator. | Restart scheduler, fix cron expression, ensure timezone configuration matches schedule. |\n| Redis memory constantly increasing | No TTL on keys, job history not cleaned, memory leak | Run `redis-cli info memory`. Check key TTLs with `TTL keyname`. Look for keys without expiration. | Implement TTL on all job keys, run janitor cleanup, increase Redis memory or implement LRU eviction. |\n| Duplicate job execution | Worker crash after processing but before acknowledgment, network partition | Check job attempt count in metadata. Look for same job ID in both results and queue. Check for split-brain scenario logs. | Implement atomic processing+acknowledgment, use idempotency keys, ensure proper network partition handling. |\n| Exponential backoff not working | Retry delay calculation error, jitter factor too large | Check `RetryManager` logs for scheduled retry times. Verify `BackoffCalculator.calculate_delay()` output. | Debug backoff calculation, ensure retry jobs are being enqueued to correct sorted set with proper score. |\n\n\n## Testing Strategy\n\n> **Milestone(s):** This section spans all five milestones, providing a comprehensive verification approach that ensures each component works correctly in isolation and as part of the integrated system.\n\nA robust testing strategy is the linchpin of reliable distributed systems. Unlike monolithic applications where failures manifest predictably, distributed job processing systems exhibit emergent behaviors: race conditions surface only under specific timing, retry mechanisms can amplify errors exponentially, and resource leaks accumulate invisibly until a catastrophic failure. This section establishes a systematic approach to verification that moves beyond naive unit testing to encompass stateful integration testing, fault injection, and real-world scenario simulation.\n\n### Testing Philosophy and Approach\n\n**Mental Model: The Building Inspector's Checklist** — Think of testing this system as inspecting a newly constructed building. Unit tests are like checking individual materials (bricks, beams, wiring) for quality. Integration tests verify that systems work together (plumbing connects to fixtures, electrical circuits power lights). End-to-end tests simulate actual occupancy (people living in the building, using all systems simultaneously). Stress tests are like inviting a thousand people to a party to see if the floor holds. Each inspection type catches different classes of defects that would otherwise remain hidden until catastrophic failure.\n\nOur testing philosophy embraces three core principles:\n\n1. **Determinism in a Non-Deterministic World**: Distributed systems are inherently non-deterministic due to timing variations, network delays, and concurrent operations. Our tests must either eliminate non-determinism through careful isolation or embrace it through statistical verification of probabilistic guarantees.\n\n2. **Stateful Verification Over Mock-Heavy Isolation**: While mocking external dependencies (Redis) simplifies unit tests, it creates false confidence. Redis operations have subtle atomicity and persistence characteristics that mocks often overlook. We favor integration tests with a real Redis instance in a controlled environment, supplemented by unit tests for pure business logic.\n\n3. **Fault Injection as First-Class Testing**: The system's reliability emerges from how it handles failures, not how it behaves in perfect conditions. We systematically inject failures at component boundaries (network timeouts, Redis unavailability, process crashes) to verify recovery mechanisms work as designed.\n\nThe testing pyramid for this system has a unique shape due to its distributed nature:\n\n![Testing Pyramid](./diagrams/system-component.svg)\n\n**Testing Layers and Their Purposes**\n\n| Layer | Scope | Key Techniques | Test Environment | Frequency |\n|-------|-------|----------------|------------------|-----------|\n| **Unit Tests** | Individual functions, classes, algorithms | Mock external dependencies, parameterized tests, property-based testing | Local development machine, CI pipeline | On every change |\n| **Integration Tests** | Interactions between 2-3 components (e.g., QueueManager + Redis) | Real Redis instance, containerized dependencies, transactional rollback | CI pipeline with Docker Compose | On every change |\n| **End-to-End Tests** | Full system workflows across all components | Multiple processes, real network communication, failure injection | Dedicated test environment | Daily or per milestone |\n| **Chaos Tests** | System behavior under injected failures | Network partitioning, process killing, Redis failure simulation | Isolated test cluster | Weekly or major releases |\n| **Performance Tests** | System behavior under load | Load generation, metric collection, scalability verification | Production-like environment | Monthly or architectural changes |\n\n**Test Environment Architecture**\n\nTo support this multi-layered approach, we establish a standardized test environment with the following characteristics:\n\n1. **Redis Isolation**: Each test class spins up a dedicated Redis instance on a unique port using Docker or Redis's built-in `--port` option. This prevents test interference and allows parallel test execution.\n\n2. **Transactional Rollback**: Integration tests wrap operations in Redis transactions or use the `FLUSHDB` command between tests to ensure clean state. For end-to-end tests, we use separate Redis databases selected via the `SELECT` command.\n\n3. **Time Control**: Scheduling and retry tests require precise control over time. We implement a mockable clock abstraction that allows tests to simulate time jumps, freeze time at specific moments, or accelerate time for testing long delays.\n\n4. **Process Management**: End-to-end tests that involve multiple worker processes use Python's `multiprocessing` module with careful cleanup to prevent orphaned processes. Each test starts fresh worker processes and verifies they terminate correctly.\n\n> **Key Insight**: The most valuable tests for distributed systems are **integration tests that verify distributed invariants** — properties that must hold true across multiple components despite concurrent operations. Examples include \"every job enqueued is eventually processed exactly once\" or \"the sum of jobs across all queues equals the total jobs in the system.\"\n\n**ADR: Testing Strategy for Redis Dependencies**\n\n> **Decision: Real Redis in Integration Tests with Clean State Isolation**\n> - **Context**: Redis operations have subtle behaviors (atomicity, expiration, transaction semantics) that are difficult to mock accurately. Mock-based tests provide false confidence, while requiring a Redis server adds complexity to the test environment.\n> - **Options Considered**:\n>   1. **Pure Mocking**: Mock all Redis calls with an in-memory simulation\n>   2. **Fake Redis Server**: Use a lightweight Redis-compatible server like `fakeredis`\n>   3. **Real Redis with Isolation**: Run actual Redis server with isolated databases/ports\n> - **Decision**: Use real Redis server with isolated state for integration tests, supplemented by mocking for unit tests of non-Redis logic.\n> - **Rationale**: Only real Redis guarantees correct behavior of atomic operations, Lua scripting, and persistence semantics. The risk of mock inaccuracy outweighs the setup complexity, especially with containerization making Redis trivial to run. We mitigate complexity through reusable test fixtures.\n> - **Consequences**: Tests require Redis installation or Docker, increasing CI setup complexity slightly. Tests run slower than with mocks but catch subtle bugs that mocks would miss. We maintain a fast unit test suite for business logic to keep overall test time reasonable.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Pure Mocking | Fast, no external dependencies | Misses Redis-specific behaviors, false confidence | No |\n| Fake Redis Server | Redis-like API, faster than real Redis | May differ from real Redis in edge cases | No |\n| Real Redis with Isolation | Accurate behavior, tests real interactions | Requires Redis installation, slower tests | **Yes** |\n\n**Common Testing Patterns**\n\nWe employ several recurring patterns across test suites:\n\n1. **Round-Trip Verification**: Enqueue a job, dequeue it, verify payload preservation — the most basic integration test that catches serialization bugs.\n\n2. **Concurrent Assertion**: Start multiple workers simultaneously, enqueue jobs, verify all jobs are processed exactly once despite concurrent access.\n\n3. **State Machine Validation**: For each job, verify it progresses through the correct states (`PENDING` → `ACTIVE` → `COMPLETED`) and that invalid transitions are prevented.\n\n4. **Timeout and Cancellation**: Verify that jobs timeout correctly, workers handle SIGTERM gracefully, and scheduled jobs can be canceled mid-execution.\n\n5. **Recovery Simulation**: Simulate worker crashes by killing processes mid-job, then verify the janitor process re-queues the orphaned jobs.\n\n### Milestone Implementation Checkpoints\n\nEach milestone includes specific verification checkpoints that validate core functionality. These checkpoints serve as progress indicators and quality gates before proceeding to the next milestone.\n\n**Milestone 1: Job Queue Core**\n\n> **Mental Model: The Conveyor Belt Factory Acceptance Test** — Imagine testing a newly installed conveyor belt system. You'd verify packages can be placed on the belt (enqueue), move along it in order (FIFO), be inspected without removal (peek), and be redirected to different lanes (multiple queues). You'd also check safety mechanisms like weight limits (payload size validation) and emergency stops (queue deletion).\n\n| Checkpoint | Test Description | Verification Method | Expected Outcome |\n|------------|------------------|---------------------|------------------|\n| **CP1.1** | Job serialization round-trip | Unit test: `Job.serialize()` → `Job.deserialize()` | Original job equals deserialized job with all fields preserved |\n| **CP1.2** | Basic enqueue/dequeue FIFO order | Integration test with Redis: Enqueue 3 jobs, dequeue 3 times | Jobs retrieved in same order as enqueued |\n| **CP1.3** | Multiple queue support | Integration test: Create 2 queues, enqueue jobs to each | Jobs only retrieved from queue they were enqueued to |\n| **CP1.4** | Payload size validation | Unit test: Attempt to enqueue job > 1MB | `ValidationError` raised before Redis interaction |\n| **CP1.5** | Queue inspection APIs | Integration test: Enqueue jobs, call `QueueManager.peek_queue()` and `get_queue_length()` | Peek returns correct jobs without removal, length matches count |\n| **CP1.6** | Job ID uniqueness | Integration test: Enqueue 1000 jobs concurrently | All job IDs are unique (no collisions) |\n| **CP1.7** | Atomic bulk enqueue | Integration test: Enqueue 10 jobs via `bulk_enqueue()`, kill Redis mid-operation | Either all jobs enqueued or none (no partial state) |\n\n**Implementation Verification for Milestone 1**\n\nAfter implementing Milestone 1, run the following verification suite:\n\n1. **Unit Test Suite**: Execute all unit tests for `Job` class serialization, validation logic, and `QueueManager` business logic (excluding Redis operations).\n   \n2. **Integration Test Suite**: Run the Redis-dependent tests which require a running Redis instance. These tests should pass with a clean Redis installation.\n\n3. **Manual Verification Script**: Execute a simple end-to-end workflow:\n   ```bash\n   # Start Redis locally\n   redis-server --port 6379\n   \n   # Run the verification script\n   python tests/milestone1_verification.py\n   ```\n\nThe verification script should:\n- Enqueue 100 jobs with varying payloads\n- Verify all jobs are stored in Redis with correct metadata\n- Peek at the queue to verify jobs are in FIFO order\n- Attempt to enqueue an oversized payload and confirm it's rejected\n- Display queue metrics showing correct job counts\n\n**Common Testing Pitfalls for Milestone 1**\n\n⚠️ **Pitfall: Testing with Production Redis**\n- **Description**: Accidentally running tests against a production Redis instance, corrupting real job data.\n- **Why Wrong**: Tests should never affect production data. This can cause data loss and service disruption.\n- **Fix**: Always use isolated Redis instances (different port or database) with automatic cleanup. Implement safety checks that refuse to connect to production Redis based on hostname or explicit test flag.\n\n⚠️ **Pitfall: Assuming Serialization is Trivial**\n- **Description**: Writing naive `json.dumps()`/`json.loads()` tests that miss edge cases like datetime objects or custom classes.\n- **Why Wrong**: Job payloads often contain complex Python types that don't serialize to JSON by default.\n- **Fix**: Use parameterized tests with diverse payload types: strings, numbers, lists, dicts, datetimes, Decimal, and custom objects with `to_dict()` methods. Verify round-trip equivalence, not just string equality.\n\n**Milestone 2: Worker Process**\n\n> **Mental Model: The Restaurant Kitchen Staff Evaluation** — When evaluating kitchen staff, you'd verify each cook can: 1) Receive orders from the ticket rail (poll queues), 2) Prepare dishes according to recipes (job handlers), 3) Work on multiple orders simultaneously (concurrency), 4) Clean up properly when closing (graceful shutdown), and 5) Signal when overwhelmed (heartbeat failure).\n\n| Checkpoint | Test Description | Verification Method | Expected Outcome |\n|------------|------------------|---------------------|------------------|\n| **CP2.1** | Worker polling from multiple queues | Integration test: Start worker, enqueue jobs to different priority queues | Worker processes jobs from higher-priority queues first |\n| **CP2.2** | Job handler dispatch | Unit test: Register multiple handlers, dispatch jobs | Correct handler called with correct arguments |\n| **CP2.3** | Graceful shutdown | Integration test: Send SIGTERM to worker while job is processing | Worker completes current job before exiting, leaves other jobs in queue |\n| **CP2.4** | Worker heartbeat | Integration test: Start worker, kill process abruptly, wait | Heartbeat key expires, stale worker detection triggers |\n| **CP2.5** | Concurrent job processing | Integration test: Worker with concurrency=3, enqueue 5 long-running jobs | 3 jobs process simultaneously, then remaining 2 |\n| **CP2.6** | Job timeout enforcement | Integration test: Enqueue job with 1-second timeout that sleeps for 5 seconds | Job killed after 1 second, marked as failed with timeout error |\n| **CP2.7** | Worker pause/resume | Integration test: Pause worker, enqueue jobs, resume worker | No jobs processed while paused, all jobs processed after resume |\n\n**Implementation Verification for Milestone 2**\n\nAfter implementing Milestone 2, run the following verification:\n\n1. **Worker Lifecycle Test**: Start a worker, enqueue jobs, verify processing, then stop the worker gracefully.\n   \n2. **Concurrency Stress Test**: Start worker with concurrency=5, enqueue 100 quick jobs, verify all processed correctly with no race conditions.\n\n3. **Failure Recovery Test**: Start worker processing a long job, kill the worker process abruptly (SIGKILL), then start a janitor process to verify orphaned job detection and re-queuing.\n\nThe verification should produce metrics showing:\n- All jobs processed exactly once\n- No jobs stuck in processing state after worker shutdown\n- Heartbeat updates occurring at correct intervals\n- Timeout errors properly recorded for long-running jobs\n\n**Milestone 3: Retry & Error Handling**\n\n> **Mental Model: The Customer Service Escalation Protocol Test** — Testing a customer service escalation system involves: 1) Verifying initial complaints are logged (job failure recording), 2) Ensuring follow-ups happen at increasing intervals (exponential backoff), 3) Confirming unresolved issues escalate to management (dead letter queue), and 4) Validating managers can review and re-open cases (manual retry).\n\n| Checkpoint | Test Description | Verification Method | Expected Outcome |\n|------------|------------------|---------------------|------------------|\n| **CP3.1** | Exponential backoff calculation | Unit test: `BackoffCalculator.calculate_delay()` with various attempt numbers | Delays follow 2^attempt pattern within configured bounds |\n| **CP3.2** | Retry scheduling | Integration test: Fail a job, verify it appears in Redis sorted set with correct score | Job scheduled for retry at correct future timestamp |\n| **CP3.3** | Retry limit enforcement | Integration test: Job with max_retries=3 fails repeatedly | After 3 failures, job moves to dead letter queue, not retried again |\n| **CP3.4** | Error serialization | Unit test: Job fails with exception, `Job.record_error()` called | Error details (type, message, traceback) stored in job metadata |\n| **CP3.5** | Dead letter queue storage | Integration test: Move job to dead letter queue, retrieve via API | Job retrievable with all error history intact |\n| **CP3.6** | Manual retry from dead letter | Integration test: Retry job from dead letter queue | Job re-enqueued with fresh attempt counter, processes successfully |\n| **CP3.7** | Jitter in backoff | Statistical test: Calculate 1000 retry delays with jitter enabled | Delays vary within ±jitter_factor range, not perfectly predictable |\n\n**Implementation Verification for Milestone 3**\n\nRetry system verification requires time manipulation to avoid actual waiting. We use a mock clock to accelerate time:\n\n1. **Retry Flow Test**: Configure a job to fail, verify it's scheduled for retry, advance mock clock to retry time, verify job executes again.\n\n2. **Dead Letter Integration Test**: Exhaust retries for a job, verify it moves to dead letter queue, retrieve it via API, manually retry it, verify success.\n\n3. **Error Context Preservation Test**: Fail a job with a complex exception containing nested attributes, verify all error context is preserved through serialization and available in dead letter queue.\n\nKey metrics to verify:\n- Retry delays follow exponential sequence with jitter\n- No jobs exceed their max_retries limit\n- Error count increments correctly with each failure\n- Dead letter queue grows only when retries exhausted\n\n**Milestone 4: Scheduling & Cron**\n\n> **Mental Model: The Calendar Alarm System Validation** — Testing a calendar alarm system involves: 1) Setting one-time alarms (delayed jobs), 2) Configuring recurring meetings (cron jobs), 3) Ensuring timezone conversions work correctly, 4) Preventing duplicate alerts for the same event (uniqueness constraints), and 5) Verifying missed alarms fire on system restart (catch-up logic).\n\n| Checkpoint | Test Description | Verification Method | Expected Outcome |\n|------------|------------------|---------------------|------------------|\n| **CP4.1** | Delayed job execution | Integration test with time mock: Schedule job for future, advance clock | Job enqueues only when scheduled time arrives, not before |\n| **CP4.2** | Cron expression parsing | Unit test: Parse various cron strings, calculate next run times | Next run times match expected schedule |\n| **CP4.3** | Recurring job execution | Integration test: Schedule cron job, advance clock through multiple periods | Job enqueues at each scheduled interval |\n| **CP4.4** | Timezone handling | Integration test: Schedule job in non-UTC timezone, verify enqueue times | Job enqueues according to specified timezone, not system time |\n| **CP4.5** | Uniqueness constraints | Integration test: Schedule unique job, attempt to enqueue duplicate within window | Duplicate job skipped (status=SKIPPED), not enqueued |\n| **CP4.6** | Scheduler catch-up | Integration test: Disable scheduler, miss scheduled time, re-enable scheduler | Missed job enqueued immediately on scheduler restart |\n| **CP4.7** | Schedule pause/resume | Integration test: Pause schedule, advance time, resume schedule | No jobs enqueued while paused, resumes correctly |\n\n**Implementation Verification for Milestone 4**\n\nScheduler verification requires careful time manipulation and Redis state inspection:\n\n1. **Cron Schedule Test**: Schedule a job with cron expression \"*/2 * * * *\" (every 2 minutes), advance mock clock 10 minutes, verify job enqueues 5 times.\n\n2. **Timezone Boundary Test**: Schedule a job for 2:30 AM in \"America/New_York\" timezone, verify correct UTC conversion accounting for daylight saving time if applicable.\n\n3. **Uniqueness Window Test**: Schedule unique job with 60-second window, enqueue it, attempt immediate re-enqueue, verify second attempt is skipped.\n\nThe verification should confirm:\n- Scheduled jobs appear in Redis sorted set with correct timestamps\n- Cron expressions calculate next run times accurately\n- Timezone conversions handle daylight saving transitions\n- Unique constraints prevent duplicates within specified windows\n\n**Milestone 5: Monitoring & Dashboard**\n\n> **Mental Model: The Air Traffic Control System Certification** — Certifying an air traffic control dashboard involves: 1) Verifying all aircraft appear on radar (real-time updates), 2) Ensuring alert thresholds trigger warnings (alerting), 3) Validating historical flight data is searchable (job history), 4) Confirming operators can intervene when needed (manual controls), and 5) Testing system doesn't overload under heavy traffic (dashboard performance).\n\n| Checkpoint | Test Description | Verification Method | Expected Outcome |\n|------------|------------------|---------------------|------------------|\n| **CP5.1** | Real-time metrics updates | Integration test: Enqueue/process jobs while dashboard connected via WebSocket/SSE | Dashboard updates within 1 second of state change |\n| **CP5.2** | Queue metrics calculation | Unit test: Simulate job flows, calculate `QueueMetrics` | All counts and rates computed correctly |\n| **CP5.3** | Alert rule evaluation | Integration test: Configure alert rule, trigger condition | Alert fires when threshold exceeded, respects cooldown |\n| **CP5.4** | Job history search | Integration test: Create jobs with various attributes, search with filters | Only matching jobs returned, pagination works |\n| **CP5.5** | Dashboard API rate limiting | Load test: Generate many concurrent dashboard requests | Requests throttled, Redis not overwhelmed |\n| **CP5.6** | Manual job controls | Integration test: Retry/delete dead letter jobs via dashboard API | Operations succeed, system state updates accordingly |\n| **CP5.7** | Metrics aggregation | Integration test: Run aggregator over sample data | Historical metrics computed, stored efficiently |\n\n**Implementation Verification for Milestone 5**\n\nDashboard verification involves both API testing and UI simulation:\n\n1. **Real-time Flow Test**: Start dashboard, enqueue jobs via API, verify WebSocket/SSE stream shows updates within expected latency.\n\n2. **Alerting Integration Test**: Configure alert for queue depth > 10, enqueue 11 jobs, verify alert triggers, acknowledge alert, verify alert state updates.\n\n3. **Search Performance Test**: Insert 10,000 jobs into history, search with complex filters, verify response time < 500ms with pagination.\n\nVerification should produce:\n- Dashboard displaying correct queue depths and worker status\n- Alert history showing triggered alerts with correct metadata\n- Job search returning accurate results with pagination\n- Manual retry operations successfully re-queuing dead letter jobs\n\n### Integration and End-to-End Tests\n\nIntegration tests verify interactions between components, while end-to-end tests simulate real-world usage scenarios across the entire system. These tests catch emergent behaviors that unit tests cannot.\n\n**Integration Test Architecture**\n\nOur integration test suite follows a layered approach:\n\n1. **Component Integration Tests**: Test interactions between 2-3 components (e.g., QueueManager + Worker + Redis).\n\n2. **Workflow Integration Tests**: Test complete workflows (enqueue → process → retry → dead letter) across multiple components.\n\n3. **Failure Scenario Tests**: Inject failures at specific points (network timeout, Redis crash, worker SIGKILL) and verify recovery.\n\n4. **Concurrency Tests**: Run multiple producers and consumers simultaneously to detect race conditions.\n\n**Key Integration Test Scenarios**\n\n| Test Scenario | Components Involved | Key Verification | Failure Injection Points |\n|---------------|---------------------|------------------|--------------------------|\n| **Happy Path Flow** | Producer → QueueManager → Redis → Worker → Result Store | Job processed exactly once, result stored | None (baseline verification) |\n| **Worker Crash Recovery** | Producer → Worker (crashed) → Janitor → New Worker | Orphaned job re-queued, processed exactly once | Kill worker mid-job execution |\n| **Redis Connection Loss** | All components with Redis circuit breaker | Operations fail fast, resume after Redis recovery | Simulate Redis network partition |\n| **Concurrent Priority Queues** | Multiple producers → QueueManager → Multiple workers | Higher-priority jobs processed first despite concurrency | Vary job durations and arrival times |\n| **Scheduler Catch-up** | Scheduler (disabled) → Time advance → Scheduler (enabled) | Missed jobs enqueued immediately, future jobs scheduled correctly | Disable scheduler during scheduled execution time |\n| **Dead Letter Manual Retry** | Failed job → DeadLetterQueue → Dashboard API → QueueManager → Worker | Manually retried job succeeds, removed from dead letter | None (manual operation verification) |\n\n**End-to-End Test Environment**\n\nEnd-to-end tests require a complete running system. We use Docker Compose to orchestrate:\n\n```yaml\n# test-environment/docker-compose.yaml\nversion: '3.8'\nservices:\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6380:6379\"  # Different port to avoid conflict\n  \n  worker-1:\n    build: .\n    command: [\"worker\", \"--queues\", \"high,low\", \"--concurrency\", \"2\"]\n    depends_on: [redis]\n  \n  worker-2:\n    build: .\n    command: [\"worker\", \"--queues\", \"medium\", \"--concurrency\", \"1\"]\n    depends_on: [redis]\n  \n  scheduler:\n    build: .\n    command: [\"scheduler\", \"--polling-interval\", \"5\"]\n    depends_on: [redis]\n  \n  dashboard:\n    build: .\n    command: [\"dashboard\", \"--port\", \"8080\"]\n    ports:\n      - \"8080:8080\"\n    depends_on: [redis]\n```\n\n**End-to-End Test Execution Pattern**\n\nEach end-to-end test follows this pattern:\n\n1. **Setup**: Start all services via Docker Compose, wait for health checks\n2. **Execution**: Perform test actions via API calls or CLI\n3. **Verification**: Query dashboard APIs, inspect Redis state, verify outcomes\n4. **Teardown**: Capture logs, stop services, clean up resources\n\n**Sample End-to-End Test: Scheduled Job with Retry**\n\nThis test verifies the complete lifecycle of a scheduled job that fails and retries:\n\n1. **Schedule a recurring job** via scheduler API to run every minute\n2. **Advance system time** by 5 minutes (using time mock in test environment)\n3. **Verify** 5 job instances were enqueued\n4. **Configure first 2 executions to fail** by registering a failing handler\n5. **Verify** failed jobs are scheduled for retry with exponential backoff\n6. **Advance time** to retry times\n7. **Configure retries to succeed** by changing handler\n8. **Verify** all jobs eventually complete successfully\n9. **Check dashboard** shows correct metrics: 5 jobs completed, 2 initial failures, 3 retries\n\n**Concurrency and Race Condition Testing**\n\nDistributed systems exhibit subtle race conditions. We employ specific techniques to detect them:\n\n| Technique | Implementation | What It Detects |\n|-----------|----------------|-----------------|\n| **Deterministic Simulation** | Use controlled thread scheduling with barriers | Race conditions in multi-threaded code |\n| **Statistical Assertion** | Run test 1000 times, verify invariant always holds | Probabilistic race conditions |\n| **Stress Testing** | Run system under heavy load with random delays | Resource leaks, deadlocks |\n| **Linearizability Checking** | Record operation history, verify appears sequential | Concurrent data structure bugs |\n\n**Example: Testing Job Uniqueness Under Concurrency**\n\n```python\n# Pseudocode for testing uniqueness constraint race condition\ndef test_unique_job_concurrent_enqueue():\n    # Schedule a unique job with 60-second window\n    schedule_id = scheduler.schedule_recurring_job(\n        job_type=\"unique_task\",\n        cron_expression=\"* * * * *\",\n        unique_key=\"task_1\",\n        unique_window_seconds=60\n    )\n    \n    # Simulate concurrent scheduler invocations (e.g., multiple scheduler processes)\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        futures = [executor.submit(scheduler._process_schedule, schedule) \n                  for _ in range(5)]\n        results = [f.result() for f in futures]\n    \n    # Only one should succeed in enqueuing the job\n    assert sum(1 for success, _ in results if success) == 1\n```\n\n**Performance and Load Testing**\n\nWhile not part of the core milestones, performance testing validates the architecture scales as expected:\n\n1. **Throughput Test**: Measure jobs processed per second with varying worker counts\n2. **Latency Test**: Measure enqueue-to-completion time under different loads\n3. **Redis Memory Test**: Monitor Redis memory growth with long-running job history\n4. **Dashboard Scalability Test**: Verify dashboard remains responsive with 10k+ jobs in history\n\n**Continuous Integration Pipeline**\n\nThe CI pipeline executes tests in this order:\n\n```mermaid\ngraph LR\n    A[Code Commit] --> B[Unit Tests<br/>Fast, no Redis]\n    B --> C[Integration Tests<br/>With Redis, slower]\n    C --> D[End-to-End Tests<br/>Docker Compose, slowest]\n    D --> E[Performance Tests<br/>Optional, on schedule]\n    E --> F[Release Candidate]\n```\n\n**Test Data Management**\n\nWe maintain realistic test data for comprehensive testing:\n\n| Data Set | Purpose | Characteristics |\n|----------|---------|-----------------|\n| **Job Payload Variants** | Test serialization edge cases | Large payloads, nested structures, binary data, special characters |\n| **Cron Expression Suite** | Test scheduler correctness | Simple intervals, complex ranges, timezone transitions, invalid expressions |\n| **Error Type Catalog** | Test error handling | Network errors, application exceptions, timeout exceptions, memory errors |\n| **Load Profiles** | Performance testing | Steady load, burst load, increasing load, spike load |\n\n**Test Maintenance Strategy**\n\nTests require maintenance as the system evolves:\n\n1. **Flaky Test Detection**: CI tracks test failure rates, automatically quarantines flaky tests\n2. **Test Data Generation**: Use property-based testing to generate edge cases automatically\n3. **Golden Master Testing**: For complex outputs (dashboard UI), maintain \"golden master\" snapshots\n4. **Test Coverage Tracking**: Monitor coverage of critical paths (error handling, recovery logic)\n\n> **Key Principle**: The most valuable tests are those that fail when the system behaves incorrectly but pass when it behaves correctly. This seems obvious but is often violated by tests that pass for the wrong reasons (e.g., mocks that don't simulate real behavior) or fail for irrelevant reasons (e.g., timing sensitivities in test environment).\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Testing Framework | `pytest` with `pytest-asyncio` | `pytest` with custom distributed test runner |\n| Redis for Tests | Docker Redis container | Redis cluster for distributed testing |\n| Mock Clock | `freezegun` library | Custom time abstraction with dependency injection |\n| Concurrency Testing | `threading`/`multiprocessing` | `asyncio` with controlled event loop scheduling |\n| Dashboard Testing | `requests` for API testing | `playwright` for full browser automation |\n| Load Testing | Locust.io for HTTP load testing | Custom job enqueue load generator |\n\n**B. Recommended File/Module Structure**\n\n```\nproject-root/\n├── tests/                           # All test code\n│   ├── unit/                        # Unit tests (no external dependencies)\n│   │   ├── test_job.py              # Job class tests\n│   │   ├── test_backoff_calculator.py\n│   │   └── test_cron_parser.py\n│   ├── integration/                 # Integration tests (with Redis)\n│   │   ├── conftest.py              # pytest fixtures (Redis setup)\n│   │   ├── test_queue_manager.py    # QueueManager + Redis tests\n│   │   ├── test_worker_integration.py\n│   │   └── test_retry_manager.py\n│   ├── e2e/                         # End-to-end tests\n│   │   ├── docker-compose.yaml      # Test environment\n│   │   ├── test_happy_path.py       # Full system workflow\n│   │   ├── test_failure_recovery.py\n│   │   └── test_scheduler_e2e.py\n│   ├── fixtures/                    # Test data fixtures\n│   │   ├── job_payloads.json\n│   │   ├── cron_expressions.yaml\n│   │   └── error_catalog.py\n│   └── utils/                       # Test utilities\n│       ├── redis_helpers.py         # Redis test helpers\n│       ├── time_helpers.py          # Mock time utilities\n│       └── process_helpers.py       # Worker process management\n├── src/                             # Main source code\n└── scripts/\n    └── run_tests.sh                 # Test execution script\n```\n\n**C. Infrastructure Starter Code**\n\nHere's a complete, reusable test infrastructure for Redis integration tests:\n\n```python\n# tests/utils/redis_helpers.py\n\"\"\"\nRedis test helpers for isolated, parallel test execution.\n\"\"\"\nimport redis\nimport random\nimport threading\nfrom contextlib import contextmanager\nfrom typing import Optional, Generator\n\nclass RedisTestContext:\n    \"\"\"Manages isolated Redis instances for parallel testing.\"\"\"\n    \n    def __init__(self, port: int = 6379, db: int = 0):\n        self.port = port\n        self.db = db\n        self._client: Optional[redis.Redis] = None\n        self._lock = threading.Lock()\n        \n    @property\n    def client(self) -> redis.Redis:\n        \"\"\"Lazy initialization of Redis client.\"\"\"\n        if self._client is None:\n            with self._lock:\n                if self._client is None:\n                    self._client = redis.Redis(\n                        host='localhost',\n                        port=self.port,\n                        db=self.db,\n                        decode_responses=True\n                    )\n                    # Ping to ensure connection works\n                    self._client.ping()\n        return self._client\n    \n    def flush(self) -> None:\n        \"\"\"Clear all data from the current database.\"\"\"\n        if self._client:\n            self._client.flushdb()\n    \n    def get_unique_key(self, prefix: str = \"test\") -> str:\n        \"\"\"Generate a unique Redis key for tests.\"\"\"\n        return f\"{prefix}:{threading.get_ident()}:{random.randint(0, 1000000)}\"\n    \n    @contextmanager\n    def pipeline(self) -> Generator[redis.Pipeline, None, None]:\n        \"\"\"Context manager for Redis pipeline with automatic execution.\"\"\"\n        pipe = self.client.pipeline()\n        try:\n            yield pipe\n            pipe.execute()\n        except Exception:\n            pipe.reset()\n            raise\n    \n    def close(self) -> None:\n        \"\"\"Close Redis connection.\"\"\"\n        if self._client:\n            self._client.close()\n            self._client = None\n\n# Global test context (can be overridden in tests)\n_test_context: Optional[RedisTestContext] = None\n\ndef get_test_context() -> RedisTestContext:\n    \"\"\"Get or create the global test context.\"\"\"\n    global _test_context\n    if _test_context is None:\n        # Use random DB to allow parallel test execution\n        _test_context = RedisTestContext(db=random.randint(10, 15))\n    return _test_context\n\n@contextmanager\ndef fresh_redis_context():\n    \"\"\"Context manager that provides a fresh Redis context and cleans up.\"\"\"\n    context = RedisTestContext(db=random.randint(100, 200))\n    try:\n        yield context\n    finally:\n        context.flush()\n        context.close()\n```\n\n**D. Core Test Skeleton Code**\n\nHere's a template for writing integration tests with proper setup/teardown:\n\n```python\n# tests/integration/test_queue_manager.py\n\"\"\"\nIntegration tests for QueueManager with real Redis.\n\"\"\"\nimport pytest\nfrom datetime import datetime\nfrom typing import List\n\nfrom src.queue_manager import QueueManager\nfrom src.job import Job, JobStatus\nfrom tests.utils.redis_helpers import fresh_redis_context\n\nclass TestQueueManagerIntegration:\n    \"\"\"Test QueueManager with Redis integration.\"\"\"\n    \n    @pytest.fixture\n    def queue_manager(self):\n        \"\"\"Create a QueueManager with fresh Redis context.\"\"\"\n        with fresh_redis_context() as redis_context:\n            # Create system config for testing\n            config = SystemConfig(\n                redis=RedisConfig(url=f\"redis://localhost:{redis_context.port}/{redis_context.db}\"),\n                queues=[QueueConfig(name=\"test-queue\", priority=1)],\n                workers=[],\n                max_payload_size=1024*1024,  # 1MB\n                job_history_size=1000,\n                retry_base_delay=1,\n                retry_max_attempts=3\n            )\n            manager = QueueManager(config=config)\n            yield manager\n            # Cleanup happens automatically via context manager\n    \n    def test_enqueue_and_dequeue_fifo(self, queue_manager):\n        \"\"\"Test basic FIFO ordering of jobs.\"\"\"\n        # TODO 1: Create 3 test jobs with sequential IDs\n        # TODO 2: Enqueue all jobs using queue_manager.enqueue_job()\n        # TODO 3: Use Redis BRPOP directly to dequeue jobs in order\n        # TODO 4: Verify jobs are returned in same order as enqueued\n        # TODO 5: Check job metadata is preserved through round-trip\n        pass\n    \n    def test_bulk_enqueue_atomicity(self, queue_manager):\n        \"\"\"Test that bulk enqueue is atomic (all or nothing).\"\"\"\n        # TODO 1: Create 10 test jobs\n        # TODO 2: Start a Redis transaction to monitor operations\n        # TODO 3: Call queue_manager.bulk_enqueue() with the jobs\n        # TODO 4: Simulate Redis failure mid-operation (mock redis.execute)\n        # TODO 5: Verify either all jobs enqueued or none (check queue length)\n        # TODO 6: Clean up test jobs if any were enqueued\n        pass\n    \n    def test_queue_priority_weighted_polling(self, queue_manager):\n        \"\"\"Test that workers poll higher-priority queues more frequently.\"\"\"\n        # TODO 1: Configure multiple queues with different priorities\n        # TODO 2: Enqueue equal numbers of jobs to each queue\n        # TODO 3: Simulate worker polling using queue_manager's internal method\n        # TODO 4: Record which queue is polled each time\n        # TODO 5: Verify higher-priority queues are polled more often\n        # TODO 6: Statistical test: run 1000 polls, check distribution matches weights\n        pass\n    \n    def test_oversized_payload_rejection(self, queue_manager):\n        \"\"\"Test that jobs larger than max_payload_size are rejected.\"\"\"\n        # TODO 1: Create a job with payload exceeding max_payload_size\n        # TODO 2: Attempt to enqueue the job\n        # TODO 3: Verify ValidationError is raised\n        # TODO 4: Verify no job was actually enqueued to Redis\n        # TODO 5: Test boundary case: payload exactly at limit should succeed\n        pass\n    \n    def test_queue_inspection_apis(self, queue_manager):\n        \"\"\"Test peek_queue and get_queue_length APIs.\"\"\"\n        # TODO 1: Enqueue 5 jobs to a test queue\n        # TODO 2: Call queue_manager.get_queue_length() - should return 5\n        # TODO 3: Call queue_manager.peek_queue(count=3) - should return first 3 jobs\n        # TODO 4: Verify peek doesn't remove jobs (call get_queue_length again)\n        # TODO 5: Dequeue one job, verify length becomes 4\n        # TODO 6: Test edge cases: peek with count larger than queue, peek empty queue\n        pass\n```\n\n**E. Language-Specific Hints for Python Testing**\n\n1. **Use `pytest` Fixtures for Dependency Injection**:\n   ```python\n   @pytest.fixture\n   def redis_client():\n       client = redis.Redis()\n       yield client\n       client.flushdb()\n       client.close()\n   ```\n\n2. **Mock Time with `freezegun`**:\n   ```python\n   from freezegun import freeze_time\n   \n   @freeze_time(\"2024-01-01 12:00:00\")\n   def test_scheduled_job():\n       # Time is frozen at Jan 1, 2024\n       schedule_job(run_at=datetime(2024, 1, 1, 12, 5, 0))\n       # Advance time 5 minutes\n       with freeze_time(\"2024-01-01 12:05:00\"):\n           process_due_jobs()  # Job should execute\n   ```\n\n3. **Test Concurrent Operations with `threading`**:\n   ```python\n   import threading\n   \n   def test_concurrent_enqueue():\n       results = []\n       lock = threading.Lock()\n       \n       def enqueue_job():\n           job_id = queue_manager.enqueue_job(job)\n           with lock:\n               results.append(job_id)\n       \n       threads = [threading.Thread(target=enqueue_job) for _ in range(10)]\n       for t in threads: t.start()\n       for t in threads: t.join()\n       \n       assert len(set(results)) == 10  # All job IDs unique\n   ```\n\n4. **Use `pytest.mark.parametrize` for Comprehensive Input Testing**:\n   ```python\n   @pytest.mark.parametrize(\"cron_expression,expected_next_run\", [\n       (\"*/15 * * * *\", \"2024-01-01 12:15:00\"),\n       (\"0 2 * * *\", \"2024-01-02 02:00:00\"),\n       (\"0 0 1 * *\", \"2024-02-01 00:00:00\"),\n   ])\n   def test_cron_expressions(cron_expression, expected_next_run):\n       # Test each cron expression\n       pass\n   ```\n\n**F. Milestone Checkpoint Verification**\n\nAfter completing each milestone, run these verification commands:\n\n```bash\n# Milestone 1: Job Queue Core\npytest tests/unit/test_job.py -v\npytest tests/integration/test_queue_manager.py -v\npython scripts/verify_milestone1.py\n\n# Milestone 2: Worker Process  \npytest tests/integration/test_worker_integration.py -v\npython scripts/start_worker_test.py --run-verification\n\n# Milestone 3: Retry & Error Handling\npytest tests/unit/test_backoff_calculator.py -v\npytest tests/integration/test_retry_manager.py -v\npython scripts/test_retry_flow.py\n\n# Milestone 4: Scheduling & Cron\npytest tests/unit/test_cron_parser.py -v\npytest tests/integration/test_scheduler.py -v\npython scripts/test_scheduled_jobs.py --time-mock\n\n# Milestone 5: Monitoring & Dashboard\npytest tests/integration/test_dashboard_api.py -v\npython scripts/load_test_dashboard.py --users 10 --duration 30\n```\n\n**Expected Outcomes for Milestone Verification:**\n\n| Milestone | Test Command | Success Indicators | Failure Symptoms |\n|-----------|--------------|-------------------|------------------|\n| **1** | `pytest tests/integration/test_queue_manager.py` | All tests pass, Redis keys created correctly | Jobs lost, serialization errors, queue priority not working |\n| **2** | `python scripts/start_worker_test.py` | Worker processes jobs, heartbeats update, graceful shutdown works | Jobs stuck in processing, orphaned workers, timeout not enforced |\n| **3** | `python scripts/test_retry_flow.py` | Failed jobs retry with increasing delays, dead letter queue receives exhausted jobs | Infinite retry loops, error context lost, manual retry fails |\n| **4** | `python scripts/test_scheduled_jobs.py` | Jobs enqueue at correct times, cron expressions evaluated correctly, uniqueness constraints work | Timezone issues, missed schedules, duplicate jobs enqueued |\n| **5** | `python scripts/load_test_dashboard.py` | Dashboard updates in real-time, alerts trigger correctly, search returns accurate results | Dashboard sluggish under load, alerts fire incorrectly, memory leaks |\n\n**G. Debugging Tips for Common Test Failures**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Tests pass locally but fail in CI** | Redis version differences, timezone issues, or race conditions | Check Redis logs in CI, add debug logging to tests, run tests with `--tb=short` | Use same Redis version in CI, fix timezone assumptions, add retries for timing-sensitive tests |\n| **Integration tests leave Redis data between tests** | Improper test isolation, missing `flushdb()` in teardown | Check Redis keys after test runs: `redis-cli keys '*'` | Use `fresh_redis_context()` fixture or ensure each test cleans up |\n| **Worker tests hang indefinitely** | Deadlock in worker shutdown, missing timeout in test | Add debug logging to worker state machine, use test timeout decorator | Ensure worker threads/processes properly join, add test timeouts |\n| **Time-sensitive tests are flaky** | System clock resolution, test execution time variability | Log actual vs expected timestamps, measure test execution time | Use mock time, add tolerance to timestamp comparisons |\n| **Concurrent tests fail randomly** | Race condition not caught in single-threaded tests | Run test 100 times, add `pytest-repeat`, use thread sanitizer | Add proper synchronization, use atomic operations, fix race condition |\n\n**Test Coverage Goals**\n\nAim for these coverage targets (measured via `pytest-cov`):\n\n| Component | Statement Coverage | Branch Coverage | Critical Path Coverage |\n|-----------|-------------------|-----------------|------------------------|\n| `Job` class | 100% | 100% | 100% (serialization round-trip) |\n| `QueueManager` | 95% | 90% | 100% (atomic operations) |\n| `Worker` | 90% | 85% | 100% (graceful shutdown) |\n| `RetryManager` | 95% | 90% | 100% (dead letter transition) |\n| `Scheduler` | 90% | 85% | 100% (cron evaluation) |\n| `Dashboard API` | 85% | 80% | 100% (alert triggering) |\n\n> **Final Testing Principle**: Write tests that fail when the system is broken and pass when it's working. This seems obvious but requires careful thought about what \"broken\" means for each component. A test that passes because mocks are too permissive is worse than no test at all — it provides false confidence.\n\n\n## Debugging Guide\n\n> **Milestone(s):** This section spans all five milestones, providing systematic diagnostic procedures for common implementation issues that arise during development and operation. The debugging guide transforms vague symptoms into specific causes and actionable fixes.\n\nBuilding a distributed background job processor involves coordinating multiple processes, network calls, and persistent state—a perfect environment for subtle bugs to emerge. When jobs aren't processing, workers are crashing, or retries aren't happening, systematic debugging is essential. This guide provides concrete procedures for identifying and resolving the most common issues, organized by symptom pattern and component.\n\nThink of debugging this system as being a **detective investigating a crime scene**. You arrive at the scene (a production issue), examine the evidence (logs, Redis data, metrics), look for patterns and anomalies, reconstruct timelines, and identify the perpetrator (the bug). Each piece of evidence tells part of the story, and your job is to connect them logically.\n\n### Common Bugs and Their Fixes\n\nThe following table categorizes common implementation bugs by their observable symptoms, underlying causes, and specific fixes. Each entry follows the pattern: you observe a symptom, investigate potential causes, and apply targeted corrections.\n\n| Symptom | Likely Cause | Investigation Steps | Fix |\n|---------|--------------|---------------------|-----|\n| **Jobs stuck in queue, workers idle** | 1. Workers polling wrong queue names<br>2. Queue priority weights misconfigured<br>3. Redis connection failures in workers<br>4. Workers paused or in shutdown state | 1. Check worker logs for connection errors<br>2. Verify `WorkerConfig.queues` matches queue names<br>3. Use `redis-cli LLEN queue:{name}` to confirm jobs exist<br>4. Check worker heartbeat status in Redis | 1. Ensure queue names in config match those used for enqueue<br>2. Reconfigure workers with correct queue list<br>3. Implement connection retry logic in `RedisClient`<br>4. Restart workers with `SIGTERM` then fresh start |\n| **Jobs disappearing without execution** | 1. Jobs moved to processing queue but worker crashed<br>2. `BRPOPLPUSH` without backup processing queue cleanup<br>3. Job validation failing silently during deserialization<br>4. Race condition in multi-worker dequeuing | 1. Check `queue:{name}:processing` lists in Redis<br>2. Verify Janitor process is running<br>3. Examine worker crash logs for segmentation faults<br>4. Check for `ValidationError` in enqueue logs | 1. Implement and run `Janitor` to requeue stale processing jobs<br>2. Use atomic operations for dequeue with proper error handling<br>3. Add detailed logging in `Job.deserialize()`<br>4. Use Redis transactions with `WATCH` for critical operations |\n| **Exponential retries happening too fast or not at all** | 1. Incorrect `attempt` counter in retry logic<br>2. Base delay miscalculation (seconds vs milliseconds)<br>3. Retry sorted set score using wrong timestamp format<br>4. Retry scheduler not polling frequently enough | 1. Inspect `job:{id}:attempts` key in Redis<br>2. Check `retry:{queue}` sorted set scores with `ZRANGE`<br>3. Verify `BackoffCalculator.calculate_delay()` implementation<br>4. Check scheduler polling interval vs retry delays | 1. Ensure `Job.attempts` increments atomically on each failure<br>2. Use Unix timestamps (seconds) for sorted set scores<br>3. Debug formula: `base_delay * (2 ** (attempt - 1))`<br>4. Configure scheduler to poll more frequently than minimum retry delay |\n| **Scheduled jobs not enqueuing at expected times** | 1. Timezone mismatch between scheduler and cron expression<br>2. `next_run_at` calculation error for cron expressions<br>3. Uniqueness constraint preventing enqueue<br>4. Scheduler process stopped or crashing | 1. Compare scheduler timezone with schedule timezone field<br>2. Check `schedule:{id}` hash for `next_run_at` value<br>3. Look for `SKIPPED` status in schedule history<br>4. Verify scheduler heartbeat in Redis | 1. Use UTC consistently or store timezone with each schedule<br>2. Test cron parsing with `croniter` library<br>3. Adjust `unique_window_seconds` or disable uniqueness<br>4. Implement scheduler supervisor with automatic restart |\n| **Memory usage growing unbounded in Redis** | 1. Missing TTL on job data after completion<br>2. Dead letter queue never purged<br>3. Job history accumulating without archival<br>4. Large payloads stored without size limits | 1. Run `redis-cli INFO memory` to check memory usage<br>2. Scan for keys without TTL: `redis-cli --scan | while read k; do echo \"$k: $(redis-cli TTL $k)\"; done`<br>3. Check `job:{id}:result` keys still present<br>4. Verify `max_payload_size` enforcement | 1. Set TTL on all temporary keys (job results, heartbeats)<br>2. Implement Janitor cleanup of old dead letter jobs<br>3. Add job history archival to external storage<br>4. Enforce payload size validation in `QueueManager._validate_job()` |\n| **Worker crashes without logging errors** | 1. Unhandled exception in job handler<br>2. Memory leak in long-running worker process<br>3. Signal handler conflicts with threading<br>4. Job timeout killing process instead of just thread | 1. Check system logs (`dmesg`, `journalctl`) for OOM killer<br>2. Add `try...except` with logging in worker main loop<br>3. Test with reduced concurrency<br>4. Monitor Python GC with `gc.get_count()` | 1. Wrap job execution in comprehensive error handler<br>2. Implement process-based isolation for memory-intensive jobs<br>3. Use `signal.signal()` carefully, preserve original handlers<br>4. Use thread/process timeout rather than process signal |\n| **Dashboard shows stale or incorrect metrics** | 1. Metrics aggregation interval too long<br>2. Race condition between metric collection and job state updates<br>3. Redis memory eviction deleting metric keys<br>4. WebSocket/SSE connection dropping silently | 1. Compare dashboard metrics with direct Redis queries<br>2. Check `metrics:queue:{name}:depth` key updates<br>3. Verify `MetricsAggregator` thread is running<br>4. Inspect browser console for WebSocket errors | 1. Reduce aggregation interval for near-real-time updates<br>2. Use atomic Redis operations for metric updates<br>3. Configure Redis maxmemory policy to `volatile-lru`<br>4. Implement WebSocket heartbeat and automatic reconnection |\n| **Duplicate execution of the same job** | 1. Worker crash after processing but before acknowledgment<br>2. Race condition in unique job constraint<br>3. Network partition causing split-brain worker behavior<br>4. Retry of non-idempotent operation without deduplication | 1. Check for duplicate job IDs in completed jobs<br>2. Examine processing queue for same job multiple times<br>3. Look for network outage timestamps in logs<br>4. Verify idempotency key usage | 1. Implement idempotency keys for critical operations<br>2. Use Redis `SETNX` for unique job locks<br>3. Add partition tolerance logic with fencing tokens<br>4. Ensure job handlers are idempotent when possible |\n| **Priority queue ordering not respected** | 1. Worker polling algorithm checking queues in fixed order<br>2. Priority weights incorrectly normalized<br>3. High-priority jobs stuck behind long-running low-priority jobs<br>4. Queue starvation due to constant high-priority influx | 1. Trace worker polling sequence in debug logs<br>2. Verify `QueueConfig.priority` values are positive integers<br>3. Check if worker concurrency is saturated<br>4. Monitor queue depths for imbalance | 1. Implement weighted random queue selection algorithm<br>2. Use priority values to calculate polling probabilities<br>3. Add preemption for critical high-priority jobs<br>4. Implement separate worker pools for different priority tiers |\n| **Graceful shutdown not working** | 1. Signal handler not properly registered<br>2. Worker waiting indefinitely on blocking Redis call<br>3. Current job not interruptible<br>4. Thread pool not shutting down correctly | 1. Test with `kill -TERM <pid>` and observe behavior<br>2. Check if worker is stuck in `BRPOP` with no timeout<br>3. Verify job timeout configuration<br>4. Examine thread states during shutdown | 1. Set timeout on blocking Redis operations<br>2. Implement cooperative cancellation for long-running jobs<br>3. Use `ThreadPoolExecutor` with `shutdown(wait=True)`<br>4. Add shutdown flag checked in main loop |\n\n> **Debugging Principle:** Always start with the simplest explanation. When jobs aren't processing, check that workers are actually running and connected to Redis before investigating complex race conditions. Use the scientific method: form a hypothesis, test it with a specific investigation, then refine.\n\n#### State Transition Anomalies\n\nJobs and workers follow specific state machines (as shown in ![Job State Machine](./diagrams/job-state-machine.svg) and ![Worker Process State Machine](./diagrams/worker-state-machine.svg)). When states don't transition as expected, use this diagnostic table:\n\n| Anomaly | Expected Transition | Actual State | Investigation | Resolution |\n|---------|-------------------|--------------|---------------|------------|\n| **Job stuck in ACTIVE** | ACTIVE → COMPLETED or FAILED | Remains ACTIVE for hours | 1. Check worker processing the job<br>2. Look for timeout configuration<br>3. Check for deadlock in job handler | 1. Implement job timeout with forced termination<br>2. Add deadlock detection in worker<br>3. Manual move to FAILED via dashboard |\n| **Worker stuck in ShuttingDown** | ShuttingDown → Stopped | Worker hangs during shutdown | 1. Check if current job has cleanup phase<br>2. Look for thread/child process not exiting<br>3. Check for finally blocks with infinite loops | 1. Add shutdown timeout to force exit<br>2. Implement interrupt handling for job threads<br>3. Use process-based isolation for clean termination |\n| **Job jumps from PENDING to DEAD_LETTER** | PENDING → ACTIVE → FAILED → RETRY_SCHEDULED → ... → DEAD_LETTER | Direct transition | 1. Check job `max_retries = 0`<br>2. Look for `RetryManager` bypass on specific errors<br>3. Check for manual move to dead letter | 1. Validate `max_retries` configuration<br>2. Review retry filter logic<br>3. Add audit log for manual operations |\n\n### Diagnostic Tools and Techniques\n\nEffective debugging requires the right tools and methodologies. This section outlines both built-in diagnostic capabilities and external tools that reveal system internals.\n\n#### Built-in Diagnostic Endpoints\n\nThe monitoring dashboard (Milestone 5) should include diagnostic endpoints that expose internal state without requiring direct Redis access. These endpoints are invaluable for production debugging.\n\n| Endpoint | Purpose | Data Format | Investigation Use |\n|----------|---------|-------------|-------------------|\n| `GET /api/debug/queue/{name}/contents` | Inspect jobs in a queue (including processing) | `{ \"pending\": [Job], \"processing\": [Job], \"retry_scheduled\": [Job] }` | Find specific stuck jobs, verify job ordering |\n| `GET /api/debug/worker/{id}/current_state` | Detailed worker internal state | `Worker` object with private fields | See what job a worker is executing, its thread states |\n| `GET /api/debug/job/{id}/trace` | Complete job lifecycle trace | `{ \"events\": [{\"timestamp\": \"...\", \"state\": \"...\", \"detail\": \"...\"}] }` | Reconstruct job history across retries and workers |\n| `POST /api/debug/job/{id}/inspect` | Force re-evaluation of job state | `{ \"current_redis_state\": {...}, \"suggested_actions\": [...] }` | Diagnose corrupted job state, suggest repair actions |\n| `GET /api/debug/redis/keys` | Scan Redis keys with pattern | `{ \"pattern\": \"...\", \"keys\": [...], \"ttls\": {...} }` | Find orphaned keys, check TTL expiration |\n| `GET /api/debug/metrics/raw` | Raw metrics before aggregation | `{ \"timestamp\": \"...\", \"counters\": {...}, \"gauges\": {...} }` | Verify metric collection vs aggregation discrepancies |\n\n#### Logging Strategy for Debugging\n\nStrategic logging transforms opaque failures into diagnosable events. Implement these logging practices:\n\n1. **Structured JSON Logging**: Each log entry includes:\n   - `component`: `\"queue_manager\"`, `\"worker\"`, `\"retry_manager\"`, etc.\n   - `job_id`: When relevant, the job identifier\n   - `worker_id`: When relevant, the worker identifier\n   - `queue`: Queue name involved\n   - `event`: Specific event type (e.g., `\"job_enqueued\"`, `\"job_failed\"`, `\"retry_scheduled\"`)\n   - `timestamp`: High-resolution timestamp\n   - `level`: Log level (DEBUG, INFO, WARNING, ERROR)\n\n2. **Correlation IDs**: Pass a `correlation_id` through the entire job lifecycle, from enqueue through all retries to completion. This allows tracing a job's journey across multiple log entries and components.\n\n3. **Debug-Level Verbosity**: Enable detailed debug logging for specific components without overwhelming production logs. Example debug events:\n   - Queue polling decisions (which queue was selected and why)\n   - Redis command execution and timing\n   - State machine transitions\n   - Lock acquisition and release\n\n4. **Health Check Logging**: Workers and schedulers should log periodic health status including:\n   - Memory usage (`psutil.Process().memory_info().rss`)\n   - Queue depths being monitored\n   - Uptime and processed job counts\n   - Redis connection latency\n\n#### External Diagnostic Tools\n\n| Tool | Purpose | Command Examples | Interpretation |\n|------|---------|------------------|----------------|\n| **redis-cli** | Direct Redis inspection | `redis-cli -h host -p port`<br>`INFO stats` for operations<br>`SLOWLOG GET 10` for slow commands | See command counts, connected clients, memory usage, slow operations |\n| **netstat/ss** | Network connection diagnosis | `netstat -tulpn \\| grep 6379`<br>`ss -t state established '( dport = :6379 )'` | Verify worker connections to Redis, check for connection leaks |\n| **strace/dtrace** | System call tracing | `strace -p <worker_pid> -f -e poll,select,read,write`<br>`sudo dtrace -n 'syscall::read:entry /pid == 12345/ { printf(\"%s\", arg0); }'` | See if process is blocked on I/O, identify hanging system calls |\n| **py-spy** (Python) | CPU profiling and sampling | `py-spy top --pid <worker_pid>`<br>`py-spy dump --pid <worker_pid>` | Identify CPU hotspots in worker code, view real-time stack traces |\n| **memory-profiler** (Python) | Memory usage tracking | Add `@profile` decorator to functions, run with `mprof` | Find memory leaks in job handlers or queue processing |\n| **tcpdump** | Network traffic analysis | `sudo tcpdump -i any port 6379 -w redis.pcap`<br>`tcpflow -c port 6379` | Debug Redis protocol issues, inspect actual commands sent |\n\n#### Interactive Debugging Session Protocol\n\nWhen faced with a complex bug, follow this structured protocol:\n\n1. **Reproduce the Issue**: \n   - Can you reproduce it consistently? If not, what are the conditions?\n   - Create a minimal test case that triggers the bug.\n\n2. **Gather Baseline Information**:\n   - System time synchronization (`ntpq -p` or `timedatectl`)\n   - Redis version and configuration (`redis-cli INFO server`)\n   - Worker configurations and versions\n   - Recent deployments or configuration changes\n\n3. **Enable Enhanced Logging**:\n   - Temporarily increase log level to DEBUG for affected components\n   - Add specific trace logs around the suspected area\n   - Capture Redis MONITOR output briefly (caution: performance impact)\n\n4. **Isolate the Component**:\n   - Does the issue occur with a single worker vs multiple?\n   - Does it affect all queues or just one?\n   - Is it time-dependent (only at certain times)?\n\n5. **Check Data Consistency**:\n   - Verify Redis data structures match expected schemas\n   - Check for orphaned keys without corresponding metadata\n   - Validate serialization/deserialization round-trips\n\n6. **Form and Test Hypotheses**:\n   - List possible causes in order of likelihood\n   - Design tests to eliminate each hypothesis\n   - Use the dashboard's debug endpoints to validate\n\n> **Debugging Insight:** Time-related bugs (scheduling, retries, timeouts) often stem from clock skew between systems. Always verify time synchronization between workers, schedulers, and Redis servers. Use NTP and consider using Redis time (`TIME` command) as a reference rather than system time.\n\n### Redis Data Inspection Guide\n\nRedis stores the entire system state. Knowing how to interpret this data is crucial for debugging. This section provides a comprehensive guide to Redis key patterns and how to inspect them.\n\n#### Key Naming Patterns\n\nThe system uses consistent key naming patterns. Understanding these patterns allows you to reconstruct system state from raw Redis data.\n\n| Key Pattern | Data Type | Purpose | Example |\n|-------------|-----------|---------|---------|\n| `queue:{name}` | List | Primary job queue (FIFO) | `queue:default`, `queue:high_priority` |\n| `queue:{name}:processing` | List | Jobs currently being processed (temporary) | `queue:default:processing` |\n| `job:{id}` | Hash | Job metadata and payload | `job:01HMAK5V2ZJX4Q6W8P3B0D9F7G` |\n| `job:{id}:result` | String (JSON) | Job execution result | `job:01HMAK5V2ZJX4Q6W8P3B0D9F7G:result` |\n| `job:{id}:errors` | List | Error history for failed job attempts | `job:01HMAK5V2ZJX4Q6W8P3B0D9F7G:errors` |\n| `retry:{queue}` | Sorted Set | Scheduled retries (score = execution timestamp) | `retry:default`, `retry:email` |\n| `dead_letter:{queue}` | List | Permanently failed jobs | `dead_letter:default` |\n| `schedule:{id}` | Hash | Schedule definition | `schedule:daily_report` |\n| `schedule:{id}:history` | List | Schedule execution history | `schedule:daily_report:history` |\n| `worker:{id}` | Hash | Worker heartbeat and status | `worker:host123-pid4567` |\n| `worker:{id}:jobs` | List | Recent jobs processed by worker | `worker:host123-pid4567:jobs` |\n| `metrics:queue:{name}:depth` | String | Current queue depth (updated periodically) | `metrics:queue:default:depth` |\n| `metrics:queue:{name}:throughput` | Sorted Set | Timestamped throughput samples | `metrics:queue:default:throughput` |\n| `alert:{id}` | Hash | Active alert information | `alert:queue_depth_high_01HMAK5V2Z` |\n| `unique:{key}` | String | Unique job constraint lock | `unique:daily_report_2024_01_15` |\n| `config:{component}` | Hash | Runtime configuration | `config:system`, `config:alert_rules` |\n\n#### Inspection Commands and Scripts\n\nUse these Redis commands to inspect system health and diagnose issues:\n\n**Basic Queue Inspection:**\n```bash\n# Check queue lengths\nredis-cli LLEN queue:default\nredis-cli LLEN queue:high_priority\n\n# Peek at next jobs in queue (without removing)\nredis-cli LRANGE queue:default 0 4 | while read line; do echo \"$line\" | python3 -m json.tool; done\n\n# Check processing queue for stuck jobs\nredis-cli LLEN queue:default:processing\nredis-cli LRANGE queue:default:processing 0 -1\n\n# Check retry schedule\nredis-cli ZRANGE retry:default 0 -1 WITHSCORES\n```\n\n**Job State Investigation:**\n```bash\n# Find all jobs in a particular state\nredis-cli --scan --pattern 'job:*' | while read key; do \n  status=$(redis-cli HGET $key status);\n  if [ \"$status\" = '\"ACTIVE\"' ]; then\n    echo \"Active job: $key\";\n    redis-cli HGET $key started_at;\n  fi;\ndone\n\n# Get complete job details\nredis-cli HGETALL job:01HMAK5V2ZJX4Q6W8P3B0D9F7G\n\n# Check job errors\nredis-cli LRANGE job:01HMAK5V2ZJX4Q6W8P3B0D9F7G:errors 0 -1\n```\n\n**Worker Health Check:**\n```bash\n# List all active workers\nredis-cli --scan --pattern 'worker:*' | grep -v ':jobs$' | while read key; do\n  heartbeat=$(redis-cli HGET $key last_heartbeat);\n  echo \"$key: $heartbeat\";\ndone\n\n# Check for stale workers (heartbeat > 60 seconds ago)\ncurrent_time=$(date +%s)\nredis-cli --scan --pattern 'worker:*' | grep -v ':jobs$' | while read key; do\n  heartbeat=$(redis-cli HGET $key last_heartbeat);\n  if [ ! -z \"$heartbeat\" ]; then\n    age=$((current_time - heartbeat));\n    if [ $age -gt 60 ]; then\n      echo \"STALE WORKER: $key (age: ${age}s)\";\n    fi;\n  fi;\ndone\n```\n\n**Scheduler State Verification:**\n```bash\n# Check all schedules\nredis-cli --scan --pattern 'schedule:*' | grep -v ':history$' | while read key; do\n  next_run=$(redis-cli HGET $key next_run_at);\n  echo \"$key: $next_run\";\ndone\n\n# Check schedule history\nredis-cli LRANGE schedule:daily_report:history 0 -1\n```\n\n**Memory Usage Analysis:**\n```bash\n# Check Redis memory info\nredis-cli INFO memory\n\n# Find largest keys\nredis-cli --scan --pattern '*' | while read key; do\n  size=$(redis-cli MEMORY USAGE $key);\n  echo \"$size $key\";\ndone | sort -n | tail -20\n\n# Check key TTLs\nredis-cli --scan --pattern 'job:*:result' | while read key; do\n  ttl=$(redis-cli TTL $key);\n  if [ $ttl -eq -1 ]; then\n    echo \"NO TTL: $key\";\n  fi;\ndone\n```\n\n#### Common Redis Data Anomalies\n\n| Anomaly | Detection Method | Root Cause | Repair Action |\n|---------|------------------|------------|---------------|\n| **Orphaned processing jobs** | `LLEN queue:*:processing` > 0 with no corresponding active worker | Worker crash during job execution | Run `Janitor.run_once()` or manually move jobs: `redis-cli RPOPLPUSH queue:default:processing queue:default` |\n| **Retry scores in past** | `ZRANGE retry:default 0 0 WITHSCORES` shows score < current time | Scheduler not running or too slow | Manually enqueue: `redis-cli ZRANGE retry:default 0 0` to get job ID, then requeue |\n| **Duplicate unique keys** | `redis-cli GET unique:{key}` exists but schedule not executed | Race condition in uniqueness check | Delete key: `redis-cli DEL unique:{key}` and manually enqueue job |\n| **Job hash missing fields** | `redis-cli HGETALL job:{id}` shows incomplete field set | Serialization bug or partial update | Reconstruct from backup or mark as failed: `redis-cli HSET job:{id} status '\"FAILED\"'` |\n| **Metrics keys without TTL** | `redis-cli TTL metrics:queue:default:depth` returns -1 | Metrics aggregation not setting expiration | Set TTL: `redis-cli EXPIRE metrics:queue:default:depth 3600` |\n| **Zombie worker entries** | `worker:{id}` exists but process is dead | Worker crash without cleanup | Remove entry: `redis-cli DEL worker:{id} worker:{id}:jobs` |\n\n#### Redis Lua Scripts for Complex Diagnostics\n\nFor complex diagnostics, use Redis Lua scripts to atomically check conditions:\n\n**Script to find jobs stuck in processing with dead workers:**\n```lua\n-- find_stale_processing.lua\nlocal processing_keys = redis.call('KEYS', 'queue:*:processing')\nlocal results = {}\nlocal current_time = tonumber(redis.call('TIME')[1])\n\nfor _, key in ipairs(processing_keys) do\n  local queue_name = string.match(key, 'queue:(.+):processing')\n  local worker_key = 'worker:active:' .. queue_name  -- hypothetical worker mapping\n  \n  -- Check if worker for this queue is alive\n  local worker_alive = redis.call('EXISTS', worker_key)\n  \n  if worker_alive == 0 then\n    local job_count = redis.call('LLEN', key)\n    if job_count > 0 then\n      table.insert(results, {queue_name, job_count, key})\n    end\n  end\nend\n\nreturn results\n```\n\n**Script to validate job state consistency:**\n```lua\n-- validate_job_state.lua\nlocal job_id = ARGV[1]\nlocal job_key = 'job:' .. job_id\nlocal job_status = redis.call('HGET', job_key, 'status')\n\nlocal inconsistencies = {}\n\n-- Check if job is ACTIVE but not in any processing queue\nif job_status == '\"ACTIVE\"' then\n  local found = false\n  local processing_keys = redis.call('KEYS', 'queue:*:processing')\n  \n  for _, key in ipairs(processing_keys) do\n    local items = redis.call('LRANGE', key, 0, -1)\n    for _, item in ipairs(items) do\n      if string.find(item, job_id) then\n        found = true\n        break\n      end\n    end\n    if found then break end\n  end\n  \n  if not found then\n    table.insert(inconsistencies, 'Job marked ACTIVE but not in any processing queue')\n  end\nend\n\n-- Check if job is PENDING but not in any main queue\nif job_status == '\"PENDING\"' then\n  -- Similar logic checking main queues\nend\n\nreturn inconsistencies\n```\n\n> **Redis Inspection Principle:** Redis is the single source of truth for the system. When in doubt, examine Redis data directly. The dashboard and logs are derived views that might have their own bugs or lag. Always validate against Redis state.\n\n### Implementation Guidance\n\nThis section provides concrete tools and code to implement the debugging capabilities described above.\n\n#### Technology Recommendations\n\n| Diagnostic Need | Simple Option | Advanced Option |\n|-----------------|---------------|-----------------|\n| Log aggregation | Structured JSON to files + `tail -f` | Elasticsearch + Logstash + Kibana (ELK) |\n| Metrics collection | Redis time-series with periodic aggregation | Prometheus + Grafana for real-time metrics |\n| Distributed tracing | Correlation IDs in logs | OpenTelemetry with Jaeger/Zipkin |\n| Interactive debugging | Python debugger (pdb) | Remote debugging with debugpy |\n| Performance profiling | cProfile for CPU, memory-profiler for memory | Py-Spy for sampling, Scalene for comprehensive |\n| Alerting | Simple threshold checks in scheduler | Prometheus Alertmanager with complex rules |\n\n#### Debug Endpoint Implementation\n\nCreate a dedicated debug module with these endpoints (protected in production):\n\n```python\n# debug_api.py\nfrom fastapi import APIRouter, HTTPException, BackgroundTasks\nfrom typing import List, Optional, Dict, Any\nimport json\nimport redis\nfrom datetime import datetime, timedelta\nimport asyncio\n\nfrom .models import (\n    Job, JobStatus, Worker, QueueMetrics, SystemConfig,\n    QueueManager, WorkerConfig, RedisClient\n)\nfrom .janitor import Janitor\n\nrouter = APIRouter(prefix=\"/api/debug\", tags=[\"debug\"])\n\n@router.get(\"/queue/{queue_name}/contents\")\nasync def get_queue_contents(\n    queue_name: str,\n    include_processing: bool = True,\n    include_retry: bool = False,\n    limit: int = 50\n) -> Dict[str, Any]:\n    \"\"\"\n    Inspect all jobs in a queue, including processing and retry queues.\n    Useful for diagnosing stuck jobs.\n    \"\"\"\n    redis_client = RedisClient.get_instance()\n    \n    # Get pending jobs\n    pending_key = f\"queue:{queue_name}\"\n    pending_jobs_raw = redis_client.lrange(pending_key, 0, limit - 1)\n    pending_jobs = []\n    for job_data in pending_jobs_raw:\n        try:\n            job = Job.deserialize(job_data.decode('utf-8'))\n            pending_jobs.append(job.to_dict())\n        except Exception as e:\n            pending_jobs.append({\"error\": str(e), \"raw\": job_data[:100]})\n    \n    result = {\"pending\": pending_jobs}\n    \n    # Get processing jobs\n    if include_processing:\n        processing_key = f\"queue:{queue_name}:processing\"\n        processing_jobs_raw = redis_client.lrange(processing_key, 0, limit - 1)\n        processing_jobs = []\n        for job_data in processing_jobs_raw:\n            try:\n                job = Job.deserialize(job_data.decode('utf-8'))\n                processing_jobs.append(job.to_dict())\n            except Exception as e:\n                processing_jobs.append({\"error\": str(e), \"raw\": job_data[:100]})\n        result[\"processing\"] = processing_jobs\n    \n    # Get retry scheduled jobs\n    if include_retry:\n        retry_key = f\"retry:{queue_name}\"\n        retry_jobs = redis_client.zrange(\n            retry_key, 0, limit - 1, withscores=True\n        )\n        retry_list = []\n        for job_data, score in retry_jobs:\n            try:\n                job = Job.deserialize(job_data.decode('utf-8'))\n                retry_list.append({\n                    \"job\": job.to_dict(),\n                    \"scheduled_for\": datetime.fromtimestamp(score)\n                })\n            except Exception as e:\n                retry_list.append({\"error\": str(e), \"raw\": job_data[:100]})\n        result[\"retry_scheduled\"] = retry_list\n    \n    return result\n\n@router.get(\"/job/{job_id}/trace\")\nasync def get_job_trace(job_id: str) -> Dict[str, Any]:\n    \"\"\"\n    Reconstruct the complete lifecycle of a job.\n    \"\"\"\n    redis_client = RedisClient.get_instance()\n    \n    # Get job metadata\n    job_key = f\"job:{job_id}\"\n    job_data = redis_client.hgetall(job_key)\n    if not job_data:\n        raise HTTPException(status_code=404, detail=\"Job not found\")\n    \n    # Decode byte strings\n    job_data_decoded = {\n        k.decode('utf-8'): v.decode('utf-8') \n        for k, v in job_data.items()\n    }\n    \n    # Get error history\n    errors_key = f\"job:{job_id}:errors\"\n    errors = redis_client.lrange(errors_key, 0, -1)\n    error_history = []\n    for error_json in errors:\n        try:\n            error_history.append(json.loads(error_json.decode('utf-8')))\n        except:\n            error_history.append({\"raw\": error_json.decode('utf-8', errors='replace')})\n    \n    # Get result if exists\n    result_key = f\"job:{job_id}:result\"\n    result = redis_client.get(result_key)\n    result_data = None\n    if result:\n        try:\n            result_data = json.loads(result.decode('utf-8'))\n        except:\n            result_data = {\"raw\": result.decode('utf-8', errors='replace')}\n    \n    # Reconstruct timeline from metadata\n    timeline = []\n    \n    # Created event\n    if 'created_at' in job_data_decoded:\n        timeline.append({\n            \"timestamp\": job_data_decoded['created_at'],\n            \"event\": \"created\",\n            \"queue\": job_data_decoded.get('queue', 'unknown')\n        })\n    \n    # Started event  \n    if 'started_at' in job_data_decoded:\n        timeline.append({\n            \"timestamp\": job_data_decoded['started_at'],\n            \"event\": \"started\",\n            \"status\": \"ACTIVE\"\n        })\n    \n    # Error events\n    for i, error in enumerate(error_history):\n        timeline.append({\n            \"timestamp\": error.get('timestamp', 'unknown'),\n            \"event\": f\"failed_attempt_{i+1}\",\n            \"error\": error.get('error_type', 'Unknown'),\n            \"message\": error.get('message', '')[:100]\n        })\n    \n    # Completed event\n    if 'completed_at' in job_data_decoded:\n        timeline.append({\n            \"timestamp\": job_data_decoded['completed_at'],\n            \"event\": \"completed\",\n            \"status\": job_data_decoded.get('status', 'unknown')\n        })\n    \n    # Sort timeline\n    timeline.sort(key=lambda x: x.get('timestamp', ''))\n    \n    return {\n        \"job_id\": job_id,\n        \"metadata\": job_data_decoded,\n        \"error_history\": error_history,\n        \"result\": result_data,\n        \"timeline\": timeline\n    }\n\n@router.post(\"/janitor/run\")\nasync def run_janitor(background_tasks: BackgroundTasks) -> Dict[str, Any]:\n    \"\"\"\n    Manually trigger the janitor process to clean up stale jobs.\n    Returns immediately, runs in background.\n    \"\"\"\n    async def cleanup_task():\n        janitor = Janitor(RedisClient.get_instance())\n        requeued = janitor.run_once()\n        return requeued\n    \n    task = asyncio.create_task(cleanup_task())\n    background_tasks.add_task(task)\n    \n    return {\n        \"status\": \"started\",\n        \"message\": \"Janitor cleanup running in background\"\n    }\n\n@router.get(\"/redis/keys\")\nasync def scan_redis_keys(\n    pattern: str = \"*\",\n    limit: int = 100,\n    with_ttl: bool = False\n) -> Dict[str, Any]:\n    \"\"\"\n    Scan Redis keys matching pattern. Use with caution in production.\n    \"\"\"\n    redis_client = RedisClient.get_instance()\n    \n    # Use SCAN instead of KEYS for production safety\n    cursor = 0\n    keys = []\n    ttls = {}\n    \n    while True:\n        cursor, batch = redis_client.scan(\n            cursor=cursor, match=pattern, count=100\n        )\n        keys.extend([k.decode('utf-8') for k in batch])\n        \n        if with_ttl:\n            for key in batch:\n                ttl = redis_client.ttl(key)\n                ttls[key.decode('utf-8')] = ttl\n        \n        if cursor == 0 or len(keys) >= limit:\n            break\n    \n    keys = keys[:limit]\n    \n    return {\n        \"pattern\": pattern,\n        \"count\": len(keys),\n        \"keys\": keys,\n        \"ttls\": ttls if with_ttl else None\n    }\n\n@router.post(\"/worker/{worker_id}/signal\")\nasync def signal_worker(\n    worker_id: str,\n    signal: str = \"status\"\n) -> Dict[str, Any]:\n    \"\"\"\n    Send a signal to a worker (status, pause, resume, shutdown).\n    In production, this would require authentication and authorization.\n    \"\"\"\n    if signal not in [\"status\", \"pause\", \"resume\", \"shutdown\"]:\n        raise HTTPException(status_code=400, detail=\"Invalid signal\")\n    \n    redis_client = RedisClient.get_instance()\n    \n    # Store signal in Redis for worker to poll\n    signal_key = f\"worker:{worker_id}:signal\"\n    redis_client.setex(signal_key, 30, signal)\n    \n    return {\n        \"worker_id\": worker_id,\n        \"signal\": signal,\n        \"sent_at\": datetime.utcnow().isoformat()\n    }\n```\n\n#### Diagnostic Worker Implementation\n\nCreate a diagnostic worker that can be run alongside normal workers to monitor system health:\n\n```python\n# diagnostic_worker.py\nimport time\nimport json\nimport psutil\nimport redis\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nfrom dataclasses import dataclass\nimport threading\n\n@dataclass\nclass SystemMetrics:\n    timestamp: datetime\n    cpu_percent: float\n    memory_rss: int\n    redis_memory_used: int\n    redis_connected_clients: int\n    queue_depths: Dict[str, int]\n    worker_count: int\n    schedule_count: int\n    \nclass DiagnosticWorker:\n    \"\"\"\n    Runs periodic system diagnostics and logs anomalies.\n    Not a job worker, but a monitoring agent.\n    \"\"\"\n    def __init__(self, redis_client: redis.Redis, interval: int = 60):\n        self.redis = redis_client\n        self.interval = interval\n        self.running = False\n        self.thread = None\n        \n    def collect_metrics(self) -> SystemMetrics:\n        \"\"\"Collect comprehensive system metrics.\"\"\"\n        # System metrics\n        cpu_percent = psutil.cpu_percent(interval=0.1)\n        memory_info = psutil.Process().memory_info()\n        \n        # Redis metrics\n        redis_info = self.redis.info()\n        \n        # Queue depths\n        queue_depths = {}\n        queue_patterns = self.redis.keys(\"queue:*\")\n        for key in queue_patterns:\n            if b\":processing\" not in key and b\":retry\" not in key:\n                queue_name = key.decode('utf-8').replace(\"queue:\", \"\")\n                depth = self.redis.llen(key)\n                queue_depths[queue_name] = depth\n        \n        # Worker count\n        worker_keys = self.redis.keys(\"worker:*\")\n        worker_count = len([k for k in worker_keys if b\":jobs\" not in k])\n        \n        # Schedule count\n        schedule_keys = self.redis.keys(\"schedule:*\")\n        schedule_count = len([k for k in schedule_keys if b\":history\" not in k])\n        \n        return SystemMetrics(\n            timestamp=datetime.utcnow(),\n            cpu_percent=cpu_percent,\n            memory_rss=memory_info.rss,\n            redis_memory_used=redis_info.get('used_memory', 0),\n            redis_connected_clients=redis_info.get('connected_clients', 0),\n            queue_depths=queue_depths,\n            worker_count=worker_count,\n            schedule_count=schedule_count\n        )\n    \n    def check_anomalies(self, metrics: SystemMetrics) -> List[Dict[str, Any]]:\n        \"\"\"Check for system anomalies based on metrics.\"\"\"\n        anomalies = []\n        \n        # High memory usage\n        if metrics.redis_memory_used > 500 * 1024 * 1024:  # 500MB\n            anomalies.append({\n                \"type\": \"high_redis_memory\",\n                \"severity\": \"warning\",\n                \"value\": metrics.redis_memory_used,\n                \"threshold\": 500 * 1024 * 1024\n            })\n        \n        # Queue depth anomalies\n        for queue_name, depth in metrics.queue_depths.items():\n            if depth > 1000:\n                anomalies.append({\n                    \"type\": \"high_queue_depth\",\n                    \"severity\": \"warning\",\n                    \"queue\": queue_name,\n                    \"depth\": depth\n                })\n        \n        # No workers running\n        if metrics.worker_count == 0:\n            anomalies.append({\n                \"type\": \"no_workers\",\n                \"severity\": \"critical\",\n                \"message\": \"No active workers detected\"\n            })\n        \n        # Redis connection issues\n        if metrics.redis_connected_clients == 0:\n            anomalies.append({\n                \"type\": \"redis_no_clients\",\n                \"severity\": \"critical\",\n                \"message\": \"Redis shows no connected clients\"\n            })\n        \n        return anomalies\n    \n    def run_diagnostics(self):\n        \"\"\"Run one diagnostic cycle.\"\"\"\n        try:\n            metrics = self.collect_metrics()\n            anomalies = self.check_anomalies(metrics)\n            \n            # Log metrics\n            self.redis.rpush(\n                \"diagnostics:metrics\",\n                json.dumps({\n                    \"timestamp\": metrics.timestamp.isoformat(),\n                    \"metrics\": {\n                        \"cpu_percent\": metrics.cpu_percent,\n                        \"memory_rss\": metrics.memory_rss,\n                        \"redis_memory\": metrics.redis_memory_used,\n                        \"worker_count\": metrics.worker_count\n                    }\n                })\n            )\n            \n            # Log anomalies\n            if anomalies:\n                for anomaly in anomalies:\n                    self.redis.rpush(\n                        \"diagnostics:anomalies\",\n                        json.dumps({\n                            \"timestamp\": datetime.utcnow().isoformat(),\n                            \"anomaly\": anomaly\n                        })\n                    )\n                    # Also log to regular logs\n                    print(f\"ANOMALY: {anomaly}\")\n            \n            # Set TTL on diagnostics keys to prevent unbounded growth\n            self.redis.expire(\"diagnostics:metrics\", 7 * 24 * 3600)  # 7 days\n            self.redis.expire(\"diagnostics:anomalies\", 7 * 24 * 3600)\n            \n        except Exception as e:\n            print(f\"Diagnostic error: {e}\")\n    \n    def start(self):\n        \"\"\"Start the diagnostic worker in a background thread.\"\"\"\n        self.running = True\n        self.thread = threading.Thread(target=self._run_loop, daemon=True)\n        self.thread.start()\n    \n    def _run_loop(self):\n        \"\"\"Main diagnostic loop.\"\"\"\n        while self.running:\n            self.run_diagnostics()\n            time.sleep(self.interval)\n    \n    def stop(self):\n        \"\"\"Stop the diagnostic worker.\"\"\"\n        self.running = False\n        if self.thread:\n            self.thread.join(timeout=10)\n```\n\n#### Debugging Helper Scripts\n\nCreate command-line scripts for common debugging tasks:\n\n```python\n#!/usr/bin/env python3\n# scripts/debug_queue.py\n\"\"\"\nCommand-line tool to inspect queue state.\n\"\"\"\nimport argparse\nimport json\nimport sys\nfrom typing import List, Dict, Any\n\nfrom job_processor.queue_manager import QueueManager\nfrom job_processor.redis_client import RedisClient\n\ndef inspect_queue(queue_name: str, limit: int = 10):\n    \"\"\"Inspect jobs in a queue.\"\"\"\n    redis = RedisClient.get_instance()\n    queue_key = f\"queue:{queue_name}\"\n    \n    print(f\"=== Queue: {queue_name} ===\")\n    print(f\"Length: {redis.llen(queue_key)}\")\n    print(\"\\nNext jobs:\")\n    \n    jobs = redis.lrange(queue_key, 0, limit - 1)\n    for i, job_data in enumerate(jobs):\n        try:\n            job_json = json.loads(job_data.decode('utf-8'))\n            print(f\"{i+1}. {job_json.get('job_id', 'unknown')}\")\n            print(f\"   Type: {job_json.get('job_type', 'unknown')}\")\n            print(f\"   Args: {job_json.get('args', [])[:3]}\")\n            if len(job_json.get('args', [])) > 3:\n                print(f\"   ... and {len(job_json['args']) - 3} more\")\n        except json.JSONDecodeError:\n            print(f\"{i+1}. INVALID JSON: {job_data[:100]}...\")\n        print()\n\ndef find_stuck_jobs(age_hours: int = 1):\n    \"\"\"Find jobs that have been active for too long.\"\"\"\n    redis = RedisClient.get_instance()\n    \n    # Scan for all job keys\n    cursor = 0\n    stuck_jobs = []\n    \n    while True:\n        cursor, keys = redis.scan(cursor, match=\"job:*\", count=100)\n        \n        for key in keys:\n            # Skip subkeys like job:{id}:errors\n            if b\":\" in key and key.count(b\":\") > 1:\n                continue\n                \n            job_data = redis.hgetall(key)\n            if not job_data:\n                continue\n                \n            # Decode\n            decoded = {}\n            for k, v in job_data.items():\n                try:\n                    decoded[k.decode('utf-8')] = v.decode('utf-8')\n                except:\n                    decoded[k.decode('utf-8', errors='replace')] = str(v)\n            \n            # Check if ACTIVE for too long\n            if decoded.get('status') == '\"ACTIVE\"':\n                started_at = decoded.get('started_at')\n                if started_at:\n                    try:\n                        # Parse timestamp - might be string in JSON format\n                        if started_at.startswith('\"'):\n                            started_at = started_at[1:-1]\n                        # TODO: Parse datetime and compare with age_hours\n                        # For now just report\n                        stuck_jobs.append({\n                            'job_id': decoded.get('job_id', 'unknown'),\n                            'started_at': started_at,\n                            'queue': decoded.get('queue', 'unknown')\n                        })\n                    except:\n                        pass\n        \n        if cursor == 0:\n            break\n    \n    if stuck_jobs:\n        print(f\"Found {len(stuck_jobs)} potentially stuck jobs:\")\n        for job in stuck_jobs:\n            print(f\"  {job['job_id']} in {job['queue']} since {job['started_at']}\")\n    else:\n        print(\"No stuck jobs found.\")\n\ndef cleanup_processing_queues():\n    \"\"\"Clean up jobs stuck in processing queues.\"\"\"\n    redis = RedisClient.get_instance()\n    \n    # Find all processing queues\n    cursor = 0\n    processing_queues = []\n    \n    while True:\n        cursor, keys = redis.scan(cursor, match=\"queue:*:processing\", count=50)\n        processing_queues.extend([k.decode('utf-8') for k in keys])\n        \n        if cursor == 0:\n            break\n    \n    total_moved = 0\n    for queue_key in processing_queues:\n        # Extract main queue name\n        main_queue = queue_key.replace(\":processing\", \"\")\n        \n        # Move all jobs back to main queue\n        while True:\n            job_data = redis.rpoplpush(queue_key, main_queue)\n            if not job_data:\n                break\n            total_moved += 1\n        \n        print(f\"Cleaned {queue_key}\")\n    \n    print(f\"Total jobs requeued: {total_moved}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Debug job queue system\")\n    subparsers = parser.add_subparsers(dest='command', help='Command')\n    \n    # inspect command\n    inspect_parser = subparsers.add_parser('inspect', help='Inspect a queue')\n    inspect_parser.add_argument('queue', help='Queue name')\n    inspect_parser.add_argument('--limit', type=int, default=10, help='Number of jobs to show')\n    \n    # stuck command\n    stuck_parser = subparsers.add_parser('stuck', help='Find stuck jobs')\n    stuck_parser.add_argument('--age-hours', type=int, default=1, help='Age threshold in hours')\n    \n    # cleanup command\n    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup processing queues')\n    \n    args = parser.parse_args()\n    \n    if args.command == 'inspect':\n        inspect_queue(args.queue, args.limit)\n    elif args.command == 'stuck':\n        find_stuck_jobs(args.age_hours)\n    elif args.command == 'cleanup':\n        cleanup_processing_queues()\n    else:\n        parser.print_help()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n#### Language-Specific Debugging Hints\n\n**Python-Specific Debugging:**\n\n1. **Async/Await Deadlocks**: Use `asyncio.all_tasks()` to see all running tasks when debugging hangs in async workers.\n\n2. **Memory Leaks**: Use `tracemalloc` to track memory allocations:\n   ```python\n   import tracemalloc\n   tracemalloc.start()\n   # ... run operations ...\n   snapshot = tracemalloc.take_snapshot()\n   top_stats = snapshot.statistics('lineno')\n   for stat in top_stats[:10]:\n       print(stat)\n   ```\n\n3. **Thread Dumps**: For hung workers, send `SIGUSR1` to get a Python traceback of all threads:\n   ```python\n   import signal\n   import traceback\n   import sys\n   \n   def debug(sig, frame):\n       \"\"\"Interrupt running process and provide a python prompt for debugging.\"\"\"\n       import pdb\n       pdb.Pdb().set_trace(frame)\n   \n   signal.signal(signal.SIGUSR1, debug)  # Register handler\n   ```\n\n4. **Pickle Serialization Issues**: When debugging job serialization, test round-trips:\n   ```python\n   import pickle\n   job_dict = job.to_dict()\n   serialized = pickle.dumps(job_dict)\n   deserialized = pickle.loads(serialized)\n   assert job_dict == deserialized  # Verify no data loss\n   ```\n\n#### Milestone Debugging Checkpoints\n\nAfter implementing each milestone, verify these debugging capabilities work:\n\n**Milestone 1 (Queue Core) Checkpoint:**\n- Run `python scripts/debug_queue.py inspect default` - should show enqueued jobs\n- Enqueue a job >1MB - should get validation error in logs\n- Kill Redis temporarily during enqueue - should see connection error handling\n\n**Milestone 2 (Worker) Checkpoint:**\n- Start worker, send `SIGTERM` - should complete current job before exiting\n- Check `redis-cli HGETALL worker:{id}` - should see heartbeat updating\n- Kill worker mid-job - janitor should move job back to queue\n\n**Milestone 3 (Retry) Checkpoint:**\n- Create job that always fails - watch retry count increment in job metadata\n- Check `redis-cli ZRANGE retry:default 0 -1 WITHSCORES` - should see scheduled retries\n- Exhaust retries - job should move to dead letter queue\n\n**Milestone 4 (Scheduler) Checkpoint:**\n- Create schedule for 1 minute in future - watch it enqueue at correct time\n- Check `redis-cli HGETALL schedule:{id}` - `next_run_at` should update\n- Change system timezone - schedule should adjust correctly\n\n**Milestone 5 (Monitoring) Checkpoint:**\n- Access `/api/debug/queue/default/contents` - should see queue contents\n- Check WebSocket connection to metrics stream - should receive updates\n- Trigger alert condition - should see alert in dashboard\n\n> **Implementation Insight:** Build debugging capabilities alongside features, not as an afterthought. Each component should expose its internal state for inspection. The time spent building debug tools pays back multifold when diagnosing production issues.\n\n\n## Future Extensions\n\n> **Milestone(s):** This section explores potential enhancements beyond the five core milestones. It demonstrates how the architecture's design decisions enable natural evolution while identifying boundaries where significant rework would be required.\n\nA well-architected system should be both complete in its current form and extensible for future needs. This section examines enhancements that could be built upon the current background job processor, categorized by the level of architectural change required. The core design—with its clear separation of concerns, well-defined interfaces, and Redis-based coordination—enables numerous extensions without fundamental rework, adhering to the **YAGNI** principle while maintaining forward compatibility.\n\n### Architecture-Enabled Extensions\n\nThese enhancements leverage existing architectural patterns, data structures, and component boundaries. They require minimal to moderate development effort and don't necessitate breaking changes to core interfaces or data models.\n\n| Extension | Description | Implementation Approach | Architectural Enablers |\n|-----------|-------------|------------------------|------------------------|\n| **Rate Limiting** | Control the maximum number of jobs processed per unit time per queue, job type, or worker group to prevent resource exhaustion. | Extend `QueueManager` with token bucket or sliding window counters stored in Redis. Workers check limits before processing jobs. | Redis atomic operations for counters, existing queue isolation, configurable worker polling. |\n| **Job Dependencies** | Allow jobs to specify prerequisite jobs that must complete successfully before dependent jobs can start. | Add `dependencies` field to `Job` storing list of prerequisite job IDs. New `DependencyManager` component tracks completion and enqueues dependents. | Job lifecycle tracking in Redis, existing event flow from completion to enqueue. |\n| **Multi-Language Worker Support** | Enable workers written in languages other than Python (Go, Rust, JavaScript) to process jobs from the same queues. | Standardize job serialization format (JSON/msgpack) and Redis key structures. Create client libraries in each language implementing the same protocol. | Language-agnostic Redis data model, explicit job serialization format, documented key naming conventions. |\n| **Priority Queues with Preemption** | Allow higher-priority jobs to interrupt lower-priority jobs already being processed. | Extend `Worker` to support job suspension/resumption or termination with requeue. Add preemption signal handling in job execution wrapper. | Job state machine with explicit `ACTIVE` state, worker heartbeat for liveness, configurable timeout handling. |\n| **Job Result Caching** | Cache job results for idempotent operations to avoid redundant computation for identical inputs. | Add optional `cache_key` and `cache_ttl` fields to `Job`. `QueueManager` checks Redis cache before enqueueing, returns cached result if present. | Redis as shared cache, job serialization includes all input parameters, existing result storage mechanism. |\n| **Job Chaining & Pipelines** | Define sequences of jobs where each job's output becomes the next job's input, with error handling across the chain. | New `JobChain` object with list of job definitions and error policy. `ChainManager` monitors execution and propagates data between jobs. | Job metadata for correlation, error handling infrastructure, job registry for dynamic job type resolution. |\n| **Dynamic Queue Configuration** | Add, remove, or modify queue configurations at runtime without restarting workers or scheduler. | Extend `QueueManager` to watch Redis for configuration changes and adjust polling weights. Workers subscribe to configuration updates via Redis Pub/Sub. | Configuration stored in Redis, worker polling strategy decoupled from static config, heartbeat mechanism for dynamic updates. |\n| **Job Tagging & Metadata Search** | Attach arbitrary tags to jobs and enable searching/filtering by tags in the dashboard and APIs. | Add `tags: Dict[str, str]` field to `Job`. Index tags in Redis sorted sets for efficient querying. Extend dashboard filters. | Flexible job metadata field, Redis sorted set pattern for indexing, dashboard filtering architecture. |\n| **Worker Resource Profiles** | Assign workers to specific resource pools (CPU-intensive, memory-heavy, GPU) and route jobs accordingly. | Add `resource_profile` field to `Job` and `Worker`. Extend queue selection logic to match profiles. New resource-specific queues. | Multiple named queues with priorities, worker queue subscription model, configurable routing rules. |\n| **Delayed Job Cancellation** | Allow producers to cancel or modify delayed/scheduled jobs before they execute. | Store delayed jobs in Redis with cancellation tokens. Add `cancel_job` API that marks jobs as cancelled in sorted set. | Delayed jobs stored in Redis sorted sets with job IDs, scheduler polling with batch retrieval. |\n\n#### Detailed Analysis: Rate Limiting Extension\n\n> **Decision: Token Bucket Rate Limiter per Queue**\n> - **Context**: Certain job types (e.g., external API calls, database-intensive operations) must be throttled to prevent overwhelming downstream systems or exhausting shared resources.\n> - **Options Considered**:\n>   1. **Fixed Window Counter**: Simple count per time window, but suffers from boundary effects allowing bursts.\n>   2. **Sliding Window Log**: Accurate but memory-intensive, storing timestamps for each request.\n>   3. **Token Bucket**: Smooths rate, allows bursting up to bucket capacity, computationally efficient.\n> - **Decision**: Implement token bucket algorithm using Redis Lua scripts for atomic operations.\n> - **Rationale**: Token bucket provides predictable smoothing with burst capacity, matches typical rate limiting requirements for external APIs, and Redis Lua scripts ensure atomicity across multiple workers checking limits concurrently.\n> - **Consequences**: Adds small overhead to each job dequeue operation, requires Redis Lua support, provides configurable burst capacity and refill rate.\n\n| Rate Limiting Implementation Comparison |\n|-----------------------------------------|\n| **Option** | **Pros** | **Cons** | **Suitability** |\n| Fixed Window Counter | Simple implementation, low memory usage | Boundary bursts allow 2x limit, less accurate | Low-precision limits where bursts acceptable |\n| Sliding Window Log | Accurate, no boundary bursts | High memory usage (stores all timestamps), slower | When precise limits are critical and traffic low |\n| Token Bucket | Smooth rate, allows controlled bursts, efficient | Slightly more complex, requires bucket size parameter | Most API rate limiting scenarios (chosen) |\n\n**Implementation Walkthrough**:\n1. **Configuration Enhancement**: Add `rate_limit` field to `QueueConfig` with `requests_per_second` and `burst_capacity` parameters.\n2. **Redis Storage**: Use key pattern `ratelimit:{queue_name}` storing bucket state (tokens, last_update) in a Redis hash.\n3. **Atomic Check Script**: Lua script that:\n   1. Calculates time elapsed since last update\n   2. Adds tokens based on elapsed time × refill rate (capped at burst capacity)\n   3. If tokens ≥ 1, decrements tokens by 1 and returns `ALLOWED`\n   4. Otherwise returns `DENIED` with time until next token available\n4. **Worker Integration**: Before processing a job from a rate-limited queue, worker executes the Lua script. If `DENIED`, worker sleeps for the suggested duration before polling again.\n5. **Dashboard Visibility**: Add rate limit status to queue metrics showing current token count, limit status, and throttled worker count.\n\n**Concrete Example**: A webhook queue limited to 10 requests/second with burst capacity of 20. When 25 jobs arrive simultaneously, the first 20 process immediately (burst), then subsequent jobs throttle to 10/second. The token bucket refills at 10 tokens/second, ensuring long-term average compliance.\n\n#### Detailed Analysis: Job Dependencies Extension\n\n> **Decision: Directed Acyclic Graph (DAG) Dependency Model**\n> - **Context**: Complex workflows require sequencing where job B cannot start until job A completes successfully, enabling multi-step data pipelines and business processes.\n> - **Options Considered**:\n>   1. **Linear Chains**: Simple sequential dependencies only (A→B→C), no branching or parallelism.\n>   2. **DAG with Explicit Graph Definition**: Full dependency graph defined upfront, allowing parallel execution of independent branches.\n>   3. **Dynamic Dependencies**: Jobs can spawn new dependencies at runtime, more flexible but harder to reason about.\n> - **Decision**: Implement DAG model with upfront graph definition stored as job metadata.\n> - **Rationale**: DAGs capture most real-world workflows (branching, parallelism), allow static validation of cycles, and enable optimal scheduling of independent branches. Upfront definition simplifies implementation and debugging.\n> - **Consequences**: Requires graph validation to prevent cycles, adds complexity to job completion handling, needs visualization in dashboard.\n\n**Implementation Walkthrough**:\n1. **Job Enhancement**: Add `dependencies: List[str]` field to `Job` storing prerequisite job IDs, and `dependent_jobs: List[str]` for reverse links.\n2. **Dependency Manager**: New component that:\n   - Validates job graphs are acyclic at enqueue time\n   - Maintains Redis sets tracking pending dependencies for each job\n   - Listens for job completion events via Redis Pub/Sub\n   - When job completes, removes it from dependency sets of dependent jobs\n   - When all dependencies satisfied for a job, enqueues it via `QueueManager`\n3. **Atomic Graph Operations**: Use Redis transactions to atomically update dependency state and enqueue jobs when ready.\n4. **Failure Handling**: If a job fails, dependent jobs can be configured to either (a) not run, (b) run with error placeholder, or (c) run after manual intervention.\n5. **Dashboard Visualization**: Display dependency graphs with status color coding, allow drilling into specific job chains.\n\n**Concrete Example**: Data pipeline with jobs: `fetch_data` → [`clean_data`, `validate_schema`] → `merge_results`. `clean_data` and `validate_schema` run in parallel after `fetch_data` completes, both must complete before `merge_results` starts. The dependency manager ensures this ordering automatically.\n\n### Extensions Requiring Major Changes\n\nThese enhancements would require significant architectural modifications, breaking changes to interfaces, or fundamental shifts in system assumptions. They represent directions for major version upgrades or entirely new system variants.\n\n| Extension | Description | Required Architectural Changes | Rationale for Major Change |\n|-----------|-------------|--------------------------------|----------------------------|\n| **Distributed Transactions Across Jobs** | Ensure atomicity across multiple job executions (all succeed or all roll back) with two-phase commit protocol. | New transaction coordinator, WAL for transaction state, participant job interface for prepare/commit/rollback, recovery mechanisms for coordinator failures. | Changes core job execution model from independent to coordinated, requires persistent transaction state, adds complexity to job handlers. |\n| **Multi-Broker Support** | Support alternative brokers beyond Redis (RabbitMQ, Kafka, PostgreSQL) with pluggable backend adapters. | Abstract broker interface, adapter pattern for each backend, migration of data models between backends, feature compatibility matrix. | Current architecture deeply coupled to Redis semantics (sorted sets, blocking pops, Lua scripts). Abstracting requires rethinking atomic operations and data structures. |\n| **Exactly-Once Processing Guarantees** | Ensure each job is processed exactly once despite worker failures, network partitions, or duplicate enqueues. | Idempotency keys with distributed locking, deterministic job ID generation, transaction log for dequeue operations, coordinated checkpointing. | Conflicts with current at-least-once semantics with retries, requires fundamental changes to dequeue/requeue logic and job deduplication at storage layer. |\n| **Geographic Distribution with Cross-Region Replication** | Deploy job processors across multiple regions with automatic failover and geographic job routing. | Active-active Redis replication with conflict resolution, latency-based routing, cross-region job migration, distributed lock coordination. | Assumes single Redis instance; multi-region requires consensus on job ownership, clock synchronization, and replication lag handling. |\n| **Stream Processing Mode** | Process continuous streams of data with windowing, aggregation, and real-time analytics alongside batch jobs. | New stream processing engine, windowed state storage, watermarking for out-of-order events, integration with job scheduler for batch/stream hybrid. | Changes from discrete job model to continuous processing model, requires different programming model, state management, and monitoring. |\n| **Machine Learning Model Serving Integration** | Specialized support for ML model inference jobs with GPU allocation, model versioning, and inference batching. | GPU-aware scheduling, model registry integration, adaptive batching based on queue depth, performance profiling and auto-scaling. | Requires specialized hardware awareness, performance optimization trade-offs different from CPU jobs, tight coupling with ML infrastructure. |\n| **Workflow Engine with Human Tasks** | Extend job dependencies to include human approval steps, forms, and long-running processes (days/weeks). | Human task queue, user assignment and notification, form rendering, long-term state persistence, suspension/resumption with context. | Time scale mismatch (seconds vs days), requires user interface integration, authentication/authorization, different state persistence requirements. |\n| **Serverless Execution Backend** | Execute jobs on serverless platforms (AWS Lambda, Google Cloud Functions) instead of persistent worker processes. | Job packaging for serverless environments, cold start optimization, cost-based scheduling, execution environment isolation. | Fundamental shift from persistent workers to ephemeral execution, changes scaling model, monitoring, and error recovery approaches. |\n\n#### Detailed Analysis: Exactly-Once Processing Guarantees\n\n> **Decision: Distributed Idempotency with Deterministic Job Chaining**\n> - **Context**: Financial and audit-sensitive workloads require stronger guarantees than at-least-once delivery, preventing duplicate processing that could cause double charges or inconsistent state.\n> - **Architectural Challenges**:\n>   1. **Dequeue Atomicity**: Current `BRPOPLPUSH` moves job to processing queue but worker crash before completion leaves job in limbo (janitor recovers but could cause duplicate processing).\n>   2. **Retry Duplication**: Exponential backoff retries create multiple execution attempts of the same logical job.\n>   3. **Producer Duplication**: Network timeouts might cause producers to enqueue the same job twice.\n> - **Required Changes**:\n>   - **Idempotency Keys**: Require producers to supply idempotency key with each job, stored and checked before processing.\n>   - **Deterministic Job IDs**: Generate job IDs as hash of job content + idempotency key to enable deduplication at storage layer.\n>   - **Two-Phase Dequeue**: Worker first acquires distributed lock on job ID, processes, marks as committed in Redis transaction, then releases lock.\n>   - **Intent Logging**: Write \"processing started\" record before execution begins, with recovery scanning for orphaned intents.\n> - **Impact**: Significant performance overhead (distributed locks, additional Redis writes), more complex failure recovery, incompatible with current simple FIFO processing model.\n\n**Migration Path Considerations**:\n1. **Phase 1**: Add optional idempotency key support alongside current semantics for gradual migration.\n2. **Phase 2**: Implement exactly-once processing as separate queue type with different guarantees.\n3. **Phase 3**: Deprecate at-least-once queues over multiple releases, providing migration tools.\n\n#### Detailed Analysis: Multi-Broker Support\n\n> **Decision: Pluggable Broker Adapter Interface**\n> - **Context**: Organizations have existing infrastructure investments (Kafka for event streaming, PostgreSQL for transactional consistency) and want to leverage them as job brokers without maintaining separate Redis clusters.\n> - **Architectural Challenges**:\n>   1. **Redis-Centric Data Structures**: Sorted sets for retries, lists for queues, pub/sub for signals—each has different semantics in other systems.\n>   2. **Atomic Operations**: Redis Lua scripts and pipelines provide atomicity that must be reimplemented for each backend.\n>   3. **Performance Characteristics**: Different latency, throughput, and consistency trade-offs affect system design.\n> - **Required Changes**:\n>   - **Broker Abstraction Layer**: Define `Broker` interface with methods for enqueue, dequeue, sorted set operations, pub/sub, and atomic transactions.\n>   - **Adapter Implementations**: Create `RedisBroker`, `KafkaBroker`, `PostgreSQLBroker` each implementing the interface with backend-specific optimizations.\n>   - **Feature Flagging**: Some features (e.g., priority queue weighting) may not be available in all backends—requires graceful degradation.\n>   - **Migration Tools**: Utilities to transfer job state between brokers during migration.\n> - **Impact**: Doubles code complexity, requires maintaining multiple backend implementations, potentially limits use of advanced Redis-specific features.\n\n**Feature Compatibility Matrix**:\n| Feature | Redis | Kafka | PostgreSQL | Notes |\n|---------|-------|-------|------------|-------|\n| FIFO Queues | ✅ Lists | ✅ Topics (partition ordering) | ✅ Table with sequence | Kafka requires single partition per queue for strict FIFO |\n| Delayed Jobs | ✅ Sorted Sets | ⚠️ Requires separate scheduler | ✅ UPDATE with WHERE scheduled_at | |\n| Priority Queues | ✅ Multiple lists with weighted polling | ❌ No native support | ✅ WITH RECURSIVE query | |\n| Exactly-Once Processing | ⚠️ With careful design | ✅ With transactional producer | ✅ Native transactions | |\n| Horizontal Scaling | ✅ Native | ✅ Native | ⚠️ Connection pooling limits | |\n\n> **Key Insight**: The architecture's clear separation between job logic and queue management enables these extensions, but the choice of Redis as the broker creates both capabilities and constraints. Extensions that align with Redis's strengths (in-memory data structures, atomic operations) are natural; those requiring different consistency models or storage patterns demand architectural evolution.\n\n### Common Pitfalls in Extension Implementation\n\n⚠️ **Pitfall: Over-Engineering for Hypothetical Use Cases**\n- **Description**: Implementing complex extensions (like distributed transactions) before any concrete requirement exists, based on \"what if\" scenarios.\n- **Why It's Wrong**: Increases system complexity, introduces bugs in unused code paths, delays delivery of core functionality, violates YAGNI principle.\n- **Fix**: Implement only extensions with proven user demand. Use the architecture's separation of concerns to keep extension points clear but unimplemented until needed.\n\n⚠️ **Pitfall: Breaking Backward Compatibility**\n- **Description**: Adding new features that require existing job definitions or worker code to be modified, forcing coordinated deployments.\n- **Why It's Wrong**: Causes production outages during rolling updates, requires complex migration procedures, frustrates users.\n- **Fix**: Design extensions with backward compatibility: additive fields only, default values for new requirements, feature flags to enable gradually, dual-read/write patterns during transitions.\n\n⚠️ **Pitfall: Neglecting Performance Impact**\n- **Description**: Adding rate limiting or dependency tracking without considering the additional Redis operations per job, causing latency spikes under load.\n- **Why It's Wrong**: Turns a high-performance job processor into a bottleneck, defeats purpose of async processing.\n- **Fix**: Profile extensions under load, use Redis pipelining, consider approximate algorithms (e.g., probabilistic rate limiting), provide benchmarks for each extension.\n\n⚠️ **Pitfall: Creating Monolithic Components**\n- **Description**: Building extensions as tightly coupled additions to core components rather than separate services or plugins.\n- **Why It's Wrong**: Makes system harder to understand, test, and maintain; increases risk of regressions.\n- **Fix**: Implement extensions as separate processes or threads with well-defined interfaces: dependency manager as separate microservice, rate limiter as Redis-side Lua scripts, dashboard extensions as plugin architecture.\n\n### Implementation Guidance\n\nWhile extensions are beyond the core milestones, their implementation builds upon the established patterns. Here's guidance for implementing the most valuable architecture-enabled extensions.\n\n#### Technology Recommendations for Extensions\n\n| Extension | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Rate Limiting | Redis token bucket with Lua script | Distributed rate limiter with Redis Cluster support, adaptive limits based on downstream health |\n| Job Dependencies | Linear chains with Redis sets for tracking | Full DAG with graph database backend (Neo4j), visual workflow editor |\n| Multi-Language Support | JSON serialization with language-specific client libraries | gRPC-based job protocol with code generation, shared memory for job payloads |\n| Priority Preemption | Job termination with requeue at front of queue | Job suspension/resumption with checkpointing, container snapshotting for long jobs |\n\n#### Recommended File/Module Structure for Extensions\n\n```\nbackground_job_processor/\n├── core/                    # Existing core components\n│   ├── queue_manager.py\n│   ├── worker.py\n│   └── ...\n├── extensions/              # New extensions directory\n│   ├── rate_limiting/\n│   │   ├── __init__.py\n│   │   ├── token_bucket.py\n│   │   ├── lua_scripts/    # Redis Lua scripts\n│   │   └── tests/\n│   ├── dependencies/\n│   │   ├── __init__.py\n│   │   ├── dependency_manager.py\n│   │   ├── graph_validator.py\n│   │   └── tests/\n│   └── multi_lang/\n│       ├── __init__.py\n│       ├── protocol.py     # Serialization protocol spec\n│       ├── go_client/      # Go client library\n│       └── rust_client/    # Rust client library\n└── dashboard_extensions/   # UI extensions\n    ├── rate_limit_panel.py\n    └── dependency_graph.py\n```\n\n#### Infrastructure Starter Code: Rate Limiting Lua Script\n\n```python\n# extensions/rate_limiting/lua_scripts/token_bucket.lua\n-- KEYS[1]: rate limit key (e.g., \"ratelimit:webhooks\")\n-- ARGV[1]: current time in milliseconds\n-- ARGV[2]: refill rate (tokens per millisecond)\n-- ARGV[3]: burst capacity (max tokens)\n-- Returns: ALLOWED or DENIED:wait_time_ms\n\nlocal key = KEYS[1]\nlocal now = tonumber(ARGV[1])\nlocal refill_rate = tonumber(ARGV[2])\nlocal capacity = tonumber(ARGV[3])\n\nlocal bucket = redis.call('HMGET', key, 'tokens', 'last_update')\nlocal tokens = tonumber(bucket[1]) or capacity\nlocal last_update = tonumber(bucket[2]) or now\n\n-- Calculate refill based on time elapsed\nlocal time_passed = math.max(0, now - last_update)\nlocal refill_amount = time_passed * refill_rate\ntokens = math.min(capacity, tokens + refill_amount)\n\nlocal result\nif tokens >= 1 then\n    tokens = tokens - 1\n    result = \"ALLOWED\"\nelse\n    -- Calculate time until next token\n    local wait_time = math.ceil((1 - tokens) / refill_rate)\n    result = \"DENIED:\" .. wait_time\nend\n\n-- Update bucket state\nredis.call('HMSET', key, 'tokens', tokens, 'last_update', now)\nredis.call('EXPIRE', key, math.ceil(capacity / refill_rate / 1000) * 2)  -- Auto-expire idle buckets\n\nreturn result\n```\n\n```python\n# extensions/rate_limiting/token_bucket.py\n\"\"\"Token bucket rate limiter implementation.\"\"\"\nimport time\nimport redis\nfrom typing import Optional, Tuple\n\nclass TokenBucketRateLimiter:\n    \"\"\"Rate limiter using token bucket algorithm with Redis Lua script.\"\"\"\n    \n    def __init__(self, redis_client: redis.Redis, script_path: str = \"token_bucket.lua\"):\n        self.redis = redis_client\n        # Load Lua script\n        with open(script_path, 'r') as f:\n            self.lua_script = self.redis.register_script(f.read())\n    \n    def check_limit(self, queue_name: str, requests_per_second: float, \n                   burst_capacity: int) -> Tuple[bool, Optional[float]]:\n        \"\"\"\n        Check if job can be processed based on rate limit.\n        \n        Args:\n            queue_name: Name of queue to check\n            requests_per_second: Refill rate in tokens per second\n            burst_capacity: Maximum tokens bucket can hold\n            \n        Returns:\n            Tuple of (allowed, wait_time_seconds). wait_time is None if allowed.\n        \"\"\"\n        key = f\"ratelimit:{queue_name}\"\n        now_ms = int(time.time() * 1000)\n        refill_per_ms = requests_per_second / 1000.0\n        \n        result = self.lua_script(\n            keys=[key],\n            args=[now_ms, refill_per_ms, burst_capacity]\n        )\n        \n        if result.startswith(\"ALLOWED\"):\n            return True, None\n        else:\n            # Result format: \"DENIED:wait_time_ms\"\n            wait_time_ms = int(result.split(\":\")[1])\n            return False, wait_time_ms / 1000.0\n```\n\n#### Core Logic Skeleton: Dependency Manager\n\n```python\n# extensions/dependencies/dependency_manager.py\n\"\"\"Manager for job dependencies and DAG execution.\"\"\"\nfrom typing import List, Dict, Set, Optional\nfrom datetime import datetime\nimport json\nfrom core.queue_manager import QueueManager\nfrom core.job import Job\n\nclass DependencyManager:\n    \"\"\"Manages job dependencies using Redis for state tracking.\"\"\"\n    \n    def __init__(self, redis_client, queue_manager: QueueManager):\n        self.redis = redis_client\n        self.queue_manager = queue_manager\n        self.pubsub = self.redis.pubsub()\n        \n    def register_job_with_dependencies(self, job: Job, \n                                      dependencies: List[str]) -> str:\n        \"\"\"\n        Register a job that depends on other jobs completing.\n        \n        Args:\n            job: Job to register (will not be enqueued until dependencies met)\n            dependencies: List of job IDs that must complete first\n            \n        Returns:\n            Job ID of registered job\n            \n        Raises:\n            ValidationError: If dependency graph would create a cycle\n        \"\"\"\n        # TODO 1: Validate job has unique ID (generate if not)\n        # TODO 2: Check for cycles in dependency graph using Redis sets\n        # TODO 3: Store job in Redis with status PENDING_DEPENDENCIES\n        # TODO 4: For each dependency, add this job to the dependent_jobs set\n        # TODO 5: For each dependency, check if already COMPLETED\n        #         - If so, decrement pending count for this job\n        # TODO 6: If all dependencies already satisfied, enqueue job immediately\n        # TODO 7: Subscribe to job completion events for dependencies\n        pass\n    \n    def _handle_job_completion(self, job_id: str, status: str) -> None:\n        \"\"\"\n        Handle completion of a job and notify dependents.\n        \n        Args:\n            job_id: ID of completed job\n            status: COMPLETED or FAILED (affects dependent jobs)\n        \"\"\"\n        # TODO 1: Retrieve list of dependent job IDs from Redis\n        # TODO 2: For each dependent job:\n        #   - Decrement pending dependency count in Redis\n        #   - If count reaches zero and job is not CANCELLED:\n        #        - Retrieve job from Redis\n        #        - Enqueue via queue_manager\n        #        - Update job status to PENDING\n        # TODO 3: If status is FAILED and dependent jobs should not run:\n        #   - Mark dependent jobs as CANCELLED\n        #   - Optionally notify via error channel\n        pass\n    \n    def start_listening(self) -> None:\n        \"\"\"Start listening for job completion events.\"\"\"\n        # TODO 1: Subscribe to job completion channel via Redis Pub/Sub\n        # TODO 2: Start thread to process messages\n        # TODO 3: For each completion message, call _handle_job_completion\n        pass\n    \n    def cancel_job_chain(self, job_id: str) -> None:\n        \"\"\"\n        Cancel a job and all its dependent jobs recursively.\n        \n        Args:\n            job_id: ID of job to cancel\n        \"\"\"\n        # TODO 1: Mark job as CANCELLED in Redis\n        # TODO 2: Get dependent jobs recursively\n        # TODO 3: Mark all as CANCELLED\n        # TODO 4: Clean up dependency tracking sets\n        pass\n```\n\n#### Language-Specific Hints for Extensions\n\n**Python-Specific Tips**:\n- Use `asyncio` for dependency manager event listening to avoid blocking threads.\n- For rate limiting Lua scripts, use `redis.register_script()` which automatically handles script caching.\n- Implement extensions as context managers for proper resource cleanup: `with RateLimiter(queue) as limiter:`.\n- Use Python's `typing` module extensively for extension interfaces to improve IDE support and catch errors early.\n\n**Multi-Language Protocol Considerations**:\n1. **Serialization**: Use JSON for simplicity or msgpack for efficiency. Ensure all languages handle the same data types consistently (especially datetime).\n2. **Error Handling**: Define standard error response format with code, message, and optional details.\n3. **Versioning**: Include protocol version in job payload to handle backward compatibility.\n4. **Connection Management**: Each client library should implement connection pooling and automatic reconnection.\n\n#### Milestone Checkpoint for Rate Limiting Extension\n\nAfter implementing rate limiting:\n\n1. **Test Command**: `python -m pytest extensions/rate_limiting/tests/ -v`\n2. **Expected Output**: Tests showing token bucket refill, burst handling, and denial with wait times.\n3. **Manual Verification**:\n   ```python\n   # Start worker with rate-limited queue\n   limiter = TokenBucketRateLimiter(redis_client)\n   # Enqueue 30 jobs to queue limited to 10/sec with burst 20\n   # First 20 should process immediately, next 10 at ~1/second\n   # Monitor dashboard to see throttling in action\n   ```\n4. **Signs of Success**: Dashboard shows queue processing rate capped at limit, worker logs show `Waiting 0.1s for rate limit` messages.\n5. **Troubleshooting**:\n   - If jobs process faster than limit: Check Lua script loading, ensure time is in milliseconds.\n   - If Redis CPU spikes: Rate limit checks are too frequent; increase worker polling interval for rate-limited queues.\n   - If bursts not working: Verify burst capacity parameter is being passed correctly to Lua script.\n\n#### Debugging Tips for Dependency Extensions\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|-----------------|-----|\n| Dependent jobs never run | Parent job completion not triggering | Check Redis Pub/Sub subscription, verify `_handle_job_completion` is called | Ensure dependency manager is started, check message format in completion channel |\n| Circular dependency deadlock | Graph validation missed a cycle | Run `validate_dag()` on job registration, check Redis for cycles | Implement Kahn's algorithm for cycle detection before accepting job |\n| Memory leak with many dependencies | Redis sets growing unbounded | Monitor Redis memory usage, check for orphaned dependency sets | Implement janitor process to clean up completed dependency chains |\n| Race condition: job runs before dependencies met | Concurrent completion events | Use Redis transactions (`MULTI/EXEC`) for dependency count updates | Implement optimistic locking with version stamps on dependency counts |\n\n> **Architectural Insight**: The most valuable extensions are those that leverage the existing Redis infrastructure without changing core job execution semantics. Rate limiting and dependencies enhance the system's capabilities while maintaining its simplicity and performance characteristics. More radical extensions (exactly-once processing, multi-broker) essentially create a different system with different trade-offs—these might be better implemented as separate projects that reuse concepts but not code.\n\n\n## Glossary\n\n> **Milestone(s):** This section spans all five milestones, providing a definitive reference for the technical terms, domain vocabulary, and acronyms used throughout the design document. A shared vocabulary ensures consistent understanding across the team and reduces ambiguity in technical discussions.\n\n### Terms and Definitions\n\nThis glossary defines all key terms used in the Background Job Processor system, organized alphabetically. Each term includes a concise definition and its relevance to the system design.\n\n| Term | Definition | Context in System |\n|------|------------|-------------------|\n| **Active Job** | A job that has been dequeued by a worker and is currently being executed. Represented by the `JobStatus.ACTIVE` status. | Workers transition jobs from `PENDING` to `ACTIVE` when they begin processing. Active jobs appear in the processing queue until completion. |\n| **Adaptive Sampling** | Adjusting the frequency of metric collection based on system load to balance observability with performance overhead. | The `MetricsAggregator` may increase sampling during high activity and decrease during idle periods to optimize Redis usage. |\n| **Alert Cooldown** | A minimum time period that must pass between consecutive alerts for the same rule to prevent notification spam. | Configured in `AlertRule.cooldown_seconds` to prevent **alert fatigue** when a condition remains unstable. |\n| **Alert Fatigue** | The problem where operators begin ignoring alerts due to excessive volume or frequency, reducing the effectiveness of monitoring. | Addressed through **alert cooldown** periods, severity-based filtering, and intelligent aggregation of similar alerts. |\n| **Architecture Decision Record (ADR)** | A documented decision capturing the context, options considered, and rationale for a specific architectural choice. | Used throughout this document to explain design choices like Redis data structures, concurrency models, and retry algorithms. |\n| **Atomic Pipeline** | A Redis feature that allows multiple commands to be executed as a single atomic operation, ensuring consistency even with concurrent access. | Used by `QueueManager.bulk_enqueue()` and `QueueManager.delete_job()` to prevent race conditions between workers. |\n| **Backoff Calculator** | A component (`BackoffCalculator`) that computes the delay between retry attempts using an exponential backoff algorithm with optional jitter. | Called by `RetryManager.handle_job_failure()` to determine when to retry failed jobs. |\n| **Blocking Queue Read** | A Redis operation like `BRPOP` that waits (blocks) for items to become available in a queue rather than returning immediately. | Used by workers in their main loop to avoid busy-waiting and reduce Redis load. |\n| **Broker** | The central message store that connects job producers and consumers. In our design, Redis serves as the broker. | All job data flows through the Redis broker, which provides persistence, atomic operations, and pub/sub capabilities. |\n| **Catch-Up Logic** | A mechanism that processes jobs missed during scheduler downtime by detecting overdue scheduled jobs and enqueuing them immediately. | Implemented in `Scheduler._poll_due_schedules()` which compares current time with `Schedule.next_run_at` to identify missed executions. |\n| **Chaos Testing** | Testing system behavior under intentionally injected failures (network partitions, process crashes, etc.) to verify resilience. | Recommended for validating the system's error handling and recovery mechanisms in production-like environments. |\n| **Circuit Breaker** | A design pattern (`CircuitBreaker`) that fails fast when a service is unavailable, preventing cascading failures and allowing time for recovery. | Can be applied to external API calls within job handlers to prevent workers from getting stuck on repeatedly failing operations. |\n| **Circuit State** | The current state of a circuit breaker: `CLOSED` (normal operation), `OPEN` (failing fast), or `HALF_OPEN` (testing recovery). | Tracked by the `CircuitBreaker.state` field and transitions based on failure thresholds and recovery timeouts. |\n| **Completed Job** | A job that has finished execution successfully. Represented by the `JobStatus.COMPLETED` status. | Moved from active to completed storage with execution metrics, available for dashboard queries until archived. |\n| **Concurrency** | The number of jobs a worker can process simultaneously, configured via `WorkerConfig.concurrency`. | Implemented using thread or process pools within a worker to handle multiple jobs in parallel without blocking the main loop. |\n| **Correlation ID** | A unique identifier passed through the entire job lifecycle for tracing and debugging purposes. | Can be stored in `Job.metadata` to link job execution logs across workers, retries, and external systems. |\n| **Cron Expression** | A time-based schedule expression using five fields (minute, hour, day of month, month, day of week) to define recurring execution times. | Parsed by the scheduler to calculate `Schedule.next_run_at` for recurring jobs. Supports standard Unix cron syntax. |\n| **DAG (Directed Acyclic Graph)** | A dependency model without cycles, where jobs depend on the completion of other jobs in a directed, non-circular manner. | Future extension: `DependencyManager` could use DAGs to manage complex job workflows with dependencies. |\n| **Dashboard Config** | Configuration (`DashboardConfig`) for the web dashboard controlling display filters, refresh intervals, and metric aggregation settings. | Used by the monitoring API to customize dashboard views without changing code. |\n| **Daylight Saving Time** | Seasonal time adjustment where clocks are set forward or backward by one hour, affecting local time calculations. | The scheduler handles DST transitions by evaluating cron expressions in the configured timezone using aware datetime objects. |\n| **Dead Letter Queue** | Storage (`DeadLetterQueue`) for jobs that have exhausted all retry attempts, allowing manual inspection and potential retry. | Implemented as a Redis sorted set keyed by job ID, with error history preserved for debugging. |\n| **Dead Letter Job** | A job that has permanently failed after exceeding its maximum retry count. Represented by the `JobStatus.DEAD_LETTER` status. | Moved to the dead letter queue by `RetryManager.move_to_dead_letter()` and accessible via the dashboard for manual handling. |\n| **Diagnostic Worker** | A background process (`DiagnosticWorker`) that periodically collects system health metrics and checks for anomalies. | Runs alongside the main components to provide proactive monitoring and early warning of system issues. |\n| **Directed Acyclic Graph** | See **DAG**. | |\n| **End-to-End Tests** | Tests that simulate real user scenarios across the entire system, verifying integration between all components. | Example: Enqueue a job → worker processes it → verify result storage → check dashboard metrics update. |\n| **Exponential Backoff** | A retry algorithm that doubles the wait time between consecutive retry attempts to prevent overloading systems. | Implemented in `BackoffCalculator.calculate_delay()` using formula: `base_delay * 2^(attempt-1) ± jitter`. |\n| **Failed Job** | A job that has finished execution with an error but may still be retried. Represented by the `JobStatus.FAILED` status. | After failure, the job is either scheduled for retry or moved to the dead letter queue based on retry count. |\n| **FIFO (First-In-First-Out)** | An ordering principle where the first element added to a queue is the first to be removed. | Default queue behavior implemented using Redis `LPUSH` (add to end) and `BRPOP` (remove from front). |\n| **Flaky Test** | A test that passes and fails intermittently without code changes, often due to timing issues, race conditions, or external dependencies. | Mitigated by using deterministic `mock clock` for time-dependent tests and proper isolation of Redis test instances. |\n| **Graceful Shutdown** | A shutdown process that allows the current operation to complete before exiting, rather than terminating abruptly. | Workers implement graceful shutdown by catching `SIGTERM`, setting `_shutdown_requested`, and finishing the current job before exiting. |\n| **Heartbeat** | A regular status update published by workers to indicate they are alive and functioning. | Implemented by `WorkerHeartbeat` which periodically writes worker status to Redis with a TTL; missing heartbeats indicate crashed workers. |\n| **Idempotent** | A property where repeated execution of an operation produces the same result as a single execution. | Critical for job handlers that may be retried; designers should ensure jobs are idempotent where possible. |\n| **Idempotency Key** | A unique identifier used to prevent duplicate processing of the same logical operation. | Future extension: Could be stored in `Job.metadata` to deduplicate identical jobs enqueued multiple times. |\n| **Integration Tests** | Tests that verify interactions between multiple components with real dependencies (like Redis). | Use `RedisTestContext` to spin up isolated Redis instances for testing queue operations, worker coordination, and retry logic. |\n| **Janitor Process** | A maintenance process (`Janitor`) that cleans up stale jobs from processing queues when workers crash unexpectedly. | Periodically scans for jobs in processing queues whose worker heartbeat has expired and requeues them. |\n| **Job** | The unit of work to be processed asynchronously, represented by the `Job` class with fields for type, arguments, metadata, and status. | The fundamental data entity that flows through the system from enqueue to completion or failure. |\n| **Job Handler** | A function registered with a worker to execute the business logic for a specific job type. | Registered via `Worker.register_handler()`; invoked by the worker when it dequeues a job of the matching type. |\n| **Job History Archival** | The process of moving old job data from Redis to external storage (like a database) to prevent unbounded memory growth. | Implemented by `Janitor._cleanup_expired_keys()` which removes completed jobs after a configurable retention period. |\n| **Job Status** | The current state of a job, represented by the `JobStatus` enum: `PENDING`, `ACTIVE`, `COMPLETED`, `FAILED`, `RETRY_SCHEDULED`, `DEAD_LETTER`. | Tracked throughout the job lifecycle and used for filtering in the dashboard and determining next actions. |\n| **Job Summary** | A condensed view of job information (`JobSummary`) used for display in the dashboard, containing key fields without full payload. | Created from `Job` instances for efficient dashboard queries that don't require the complete job serialization. |\n| **Jitter** | Random variation added to retry delays to prevent synchronization of multiple retrying jobs (thundering herd). | Applied by `BackoffCalculator` using a configurable `jitter_factor` that adds ± random percentage to calculated delays. |\n| **Lua Script** | Redis server-side scripting language that allows atomic execution of multiple commands on the Redis server. | Used for complex atomic operations like `TokenBucketRateLimiter.check_limit()` that require multiple Redis commands. |\n| **Metric Aggregation** | The process of combining raw job events into statistical summaries (counts, rates, averages) for monitoring. | Performed by `MetricsAggregator` which periodically reads Redis streams and computes aggregated metrics for the dashboard. |\n| **Metric Point** | A single metric data point (`MetricPoint`) with timestamp, name, value, and labels for dimensional analysis. | The basic unit of metric storage; aggregated into time series for dashboard charts and alert evaluation. |\n| **Mock Clock** | A simulated time control used in tests to manipulate and advance time without waiting for real clock progression. | Essential for testing retry delays, scheduler cron evaluation, and time-based job expiration without real-world delays. |\n| **Pending Job** | A job that has been enqueued but not yet picked up by a worker. Represented by the `JobStatus.PENDING` status. | Stored in Redis lists (queues) waiting for worker consumption. Dashboard shows pending count per queue. |\n| **Polling Interval** | The time period between scheduler checks for due jobs, configured via `Scheduler.polling_interval_seconds`. | Balances schedule accuracy with Redis load; shorter intervals increase timeliness but increase Redis queries. |\n| **Priority Weighted Polling** | A worker polling strategy that checks higher-priority queues more frequently than lower-priority ones based on configured weights. | Implemented by workers using algorithm that selects queues probabilistically based on `QueueConfig.priority` values. |\n| **Processing Queue** | A temporary Redis list holding jobs currently being executed by a worker, used for reliability during worker crashes. | When a worker dequeues a job, it moves it from the main queue to a processing queue; on completion, removes it. |\n| **Property-Based Testing** | Testing by generating random inputs that must satisfy certain properties, rather than testing specific predefined cases. | Useful for validating job serialization/deserialization invariants and queue operation correctness across diverse inputs. |\n| **Protocol Versioning** | A technique for maintaining backward compatibility in APIs by including version identifiers in messages or endpoints. | Future extension: Could be added to job serialization format to allow evolution of `Job` fields without breaking existing jobs. |\n| **Queue** | A named channel for jobs with configurable priority, implemented as a Redis list with associated metadata. | Defined by `QueueConfig`; workers subscribe to specific queues and poll them according to priority weights. |\n| **Queue Config** | Configuration (`QueueConfig`) for a named queue including its priority and optional maximum length. | Loaded into `QueueManager.queue_configs` to control queue behavior and worker polling strategy. |\n| **Queue Full Error** | An exception (`QueueFullError`) raised when attempting to enqueue a job to a queue that has reached its maximum capacity. | Prevented by `QueueManager._validate_job()` checking queue length against `QueueConfig.max_length` before enqueue. |\n| **Queue Manager** | The component (`QueueManager`) responsible for validating, enqueuing, and managing jobs in Redis queues. | Provides the primary API for producers to submit jobs and inspect queue state. |\n| **Queue Metrics** | Performance metrics for a specific queue (`QueueMetrics`) including depth, processing rate, error rate, and age statistics. | Computed by `MetricsAggregator` and displayed in the dashboard's queue overview section. |\n| **Race Condition** | A bug where the outcome depends on the timing of concurrent operations, often leading to inconsistent state. | Mitigated throughout the system using Redis atomic operations, pipelines, and Lua scripts for critical sections. |\n| **Real-Time Dashboard** | A web interface that updates without page refresh, showing current system state using WebSocket or SSE connections. | Implemented with Server-Sent Events (SSE) streaming metric updates to the browser as they're collected. |\n| **Redis Client** | A wrapper (`RedisClient`) around the Redis connection that provides error handling, connection pooling, and command execution. | Used by all components to interact with Redis; ensures consistent connection management and error recovery. |\n| **Redis Config** | Configuration (`RedisConfig`) for Redis connection parameters: URL, timeouts, retry behavior, and connection pool size. | Loaded into `SystemConfig.redis` and used to initialize `RedisClient` instances. |\n| **Redis Streams** | A Redis data type that provides an append-only log structure with consumer groups for reliable message processing. | Used for job event logging (completions, failures) that feed into the `MetricsAggregator` for real-time metrics. |\n| **Redis Test Context** | A test utility (`RedisTestContext`) that manages isolated Redis instances for integration testing. | Provides clean Redis databases for each test case and ensures proper cleanup after tests complete. |\n| **Retry Filter** | Logic to determine if a specific error type should bypass retries and move directly to the dead letter queue. | Future extension: Could be implemented as a configurable mapping of exception types to retry policies in `RetryManager`. |\n| **Retry Manager** | The component (`RetryManager`) that handles job failures, schedules retries with exponential backoff, and manages the dead letter queue. | Centralized error handling logic invoked by workers when jobs fail during execution. |\n| **Retry Scheduled Job** | A job that has failed and been scheduled for retry at a future time. Represented by the `JobStatus.RETRY_SCHEDULED` status. | Stored in Redis sorted set with execution timestamp as score; periodically moved back to main queue by scheduler. |\n| **Scheduler** | The component (`Scheduler`) that enqueues jobs for future or recurring execution based on cron expressions or specific timestamps. | Runs as a separate process that polls for due schedules and enqueues jobs into the appropriate worker queues. |\n| **Schedule Status** | The current state of a scheduled job execution: `PENDING`, `ENQUEUED`, `SKIPPED`, or `ERROR`. | Tracked in `ScheduledJob.status` to monitor scheduler operation and detect issues with schedule execution. |\n| **Sensitive Data Redaction** | The process of masking confidential information (like passwords, tokens) in job logs and dashboard displays. | Implemented in dashboard API by filtering sensitive fields from `Job.metadata` before returning to clients. |\n| **Server-Sent Events (SSE)** | An HTTP-based technology for server-to-client real-time updates, simpler than WebSockets for unidirectional data flow. | Used by the dashboard to stream real-time metric updates from server to browser without polling. |\n| **SIGTERM** | A Unix signal sent to a process to request graceful termination (as opposed to SIGKILL which forces immediate termination). | Workers catch SIGTERM to initiate graceful shutdown, allowing current job to complete before exiting. |\n| **Split-Brain** | A scenario in distributed systems where a network partition causes components to operate independently, potentially leading to data inconsistency. | Mitigated in our design by using Redis as a single source of truth; network partition would prevent all components from operating. |\n| **Statistical Assertion** | A test verification that holds true over many random trials, rather than for a single specific execution. | Used in property-based testing to validate system properties like \"no job is lost\" across many random job sequences. |\n| **System Alert** | A notification of a system condition requiring attention (`SystemAlert`), with severity, message, and timestamps. | Generated by `AlertManager` when alert rules evaluate to true, displayed in dashboard and potentially sent via external channels. |\n| **System Config** | The root configuration object (`SystemConfig`) containing all subsystem configurations: Redis, queues, workers, and global settings. | Loaded from environment variables via `SystemConfig.from_env()` and passed to all components during initialization. |\n| **System Metrics** | Comprehensive system health metrics (`SystemMetrics`) including CPU, memory, Redis usage, queue depths, and worker counts. | Collected by `DiagnosticWorker` for system-level monitoring beyond job-specific metrics. |\n| **Test Fixture** | Reusable setup and teardown code for tests that establishes a consistent initial state for test execution. | Provided by `RedisTestContext` which sets up and tears down Redis instances for integration tests. |\n| **Thundering Herd** | A problem where many retried jobs simultaneously become ready and overwhelm the system when they retry at the same time. | Prevented by adding **jitter** to retry delays, staggering retry times across different jobs. |\n| **Timezone** | A geographical region's standard time, used for evaluating cron expressions in the correct local context. | Configured per schedule via `Schedule.timezone`; cron expressions are evaluated in this timezone rather than UTC only. |\n| **Token Bucket** | A rate limiting algorithm that allows bursts up to a capacity while maintaining a steady average rate over time. | Implemented by `TokenBucketRateLimiter` for future extension to limit job enqueue rates per producer. |\n| **TTL (Time-To-Live)** | An expiration time set on Redis keys to automatically remove data after a period, preventing memory bloat. | Applied to worker heartbeats, metric data, and completed job records to ensure Redis memory doesn't grow unbounded. |\n| **Two-Tiered Storage** | An architecture with fast cache (Redis) for recent data and slow storage (database) for historical data. | Future extension: Older job history could be moved from Redis to a database while keeping recent data in Redis for fast access. |\n| **ULID (Universally Unique Lexicographically Sortable Identifier)** | An identifier format that combines timestamp (lexicographically sortable) with randomness for uniqueness. | Alternative to UUID for job IDs; provides natural chronological ordering when sorted lexicographically. |\n| **Uniqueness Window** | A time period during which duplicate enqueueing of the same scheduled job is prevented by the scheduler. | Configured via `Schedule.unique_window_seconds`; prevents multiple instances of the same recurring job within the window. |\n| **Validation Error** | An exception (`ValidationError`) raised when job validation fails, such as payload exceeding size limits or missing required fields. | Thrown by `QueueManager._validate_job()` before enqueue to prevent malformed jobs from entering the system. |\n| **Worker** | A process (`Worker`) that fetches jobs from queues and executes them using registered handlers, with configurable concurrency. | The core processing unit of the system; multiple workers can run simultaneously for horizontal scaling. |\n| **Worker Config** | Configuration (`WorkerConfig`) for a worker process: subscribed queues, concurrency, heartbeat interval, and job timeout. | Passed to `Worker` instance at startup; determines which queues the worker polls and how many jobs it processes concurrently. |\n| **Worker Heartbeat** | A component (`WorkerHeartbeat`) that periodically updates worker liveness in Redis, allowing detection of crashed workers. | Runs in a separate thread within each worker process; updates a Redis key with TTL; missing updates indicate worker failure. |\n| **Worker Status** | Current status of a worker (`WorkerStatus`): active/inactive, current job, processed counts, and last heartbeat time. | Computed by the monitoring API from worker heartbeats and active job tracking; displayed in the dashboard's worker view. |\n| **YAGNI (You Ain't Gonna Need It)** | A principle of not implementing features until they are actually needed, to avoid unnecessary complexity. | Applied throughout the design: features like job dependencies, rate limiting, and multi-language support are marked as future extensions. |\n| **Zombie Job** | A job stuck in a processing queue with no active worker processing it, typically because the worker crashed. | Cleaned up by the **janitor process** which detects stale processing queues and requeues their jobs. |\n"}