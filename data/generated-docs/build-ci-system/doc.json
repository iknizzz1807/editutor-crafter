{"html":"<h1 id=\"cicd-pipeline-orchestrator-design-document\">CI/CD Pipeline Orchestrator: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This document outlines the design for a custom Continuous Integration (CI) system that automatically runs defined test and build pipelines in response to code changes. The key architectural challenge is orchestrating isolated, parallel job executions reliably while providing real-time feedback to developers, balancing simplicity for educational purposes with the core patterns used in production systems like GitHub Actions.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>This section establishes the foundational problem our CI/CD Pipeline Orchestrator aims to solve. We begin by constructing an intuitive mental model, then formalize the core integration challenges, and finally survey the landscape of existing solutions to understand where our design fits.</p>\n<h3 id=\"mental-model-the-automated-factory-floor\">Mental Model: The Automated Factory Floor</h3>\n<p>Imagine a modern, fully automated factory that produces software. In this factory:</p>\n<ul>\n<li><strong>Code changes</strong> (a <code>git push</code>, a pull request) are the <strong>work orders</strong> arriving at the receiving dock.</li>\n<li>Each <strong>pipeline configuration file</strong> (e.g., <code>.ci.yml</code>) is an <strong>assembly line blueprint</strong>, detailing the specific sequence of operations required to validate and package a particular product (your application).</li>\n<li>A <strong>pipeline run</strong> is an <strong>active assembly line</strong> instantiated to process a single work order.</li>\n<li><strong>Stages</strong> (like <code>test</code>, <code>build</code>, <code>deploy</code>) are <strong>logical workstations</strong> along the assembly line, grouping related tasks. Some workstations can operate in parallel, while others must wait for upstream stations to finish.</li>\n<li><strong>Jobs</strong> within a stage are the <strong>robotic stations</strong> at each workstation. Each job is a self-contained unit of work, like running a linter, executing a test suite, or compiling a binary.</li>\n<li><strong>Steps</strong> are the <strong>individual commands</strong> the robot executes, such as <code>npm install</code> or <code>go test ./...</code>.</li>\n<li><strong>Docker containers</strong> provide <strong>identical, sterile workbenches</strong> for each robotic station. Every job gets a fresh, predictable environment, ensuring no oil (state) from assembling a car (web service) contaminates the bench for assembling a motorcycle (CLI tool).</li>\n<li>The <strong>Orchestrator</strong> is the <strong>factory floor manager</strong>. It receives work orders, consults the blueprints, schedules tasks on available stations, and monitors progress.</li>\n<li>The <strong>Dashboard</strong> is the <strong>factory control room&#39;s giant display</strong>, showing the status of every active assembly line, streaming live video from each station (logs), and flashing alerts when a robot malfunctions (a job fails).</li>\n</ul>\n<p>This mental model frames the CI system not as a monolithic application, but as a <strong>coordination engine for isolated, parallelizable tasks</strong>. The primary architectural challenge becomes reliably managing this coordination—dispatching work, isolating execution, capturing output, and reporting results—without creating a tangled mess of dependencies or becoming a single point of failure.</p>\n<h3 id=\"the-core-integration-problem\">The Core Integration Problem</h3>\n<p>Before automated CI, integrating code changes was a manual, error-prone, and time-consuming process. A developer would:</p>\n<ol>\n<li>Pull the latest <code>main</code> branch.</li>\n<li>Run the test suite locally, hoping their environment (OS, dependency versions, environment variables) matches what other developers and production use.</li>\n<li>Manually build artifacts, often forgetting a step documented in a wiki.</li>\n<li>Deploy to a staging environment, crossing fingers that it works.</li>\n</ol>\n<p>This approach suffers from several critical flaws:</p>\n<ol>\n<li><strong>The &quot;It Works on My Machine&quot; Syndrome:</strong> Inconsistent environments lead to bugs that appear only in production or on a colleague&#39;s computer. The lack of <strong>isolation and reproducibility</strong> makes diagnosing issues a nightmare.</li>\n<li><strong>Feedback Delay:</strong> Manual steps mean integration happens infrequently (perhaps once a day in a &quot;daily build&quot;), allowing bugs to accumulate and making it hard to pinpoint which change introduced a regression. Developers need <strong>immediate feedback</strong> on whether their change breaks the system.</li>\n<li><strong>Resource Contention and Scaling:</strong> Running long test suites on a developer&#39;s laptop ties up their machine. As the team grows, coordinating who runs what test when becomes impossible. The system needs to <strong>manage shared resources</strong> (compute, memory) and <strong>scale execution</strong> horizontally.</li>\n<li><strong>Process Consistency:</strong> Humans forget steps. An automated process defined in code (<strong>Pipeline as Code</strong>) ensures that every change undergoes the exact same rigorous validation, every single time.</li>\n</ol>\n<p>A CI system directly addresses these flaws by providing:</p>\n<ul>\n<li><strong>Environment Isolation:</strong> Each job runs in a fresh, containerized environment defined by a Docker image, guaranteeing consistency.</li>\n<li><strong>Automation and Triggering:</strong> Code changes automatically trigger the defined pipeline via webhooks, removing manual intervention.</li>\n<li><strong>Parallel Execution:</strong> Independent jobs and stages run concurrently, slashing total feedback time.</li>\n<li><strong>Centralized Logging and Artifacts:</strong> All output and build products are captured in a single place, accessible to the entire team.</li>\n<li><strong>Visibility:</strong> Everyone can see the health of the codebase via the build status dashboard and real-time logs.</li>\n</ul>\n<h3 id=\"survey-of-existing-ci-systems\">Survey of Existing CI Systems</h3>\n<p>To inform our design, we examine how popular CI systems architecturally address the core problem. They generally fall along two axes: <strong>deployment model</strong> (hosted vs. self-hosted) and <strong>execution architecture</strong> (monolithic vs. distributed).</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">System</th>\n<th align=\"left\">Deployment Model</th>\n<th align=\"left\">Execution Architecture</th>\n<th align=\"left\">Key Architectural Insight</th>\n<th align=\"left\">Primary Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>GitHub Actions</strong></td>\n<td align=\"left\">Hosted (SaaS)</td>\n<td align=\"left\">Distributed, event-driven with managed runners.</td>\n<td align=\"left\">Tight integration with the GitHub platform. Uses a YAML-based workflow file stored in the repository. Jobs are dispatched to scalable, managed (or self-hosted) runner machines. Employs a rich marketplace of reusable actions.</td>\n<td align=\"left\">Teams deeply embedded in the GitHub ecosystem seeking a seamless, low-maintenance CI/CD solution.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Jenkins</strong></td>\n<td align=\"left\">Self-hosted</td>\n<td align=\"left\">Traditionally monolithic master/agent, plugin-based.</td>\n<td align=\"left\">Extreme extensibility via plugins. The Jenkins master server handles webhooks, scheduling, and the UI. Build execution is delegated to agent nodes (which can be on different machines). Configuration is often managed via a web UI or Jenkinsfile (Pipeline as Code).</td>\n<td align=\"left\">Organizations requiring maximum flexibility, control over infrastructure, and support for a wide variety of tools and languages.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>GitLab CI/CD</strong></td>\n<td align=\"left\">Hybrid (SaaS &amp; Self-hosted)</td>\n<td align=\"left\">Integrated, distributed with GitLab Runners.</td>\n<td align=\"left\">Deeply integrated into the GitLab DevOps platform. Configuration is via <code>.gitlab-ci.yml</code>. Runners (which can be shared or project-specific) pick up jobs from a central coordinator. Supports auto-scaling runners with Kubernetes.</td>\n<td align=\"left\">Teams using GitLab who want a unified experience from source control to deployment.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>CircleCI</strong></td>\n<td align=\"left\">Hosted (SaaS)</td>\n<td align=\"left\">Distributed, container-based.</td>\n<td align=\"left\">Strong focus on parallelism and test splitting to optimize speed. Uses a declarative <code>config.yml</code>. Jobs run in isolated containers on managed infrastructure or on self-hosted runner machines.</td>\n<td align=\"left\">Teams prioritizing fast build times and a robust, cloud-native CI service.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Buildkite</strong></td>\n<td align=\"left\">Hybrid (Agent-based)</td>\n<td align=\"left\">Distributed, with self-hosted agents.</td>\n<td align=\"left\">The control plane is hosted, but all build execution happens on customer-managed agents. This provides the reliability of a SaaS dashboard with the security and flexibility of controlling your own compute.</td>\n<td align=\"left\">Organizations that need the simplicity of SaaS but must run builds on their own infrastructure due to compliance, network, or resource needs.</td>\n</tr>\n</tbody></table>\n<p><strong>Architectural Patterns Comparison:</strong></p>\n<blockquote>\n<p><strong>Decision: Monolithic vs. Distributed Orchestration</strong></p>\n<ul>\n<li><strong>Context:</strong> We must decide how to coordinate and execute pipeline jobs. A monolithic design runs everything within a single process/machine, while a distributed design separates coordination from execution.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Monolithic Server:</strong> A single binary that handles webhooks, queuing, scheduling, and job execution (e.g., spawning Docker containers directly).</li>\n<li><strong>Distributed with Central Queue:</strong> A coordinator handles webhooks and scheduling, placing jobs into a persistent queue. Separate worker processes (on the same or different machines) consume jobs from the queue and execute them.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> <strong>Distributed with Central Queue</strong>.</li>\n<li><strong>Rationale:</strong> Even for an educational project, this pattern is fundamental to reliable, scalable systems. It decouples components, allowing them to fail and restart independently. The webhook handler and dashboard remain responsive even if workers are bogged down with long jobs. It also cleanly maps to our mental model (factory manager vs. robotic stations).</li>\n<li><strong>Consequences:</strong> Introduces complexity of managing a queue (persistence, delivery semantics) and worker coordination. However, it provides a clear path for scaling horizontally by adding more workers and makes the system more resilient to partial failures.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Option</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Our Choice?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Monolithic Server</strong></td>\n<td align=\"left\">Simpler to deploy (one binary). Easier to reason about data flow (in-process). No network overhead for job dispatch.</td>\n<td align=\"left\">Components are tightly coupled. A long-running job can block the entire system (UI, webhooks). Scaling requires running entire monolith copies. Harder to isolate failures.</td>\n<td align=\"left\">❌</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Distributed with Central Queue</strong></td>\n<td align=\"left\">Loose coupling, high availability. UI and webhooks remain responsive under load. Workers can be scaled independently. Clear separation of concerns.</td>\n<td align=\"left\">More moving parts (queue, workers). Requires job serialization. Must handle network failures and queue semantics (at-least-once vs. exactly-once delivery).</td>\n<td align=\"left\">✅</td>\n</tr>\n</tbody></table>\n<p>Our design takes inspiration from the distributed, queue-based pattern used by GitHub Actions (with its job queue and runner model) and Jenkins (with its master/agent separation), but aims for a <strong>simplified, self-contained architecture</strong> suitable for learning and deployment on a single machine initially. We will use a <strong>persistent queue</strong> (like Redis or an embedded database) to allow components to be restarted without losing jobs, and a <strong>pool of worker goroutines</strong> (or processes) that pull jobs and execute them in Docker containers.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>This section defines the concrete boundaries of our CI/CD Pipeline Orchestrator project. Establishing clear goals and non-goals is critical for maintaining focus, managing complexity, and ensuring the implementation remains achievable within the educational context. Think of this as the <strong>project requirements document</strong> for our automated factory floor—it specifies what features our factory must include (the assembly lines, robotic stations, and control panels) and explicitly states what&#39;s outside our current construction plans (like building multiple factories worldwide or manufacturing custom robot parts).</p>\n<h3 id=\"goals-must-have\">Goals (Must Have)</h3>\n<p>These are the core capabilities our CI system must implement to be considered functionally complete. Each goal corresponds directly to one of the four project milestones and represents a fundamental building block of a production-ready CI system, scaled down to an educational implementation.</p>\n<table>\n<thead>\n<tr>\n<th>Goal</th>\n<th>Corresponding Milestone</th>\n<th>Description</th>\n<th>Rationale &amp; Key Capabilities</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1. Pipeline Configuration Parser</strong></td>\n<td>Milestone 1</td>\n<td>Parse YAML configuration files into executable pipeline definitions with dependency resolution.</td>\n<td>Every CI system needs a way to define workflows. YAML is industry-standard (GitHub Actions, GitLab CI). The parser must handle: <br>• <strong>Structured Pipeline Objects</strong>: Convert YAML into <code>PipelineConfig</code>, <code>Stage</code>, <code>Job</code>, and <code>Step</code> representations <br>• <strong>Environment Variable Substitution</strong>: Resolve <code>$VAR</code> and <code>${VAR}</code> references from pipeline config, system env, and secrets <br>• <strong>Conditional Execution</strong>: Support <code>if</code> expressions for branch/event filtering <br>• <strong>Matrix Build Expansion</strong>: Generate cartesian product of axis values (e.g., <code>os: [linux, macos]</code>) into parallel job configurations <br>• <strong>DAG Construction</strong>: Build dependency graphs from explicit <code>needs:</code> declarations or implicit stage ordering</td>\n</tr>\n<tr>\n<td><strong>2. Containerized Job Execution Engine</strong></td>\n<td>Milestone 2</td>\n<td>Execute pipeline jobs in isolated Docker containers with real-time log capture and artifact collection.</td>\n<td>Reliable, repeatable execution is the heart of CI. Containerization provides: <br>• <strong>Isolation</strong>: Each job runs in a fresh container, preventing state contamination <br>• <strong>Consistency</strong>: Same environment across all runs (specified by Docker image) <br>• <strong>Resource Control</strong>: Memory/CPU limits via Docker <br>• <strong>Step Execution</strong>: Sequential shell command execution with exit code checking <br>• <strong>Output Capture</strong>: Real-time streaming of stdout/stderr to persistent storage <br>• <strong>Artifact Collection</strong>: Copying files matching glob patterns from container to storage</td>\n</tr>\n<tr>\n<td><strong>3. Webhook Handling &amp; Job Queue System</strong></td>\n<td>Milestone 3</td>\n<td>Receive Git webhooks, validate signatures, and manage queued pipeline runs with worker coordination.</td>\n<td>The system must respond automatically to code changes and handle multiple concurrent executions: <br>• <strong>Webhook Processing</strong>: HTTP endpoint for GitHub/GitLab push, PR, and tag events with signature verification <br>• <strong>Event Parsing</strong>: Extract commit SHA, branch, author, and changed files from webhook payloads <br>• <strong>Queue Management</strong>: Reliable job queuing with at-least-once delivery semantics <br>• <strong>Worker Pool</strong>: Concurrent job execution with configurable parallelism limits <br>• <strong>Priority Scheduling</strong>: Higher priority for production deployment pipelines vs. feature branch tests</td>\n</tr>\n<tr>\n<td><strong>4. Basic Web Dashboard</strong></td>\n<td>Milestone 4</td>\n<td>Provide a web interface for viewing pipeline history, real-time logs, and visualizations.</td>\n<td>Developers need visibility into CI runs: <br>• <strong>Build List View</strong>: Paginated table of <code>PipelineRun</code> history with status, trigger, branch, and duration <br>• <strong>Real-time Log Streaming</strong>: Live tail of job output as it executes (via Server-Sent Events or WebSockets) <br>• <strong>Status Badges</strong>: SVG images showing pass/fail status for embedding in repository READMEs <br>• <strong>Pipeline Visualization</strong>: DAG rendering of stages and jobs showing dependencies and current status <br>• <strong>Artifact Access</strong>: Download links for artifacts generated by completed jobs</td>\n</tr>\n<tr>\n<td><strong>5. Single-Machine Deployment</strong></td>\n<td>Cross-cutting</td>\n<td>All components run on a single machine using local Docker and a file-based or SQLite database.</td>\n<td>For educational simplicity and minimal infrastructure requirements: <br>• <strong>Local Execution</strong>: No need for cloud VMs or Kubernetes clusters <br>• <strong>Simplified Networking</strong>: All communication happens via localhost or Unix sockets <br>• <strong>Unified Logging</strong>: All logs accessible from the same machine <br>• <strong>Easy Reset</strong>: Clear all state by stopping processes and deleting local files</td>\n</tr>\n<tr>\n<td><strong>6. Core Data Model Persistence</strong></td>\n<td>Cross-cutting</td>\n<td>Store pipeline definitions, run history, job logs, and artifacts in persistent storage.</td>\n<td>The system must retain history across restarts: <br>• <strong>Structured Storage</strong>: SQL tables for <code>PipelineConfig</code>, <code>PipelineRun</code>, <code>JobRun</code>, and <code>StepRun</code> entities <br>• <strong>Log Storage</strong>: Job output stored in files or database blobs with efficient retrieval <br>• <strong>Artifact Storage</strong>: File system directory structure organized by run/job identifiers <br>• <strong>State Recovery</strong>: Resume monitoring of in-progress runs after system restart</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The goals follow a <strong>vertical slice architecture</strong>—each milestone delivers a complete end-to-end flow for one aspect of the system (parsing → execution → triggering → visualization). This approach provides working software at each stage rather than implementing all layers of one component before moving to the next.</p>\n</blockquote>\n<h3 id=\"non-goals-out-of-scope\">Non-Goals (Out of Scope)</h3>\n<p>These are features explicitly excluded from the current implementation. Listing non-goals prevents scope creep and clarifies that while these capabilities exist in production CI systems, they are not required for our educational version. Each exclusion is accompanied by reasoning and suggestions for how they could be added as future extensions.</p>\n<table>\n<thead>\n<tr>\n<th>Non-Goal</th>\n<th>Reasoning</th>\n<th>What We&#39;re Doing Instead</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Distributed Workers Across Multiple Machines</strong></td>\n<td>Adds significant complexity in networking, service discovery, artifact synchronization, and fault tolerance. The educational focus is on job execution logic, not distributed systems.</td>\n<td>All workers run on the same machine as the orchestrator, sharing local Docker daemon and file system.</td>\n</tr>\n<tr>\n<td><strong>Complex Secret Management</strong></td>\n<td>Production secret management involves encryption, key rotation, access controls, and integration with Vault/Key Management Services. This is security-focused rather than CI-core logic.</td>\n<td>Environment variables (including &quot;secrets&quot;) are passed as plaintext to containers. For educational purposes, we assume secrets are stored in the pipeline configuration or system environment variables without encryption.</td>\n</tr>\n<tr>\n<td><strong>Full-Featured SaaS UI</strong></td>\n<td>A production-grade UI with user accounts, team management, analytics dashboards, and complex filtering would dwarf the core CI logic in implementation effort.</td>\n<td>Basic dashboard with essential views (run list, logs, DAG visualization) using server-rendered HTML or minimal JavaScript. No authentication required.</td>\n</tr>\n<tr>\n<td><strong>Custom Docker Image Building</strong></td>\n<td>Building Docker images as part of pipelines requires Docker-in-Docker, privileged containers, and layer caching—complex concerns orthogonal to job execution.</td>\n<td>Jobs use pre-existing Docker images from public registries (e.g., <code>alpine</code>, <code>golang:latest</code>). The pipeline configuration specifies which image to use.</td>\n</tr>\n<tr>\n<td><strong>Advanced Caching Mechanisms</strong></td>\n<td>Dependency caching (e.g., npm packages, Go modules) across pipeline runs requires distributed storage, cache key generation, and invalidation logic.</td>\n<td>Each job starts with a clean container without any cross-run caching. Users can implement caching within their own scripts if needed.</td>\n</tr>\n<tr>\n<td><strong>Multi-Repository Pipeline Triggers</strong></td>\n<td>Pipelines that trigger based on changes across multiple repositories require complex event correlation and permission models.</td>\n<td>Pipelines are triggered only by events in the repository where the pipeline configuration file resides (single-repo scope).</td>\n</tr>\n<tr>\n<td><strong>Manual Approval Gates &amp; Human Intervention</strong></td>\n<td>Waiting for human approval adds stateful pause/resume logic, notification systems, and UI for approval actions.</td>\n<td>All pipeline steps run automatically once triggered. Conditional <code>if</code> expressions can skip jobs but not pause for manual review.</td>\n</tr>\n<tr>\n<td><strong>Extensive Plugin Ecosystem</strong></td>\n<td>Supporting third-party plugins requires plugin loading, versioning, sandboxing, and a marketplace infrastructure.</td>\n<td>Core functionality is built directly into the system. Extensibility is limited to shell commands within job steps.</td>\n</tr>\n<tr>\n<td><strong>Multi-tenant Isolation</strong></td>\n<td>Running pipelines for multiple organizations/teams requires resource quotas, network isolation, and authentication/authorization.</td>\n<td>The system assumes a single tenant (e.g., one development team or individual user).</td>\n</tr>\n<tr>\n<td><strong>Advanced Notification Integrations</strong></td>\n<td>Slack, email, webhook notifications require configurable templates, retry logic, and integration credential management.</td>\n<td>Pipeline status is only visible via the web dashboard and status badges. No external notifications are sent.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> By explicitly declaring these non-goals, we create a <strong>constrained design space</strong> that allows for deep implementation of core CI concepts without being overwhelmed by peripheral features. Each non-goal represents a potential future extension point that could be built upon the solid foundation established here.</p>\n</blockquote>\n<h4 id=\"architecture-decision-record-scope-definition-strategy\">Architecture Decision Record: Scope Definition Strategy</h4>\n<blockquote>\n<p><strong>Decision: Vertical Slice with Clear Boundaries</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to build a functional CI system for educational purposes within limited time/complexity constraints. The system must demonstrate core CI concepts while remaining implementable by learners.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Horizontal Layering</strong>: Implement all data access logic first, then all business logic, then all UI logic. Provides clean separation but delays end-to-end functionality.</li>\n<li><strong>Vertical Slicing (Chosen)</strong>: Build complete feature slices (parser → executor → webhook → dashboard) that each deliver visible value and can be tested independently.</li>\n<li><strong>Minimal Viable Product then Expand</strong>: Build the absolute simplest CI (single job, no parallelism) then incrementally add features, risking significant refactoring at each stage.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Adopt vertical slicing organized by milestones, with each milestone delivering a complete, testable subsystem.</li>\n<li><strong>Rationale</strong>: Vertical slicing provides early feedback, maintains learner motivation with visible progress, and aligns with the project&#39;s milestone structure. Each slice can be designed with clean interfaces that allow for later expansion without breaking existing functionality.</li>\n<li><strong>Consequences</strong>: <ul>\n<li><strong>Positive</strong>: Working software after each milestone; natural integration testing as slices connect; clear boundaries between components.</li>\n<li><strong>Negative</strong>: Some duplication of effort (e.g., data models may need refinement across slices); later milestones may reveal early design limitations.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Horizontal Layering</td>\n<td>Clean separation of concerns; each layer can be optimized independently; easier to swap implementations (e.g., database)</td>\n<td>No working end-to-end system until all layers complete; harder to test integration early; less motivating for learners</td>\n<td>No</td>\n</tr>\n<tr>\n<td><strong>Vertical Slicing</strong></td>\n<td><strong>Delivers user-visible value each milestone; natural integration testing; aligns with project structure; maintains motivation</strong></td>\n<td><strong>May require refactoring as understanding deepens; some duplication across slices</strong></td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>MVP then Expand</td>\n<td>Starts with simplest possible system; forces prioritization of essentials</td>\n<td>Significant refactoring as features added; early design may not accommodate later complexity well</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h4 id=\"common-pitfalls-in-scope-definition\">Common Pitfalls in Scope Definition</h4>\n<p>⚠️ <strong>Pitfall: Implementing Production-Grade Features Prematurely</strong></p>\n<ul>\n<li><strong>Description</strong>: Learners often try to implement distributed workers, Kubernetes integration, or OAuth authentication before completing core job execution.</li>\n<li><strong>Why it&#39;s wrong</strong>: These features introduce massive complexity early, diverting focus from the fundamental CI algorithms (parsing, execution, queuing). The system becomes over-engineered and may never reach basic functionality.</li>\n<li><strong>How to avoid</strong>: Strictly adhere to the non-goals list. When tempted to add a feature, ask: &quot;Is this required for any of the four milestones?&quot; If not, document it as a future extension and move on.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Neglecting the &quot;Basic&quot; in Basic Web Dashboard</strong></p>\n<ul>\n<li><strong>Description</strong>: Spending excessive time on polished UI animations, complex filtering, or real-time updates across all views instead of focusing on core log streaming and status display.</li>\n<li><strong>Why it&#39;s wrong</strong>: The dashboard&#39;s primary educational value is demonstrating real-time log streaming and pipeline visualization—not UI polish. Time spent on cosmetic features detracts from implementing core streaming mechanisms.</li>\n<li><strong>How to avoid</strong>: Implement the dashboard as server-rendered HTML with minimal JavaScript. Use Server-Sent Events (SSE) for log streaming as it&#39;s simpler than WebSockets. Accept that the UI will be functional but not beautiful.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Over-Engineering the Parser</strong></p>\n<ul>\n<li><strong>Description</strong>: Creating an overly complex YAML parser with custom DSL extensions, template inheritance, or complex validation beyond what&#39;s needed for the milestone acceptance criteria.</li>\n<li><strong>Why it&#39;s wrong</strong>: The parser should be robust but not exhaustive. Production CI systems have evolved complex syntax over years; our educational version needs only the specified features (matrix, env vars, conditionals).</li>\n<li><strong>How to avoid</strong>: Implement exactly what the Milestone 1 acceptance criteria specify. Use existing YAML parsing libraries rather than writing custom parsers. Validate only required fields.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<blockquote>\n<p><strong>Technology Stack Recommendations</strong>: The following table recommends specific technologies for each component, balancing simplicity for learners against production relevance.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Recommended)</th>\n<th>Advanced Option (Alternative)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Language &amp; Runtime</strong></td>\n<td>Go (static binary, excellent concurrency primitives)</td>\n<td>Python (faster prototyping) or Rust (maximum performance)</td>\n</tr>\n<tr>\n<td><strong>Configuration Parser</strong></td>\n<td><code>gopkg.in/yaml.v3</code> (stable YAML parsing)</td>\n<td>Custom parser with <code>github.com/goccy/go-yaml</code> for advanced features</td>\n</tr>\n<tr>\n<td><strong>Container Runtime</strong></td>\n<td>Docker Engine via <code>github.com/docker/docker/client</code></td>\n<td><code>containerd</code> via direct API or <code>podman</code> compatibility layer</td>\n</tr>\n<tr>\n<td><strong>Queue Backend</strong></td>\n<td>In-memory channel with persistent SQLite backup</td>\n<td>Redis via <code>github.com/go-redis/redis</code> for distributed scenarios</td>\n</tr>\n<tr>\n<td><strong>Database</strong></td>\n<td>SQLite with <code>github.com/mattn/go-sqlite3</code></td>\n<td>PostgreSQL via <code>github.com/lib/pq</code> for concurrent access</td>\n</tr>\n<tr>\n<td><strong>Web Framework</strong></td>\n<td>Standard library <code>net/http</code> with minimal routing</td>\n<td><code>github.com/gin-gonic/gin</code> for productivity features</td>\n</tr>\n<tr>\n<td><strong>Real-time Streaming</strong></td>\n<td>Server-Sent Events (SSE) using <code>net/http</code></td>\n<td>WebSockets via <code>github.com/gorilla/websocket</code></td>\n</tr>\n<tr>\n<td><strong>Frontend</strong></td>\n<td>Server-rendered HTML with vanilla JavaScript</td>\n<td>React/Vite SPA with TypeScript</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>File/Module Structure</strong>: Organize the codebase by component boundaries aligned with the milestones.</p>\n</blockquote>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>build-your-own-ci/\n├── cmd/\n│   ├── server/                 # Main orchestrator server\n│   │   └── main.go\n│   ├── worker/                 # Worker process (can be same binary with flag)\n│   │   └── main.go\n│   └── dashboard/              # Dashboard web server\n│       └── main.go\n├── internal/                   # Private application code\n│   ├── config/\n│   │   ├── parser.go           # PipelineConfig parsing (Milestone 1)\n│   │   ├── dag.go              # DAG construction\n│   │   ├── matrix.go           # Matrix expansion\n│   │   └── validator.go        # YAML validation\n│   ├── executor/\n│   │   ├── docker.go           # Container execution (Milestone 2)\n│   │   ├── artifacts.go        # Artifact collection\n│   │   └── logger.go           # Log capture and streaming\n│   ├── orchestrator/\n│   │   ├── queue.go            # Job queue (Milestone 3)\n│   │   ├── scheduler.go        # Worker scheduling\n│   │   └── webhook.go          # Webhook handler\n│   ├── dashboard/\n│   │   ├── server.go           # HTTP handlers (Milestone 4)\n│   │   ├── sse.go              # Server-Sent Events for logs\n│   │   └── badges.go           # SVG badge generation\n│   ├── storage/\n│   │   ├── database.go         # SQLite interface\n│   │   ├── models.go           # PipelineRun, JobRun structs\n│   │   └── artifacts.go        # File system artifact storage\n│   └── types/                  # Shared data types\n│       ├── pipeline.go         # PipelineConfig, Stage, Job, Step\n│       └── runs.go             # PipelineRun, JobRun, StepRun\n├── pkg/                        # Public libraries (if any)\n├── web/                        # Frontend assets\n│   ├── static/\n│   │   ├── css/\n│   │   └── js/\n│   └── templates/              # Go HTML templates\n│       ├── index.html\n│       └── run_detail.html\n├── migrations/                 # SQL schema migrations\n│   └── 001_initial.sql\n├── docker-compose.yml          # For optional Redis/PostgreSQL\n├── go.mod\n└── README.md</code></pre></div>\n\n<blockquote>\n<p><strong>Infrastructure Starter Code</strong>: Basic SQLite setup and Docker client wrapper.</p>\n</blockquote>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/database.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">database/sql</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _ </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#B392F0\">github.com/mattn/go-sqlite3</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DB</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    *</span><span style=\"color:#B392F0\">sql</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSQLiteDB</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sql.</span><span style=\"color:#B392F0\">Open</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"sqlite3\"</span><span style=\"color:#E1E4E8\">, path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to open database: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Enable foreign keys and journaling for data integrity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db.</span><span style=\"color:#B392F0\">Exec</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`PRAGMA foreign_keys = ON; PRAGMA journal_mode = WAL;`</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to set pragmas: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Run migrations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> runMigrations</span><span style=\"color:#E1E4E8\">(db); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"migrations failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">{db}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> runMigrations</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">db</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">sql</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    migrations </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        `CREATE TABLE IF NOT EXISTS pipeline_configs (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            id TEXT PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            repo TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            path TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            raw_yaml TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            parsed_config BLOB,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        )`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        `CREATE TABLE IF NOT EXISTS pipeline_runs (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            id TEXT PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            pipeline_config_id TEXT REFERENCES pipeline_configs(id),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            status TEXT CHECK(status IN ('pending', 'running', 'succeeded', 'failed', 'cancelled', 'skipped')),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            trigger_event TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            commit_sha TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            branch TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            started_at TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            finished_at TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        )`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        `CREATE TABLE IF NOT EXISTS job_runs (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            id TEXT PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            pipeline_run_id TEXT REFERENCES pipeline_runs(id) ON DELETE CASCADE,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_name TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            status TEXT CHECK(status IN ('pending', 'running', 'succeeded', 'failed', 'cancelled', 'skipped')),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            container_id TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            worker_id TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            log_path TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            started_at TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            finished_at TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        )`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        `CREATE TABLE IF NOT EXISTS step_runs (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            id TEXT PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_run_id TEXT REFERENCES job_runs(id) ON DELETE CASCADE,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            step_name TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            command TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            exit_code INTEGER,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            output TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            started_at TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            finished_at TIMESTAMP</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        )`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        `CREATE INDEX IF NOT EXISTS idx_pipeline_runs_status ON pipeline_runs(status)`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        `CREATE INDEX IF NOT EXISTS idx_job_runs_pipeline_id ON job_runs(pipeline_run_id)`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, migration </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> migrations {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> db.</span><span style=\"color:#B392F0\">Exec</span><span style=\"color:#E1E4E8\">(migration)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"migration failed: </span><span style=\"color:#79B8FF\">%w\\n</span><span style=\"color:#9ECBFF\">Query: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err, migration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper for paginated queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">db </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetPipelineRuns</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement query with JOIN to pipeline_configs for repo info</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rows, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> db.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        SELECT id, status, trigger_event, commit_sha, branch, started_at, finished_at</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        FROM pipeline_runs </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ORDER BY created_at DESC </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        LIMIT ? OFFSET ?</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    `</span><span style=\"color:#E1E4E8\">, limit, offset)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rows.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> runs []</span><span style=\"color:#B392F0\">PipelineRun</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> rows.</span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> run </span><span style=\"color:#B392F0\">PipelineRun</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Scan row into struct fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        runs </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(runs, run)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> runs, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/executor/docker.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> executor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/api/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/api/types/container</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/client</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DockerExecutor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">client</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDockerExecutor</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cli, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> client.</span><span style=\"color:#B392F0\">NewClientWithOpts</span><span style=\"color:#E1E4E8\">(client.FromEnv, client.</span><span style=\"color:#B392F0\">WithAPIVersionNegotiation</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">{client: cli}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">PullImage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">image</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if image exists locally with d.client.ImageList</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If not present, pull with d.client.ImagePull</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Stream pull progress to logs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateContainer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ContainerConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Define container.Config with Image, Env, Cmd, WorkingDir</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Define container.HostConfig with Binds (for workspace), NetworkMode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call d.client.ContainerCreate and return container ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RunContainer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">containerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">output</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start container with d.client.ContainerStart</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Attach to container logs with d.client.ContainerLogs (follow=true)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Stream logs to output writer in real-time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Wait for container completion with d.client.ContainerWait</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return exit code and any error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CleanupContainer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">containerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Force remove container with d.client.ContainerRemove</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Ignore \"container not found\" errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>Core Logic Skeleton</strong>: Key functions with TODO comments mapping to algorithm steps.</p>\n</blockquote>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/config/parser.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ParseConfig parses and validates a YAML pipeline configuration file.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Algorithm steps correspond to the parsing flowchart (diag-parser-flowchart).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ParseConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">yamlContent</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">PipelineConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">PipelineConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse raw YAML into a temporary map structure using yaml.Unmarshal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate required top-level fields exist (at least one job)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse environment variables section (if present) into map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse matrix definitions (if any) for later expansion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse jobs section, for each job:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Extract steps as []Step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse job-level environment variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse 'if' conditions for conditional execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse 'needs' dependencies for DAG construction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Validate no circular dependencies in 'needs' graph</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Expand matrix jobs by computing cartesian product of axis values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Build DAG of job dependencies based on 'needs' or stage ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Return populated PipelineConfig struct with all parsed data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExpandMatrix takes a job definition with matrix axes and returns multiple job configurations.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ExpandMatrix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">baseJob</span><span style=\"color:#B392F0\"> Job</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">matrixAxes</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#B392F0\">Job</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate matrix axes have at least one dimension with non-empty values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compute cartesian product of all axis values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each combination in the product:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Clone the base job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Substitute matrix values into job name (e.g., \"test-${{matrix.os}}\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add matrix values as environment variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Apply axis-specific overrides if defined in matrix.include/exclude</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return the list of expanded jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Job</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/orchestrator/queue.go  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> orchestrator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EnqueueRun places a pipeline run into the job queue for execution.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implements priority scheduling and rate limiting.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">q </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Queue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EnqueueRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">run</span><span style=\"color:#B392F0\"> PipelineRun</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Determine priority based on branch/event (production = high priority)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check rate limits for the repository to prevent overload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Generate unique queue ID for the run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Serialize run to JSON for storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Store in database with status = STATUS_PENDING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If using in-memory queue, also add to priority queue structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Notify available workers via channel or condition variable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return queue ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DequeueJob retrieves the next available job from the queue for a worker to process.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">q </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Queue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DequeueJob</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">workerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire queue lock for thread-safe access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Find highest priority run that has pending jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Select a job from that run that has all dependencies satisfied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update job status to STATUS_RUNNING and assign workerID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update database record for the job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return the JobRun for execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> job </span><span style=\"color:#B392F0\">JobRun</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> job, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>Language-Specific Hints</strong>:</p>\n</blockquote>\n<ul>\n<li><strong>Go Modules</strong>: Use <code>go mod init build-your-own-ci</code> to initialize the project. Dependencies will be automatically managed.</li>\n<li><strong>Concurrency</strong>: Use <code>sync.Mutex</code> for protecting shared queue state, and <code>goroutines</code> with <code>channels</code> for worker coordination.</li>\n<li><strong>Context Propagation</strong>: Pass <code>context.Context</code> through all execution paths to enable timeouts and cancellation.</li>\n<li><strong>Error Handling</strong>: Use Go&#39;s multi-value returns with <code>error</code>. Wrap errors with <code>fmt.Errorf(&quot;... %w&quot;, err)</code> to preserve stack traces.</li>\n<li><strong>Docker Client</strong>: The official Docker client library is thread-safe; create one instance and reuse it across goroutines.</li>\n<li><strong>SQLite Concurrency</strong>: SQLite supports concurrent reads but only one write at a time. Use <code>PRAGMA journal_mode=WAL</code> to improve concurrency.</li>\n</ul>\n<blockquote>\n<p><strong>Milestone Checkpoint</strong>: After completing the Goals and Non-Goals section, learners should:</p>\n</blockquote>\n<ol>\n<li><strong>Create the project structure</strong> with the recommended directory layout.</li>\n<li><strong>Set up the database</strong> by running the provided SQLite initialization code.</li>\n<li><strong>Verify Docker is accessible</strong> by running a simple test:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   docker</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#79B8FF\"> --rm</span><span style=\"color:#9ECBFF\"> alpine</span><span style=\"color:#9ECBFF\"> echo</span><span style=\"color:#9ECBFF\"> \"Hello from CI\"</span></span></code></pre></div>\n<ol start=\"4\">\n<li><strong>Create a simple webhook endpoint</strong> that logs incoming requests:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/webhook</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"test\": \"payload\"}'</span></span></code></pre></div>\n<p>   Should see log output in the server console.</p>\n<blockquote>\n<p><strong>Debugging Tips</strong> for scope-related issues:</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;I&#39;m implementing Kubernetes pod scheduling instead of Docker containers&quot;</td>\n<td>Scope creep into advanced orchestration</td>\n<td>Check if feature is in Milestone 2 acceptance criteria</td>\n<td>Revert to simple Docker client; Kubernetes is a non-goal</td>\n</tr>\n<tr>\n<td>&quot;Dashboard has real-time updates for everything but logs aren&#39;t streaming&quot;</td>\n<td>Over-investment in UI polish before core functionality</td>\n<td>Verify Server-Sent Events endpoint returns logs</td>\n<td>Focus on log streaming first; add other real-time updates later</td>\n</tr>\n<tr>\n<td>&quot;Parser supports Jinja2 templating but matrix builds don&#39;t work&quot;</td>\n<td>Implementing nice-to-have features before required ones</td>\n<td>Compare implemented features against Milestone 1 checklist</td>\n<td>Remove templating; implement matrix expansion per acceptance criteria</td>\n</tr>\n</tbody></table>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>This section paints the architectural blueprint of our CI/CD Pipeline Orchestrator. Before diving into the gritty details of each component, we establish a shared mental model of the system as a whole. Understanding the <strong>responsibilities</strong> of each major subsystem, <strong>how they connect</strong>, and <strong>where code should live</strong> is crucial for building a coherent system that scales from a single binary to a distributed service.</p>\n<p>Think of the architecture as a <strong>modern restaurant kitchen</strong>. The <code>Webhook Listener</code> is the host who greets customers (Git events) and takes their orders (code changes). The <code>Orchestrator</code> is the head chef who breaks down the order into individual dishes (jobs) and posts tickets (queue messages) on the kitchen rail. The <code>Worker Pool</code> comprises line cooks who each grab a ticket and prepare a dish in their own isolated workstation (container). Finally, the <code>Dashboard</code> is the expo station and dining room TV, showing the status of every order and allowing the customer (developer) to watch the cooking process in real-time. Each station is specialized, communicates through clear protocols, and together they deliver a consistent dining experience.</p>\n<h3 id=\"component-map-and-responsibilities\">Component Map and Responsibilities</h3>\n<p>The system is decomposed into four core subsystems, each with a distinct responsibility and clear boundaries. This separation of concerns allows for independent development, testing, and scaling.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Responsibility</th>\n<th align=\"left\">Key Functions</th>\n<th align=\"left\">Owned Data (In-Memory/Persistent)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Webhook Listener</strong></td>\n<td align=\"left\">Acts as the secure public gateway for Git hosting services (GitHub, GitLab). It validates incoming requests and translates external events into internal pipeline triggers.</td>\n<td align=\"left\">1. Validate webhook signatures.<br>2. Parse event payloads (push, pull request, tag).<br>3. Match events to pipeline configurations (e.g., via branch filters).<br>4. Create an initial <code>PipelineRun</code> record.</td>\n<td align=\"left\">Webhook secret keys (configuration). Temporary in-memory cache for rate-limiting per repository.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Orchestrator</strong></td>\n<td align=\"left\">The brain of the operation. It manages the lifecycle of pipeline runs, coordinates job execution order based on dependencies, and serves as the interface between the queue and the workers.</td>\n<td align=\"left\">1. Parse pipeline configuration (<code>ParseConfig</code>).<br>2. Manage the job queue (<code>EnqueueRun</code>, <code>DequeueJob</code>).<br>3. Expand matrix builds and build the execution DAG.<br>4. Dispatch <code>JobRun</code> instances to workers.<br>5. Update overall <code>PipelineRun</code> status based on job outcomes.</td>\n<td align=\"left\"><code>PipelineConfig</code> cache. In-memory or persistent job queue. The authoritative state of all <code>PipelineRun</code>, <code>JobRun</code>, and <code>StepRun</code> entities (persisted in a database).</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Worker Pool</strong></td>\n<td align=\"left\">A fleet of stateless executors that carry out the actual work. Each worker claims a job and runs its steps in an isolated, containerized environment.</td>\n<td align=\"left\">1. Poll for or receive <code>JobRun</code> assignments.<br>2. Execute job steps sequentially in a Docker container (<code>ExecuteJob</code>).<br>3. Stream real-time log output (<code>StreamLogs</code>).<br>4. Collect and upload artifacts.<br>5. Report job status (success/failure) back to the Orchestrator.</td>\n<td align=\"left\">Ephemeral container IDs and local log buffers during job execution. No long-term state; workers are disposable and horizontally scalable.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Dashboard</strong></td>\n<td align=\"left\">The user-facing control panel and observation deck. It provides visibility into pipeline execution history and real-time progress, and serves status badges.</td>\n<td align=\"left\">1. Display a paginated list of pipeline runs (<code>GetRunHistory</code>).<br>2. Stream live logs from running jobs to the browser.<br>3. Generate and serve status badge SVGs.<br>4. Visualize pipeline DAGs for each run.<br>5. Provide download links for artifacts.</td>\n<td align=\"left\">Cached badge SVGs and session data for active log streams. It primarily reads data owned by the Orchestrator.</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-system-component.svg\" alt=\"High-Level System Component Diagram\"></p>\n<h3 id=\"how-components-connect\">How Components Connect</h3>\n<p>The flow of data through the system follows a unidirectional pipeline, reminiscent of an assembly line. This design minimizes coupling and makes the system easy to reason about. The primary connections are HTTP for external communication, an internal job queue for work dispatch, and a shared database for state persistence.</p>\n<p><strong>Primary Data Flow: The Happy Path of a Git Push</strong></p>\n<ol>\n<li><strong>Event Ingestion:</strong> A developer pushes code to a repository on GitHub. GitHub&#39;s servers send an HTTP POST request (a webhook) to the publicly accessible endpoint of our <strong>Webhook Listener</strong>.</li>\n<li><strong>Trigger &amp; Creation:</strong> The <strong>Webhook Listener</strong> validates the request&#39;s signature using a shared secret, parses the JSON payload, and determines which pipeline configuration (e.g., <code>.ci/pipeline.yml</code>) should be triggered. It then calls the <strong>Orchestrator</strong> to create a new <code>PipelineRun</code> record, passing along the commit SHA, branch, and event type.</li>\n<li><strong>Orchestration &amp; Queuing:</strong> The <strong>Orchestrator</strong> loads the relevant <code>PipelineConfig</code>, resolves environment variables, expands any matrix definitions, and constructs the execution DAG. It then creates individual <code>JobRun</code> records for each job and places references to them into the <strong>Job Queue</strong> via <code>EnqueueRun</code>. The <code>PipelineRun</code> status is set to <code>STATUS_PENDING</code>.</li>\n<li><strong>Job Execution:</strong> An idle <strong>Worker</strong> from the <strong>Worker Pool</strong> calls <code>DequeueJob</code> on the <strong>Orchestrator</strong> (or polls the queue directly). Upon receiving a <code>JobRun</code>, the worker executes it by calling <code>ExecuteJob</code>. This involves pulling the specified Docker image, creating a container, injecting environment variables and secrets, and running the job&#39;s shell commands sequentially.</li>\n<li><strong>Real-Time Observation:</strong> As the job runs, the <strong>Worker</strong> captures <code>stdout</code> and <code>stderr</code>. The <strong>Dashboard</strong> opens a Server-Sent Events (SSE) or WebSocket connection to stream logs. The worker pushes log chunks to a channel that the <strong>Dashboard</strong> subscribes to (or writes to a location the Dashboard can read), enabling the developer to see output in their browser in real-time.</li>\n<li><strong>Status Propagation &amp; Completion:</strong> Upon job completion (success or failure), the <strong>Worker</strong> updates the <code>JobRun</code> status (<code>STATUS_SUCCEEDED</code> or <code>STATUS_FAILED</code>) and uploads any artifacts to persistent storage. The <strong>Orchestrator</strong> is notified and updates the overall <code>PipelineRun</code> status accordingly (e.g., if all jobs succeed, the run succeeds; if any job fails, the run fails).</li>\n<li><strong>History &amp; Inspection:</strong> The <strong>Dashboard</strong> uses <code>GetRunHistory</code> to fetch completed runs from the database (via the <strong>Orchestrator</strong> or directly) and displays them in a list. The developer can click on any run to see its detailed DAG visualization, archived logs, and download artifacts.</li>\n</ol>\n<p><strong>Key Communication Channels:</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">From → To</th>\n<th align=\"left\">Protocol/Mechanism</th>\n<th align=\"left\">Data Exchanged</th>\n<th align=\"left\">Direction</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Git Host → Webhook Listener</td>\n<td align=\"left\">HTTP(S) POST</td>\n<td align=\"left\">Webhook payload (JSON)</td>\n<td align=\"left\">Inbound</td>\n</tr>\n<tr>\n<td align=\"left\">Webhook Listener → Orchestrator</td>\n<td align=\"left\">Internal Function Call / RPC</td>\n<td align=\"left\"><code>PipelineRun</code> creation request</td>\n<td align=\"left\">Internal</td>\n</tr>\n<tr>\n<td align=\"left\">Orchestrator → Job Queue</td>\n<td align=\"left\">Database insert / Pub-Sub</td>\n<td align=\"left\"><code>JobRun</code> reference (ID, metadata)</td>\n<td align=\"left\">Internal</td>\n</tr>\n<tr>\n<td align=\"left\">Worker Pool → Job Queue</td>\n<td align=\"left\">Database query / Pub-Sub consume</td>\n<td align=\"left\">Claimed <code>JobRun</code> reference</td>\n<td align=\"left\">Internal</td>\n</tr>\n<tr>\n<td align=\"left\">Worker → Dashboard (Logs)</td>\n<td align=\"left\">SSE / WebSocket / Shared Bus</td>\n<td align=\"left\">Real-time log lines</td>\n<td align=\"left\">Internal</td>\n</tr>\n<tr>\n<td align=\"left\">Dashboard → Orchestrator / DB</td>\n<td align=\"left\">HTTP API / Direct SQL</td>\n<td align=\"left\">Queries for <code>PipelineRun</code> history</td>\n<td align=\"left\">Internal</td>\n</tr>\n<tr>\n<td align=\"left\">Worker → Docker Daemon</td>\n<td align=\"left\">Docker Socket (Unix/HTTP)</td>\n<td align=\"left\">Container lifecycle commands</td>\n<td align=\"left\">Outbound (Infra)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The critical insight here is the <strong>separation of the control plane (Orchestrator) from the data plane (Worker Pool)</strong>. The Orchestrator makes all scheduling and state decisions, while Workers are dumb executors. This allows you to scale workers horizontally without complex coordination, and even run workers on different machines, as long as they can communicate with the central Orchestrator and Docker daemon.</p>\n</blockquote>\n<h3 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h3>\n<p>A well-organized codebase mirrors the architectural boundaries. For our Go implementation, we adopt a standard project layout centered around an <code>internal</code> package, which prevents external imports of our core logic. Each major component gets its own subpackage.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>build-your-own-ci/\n├── cmd/                           # Application entry points\n│   ├── ci-server/                 # Main server binary (Orchestrator + Webhook Listener + Dashboard)\n│   │   └── main.go\n│   └── ci-worker/                 # Optional separate worker binary\n│       └── main.go\n├── internal/                      # Private application code\n│   ├── config/                    # Pipeline configuration parsing (Milestone 1)\n│   │   ├── parser.go              # `ParseConfig`, YAML unmarshaling\n│   │   ├── dag.go                 # DAG construction, dependency resolution\n│   │   ├── matrix.go              # Matrix expansion logic\n│   │   ├── envsubst.go            # Environment variable substitution\n│   │   └── config_test.go\n│   ├── orchestrator/              # Core orchestration logic (Milestone 1, 3)\n│   │   ├── orchestrator.go        # Manages runs, queue interface\n│   │   ├── queue/                 * Abstract queue interface and implementations\n│   │   │   ├── queue.go           # Interface (EnqueueRun, DequeueJob)\n│   │   │   ├── memory.go          # In-memory channel-based queue\n│   │   │   └── redis.go           # Redis-backed queue\n│   │   └── store/                 * Abstract storage interface and implementations\n│   │       ├── store.go           # Interface for PipelineRun, JobRun, StepRun\n│   │       ├── memory.go          # In-memory store (for dev)\n│   │       └── sqlite.go          # SQLite persistence\n│   ├── worker/                    # Job execution engine (Milestone 2)\n│   │   ├── executor.go            # `ExecuteJob`, container management\n│   │   ├── docker_client.go       # Wrapper around Docker SDK\n│   │   ├── logstream.go           # `StreamLogs`, real-time output handling\n│   │   ├── artifact.go            # Artifact collection and upload\n│   │   └── pool.go                * Worker pool management\n│   ├── webhook/                   # Webhook handling (Milestone 3)\n│   │   ├── handler.go             # `HandleWebhook`, HTTP handler\n│   │   ├── github.go              # GitHub payload parsing &amp; validation\n│   │   ├── gitlab.go              # GitLab payload parsing &amp; validation\n│   │   └── verifier.go            # Signature verification\n│   └── dashboard/                 # Web dashboard (Milestone 4)\n│       ├── server.go              # HTTP server, route definitions\n│       ├── handlers.go            # `GetRunHistory`, badge handler, DAG data endpoint\n│       ├── sse.go                 # Server-Sent Events for log streaming\n│       └── assets/                # Static HTML, JS, CSS\n│           ├── index.html\n│           └── app.js\n├── pkg/                           # Public, reusable libraries (if any)\n│   └── ciutils/                   # E.g., shared logging, constants\n├── go.mod\n├── go.sum\n└── docker-compose.yml             # For local Redis, Docker-in-Docker setup</code></pre></div>\n\n<p><strong>Package Dependencies Flow:</strong> The <code>cmd/ci-server</code> imports from <code>internal/webhook</code>, <code>internal/orchestrator</code>, and <code>internal/dashboard</code>. The <code>internal/orchestrator</code> imports <code>internal/config</code> and <code>internal/worker</code> (for job definitions and execution interface). The <code>internal/worker</code> is the only package that directly interacts with the Docker SDK. This structure enforces a clear dependency hierarchy and prevents circular imports.</p>\n<h3 id=\"architecture-decision-record-monolithic-vs-microservices-deployment\">Architecture Decision Record: Monolithic vs. Microservices Deployment</h3>\n<blockquote>\n<p><strong>Decision: Deploy as a Single Monolithic Binary with Optional Worker Separation</strong></p>\n<ul>\n<li><strong>Context</strong>: We are building a learning-focused CI system that must be simple to run locally and deploy. We need to balance conceptual clarity with the ability to scale components independently.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Pure Monolith</strong>: A single binary containing the Webhook Listener, Orchestrator, Worker Pool, and Dashboard. All components run in one process.</li>\n<li><strong>Full Microservices</strong>: Separate binaries (and possibly containers) for each major component, communicating via network calls (gRPC/HTTP).</li>\n<li><strong>Hybrid Monolith with Extractable Workers</strong>: A primary server binary (Orchestrator, Listener, Dashboard) and a separate, optional worker binary. The worker connects back to the main server&#39;s queue and API.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: We choose the <strong>Hybrid</strong> approach (Option 3). The primary <code>ci-server</code> will embed the Webhook Listener, Orchestrator, and Dashboard. The Worker Pool can be run either as goroutines within the same process (simple mode) or as separate <code>ci-worker</code> processes (scalable mode).</li>\n<li><strong>Rationale</strong>: A pure monolith is the simplest for learners to grasp and run, but it conflates the very different concerns of job scheduling and job execution. The hybrid model maintains logical separation in the code (<code>internal/worker</code> package) while offering deployment flexibility. Learners can start with the all-in-one server and later extract workers without rewriting core logic. It mirrors the pattern of real-world systems like Jenkins (controller/agent) or GitHub Actions (runner).</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Simplifies initial development and debugging (single binary). Clear component boundaries in code. Allows scaling workers horizontally by launching more <code>ci-worker</code> processes.</li>\n<li><strong>Negative</strong>: Introduces a slight operational complexity for the multi-process deployment. Requires the queue and storage to be accessible from separate processes (i.e., cannot use simple in-memory channels).</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Option</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Pure Monolith</strong></td>\n<td align=\"left\">Ultimate simplicity for running and debugging. No inter-process communication.</td>\n<td align=\"left\">Components cannot scale independently. A long-running job can block the webhook handler. Blurs architectural boundaries.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Full Microservices</strong></td>\n<td align=\"left\">Maximum scalability and fault isolation. Clear deployment boundaries.</td>\n<td align=\"left\">High operational and development complexity. Network latency and failure modes add significant learning overhead.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Hybrid Monolith</strong></td>\n<td align=\"left\">Good separation of concerns in code. Flexible deployment: all-in-one or scaled workers. Mimics production patterns.</td>\n<td align=\"left\">Slightly more complex than a pure monolith. Requires shared, persistent queue/storage for multi-process mode.</td>\n<td align=\"left\"><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete starting points for organizing and implementing the high-level architecture in Go.</p>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option (Starting Point)</th>\n<th align=\"left\">Advanced Option (For Extension)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Web Transport</strong></td>\n<td align=\"left\"><code>net/http</code> with Gorilla Mux or <code>http.ServeMux</code></td>\n<td align=\"left\"><code>chi</code> router for middleware, <code>gRPC</code> for internal RPC</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Queue Backend</strong></td>\n<td align=\"left\">In-memory channel (<code>chan interface{}</code>) with a wrapper</td>\n<td align=\"left\"><code>Redis</code> (go-redis), <code>PostgreSQL</code> with SKIP LOCKED, <code>RabbitMQ</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Data Persistence</strong></td>\n<td align=\"left\">In-memory map with <code>sync.RWMutex</code> (for dev)</td>\n<td align=\"left\"><code>SQLite</code> (go-sqlite3) for single-binary persistence, <code>PostgreSQL</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Docker Client</strong></td>\n<td align=\"left\">Official Docker SDK for Go (<code>github.com/docker/docker/client</code>)</td>\n<td align=\"left\">Direct use of <code>containerd</code> API for lower-level control</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Real-time Logs</strong></td>\n<td align=\"left\">Server-Sent Events (SSE) via <code>http.ResponseWriter</code></td>\n<td align=\"left\"><code>WebSocket</code> (gorilla/websocket) for bidirectional communication</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Frontend</strong></td>\n<td align=\"left\">Plain HTML/JavaScript with vanilla JS or Alpine.js</td>\n<td align=\"left\">React/Vite SPA with a separate build process</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure Starter</strong></p>\n<p>The provided structure above can be initialized with the following commands and stub files.</p>\n<p>First, create the module and base directories:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> build-your-own-ci/cmd/ci-server</span><span style=\"color:#9ECBFF\"> build-your-own-ci/cmd/ci-worker</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> build-your-own-ci/internal/config</span><span style=\"color:#9ECBFF\"> internal/orchestrator/queue</span><span style=\"color:#9ECBFF\"> internal/orchestrator/store</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> build-your-own-ci/internal/worker</span><span style=\"color:#9ECBFF\"> internal/webhook</span><span style=\"color:#9ECBFF\"> internal/dashboard/assets</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> build-your-own-ci</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> mod</span><span style=\"color:#9ECBFF\"> init</span><span style=\"color:#9ECBFF\"> github.com/yourusername/build-your-own-ci</span></span></code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code: Core Data Types and Store Interface</strong></p>\n<p>To ensure consistency across components, we define the core data structures and a generic storage interface in a shared internal package. Place this in <code>internal/types/types.go</code>.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PipelineConfig holds the parsed and validated pipeline configuration.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PipelineConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Raw YAML source (or file path). Useful for debugging.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Source         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Map of job names to their definition.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Jobs           </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">JobConfig</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Global environment variables defined in the pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Environment    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Matrix definitions for parallel job expansion.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MatrixAxes     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// JobConfig defines a single job within a pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> JobConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RunsOn       </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">   // Docker image, e.g., \"ubuntu:latest\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Steps        []</span><span style=\"color:#B392F0\">StepConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Needs        []</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Dependencies on other jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Environment  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Matrix       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#6A737D\">// Per-job matrix overrides</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StepConfig defines a single shell command or action.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StepConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Command </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Env     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    If      </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Conditional expression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PipelineRun represents a single execution instance of a pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PipelineRun</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status    </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // e.g., STATUS_PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Trigger   </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // EVENT_PUSH, EVENT_PULL_REQUEST, EVENT_TAG</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CommitSHA </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Branch    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartedAt </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FinishedAt </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Config    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineConfig</span><span style=\"color:#6A737D\"> // Reference to the pipeline config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// JobRun represents a single execution instance of a job.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> JobRun</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PipelineRunID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JobName      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AssignedWorker </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Worker ID that claimed this job</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContainerID  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LogKey       </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Reference to where logs are stored (e.g., file path, object storage key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ArtifactKeys []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartedAt    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FinishedAt   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Env          </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Resolved environment for this specific run</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StepRun represents the execution of a single step.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StepRun</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JobRunID  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StepName  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Command   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExitCode  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Output    </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // May be truncated; full logs via LogKey</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Store defines the interface for persistent storage of runs.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implementations can be in-memory, SQLite, PostgreSQL, etc.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Store</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // PipelineRun operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreatePipelineRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">run</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetPipelineRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdatePipelineRunStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListPipelineRuns</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // JobRun operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">job</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateJobRunStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateJobRunLogKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logKey</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListJobRunsForPipeline</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">pipelineRunID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // StepRun operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateStepRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">step</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StepRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateStepRunOutput</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">output</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exitCode</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListStepRunsForJob</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">jobRunID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StepRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton: Main Server Entry Point</strong></p>\n<p>The <code>cmd/ci-server/main.go</code> ties all components together. This is a skeleton showing the orchestration.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> main</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os/signal</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">syscall</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/config</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/dashboard</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/orchestrator</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/orchestrator/queue/memory</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/orchestrator/store/sqlite</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/webhook</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/worker</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithCancel</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 1. Initialize storage (using SQLite for simplicity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sqlite.</span><span style=\"color:#B392F0\">NewStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ci_pipeline.db\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to initialize store: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> store.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 2. Initialize the job queue (in-memory for single process)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    jobQueue </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> memory.</span><span style=\"color:#B392F0\">NewQueue</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 3. Create the Orchestrator, injecting dependencies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    orch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> orchestrator.</span><span style=\"color:#B392F0\">NewOrchestrator</span><span style=\"color:#E1E4E8\">(store, jobQueue)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 4. Create the Webhook Handler, passing a callback to the Orchestrator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhookHandler </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> webhook.</span><span style=\"color:#B392F0\">NewHandler</span><span style=\"color:#E1E4E8\">(store, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">event</span><span style=\"color:#B392F0\"> webhook</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Event</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Load the pipeline config for the repository/event from a file (e.g., .ci/pipeline.yml)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Call orch.CreatePipelineRun with the config and event details</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 5. Create the Dashboard server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dashboardServer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> dashboard.</span><span style=\"color:#B392F0\">NewServer</span><span style=\"color:#E1E4E8\">(store, orch)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 6. Optionally, start an in-process worker pool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    workerPool </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> worker.</span><span style=\"color:#B392F0\">NewPool</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, jobQueue, store) </span><span style=\"color:#6A737D\">// 2 concurrent workers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> workerPool.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 7. Set up HTTP routing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewServeMux</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/webhook/github\"</span><span style=\"color:#E1E4E8\">, webhookHandler.HandleGitHubWebhook)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/runs\"</span><span style=\"color:#E1E4E8\">, dashboardServer.HandleListRuns)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/runs/\"</span><span style=\"color:#E1E4E8\">, dashboardServer.HandleGetRun)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/logs/\"</span><span style=\"color:#E1E4E8\">, dashboardServer.HandleStreamLogs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">Handle</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, http.</span><span style=\"color:#B392F0\">FileServer</span><span style=\"color:#E1E4E8\">(http.</span><span style=\"color:#B392F0\">Dir</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"./internal/dashboard/assets\"</span><span style=\"color:#E1E4E8\">)))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    srv </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:    </span><span style=\"color:#9ECBFF\">\":8080\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler: mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 8. Graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sig </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Signal</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        signal.</span><span style=\"color:#B392F0\">Notify</span><span style=\"color:#E1E4E8\">(sig, syscall.SIGINT, syscall.SIGTERM)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        &#x3C;-</span><span style=\"color:#E1E4E8\">sig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Shutting down server...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        srv.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Server starting on :8080\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> srv.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.ErrServerClosed {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Server failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li><strong>Dependency Injection</strong>: Use interfaces (like <code>Store</code> and <code>Queue</code>) liberally. Pass dependencies as parameters to constructors (as shown above). This makes components testable and interchangeable.</li>\n<li><strong>Context Propagation</strong>: Use <code>context.Context</code> throughout for cancellation and timeouts, especially in long-running operations like job execution and log streaming.</li>\n<li><strong>Error Handling</strong>: In Go, handle errors immediately where they occur. For the orchestrator, consider defining custom error types (e.g., <code>ErrConfigNotFound</code>, <code>ErrJobDependencyCycle</code>) for better error reporting.</li>\n<li><strong>Concurrency</strong>: Use <code>sync.WaitGroup</code> to coordinate goroutines, like waiting for all workers to finish on shutdown. Use <code>sync.RWMutex</code> to protect in-memory data structures in the simple queue and store implementations.</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>This section defines the core data structures that represent the <strong>blueprint</strong>, <strong>execution records</strong>, and <strong>runtime state</strong> of our CI/CD Pipeline Orchestrator. Just as a construction project requires architectural drawings (blueprints), work orders (execution plans), and daily progress reports (runtime logs), our system needs three fundamental categories of data: configuration models that define what to do, run models that track ongoing and completed executions, and the supporting infrastructure that persists this information reliably. This data model serves as the shared language between all system components, ensuring the Webhook Listener, Orchestrator, Worker Pool, and Dashboard all understand the same concepts of pipelines, jobs, and steps.</p>\n<h3 id=\"core-types-and-structures\">Core Types and Structures</h3>\n<p>Think of the data model as a <strong>family tree of execution</strong>. The <code>PipelineConfig</code> is the founding ancestor—a static definition. Each time it&#39;s triggered, it spawns a <code>PipelineRun</code> (a child), which then gives birth to multiple <code>JobRun</code> grandchildren. Each <code>JobRun</code> further produces <code>StepRun</code> great-grandchildren, representing the finest granularity of work. This hierarchy ensures traceability from a single shell command back to the original code change that initiated it.</p>\n<p>The following tables define each entity with all its fields, their types, and their purpose. These exact names and structures must be used consistently across the codebase.</p>\n<h4 id=\"configuration-entities-blueprint\">Configuration Entities (Blueprint)</h4>\n<p>These structures represent the parsed and validated pipeline definition. They are <strong>immutable</strong> after parsing and define the &quot;what&quot; and &quot;in what order&quot; of the work.</p>\n<p><strong>Table: <code>PipelineConfig</code> (The Master Blueprint)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Source</code></td>\n<td><code>string</code></td>\n<td>The source location of the original configuration file (e.g., repository URL and path). Useful for debugging and auditing.</td>\n</tr>\n<tr>\n<td><code>Jobs</code></td>\n<td><code>map[string]JobConfig</code></td>\n<td>A dictionary of all jobs defined in the pipeline, keyed by the job&#39;s unique name. This map is the primary definition of work units.</td>\n</tr>\n<tr>\n<td><code>Environment</code></td>\n<td><code>map[string]string</code></td>\n<td>Global environment variables that should be available to all jobs in the pipeline, unless overridden at the job or step level.</td>\n</tr>\n<tr>\n<td><code>MatrixAxes</code></td>\n<td><code>map[string][]interface{}</code></td>\n<td><strong>Optional.</strong> Defines axes for matrix builds. Each key is an axis name (e.g., <code>go_version</code>), and the value is a list of possible values (e.g., <code>[&quot;1.19&quot;, &quot;1.20&quot;]</code>). The orchestrator will compute the cartesian product to expand into multiple job instances.</td>\n</tr>\n</tbody></table>\n<p><strong>Table: <code>JobConfig</code> (An Individual Work Unit Blueprint)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>The unique identifier for this job within the pipeline (e.g., <code>&quot;test-linux&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>RunsOn</code></td>\n<td><code>string</code></td>\n<td>The Docker image or a runner label specifying the execution environment (e.g., <code>&quot;ubuntu:22.04&quot;</code> or <code>&quot;linux-arm64&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>Steps</code></td>\n<td><code>[]StepConfig</code></td>\n<td>An ordered list of shell commands or actions to execute within the job&#39;s container. Execution stops if any step fails.</td>\n</tr>\n<tr>\n<td><code>Needs</code></td>\n<td><code>[]string</code></td>\n<td><strong>Optional.</strong> A list of job names that must complete successfully before this job can start. This defines the dependency graph (DAG). If empty, the job can start immediately.</td>\n</tr>\n<tr>\n<td><code>Environment</code></td>\n<td><code>map[string]string</code></td>\n<td><strong>Optional.</strong> Job-specific environment variables, which merge with and override pipeline-level environment variables.</td>\n</tr>\n<tr>\n<td><code>Matrix</code></td>\n<td><code>map[string]interface{}</code></td>\n<td><strong>Optional.</strong> A job-specific matrix definition. If present, this job will be expanded into multiple instances using the cartesian product of the defined axes. This overrides any pipeline-level <code>MatrixAxes</code>.</td>\n</tr>\n</tbody></table>\n<p><strong>Table: <code>StepConfig</code> (A Single Command Instruction)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>A human-readable label for this step (e.g., <code>&quot;Run unit tests&quot;</code>). Used for logging and UI display.</td>\n</tr>\n<tr>\n<td><code>Command</code></td>\n<td><code>string</code></td>\n<td>The shell command to execute (e.g., <code>&quot;go test ./...&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>Env</code></td>\n<td><code>map[string]string</code></td>\n<td><strong>Optional.</strong> Step-specific environment variables, which merge with and override job and pipeline-level variables.</td>\n</tr>\n<tr>\n<td><code>If</code></td>\n<td><code>string</code></td>\n<td><strong>Optional.</strong> A conditional expression that determines if the step should run. Evaluated at runtime. Common expressions check branch name (<code>github.ref == &#39;refs/heads/main&#39;</code>) or event type.</td>\n</tr>\n</tbody></table>\n<h4 id=\"run-entities-execution-records\">Run Entities (Execution Records)</h4>\n<p>These structures represent a <strong>specific instance</strong> of pipeline execution. They are <strong>mutable</strong>, tracking the live state, timings, and results as work progresses.</p>\n<p><strong>Table: <code>PipelineRun</code> (A Single Pipeline Execution Instance)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ID</code></td>\n<td><code>string</code></td>\n<td>A globally unique identifier for this run (e.g., a UUID). Used in URLs and for cross-referencing.</td>\n</tr>\n<tr>\n<td><code>Status</code></td>\n<td><code>string</code></td>\n<td>The current high-level state of the pipeline run. Must be one of the <code>STATUS_</code> constants (e.g., <code>STATUS_RUNNING</code>).</td>\n</tr>\n<tr>\n<td><code>Trigger</code></td>\n<td><code>string</code></td>\n<td>The webhook event type that initiated this run (e.g., <code>EVENT_PUSH</code>, <code>EVENT_PULL_REQUEST</code>).</td>\n</tr>\n<tr>\n<td><code>CommitSHA</code></td>\n<td><code>string</code></td>\n<td>The full Git commit hash that triggered the pipeline (e.g., <code>&quot;a1b2c3d...&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>Branch</code></td>\n<td><code>string</code></td>\n<td>The Git branch name from the triggering event (e.g., <code>&quot;feature/login&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>CreatedAt</code></td>\n<td><code>time.Time</code></td>\n<td>The timestamp when the <code>PipelineRun</code> was first created (when the webhook was processed).</td>\n</tr>\n<tr>\n<td><code>UpdatedAt</code></td>\n<td><code>time.Time</code></td>\n<td>The timestamp of the last status update. Useful for detecting stalled runs.</td>\n</tr>\n<tr>\n<td><code>StartedAt</code></td>\n<td><code>*time.Time</code></td>\n<td><strong>Optional.</strong> The timestamp when the first job in the pipeline began execution. <code>nil</code> if the pipeline hasn&#39;t started.</td>\n</tr>\n<tr>\n<td><code>FinishedAt</code></td>\n<td><code>*time.Time</code></td>\n<td><strong>Optional.</strong> The timestamp when the final job in the pipeline completed (successfully or not). <code>nil</code> if the pipeline is still running.</td>\n</tr>\n<tr>\n<td><code>Config</code></td>\n<td><code>*PipelineConfig</code></td>\n<td>A <strong>pointer</strong> to the immutable <code>PipelineConfig</code> that defines this run&#39;s structure. This is a denormalization for convenience; the config could be stored separately and linked by a foreign key.</td>\n</tr>\n</tbody></table>\n<p><strong>Table: <code>JobRun</code> (An Instance of a Job Execution)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ID</code></td>\n<td><code>string</code></td>\n<td>A unique identifier for this job execution (e.g., a UUID).</td>\n</tr>\n<tr>\n<td><code>PipelineRunID</code></td>\n<td><code>string</code></td>\n<td>The ID of the parent <code>PipelineRun</code>. Establishes the &quot;belongs to&quot; relationship.</td>\n</tr>\n<tr>\n<td><code>JobName</code></td>\n<td><code>string</code></td>\n<td>The name of the job as defined in the original <code>JobConfig</code> (e.g., <code>&quot;test-go-1.19&quot;</code>). This is the key to look up the configuration in <code>PipelineConfig.Jobs</code>.</td>\n</tr>\n<tr>\n<td><code>Status</code></td>\n<td><code>string</code></td>\n<td>The current state of the job. Must be one of the <code>STATUS_</code> constants.</td>\n</tr>\n<tr>\n<td><code>AssignedWorker</code></td>\n<td><code>string</code></td>\n<td><strong>Optional.</strong> The identifier of the worker process that picked up this job from the queue. Used for monitoring and cleanup.</td>\n</tr>\n<tr>\n<td><code>ContainerID</code></td>\n<td><code>string</code></td>\n<td><strong>Optional.</strong> The Docker container ID where the job is executing (or executed). Crucial for log retrieval and forced cleanup.</td>\n</tr>\n<tr>\n<td><code>LogKey</code></td>\n<td><code>string</code></td>\n<td><strong>Optional.</strong> A reference to where the job&#39;s combined stdout/stderr logs are stored (e.g., a file path or an object storage key).</td>\n</tr>\n<tr>\n<td><code>ArtifactKeys</code></td>\n<td><code>[]string</code></td>\n<td><strong>Optional.</strong> A list of references to artifacts produced by this job (e.g., paths in persistent storage).</td>\n</tr>\n<tr>\n<td><code>StartedAt</code></td>\n<td><code>*time.Time</code></td>\n<td><strong>Optional.</strong> The timestamp when the job&#39;s container started and the first step began.</td>\n</tr>\n<tr>\n<td><code>FinishedAt</code></td>\n<td><code>*time.Time</code></td>\n<td><strong>Optional.</strong> The timestamp when the job&#39;s final step completed.</td>\n</tr>\n<tr>\n<td><code>Env</code></td>\n<td><code>map[string]string</code></td>\n<td>The final, fully-resolved environment variables for this specific job run, including pipeline, job, matrix, and system variables. This is a snapshot used for execution.</td>\n</tr>\n</tbody></table>\n<p><strong>Table: <code>StepRun</code> (A Record of a Single Command Execution)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ID</code></td>\n<td><code>string</code></td>\n<td>A unique identifier for this step execution.</td>\n</tr>\n<tr>\n<td><code>JobRunID</code></td>\n<td><code>string</code></td>\n<td>The ID of the parent <code>JobRun</code>.</td>\n</tr>\n<tr>\n<td><code>StepName</code></td>\n<td><code>string</code></td>\n<td>The name of the step from the <code>StepConfig</code>.</td>\n</tr>\n<tr>\n<td><code>Command</code></td>\n<td><code>string</code></td>\n<td>The actual shell command that was executed, after any environment variable substitution.</td>\n</tr>\n<tr>\n<td><code>ExitCode</code></td>\n<td><code>int</code></td>\n<td>The exit code returned by the shell after executing the command. <code>0</code> indicates success; non-zero indicates failure.</td>\n</tr>\n<tr>\n<td><code>Output</code></td>\n<td><code>string</code></td>\n<td><strong>Optional.</strong> The combined stdout and stderr output of the command. For very large outputs, this may be truncated, and the full logs should be read from the <code>JobRun.LogKey</code>.</td>\n</tr>\n<tr>\n<td><code>StartedAt</code></td>\n<td><code>time.Time</code></td>\n<td>The timestamp when the command began execution.</td>\n</tr>\n<tr>\n<td><code>Duration</code></td>\n<td><code>time.Duration</code></td>\n<td>How long the command took to execute. Can be calculated as <code>FinishedAt - StartedAt</code>.</td>\n</tr>\n</tbody></table>\n<h3 id=\"relationships-and-lifecycle\">Relationships and Lifecycle</h3>\n<blockquote>\n<p>The data model relationships form a <strong>composition hierarchy</strong>: a <code>PipelineRun</code> <strong>has many</strong> <code>JobRun</code>s, and a <code>JobRun</code> <strong>has many</strong> <code>StepRun</code>s. This is a classic one-to-many relationship that can be efficiently represented in a relational database with foreign keys or in a document store with nested documents. The configuration (<code>PipelineConfig</code>, <code>JobConfig</code>, <code>StepConfig</code>) is referenced but not owned by the runs, as the same configuration can spawn many runs over time.</p>\n</blockquote>\n<p><img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-data-model.svg\" alt=\"Data Model Relationship Diagram\"></p>\n<h4 id=\"entity-relationships\">Entity Relationships</h4>\n<p>The diagram above illustrates the following key relationships:</p>\n<ol>\n<li><strong><code>PipelineRun</code> to <code>JobRun</code> (One-to-Many):</strong> A single pipeline execution consists of multiple job executions. The <code>JobRun.PipelineRunID</code> field is the foreign key linking back. When a pipeline is triggered, the orchestrator creates one <code>JobRun</code> for each job defined in the <code>PipelineConfig.Jobs</code> map, after applying matrix expansion. The <code>JobRun.JobName</code> field matches a key in that map.</li>\n<li><strong><code>JobRun</code> to <code>StepRun</code> (One-to-Many):</strong> Each job execution sequences through its defined steps. The <code>StepRun.JobRunID</code> links to its parent. The worker creates a <code>StepRun</code> record just before executing each command in <code>JobConfig.Steps</code>.</li>\n<li><strong><code>PipelineRun</code> references <code>PipelineConfig</code> (One-to-One):</strong> The <code>PipelineRun.Config</code> field points to the full configuration used for that run. This is a <strong>denormalization</strong> for performance and simplicity, allowing any component to inspect the run&#39;s definition without a separate lookup. In a persistent storage strategy, we might store the config as a serialized JSON blob alongside the run record or in a separate table linked by ID.</li>\n<li><strong><code>JobRun</code> references <code>JobConfig</code> (Indirect):</strong> <code>JobRun</code> does not directly store a <code>JobConfig</code>. Instead, it stores the <code>JobName</code>, which is used to look up the configuration within the parent <code>PipelineRun.Config.Jobs</code> map. This ensures consistency—the run always uses the configuration that was active at the time of its creation.</li>\n</ol>\n<h4 id=\"state-lifecycles\">State Lifecycles</h4>\n<p>Each run entity has a well-defined lifecycle, moving through a series of states. These states are not just for display; they drive the system&#39;s logic (e.g., a job with <code>STATUS_SUCCEEDED</code> allows its dependent jobs to start).</p>\n<p><strong>PipelineRun State Flow:</strong>\nThe pipeline&#39;s overall status is typically derived from the states of its constituent jobs. A common aggregation logic is:</p>\n<ul>\n<li>If any job has <code>STATUS_FAILED</code>, the pipeline is <code>STATUS_FAILED</code>.</li>\n<li>If all jobs are <code>STATUS_SUCCEEDED</code>, the pipeline is <code>STATUS_SUCCEEDED</code>.</li>\n<li>If all jobs are either <code>STATUS_SUCCEEDED</code> or <code>STATUS_SKIPPED</code>, the pipeline is <code>STATUS_SUCCEEDED</code>.</li>\n<li>If any job is <code>STATUS_RUNNING</code>, the pipeline is <code>STATUS_RUNNING</code>.</li>\n<li>Otherwise, it&#39;s <code>STATUS_PENDING</code>.</li>\n</ul>\n<p><strong>JobRun State Machine:</strong>\nThe job state machine is more explicit and is managed by the orchestrator and worker components.</p>\n<p><img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-job-state-machine.svg\" alt=\"Job Run State Machine\"></p>\n<p><strong>Table: JobRun State Transitions</strong></p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Action Taken by System</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STATUS_PENDING</code></td>\n<td><code>start</code> (job dequeued by a worker)</td>\n<td><code>STATUS_RUNNING</code></td>\n<td>Worker updates <code>JobRun.Status</code>, sets <code>StartedAt</code>, and begins container setup.</td>\n</tr>\n<tr>\n<td><code>STATUS_RUNNING</code></td>\n<td><code>finish(success)</code> (all steps exited with code 0)</td>\n<td><code>STATUS_SUCCEEDED</code></td>\n<td>Worker updates status, sets <code>FinishedAt</code>, collects artifacts, and cleans up the container.</td>\n</tr>\n<tr>\n<td><code>STATUS_RUNNING</code></td>\n<td><code>finish(error)</code> (any step failed, or container error)</td>\n<td><code>STATUS_FAILED</code></td>\n<td>Worker updates status, sets <code>FinishedAt</code>, saves logs, and cleans up the container.</td>\n</tr>\n<tr>\n<td><code>STATUS_PENDING</code> or <code>STATUS_RUNNING</code></td>\n<td><code>cancel</code> (user request or timeout)</td>\n<td><code>STATUS_CANCELLED</code></td>\n<td>Orchestrator or worker sends SIGKILL to container, updates status, and performs cleanup.</td>\n</tr>\n<tr>\n<td><code>STATUS_PENDING</code></td>\n<td><code>skip</code> (conditional <code>if</code> expression evaluated to false)</td>\n<td><code>STATUS_SKIPPED</code></td>\n<td>Orchestrator evaluates condition before enqueuing job; marks it skipped without worker assignment.</td>\n</tr>\n<tr>\n<td>Any state</td>\n<td><code>retry</code> (manual user action)</td>\n<td><code>STATUS_PENDING</code></td>\n<td>A new <code>JobRun</code> is typically created, but the old one retains its final state for history.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight:</strong> The <code>STATUS_SKIPPED</code> state is special. It is determined during the pipeline&#39;s initial &quot;planning&quot; phase by the orchestrator, based on the <code>StepConfig.If</code> or <code>JobConfig</code> conditions. A skipped job never consumes a worker or a container, but it remains in the run history for a complete audit trail.</p>\n</blockquote>\n<h3 id=\"storage-strategy-adr\">Storage Strategy ADR</h3>\n<blockquote>\n<p><strong>Decision: Use SQLite as the Primary Persistent Store for Run History and Job Queue</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a storage mechanism for pipeline and job run records, their statuses, logs, and artifacts metadata. This storage must support concurrent access from multiple components (Webhook Listener, Orchestrator, Workers, Dashboard) and persist across system restarts. The project emphasizes simplicity for educational purposes and ease of setup, but also requires reliability for core queue operations to prevent job loss.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>SQLite (Single-file relational database)</strong></li>\n<li><strong>PostgreSQL (Dedicated relational database server)</strong></li>\n<li><strong>In-memory storage with periodic file backup (e.g., gob encoding)</strong></li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use SQLite as the primary store for all run-related entities (<code>PipelineRun</code>, <code>JobRun</code>, <code>StepRun</code>). The job queue will also be backed by a SQLite table, using transactional <code>SELECT ... FOR UPDATE SKIP LOCKED</code> patterns for reliable, concurrent job dequeuing.</li>\n<li><strong>Rationale</strong>:<ol>\n<li><strong>Simplicity and Zero Dependencies</strong>: SQLite requires no separate server process or setup. The entire database is a single file, making deployment and backup trivial. This aligns perfectly with the project&#39;s educational goal of focusing on CI/CD logic rather than infrastructure.</li>\n<li><strong>Adequate Concurrency for Learning Scale</strong>: While not designed for high-write, distributed scenarios, SQLite&#39;s write-ahead log (WAL) mode supports multiple concurrent readers and one writer efficiently. For a learning project with a modest number of concurrent jobs (e.g., &lt;10 workers), this is more than sufficient. It correctly handles the transactional semantics needed for the job queue.</li>\n<li><strong>Relational Model Fit</strong>: Our data model is inherently relational (run -&gt; job -&gt; step). SQLite allows us to define schemas, foreign keys, and perform efficient joins (e.g., fetching all jobs for a pipeline run for the dashboard), which would be more complex in a simple file-based store.</li>\n<li><strong>Persistence Guarantees</strong>: Unlike a pure in-memory store, SQLite provides ACID transactions. This ensures that once a webhook is processed and a <code>PipelineRun</code> is committed, it survives a system crash. This is a non-negotiable requirement for a reliable CI system.</li>\n</ol>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Drastically simplifies the development and testing environment. Learners can run the entire system with a single binary and a database file. The system becomes self-contained.</li>\n<li><strong>Negative</strong>: Becomes a bottleneck at very high scale (hundreds of concurrent job starts per minute). The single-writer limitation can cause contention. This is an acceptable trade-off for the project&#39;s scope.</li>\n<li><strong>Mitigation</strong>: The design isolates storage behind the <code>Store</code> interface. If scaling is needed later, the implementation can be swapped for PostgreSQL or a distributed queue like Redis without changing the core application logic.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Table: Storage Options Comparison</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Suitability for Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>SQLite</strong></td>\n<td>Zero setup, single file, ACID transactions, good enough concurrency in WAL mode.</td>\n<td>Single-writer bottleneck, not distributed, less performant under extreme write load.</td>\n<td><strong>CHOSEN.</strong> Excellent fit for simplicity, reliability, and educational goals.</td>\n</tr>\n<tr>\n<td><strong>PostgreSQL</strong></td>\n<td>High concurrency, robust, scalable, full-featured SQL.</td>\n<td>Requires running a separate database server, adds deployment complexity.</td>\n<td>Overkill for the initial learning project. A good choice for a &quot;Phase 2&quot; scaling enhancement.</td>\n</tr>\n<tr>\n<td><strong>In-Memory + File Backup</strong></td>\n<td>Extremely fast, simple to implement for basic reads/writes.</td>\n<td>Data loss on crash unless complex flushing logic is added. Poor support for concurrent access and complex queries.</td>\n<td>Unreliable for a core system component. Could be used for a non-persistent cache, but not for primary storage.</td>\n</tr>\n</tbody></table>\n<p><strong>Common Pitfalls: Data Storage</strong></p>\n<p>⚠️ <strong>Pitfall: Ignoring Database Migrations</strong></p>\n<ul>\n<li><strong>Description</strong>: Hardcoding the database schema creation (<code>CREATE TABLE ...</code>) in the application startup logic. When you need to add a new field (like <code>Branch</code> to <code>PipelineRun</code>), you either drop all existing data or face complex manual upgrade steps.</li>\n<li><strong>Why it&#39;s wrong</strong>: It makes the application brittle and destroys valuable historical run data during development and updates.</li>\n<li><strong>Fix</strong>: Use a simple migration system from day one. This can be as basic as a <code>migrations/</code> folder containing sequential SQL files (e.g., <code>001_initial_schema.sql</code>, <code>002_add_branch_column.sql</code>). The application checks a <code>schema_version</code> table on startup and applies any missing migrations.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Storing Large Logs in the Database</strong></p>\n<ul>\n<li><strong>Description</strong>: Storing the full stdout/stderr log text (which can be megabytes) directly in the <code>StepRun.Output</code> or <code>JobRun</code> table.</li>\n<li><strong>Why it&#39;s wrong</strong>: Bloating the database table makes queries slower, backups huge, and can hit size limits. It also makes real-time streaming less efficient.</li>\n<li><strong>Fix</strong>: Follow the model defined above: store logs externally (e.g., in a rotating file on disk, or in an object store like S3/MinIO) and keep only a small reference (<code>LogKey</code>) in the database. The <code>StepRun.Output</code> should contain only a truncated preview (last few hundred lines) for quick UI display.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Using Transactions for Queue Operations</strong></p>\n<ul>\n<li><strong>Description</strong>: Implementing the job queue with a simple <code>SELECT</code> followed by an <code>UPDATE</code> to mark a job as &quot;taken,&quot; without wrapping it in a transaction.</li>\n<li><strong>Why it&#39;s wrong</strong>: Under concurrency, two workers might <code>SELECT</code> the same <code>STATUS_PENDING</code> job simultaneously, leading to duplicate execution—a critical bug in a CI system.</li>\n<li><strong>Fix</strong>: Use a transactional pattern. For SQLite, this looks like:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">    BEGIN</span><span style=\"color:#F97583\"> IMMEDIATE</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    SELECT</span><span style=\"color:#E1E4E8\"> id, ... </span><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> job_run </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#F97583\"> status</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'pending'</span><span style=\"color:#F97583\"> ORDER BY</span><span style=\"color:#E1E4E8\"> created_at </span><span style=\"color:#F97583\">LIMIT</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> FOR</span><span style=\"color:#F97583\"> UPDATE</span><span style=\"color:#F97583\"> SKIP</span><span style=\"color:#E1E4E8\"> LOCKED;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Application logic: if a row is found, update its status to 'running' and set assigned_worker.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    UPDATE</span><span style=\"color:#E1E4E8\"> job_run </span><span style=\"color:#F97583\">SET</span><span style=\"color:#F97583\"> status</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'running'</span><span style=\"color:#E1E4E8\">, assigned_worker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ? </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#E1E4E8\"> id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ?;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    COMMIT</span><span style=\"color:#E1E4E8\">;</span></span></code></pre></div>\n<pre><code>The `FOR UPDATE SKIP LOCKED` clause (or equivalent with `BEGIN IMMEDIATE` and careful locking in SQLite) ensures safe concurrent access.\n</code></pre>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Storage &amp; ORM</strong></td>\n<td>SQLite with <code>database/sql</code> and manual SQL queries.</td>\n<td>Use a lightweight ORM like <code>sqlc</code> (generate type-safe Go from SQL) or <code>entgo</code>.</td>\n</tr>\n<tr>\n<td><strong>Migrations</strong></td>\n<td>Custom migration runner reading SQL files from an embedded filesystem (<code>//go:embed</code>).</td>\n<td>Use a dedicated library like <code>golang-migrate</code>.</td>\n</tr>\n<tr>\n<td><strong>Log Storage</strong></td>\n<td>Rotating files in a local directory (e.g., <code>/var/log/ciserver/jobs/</code>).</td>\n<td>Object storage (S3-compatible like MinIO for local development, AWS S3 for production).</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>build-your-own-ci/\n  cmd/\n    server/                 # Main application entry point\n      main.go\n  internal/\n    store/                 # Data model persistence layer\n      store.go             # Store interface definition\n      sqlite/              # SQLite implementation of Store\n        sqlite.go\n        migrations/        # SQL migration files\n          001_initial.sql\n          002_add_artifact_keys.sql\n        queries/           # Optional: if using sqlc\n          jobs.sql\n          runs.sql\n    models/                # Core data type definitions (structs)\n      pipeline.go          # PipelineConfig, JobConfig, StepConfig\n      run.go               # PipelineRun, JobRun, StepRun\n      constants.go         # STATUS_*, EVENT_* constants\n  ... (other components like orchestrator, worker, dashboard)</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong>\nThe following is a complete, ready-to-use foundation for the SQLite store with a basic migration mechanism.</p>\n<p><strong>File: <code>internal/store/sqlite/sqlite.go</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> sqlite</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">database/sql</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">embed</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // SQLite driver</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _ </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#B392F0\">modernc.org/sqlite</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">//go:embed migrations/*.sql</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> migrationsFS </span><span style=\"color:#B392F0\">embed</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FS</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DB wraps a sql.DB connection and provides application-specific methods.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DB</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">sql</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDB opens a connection to the SQLite database at the given path,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// applies any pending migrations, and returns a ready-to-use DB instance.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDB</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">dataSourceName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Enable WAL mode for better concurrency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dsn </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">?_journal=WAL&#x26;_timeout=5000\"</span><span style=\"color:#E1E4E8\">, dataSourceName)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sql.</span><span style=\"color:#B392F0\">Open</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"sqlite\"</span><span style=\"color:#E1E4E8\">, dsn)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"open sqlite db: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Test connection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Ping</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ping sqlite db: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">{conn: conn}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply migrations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> db.</span><span style=\"color:#B392F0\">migrate</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"migrate database: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> db, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// migrate ensures the database schema is up-to-date.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">db </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">migrate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create migrations table if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> db.conn.</span><span style=\"color:#B392F0\">Exec</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        CREATE TABLE IF NOT EXISTS schema_migrations (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            version INTEGER PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            applied_at DATETIME DEFAULT CURRENT_TIMESTAMP</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        );</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    `</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read migration files from embedded filesystem</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    files, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> migrationsFS.</span><span style=\"color:#B392F0\">ReadDir</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"migrations\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get the last applied migration version</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> currentVersion </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    row </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> db.conn.</span><span style=\"color:#B392F0\">QueryRow</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"SELECT MAX(version) FROM schema_migrations\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    row.</span><span style=\"color:#B392F0\">Scan</span><span style=\"color:#E1E4E8\">(¤tVersion) </span><span style=\"color:#6A737D\">// If NULL, currentVersion stays 0.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply migrations in order</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, file </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> files {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Simple numeric prefix: 001_initial.sql -> version 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> version </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sscanf</span><span style=\"color:#E1E4E8\">(file.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">_\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">version)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid migration filename </span><span style=\"color:#79B8FF\">%q</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, file.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">(), err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> version </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> currentVersion {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Applying migration </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, version, file.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            content, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> migrationsFS.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"migrations/\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Execute migration within a transaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tx, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> db.conn.</span><span style=\"color:#B392F0\">Begin</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tx.</span><span style=\"color:#B392F0\">Exec</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(content)); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tx.</span><span style=\"color:#B392F0\">Rollback</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to execute migration </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, version, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Record the migration version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tx.</span><span style=\"color:#B392F0\">Exec</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"INSERT INTO schema_migrations (version) VALUES (?)\"</span><span style=\"color:#E1E4E8\">, version)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tx.</span><span style=\"color:#B392F0\">Rollback</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tx.</span><span style=\"color:#B392F0\">Commit</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Migration </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> applied successfully\"</span><span style=\"color:#E1E4E8\">, version)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close closes the database connection.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">db </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> db.conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Conn returns the underlying *sql.DB (use for implementing Store methods).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">db </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Conn</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">sql</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> db.conn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>File: <code>internal/store/sqlite/migrations/001_initial.sql</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Initial schema for CI/CD Pipeline Orchestrator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Includes tables for pipeline runs, job runs, step runs, and a simple queue mechanism.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">PRAGMA foreign_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> ON</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Pipeline runs represent a single execution of a pipeline config.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> TABLE</span><span style=\"color:#B392F0\"> pipeline_runs</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    id </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> PRIMARY KEY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    status</span><span style=\"color:#F97583\"> TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#F97583\"> DEFAULT</span><span style=\"color:#9ECBFF\"> 'pending'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    trigger </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    commit_sha </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    branch </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at </span><span style=\"color:#F97583\">DATETIME</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#F97583\"> DEFAULT</span><span style=\"color:#E1E4E8\"> CURRENT_TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    updated_at </span><span style=\"color:#F97583\">DATETIME</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#F97583\"> DEFAULT</span><span style=\"color:#E1E4E8\"> CURRENT_TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at </span><span style=\"color:#F97583\">DATETIME</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    finished_at </span><span style=\"color:#F97583\">DATETIME</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config_json </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#6A737D\">  -- Serialized PipelineConfig as JSON</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Job runs represent an instance of a job within a pipeline run.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> TABLE</span><span style=\"color:#B392F0\"> job_runs</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    id </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> PRIMARY KEY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_name </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    status</span><span style=\"color:#F97583\"> TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#F97583\"> DEFAULT</span><span style=\"color:#9ECBFF\"> 'pending'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assigned_worker </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    container_id </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_key </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    artifact_keys </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\">-- JSON array of strings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at </span><span style=\"color:#F97583\">DATETIME</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    finished_at </span><span style=\"color:#F97583\">DATETIME</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    env_json </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\">-- Serialized map[string]string as JSON</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    FOREIGN KEY</span><span style=\"color:#E1E4E8\"> (pipeline_run_id) </span><span style=\"color:#F97583\">REFERENCES</span><span style=\"color:#E1E4E8\"> pipeline_runs(id) </span><span style=\"color:#F97583\">ON DELETE CASCADE</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Step runs represent the execution of a single command within a job.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> TABLE</span><span style=\"color:#B392F0\"> step_runs</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    id </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> PRIMARY KEY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_run_id </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step_name </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    command </span><span style=\"color:#F97583\">TEXT</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exit_code </span><span style=\"color:#F97583\">INTEGER</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    output</span><span style=\"color:#F97583\"> TEXT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at </span><span style=\"color:#F97583\">DATETIME</span><span style=\"color:#F97583\"> NOT NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration_ns </span><span style=\"color:#F97583\">INTEGER</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\">-- Stored as nanoseconds for precision</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    FOREIGN KEY</span><span style=\"color:#E1E4E8\"> (job_run_id) </span><span style=\"color:#F97583\">REFERENCES</span><span style=\"color:#E1E4E8\"> job_runs(id) </span><span style=\"color:#F97583\">ON DELETE CASCADE</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Indexes for efficient querying</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> INDEX</span><span style=\"color:#B392F0\"> idx_job_runs_pipeline_run_id</span><span style=\"color:#F97583\"> ON</span><span style=\"color:#E1E4E8\"> job_runs(pipeline_run_id);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> INDEX</span><span style=\"color:#B392F0\"> idx_job_runs_status</span><span style=\"color:#F97583\"> ON</span><span style=\"color:#E1E4E8\"> job_runs(</span><span style=\"color:#F97583\">status</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#F97583\"> status</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'pending'</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> INDEX</span><span style=\"color:#B392F0\"> idx_step_runs_job_run_id</span><span style=\"color:#F97583\"> ON</span><span style=\"color:#E1E4E8\"> step_runs(job_run_id);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> INDEX</span><span style=\"color:#B392F0\"> idx_pipeline_runs_created_at</span><span style=\"color:#F97583\"> ON</span><span style=\"color:#E1E4E8\"> pipeline_runs(created_at </span><span style=\"color:#F97583\">DESC</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Trigger to automatically update `updated_at` on pipeline_runs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> TRIGGER</span><span style=\"color:#B392F0\"> update_pipeline_runs_timestamp</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">AFTER</span><span style=\"color:#F97583\"> UPDATE</span><span style=\"color:#F97583\"> ON</span><span style=\"color:#E1E4E8\"> pipeline_runs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">BEGIN</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    UPDATE</span><span style=\"color:#E1E4E8\"> pipeline_runs </span><span style=\"color:#F97583\">SET</span><span style=\"color:#E1E4E8\"> updated_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CURRENT_TIMESTAMP </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#E1E4E8\"> id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> NEW</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">id</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">END</span><span style=\"color:#E1E4E8\">;</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong>\nThe <code>Store</code> interface defines the persistence contract. Here is its definition and a skeleton for its SQLite implementation.</p>\n<p><strong>File: <code>internal/store/store.go</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> store</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Store defines the interface for all persistence operations.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Store</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // PipelineRun operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreatePipelineRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">run</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetPipelineRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdatePipelineRunStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListPipelineRuns</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // JobRun operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateJobRunStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateJobRunAssignedWorker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">workerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateJobRunWithContainerInfo</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">containerID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logKey</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    MarkJobRunFinished</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">finishedAt</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    AddArtifactKeyToJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobRunID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">artifactKey</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // StepRun operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateStepRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">step</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StepRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateStepRunOutput</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">output</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    MarkStepRunFinished</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exitCode</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Queue operations (implemented using the job_runs table)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    EnqueueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobRunID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DequeueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">workerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Close releases any resources held by the store.</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>File: <code>internal/store/sqlite/queries.go</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> sqlite</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">database/sql</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/build-your-own-ci/internal/store</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Ensure *DB implements store.Store</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Store</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">)(</span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreatePipelineRun implements store.Store.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">db </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreatePipelineRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">run</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Serialize run.Config to JSON (use json.Marshal)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Prepare SQL INSERT statement for pipeline_runs table with all fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute the INSERT, using the generated ID from the run parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle potential errors (e.g., duplicate ID) and return appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use db.conn.ExecContext or db.conn.PrepareContext</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DequeueJobRun implements store.Store.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This is a critical method that must use a transaction to avoid double-processing.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">db </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DequeueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">workerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Begin an immediate transaction (db.conn.BeginTx with isolation level sql.LevelSerializable)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Query for a single pending job: SELECT ... FROM job_runs WHERE status = 'pending' ORDER BY created_at LIMIT 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If a row is found, update its status to 'running' and set assigned_worker to workerID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Commit the transaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If the transaction succeeded and a row was updated, deserialize the env_json and artifact_keys fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Construct and return a models.JobRun object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: If no row was found, return (nil, nil) — no error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Ensure proper rollback on any error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Important: Use FOR UPDATE SKIP LOCKED style locking if your SQLite driver supports it, or rely on the transaction isolation.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ListPipelineRuns implements store.Store.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">db </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DB</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ListPipelineRuns</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Query pipeline_runs ordered by created_at DESC with LIMIT and OFFSET</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through rows, deserialize config_json for each run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Build a slice of *models.PipelineRun</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return the slice and any error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints (Go)</strong></p>\n<ul>\n<li>Use <code>encoding/json</code> to serialize <code>PipelineConfig</code> and <code>Env</code> maps for storage. Remember to handle <code>time.Time</code> fields correctly (they serialize to RFC3339 by default).</li>\n<li>Use <code>sql.NullString</code> and <code>sql.NullTime</code> for nullable database fields if you don&#39;t want to use pointers. The starter code uses pointers (<code>*time.Time</code>) for clarity in the models.</li>\n<li>The <code>modernc.org/sqlite</code> driver is a pure-Go alternative to CGO-based drivers, simplifying cross-compilation.</li>\n<li>For the <code>Duration</code> field in <code>StepRun</code>, store it as an integer number of nanoseconds in the database for precision and easy conversion (<code>time.Duration</code> is an alias for <code>int64</code> nanoseconds in Go).</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong>\nAfter implementing the data model and basic store methods, verify your setup:</p>\n<ol>\n<li>Run the SQLite migrations: Start your application; it should create a <code>.db</code> file and the tables without errors.</li>\n<li>Write a small test program in <code>cmd/teststore/main.go</code> that:<ul>\n<li>Creates a <code>DB</code> instance.</li>\n<li>Creates a simple <code>PipelineRun</code> with a minimal <code>PipelineConfig</code> and inserts it.</li>\n<li>Queries it back and prints the result.</li>\n<li>Runs <code>DequeueJobRun</code> (should return <code>nil, nil</code> as no pending jobs exist).</li>\n</ul>\n</li>\n<li>Use the SQLite CLI to inspect the database: <code>sqlite3 ciserver.db &quot;SELECT * FROM pipeline_runs;&quot;</code></li>\n</ol>\n<p><strong>G. Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Database is locked&quot; errors under concurrency.</td>\n<td>SQLite&#39;s default journaling mode (<code>DELETE</code>) doesn&#39;t handle concurrent writes well.</td>\n<td>Check the journal mode: <code>PRAGMA journal_mode;</code></td>\n<td>Enable Write-Ahead Logging by appending <code>_journal=WAL</code> to your DSN (already done in starter code).</td>\n</tr>\n<tr>\n<td><code>DequeueJobRun</code> gives the same job to two workers.</td>\n<td>The dequeuing logic is not properly transactional or lacks locking.</td>\n<td>Add debug logs before and after the transaction; check if two workers call it at nearly the same time.</td>\n<td>Ensure you use <code>BEGIN IMMEDIATE</code> and update the row within the same transaction before committing. The <code>FOR UPDATE SKIP LOCKED</code> pattern is ideal but verify your SQLite version supports it.</td>\n</tr>\n<tr>\n<td>JSON serialization of <code>PipelineConfig</code> fails.</td>\n<td>Circular references or unsupported types in the struct (e.g., a <code>map[string]interface{}</code> with values that <code>json</code> can&#39;t handle).</td>\n<td>Print the struct before marshalling with <code>fmt.Printf(&quot;%+v&quot;, config)</code>.</td>\n<td>Ensure all nested types are JSON-serializable. Consider using <code>json.RawMessage</code> for the <code>config_json</code> field if you need to delay parsing.</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-pipeline-configuration-parser-milestone-1\">Component: Pipeline Configuration Parser (Milestone 1)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1</p>\n</blockquote>\n<p>This component is the foundational translator that converts human-readable pipeline definitions into a structured, executable plan the CI system can understand and act upon. It sits at the very beginning of the pipeline lifecycle, transforming a static YAML configuration file into a dynamic <strong>Directed Acyclic Graph (DAG)</strong> of jobs, ready for the execution engine. Its correctness is paramount—a misparsed configuration can lead to failed builds, security vulnerabilities, or incorrect automation.</p>\n<h3 id=\"mental-model-the-blueprint-interpreter\">Mental Model: The Blueprint Interpreter</h3>\n<p>Imagine you&#39;re an architect reviewing a set of blueprint drawings for a new building. The blueprints (your YAML file) contain symbols, notes, and diagrams that specify what to build, in what order, and with what materials. Your job as the architect is to interpret these drawings and produce a precise <strong>bill of materials</strong> and <strong>construction schedule</strong> that foremen and workers can execute. You must:</p>\n<ol>\n<li><strong>Decode the symbols</strong> (parse YAML syntax).</li>\n<li><strong>Check for errors and contradictions</strong> (validate structure).</li>\n<li><strong>Resolve any placeholder references</strong> (substitute environment variables).</li>\n<li><strong>Expand any &quot;optional wings&quot; that have multiple design choices</strong> (expand matrix builds).</li>\n<li><strong>Create a task dependency chart</strong> showing which foundations must be poured before which walls can be built (build the job DAG).</li>\n</ol>\n<p>This component is that architect. It takes the creative, declarative &quot;what&quot; from the developer and converts it into the procedural, unambiguous &quot;how&quot; for the CI system&#39;s execution engine. A flawed interpretation leads to a building that collapses; a correct one ensures a smooth, automated assembly line.</p>\n<h3 id=\"interface-and-api\">Interface and API</h3>\n<p>The parser exposes a clean, functional interface centered around the <code>PipelineConfig</code> structure. Its primary job is to produce this validated configuration object from raw YAML text.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ParseConfig</code></td>\n<td><code>yamlContent string</code></td>\n<td><code>(PipelineConfig, error)</code></td>\n<td>The main entry point. Accepts a string containing the entire pipeline configuration file (e.g., <code>.ci.yml</code>). It performs syntax parsing, structural validation, and initializes the core <code>PipelineConfig</code> object.</td>\n</tr>\n<tr>\n<td><code>ResolveEnvVars</code></td>\n<td><code>config *PipelineConfig</code>, <code>env map[string]string</code></td>\n<td><code>error</code></td>\n<td>Processes the configuration, substituting all environment variable references (e.g., <code>$VAR</code>, <code>${VAR}</code>) in <code>command</code> strings, <code>image</code> names, and other fields with their actual values from the provided <code>env</code> map. It respects precedence: job-level env overrides pipeline-level env, which overrides system-provided env.</td>\n</tr>\n<tr>\n<td><code>ExpandMatrix</code></td>\n<td><code>jobConfig JobConfig</code></td>\n<td><code>[]JobConfig, error</code></td>\n<td>Takes a single <code>JobConfig</code> that contains a <code>matrix</code> definition and returns a slice of new <code>JobConfig</code> objects, each representing one unique combination (cartesian product) of the matrix axes. For example, a job with <code>os: [ubuntu-latest, macos-latest]</code> and <code>node-version: [14, 16]</code> expands into four separate job configurations.</td>\n</tr>\n<tr>\n<td><code>BuildExecutionGraph</code></td>\n<td><code>config PipelineConfig</code></td>\n<td><code>(map[string][]string, error)</code></td>\n<td>Analyzes the <code>needs</code> dependencies declared between jobs and constructs an adjacency list representing the execution DAG. The returned map has job names as keys and a slice of job names that depend on that key as values. This graph is used by the orchestrator to schedule jobs in the correct order.</td>\n</tr>\n<tr>\n<td><code>ValidateConfig</code></td>\n<td><code>config PipelineConfig</code></td>\n<td><code>error</code></td>\n<td>Performs semantic validation beyond basic parsing. Checks for circular dependencies, validates that all <code>needs</code> references point to existing jobs, ensures <code>if</code> conditions have valid syntax, and warns about potentially insecure patterns (like commands constructed from unsanitized env vars).</td>\n</tr>\n</tbody></table>\n<p>These functions are typically used in sequence by the orchestrator:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">rawYAML </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> readFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\".ci.yml\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> ParseConfig</span><span style=\"color:#E1E4E8\">(rawYAML)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">err </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> ResolveEnvVars</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config, combinedEnvMap)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">executionGraph, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> BuildExecutionGraph</span><span style=\"color:#E1E4E8\">(config)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\"> }</span></span></code></pre></div>\n\n<h3 id=\"internal-behavior-and-algorithm\">Internal Behavior and Algorithm</h3>\n<p>The transformation from raw YAML to an execution plan follows a strict, multi-stage pipeline. Each stage validates and enriches the data, catching errors as early as possible.</p>\n<ol>\n<li><p><strong>Lexical Analysis &amp; Syntax Parsing</strong>: The raw YAML string is fed into a YAML parser (like <code>gopkg.in/yaml.v3</code>). This stage checks for basic YAML syntax errors—unclosed quotes, invalid indentation, malformed mappings. On success, it produces a generic nested <code>map[string]interface{}</code> and <code>[]interface{}</code> tree in memory.</p>\n</li>\n<li><p><strong>Unmarshaling into Structured Types</strong>: The generic tree is unmarshaled into our strongly-typed <code>PipelineConfig</code> struct. This involves mapping YAML keys like <code>jobs</code> and <code>environment</code> to the corresponding Go struct fields. This step fails if required top-level fields (like <code>jobs</code>) are missing or if data types are incompatible (e.g., a <code>job.steps</code> is provided as a string instead of a list).</p>\n</li>\n<li><p><strong>Per-Job Validation and Enrichment</strong>: Each <code>JobConfig</code> within <code>PipelineConfig.Jobs</code> is validated individually.</p>\n<ul>\n<li>The <code>runs-on</code> field is checked for a non-empty value (specifying the Docker image).</li>\n<li>The <code>steps</code> list is validated to ensure it&#39;s non-empty and each <code>StepConfig</code> has a <code>name</code> and <code>command</code>.</li>\n<li>The <code>if</code> condition string for each step is parsed into a simple abstract syntax tree (AST) to validate its grammar (e.g., <code>branch == &#39;main&#39; &amp;&amp; event == &#39;push&#39;</code>), though full evaluation happens later during runtime.</li>\n<li>Default values are injected where appropriate (e.g., if no <code>environment</code> is specified at the job level, an empty map is created).</li>\n</ul>\n</li>\n<li><p><strong>Environment Variable Resolution</strong>: This is a recursive, context-aware substitution process.</p>\n<ul>\n<li>A combined environment map is built, merging (in order of increasing precedence): system environment, pipeline-level <code>environment</code>, job-level <code>environment</code>, and runtime-provided secrets/context (like <code>GIT_COMMIT_SHA</code>).</li>\n<li>The parser traverses every string field in <code>PipelineConfig</code> that is designated as &quot;expandable&quot; (primarily <code>StepConfig.Command</code>, <code>JobConfig.RunsOn</code>). It identifies patterns like <code>$VAR</code> or <code>${VAR}</code>.</li>\n<li>For each match, it looks up the variable name in the combined environment map. If found, it replaces the pattern with the value. If not found and the pattern is <code>${VAR}</code>, it may be replaced with an empty string or cause an error based on configuration; <code>$VAR</code> patterns without braces are left untouched if not followed by a valid terminator.</li>\n<li><strong>Security Note</strong>: This step must avoid recursive substitution (where a variable&#39;s value contains another variable reference) unless explicitly intended, to prevent injection or infinite loops.</li>\n</ul>\n</li>\n<li><p><strong>Matrix Expansion</strong>: For each job that defines a <code>matrix</code> field, the expansion algorithm runs.</p>\n<ul>\n<li>It iterates over each key-value pair in the <code>matrix</code> map, where the key is an axis name (e.g., <code>os</code>) and the value is a list of choices.</li>\n<li>It computes the <strong>cartesian product</strong> of all axes. For a matrix with axes <code>os: [a, b]</code> and <code>version: [1, 2]</code>, the product is <code>[a,1], [a,2], [b,1], [b,2]</code>.</li>\n<li>For each combination, it creates a new <code>JobConfig</code> clone of the original. It then injects the combination&#39;s values into the new job&#39;s environment map (e.g., adds <code>MATRIX_OS=a, MATRIX_VERSION=1</code>) and also substitutes any references to the matrix axis in the job&#39;s <code>name</code> or <code>runs-on</code> field (e.g., <code>runs-on: ${{ matrix.os }}-runner</code> becomes <code>runs-on: a-runner</code>).</li>\n<li>The original job (if it had a matrix) is replaced by the list of expanded jobs in the pipeline&#39;s job map.</li>\n</ul>\n</li>\n<li><p><strong>Dependency Graph Construction</strong>: The final step is to analyze job dependencies to build an execution schedule.</p>\n<ul>\n<li>It initializes an adjacency list representation of a graph, where nodes are job names.</li>\n<li>For each job, it examines its <code>needs</code> field (a list of job names that must complete successfully before this job can start).</li>\n<li>For each dependency <code>dep</code> in <code>needs</code>, it adds a directed edge from <code>dep</code> to the current job in the adjacency list.</li>\n<li>Once all edges are added, it runs a <strong>cycle detection algorithm</strong> (like Depth-First Search) on the graph. If a cycle is found (e.g., Job A needs Job B, and Job B needs Job A), it returns an error because a cyclic dependency cannot be executed.</li>\n<li>The resulting graph is a <strong>DAG</strong>. The orchestrator will later use <strong>topological sorting</strong> on this DAG to determine a valid linear order of execution that respects all dependencies.</li>\n</ul>\n</li>\n</ol>\n<p>The entire process can be visualized in the provided flowchart: <img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-parser-flowchart.svg\" alt=\"Pipeline Configuration Parsing and DAG Construction\"></p>\n<h3 id=\"adr-yaml-vs-custom-dsl-for-configuration\">ADR: YAML vs. Custom DSL for Configuration</h3>\n<blockquote>\n<p><strong>Decision: Use YAML for Pipeline Configuration</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a human-readable, writable, and maintainable format for developers to define their CI/CD pipelines. The format must support complex nested structures (jobs, steps, environment) and be easy to integrate with existing Git repository files. The learning curve for contributors writing pipelines should be minimal.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>YAML (YAML Ain&#39;t Markup Language)</strong>: A superset of JSON designed for human readability, with support for comments, multiline strings, and anchors/aliases for reducing duplication.</li>\n<li><strong>JSON (JavaScript Object Notation)</strong>: A ubiquitous data interchange format that is simple to parse and generate but lacks comments and can be verbose for humans to write.</li>\n<li><strong>Custom Domain-Specific Language (DSL)</strong>: A dedicated language (e.g., using a parser generator or an embedded DSL in a language like Python or Ruby) that could offer perfect syntactic fit, validation at definition time, and powerful abstractions.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: We will use <strong>YAML</strong> as the primary configuration format, specifically a schema similar to GitHub Actions and GitLab CI for familiarity.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Familiarity and Ecosystem</strong>: YAML is the de facto standard for modern CI/CD systems (GitHub Actions, GitLab CI, CircleCI). Using it reduces cognitive load for users and leverages existing editor support (syntax highlighting, snippets).</li>\n<li><strong>Human-Friendliness</strong>: Support for comments is critical for documenting complex pipelines. Readable multiline strings are essential for embedding shell scripts directly in the config.</li>\n<li><strong>Adequate Expressiveness</strong>: While not a programming language, YAML&#39;s structure (maps, lists, scalars) is sufficient to represent pipelines, dependencies, and matrices. Advanced logic (conditionals) can be embedded as string expressions evaluated at runtime.</li>\n<li><strong>Implementation Simplicity</strong>: Mature, robust YAML parsers exist for all our target languages (e.g., <code>go-yaml</code>, <code>PyYAML</code>, <code>serde_yaml</code>). We avoid the significant complexity of designing, parsing, and maintaining a custom DSL.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Quick onboarding for users, extensive prior art for schema design, trivial parsing logic.</li>\n<li><strong>Negative</strong>: YAML has well-known pitfalls (ambiguous syntax, security concerns with <code>!!</code> tags). We must strictly limit the schema and disable advanced YAML features to avoid issues. Runtime validation is mandatory since the format is not type-safe.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>YAML</strong></td>\n<td>Human-readable, supports comments, wide ecosystem, familiar to developers.</td>\n<td>Can be tricky with indentation, requires careful schema validation, potential security issues with custom tags.</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td><strong>JSON</strong></td>\n<td>Extremely simple to parse, universal support, unambiguous structure.</td>\n<td>No comments, verbose, harder for humans to write and read.</td>\n<td>No</td>\n</tr>\n<tr>\n<td><strong>Custom DSL</strong></td>\n<td>Maximum expressiveness, can enforce correctness at syntax level, enable powerful abstractions.</td>\n<td>High implementation cost, steep learning curve for users, requires custom tooling (editors, linters).</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls-parser\">Common Pitfalls: Parser</h3>\n<p>⚠️ <strong>Pitfall: Insecure Environment Variable Substitution</strong></p>\n<ul>\n<li><strong>Description</strong>: Directly substituting environment variables into shell commands without any sanitization can lead to <strong>command injection</strong>. For example, if an environment variable <code>BRANCH</code> contains <code>main; rm -rf /</code>, a command like <code>echo &quot;Building $BRANCH&quot;</code> would execute the malicious <code>rm</code> command.</li>\n<li><strong>Why it&#39;s wrong</strong>: This is a critical security vulnerability that could allow an attacker who controls an environment variable (e.g., via a pull request title or comment) to execute arbitrary code on the CI worker.</li>\n<li><strong>How to fix</strong>: Never directly interpolate variables into the shell command string that will be passed to <code>sh -c</code>. Instead, pass environment variables to the container as true environment variables (<code>os.Environ</code> or Docker <code>--env</code>). Then, in the shell script, they are referenced natively by the shell, which doesn&#39;t involve a parsing/expansion step by our Go code. For step commands written directly in YAML, we must ensure the command is executed as <code>sh -c &#39;your script&#39;</code> where the variables are already present in the shell&#39;s environment.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Circular Dependencies</strong></p>\n<ul>\n<li><strong>Description</strong>: A pipeline where Job A <code>needs</code> Job B, and Job B <code>needs</code> Job A (directly or transitively) creates a cycle. The parser must detect this; otherwise, the orchestrator will deadlock trying to schedule these jobs, as neither can start without the other finishing.</li>\n<li><strong>Why it&#39;s wrong</strong>: The execution graph is no longer a <strong>DAG</strong> (Directed <em>Acyclic</em> Graph), breaking the fundamental scheduling algorithm. The pipeline will never start or will get stuck in a logical loop.</li>\n<li><strong>How to fix</strong>: Implement cycle detection as part of <code>BuildExecutionGraph</code>. Use a standard DFS-based algorithm to traverse the dependency graph. If, during traversal, you encounter a node that is already in the current recursion stack, a cycle exists. Report a clear error to the user listing the jobs involved in the cycle.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Combinatorial Explosion in Matrix Builds</strong></p>\n<ul>\n<li><strong>Description</strong>: A matrix definition with many axes, each with many values, can generate an enormous number of jobs. For example, <code>os: [a,b,c,d,e]</code> x <code>version: [1,2,3,4,5]</code> x <code>test-suite: [unit, integration, e2e]</code> creates 5<em>5</em>3 = 75 jobs. This can overwhelm system resources and queue capacity.</li>\n<li><strong>Why it&#39;s wrong</strong>: While functionally correct, it&#39;s often not the user&#39;s intent to run <em>every</em> combination. It can lead to excessive resource consumption, long queue times, and cost overruns.</li>\n<li><strong>How to fix</strong>: Implement guardrails. <strong>1.</strong> Set a conservative default limit (e.g., max 50 matrix combinations per job). <strong>2.</strong> Allow users to explicitly increase this limit via a configuration flag (accepting the risk). <strong>3.</strong> Provide an <code>exclude</code> keyword in the matrix schema (like GitHub Actions) to let users prune specific unwanted combinations. <strong>4.</strong> Log a prominent warning when a matrix expands beyond a certain threshold.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Unvalidated <code>if</code> Conditions</strong></p>\n<ul>\n<li><strong>Description</strong>: The <code>if</code> field in a step or job is a string expression (e.g., <code>github.event_name == &#39;push&#39;</code>). If the parser only treats it as an opaque string and leaves full evaluation to the runtime, a syntax error in the expression will only surface when the step is about to run, causing confusing failures.</li>\n<li><strong>Why it&#39;s wrong</strong>: Failures should be caught as early as possible (at configuration parsing time). A syntax error in an <code>if</code> condition is a configuration error, not a runtime failure.</li>\n<li><strong>How to fix</strong>: During the <code>ValidateConfig</code> phase, parse each <code>if</code> string using a simple expression parser. You don&#39;t need to evaluate it (values aren&#39;t known yet), but you can validate the grammar—ensuring parentheses are balanced, operators are valid (<code>==</code>, <code>!=</code>, <code>&amp;&amp;</code>, <code>||</code>), and operands look like identifiers or literals. This provides immediate feedback to the user.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Incorrect Variable Scoping and Precedence</strong></p>\n<ul>\n<li><strong>Description</strong>: Environment variables can be defined at the pipeline level, job level, and step level. If the resolution order is not clearly defined and implemented, a step might get the wrong value (e.g., a job-level variable overwriting a pipeline-level secret).</li>\n<li><strong>Why it&#39;s wrong</strong>: This leads to non-deterministic behavior and hard-to-debug pipeline failures. It can also cause secret leakage if a less-specific scope overrides a more-specific one.</li>\n<li><strong>How to fix</strong>: Define and document a strict precedence order. A common and intuitive order is (from lowest to highest priority): System Environment &lt; Pipeline <code>environment</code> &lt; Job <code>environment</code> &lt; Step <code>env</code>. When resolving a variable for a step, search in reverse order: first in step <code>env</code>, then job <code>environment</code>, then pipeline <code>environment</code>, then system. The first match wins.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>YAML Parsing</td>\n<td><code>gopkg.in/yaml.v3</code> (pure Go, stable)</td>\n<td><code>github.com/go-yaml/yaml</code> (same, v2/v3)</td>\n</tr>\n<tr>\n<td>Expression Parsing (for <code>if</code>)</td>\n<td>Simple string splitting and token matching for a limited grammar (<code>==</code>, <code>!=</code>, <code>&amp;&amp;</code>, `</td>\n<td></td>\n</tr>\n<tr>\n<td>DAG Algorithms</td>\n<td>Implement DFS cycle detection and topological sort manually (50-100 lines of Go).</td>\n<td>Use a graph library like <code>github.com/yourbasic/graph</code> for more complex operations.</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ci-system/\n├── cmd/\n│   └── server/                 # Main application entry point\n├── internal/\n│   ├── parser/                 # This component\n│   │   ├── config.go           # Core structs: PipelineConfig, JobConfig, StepConfig\n│   │   ├── parser.go           # ParseConfig, ValidateConfig, ResolveEnvVars\n│   │   ├── matrix.go           # ExpandMatrix function\n│   │   ├── dag.go              # BuildExecutionGraph, cycle detection\n│   │   └── parser_test.go      # Unit tests\n│   ├── orchestrator/           # Uses the parsed config\n│   ├── worker/                 # Executes jobs\n│   └── store/                  # Data persistence\n├── pkg/\n│   └── yamlutils/              # (Optional) Shared YAML helpers\n└── go.mod</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here is a complete, ready-to-use foundation for the YAML parsing and core data structures. The learner should copy this into <code>internal/parser/config.go</code>.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/parser/config.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">gopkg.in/yaml.v3</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PipelineConfig represents the fully parsed and validated pipeline blueprint.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PipelineConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Source      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                  `yaml:\"-\"`</span><span style=\"color:#6A737D\"> // File path or URL where config was loaded from</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Jobs        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">JobConfig</span><span style=\"color:#9ECBFF\">    `yaml:\"jobs\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Environment </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `yaml:\"environment,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MatrixAxes  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`yaml:\"matrix,omitempty\"`</span><span style=\"color:#6A737D\"> // Top-level matrix (legacy style, less common)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// JobConfig defines a single runnable job within a pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> JobConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RunsOn      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"runs-on\"`</span><span style=\"color:#6A737D\"> // Docker image identifier</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Steps       []</span><span style=\"color:#B392F0\">StepConfig</span><span style=\"color:#9ECBFF\">      `yaml:\"steps\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Needs       []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `yaml:\"needs,omitempty\"`</span><span style=\"color:#6A737D\"> // Job dependencies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Environment </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"env,omitempty\"`</span><span style=\"color:#6A737D\">   // Job-specific environment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Matrix      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`yaml:\"matrix,omitempty\"`</span><span style=\"color:#6A737D\"> // Matrix definition for this job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Internal fields after processing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Expanded    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">              `yaml:\"-\"`</span><span style=\"color:#6A737D\"> // True if this job came from matrix expansion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StepConfig defines a single command to run within a job.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StepConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Command </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"run\"`</span><span style=\"color:#6A737D\"> // The shell command to execute</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Env     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"env,omitempty\"`</span><span style=\"color:#6A737D\"> // Step-specific environment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    If      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"if,omitempty\"`</span><span style=\"color:#6A737D\"> // Conditional expression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ParseConfig is the main entry point. It unmarshals YAML and performs basic validation.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ParseConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">yamlContent</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">PipelineConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">PipelineConfig</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use yaml.Unmarshal to parse yamlContent into a temporary map.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Perform basic top-level validation: ensure \"jobs\" key exists and is a map.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Unmarshal the YAML node for \"jobs\" directly into config.Jobs using yaml.Unmarshal.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each job in config.Jobs, validate required fields: name, runs-on, steps.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate that each step has a 'name' and 'run' field.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return the populated PipelineConfig and nil error, or a descriptive error if validation fails.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use a custom YAML unmarshaler or two-pass parsing if you need more control.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ParseConfig not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper function to extract a string slice from a YAML node (for matrix values).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> parseStringSlice</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">node</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">yaml</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Node</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // (Implementation provided: handles YAML sequence nodes)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>The following skeletons guide the implementation of the core algorithms. The learner must fill in the TODOs.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/parser/matrix.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExpandMatrix takes a job with a matrix definition and returns a slice of jobs,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// one for each combination of matrix axes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ExpandMatrix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">job</span><span style=\"color:#B392F0\"> JobConfig</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">JobConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if the job has a \"matrix\" field. If not, return a slice containing just the original job.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate that each matrix axis value is a list (slice). Log an error if not.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Compute the cartesian product of all axis values.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    - Example: axes = {\"os\": [\"linux\", \"macos\"], \"node\": [\"14\", \"16\"]}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    - Product = [{\"os\":\"linux\",\"node\":\"14\"}, {\"os\":\"linux\",\"node\":\"16\"}, ...]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each combination in the product:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    a. Create a deep copy of the original job (important to not share references).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    b. Set job.Expanded = true.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    c. For each axis in the combination, add it to the job's Environment map with a prefix (e.g., \"MATRIX_OS\").</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    d. Replace placeholders in job.Name and job.RunsOn (e.g., \"${{ matrix.os }}\" -> \"linux\").</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    e. Append the new job to the result slice.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply any \"exclude\" rules if your matrix schema supports them (filter out combinations).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Enforce a maximum expansion limit (e.g., 50). Return an error if exceeded.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return the slice of expanded jobs.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ExpandMatrix not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/parser/dag.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BuildExecutionGraph constructs an adjacency list of job dependencies.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns a map where key = job name, value = list of jobs that depend on the key.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> BuildExecutionGraph</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> PipelineConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    graph </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Initialize graph with all jobs as keys.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> jobName </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> config.Jobs {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        graph[jobName] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Build the adjacency list.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For each job in config.Jobs:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   For each neededJobName in job.Needs:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //      Append the current job's name to graph[neededJobName] (edge from needed -> current).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Detect cycles.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   Implement a DFS function that tracks visited nodes and the recursion stack.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   For each unvisited job node in the graph, run DFS.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   If during DFS you visit a node already in the current stack, you found a cycle.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   Return an error describing the cycle path.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If no cycles, return the graph.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> graph, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"BuildExecutionGraph not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/parser/parser.go (additional functions)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> envVarRegex </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> regexp.</span><span style=\"color:#B392F0\">MustCompile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`\\$\\{?([a-zA-Z_][a-zA-Z0-9_]*)\\}?`</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ResolveEnvVars performs substitution of environment variable references in a configuration.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It modifies the passed config in-place.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ResolveEnvVars</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">PipelineConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">env</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Build the final environment map for each job, merging:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   baseEnv (system + pipeline environment), job.Environment, and step.Env.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Traverse all string fields in config where substitution is allowed:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - JobConfig.RunsOn</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - StepConfig.Command</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Possibly JobConfig.Name for matrix placeholders</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each string, find all matches of envVarRegex.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each match, look up the variable name in the appropriate environment map.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   If found, replace the match with the value. If not found, either leave it as is (for $VAR)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   or replace with empty string (for ${VAR}) based on your policy.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Be careful to avoid infinite recursion if a variable's value contains another variable reference.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   You may want to limit the number of substitution passes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return an error if a required variable (e.g., ${SECRET_KEY}) is not defined.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ResolveEnvVars not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li>Use <code>gopkg.in/yaml.v3</code> for parsing. Prefer <code>yaml.UnmarshalStrict</code> to reject unknown fields, helping catch typos in the YAML.</li>\n<li>When unmarshaling directly into structs, define YAML tags (<code>yaml:&quot;runs-on&quot;</code>) to match the common CI schema.</li>\n<li>For deep copying <code>JobConfig</code> during matrix expansion, a simple way is to marshal to JSON and unmarshal back (<code>json.Marshal/Unmarshal</code>) since the struct contains only basic types. For production, consider a dedicated copying library.</li>\n<li>Use <code>regexp.MustCompile</code> for environment variable patterns, compiling them once at package init for efficiency.</li>\n<li>Implement DFS for cycle detection using a <code>map[string]int</code> to track status: <code>0=unvisited, 1=visiting, 2=visited</code>.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong>\nAfter implementing the parser component, you should be able to run a verification test.</p>\n<ol>\n<li><strong>Run Unit Tests</strong>: Execute <code>go test ./internal/parser/... -v</code> from the project root. You should see tests passing for parsing a simple pipeline, detecting a circular dependency, and expanding a 2x2 matrix.</li>\n<li><strong>Manual Verification</strong>:<ul>\n<li>Create a file <code>test.yml</code> with a pipeline definition containing a matrix and dependencies.</li>\n<li>Write a small Go program in <code>cmd/testparser/main.go</code> that calls <code>ParseConfig</code>, <code>ExpandMatrix</code>, and <code>BuildExecutionGraph</code>.</li>\n<li>Run it: <code>go run cmd/testparser/main.go</code>. It should print the parsed structure, the list of expanded jobs, and the dependency graph without errors.</li>\n</ul>\n</li>\n<li><strong>Signs of Trouble</strong>:<ul>\n<li><strong>Panic on nil map</strong>: You forgot to initialize <code>PipelineConfig.Jobs</code> map before unmarshaling into it.</li>\n<li><strong>Circular dependency not detected</strong>: Your DFS algorithm is missing the &quot;visiting&quot; state. A job depending on itself should be caught.</li>\n<li><strong>Matrix expansion produces duplicate combinations</strong>: Check your cartesian product logic; a nested loop over axes is needed.</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"component-job-execution-engine-milestone-2\">Component: Job Execution Engine (Milestone 2)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2</p>\n</blockquote>\n<p>This component transforms pipeline definitions into concrete execution. It&#39;s the <strong>factory floor</strong> where the automated assembly line comes to life, running each job&#39;s commands in isolated, predictable environments and producing build artifacts. The core challenge is providing strong isolation without sacrificing performance, capturing real-time output, and managing resources cleanly even when jobs fail.</p>\n<h3 id=\"mental-model-the-sandboxed-playground\">Mental Model: The Sandboxed Playground</h3>\n<p>Imagine giving each job its own <strong>identical, fresh sandbox</strong>—a containerized environment where it can &quot;play&quot; (execute commands) without affecting other jobs or the underlying system. This sandbox has:</p>\n<ol>\n<li><strong>Fresh Sand</strong>: Every job starts with a clean workspace—no leftover files from previous jobs.</li>\n<li><strong>Controlled Toys</strong>: Only specified tools and dependencies (via the Docker image) are available.</li>\n<li><strong>Supervised Play</strong>: An attendant (the execution engine) watches each command, intervenes if rules are broken (non-zero exit codes), and records everything said (stdout/stderr).</li>\n<li><strong>Keepsake Collection</strong>: When playtime ends, the attendant gathers designated creations (artifacts) for safekeeping before dismantling the sandbox.</li>\n</ol>\n<p>This mental model emphasizes the core guarantees: <strong>isolation, reproducibility, and observability</strong>. Each job gets an identical starting point, runs without interference, and leaves no mess behind.</p>\n<h3 id=\"interface-and-api\">Interface and API</h3>\n<p>The execution engine exposes a clean interface for orchestrators to trigger job execution and monitor progress. The primary abstraction is the <code>Executor</code> interface, which hides the Docker implementation details.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ExecuteJob</code></td>\n<td><code>ctx context.Context</code>, <code>job JobRun</code>, <code>env map[string]string</code></td>\n<td><code>(JobRun, error)</code></td>\n<td>Executes all steps of a job in sequence within a Docker container. Injects environment variables, captures logs, and updates the job status. Returns the updated <code>JobRun</code> with timestamps and exit codes.</td>\n</tr>\n<tr>\n<td><code>StreamLogs</code></td>\n<td><code>ctx context.Context</code>, <code>jobID string</code>, <code>writer io.Writer</code></td>\n<td><code>error</code></td>\n<td>Streams the real-time log output of a currently running job to the provided writer (e.g., an HTTP response). Blocks until the job completes or the context is cancelled.</td>\n</tr>\n<tr>\n<td><code>CollectArtifacts</code></td>\n<td><code>ctx context.Context</code>, <code>job JobRun</code>, <code>patterns []string</code></td>\n<td><code>([]string, error)</code></td>\n<td>Copies files matching the glob patterns from the completed job container to persistent storage (e.g., local disk, cloud storage). Returns the storage keys/URLs of collected artifacts.</td>\n</tr>\n<tr>\n<td><code>CancelJob</code></td>\n<td><code>ctx context.Context</code>, <code>jobID string</code></td>\n<td><code>error</code></td>\n<td>Forcefully stops a running job, kills its container, and updates the job status to <code>STATUS_CANCELLED</code>. Performs cleanup of container resources.</td>\n</tr>\n<tr>\n<td><code>GetJobStatus</code></td>\n<td><code>ctx context.Context</code>, <code>jobID string</code></td>\n<td><code>(string, error)</code></td>\n<td>Returns the current status (<code>STATUS_RUNNING</code>, <code>STATUS_SUCCEEDED</code>, etc.) of a job by inspecting its container and internal state.</td>\n</tr>\n</tbody></table>\n<p>Additionally, the engine maintains internal state through these key data structures:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ContainerID</code></td>\n<td><code>string</code></td>\n<td>Docker container identifier assigned when the job starts execution. Used for all container operations.</td>\n</tr>\n<tr>\n<td><code>LogKey</code></td>\n<td><code>string</code></td>\n<td>Reference to where the job&#39;s combined stdout/stderr output is stored (e.g., file path, object storage key).</td>\n</tr>\n<tr>\n<td><code>ArtifactKeys</code></td>\n<td><code>[]string</code></td>\n<td>List of storage references for artifacts collected from this job run.</td>\n</tr>\n<tr>\n<td><code>StartedAt</code></td>\n<td><code>*time.Time</code></td>\n<td>Timestamp when job execution began (set when container starts).</td>\n</tr>\n<tr>\n<td><code>FinishedAt</code></td>\n<td><code>*time.Time</code></td>\n<td>Timestamp when job execution completed (set when container stops).</td>\n</tr>\n</tbody></table>\n<p>The state transitions for a <code>JobRun</code> follow a predictable lifecycle:</p>\n<p><img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-job-state-machine.svg\" alt=\"Job Run State Machine\"></p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Action Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STATUS_PENDING</code></td>\n<td><code>start</code></td>\n<td><code>STATUS_RUNNING</code></td>\n<td>Create Docker container, inject environment variables, begin first step. Set <code>StartedAt</code>.</td>\n</tr>\n<tr>\n<td><code>STATUS_RUNNING</code></td>\n<td><code>step_complete(exit_code=0)</code></td>\n<td><code>STATUS_RUNNING</code></td>\n<td>Continue to next step in sequence. Append step output to logs.</td>\n</tr>\n<tr>\n<td><code>STATUS_RUNNING</code></td>\n<td><code>step_complete(exit_code≠0)</code></td>\n<td><code>STATUS_FAILED</code></td>\n<td>Stop job execution, capture final step output, kill container. Set <code>FinishedAt</code>.</td>\n</tr>\n<tr>\n<td><code>STATUS_RUNNING</code></td>\n<td><code>all_steps_complete</code></td>\n<td><code>STATUS_SUCCEEDED</code></td>\n<td>Stop container gracefully, collect artifacts. Set <code>FinishedAt</code>.</td>\n</tr>\n<tr>\n<td><code>STATUS_RUNNING</code></td>\n<td><code>timeout</code></td>\n<td><code>STATUS_FAILED</code></td>\n<td>Force-kill container, record timeout error in logs. Set <code>FinishedAt</code>.</td>\n</tr>\n<tr>\n<td><code>STATUS_RUNNING</code></td>\n<td><code>cancel</code></td>\n<td><code>STATUS_CANCELLED</code></td>\n<td>Force-kill container, record cancellation. Set <code>FinishedAt</code>.</td>\n</tr>\n<tr>\n<td>Any state</td>\n<td><code>cleanup</code></td>\n<td>(unchanged)</td>\n<td>Remove container if still exists, release any held resources.</td>\n</tr>\n</tbody></table>\n<h3 id=\"internal-behavior-and-algorithm\">Internal Behavior and Algorithm</h3>\n<p>When <code>ExecuteJob</code> is called, the engine follows a precise sequence of operations to ensure reliable, isolated execution:</p>\n<ol>\n<li><p><strong>Preparation Phase</strong></p>\n<ul>\n<li>Generate a unique workspace directory on the host filesystem (e.g., <code>/workspaces/job-&lt;id&gt;</code>).</li>\n<li>Create a temporary directory for log aggregation (<code>/workspaces/job-&lt;id&gt;/logs</code>).</li>\n<li>Merge environment variables: system defaults → pipeline-level variables → job-level variables → step-level variables, with later sources overriding earlier ones.</li>\n<li>Validate the Docker image reference exists locally or is pullable from a registry.</li>\n</ul>\n</li>\n<li><p><strong>Container Creation</strong></p>\n<ul>\n<li>Construct Docker <code>ContainerConfig</code> with:<ul>\n<li>Image: <code>job.RunsOn</code> (e.g., <code>golang:1.21</code>)</li>\n<li>Working directory: <code>/workspace</code> (mounted from host workspace)</li>\n<li>Environment variables: The merged map from step 1</li>\n<li>Entrypoint: <code>[&quot;/bin/sh&quot;, &quot;-c&quot;]</code> to execute shell commands</li>\n<li>Network: <code>bridge</code> with no extra privileges (security baseline)</li>\n</ul>\n</li>\n<li>Create the container with a unique name (<code>ci-job-&lt;job-id&gt;</code>).</li>\n<li>Start the container, which immediately pauses waiting for commands.</li>\n</ul>\n</li>\n<li><p><strong>Step Execution Loop</strong>\nFor each <code>StepConfig</code> in <code>job.Steps</code>:</p>\n<ul>\n<li>Evaluate the <code>If</code> condition using the current environment. If false, mark step as <code>STATUS_SKIPPED</code> and continue.</li>\n<li>Prepare the command by performing environment variable substitution on <code>StepConfig.Command</code>.</li>\n<li>Create an exec instance in the running container for the command.</li>\n<li>Attach multiplexed stdout/stderr streams to both:<ul>\n<li>Real-time streaming buffer (for <code>StreamLogs</code> consumers)</li>\n<li>Cumulative log file (for permanent storage)</li>\n</ul>\n</li>\n<li>Start the exec instance and wait for completion with a timeout (default 1 hour).</li>\n<li>Capture the exit code. If non-zero, break the loop and transition job to <code>STATUS_FAILED</code>.</li>\n</ul>\n</li>\n<li><p><strong>Completion Handling</strong></p>\n<ul>\n<li>If all steps succeeded:<ul>\n<li>Call <code>CollectArtifacts</code> to copy files matching patterns from container <code>/workspace</code> to persistent storage.</li>\n<li>Update <code>JobRun</code> with <code>ArtifactKeys</code> and status <code>STATUS_SUCCEEDED</code>.</li>\n</ul>\n</li>\n<li>If any step failed or timed out:<ul>\n<li>Still attempt artifact collection (for debugging failed runs).</li>\n<li>Update status to <code>STATUS_FAILED</code> with the failing exit code.</li>\n</ul>\n</li>\n<li>Stop the container (gracefully if succeeded, forcefully if failed).</li>\n<li>Set <code>FinishedAt</code> timestamp.</li>\n</ul>\n</li>\n<li><p><strong>Cleanup Phase</strong></p>\n<ul>\n<li>Remove the Docker container (regardless of success/failure).</li>\n<li>Delete the temporary host workspace directory.</li>\n<li>Close all open file handles and network connections.</li>\n<li>Update the <code>Store</code> with the final <code>JobRun</code> state.</li>\n</ul>\n</li>\n</ol>\n<p>For <code>StreamLogs</code>, the algorithm is:</p>\n<ol>\n<li>Locate the job&#39;s running container by <code>ContainerID</code>.</li>\n<li>Attach to the container&#39;s combined stdout/stderr stream (Docker&#39;s <code>logs</code> API with <code>follow=true</code>).</li>\n<li>Stream bytes directly to the provided <code>io.Writer</code>.</li>\n<li>Continue streaming until either:<ul>\n<li>The container stops (job completes)</li>\n<li>The client disconnects (context cancellation)</li>\n<li>A timeout occurs (configured keepalive)</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"adr-container-vs-process-isolation\">ADR: Container vs. Process Isolation</h3>\n<blockquote>\n<p><strong>Decision: Use Docker Containers for Job Isolation</strong></p>\n<ul>\n<li><strong>Context</strong>: We need strong isolation between concurrent job executions to prevent contamination (file system, environment variables, processes) while maintaining reasonable implementation complexity for an educational project.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Docker Containers</strong>: Full OS-level isolation via namespaces, cgroups, and union filesystems.</li>\n<li><strong>Linux Namespaces/Cgroups Direct</strong>: Using Go libraries to create namespaces directly without Docker.</li>\n<li><strong>Simple Subprocess Execution</strong>: Running commands as child processes with environment cleanup.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use Docker containers via the Docker Engine API.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Standardization</strong>: Docker provides a consistent, well-documented interface for creating isolated environments across different host systems.</li>\n<li><strong>Ecosystem</strong>: Access to thousands of pre-built images (language runtimes, tools) without manual installation.</li>\n<li><strong>Safety</strong>: Strong isolation by default (filesystem, network, process tree) prevents &quot;noisy neighbor&quot; issues.</li>\n<li><strong>Implementation Simplicity</strong>: The Docker Go SDK provides high-level abstractions that reduce boilerplate compared to raw namespace manipulation.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Requires Docker</strong>: Users must have Docker installed and running, adding a system dependency.</li>\n<li><strong>Performance Overhead</strong>: Container startup adds ~100-500ms per job versus subprocess execution.</li>\n<li><strong>Resource Management</strong>: Need explicit cleanup logic to avoid accumulating stopped containers.</li>\n<li><strong>Security Considerations</strong>: Must run containers with minimal privileges (no <code>--privileged</code>, read-only root filesystem where possible).</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Docker Containers</td>\n<td>Strong isolation, vast image ecosystem, well-documented API, cross-platform</td>\n<td>Requires Docker daemon, container startup overhead, extra dependency</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Linux Namespaces/Cgroups Direct</td>\n<td>Minimal overhead, fine-grained control, no Docker dependency</td>\n<td>Complex implementation, requires root privileges, less portable</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Simple Subprocess Execution</td>\n<td>Fastest startup, simplest implementation, no extra dependencies</td>\n<td>No filesystem isolation, environment contamination risk, limited reproducibility</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls-execution-engine\">Common Pitfalls: Execution Engine</h3>\n<p>⚠️ <strong>Pitfall: Zombie Containers Accumulation</strong></p>\n<ul>\n<li><strong>Description</strong>: Forgetting to remove containers after job completion (especially after failures or cancellations) leads to hundreds of stopped containers consuming disk space.</li>\n<li><strong>Why it&#39;s wrong</strong>: Disk exhaustion can crash the host system; container limit errors prevent new jobs from starting.</li>\n<li><strong>Fix</strong>: Implement a <code>defer</code> in <code>ExecuteJob</code> that always calls container removal. Add a periodic cleanup goroutine that removes containers older than a threshold.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Log Stream Blocking Deadlocks</strong></p>\n<ul>\n<li><strong>Description</strong>: Writing log output synchronously to both the streaming buffer and persistent storage can block if one destination is slow (e.g., disk I/O congestion), causing the entire job to stall.</li>\n<li><strong>Why it&#39;s wrong</strong>: Jobs appear to hang; real-time logs stop updating; timeouts trigger incorrectly.</li>\n<li><strong>Fix</strong>: Use a buffered channel as a log queue. Start separate goroutines for streaming and disk writing that consume from this channel. Implement backpressure detection.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Artifact Path Confusion</strong></p>\n<ul>\n<li><strong>Description</strong>: Collecting artifacts using container paths instead of host paths (or vice versa) results in empty artifact collections or &quot;file not found&quot; errors.</li>\n<li><strong>Why it&#39;s wrong</strong>: Developers expect build outputs but get empty archives; debugging information is lost.</li>\n<li><strong>Fix</strong>: Always use <code>docker cp</code> with container paths (e.g., <code>/workspace/dist/*.tar</code>) for extraction. Document clearly that patterns are evaluated inside the container&#39;s workspace.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Timeout Cascade Failure</strong></p>\n<ul>\n<li><strong>Description</strong>: Setting a single global timeout for the entire job (e.g., 1 hour) without per-step timeouts can let a single hanging step consume all allotted time, delaying failure feedback.</li>\n<li><strong>Why it&#39;s wrong</strong>: Developers wait unnecessarily for hung jobs; resource utilization is inefficient.</li>\n<li><strong>Fix</strong>: Implement dual timeouts: (1) per-step timeout (default 30 minutes), (2) total job timeout (default 2 hours). Cancel the job immediately when either triggers.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Environment Variable Injection Leakage</strong></p>\n<ul>\n<li><strong>Description</strong>: Printing environment variables (including secrets) to logs during debugging or command echoing exposes sensitive data in build logs.</li>\n<li><strong>Why it&#39;s wrong</strong>: Secret leakage violates security policies; credentials become publicly accessible.</li>\n<li><strong>Fix</strong>: Never log raw environment variables. Mask secrets in logs by replacing values with <code>***</code> during output redaction. Use Docker&#39;s secret mounting feature for production.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Container Runtime</td>\n<td>Docker Engine via official Go SDK</td>\n<td>containerd via direct Go client (more complex but Docker-independent)</td>\n</tr>\n<tr>\n<td>Log Streaming</td>\n<td>File-based with periodic polling</td>\n<td>In-memory ring buffer with Server-Sent Events (SSE)</td>\n</tr>\n<tr>\n<td>Artifact Storage</td>\n<td>Local filesystem directory</td>\n<td>S3-compatible object storage (MinIO for local testing)</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td><code>sync.WaitGroup</code> for job parallelism</td>\n<td>Worker pool with rate limiting and priority queue</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ci-system/\n├── cmd/\n│   ├── server/                 # Main orchestrator server\n│   └── worker/                 # Dedicated worker binary (optional)\n├── internal/\n│   ├── executor/              # This component\n│   │   ├── docker.go          # Docker client wrapper and container operations\n│   │   ├── executor.go        # Main ExecuteJob and StreamLogs logic\n│   │   ├── artifacts.go       # Artifact collection utilities\n│   │   ├── logging.go         # Log streaming and aggregation\n│   │   └── executor_test.go   # Integration tests with testcontainers\n│   ├── queue/                 # Job queue system\n│   ├── parser/                # Pipeline configuration parser\n│   └── store/                 # Data persistence layer\n└── pkg/\n    └── types/                 # Shared types (JobRun, StepRun, etc.)</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Docker Client Wrapper (<code>internal/executor/docker.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> executor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/api/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/api/types/container</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/api/types/mount</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/client</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/docker/docker/pkg/stdcopy</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DockerExecutor wraps the Docker client with convenience methods.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DockerExecutor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cli </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">client</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDockerExecutor creates a new executor with a Docker client.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDockerExecutor</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cli, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> client.</span><span style=\"color:#B392F0\">NewClientWithOpts</span><span style=\"color:#E1E4E8\">(client.FromEnv, client.</span><span style=\"color:#B392F0\">WithAPIVersionNegotiation</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create Docker client: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">{cli: cli}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateContainer creates and starts a container with the given configuration.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns the container ID and a cleanup function that removes the container.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateContainer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">container</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">hostConfig</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">container</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HostConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Pull image if not present locally</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ImageInspectWithRaw</span><span style=\"color:#E1E4E8\">(ctx, config.Image)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        reader, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ImagePull</span><span style=\"color:#E1E4E8\">(ctx, config.Image, </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ImagePullOptions</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to pull image </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, config.Image, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        io.</span><span style=\"color:#B392F0\">Copy</span><span style=\"color:#E1E4E8\">(io.Discard, reader) </span><span style=\"color:#6A737D\">// Drain the response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        reader.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create container</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ContainerCreate</span><span style=\"color:#E1E4E8\">(ctx, config, hostConfig, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create container: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start container</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ContainerStart</span><span style=\"color:#E1E4E8\">(ctx, resp.ID, </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ContainerStartOptions</span><span style=\"color:#E1E4E8\">{}); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        d.cli.</span><span style=\"color:#B392F0\">ContainerRemove</span><span style=\"color:#E1E4E8\">(ctx, resp.ID, </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ContainerRemoveOptions</span><span style=\"color:#E1E4E8\">{Force: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to start container: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Cleanup function that removes the container</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        d.cli.</span><span style=\"color:#B392F0\">ContainerRemove</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), resp.ID, </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ContainerRemoveOptions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Force:         </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            RemoveVolumes: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> resp.ID, cleanup, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecInContainer executes a command in a running container.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns exit code, combined output, and any error.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecInContainer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">containerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cmd</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">env</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    execConfig </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ExecConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cmd:          cmd,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Env:          env,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        AttachStdout: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        AttachStderr: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    execResp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ContainerExecCreate</span><span style=\"color:#E1E4E8\">(ctx, containerID, execConfig)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create exec instance: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attachResp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ContainerExecAttach</span><span style=\"color:#E1E4E8\">(ctx, execResp.ID, </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ExecStartCheck</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to attach to exec: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> attachResp.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Capture output</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> outputBuf </span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    outputDone </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stdcopy.</span><span style=\"color:#B392F0\">StdCopy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">outputBuf, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">outputBuf, attachResp.Reader)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        outputDone </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ContainerExecStart</span><span style=\"color:#E1E4E8\">(ctx, execResp.ID, </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ExecStartCheck</span><span style=\"color:#E1E4E8\">{}); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to start exec: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Wait for completion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">outputDone:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"error reading output: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, ctx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Inspect to get exit code</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inspectResp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ContainerExecInspect</span><span style=\"color:#E1E4E8\">(ctx, execResp.ID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, outputBuf.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(), fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to inspect exec: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> inspectResp.ExitCode, outputBuf.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StreamLogs streams container logs to the provided writer.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StreamLogs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">containerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">writer</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    options </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ContainerLogsOptions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ShowStdout: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ShowStderr: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Follow:     </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamps: </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reader, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.cli.</span><span style=\"color:#B392F0\">ContainerLogs</span><span style=\"color:#E1E4E8\">(ctx, containerID, options)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to get container logs: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> reader.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Demultiplex Docker's log stream (stdout/stderr are interleaved)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stdcopy.</span><span style=\"color:#B392F0\">StdCopy</span><span style=\"color:#E1E4E8\">(writer, writer, reader)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Main Execution Logic (<code>internal/executor/executor.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> executor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">ci-system/pkg/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Executor implements the job execution interface.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Executor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    docker     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DockerExecutor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    workspace  </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">          // Base directory for job workspaces</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logStore   </span><span style=\"color:#B392F0\">LogStore</span><span style=\"color:#6A737D\">        // Interface for storing logs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    artifactStore </span><span style=\"color:#B392F0\">ArtifactStore</span><span style=\"color:#6A737D\"> // Interface for storing artifacts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteJob implements the main job execution algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteJob</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">env</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Prepare workspace directory on host</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Create directory at filepath.Join(e.workspace, job.ID)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Set up log file in workspace/logs/job.log</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Merge environment variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Start with system env (os.Environ())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Override with pipeline env (from job.PipelineRun.Config.Environment)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Override with job env (job.Env)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add CI-specific variables (CI=true, JOB_ID=job.ID, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create Docker container</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Build container config with image from job.JobName's RunsOn</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Mount host workspace directory to /workspace in container</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Set environment variables from step 2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use e.docker.CreateContainer() and defer the cleanup function</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update job status to STATUS_RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Set job.StartedAt = time.Now()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store updated job in database</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute steps sequentially</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, step </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> job.Steps {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5.1: Evaluate step.If condition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Parse condition expression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Evaluate against current environment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - If false, mark step as STATUS_SKIPPED and continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5.2: Prepare command with env var substitution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Replace $VAR and ${VAR} patterns in step.Command</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Handle default values ${VAR:-default} syntax</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5.3: Execute command in container</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use e.docker.ExecInContainer() with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Capture exit code and output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5.4: Stream output in real-time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Write to log file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Notify log streaming clients via channel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5.5: Handle step completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - If exit code != 0, break loop and mark job as STATUS_FAILED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Record step execution in StepRun structure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle job completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If all steps succeeded:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     - Collect artifacts using e.CollectArtifacts()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     - Update job.Status = STATUS_SUCCEEDED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If any step failed:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     - Still attempt artifact collection (for debugging)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     - Update job.Status = STATUS_FAILED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Stop container (already handled by defer)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Remove temporary workspace directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Set job.FinishedAt = time.Now()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store final job state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> job, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StreamLogs streams job logs to the writer in real-time.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StreamLogs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">writer</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up job by ID to get ContainerID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Query database for job.ContainerID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If container is still running, attach to logs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use e.docker.StreamLogs() with the container ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Stream directly to writer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If container has stopped, stream from stored logs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Read from log file in workspace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Write to writer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle client disconnection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Watch for context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Clean up resources when client disconnects</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Artifact Collection (<code>internal/executor/artifacts.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> executor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CollectArtifacts copies artifacts from container to storage.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CollectArtifacts</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">patterns</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> artifactKeys []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: For each pattern in patterns:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Execute `find` command in container to locate matching files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse output to get list of files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each matching file:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use docker cp to copy from container to temporary host location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Compress if file is large (>10MB)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Upload to artifact storage (e.artifactStore)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Record the storage key in artifactKeys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create artifact manifest</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - JSON file listing all artifacts with metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Upload manifest as additional artifact</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> artifactKeys, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<ul>\n<li><strong>Docker SDK</strong>: Use <code>github.com/docker/docker/client</code> version <code>v20.10.17+</code> for stable API. Always call <code>client.WithAPIVersionNegotiation()</code> to handle different Docker daemon versions.</li>\n<li><strong>Context Propagation</strong>: Pass <code>context.Context</code> through all Docker operations to enable proper cancellation and timeouts. Use <code>context.WithTimeout</code> for step execution.</li>\n<li><strong>Concurrent Log Handling</strong>: Use <code>io.MultiWriter</code> to send output to both file and streaming buffer simultaneously. Consider a ring buffer implementation for memory-efficient log retention.</li>\n<li><strong>Resource Cleanup</strong>: Implement <code>ContainerRemove</code> with <code>Force: true</code> in a deferred function to ensure containers are always cleaned up, even on panic.</li>\n<li><strong>Error Wrapping</strong>: Use <code>fmt.Errorf(&quot;... %w&quot;, err)</code> throughout to create actionable error chains that include Docker API errors.</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the execution engine, verify functionality:</p>\n<ol>\n<li><p><strong>Run unit tests</strong>: <code>go test ./internal/executor/... -v</code> should pass tests for:</p>\n<ul>\n<li>Environment variable merging</li>\n<li>Command substitution</li>\n<li>Step execution ordering</li>\n</ul>\n</li>\n<li><p><strong>Integration test with Docker</strong>:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Start a test job manually</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/test/execute</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -d</span><span style=\"color:#9ECBFF\"> '{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"image\": \"alpine:latest\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"commands\": [\"echo 'Hello</span><span style=\"color:#9ECBFF\"> CI'\", \"ls -la\"],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       \"env\": {\"TEST_VAR\": \"value\"}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">     }'</span></span></code></pre></div>\n<p>   Expected: Returns a job ID immediately, creates an Alpine container, executes commands, streams output.</p>\n<ol start=\"3\">\n<li><strong>Verify container cleanup</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   docker</span><span style=\"color:#9ECBFF\"> ps</span><span style=\"color:#79B8FF\"> -a</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#9ECBFF\"> ci-job-</span><span style=\"color:#6A737D\">  # Should show no containers after job completes</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Check artifact collection</strong>:\nCreate a test job that produces a file (<code>touch output.txt</code>), specify artifact pattern <code>output.txt</code>, and verify the file appears in the artifact storage directory.</li>\n</ol>\n<p>Signs of problems:</p>\n<ul>\n<li>Containers remain running after job completion → Check cleanup deferred function.</li>\n<li>Logs appear truncated → Check buffer sizes in <code>StreamLogs</code>.</li>\n<li>Environment variables not available in commands → Verify merging order and substitution logic.</li>\n<li>&quot;Permission denied&quot; when copying artifacts → Check Docker volume mount permissions.</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job hangs at &quot;Creating container&quot;</td>\n<td>Docker daemon not running or unreachable</td>\n<td>Run <code>docker ps</code> to check daemon status; check <code>e.docker.cli.Ping()</code> in code</td>\n<td>Start Docker daemon; ensure client uses correct socket path</td>\n</tr>\n<tr>\n<td>Logs stop streaming but job continues</td>\n<td>Writer blocking (e.g., slow HTTP client)</td>\n<td>Check if <code>StreamLogs</code> goroutine is stuck; monitor buffer sizes</td>\n<td>Implement non-blocking writes with channel buffering; add write timeouts</td>\n</tr>\n<tr>\n<td>&quot;No space left on device&quot; error</td>\n<td>Workspace directories not cleaned up</td>\n<td>Check disk usage <code>df -h</code>; list workspace directories</td>\n<td>Add workspace cleanup in defer; implement periodic cleanup job</td>\n</tr>\n<tr>\n<td>Artifact patterns match no files</td>\n<td>Paths relative to wrong directory</td>\n<td>Log container working directory; run <code>pwd</code> command in container</td>\n<td>Ensure patterns are relative to <code>/workspace</code>; document this requirement</td>\n</tr>\n<tr>\n<td>Environment variables with special characters break commands</td>\n<td>Improper shell escaping</td>\n<td>Log the actual command sent to Docker exec</td>\n<td>Use <code>sh -c</code> with proper quoting; consider <code>exec.Command</code> with explicit args</td>\n</tr>\n<tr>\n<td>Job succeeds but status remains &quot;RUNNING&quot;</td>\n<td>Database update failed after execution</td>\n<td>Check for panics after container cleanup; verify transaction commits</td>\n<td>Wrap entire ExecuteJob in transaction; add final status update in defer</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-webhook-amp-queue-system-milestone-3\">Component: Webhook &amp; Queue System (Milestone 3)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3</p>\n</blockquote>\n<p>This component serves as the <strong>entry point and traffic controller</strong> for the entire CI/CD system. Its primary responsibility is to reliably accept events from external Git services, validate their authenticity, transform them into executable pipeline runs, and manage the orderly dispatch of work to available workers. This dual role—acting as both a secure gateway and a fair scheduler—makes it a critical subsystem for system reliability and performance.</p>\n<h3 id=\"mental-model-the-restaurant-host-and-kitchen-ticket-rail\">Mental Model: The Restaurant Host and Kitchen Ticket Rail</h3>\n<p>Imagine a busy restaurant. <strong>Customers</strong> (Git events like pushes or pull requests) arrive at the door. The <strong>host</strong> (webhook handler) greets them, checks their reservation (validates the webhook signature), and understands their order (parses the payload to determine which pipeline to run). The host then creates a <strong>ticket</strong> (a <code>PipelineRun</code>) that describes the complete meal (the series of jobs to execute) and places it on the <strong>kitchen ticket rail</strong> (the job queue).</p>\n<p>The ticket rail has a specific order: priority tickets (like production deployments) go to the front. Multiple <strong>chefs</strong> (workers) watch the rail. When a chef becomes free, they take the next available ticket from the rail (<code>DequeueJob</code>), prepare that specific dish (execute the job in a container), and mark the ticket as done when finished. The host never directly interrupts the chefs; all coordination happens through the ticket rail. This separation ensures that even during a dinner rush (a flood of commits), orders are processed fairly and no single order blocks the entire kitchen.</p>\n<h3 id=\"interface-and-api\">Interface and API</h3>\n<p>This component exposes two primary interfaces: an <strong>HTTP handler</strong> for external Git services and a <strong>programmatic queue API</strong> for internal system components. The following table details the key methods and their contracts.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HandleWebhook</code></td>\n<td><code>payload []byte</code>, <code>signature string</code></td>\n<td><code>(PipelineRun, error)</code></td>\n<td>The primary public HTTP handler. Validates the incoming webhook signature, parses the JSON payload, determines the appropriate pipeline configuration from the repository, creates a new <code>PipelineRun</code>, and enqueues it. Returns the created run or an error if validation fails.</td>\n</tr>\n<tr>\n<td><code>EnqueueRun</code></td>\n<td><code>run PipelineRun</code></td>\n<td><code>(string, error)</code></td>\n<td>Internal method that inserts a complete pipeline run (with all its constituent <code>JobRun</code> records in <code>STATUS_PENDING</code>) into the queue for execution. Returns a queue identifier or an error if the queue is unavailable.</td>\n</tr>\n<tr>\n<td><code>DequeueJob</code></td>\n<td><code>workerID string</code></td>\n<td><code>(JobRun, error)</code></td>\n<td>Called by worker processes to request the next available job. Atomically marks a pending <code>JobRun</code> as <code>STATUS_RUNNING</code> and assigns it to the requesting worker. Returns an error if no jobs are available or if the worker fails to check in.</td>\n</tr>\n<tr>\n<td><code>MarkJobComplete</code></td>\n<td><code>jobID string</code>, <code>status string</code></td>\n<td><code>error</code></td>\n<td>Updates the status of a <code>JobRun</code> (to <code>STATUS_SUCCEEDED</code>, <code>STATUS_FAILED</code>, etc.) and triggers the evaluation of dependent jobs. If all jobs in a <code>PipelineRun</code> are complete, updates the pipeline run status accordingly.</td>\n</tr>\n<tr>\n<td><code>GetQueueStats</code></td>\n<td>-</td>\n<td><code>(pending int, running int)</code></td>\n<td>Returns operational metrics: the count of jobs in <code>STATUS_PENDING</code> and <code>STATUS_RUNNING</code>. Used for monitoring and dashboard display.</td>\n</tr>\n</tbody></table>\n<h3 id=\"internal-behavior-and-algorithm\">Internal Behavior and Algorithm</h3>\n<p>The component orchestrates a multi-step process from an incoming HTTP request to a job being picked up by a worker. The sequence diagram <img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-webhook-sequence.svg\" alt=\"Sequence: Webhook to Job Execution\"> illustrates this flow. The following numbered steps detail the internal logic.</p>\n<ol>\n<li><p><strong>HTTP Request Reception &amp; Signature Verification</strong>\nThe HTTP server receives a <code>POST</code> request at the <code>/webhook</code> endpoint. The handler immediately extracts the <code>X-Hub-Signature-256</code> header (for GitHub) or <code>X-GitLab-Token</code> header and the raw request body.</p>\n<blockquote>\n<p><strong>Security Principle:</strong> Never trust the network. The signature is an HMAC hex digest of the request body using a shared secret token. The handler must compute the HMAC of the received payload and perform a <strong>constant-time comparison</strong> with the provided signature to prevent timing attacks. If the signature is invalid or missing, the request is rejected with HTTP 403.</p>\n</blockquote>\n</li>\n<li><p><strong>Payload Parsing and Event Classification</strong>\nThe valid JSON payload is unmarshaled into a generic event structure. The handler inspects key fields to classify the event type:</p>\n<ul>\n<li>For GitHub: The <code>X-GitHub-Event</code> header indicates <code>push</code>, <code>pull_request</code>, or <code>create</code> (for tags).</li>\n<li>For GitLab: The <code>object_kind</code> field indicates <code>push</code>, <code>merge_request</code>, or <code>tag_push</code>.\nThe handler extracts the critical context: <strong>repository SSH URL</strong>, <strong>branch or tag name</strong>, <strong>commit SHA</strong>, and <strong>event type</strong> (<code>EVENT_PUSH</code>, <code>EVENT_PULL_REQUEST</code>, <code>EVENT_TAG</code>).</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Configuration Matching</strong>\nUsing the repository URL and branch, the system locates the pipeline configuration file (e.g., <code>.ci/pipeline.yaml</code>) from the source code. In a real-world scenario, this would involve cloning the repository at the specific commit. For our system&#39;s scope, we assume the configuration is already present or fetched via a lightweight API call to the Git provider. The configuration&#39;s <code>on:</code> block (or equivalent) is evaluated against the event context. For example, a pipeline configured with <code>on: push: branches: [main]</code> will only trigger for push events to the <code>main</code> branch.</p>\n</li>\n<li><p><strong>PipelineRun and JobRun Creation</strong>\nIf the event matches, the handler instantiates a new <code>PipelineRun</code>. It generates unique IDs for the run and for each <code>JobRun</code> derived from the parsed <code>PipelineConfig</code>. The <code>JobRun</code> statuses are set to <code>STATUS_PENDING</code>. The <code>Trigger</code> field is set to the event type, and the <code>CommitSHA</code>, <code>Branch</code>, and repository metadata are recorded. The complete <code>PipelineRun</code> is persisted via the <code>Store</code> interface (<code>CreatePipelineRun</code>).</p>\n</li>\n<li><p><strong>Queue Insertion with Priority</strong>\nThe <code>EnqueueRun</code> method is called with the new <code>PipelineRun</code>. The queue backend stores a reference to the run. <strong>Priority scheduling</strong> is applied here: runs triggered by tags or the main branch may be assigned a higher priority score than those from feature branches. The queue orders items by this score (and secondarily by creation time).</p>\n</li>\n<li><p><strong>Worker Polling and Job Dispatch</strong>\nWorker processes, started as part of the worker pool, continuously call <code>DequeueJob</code>, providing their unique <code>workerID</code>. The queue logic performs an atomic transaction:\na. Find the highest-priority <code>PipelineRun</code> with at least one <code>STATUS_PENDING</code> job where the job&#39;s dependencies (the <code>Needs</code> field) are all satisfied (i.e., in <code>STATUS_SUCCEEDED</code>).\nb. Select a pending <code>JobRun</code> from that pipeline.\nc. Update the <code>JobRun</code>: set <code>Status</code> to <code>STATUS_RUNNING</code>, <code>AssignedWorker</code> to <code>workerID</code>, and <code>StartedAt</code> to the current time.\nd. Return the updated <code>JobRun</code> object to the worker.\nThis atomicity prevents the same job from being assigned to two workers.</p>\n</li>\n<li><p><strong>Job Completion and State Propagation</strong>\nWhen a worker finishes executing a job, it calls <code>MarkJobComplete</code> with the final status. The queue component updates the <code>JobRun</code> (<code>FinishedAt</code>, <code>Status</code>) and then <strong>re-evaluates the dependency graph</strong>. It checks all jobs that <code>Need</code> the now-completed job. If all dependencies for a pending job are met, that job becomes eligible for dequeuing. This process continues until the pipeline run reaches a terminal state (all jobs succeeded, failed, or were cancelled).</p>\n</li>\n</ol>\n<h3 id=\"adr-in-memory-vs-persistent-queue-backend\">ADR: In-Memory vs. Persistent Queue Backend</h3>\n<blockquote>\n<p><strong>Decision: Use a hybrid queue model with SQLite for persistence and an in-memory priority heap for dispatch.</strong></p>\n<ul>\n<li><strong>Context</strong>: The job queue must be persistent to survive server restarts but also support efficient priority-based dequeue operations and concurrent worker access. Given the educational nature of the project, we want to minimize external dependencies while demonstrating production-like queue semantics.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pure in-memory channel (Go <code>chan</code>): Simple and fast for a single process, but jobs are lost on crash and scaling to multiple processes is impossible.</li>\n<li>External Redis: Excellent for persistence, high performance, and built-in support for priority queues and pub/sub. However, it introduces an additional moving part and operational complexity for learners.</li>\n<li>Database (PostgreSQL/SQLite) with advisory locks: Uses the existing persistence layer. Provides durability and transactional safety. Requires careful design to avoid table-level contention and to implement efficient priority ordering.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use SQLite (via the <code>Store</code> interface) as the system of record for all <code>JobRun</code> states, combined with an <strong>in-memory priority heap</strong> within the orchestrator process that holds references to pending jobs. The heap is rebuilt from the database on startup.</li>\n<li><strong>Rationale</strong>: This hybrid approach balances simplicity with required durability. SQLite is already a project dependency for the <code>Store</code>. By storing the canonical job state there, we guarantee no job is lost. The in-memory heap provides O(log n) dequeue performance. On startup, a simple query (<code>SELECT * FROM job_runs WHERE status = &#39;PENDING&#39;</code>) rebuilds the heap. For a single-server educational project, this is sufficient and avoids introducing Redis.</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: No new external dependencies. Job state is durable. Priority scheduling is straightforward to implement with <code>container/heap</code>.</li>\n<li><strong>Negative</strong>: The queue is not distributed. Multiple orchestrator processes would have separate, unsynchronized heaps leading to job duplication. This aligns with our non-goal of distributed workers. If the orchestrator process crashes, the in-memory heap is lost, but can be safely rebuilt from the database, though any jobs <code>STATUS_RUNNING</code> at the time of the crash will need a recovery mechanism (e.g., a timeout-based reset to <code>STATUS_PENDING</code>).</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>In-Memory Channel (<code>chan</code>)</td>\n<td>Extremely simple, native concurrency, very fast.</td>\n<td>No persistence (jobs lost on crash), single-process only, no priority support.</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Redis (External)</td>\n<td>Persistent, high performance, supports pub/sub and priority queues, battle-tested for distributed systems.</td>\n<td>Adds operational complexity, requires running another service, network dependency.</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Hybrid (SQLite + In-Memory Heap)</td>\n<td>Durable (via SQLite), good single-process performance, leverages existing store, supports priority.</td>\n<td>Not distributed, requires startup recovery logic, potential state drift if not carefully managed.</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Insecure Webhook Signature Verification</strong></p>\n<ul>\n<li><strong>Description</strong>: Implementing signature verification using a simple string equality (<code>==</code>) operator, which is vulnerable to timing attacks. An attacker could theoretically deduce the secret by measuring tiny differences in response time.</li>\n<li><strong>Why it&#39;s wrong</strong>: It compromises the entire system&#39;s security. An attacker could forge webhook events, triggering arbitrary pipeline executions, potentially leading to unauthorized code deployment or resource exhaustion.</li>\n<li><strong>Fix</strong>: Always use a constant-time comparison function. In Go, use <code>crypto/hmac</code>&#39;s <code>hmac.Equal</code>. Never log the secret or the computed signature.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Queue Starvation</strong></p>\n<ul>\n<li><strong>Description</strong>: Low-priority jobs (e.g., from long-running feature branches) are never executed because higher-priority jobs (from <code>main</code>) continuously enter the queue.</li>\n<li><strong>Why it&#39;s wrong</strong>: It violates fairness. Developers on feature branches receive no feedback, undermining the purpose of CI. The queue backlog grows indefinitely.</li>\n<li><strong>Fix</strong>: Implement a <strong>fair priority queue</strong>. Instead of strict priority, use a weighted round-robin or aging strategy. For example, increase a job&#39;s effective priority the longer it waits in the queue. A simple method is to periodically promote a batch of old, low-priority jobs.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Lost Jobs on Orchestrator Crash</strong></p>\n<ul>\n<li><strong>Description</strong>: If the orchestrator process crashes while a job is <code>STATUS_RUNNING</code>, the corresponding worker may complete the job, but there is no process to call <code>MarkJobComplete</code>. The job remains <code>STATUS_RUNNING</code> forever in the database, and dependent jobs are blocked.</li>\n<li><strong>Why it&#39;s wrong</strong>: It causes pipeline hangs and requires manual database intervention to fix.</li>\n<li><strong>Fix</strong>: Implement a <strong>dead man&#39;s switch</strong> or <strong>heartbeat</strong>. Workers must periodically update a <code>last_heartbeat</code> timestamp on their assigned <code>JobRun</code>. A recovery goroutine (or startup routine) scans for jobs where <code>status = &#39;RUNNING&#39;</code> and <code>last_heartbeat</code> is older than a threshold (e.g., 5 minutes) and resets them to <code>STATUS_PENDING</code>. This allows another worker to retry the job.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Concurrent Worker Coordination Race Condition</strong></p>\n<ul>\n<li><strong>Description</strong>: Two workers call <code>DequeueJob</code> at nearly the same time. The naive implementation reads the same pending job, and both proceed to execute it, leading to duplicate execution, resource contention, and potential artifact corruption.</li>\n<li><strong>Why it&#39;s wrong</strong>: Breaks the core queue guarantee of &quot;exactly once&quot; delivery (within the context of a single job run). Wastes resources and produces confusing, non-deterministic results.</li>\n<li><strong>Fix</strong>: The dequeue operation <strong>must be atomic</strong>. Implement it within a database transaction that includes a <code>SELECT ... FOR UPDATE</code> (or SQLite&#39;s <code>BEGIN EXCLUSIVE</code> transaction) to lock the row, check its status, and update it. The in-memory heap should only be used as a fast index; the final assignment must be committed via the database.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete starter code and skeletons to implement the Webhook &amp; Queue system in Go.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> with Gorilla Mux or <code>http.ServeMux</code></td>\n<td><code>chi</code> router for middleware and structured routes</td>\n</tr>\n<tr>\n<td>Queue Backend</td>\n<td>SQLite (<code>modernc.org/sqlite</code>) + <code>container/heap</code></td>\n<td>Redis (<code>go-redis/redis</code>) with Sorted Sets (<code>ZSET</code>)</td>\n</tr>\n<tr>\n<td>Webhook Verification</td>\n<td><code>crypto/hmac</code> for manual signing</td>\n<td>Use provider SDKs (e.g., <code>google/go-github</code>)</td>\n</tr>\n<tr>\n<td>Worker Pool</td>\n<td>Goroutines + channels managed in-process</td>\n<td>External worker binaries coordinated via RPC</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>your-ci-system/\n├── cmd/\n│   ├── orchestrator/          # Main orchestrator process (webhook + queue)\n│   │   └── main.go\n│   └── worker/                # Worker process (job execution)\n│       └── main.go\n├── internal/\n│   ├── queue/                 # This component\n│   │   ├── queue.go           # Core Queue interface and hybrid implementation\n│   │   ├── webhook.go         # HTTP handlers and signature verification\n│   │   ├── worker_pool.go     # In-process worker pool management\n│   │   └── priority_heap.go   # In-memory heap implementation\n│   ├── store/                 # Data persistence (from Data Model section)\n│   │   └── sqlite.go\n│   └── executor/              # Job Execution Engine (from Milestone 2)\n│       └── docker_executor.go\n└── webhook_payloads/          # Example JSON payloads for testing\n    ├── github_push.json\n    └── gitlab_merge_request.json</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Complete HTTP Webhook Handler Helper (<code>internal/queue/webhook.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> queue</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/hmac</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha256</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/hex</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WebhookVerifier handles signature verification for different Git providers.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WebhookVerifier</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gitHubSecret []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gitLabToken  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewWebhookVerifier creates a verifier with provided secrets.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewWebhookVerifier</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">gitHubSecret</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">gitLabToken</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WebhookVerifier</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">WebhookVerifier</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gitHubSecret: []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(gitHubSecret),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gitLabToken:  gitLabToken,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// VerifyGitHubSignature validates a GitHub webhook signature.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// signatureHeader is the value of the \"X-Hub-Signature-256\" header.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// body is the raw HTTP request body.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WebhookVerifier</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">VerifyGitHubSignature</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">signatureHeader</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">body</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(v.gitHubSecret) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"github secret not configured\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    const</span><span style=\"color:#79B8FF\"> signaturePrefix</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sha256=\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(signatureHeader) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(signaturePrefix) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid signature format\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actualSignature </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> signatureHeader[</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(signaturePrefix):]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mac </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hmac.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(sha256.New, v.gitHubSecret)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mac.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(body)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expectedSignature </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hex.</span><span style=\"color:#B392F0\">EncodeToString</span><span style=\"color:#E1E4E8\">(mac.</span><span style=\"color:#B392F0\">Sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use constant-time comparison to prevent timing attacks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">hmac.</span><span style=\"color:#B392F0\">Equal</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(expectedSignature), []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(actualSignature)) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid signature\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// VerifyGitLabToken validates a GitLab webhook token.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// tokenHeader is the value of the \"X-GitLab-Token\" header.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WebhookVerifier</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">VerifyGitLabToken</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tokenHeader</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> v.gitLabToken </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"gitlab token not configured\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">hmac.</span><span style=\"color:#B392F0\">Equal</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(tokenHeader), []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(v.gitLabToken)) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid token\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExtractEventDetails parses common fields from GitHub and GitLab payloads.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns repoSSHUrl, branch, commitSHA, eventType.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ExtractEventDetails</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">body</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // This is a simplified example. A real implementation would fully unmarshal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // into provider-specific structs.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    provider </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"User-Agent\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> repo, branch, sha, event </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> contains</span><span style=\"color:#E1E4E8\">(provider, </span><span style=\"color:#9ECBFF\">\"GitHub\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-GitHub-Event\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Parse GitHub JSON to extract repo, ref, and sha...</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // (Use github.com/google/go-github/v50/github for full parsing)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#B392F0\"> contains</span><span style=\"color:#E1E4E8\">(provider, </span><span style=\"color:#9ECBFF\">\"GitLab\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-Gitlab-Event\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Parse GitLab JSON...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unknown Git provider\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return extracted values or placeholders</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> repo, branch, sha, event, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">s</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">substr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //... helper implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Middleware to read and restore the request body for signature verification.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> readBodyMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        body, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">(r.Body)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Bad request\"</span><span style=\"color:#E1E4E8\">, http.StatusBadRequest)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Restore the body for subsequent handlers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r.Body </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">NopCloser</span><span style=\"color:#E1E4E8\">(bytes.</span><span style=\"color:#B392F0\">NewBuffer</span><span style=\"color:#E1E4E8\">(body))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Store the raw body in the request context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithValue</span><span style=\"color:#E1E4E8\">(r.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\"raw_body\"</span><span style=\"color:#E1E4E8\">, body)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r.</span><span style=\"color:#B392F0\">WithContext</span><span style=\"color:#E1E4E8\">(ctx))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Priority Heap for In-Memory Queue (<code>internal/queue/priority_heap.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> queue</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">container/heap</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Item represents an entry in the priority queue.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Item</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JobRunID      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PipelineRunID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Priority      </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">    // Higher number = higher priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index         </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\"> // Internal index for heap.Interface</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PriorityQueue implements heap.Interface.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PriorityQueue</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Item</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pq </span><span style=\"color:#B392F0\">PriorityQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Len</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(pq) }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pq </span><span style=\"color:#B392F0\">PriorityQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Less</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">j</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // We want Pop to give us the highest priority, so use greater than.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> pq[i].Priority </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> pq[j].Priority {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // If priorities equal, older items (smaller CreatedAt) have higher priority.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> pq[i].CreatedAt.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(pq[j].CreatedAt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> pq[i].Priority </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> pq[j].Priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pq </span><span style=\"color:#B392F0\">PriorityQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Swap</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">j</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pq[i], pq[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pq[j], pq[i]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pq[i].index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pq[j].index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> j</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PriorityQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Push</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">x</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">pq)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    item </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> x.(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Item</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    item.index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> n</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    *</span><span style=\"color:#E1E4E8\">pq </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">pq, item)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PriorityQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Pop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    old </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">pq</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(old)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    item </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> old[n</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    old[n</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\">  // avoid memory leak</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    item.index </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#6A737D\"> // for safety</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    *</span><span style=\"color:#E1E4E8\">pq </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\"> : n</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> item</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Hybrid Queue Manager (<code>internal/queue/queue.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> queue</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Queue defines the interface for enqueuing and dequeuing jobs.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Queue</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    EnqueueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DequeueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">workerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    MarkJobRunComplete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HybridQueue implements Queue using SQLite for persistence and an in-memory heap for priority.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HybridQueue</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store </span><span style=\"color:#B392F0\">Store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    heap  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PriorityQueue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHybridQueue</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">store</span><span style=\"color:#B392F0\"> Store</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hq </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        store: store,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        heap:  </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">PriorityQueue</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    heap.</span><span style=\"color:#B392F0\">Init</span><span style=\"color:#E1E4E8\">(hq.heap)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> hq.</span><span style=\"color:#B392F0\">recoverPendingJobs</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hq</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EnqueueJobRun persists the job and pushes it onto the in-memory heap.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EnqueueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate job status is STATUS_PENDING.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Persist the job via hq.store.UpdateJobRun (ensure it's marked PENDING).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate priority: e.g., priority=10 for main branch, priority=5 for tags, priority=1 for other branches.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Acquire mutex lock (hq.mu.Lock()), push a new *Item onto the heap, release lock.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log the enqueue operation.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DequeueJobRun atomically assigns the highest-priority eligible job to a worker.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DequeueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">workerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire mutex lock (hq.mu.Lock()).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If heap is empty, release lock and return an error indicating no jobs.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start a database transaction (hq.store.BeginTx).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Peek at the highest-priority item from the heap (do not pop yet).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: In the transaction, SELECT the corresponding JobRun FOR UPDATE where status = STATUS_PENDING.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify the job's dependencies are met by checking the status of all jobs in its Needs list.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: If job is eligible, update its status to STATUS_RUNNING, set AssignedWorker and StartedAt.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Commit the transaction. On success, pop the item from the heap and return the updated JobRun.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: If the job is not eligible (dependencies not met), remove it from the heap temporarily, continue to the next item. Consider re-adding it later.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: On any error, rollback the transaction, release the lock, and return the error.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MarkJobRunComplete updates job status and potentially unblocks dependent jobs.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">MarkJobRunComplete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Update the JobRun in the store: set status, FinishedAt.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Query for all JobRuns in the same PipelineRun that have a 'Needs' field containing this completed job's name.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each dependent job, check if ALL of its needed jobs are now complete (STATUS_SUCCEEDED or STATUS_FAILED/SKIPPED depending on semantics).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If a dependent job's dependencies are now satisfied and it's still PENDING, ensure it has an Item in the heap (it may have been skipped earlier in Dequeue). You may need to re-add it.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If all jobs in the PipelineRun are complete, update the PipelineRun status accordingly.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// recoverPendingJobs runs on startup to rebuild the heap from persistent storage.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recoverPendingJobs</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Query hq.store for all JobRuns with status = STATUS_PENDING.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each, calculate priority and push onto the heap.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Also query for JobRuns with status = STATUS_RUNNING but with old heartbeat. Reset them to PENDING and push onto heap.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Concurrency</strong>: Use <code>sync.Mutex</code> to protect the in-memory heap. For database operations, rely on SQL transactions (<code>BEGIN EXCLUSIVE</code> in SQLite) for atomicity.</li>\n<li><strong>Context Propagation</strong>: Pass <code>context.Context</code> through all queue methods to enable cancellation and timeouts, especially important for long-running database queries.</li>\n<li><strong>Error Handling</strong>: Distinguish between transient errors (e.g., database temporary unavailability) and permanent errors (e.g., invalid job data). For transient errors, implement retry with exponential backoff in the worker.</li>\n<li><strong>Logging</strong>: Use structured logging (e.g., <code>log/slog</code>) with fields like <code>job_id</code>, <code>pipeline_run_id</code>, and <code>worker_id</code> to trace execution across components.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>To verify your Webhook &amp; Queue system is functioning:</p>\n<ol>\n<li><strong>Start the Orchestrator</strong>: Run <code>go run cmd/orchestrator/main.go</code>. It should start an HTTP server on port 8080.</li>\n<li><strong>Send a Test Webhook</strong>: Use <code>curl</code> to simulate a GitHub push event:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">    curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/webhook</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      -H</span><span style=\"color:#9ECBFF\"> \"X-GitHub-Event: push\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      -H</span><span style=\"color:#9ECBFF\"> \"X-Hub-Signature-256: sha256=&#x3C;compute-using-secret>\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      -d</span><span style=\"color:#9ECBFF\"> @webhook_payloads/github_push.json</span></span></code></pre></div>\n<pre><code>You should see logs indicating the webhook was received, validated, and a pipeline run was created.\n</code></pre>\n<ol start=\"3\">\n<li><strong>Check the Queue</strong>: A second log line should indicate the job was enqueued. You can also expose a simple <code>GET /queue/stats</code> endpoint that uses <code>GetQueueStats</code> to display pending/running counts.</li>\n<li><strong>Start a Worker</strong>: In another terminal, run <code>go run cmd/worker/main.go</code>. The worker should log that it&#39;s polling for jobs, then almost immediately dequeue and start executing the test job.</li>\n<li><strong>Verify Database State</strong>: Query the SQLite database: <code>sqlite3 ci.db &quot;SELECT id, status FROM job_runs;&quot;</code>. You should see the job&#39;s status transition from <code>PENDING</code> to <code>RUNNING</code> to <code>SUCCEEDED</code>.</li>\n</ol>\n<p><strong>Signs of Trouble</strong>:</p>\n<ul>\n<li><strong>Webhook returns 403</strong>: Check your secret token and signature calculation. Ensure the middleware reads the body correctly.</li>\n<li><strong>Job stays in PENDING</strong>: The worker may not be running, or the <code>DequeueJobRun</code> logic has a bug (e.g., dependency check failing incorrectly).</li>\n<li><strong>Duplicate job execution</strong>: The atomicity in <code>DequeueJobRun</code> is broken. Verify your database transaction and row-level locking.</li>\n</ul>\n<h2 id=\"component-web-dashboard-milestone-4\">Component: Web Dashboard (Milestone 4)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4</p>\n</blockquote>\n<p>This component serves as the <strong>visual command center</strong> for the CI/CD Pipeline Orchestrator, providing developers with real-time visibility into pipeline execution, historical trends, and diagnostic information. It transforms raw execution data—status updates, log streams, and dependency graphs—into an intuitive, actionable interface. The dashboard acts as the primary feedback mechanism, closing the loop between automated systems and human operators by delivering immediate, contextualized insights about build health and execution progress.</p>\n<p><img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-dashboard-component.svg\" alt=\"Web Dashboard Component Breakdown\"></p>\n<h3 id=\"mental-model-the-airport-flight-display-board\">Mental Model: The Airport Flight Display Board</h3>\n<p>Think of the Web Dashboard as an <strong>airport flight information display system</strong>. Each pipeline run is a <em>flight</em> with a unique identifier (like a flight number). The overall status (pending, running, succeeded, failed) corresponds to a flight&#39;s status (scheduled, boarding, in-air, landed, cancelled). Individual jobs within the pipeline are like <em>gates</em> or <em>boarding areas</em>—subcomponents of the flight operation with their own status and timing. The real-time log streaming is the <em>live announcements and status updates</em> broadcast to passengers (developers) waiting at the gate, providing immediate feedback about delays or progress. The dependency graph visualization is the <em>airport terminal map</em> showing how gates connect via walkways (dependencies), helping you understand the overall flow. Finally, the status badges are the <em>flight status displays on external airport websites</em>—simple, embeddable indicators for external consumers. This mental model clarifies the dashboard&#39;s dual role: providing detailed operational data for engineers while offering high-level status indicators for external systems and quick glances.</p>\n<h3 id=\"interface-and-api\">Interface and API</h3>\n<p>The dashboard exposes a RESTful HTTP API alongside serving static web assets. The API follows resource-oriented design, with primary resources being <code>PipelineRun</code> and <code>JobRun</code> entities. Real-time updates are delivered via Server-Sent Events (SSE), providing efficient, unidirectional streaming from server to client.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Parameters</th>\n<th>Returns (Content-Type)</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>GET</code></td>\n<td><code>/</code></td>\n<td>None</td>\n<td><code>text/html</code></td>\n<td>Serves the main dashboard Single Page Application (SPA) HTML</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/api/runs</code></td>\n<td><code>limit</code> (query, default 50), <code>offset</code> (query, default 0)</td>\n<td><code>application/json</code> (<code>[]PipelineRun</code>)</td>\n<td>Returns paginated list of pipeline runs, sorted by <code>CreatedAt</code> descending</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/api/runs/:id</code></td>\n<td><code>id</code> (path parameter)</td>\n<td><code>application/json</code> (<code>PipelineRun</code> with nested <code>JobRun</code>s)</td>\n<td>Returns detailed information for a specific pipeline run</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/api/runs/:id/logs</code></td>\n<td><code>id</code> (path), <code>job</code> (query, optional), <code>step</code> (query, optional)</td>\n<td><code>text/event-stream</code></td>\n<td>Server-Sent Events stream of log output for a run, optionally filtered by job or step</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/api/runs/:id/dag</code></td>\n<td><code>id</code> (path)</td>\n<td><code>application/json</code> (DAG structure)</td>\n<td>Returns dependency graph data for visualization of a specific pipeline run</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/api/runs/:id/artifacts</code></td>\n<td><code>id</code> (path)</td>\n<td><code>application/json</code> (<code>[]ArtifactMetadata</code>)</td>\n<td>Lists artifact metadata (name, size, path) available for download</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/api/artifacts/:key/download</code></td>\n<td><code>key</code> (path)</td>\n<td><code>application/octet-stream</code> (file)</td>\n<td>Streams the artifact file for download; requires authentication in production</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/badge/:repo.svg</code></td>\n<td><code>repo</code> (path), <code>branch</code> (query, default &quot;main&quot;)</td>\n<td><code>image/svg+xml</code></td>\n<td>Returns an SVG status badge showing pipeline status for a repository/branch</td>\n</tr>\n<tr>\n<td><code>GET</code></td>\n<td><code>/assets/*</code></td>\n<td>Wildcard path</td>\n<td>Static files (JS, CSS, images)</td>\n<td>Serves static assets for the dashboard SPA</td>\n</tr>\n<tr>\n<td><code>POST</code></td>\n<td><code>/api/runs/:id/cancel</code></td>\n<td><code>id</code> (path)</td>\n<td><code>application/json</code> (<code>{&quot;success&quot;: boolean}</code>)</td>\n<td>Attempts to cancel a running pipeline; marks as <code>STATUS_CANCELLED</code></td>\n</tr>\n</tbody></table>\n<p>The API is designed for simplicity and leverages the existing <code>Store</code> interface for data persistence. The <code>PipelineRun</code> returned from <code>/api/runs/:id</code> includes its nested <code>JobRun</code> slices, each potentially containing <code>StepRun</code> data for comprehensive inspection. The DAG endpoint returns a specialized structure optimized for frontend visualization libraries.</p>\n<h3 id=\"internal-behavior-and-algorithm\">Internal Behavior and Algorithm</h3>\n<p>The dashboard component orchestrates multiple subsystems: an HTTP server, real-time event broadcasting, SVG generation, and data aggregation. Its operation follows these coordinated processes:</p>\n<ol>\n<li><p><strong>HTTP Server Initialization and Static Asset Serving</strong></p>\n<ul>\n<li>The component starts an HTTP server listening on a configured port (e.g., <code>:8080</code>)</li>\n<li>It registers route handlers for API endpoints and static file serving</li>\n<li>For the root path <code>/</code>, it serves a pre-built <code>index.html</code> that loads the JavaScript SPA</li>\n<li>Static assets (JS, CSS, images) are served from an embedded filesystem or directory with appropriate cache headers (1-hour max-age for immutable assets)</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Run Listing and Detail Retrieval</strong></p>\n<ul>\n<li>When <code>/api/runs</code> is requested, the handler calls <code>ListPipelineRuns</code> on the <code>Store</code> with <code>limit</code> and <code>offset</code> parsed from query parameters</li>\n<li>The results are serialized to JSON, with timestamps formatted as RFC3339 strings for consistency</li>\n<li>For <code>/api/runs/:id</code>, the handler retrieves the <code>PipelineRun</code> by ID, then fetches all associated <code>JobRun</code>s using a <code>GetJobRunsForPipeline</code> method (not defined in conventions but implied)</li>\n<li>The nested structure is assembled and returned as a single JSON object</li>\n</ul>\n</li>\n<li><p><strong>Real-Time Log Streaming via Server-Sent Events</strong></p>\n<ul>\n<li>When a client connects to <code>/api/runs/:id/logs</code>, the server establishes an SSE connection by setting headers: <code>Content-Type: text/event-stream</code>, <code>Cache-Control: no-cache</code>, <code>Connection: keep-alive</code></li>\n<li>The handler registers the client&#39;s connection channel with a central <code>LogBroadcaster</code> component, keyed by pipeline run ID (and optionally job/step filters)</li>\n<li>As the job execution engine writes log lines (via <code>StreamLogs</code>), it publishes them to the <code>LogBroadcaster</code></li>\n<li>The broadcaster formats each log line as an SSE event: <code>data: &lt;JSON-encoded log chunk&gt;\\n\\n</code> and sends to all registered clients</li>\n<li>The connection is kept alive with periodic heartbeat comments (<code>: ping\\n\\n</code>) every 30 seconds to prevent timeout</li>\n<li>On client disconnect or pipeline completion, the handler unregisters the client channel</li>\n</ul>\n</li>\n<li><p><strong>SVG Badge Generation</strong></p>\n<ul>\n<li>The badge endpoint <code>/badge/:repo.svg</code> extracts repository identifier and branch from request parameters</li>\n<li>It queries the <code>Store</code> for the most recent <code>PipelineRun</code> for that repository/branch with status in <code>[STATUS_SUCCEEDED, STATUS_FAILED, STATUS_CANCELLED]</code></li>\n<li>Based on the status, it selects a color (green for success, red for failure, orange for cancelled, gray for pending/running)</li>\n<li>It renders an inline SVG using a template with dynamic text (<code>{repo} | {branch} | {status}</code>), color, and optional animation for running builds</li>\n<li>The response includes caching headers: <code>Cache-Control: max-age=60</code> for dynamic status, <code>max-age=3600</code> for completed builds</li>\n</ul>\n</li>\n<li><p><strong>Dependency Graph Visualization Data Preparation</strong></p>\n<ul>\n<li>The DAG endpoint <code>/api/runs/:id/dag</code> retrieves the pipeline run and its configuration</li>\n<li>It reconstructs the execution graph using the original <code>BuildExecutionGraph</code> algorithm from the parsed <code>PipelineConfig</code></li>\n<li>It transforms this into a frontend-friendly format: nodes with <code>id</code>, <code>label</code>, <code>status</code>, <code>type</code> (&quot;stage&quot; or &quot;job&quot;), and edges with <code>source</code>, <code>target</code>, <code>type</code> (&quot;depends_on&quot;)</li>\n<li>The graph is enriched with runtime data: actual start/end times, durations, and status colors from the <code>JobRun</code> records</li>\n<li>The resulting JSON structure is optimized for libraries like Cytoscape.js or D3.js</li>\n</ul>\n</li>\n<li><p><strong>Artifact Listing and Secure Download</strong></p>\n<ul>\n<li>The artifacts endpoint lists files stored during <code>CollectArtifacts</code> execution</li>\n<li>Each artifact is represented with metadata: <code>name</code>, <code>size_bytes</code>, <code>storage_key</code>, <code>content_type</code> (inferred from extension)</li>\n<li>The download endpoint streams files from persistent storage (e.g., local filesystem, S3-compatible storage) with proper content-disposition headers</li>\n<li>In a production system, this endpoint would validate authentication tokens; for the educational implementation, it may be omitted or use a simple shared secret</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"adr-real-time-log-streaming-websockets-vs-server-sent-events\">ADR: Real-time Log Streaming: WebSockets vs. Server-Sent Events</h3>\n<blockquote>\n<p><strong>Decision: Use Server-Sent Events (SSE) for real-time log streaming</strong></p>\n<ul>\n<li><strong>Context</strong>: The dashboard needs to push real-time log output from running jobs to connected browsers with minimal latency. The solution must be simple to implement, reliable, and handle potentially long-lived connections (jobs running 30+ minutes).</li>\n<li><strong>Options Considered</strong>: <ol>\n<li><strong>WebSockets</strong>: Full-duplex communication channel over a single TCP connection. Provides bidirectional messaging.</li>\n<li><strong>Server-Sent Events (SSE)</strong>: Unidirectional server-to-client streaming over standard HTTP, automatically reconnects.</li>\n<li><strong>Long-polling</strong>: Client repeatedly requests updates, server holds connection until new data arrives.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement Server-Sent Events for log streaming.</li>\n<li><strong>Rationale</strong>: <ul>\n<li><strong>Simplicity</strong>: SSE uses plain HTTP with minimal protocol overhead; no special framing or handshake needed. The browser&#39;s <code>EventSource</code> API is trivial to use.</li>\n<li><strong>Unidirectional nature fits the use case</strong>: Log streaming is purely server→client; we don&#39;t need client→server messages during streaming.</li>\n<li><strong>Automatic reconnection</strong>: The <code>EventSource</code> API automatically reconnects on failure, providing resilience to network interruptions.</li>\n<li><strong>HTTP compatibility</strong>: Works seamlessly with existing HTTP infrastructure (load balancers, firewalls) and doesn&#39;t require special WebSocket support.</li>\n<li><strong>Resource efficiency</strong>: Less overhead than maintaining full WebSocket connections when only unidirectional streaming is needed.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Easier implementation and debugging (plain text protocol), better browser compatibility for our needs, automatic reconnection handling.</li>\n<li><strong>Negative</strong>: Limited to UTF-8 text data (sufficient for logs), maximum concurrent connections per browser (6 per domain, but sufficient for our use case), no bidirectional communication without separate HTTP requests.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>WebSockets</strong></td>\n<td>Full-duplex, low latency, efficient for high-frequency messages</td>\n<td>More complex protocol, requires connection upgrade, manual reconnection logic</td>\n<td>No</td>\n</tr>\n<tr>\n<td><strong>Server-Sent Events (SSE)</strong></td>\n<td>Simple HTTP-based, automatic reconnection, perfect for server→client streaming</td>\n<td>Unidirectional only, limited to text data, browser connection limits</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td><strong>Long-polling</strong></td>\n<td>Works with any HTTP server, simple to implement</td>\n<td>Higher latency, repeated connection overhead, less efficient</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Browser Connection Limits Exhaustion</strong>\n<em>Description</em>: Modern browsers limit concurrent connections to a single domain (typically 6). If the dashboard opens multiple SSE connections for different pipeline runs simultaneously, it can exhaust this limit, blocking other resources (CSS, JS, API calls).\n<em>Why it&#39;s wrong</em>: This causes the dashboard to freeze or fail to load additional runs while streaming logs from multiple active pipelines.\n<em>Fix</em>: Implement connection management: 1) Close SSE connections when users navigate away from the log view; 2) Use a single multiplexed SSE connection that streams all logs with filtering on the client side; 3) Implement exponential backoff for reconnection attempts.</p>\n<p>⚠️ <strong>Pitfall: Log Streaming Backpressure and Memory Buildup</strong>\n<em>Description</em>: When a job produces logs faster than the browser can consume them (slow network, busy client), unbuffered writes to the SSE connection can cause memory buildup on the server as it buffers unsent data.\n<em>Why it&#39;s wrong</em>: This can lead to memory exhaustion, crashing the dashboard server during high-output jobs.\n<em>Fix</em>: Implement backpressure control: 1) Use buffered channels with limited capacity for log broadcasting; 2) Drop old log lines if buffer exceeds limit (with warning); 3) Monitor client readiness using the underlying TCP flow control.</p>\n<p>⚠️ <strong>Pitfall: Missing Cache Headers on SVG Badges</strong>\n<em>Description</em>: SVG badges served without proper <code>Cache-Control</code> headers cause every page load (e.g., README display) to request a fresh badge, overwhelming the server.\n<em>Why it&#39;s wrong</em>: This creates unnecessary load and delays badge display. GitHub caches external images aggressively, but without proper headers, requests still reach the server.\n<em>Fix</em>: Set appropriate cache headers based on build status: <code>max-age=60</code> for running/pending builds (frequent updates), <code>max-age=3600</code> for completed builds (stable status). Use <code>ETag</code> or <code>Last-Modified</code> headers for conditional requests.</p>\n<p>⚠️ <strong>Pitfall: Cross-Origin Requests Blocked for Embedded Badges</strong>\n<em>Description</em>: When badges are embedded in external sites (GitHub README), the browser&#39;s same-origin policy may block the SVG image request if CORS headers aren&#39;t set.\n<em>Why it&#39;s wrong</em>: Badges appear as broken images on GitHub or other external sites.\n<em>Fix</em>: Add CORS headers to the badge endpoint: <code>Access-Control-Allow-Origin: *</code> (or specific origins). Also ensure the endpoint responds correctly to <code>OPTIONS</code> preflight requests if needed.</p>\n<p>⚠️ <strong>Pitfall: Inefficient DAG Data Structure for Large Pipelines</strong>\n<em>Description</em>: Returning the entire execution graph with all job details for pipelines with 100+ jobs creates massive JSON payloads (MBs), slowing page load.\n<em>Why it&#39;s wrong</em>: The dashboard becomes unresponsive while loading, and the visualization may render poorly with too many nodes.\n<em>Fix</em>: Implement pagination or progressive loading for the DAG: 1) Return only top-level stages initially; 2) Load job details on demand when nodes are expanded; 3) Use server-side filtering to return only visible portions of the graph.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td>Standard library <code>net/http</code> with <code>gorilla/mux</code> for routing</td>\n<td><code>chi</code> router for lightweight routing with middleware</td>\n</tr>\n<tr>\n<td>Static File Serving</td>\n<td><code>embed</code> package (Go 1.16+) for embedding built frontend</td>\n<td>CDN serving with cache invalidation</td>\n</tr>\n<tr>\n<td>Real-time Streaming</td>\n<td>Server-Sent Events via <code>http.ResponseWriter</code></td>\n<td><code>github.com/r3labs/sse</code> library for SSE utilities</td>\n</tr>\n<tr>\n<td>Frontend Framework</td>\n<td>Vanilla JavaScript with minimal dependencies</td>\n<td>React/Vue.js SPA with state management</td>\n</tr>\n<tr>\n<td>SVG Generation</td>\n<td>Go template with inline SVG XML</td>\n<td><code>github.com/ajstarks/svgo</code> for programmatic SVG</td>\n</tr>\n<tr>\n<td>DAG Visualization</td>\n<td>Preprocess JSON for <code>cytoscape.js</code></td>\n<td>Custom Canvas rendering with <code>d3.js</code></td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n├── cmd/\n│   └── dashboard/\n│       └── main.go              # Dashboard entry point\n├── internal/\n│   ├── dashboard/\n│   │   ├── server.go            # HTTP server setup and routing\n│   │   ├── handlers.go          # HTTP route handlers\n│   │   ├── broadcaster.go       # Log broadcasting for SSE\n│   │   ├── badges.go            # SVG badge generation\n│   │   ├── dag.go               # DAG data preparation\n│   │   └── artifacts.go         # Artifact serving logic\n│   ├── store/                   # Storage interface and implementations\n│   └── models.go                # Shared data structures (PipelineRun, etc.)\n├── web/\n│   ├── public/                  # Static assets (built frontend)\n│   │   ├── index.html\n│   │   ├── app.js\n│   │   └── style.css\n│   └── src/                     # Frontend source (optional)\n│       └── ...                  # JavaScript/TypeScript source files\n└── go.mod</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (COMPLETE, ready to use)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/dashboard/broadcaster.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> dashboard</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogEvent represents a single log line or chunk for broadcasting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEvent</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tPipelineRunID </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"pipeline_run_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tJobRunID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"job_run_id,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tStepName      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"step_name,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTimestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tLine          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"line\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tStream        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"stream\"`</span><span style=\"color:#6A737D\"> // \"stdout\" or \"stderr\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ClientChannel represents a connected SSE client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClientChannel</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tID       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tPipelineRunID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tJobFilter    </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // optional</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tStepFilter   </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // optional</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tSend         </span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogBroadcaster manages real-time log distribution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogBroadcaster</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tmu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tclients       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#6A737D\"> // pipelineID -> clientID -> client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tclientCounter </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLogBroadcaster creates a new broadcaster instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLogBroadcaster</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tclients: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterClient adds a new client for a specific pipeline run</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">pipelineRunID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobFilter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">stepFilter</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tb.clientCounter</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tclientID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"client-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, b.clientCounter)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tclient </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tID:            clientID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tPipelineRunID: pipelineRunID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tJobFilter:     jobFilter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tStepFilter:    stepFilter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tSend:          </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#6A737D\">// Buffered channel for backpressure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.clients[pipelineRunID]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tb.clients[pipelineRunID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tb.clients[pipelineRunID][clientID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UnregisterClient removes a client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UnregisterClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">pipelineRunID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">clientID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> pipelineClients, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.clients[pipelineRunID]; ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tif</span><span style=\"color:#E1E4E8\"> client, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> pipelineClients[clientID]; ok {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">\t\t\tclose</span><span style=\"color:#E1E4E8\">(client.Send)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">\t\t\tdelete</span><span style=\"color:#E1E4E8\">(pipelineClients, clientID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tif</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(pipelineClients) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">\t\t\tdelete</span><span style=\"color:#E1E4E8\">(b.clients, pipelineRunID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Broadcast sends a log event to all interested clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Broadcast</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">event</span><span style=\"color:#B392F0\"> LogEvent</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tb.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tclients, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.clients[event.PipelineRunID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tjsonData, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tsseData </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"data: </span><span style=\"color:#79B8FF\">%s\\n\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, jsonData)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tfor</span><span style=\"color:#E1E4E8\"> _, client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> clients {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t// Apply filters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tif</span><span style=\"color:#E1E4E8\"> client.JobFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> client.JobFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> event.JobRunID {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tcontinue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tif</span><span style=\"color:#E1E4E8\"> client.StepFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> client.StepFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> event.StepName {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tcontinue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t// Non-blocking send with buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tselect</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#E1E4E8\"> client.Send </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(sseData):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t\t// Successfully sent</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tdefault</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t\t// Buffer full, drop event for this client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t\t// Optionally log or implement backpressure strategy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SSEHandler sets up Server-Sent Events response</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SSEHandler</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">pipelineRunID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobFilter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">stepFilter</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Set SSE headers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tw.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"text/event-stream\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tw.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Cache-Control\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"no-cache\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tw.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Connection\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"keep-alive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tw.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Allow-Origin\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Create flusher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tflusher, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.(</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Flusher</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\thttp.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Streaming unsupported\"</span><span style=\"color:#E1E4E8\">, http.StatusInternalServerError)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Register client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tclient </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.</span><span style=\"color:#B392F0\">RegisterClient</span><span style=\"color:#E1E4E8\">(pipelineRunID, jobFilter, stepFilter)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> b.</span><span style=\"color:#B392F0\">UnregisterClient</span><span style=\"color:#E1E4E8\">(pipelineRunID, client.ID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Send initial comment to establish connection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tfmt.</span><span style=\"color:#B392F0\">Fprintf</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\": connected</span><span style=\"color:#79B8FF\">\\n\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tflusher.</span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Heartbeat ticker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\theartbeat </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> heartbeat.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Handle client close</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tctx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tfor</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tselect</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t\t// Client disconnected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">heartbeat.C:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t\t// Send heartbeat comment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tfmt.</span><span style=\"color:#B392F0\">Fprintf</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\": ping</span><span style=\"color:#79B8FF\">\\n\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tflusher.</span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#E1E4E8\"> data, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">client.Send:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tif</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t\t\t// Channel closed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t_, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t\t\t\t// Client likely disconnected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tflusher.</span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code (signature + TODOs only)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/dashboard/handlers.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> dashboard</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">yourproject/internal/store</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DashboardServer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tStore         </span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tBroadcaster   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// ... other dependencies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRuns handles GET /api/runs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetRuns</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Parse limit and offset query parameters with defaults (limit=50, offset=0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Call store.ListPipelineRuns with the parsed parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Handle potential error from store (return HTTP 500 with appropriate message)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Set Content-Type header to application/json</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Encode the runs slice as JSON and write to response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Consider adding pagination metadata (total count, next/prev links) in response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRunDetail handles GET /api/runs/:id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetRunDetail</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Extract pipeline run ID from URL path (use gorilla/mux or chi)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Call store.GetPipelineRun to retrieve the main run object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Call store.GetJobRunsForPipeline to retrieve all associated job runs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Assemble a response DTO that nests job runs within the pipeline run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: For each job run, optionally fetch step details if needed for the UI</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Encode and return the combined response as JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: Handle not found errors (return HTTP 404) and other errors appropriately</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StreamLogs handles GET /api/runs/:id/logs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StreamLogs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Extract pipeline run ID from URL path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Extract optional job and step filters from query parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Validate that the pipeline run exists and is accessible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Call broadcaster.SSEHandler with the appropriate parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Note: This method should not return until the SSE connection closes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetBadge handles GET /badge/:repo.svg</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetBadge</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Extract repository identifier from URL path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Extract branch from query parameter (default to \"main\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Query store for most recent completed pipeline run for repo/branch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Determine badge color based on status: success=#4CAF50, failure=#F44336, running=#2196F3, pending=#FF9800</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Generate SVG using template or programmatic generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Set appropriate Cache-Control headers based on status (running: max-age=60, completed: max-age=3600)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: Set Content-Type to image/svg+xml and CORS headers (Access-Control-Allow-Origin: *)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 8: Write SVG to response body</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetDAG handles GET /api/runs/:id/dag</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetDAG</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Extract pipeline run ID from URL path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Retrieve pipeline run and its configuration from store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: If config not stored with run, you may need to retrieve from original source</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Reconstruct execution graph using BuildExecutionGraph on the config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Transform graph into frontend-friendly format with nodes and edges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Enrich nodes with runtime data (status, timestamps) from job runs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: For large pipelines, consider implementing pagination or level-of-detail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 8: Return JSON structure optimized for visualization library consumption</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ListArtifacts handles GET /api/runs/:id/artifacts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ListArtifacts</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Extract pipeline run ID from URL path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Query store for artifact metadata associated with this run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Format response as array of objects with name, size, storage_key, content_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Return JSON response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Consider implementing authentication/authorization for production use</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li>Use Go&#39;s <code>embed</code> package (Go 1.16+) to bundle static frontend files directly into the binary: <code>//go:embed web/public/*</code> then <code>http.FS</code> to serve.</li>\n<li>For SSE, ensure you call <code>http.Flusher.Flush()</code> after writing each event to immediately send data to the client.</li>\n<li>Use <code>context.Context</code> to detect client disconnection in streaming handlers via <code>r.Context().Done()</code>.</li>\n<li>For JSON serialization of time fields, use <code>json:&quot;,omitempty&quot;</code> for nullable timestamps (<code>*time.Time</code>) and consider custom marshaler for formatting.</li>\n<li>Implement middleware for CORS headers: <code>w.Header().Set(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;)</code> for development; restrict in production.</li>\n<li>Use <code>sync.RWMutex</code> in the <code>LogBroadcaster</code> for concurrent client map access—read lock for broadcasting, write lock for registration/deregistration.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the dashboard component, verify functionality:</p>\n<ol>\n<li><strong>Start the dashboard server</strong>: <code>go run cmd/dashboard/main.go</code> should start on port 8080 (or configured port).</li>\n<li><strong>Access the dashboard</strong>: Navigate to <code>http://localhost:8080/</code> in a browser. You should see the dashboard interface (even if empty).</li>\n<li><strong>Trigger a pipeline run</strong> via webhook (from Milestone 3) or manually via API.</li>\n<li><strong>Verify real-time updates</strong>: Open the dashboard while a pipeline is running. You should see:<ul>\n<li>The run appear in the list with <code>STATUS_RUNNING</code></li>\n<li>Clicking on the run should show detailed view with job statuses</li>\n<li>Clicking &quot;View Logs&quot; should open a streaming log view that updates in real-time</li>\n</ul>\n</li>\n<li><strong>Check badge generation</strong>: Visit <code>http://localhost:8080/badge/my-repo.svg?branch=main</code>. You should see an SVG badge with status.</li>\n<li><strong>Verify artifact listing</strong>: After a job with artifacts completes, the artifacts should be listed and downloadable.</li>\n</ol>\n<p><strong>Expected signs of correct implementation</strong>:</p>\n<ul>\n<li>No browser errors in developer console</li>\n<li>Logs stream without requiring page refresh</li>\n<li>Badge images render correctly in <code>&lt;img&gt;</code> tags</li>\n<li>DAG visualization shows correct node relationships</li>\n</ul>\n<p><strong>Common issues and checks</strong>:</p>\n<ul>\n<li><em>Logs not streaming</em>: Check browser network tab for SSE connection status (200). Verify <code>LogBroadcaster.Broadcast</code> is being called from the job execution engine.</li>\n<li><em>Badge not updating</em>: Check cache headers; force refresh with Ctrl+F5. Verify the store query returns the correct latest run.</li>\n<li><em>CORS errors for badges</em>: Check response headers include <code>Access-Control-Allow-Origin: *</code>.</li>\n<li><em>High memory usage during log streaming</em>: Check the buffered channel size in <code>LogBroadcaster</code> and implement backpressure strategy.</li>\n</ul>\n<p><strong>G. Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dashboard shows &quot;Loading...&quot; forever</td>\n<td>JavaScript bundle not loading or API endpoints failing</td>\n<td>Open browser dev tools → Network tab, check for failed requests (404/500). Check console for errors.</td>\n<td>Ensure static files are correctly embedded/served. Verify API endpoints return valid JSON.</td>\n</tr>\n<tr>\n<td>Logs stop updating after a few seconds</td>\n<td>SSE connection dropping due to timeout or network issue</td>\n<td>Check browser network tab → SSE connection status (disconnected). Check server logs for errors.</td>\n<td>Implement heartbeat in SSE handler. Increase server timeout settings. Check for proxy/firewall killing idle connections.</td>\n</tr>\n<tr>\n<td>Badge shows wrong status (e.g., success when failed)</td>\n<td>Cache returning old badge or query fetching wrong run</td>\n<td>Check response headers for <code>Cache-Control</code>. Verify store query orders by <code>CreatedAt DESC</code> and filters by status.</td>\n<td>Adjust cache headers. Debug the database query for latest run.</td>\n</tr>\n<tr>\n<td>DAG visualization renders nodes overlapping</td>\n<td>Frontend graph layout algorithm issue or missing layout config</td>\n<td>Check browser console for visualization library errors. Inspect the JSON data structure from <code>/api/runs/:id/dag</code>.</td>\n<td>Ensure DAG endpoint returns proper node/edge structure. Configure layout options in frontend (e.g., <code>dagre</code> layout for Cytoscape).</td>\n</tr>\n<tr>\n<td>Artifact download fails with 404</td>\n<td>Storage key not found or path mapping incorrect</td>\n<td>Check server logs for error when accessing artifact. Verify the storage key exists in the artifact storage.</td>\n<td>Ensure <code>CollectArtifacts</code> correctly stores artifacts and records keys in <code>JobRun.ArtifactKeys</code>. Verify download handler reads from correct storage backend.</td>\n</tr>\n<tr>\n<td>High CPU when multiple users view logs</td>\n<td>Unoptimized broadcast to many clients or inefficient filtering</td>\n<td>Profile server CPU. Check number of registered clients in <code>LogBroadcaster</code>.</td>\n<td>Implement client filtering at broadcast time. Consider connection pooling or multiplexing.</td>\n</tr>\n</tbody></table>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>This section traces the journey of a single code change through the entire CI/CD system, from the initial Git push to the final pipeline completion notification. Understanding these interactions is critical for debugging and for appreciating how the isolated components defined in previous sections collaborate to deliver a seamless automation experience. We will follow a concrete example, define the precise data formats exchanged between components, and explain the sophisticated concurrency patterns that enable efficient parallel execution.</p>\n<h3 id=\"happy-path-from-push-to-pipeline-completion\">Happy Path: From Push to Pipeline Completion</h3>\n<p>Imagine a developer pushes a commit to the <code>main</code> branch of their repository. This event triggers a cascade of actions across our system, transforming a simple HTTP payload into a fully executed pipeline with logs, artifacts, and a status report. The journey can be visualized in the sequence diagram:</p>\n<p><img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-webhook-sequence.svg\" alt=\"Sequence: Webhook to Job Execution\"></p>\n<p>Let&#39;s walk through each step in detail, following the life of a hypothetical pipeline run with ID <code>run_abc123</code>.</p>\n<ol>\n<li><p><strong>Git Host Fires the Webhook:</strong> Upon the push event, the Git hosting service (e.g., GitHub) packages event details into a JSON payload. It signs this payload with a secret key known only to itself and our CI system, then dispatches an HTTP POST request to our configured webhook endpoint URL (e.g., <code>https://ci.example.com/webhooks/github</code>).</p>\n</li>\n<li><p><strong>Webhook Listener Validates and Parses:</strong> Our <code>DashboardServer</code>&#39;s HTTP handler receives the request. The <code>WebhookVerifier</code> is invoked to <code>VerifyGitHubSignature</code>, ensuring the payload is authentic and untampered. The <code>ExtractEventDetails</code> function parses the JSON to extract the <strong>repository URL</strong>, <strong>commit SHA</strong> (<code>abc123</code>), <strong>branch name</strong> (<code>main</code>), and <strong>event type</strong> (<code>EVENT_PUSH</code>).</p>\n</li>\n<li><p><strong>Pipeline Configuration Retrieval:</strong> The system clones the repository at the given commit SHA into a temporary workspace. It looks for the pipeline configuration file (e.g., <code>.ci/pipeline.yaml</code>) in the root of the repository. The raw YAML content is read from the cloned code.</p>\n</li>\n<li><p><strong>Configuration Parsing and Enrichment:</strong> The YAML string is passed to <code>ParseConfig</code>. This function validates the syntax and structure, returning a <code>PipelineConfig</code> object. <code>ResolveEnvVars</code> is called to substitute any <code>$VARIABLE</code> references within the config, using a combined map of system environment variables and pipeline-defined defaults. If any jobs use the <code>matrix</code> keyword, <code>ExpandMatrix</code> is invoked, transforming a single <code>JobConfig</code> into multiple <code>JobConfig</code> instances (e.g., <code>test-go-1.19</code>, <code>test-go-1.20</code>). Finally, <code>BuildExecutionGraph</code> analyzes the <code>Needs</code> clauses in each job to produce a dependency graph (a DAG).</p>\n</li>\n<li><p><strong>Pipeline Run Creation:</strong> The orchestrator (logic within the webhook handler) creates a new <code>PipelineRun</code> instance. It populates the struct with the <code>Trigger</code> (<code>EVENT_PUSH</code>), <code>CommitSHA</code>, <code>Branch</code>, <code>Status</code> (<code>STATUS_PENDING</code>), and a pointer to the fully resolved <code>PipelineConfig</code>. This run is persisted via <code>CreatePipelineRun</code>.</p>\n</li>\n<li><p><strong>Job Runs Generation and Enqueueing:</strong> For each job defined in the expanded pipeline configuration, a corresponding <code>JobRun</code> is created. Its <code>Status</code> is set to <code>STATUS_PENDING</code>, and its <code>Needs</code> dependencies are recorded. Jobs with no dependencies (the entry points of the DAG) are immediately eligible for execution. The orchestrator calls <code>EnqueueJobRun</code> for each eligible job. This method does two things: it persists the <code>JobRun</code> to the <code>Store</code> (e.g., SQLite), and then pushes a reference (an <code>Item</code> containing the <code>JobRunID</code> and <code>Priority</code>) onto the in-memory <code>PriorityQueue</code> within the <code>HybridQueue</code>.</p>\n</li>\n<li><p><strong>Worker Polls for Work:</strong> A worker process, which is part of a configured pool, is idle and continuously calling <code>DequeueJobRun</code>. This method performs an atomic operation: it locks the queue, pops the highest-priority <code>Item</code> from the heap, loads the full <code>JobRun</code> data from the <code>Store</code>, updates the job&#39;s <code>Status</code> to <code>STATUS_RUNNING</code> and <code>AssignedWorker</code> to the worker&#39;s ID, and finally returns the <code>JobRun</code> struct to the worker.</p>\n</li>\n<li><p><strong>Job Execution in Container:</strong> The worker receives the <code>JobRun</code> and calls <code>ExecuteJob</code>. This function interacts with the Docker daemon: it pulls the required image (if not cached), creates a container with a fresh workspace volume, and injects the resolved environment variables (including any secrets). It then iterates through the job&#39;s <code>Steps</code>, executing each shell command sequentially via <code>StreamLogs</code>. The output (stdout/stderr) is captured in real-time. Each line of output is packaged into a <code>LogEvent</code> and sent to the <code>LogBroadcaster</code>.</p>\n</li>\n<li><p><strong>Real-Time Log Streaming:</strong> The <code>LogBroadcaster</code> receives the <code>LogEvent</code> for <code>run_abc123</code>. It holds a registry of all connected dashboard clients (browsers) that have subscribed to logs for this run via the <code>SSEHandler</code>. The broadcaster iterates through these clients and sends the log line as a Server-Sent Event (SSE) formatted string (<code>data: {&quot;job&quot;: &quot;...&quot;, &quot;line&quot;: &quot;...&quot;}\\n\\n</code>). The user&#39;s dashboard page receives this event and appends the line to the relevant job&#39;s log viewer.</p>\n</li>\n<li><p><strong>Step Completion and Artifact Collection:</strong> After a step finishes, its <code>ExitCode</code> is recorded. If non-zero, the job is marked as <code>STATUS_FAILED</code>, and execution stops. If all steps succeed, <code>CollectArtifacts</code> is called. This function uses <code>docker cp</code> to copy files matching the job&#39;s artifact patterns (e.g., <code>./bin/*</code>) from the container&#39;s workspace to a persistent storage location (e.g., a local directory or cloud bucket). The resulting file paths are stored as <code>ArtifactKeys</code> in the <code>JobRun</code>.</p>\n</li>\n<li><p><strong>Job Completion and Dependency Unblocking:</strong> The worker calls <code>MarkJobRunComplete</code> to update the job&#39;s <code>Status</code> to <code>STATUS_SUCCEEDED</code> and its <code>FinishedAt</code> timestamp. Crucially, this function also queries the <code>Store</code> to find all <code>JobRun</code> records whose <code>Needs</code> dependencies are now satisfied (i.e., all the jobs they depend on have succeeded). For each of these now-eligible jobs, it calls <code>EnqueueJobRun</code> to push them onto the queue. This is the mechanism that drives the DAG forward.</p>\n</li>\n<li><p><strong>Pipeline Run Completion:</strong> Steps 7-11 repeat for all jobs in the DAG. The orchestrator (or a separate monitoring process) periodically checks if all <code>JobRun</code> records for a <code>PipelineRun</code> have reached a terminal state (<code>STATUS_SUCCEEDED</code>, <code>STATUS_FAILED</code>, or <code>STATUS_CANCELLED</code>). Once this is true, it updates the <code>PipelineRun</code>&#39;s overall <code>Status</code> to reflect the aggregate outcome (e.g., if any job failed, the pipeline run fails).</p>\n</li>\n<li><p><strong>Dashboard Reflection:</strong> Throughout this process, the user has the dashboard open on the build detail page for <code>run_abc123</code>. They see:</p>\n<ul>\n<li>The pipeline run status change from &quot;Pending&quot; to &quot;Running&quot; to &quot;Succeeded&quot;.</li>\n<li>The DAG visualization update in real-time as jobs move from pending to running to succeeded.</li>\n<li>A live, streaming log for each job as it executes.</li>\n<li>Upon completion, an &quot;Artifacts&quot; section appears with download links.</li>\n</ul>\n</li>\n</ol>\n<p>This end-to-end flow demonstrates the seamless integration of parsing, isolation, queueing, and visualization that defines a robust CI/CD system.</p>\n<h3 id=\"message-and-event-formats\">Message and Event Formats</h3>\n<p>The components communicate using well-defined data structures. The primary &quot;messages&quot; are the core entities (<code>PipelineRun</code>, <code>JobRun</code>) stored in the database, and the <code>LogEvent</code> streamed to the dashboard. Additionally, webhooks and internal queue items have specific formats.</p>\n<h4 id=\"webhook-payload-github-push-event-example\">Webhook Payload (GitHub Push Event Example)</h4>\n<p>The CI system must handle a subset of fields from provider-specific payloads. The <code>ExtractEventDetails</code> function abstracts these differences.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Description</th>\n<th align=\"left\">Source (GitHub Example)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>repository.full_name</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Repository identifier in &quot;owner/name&quot; format. Used to match pipeline configs and clone.</td>\n<td align=\"left\"><code>&quot;octocat/Hello-World&quot;</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>ref</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Full Git ref that was pushed. Parsed to extract branch/tag name.</td>\n<td align=\"left\"><code>&quot;refs/heads/main&quot;</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>head_commit.id</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">The SHA of the most recent commit in the push. The commit that will be built.</td>\n<td align=\"left\"><code>&quot;abc123...&quot;</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>head_commit.message</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Commit message. Could be used for conditional logic or display.</td>\n<td align=\"left\"><code>&quot;Fix login bug&quot;</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>action</code> / <code>event</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Type of event: <code>push</code>, <code>pull_request.opened</code>, <code>tag</code>. Mapped to <code>EVENT_PUSH</code>, etc.</td>\n<td align=\"left\"><code>&quot;push&quot;</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>pull_request</code> (optional)</td>\n<td align=\"left\"><code>object</code></td>\n<td align=\"left\">For PR events, contains base/head refs and SHA. Used for building the merge commit.</td>\n<td align=\"left\"><code>{&quot;base&quot;: {&quot;ref&quot;: &quot;main&quot;, &quot;sha&quot;: &quot;...&quot;}, ...}</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>signature</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">The value of the <code>X-Hub-Signature-256</code> HTTP header. Used for verification.</td>\n<td align=\"left\"><code>&quot;sha256=...&quot;</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"internal-queue-item-item\">Internal Queue Item (<code>Item</code>)</h4>\n<p>The <code>PriorityQueue</code> manages <code>Item</code> structs, which are lightweight references to persisted <code>JobRun</code> records.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>JobRunID</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Primary key of the <code>JobRun</code> in the <code>Store</code>. The worker fetches the full details using this ID.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>PipelineRunID</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Foreign key to the parent <code>PipelineRun</code>. Useful for priority grouping or cleanup.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Priority</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Numerical priority. <strong>Higher</strong> numbers are dequeued first. Critical deployments might have priority 100, while regular tests have 10.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>CreatedAt</code></td>\n<td align=\"left\"><code>time.Time</code></td>\n<td align=\"left\">Timestamp of when the item was enqueued. Used as a tie-breaker for equal priority (FIFO).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>index</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Internal field used by the heap implementation (<code>container/heap</code>) to track position.</td>\n</tr>\n</tbody></table>\n<h4 id=\"log-event-logevent\">Log Event (<code>LogEvent</code>)</h4>\n<p>This is the data structure broadcast to all connected dashboard clients whenever a job produces a line of output.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>PipelineRunID</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">The pipeline run this log line belongs to. Allows clients to filter subscriptions.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>JobRunID</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">The specific job producing the output.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>StepName</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">The name of the step (e.g., <code>&quot;Install dependencies&quot;</code>) that emitted the line.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Timestamp</code></td>\n<td align=\"left\"><code>time.Time</code></td>\n<td align=\"left\">The exact time the line was captured on the worker. Used for ordering and display.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Line</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">The raw text of the log line (without newline).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Stream</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Either <code>&quot;stdout&quot;</code> or <code>&quot;stderr&quot;</code>. Allows the UI to color-code output.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The <code>LogEvent</code> is kept minimal. It does not contain the full <code>JobRun</code> or <code>StepRun</code> data. The dashboard client is responsible for fetching static metadata (job names, pipeline structure) via separate REST API calls and then correlating streaming log lines using the IDs.</p>\n</blockquote>\n<h3 id=\"concurrency-and-parallelism-patterns\">Concurrency and Parallelism Patterns</h3>\n<p>The system employs several layers of concurrency to maximize resource utilization and provide fast feedback. Coordination is crucial to prevent race conditions and ensure deterministic outcomes.</p>\n<h4 id=\"1-parallel-job-execution-via-worker-pool\">1. Parallel Job Execution via Worker Pool</h4>\n<p>This is the most fundamental concurrency pattern. The <code>HybridQueue</code> and worker pool enable multiple jobs to run simultaneously on a single host.</p>\n<ul>\n<li><strong>Pattern:</strong> Producer-Consumer with Priority Scheduling.</li>\n<li><strong>Producers:</strong> The webhook handler (<code>EnqueueJobRun</code>) and the job completion logic (<code>MarkJobRunComplete</code>) that unblocks dependent jobs.</li>\n<li><strong>Consumers:</strong> The worker goroutines, which call <code>DequeueJobRun</code> in a loop.</li>\n<li><strong>Coordination:</strong> The <code>HybridQueue</code> uses a mutex (<code>mu sync.Mutex</code>) to ensure that the <code>PriorityQueue</code> (a heap) and the <code>Store</code> updates are modified atomically. The <code>DequeueJobRun</code> method is the critical section: it must atomically &quot;claim&quot; a job so that no two workers receive the same <code>JobRun</code>. This is implemented by marking the job as <code>STATUS_RUNNING</code> within the same transaction that removes the <code>Item</code> from the heap.</li>\n<li><strong>Limit:</strong> The <code>workerPool</code> configuration (e.g., <code>maxConcurrency: 4</code>) limits the number of concurrent jobs. This is enforced by the pool manager, which only spins up a fixed number of worker goroutines.</li>\n</ul>\n<h4 id=\"2-dag-based-dependency-scheduling\">2. DAG-Based Dependency Scheduling</h4>\n<p>Jobs within a pipeline are not all independent; they form a Directed Acyclic Graph (DAG). The system must enforce these dependencies while maximizing parallelism where possible.</p>\n<ul>\n<li><strong>Pattern:</strong> Reactive Fan-out. Jobs are enqueued only when <strong>all</strong> of their upstream dependencies (listed in <code>Needs</code>) have completed successfully.</li>\n<li><strong>Algorithm:</strong> When a job finishes (<code>MarkJobRunComplete</code>), the system performs a query: <code>SELECT * FROM job_runs WHERE pipeline_run_id = ? AND status = ? AND needs_met = FALSE</code>. For each candidate job, it checks if all jobs in its <code>Needs</code> list have <code>Status == STATUS_SUCCEEDED</code>. If true, it sets <code>needs_met = TRUE</code> and calls <code>EnqueueJobRun</code>. This check can be optimized with a materialized view or additional indexing.</li>\n<li><strong>Consequence:</strong> This pattern allows multiple independent chains of jobs to run in parallel. For example, a <code>lint</code> job and a <code>test-unit</code> job with no <code>Needs</code> can start immediately and concurrently.</li>\n</ul>\n<h4 id=\"3-matrix-build-parallelism\">3. Matrix Build Parallelism</h4>\n<p>A single job definition can be expanded into multiple parallel job instances, each with a different combination of parameters (e.g., Go version, operating system).</p>\n<ul>\n<li><strong>Pattern:</strong> Parametric Fan-out.</li>\n<li><strong>Expansion:</strong> During parsing (<code>ExpandMatrix</code>), the cartesian product of all matrix axes is computed. For a matrix defining <code>go: [1.19, 1.20]</code> and <code>os: [linux, macos]</code>, four distinct <code>JobConfig</code> objects are created: <code>{go:1.19, os:linux}</code>, <code>{go:1.19, os:macos}</code>, etc. Each gets a unique <code>JobName</code> suffix (e.g., <code>test-go-1.19-linux</code>).</li>\n<li><strong>Execution:</strong> These expanded jobs are treated as independent nodes in the DAG. If the original job had <code>Needs: [build]</code>, all four expanded jobs inherit that dependency. Once <code>build</code> succeeds, all four matrix jobs can be enqueued <strong>and run in parallel</strong>, subject to worker pool limits.</li>\n<li><strong>Synchronization (Fan-in):</strong> If a downstream job <code>deploy</code> <code>Needs: [test]</code>, it must wait for <strong>all</strong> expanded matrix jobs named <code>test-*</code> to succeed. This is a fan-in synchronization point. The dependency resolution logic in <code>MarkJobRunComplete</code> handles this naturally because <code>deploy</code>&#39;s <code>Needs</code> list would contain the original job name <code>test</code>, and the system must interpret this as &quot;wait for all jobs where the base name is <code>test</code>&quot;.</li>\n</ul>\n<blockquote>\n<p><strong>ADR: Fan-in Synchronization Semantics for Matrix Jobs</strong></p>\n<ul>\n<li><strong>Context:</strong> When a job <code>C</code> depends on a job <code>B</code> that uses a matrix, we must define what &quot;job <code>B</code> is complete&quot; means. Does <code>C</code> start after the first <code>B</code> variant succeeds, after all succeed, or after any succeed?</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>All must succeed:</strong> <code>C</code> waits for every expanded variant of <code>B</code> (e.g., <code>B-go1.19</code>, <code>B-go1.20</code>) to reach <code>STATUS_SUCCEEDED</code>.</li>\n<li><strong>Any succeeds:</strong> <code>C</code> starts as soon as any single variant of <code>B</code> succeeds. This is risky and rarely desired.</li>\n<li><strong>First succeeds:</strong> <code>C</code> starts after the first variant succeeds and cancels the others. This requires complex coordination.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> <strong>Option 1 (All must succeed).</strong> This is the standard semantics in GitHub Actions and Jenkins. It ensures the downstream job (like a deployment) only proceeds if the test matrix passes <strong>entirely</strong>.</li>\n<li><strong>Rationale:</strong> Provides the safest and most intuitive behavior for CI/CD pipelines. A deployment should not proceed if tests for any configuration are failing.</li>\n<li><strong>Consequences:</strong> Requires the dependency resolution logic to treat a dependency on a matrix job name as a dependency on all its expanded children. The system must track the mapping from the original logical job name to its physical expanded <code>JobRun</code> IDs.</li>\n</ul>\n</blockquote>\n<h4 id=\"4-real-time-log-streaming-concurrency\">4. Real-Time Log Streaming Concurrency</h4>\n<p>The <code>LogBroadcaster</code> must efficiently deliver log events to many potentially idle dashboard clients.</p>\n<ul>\n<li><strong>Pattern:</strong> Publish-Subscribe with Client Registration.</li>\n<li><strong>Registration:</strong> When a browser connects to <code>/api/runs/run_abc123/logs</code>, the <code>SSEHandler</code> calls <code>RegisterClient</code>, creating a <code>ClientChannel</code> with a buffered <code>Send chan []byte</code>. The channel is stored in a nested map: <code>clients[pipelineRunID][clientID]</code>.</li>\n<li><strong>Broadcasting:</strong> When <code>Broadcast(event)</code> is called, it acquires a read lock (<code>mu.RLock</code>), iterates over the clients subscribed to that <code>pipelineRunID</code>, and attempts to send the formatted log line into each client&#39;s <code>Send</code> channel.</li>\n<li><strong>Non-Blocking Send:</strong> The send operation must <strong>never block</strong> the broadcaster if a client is slow (e.g., a user on a slow network). This is achieved by using a buffered channel for <code>Send</code> and a <code>select</code> with a <code>default</code> case that drops the log line if the buffer is full. This implements <strong>backpressure</strong> protection.</li>\n<li><strong>Cleanup:</strong> If the send fails (channel full), or if the client&#39;s HTTP connection closes, the <code>UnregisterClient</code> function is called to remove the channel from the map and close it, preventing memory leaks.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides starter code for the core coordination mechanisms: the hybrid queue and the log broadcaster. The queue ensures reliable, prioritized job dispatch, while the broadcaster enables the real-time log streaming that brings the dashboard to life.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option</th>\n<th align=\"left\">Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Queue Persistence</strong></td>\n<td align=\"left\"><strong>SQLite (<code>database/sql</code> + <code>mattn/go-sqlite3</code>)</strong>: Embedded, zero-config, ACID transactions.</td>\n<td align=\"left\">PostgreSQL (<code>lib/pq</code>): Better for high concurrency and scaling to multiple orchestrate instances.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Priority Queue</strong></td>\n<td align=\"left\"><strong><code>container/heap</code></strong>: Standard library implementation, perfect for in-memory priority queue.</td>\n<td align=\"left\">Custom heap based on slice with manual <code>heap.Interface</code>.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Real-time Transport</strong></td>\n<td align=\"left\"><strong>Server-Sent Events (SSE)</strong>: Simpler, works over standard HTTP, automatic reconnection.</td>\n<td align=\"left\">WebSockets (<code>gorilla/websocket</code>): Full-duplex, more overhead but enables bidirectional communication.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Concurrency Control</strong></td>\n<td align=\"left\"><strong><code>sync.Mutex</code> &amp; <code>sync.RWMutex</code></strong>: Straightforward for coordinating access to shared in-memory structures.</td>\n<td align=\"left\">Channels: Can be used for a more Go-idiomatic worker pool design.</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    server/                 # Main application entry point\n      main.go\n    worker/                 # Optional separate worker binary\n      main.go\n  internal/\n    queue/                  # Hybrid queue implementation\n      queue.go              # HybridQueue, PriorityQueue, Item\n      store.go              # Store interface and SQLite implementation\n    orchestrator/           # Logic for DAG scheduling &amp; run creation\n      orchestrator.go       # Calls parser, creates runs, triggers queue\n    dashboard/              # Web dashboard server\n      server.go             # DashboardServer, HTTP handlers\n      broadcaster.go        # LogBroadcaster, ClientChannel\n    executor/               # Job execution engine (Milestone 2)\n      docker.go             # Docker client wrapper\n    webhook/                # Webhook handling (Milestone 3)\n      verifier.go           # WebhookVerifier, signature validation\n    parser/                 # Pipeline configuration parser (Milestone 1)\n      parser.go</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-logbroadcaster\">C. Infrastructure Starter Code: LogBroadcaster</h4>\n<p>This is a complete, ready-to-use component for managing real-time log subscriptions.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/dashboard/broadcaster.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> dashboard</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEvent</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PipelineRunID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JobRunID      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StepName      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Line          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Stream        </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // \"stdout\" or \"stderr\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClientChannel</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID            </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PipelineRunID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JobFilter     </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // If empty, receive all jobs for the pipeline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StepFilter    </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // If empty, receive all steps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Send          </span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#6A737D\"> // Buffered channel for SSE data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogBroadcaster</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clients       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#6A737D\"> // pipelineRunID -> clientID -> ClientChannel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clientCounter </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLogBroadcaster</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        clients: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">pipelineRunID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jobFilter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">stepFilter</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.clients[pipelineRunID]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b.clients[pipelineRunID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clientID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"client-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, b.clientCounter)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.clientCounter</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ClientChannel</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ID:            clientID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        PipelineRunID: pipelineRunID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        JobFilter:     jobFilter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StepFilter:    stepFilter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Send:          </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#6A737D\">// Buffer 100 log lines</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.clients[pipelineRunID][clientID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UnregisterClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">pipelineRunID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">clientID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> pipelineClients, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.clients[pipelineRunID]; ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> ch, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> pipelineClients[clientID]; ok {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            close</span><span style=\"color:#E1E4E8\">(ch.Send)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            delete</span><span style=\"color:#E1E4E8\">(pipelineClients, clientID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(pipelineClients) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            delete</span><span style=\"color:#E1E4E8\">(b.clients, pipelineRunID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogBroadcaster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Broadcast</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">event</span><span style=\"color:#B392F0\"> LogEvent</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipelineClients, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.clients[event.PipelineRunID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`{\"job\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\",\"step\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\",\"stream\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\",\"line\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"}`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        event.JobRunID, event.StepName, event.Stream, </span><span style=\"color:#B392F0\">escapeJSONString</span><span style=\"color:#E1E4E8\">(event.Line))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sseMsg </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"data: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> pipelineClients {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Apply client-specific filters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> client.JobFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> client.JobFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> event.JobRunID {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> client.StepFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> client.StepFilter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> event.StepName {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Non-blocking send to prevent broadcaster from stalling on slow clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> client.Send </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> sseMsg:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Sent successfully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Channel buffer full; drop the log line for this client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Optionally log or increment a metric</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> escapeJSONString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">s</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simple escape for quotes and newlines. Use encoding/json for robust escaping in production.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">ReplaceAll</span><span style=\"color:#E1E4E8\">(strings.</span><span style=\"color:#B392F0\">ReplaceAll</span><span style=\"color:#E1E4E8\">(s, </span><span style=\"color:#9ECBFF\">`\"`</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">`\\\"`</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">n\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code-hybridqueue-dequeue\">D. Core Logic Skeleton Code: HybridQueue Dequeue</h4>\n<p>The <code>DequeueJobRun</code> method is the heart of the worker coordination, requiring careful atomic operations.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/queue/queue.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> queue</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">container/heap</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HybridQueue</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store </span><span style=\"color:#B392F0\">Store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    heap  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PriorityQueue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DequeueJobRun atomically assigns the highest-priority eligible job to a worker.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">q </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DequeueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">workerID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    q.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> q.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if the in-memory heap is empty. If so, return a sentinel error (e.g., ErrNoJobs).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Pop the highest-priority Item from the heap using heap.Pop(q.heap).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use the Item.JobRunID to fetch the full JobRun from the persistent store (q.store.GetJobRun).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate the job is still in a PENDING state. If not (e.g., already taken by another worker), discard this item and loop back to step 1 (or return error).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update the JobRun in the store: set Status = STATUS_RUNNING, AssignedWorker = workerID, StartedAt = time.Now().</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Persist the updated JobRun using q.store.UpdateJobRun.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return the updated JobRun to the caller (the worker).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Wrap steps 3-6 in a database transaction if your store supports it to ensure atomicity.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-go\">E. Language-Specific Hints (Go)</h4>\n<ul>\n<li><strong>Atomic Updates:</strong> Use <code>database/sql</code> transactions (<code>BEGIN</code>, <code>COMMIT</code>) in the <code>Store</code> implementation to ensure the job state transition from <code>PENDING</code> to <code>RUNNING</code> is atomic.</li>\n<li><strong>Heap Management:</strong> Implement <code>heap.Interface</code> (<code>Len</code>, <code>Less</code>, <code>Swap</code>, <code>Push</code>, <code>Pop</code>) on your <code>PriorityQueue</code> type. The <code>Less</code> method should define priority ordering: higher <code>Priority</code> first, with <code>CreatedAt</code> as a tie-breaker for FIFO behavior.</li>\n<li><strong>Graceful Shutdown:</strong> Implement a context cancellation mechanism in <code>DequeueJobRun</code> so worker goroutines can be shut down cleanly when the application stops.</li>\n<li><strong>Log Streaming:</strong> In your <code>SSEHandler</code>, after registering the client, run a loop that reads from <code>client.Send</code> and writes to the <code>http.ResponseWriter</code>. Remember to set the correct SSE headers (<code>Content-Type: text/event-stream</code>, <code>Cache-Control: no-cache</code>).</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint-milestone-3-amp-4-integration\">F. Milestone Checkpoint (Milestone 3 &amp; 4 Integration)</h4>\n<p>After implementing the queue and broadcaster, verify the happy path works end-to-end.</p>\n<ol>\n<li><strong>Start the system:</strong> <code>go run cmd/server/main.go</code></li>\n<li><strong>Simulate a webhook:</strong> Use <code>curl</code> to send a mock GitHub push payload to <code>http://localhost:8080/webhooks/github</code>. Include a valid signature header if you have verification enabled.</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">    curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/webhooks/github</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      -H</span><span style=\"color:#9ECBFF\"> \"X-GitHub-Event: push\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      -d</span><span style=\"color:#9ECBFF\"> '{\"ref\":\"refs/heads/main\",\"repository\":{\"full_name\":\"test/repo\"},\"head_commit\":{\"id\":\"abc123\"}}'</span></span></code></pre></div>\n<ol start=\"3\">\n<li><strong>Observe the dashboard:</strong> Open <code>http://localhost:8080</code> in your browser. You should see a new pipeline run appear in the list within a few seconds.</li>\n<li><strong>Check worker activity:</strong> If you have a separate worker process, start it (<code>go run cmd/worker/main.go</code>). Observe logs showing it dequeuing and executing jobs.</li>\n<li><strong>Watch real-time logs:</strong> Click on the new pipeline run in the dashboard. Navigate to the logs tab. As the job executes (runs <code>echo &quot;Hello&quot;</code> in a container), you should see log lines appear in the browser in real-time, without needing to refresh.</li>\n<li><strong>Verify completion:</strong> The pipeline run should eventually reach a terminal status (<code>Succeeded</code> or <code>Failed</code>). Artifacts, if defined, should be listed on the run detail page.</li>\n</ol>\n<p><strong>Signs of Trouble:</strong></p>\n<ul>\n<li><strong>Webhook ignored:</strong> Check server logs for signature verification errors or parsing failures.</li>\n<li><strong>Jobs stuck in &quot;Pending&quot;:</strong> Verify the worker is running and connected to the same queue/store. Check for deadlocks in the <code>DequeueJobRun</code> logic.</li>\n<li><strong>Logs not streaming:</strong> Ensure the <code>LogBroadcaster.Broadcast</code> is being called from <code>StreamLogs</code> in the executor. Use browser developer tools to check the SSE connection status and any JavaScript errors.</li>\n</ul>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>In any automated system, things will fail. The true test of a CI/CD system&#39;s robustness lies not in preventing failures (which is impossible), but in how it detects, recovers from, and communicates about those failures. This section catalogs the potential failure modes across our CI/CD Pipeline Orchestrator components and defines comprehensive strategies for recovery and user-facing error reporting.</p>\n<p><strong>Mental Model: The Factory Incident Response Team</strong><br>Imagine our CI/CD system as an automated factory. When something goes wrong—a conveyor belt jams, a robotic arm malfunctions, or a parts delivery is delayed—the factory doesn&#39;t just shut down. Instead, an incident response team springs into action: they <strong>detect</strong> the problem (alarms, sensors), <strong>contain</strong> it (isolate the affected area), <strong>recover</strong> (repair or work around), and <strong>communicate</strong> (alert supervisors, update status boards). This section defines our digital incident response team: the patterns and mechanisms that keep the factory running even when individual components fail.</p>\n<h3 id=\"categorized-failure-modes\">Categorized Failure Modes</h3>\n<p>Failures can originate from various sources: malformed user input, infrastructure issues, network problems, or bugs in our own code. Categorizing them helps us apply appropriate handling strategies. The following tables detail failure modes across our four main component categories, their detection mechanisms, and immediate mitigation actions.</p>\n<h4 id=\"parsing-and-configuration-failures\">Parsing and Configuration Failures</h4>\n<p>These occur during the interpretation of pipeline configuration files, environment variable resolution, and dependency graph construction.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Immediate Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Malformed YAML syntax</strong></td>\n<td><code>ParseConfig()</code> returns a YAML parsing error with line/column information</td>\n<td>Return error immediately without proceeding; log with context (file path, problematic line)</td>\n</tr>\n<tr>\n<td><strong>Missing required fields</strong> (e.g., <code>JobConfig</code> without <code>Steps</code>)</td>\n<td>Validation step after unmarshaling checks each required field</td>\n<td>Return validation error with specific field name; suggest correction in error message</td>\n</tr>\n<tr>\n<td><strong>Circular job dependencies</strong></td>\n<td><code>BuildExecutionGraph()</code> detects cycles during topological sort</td>\n<td>Return error listing the circular chain (e.g., &quot;Job A → Job B → Job A&quot;); reject entire pipeline</td>\n</tr>\n<tr>\n<td><strong>Invalid matrix axis combinations</strong></td>\n<td><code>ExpandMatrix()</code> validates each axis value type and compatibility</td>\n<td>Return error specifying invalid axis and value; skip expansion for that job</td>\n</tr>\n<tr>\n<td><strong>Environment variable substitution errors</strong></td>\n<td><code>ResolveEnvVars()</code> detects undefined variables when strict mode enabled</td>\n<td>Option 1 (default): Leave unresolved <code>${VAR}</code> as literal. Option 2 (strict): Return error listing undefined variables</td>\n</tr>\n<tr>\n<td><strong>Invalid conditional expressions</strong> in <code>If</code> fields</td>\n<td>Syntax validation of <code>If</code> expression during parsing</td>\n<td>Return error with expression and parsing failure; mark step/job as <code>STATUS_SKIPPED</code> with error reason</td>\n</tr>\n<tr>\n<td><strong>Combinatorial explosion</strong> from large matrix</td>\n<td><code>ExpandMatrix()</code> computes total combinations and compares against configurable limit (e.g., 100)</td>\n<td>Return error exceeding limit; suggest reducing matrix dimensions or increasing limit</td>\n</tr>\n<tr>\n<td><strong>Invalid glob patterns</strong> for artifacts</td>\n<td>Pattern syntax validation during configuration parsing</td>\n<td>Return error with invalid pattern; continue without artifact collection for that pattern</td>\n</tr>\n</tbody></table>\n<h4 id=\"execution-engine-failures\">Execution Engine Failures</h4>\n<p>These occur during container lifecycle management, command execution, and artifact collection.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Immediate Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Docker daemon unavailable</strong></td>\n<td>Docker client connection fails during <code>ExecuteJob()</code></td>\n<td>Mark job as <code>STATUS_FAILED</code> with &quot;Infrastructure unavailable&quot; error; implement retry at queue level</td>\n</tr>\n<tr>\n<td><strong>Container image pull failure</strong> (network, authentication, missing image)</td>\n<td>Docker image pull returns non-zero exit code or timeout</td>\n<td>Mark job as <code>STATUS_FAILED</code> with specific pull error; cache error to avoid repeated pulls for same job</td>\n</tr>\n<tr>\n<td><strong>Container startup failure</strong> (insufficient resources, port conflicts)</td>\n<td>Container create/start returns error from Docker API</td>\n<td>Mark job as <code>STATUS_FAILED</code> with startup error; ensure cleanup of partially created containers</td>\n</tr>\n<tr>\n<td><strong>Step command non-zero exit code</strong></td>\n<td>Command execution returns exit code ≠ 0</td>\n<td>Stop job execution immediately; mark job as <code>STATUS_FAILED</code>; continue to artifact collection if configured</td>\n</tr>\n<tr>\n<td><strong>Step timeout exceeded</strong></td>\n<td>Per-step or per-job timeout timer fires</td>\n<td>Kill container process; mark step as failed with timeout error; proceed to job cleanup</td>\n</tr>\n<tr>\n<td><strong>Log stream blocking/disconnection</strong></td>\n<td><code>StreamLogs()</code> detects write failures or broken pipe</td>\n<td>Close log stream; continue job execution but log warning; store partial logs up to disconnection point</td>\n</tr>\n<tr>\n<td><strong>Artifact collection failures</strong> (file not found, permission denied)</td>\n<td><code>CollectArtifacts()</code> returns error for specific patterns</td>\n<td>Log warning for failed patterns; continue collecting other patterns; store partial artifact list</td>\n</tr>\n<tr>\n<td><strong>Container cleanup failure</strong> (zombie containers)</td>\n<td>Periodic health check scans for containers older than max job duration</td>\n<td>Force-kill containers; remove with Docker API; log cleanup event for audit</td>\n</tr>\n<tr>\n<td><strong>Resource exhaustion</strong> (memory, disk, containers)</td>\n<td>Docker daemon returns resource errors; system monitoring</td>\n<td>Reject new jobs with &quot;Resource unavailable&quot;; implement backpressure to queue</td>\n</tr>\n</tbody></table>\n<h4 id=\"queue-and-webhook-system-failures\">Queue and Webhook System Failures</h4>\n<p>These occur during webhook processing, queue operations, and worker coordination.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Immediate Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Invalid webhook signature/token</strong></td>\n<td><code>VerifyGitHubSignature()</code> or <code>VerifyGitLabToken()</code> returns validation error</td>\n<td>Respond with HTTP 401 Unauthorized; log attempt with source IP for security monitoring</td>\n</tr>\n<tr>\n<td><strong>Malformed webhook payload</strong></td>\n<td><code>ExtractEventDetails()</code> fails to parse required fields (SHA, branch, event type)</td>\n<td>Respond with HTTP 400 Bad Request; log payload snippet for debugging</td>\n</tr>\n<tr>\n<td><strong>Webhook replay attack</strong> (duplicate delivery ID)</td>\n<td>Track webhook delivery IDs in short-term cache (5 minutes)</td>\n<td>Ignore duplicate; respond with HTTP 200 OK but skip processing</td>\n</tr>\n<tr>\n<td><strong>Queue persistence failure</strong></td>\n<td><code>EnqueueJobRun()</code> returns error when writing to database</td>\n<td>Retry with exponential backoff (3 attempts); if all fail, respond with HTTP 503 Service Unavailable</td>\n</tr>\n<tr>\n<td><strong>Worker crash during job execution</strong></td>\n<td>Heartbeat timeout: worker fails to send periodic &quot;alive&quot; signal</td>\n<td>Re-queue job with original priority; mark original <code>JobRun</code> as orphaned; spawn new worker if needed</td>\n</tr>\n<tr>\n<td><strong>Queue starvation</strong> (low-priority jobs never run)</td>\n<td>Monitor queue age metrics; alert when low-priority jobs exceed age threshold</td>\n<td>Temporarily elevate priority of oldest jobs; implement fair scheduling algorithm</td>\n</tr>\n<tr>\n<td><strong>Lost jobs on system crash</strong> (in-memory heap only)</td>\n<td><code>recoverPendingJobs()</code> on startup rebuilds heap from persistent storage</td>\n<td>Design: Use hybrid queue with persistence; recovery scans for <code>STATUS_PENDING</code> jobs in database</td>\n</tr>\n<tr>\n<td><strong>Concurrent dequeue race condition</strong></td>\n<td>Database optimistic locking or <code>SELECT FOR UPDATE</code> in <code>DequeueJobRun()</code></td>\n<td>Retry dequeue on version conflict; ensure exactly-once delivery through atomic operations</td>\n</tr>\n</tbody></table>\n<h4 id=\"dashboard-and-network-failures\">Dashboard and Network Failures</h4>\n<p>These occur during real-time streaming, data retrieval, and user interface interactions.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Immediate Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>SSE/WebSocket connection drop</strong></td>\n<td><code>Broadcast()</code> detects closed client channel</td>\n<td><code>UnregisterClient()</code> automatically; log disconnection; client can reconnect</td>\n</tr>\n<tr>\n<td><strong>Log backpressure</strong> (client slower than log production)</td>\n<td>Channel send with timeout; buffer fills</td>\n<td>Drop oldest log lines when buffer exceeds limit; log warning about backpressure</td>\n</tr>\n<tr>\n<td><strong>Database query timeout</strong></td>\n<td>Context timeout in <code>ListPipelineRuns()</code> or <code>GetRunDetail()</code></td>\n<td>Return HTTP 504 Gateway Timeout; implement query pagination and indexes</td>\n</tr>\n<tr>\n<td><strong>SVG badge generation failure</strong></td>\n<td>Template rendering error in <code>GetBadge()</code></td>\n<td>Return fallback &quot;unknown&quot; status badge; log rendering error</td>\n</tr>\n<tr>\n<td><strong>CORS errors</strong> (dashboard hosted separately)</td>\n<td>Browser console errors; preflight request failures</td>\n<td>Configure appropriate CORS headers in dashboard endpoints</td>\n</tr>\n<tr>\n<td><strong>Large log file rendering freeze</strong></td>\n<td>Browser memory exhaustion when loading multi-MB logs</td>\n<td>Implement client-side virtual scrolling; server-side log truncation for initial load</td>\n</tr>\n<tr>\n<td><strong>Real-time log staleness</strong> (logs stop updating)</td>\n<td>Client-side timer detects no new events for &gt;30 seconds</td>\n<td>Auto-reconnect logic in JavaScript; server-side connection health checks</td>\n</tr>\n</tbody></table>\n<h3 id=\"recovery-and-retry-strategy\">Recovery and Retry Strategy</h3>\n<p>Not all failures are equal. <strong>Transient failures</strong> (network blips, temporary resource constraints) often resolve themselves and warrant retries. <strong>Persistent failures</strong> (buggy user code, configuration errors) require different handling. Our strategy distinguishes between these and defines clear recovery paths.</p>\n<h4 id=\"retry-policies-for-transient-failures\">Retry Policies for Transient Failures</h4>\n<p>We implement a layered retry strategy with increasing delays to avoid overwhelming recovering systems:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Category</th>\n<th>Retryable?</th>\n<th>Retry Policy</th>\n<th>Max Attempts</th>\n<th>Backoff Strategy</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Docker image pull failure</td>\n<td>Yes (network timeouts)</td>\n<td>Exponential backoff</td>\n<td>3</td>\n<td>2s, 4s, 8s</td>\n<td>Reset after successful pull of same image</td>\n</tr>\n<tr>\n<td>Webhook processing (queue full)</td>\n<td>Yes</td>\n<td>Immediate retry with jitter</td>\n<td>5</td>\n<td>100ms ± 50ms jitter</td>\n<td>After max attempts, respond with HTTP 503</td>\n</tr>\n<tr>\n<td>Worker dequeue (temporary DB issue)</td>\n<td>Yes</td>\n<td>Linear backoff</td>\n<td>10</td>\n<td>1s increments</td>\n<td>Worker sleeps between attempts; continues other jobs</td>\n</tr>\n<tr>\n<td>Artifact upload (network)</td>\n<td>Yes</td>\n<td>Exponential backoff</td>\n<td>4</td>\n<td>1s, 2s, 4s, 8s</td>\n<td>Store artifacts locally first, async upload</td>\n</tr>\n<tr>\n<td>Step execution (timeout due to load)</td>\n<td>Conditional</td>\n<td>Configurable per step</td>\n<td>1 (default)</td>\n<td>N/A</td>\n<td>Only retry if <code>retry_on_timeout: true</code> in config</td>\n</tr>\n<tr>\n<td>SSE connection drop</td>\n<td>Yes (client-side)</td>\n<td>Exponential backoff</td>\n<td>∞</td>\n<td>1s, 2s, 4s, 8s, 16s, 30s (cap)</td>\n<td>Implemented in browser JavaScript</td>\n</tr>\n</tbody></table>\n<p><strong>Mental Model: The Gradual Knock</strong><br>Imagine knocking on a door when someone might be temporarily unavailable. You don&#39;t pound relentlessly—you knock, wait a moment, knock again a bit louder, wait longer, and eventually give up. Our retry policies follow this intuition: gradually increasing delays prevent overwhelming a recovering service while persistently trying to complete the operation.</p>\n<h4 id=\"cleanup-procedures-for-stuck-resources\">Cleanup Procedures for Stuck Resources</h4>\n<p>Some failures leave behind resources that need cleanup to prevent resource leaks and system degradation:</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Detection Method</th>\n<th>Cleanup Procedure</th>\n<th>Trigger Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Orphaned containers</strong> (no corresponding active <code>JobRun</code>)</td>\n<td>Periodic scan: Docker containers with CI labels older than max job duration</td>\n<td>Force stop (<code>docker stop</code>), then remove (<code>docker rm</code>)</td>\n<td>Every 5 minutes</td>\n</tr>\n<tr>\n<td><strong>Stuck <code>STATUS_RUNNING</code> jobs</strong> (no heartbeat updates)</td>\n<td>Worker heartbeat timeout (e.g., 5 minutes no update)</td>\n<td>Mark job as <code>STATUS_FAILED</code> with &quot;timeout&quot;; trigger container cleanup</td>\n<td>On heartbeat check (every 30 seconds)</td>\n</tr>\n<tr>\n<td><strong>Old pipeline run artifacts</strong></td>\n<td>Scan artifact storage for runs older than retention period (default 30 days)</td>\n<td>Delete files from storage; update database records</td>\n<td>Daily cron job</td>\n</tr>\n<tr>\n<td><strong>Expired webhook delivery ID cache</strong></td>\n<td>TTL-based cache eviction</td>\n<td>Automatic removal from cache</td>\n<td>On cache insert with TTL</td>\n</tr>\n<tr>\n<td><strong>Dead worker entries</strong> in worker registry</td>\n<td>Worker heartbeat timeout</td>\n<td>Remove from registry; reassign any jobs assigned to that worker</td>\n<td>On heartbeat check</td>\n</tr>\n<tr>\n<td><strong>Partial queue items</strong> (persisted but not in heap)</td>\n<td><code>recoverPendingJobs()</code> on startup</td>\n<td>Re-add to in-memory heap; log recovery count</td>\n<td>System startup</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Retry at Queue Level vs. Execution Level</strong></p>\n<ul>\n<li><strong>Context</strong>: When a job fails due to transient infrastructure issues, we need to decide where retry logic resides.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Execution-level retry</strong>: Each step/job retries itself immediately within <code>ExecuteJob()</code>.</li>\n<li><strong>Queue-level retry</strong>: Failed jobs return to the queue with incremented attempt count.</li>\n<li><strong>Hybrid approach</strong>: Transient failures (network) retry at execution level; persistent failures (test failures) don&#39;t retry.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Queue-level retry for infrastructure failures only.</li>\n<li><strong>Rationale</strong>: Queue retry maintains proper priority ordering and respects worker concurrency limits. Immediate execution retry could bypass queue fairness and cause cascading failures. We distinguish failure types: exit code ≠ 0 (user error) never retries; Docker daemon errors (infrastructure) retry via queue.</li>\n<li><strong>Consequences</strong>: Simpler execution engine logic, but requires accurate failure classification. May delay retry due to queue position.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Execution-level retry</td>\n<td>Faster retry, no queue overhead</td>\n<td>Bypasses priority, may overload recovering system</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Queue-level retry</td>\n<td>Maintains fairness, respects limits</td>\n<td>Slower retry, more complex failure classification</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Hybrid approach</td>\n<td>Optimized for different failure types</td>\n<td>Most complex implementation, hard to debug</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"user-facing-error-reporting\">User-Facing Error Reporting</h3>\n<p>When failures occur, developers need clear, actionable information to understand what went wrong and how to fix it. Our error reporting strategy spans multiple channels with consistent information.</p>\n<h4 id=\"dashboard-error-display\">Dashboard Error Display</h4>\n<p>The web dashboard surfaces errors through structured UI components:</p>\n<table>\n<thead>\n<tr>\n<th>Error Context</th>\n<th>Dashboard Presentation</th>\n<th>Example Content</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Pipeline configuration error</strong></td>\n<td>Red banner at top of pipeline run view with parse error details</td>\n<td>&quot;YAML parse error at line 12, column 5: unexpected &#39;:&#39;&quot; with link to config file</td>\n</tr>\n<tr>\n<td><strong>Job failure</strong></td>\n<td>Job node turns red in DAG; click reveals failure reason and step details</td>\n<td>&quot;Job &#39;test&#39; failed at step &#39;run-tests&#39;: Exit code 1&quot; with collapsed log snippet</td>\n</tr>\n<tr>\n<td><strong>Step failure</strong></td>\n<td>Expanded step view shows exit code, duration, and error output highlighted</td>\n<td>&quot;Command &#39;npm test&#39; failed with exit code 1. Output: [last 10 lines of stderr]&quot;</td>\n</tr>\n<tr>\n<td><strong>Infrastructure error</strong></td>\n<td>Orange warning icon with generic message to avoid exposing internals</td>\n<td>&quot;Infrastructure issue: Job could not start. Our team has been notified.&quot;</td>\n</tr>\n<tr>\n<td><strong>Real-time streaming failure</strong></td>\n<td>Connection status indicator in log viewer; auto-reconnect countdown</td>\n<td>&quot;Disconnected from log stream. Reconnecting in 5...4...3...&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"structured-error-logging\">Structured Error Logging</h4>\n<p>All components log errors with consistent structured fields for debugging:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">log</span><pre class=\"arch-pre shiki-highlighted\"><code>[ERROR] 2023-10-05T14:30:22Z component=execution_engine job_run_id=job_abc123 \nerror=&quot;container start failed&quot; error_type=infrastructure \ndetail=&quot;Docker API error: port 8080 already allocated&quot; \ncontainer_id=container_xyz recovery_action=requeue attempt=2/3</code></pre></div>\n\n<p>Key logging principles:</p>\n<ol>\n<li><strong>Structured fields</strong>: Machine-parsable key-value pairs for aggregation</li>\n<li><strong>Error categorization</strong>: <code>error_type</code> = {<code>user_error</code>, <code>infrastructure</code>, <code>system_bug</code>}</li>\n<li><strong>Correlation IDs</strong>: Include <code>pipeline_run_id</code>, <code>job_run_id</code> in all related logs</li>\n<li><strong>Sensitive data redaction</strong>: Never log secrets or environment variables in plain text</li>\n<li><strong>Recovery actions logged</strong>: Document what the system did in response</li>\n</ol>\n<h4 id=\"build-status-badges\">Build Status Badges</h4>\n<p>Even in failure states, badges provide immediate visual feedback:</p>\n<table>\n<thead>\n<tr>\n<th>Status</th>\n<th>Badge Color</th>\n<th>Text</th>\n<th>Hover Tooltip</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STATUS_FAILED</code> (user error)</td>\n<td>Bright red</td>\n<td>failed</td>\n<td>&quot;Tests failed: click for details&quot;</td>\n</tr>\n<tr>\n<td><code>STATUS_FAILED</code> (infrastructure)</td>\n<td>Orange</td>\n<td>error</td>\n<td>&quot;System error: retrying&quot;</td>\n</tr>\n<tr>\n<td><code>STATUS_CANCELLED</code></td>\n<td>Grey</td>\n<td>cancelled</td>\n<td>&quot;Manually cancelled&quot;</td>\n</tr>\n<tr>\n<td><code>STATUS_SKIPPED</code></td>\n<td>Light grey</td>\n<td>skipped</td>\n<td>&quot;Skipped due to condition&quot;</td>\n</tr>\n</tbody></table>\n<p>Badges include a timestamp to indicate freshness and prevent caching of stale status.</p>\n<h4 id=\"error-communication-guidelines\">Error Communication Guidelines</h4>\n<blockquote>\n<p><strong>Design Principle: The Three Layers of Error Information</strong></p>\n<ol>\n<li><strong>User Actionable</strong>: What the developer should do (e.g., &quot;Fix syntax in .ci.yml line 12&quot;)</li>\n<li><strong>Debug Context</strong>: Technical details for investigation (e.g., &quot;Exit code 127: command not found&quot;)</li>\n<li><strong>System Internal</strong>: Infrastructure details for administrators (e.g., &quot;Docker daemon OOM kill&quot;)</li>\n</ol>\n</blockquote>\n<p>We follow these communication patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Audience</th>\n<th>Information Level</th>\n<th>Delivery Channel</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Developer</strong></td>\n<td>User-actionable + limited debug context</td>\n<td>Dashboard UI, badge status, PR comments (future)</td>\n</tr>\n<tr>\n<td><strong>System Admin</strong></td>\n<td>Debug context + system internal</td>\n<td>Structured logs, monitoring alerts</td>\n</tr>\n<tr>\n<td><strong>Support Team</strong></td>\n<td>All three layers</td>\n<td>Centralized log aggregation with correlation IDs</td>\n</tr>\n</tbody></table>\n<p><strong>Common Reporting Anti-Patterns to Avoid</strong>:</p>\n<ul>\n<li>⚠️ <strong>Exposing internal infrastructure details</strong> to end-users (security risk)</li>\n<li>⚠️ <strong>Generic error messages</strong> like &quot;Something went wrong&quot; (not actionable)</li>\n<li>⚠️ <strong>Logging secrets</strong> in error messages (security breach)</li>\n<li>⚠️ <strong>Silent failures</strong> where jobs disappear without trace (debugging nightmare)</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error categorization</td>\n<td>String constants in code</td>\n<td>Structured error types with Go 1.13+ <code>errors.As/Is</code></td>\n</tr>\n<tr>\n<td>Retry logic</td>\n<td>Simple for-loop with <code>time.Sleep</code></td>\n<td><code>github.com/cenkalti/backoff/v4</code> exponential backoff</td>\n</tr>\n<tr>\n<td>Structured logging</td>\n<td><code>log.Printf</code> with fmt strings</td>\n<td><code>github.com/sirupsen/logrus</code> or <code>go.uber.org/zap</code></td>\n</tr>\n<tr>\n<td>Health checks</td>\n<td>Basic endpoint returning 200 OK</td>\n<td>Comprehensive checks (DB, Docker, queue depth)</td>\n</tr>\n<tr>\n<td>Metrics collection</td>\n<td>Count errors in log analysis</td>\n<td>Prometheus counters and histograms</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    errors/                    # Error types and categorization\n      types.go                 # Error constants and types\n      categorization.go        # Error classification logic\n    recovery/                  # Retry and cleanup logic\n      retry.go                 # Retry policies with backoff\n      cleanup.go               # Stuck resource cleanup\n    logging/                   # Structured logging setup\n      structured.go            # Logger configuration\n      redaction.go             # Secret redaction in logs\n  cmd/\n    cleanup-agent/             # Optional: separate cleanup daemon\n      main.go</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Complete structured logging wrapper</strong> (ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/logging/structured.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ErrorType categorizes errors for appropriate handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ErrorType</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeUser</span><span style=\"color:#B392F0\">      ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"user_error\"</span><span style=\"color:#6A737D\">      // Malformed config, test failures</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeInfra</span><span style=\"color:#B392F0\">     ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"infrastructure\"</span><span style=\"color:#6A737D\">  // Docker, network, disk issues</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeSystem</span><span style=\"color:#B392F0\">    ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"system_bug\"</span><span style=\"color:#6A737D\">      // Bugs in our code</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogEntry represents a structured log entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Level       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"component\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"error,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorType   </span><span style=\"color:#B392F0\">ErrorType</span><span style=\"color:#9ECBFF\">              `json:\"error_type,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Fields      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"fields\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Caller      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"caller,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Logger provides structured logging methods</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLogger creates a new logger for a component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">{component: component}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error logs an error with structured fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">errorType</span><span style=\"color:#B392F0\"> ErrorType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Level:     </span><span style=\"color:#9ECBFF\">\"ERROR\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component: l.component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message:   err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Error:     err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ErrorType: errorType,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Fields:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add caller information (file:line)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, file, line, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> runtime.</span><span style=\"color:#B392F0\">Caller</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Shorten file path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Split</span><span style=\"color:#E1E4E8\">(file, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(parts) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            file </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(parts[</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(parts)</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">:], </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry.Caller </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, file, line)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Copy fields, redacting secrets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> fields {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(strings.</span><span style=\"color:#B392F0\">ToLower</span><span style=\"color:#E1E4E8\">(k), </span><span style=\"color:#9ECBFF\">\"secret\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(strings.</span><span style=\"color:#B392F0\">ToLower</span><span style=\"color:#E1E4E8\">(k), </span><span style=\"color:#9ECBFF\">\"token\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(strings.</span><span style=\"color:#B392F0\">ToLower</span><span style=\"color:#E1E4E8\">(k), </span><span style=\"color:#9ECBFF\">\"password\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry.Fields[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"[REDACTED]\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry.Fields[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add correlation IDs from context if available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> runID, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"pipeline_run_id\"</span><span style=\"color:#E1E4E8\">).(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry.Fields[</span><span style=\"color:#9ECBFF\">\"pipeline_run_id\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> runID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> jobID, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"job_run_id\"</span><span style=\"color:#E1E4E8\">).(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry.Fields[</span><span style=\"color:#9ECBFF\">\"job_run_id\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> jobID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // In simple implementation, output as JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For production, integrate with logrus/zap</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">toJSON</span><span style=\"color:#E1E4E8\">(entry))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// toJSON is a simple JSON marshaller (use encoding/json in production)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> toJSON</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simplified for example</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        `{\"timestamp\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\",\"level\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\",\"component\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\",\"message\":\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"}`</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry.Timestamp.</span><span style=\"color:#B392F0\">Format</span><span style=\"color:#E1E4E8\">(time.RFC3339),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry.Level,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry.Component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        strings.</span><span style=\"color:#B392F0\">ReplaceAll</span><span style=\"color:#E1E4E8\">(entry.Message, </span><span style=\"color:#9ECBFF\">`\"`</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">`\\\"`</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Errorf logs a formatted error message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">errorType</span><span style=\"color:#B392F0\"> ErrorType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">args</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(format, args</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(ctx, err, errorType, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Error categorization helper</strong> (learner implements):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/errors/categorization.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CategorizeError determines the ErrorType from an error message and context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CategorizeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ErrorType</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if error is nil - return empty string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check error message for infrastructure patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Patterns indicating infrastructure: \"connection refused\", \"timeout\", </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // \"no space left\", \"port already in use\", \"image not found\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check component-specific categorization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parser errors are user_error, execution engine Docker errors are infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Default to system_bug if not clearly infrastructure or user_error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // (assuming our code is correct, but handle unknown patterns)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return the determined ErrorType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ErrorTypeSystem </span><span style=\"color:#6A737D\">// Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Retry with exponential backoff</strong> (learner implements):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/recovery/retry.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> recovery</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RetryWithBackoff executes f with exponential backoff until success or max attempts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> RetryWithBackoff</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    maxAttempts</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    initialDelay</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    f</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    shouldRetry</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize attempt counter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate delay with exponential backoff: delay = initialDelay * 2^(attempt-1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Loop while attempts &#x3C; maxAttempts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute the function f</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If no error, return success immediately</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Check if error should be retried using shouldRetry function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // If not retryable, return error immediately</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Wait for delay with context cancellation support</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use time.Sleep or better, select with context.Done()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Increment attempt counter and recalculate delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: After max attempts, return the last error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Stuck job detection and cleanup</strong> (learner implements):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/recovery/cleanup.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> recovery</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourproject/internal/store</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupStuckJobs finds and marks jobs stuck in RUNNING state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CleanupStuckJobs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">s</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Store</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanedCount </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate cutoff time: current time - timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Query database for jobs with Status = STATUS_RUNNING </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // AND StartedAt &#x3C; cutoff</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each stuck job:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Mark as STATUS_FAILED with reason \"timeout\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Log cleanup action with job ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Attempt to stop Docker container (if ContainerID exists)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Increment cleanedCount</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return count of cleaned jobs and any error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> cleanedCount, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Error wrapping</strong>: Use <code>fmt.Errorf(&quot;... %w&quot;, err)</code> in Go 1.13+ to wrap errors while preserving original error for <code>errors.Is/As</code></li>\n<li><strong>Context cancellation</strong>: Always check <code>ctx.Done()</code> in retry loops to allow graceful shutdown</li>\n<li><strong>Structured logging keys</strong>: Use consistent naming like <code>snake_case</code> for log fields</li>\n<li><strong>Docker client errors</strong>: Check for <code>errdefs.IsNotFound(err)</code> to distinguish missing images from other Docker errors</li>\n<li><strong>Timeout handling</strong>: Use <code>context.WithTimeout</code> for any external calls (DB, Docker API, network)</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing error handling, verify with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test error categorization</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/errors/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test retry logic with mock failing function</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/recovery/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Integration test: Trigger a pipeline with malformed YAML</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/webhooks/github</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"ref\": \"refs/heads/main\", \"repository\": {\"default_branch\": \"main\"}}'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 400 with clear error about missing fields</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check structured logs appear</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">tail</span><span style=\"color:#79B8FF\"> -f</span><span style=\"color:#9ECBFF\"> ci-server.log</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#9ECBFF\"> ERROR</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see structured JSON logs with component, error_type fields</span></span></code></pre></div>\n\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Jobs stuck in PENDING forever</strong></td>\n<td>Queue worker crashed or deadlock</td>\n<td>Check worker logs for panics; run <code>SELECT * FROM job_runs WHERE status = &#39;pending&#39;</code></td>\n<td>Restart workers; implement worker health checks</td>\n</tr>\n<tr>\n<td><strong>Logs stop streaming mid-job</strong></td>\n<td>SSE connection dropped due to network or backpressure</td>\n<td>Check browser console for errors; server logs for &quot;broken pipe&quot;</td>\n<td>Implement client reconnection; increase log buffer size</td>\n</tr>\n<tr>\n<td><strong>&quot;Infrastructure error&quot; with no details</strong></td>\n<td>Docker daemon down or resource exhausted</td>\n<td>Run <code>docker ps</code> and <code>docker info</code>; check system metrics</td>\n<td>Restart Docker; add resource limits; implement health checks</td>\n</tr>\n<tr>\n<td><strong>Webhooks ignored silently</strong></td>\n<td>Signature validation failing</td>\n<td>Check webhook logs for &quot;invalid signature&quot;; verify secret matches</td>\n<td>Update webhook secret in both Git provider and config</td>\n</tr>\n<tr>\n<td><strong>Artifacts missing after successful job</strong></td>\n<td>Globbing pattern doesn&#39;t match or permission issues</td>\n<td>Check job logs for artifact collection errors; inspect container filesystem</td>\n<td>Fix glob patterns; ensure artifacts are in workspace</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>Building a CI/CD system that reliably orchestrates complex workflows requires a robust testing strategy for the system <em>itself</em>. The core challenge lies in testing a distributed, stateful system that interacts with external services (Docker, Git hosts) and manages concurrent execution. This section outlines a pragmatic, multi-layered approach to verifying each component and their integration, ensuring the system behaves correctly under expected and unexpected conditions.</p>\n<p>A well-structured testing strategy not only catches bugs early but also serves as living documentation of the system&#39;s expected behavior. Given the educational nature of this project, the focus is on <strong>practical verification</strong> that a learner can execute after each milestone to gain confidence in their implementation.</p>\n<h3 id=\"testing-pyramid-for-a-ci-system\">Testing Pyramid for a CI System</h3>\n<p>The classic testing pyramid—emphasizing many unit tests, fewer integration tests, and even fewer end-to-end (E2E) tests—applies well to our CI orchestrator. However, the nature of a CI system, which heavily relies on external infrastructure (containers, queues, HTTP), shifts the weight more towards integration tests.</p>\n<blockquote>\n<p><strong>Mental Model: The Construction Site Inspection</strong>\nThink of testing the CI system like inspecting a construction site at different levels of detail. <strong>Unit tests</strong> are like checking individual tools (a hammer, a saw) to ensure they work correctly in isolation. <strong>Integration tests</strong> are like verifying that teams of workers (plumbers and electricians) can coordinate when the water and power are connected to the building. <strong>End-to-end tests</strong> are the final building inspection, where you flip the light switch and expect the bulb to illuminate, verifying the entire system from foundation to finish.</p>\n</blockquote>\n<p>The following table outlines the three testing tiers, their scope, tools, and what they validate for our CI system:</p>\n<table>\n<thead>\n<tr>\n<th>Test Tier</th>\n<th>Scope &amp; Responsibility</th>\n<th>Primary Tools/Mocks</th>\n<th>Key Verification Points</th>\n<th>Recommended Coverage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Unit Tests</strong></td>\n<td>Isolated logic of a single function, struct, or algorithm. No external dependencies.</td>\n<td>Standard library testing, <code>testify</code> for assertions, <code>gomock</code> for generating mocks.</td>\n<td>- YAML parsing into <code>PipelineConfig</code> <br> - Environment variable substitution logic <br> - Matrix expansion algorithm <br> - DAG dependency sorting <br> - Priority queue ordering <br> - Log event broadcasting logic</td>\n<td>High (70-80%). Focus on pure business logic.</td>\n</tr>\n<tr>\n<td><strong>Integration Tests</strong></td>\n<td>Interaction between two or more components, or with a lightweight version of an external service.</td>\n<td><code>testcontainers</code> for Docker, an in-memory SQL database, a real HTTP server for webhook testing.</td>\n<td>- <code>ParseConfig</code> + <code>BuildExecutionGraph</code> produces correct job order <br> - <code>ExecuteJob</code> successfully runs a command in a real container <br> - <code>HybridQueue</code> persists and recovers jobs <br> - Webhook handler validates signatures and creates <code>PipelineRun</code> <br> - Dashboard API returns correct run history</td>\n<td>Medium (20-25%). Critical for core workflows.</td>\n</tr>\n<tr>\n<td><strong>End-to-End (E2E) Tests</strong></td>\n<td>The entire system working together, from webhook to dashboard update, using production-like configurations.</td>\n<td>Full system binary, real Docker daemon, mock Git server (e.g., <code>gitea</code> in testcontainers), headless browser for UI.</td>\n<td>- A git push triggers a pipeline via webhook <br> - Jobs execute in parallel as defined <br> - Logs stream to the dashboard in real-time <br> - Artifacts are saved and downloadable <br> - The system recovers from a restart mid-pipeline</td>\n<td>Low (5-10%). Complex and slow, but essential for confidence.</td>\n</tr>\n</tbody></table>\n<p><strong>Testing External Dependencies:</strong> A key decision is how to handle tests for components that interact with Docker, a database, or a Git host.</p>\n<blockquote>\n<p><strong>Decision: Use Test Doubles for Unit Tests, Lightweight Real Services for Integration</strong></p>\n<ul>\n<li><strong>Context</strong>: Unit tests must be fast and deterministic. Spinning up a Docker daemon or a PostgreSQL instance is slow and flaky.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Mock everything</strong>: Use interfaces and generated mocks for all external interactions (Docker client, store, queue).</li>\n<li><strong>Use lightweight fakes</strong>: Implement in-memory fakes (e.g., a <code>FakeDockerClient</code> that records calls).</li>\n<li><strong>Use real services in containers</strong>: Leverage <code>testcontainers</code> to spin up real Docker, Redis, or Postgres for integration tests.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use <strong>Option 1 (mocks) for unit tests</strong> and <strong>Option 3 (testcontainers) for integration tests</strong>. Option 2 is avoided as it requires maintaining fake implementations that must mimic complex external behavior.</li>\n<li><strong>Rationale</strong>: Mocks keep unit tests fast and focused on the component&#39;s logic, not the external service&#39;s behavior. Using real services in integration tests provides the highest fidelity, catching issues like API incompatibility or connection handling. <code>testcontainers</code> provides a clean, programmatic way to manage these dependencies.</li>\n<li><strong>Consequences</strong>: Integration test suites will be slower and require a working Docker environment. This is an acceptable trade-off for the confidence gained.</li>\n</ul>\n</blockquote>\n<p><strong>Structured Logging for Testability:</strong> The system&#39;s use of structured logging with correlation IDs (as defined in the <code>LogEntry</code> type) is a powerful testing tool. During integration and E2E tests, logs can be captured and assertions can be made about the sequence and content of log messages, providing a trace of the system&#39;s internal state.</p>\n<h3 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h3>\n<p>After completing each milestone, learners should run a series of verification steps to confirm their implementation aligns with the design. These checkpoints mix automated tests, manual commands, and visual inspection.</p>\n<h4 id=\"milestone-1-pipeline-configuration-parser\">Milestone 1: Pipeline Configuration Parser</h4>\n<p>The goal is to verify that YAML configuration is correctly parsed, validated, and transformed into an executable plan.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Verification Method</th>\n<th>Steps to Execute</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1. Basic Parsing</strong></td>\n<td>Unit Test</td>\n<td>Run <code>go test ./internal/parser/... -v -run TestParseConfig</code></td>\n<td>All tests pass. A test parsing a simple pipeline YAML returns a <code>PipelineConfig</code> struct with correctly populated <code>Jobs</code> map and <code>Environment</code>.</td>\n</tr>\n<tr>\n<td><strong>2. Environment Variable Resolution</strong></td>\n<td>Unit Test / Manual CLI</td>\n<td>1. Write a test YAML with <code>command: &quot;echo $USER ${HOME}&quot;</code>. <br> 2. Call <code>ParseConfig</code> and then <code>ResolveEnvVars</code> with a provided env map. <br> 3. Inspect the resolved command in the resulting <code>StepConfig</code>.</td>\n<td>The <code>Command</code> field in the <code>StepConfig</code> has the variable references replaced with values from the provided map (e.g., <code>&quot;echo alice /home/alice&quot;</code>). System environment variables are also substituted when no override is provided.</td>\n</tr>\n<tr>\n<td><strong>3. Matrix Expansion</strong></td>\n<td>Unit Test</td>\n<td>Run a test for <code>ExpandMatrix</code> with a job containing <code>matrix: { go: [&quot;1.19&quot;, &quot;1.20&quot;], os: [&quot;linux&quot;] }</code>.</td>\n<td>The function returns a slice of <code>JobConfig</code> structs with length 2. Each job&#39;s <code>Env</code> map should contain <code>GO_VERSION=1.19</code> and <code>OS=linux</code> (or the equivalent for the second combination). The <code>Name</code> field should be updated to reflect the combination (e.g., <code>&quot;build (go=1.19, os=linux)&quot;</code>).</td>\n</tr>\n<tr>\n<td><strong>4. DAG Construction</strong></td>\n<td>Unit Test &amp; Visual Inspection</td>\n<td>1. Write a test pipeline with 3 jobs: <code>A</code>, <code>B</code> (needs: <code>A</code>), <code>C</code> (needs: <code>A</code>). <br> 2. Call <code>BuildExecutionGraph</code>. <br> 3. Assert the adjacency list: <code>A</code> has no dependencies, <code>B</code> and <code>C</code> depend on <code>A</code>. <br> 4. Print the topological order.</td>\n<td>The graph is constructed without cycles. A topological sort yields <code>[A, B, C]</code> (or <code>[A, C, B]</code>), confirming <code>A</code> must run before <code>B</code> and <code>C</code>, which can run in parallel.</td>\n</tr>\n<tr>\n<td><strong>5. Validation &amp; Error Cases</strong></td>\n<td>Unit Test</td>\n<td>Run tests for invalid YAML, circular dependencies, and invalid matrix axes.</td>\n<td>The parser returns descriptive, actionable error messages (e.g., <code>&quot;job &#39;B&#39; forms a circular dependency with job &#39;A&#39;&quot;</code>). Tests for invalid input pass (i.e., they expect an error).</td>\n</tr>\n</tbody></table>\n<p><strong>Integration Checkpoint:</strong> Create a small Go program that uses your parser package to read a file <code>.ci/pipeline.yaml</code> and print the execution plan (job order and expanded matrix jobs). Run it against a sample configuration. You should see a human-readable list of jobs and their dependencies.</p>\n<h4 id=\"milestone-2-job-execution-engine\">Milestone 2: Job Execution Engine</h4>\n<p>The goal is to verify that jobs can be run in isolated containers, with output capture and artifact collection.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Verification Method</th>\n<th>Steps to Execute</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1. Container Lifecycle</strong></td>\n<td>Integration Test</td>\n<td>Write a test that calls <code>ExecuteJob</code> for a job with a single step: <code>command: &quot;echo &#39;Hello World&#39;&quot;</code>. Use a simple image like <code>alpine:latest</code>.</td>\n<td>The test passes. The <code>JobRun</code> returned has <code>Status: STATUS_SUCCEEDED</code>. The <code>LogKey</code> is populated. The container is created, runs, and is removed (verified by checking <code>docker ps -a</code> after test completion).</td>\n</tr>\n<tr>\n<td><strong>2. Step Execution &amp; Exit Codes</strong></td>\n<td>Integration Test</td>\n<td>Test two jobs: one with <code>command: &quot;exit 0&quot;</code> and one with <code>command: &quot;exit 1&quot;</code>.</td>\n<td>The first job returns <code>STATUS_SUCCEEDED</code> with <code>ExitCode: 0</code>. The second job returns <code>STATUS_FAILED</code> with <code>ExitCode: 1</code>. The execution stops after the failing step.</td>\n</tr>\n<tr>\n<td><strong>3. Log Streaming</strong></td>\n<td>Integration Test / Manual</td>\n<td>In a test, call <code>StreamLogs</code> while a job is running (e.g., a job with <code>command: &quot;for i in 1 2 3; do echo $i; sleep 0.5; done&quot;</code>). Capture the output to a buffer.</td>\n<td>The buffer contains the lines <code>&quot;1&quot;</code>, <code>&quot;2&quot;</code>, <code>&quot;3&quot;</code> in real-time, as they are produced. The output is captured via stdout/stderr.</td>\n</tr>\n<tr>\n<td><strong>4. Environment Injection</strong></td>\n<td>Integration Test</td>\n<td>Execute a job with an <code>Env</code> map <code>{&quot;GREETING&quot;: &quot;Hello&quot;}</code> and a step command <code>&quot;echo $GREETING&quot;</code>. Capture the logs.</td>\n<td>The log output contains the line <code>&quot;Hello&quot;</code>. The environment variable was successfully injected into the container.</td>\n</tr>\n<tr>\n<td><strong>5. Artifact Collection</strong></td>\n<td>Integration Test</td>\n<td>Run a job that creates a file (<code>touch output.txt</code>) and specify artifact patterns <code>[&quot;output.txt&quot;, &quot;*.txt&quot;]</code>. Call <code>CollectArtifacts</code> after job completion.</td>\n<td>The function returns a list of artifact keys (e.g., file paths in persistent storage). The specified file is copied from the container&#39;s workspace and can be retrieved from the storage layer.</td>\n</tr>\n<tr>\n<td><strong>6. Parallel Execution</strong></td>\n<td>Integration Test / Manual</td>\n<td>Use the <code>WorkerPool</code> to enqueue two independent jobs that each run for 2 seconds. Measure total execution time.</td>\n<td>Both jobs start nearly simultaneously. The total time is just over 2 seconds (not 4 seconds), confirming parallel execution.</td>\n</tr>\n</tbody></table>\n<p><strong>Integration Checkpoint:</strong> Start a standalone worker process, and manually enqueue a <code>JobRun</code> (by inserting directly into the queue/store). Observe the worker picks it up, runs the container, and updates the status. Check the database: the <code>JobRun</code> record should progress from <code>STATUS_PENDING</code> to <code>STATUS_RUNNING</code> to <code>STATUS_SUCCEEDED</code>.</p>\n<h4 id=\"milestone-3-webhook-amp-queue-system\">Milestone 3: Webhook &amp; Queue System</h4>\n<p>The goal is to verify that the system correctly receives events, queues work, and coordinates workers.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Verification Method</th>\n<th>Steps to Execute</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1. Webhook Signature Verification</strong></td>\n<td>Unit Test</td>\n<td>Use the <code>WebhookVerifier</code> to test <code>VerifyGitHubSignature</code> with a known secret, payload, and signature. Test both valid and invalid signatures.</td>\n<td>The valid signature passes (no error). The invalid signature returns a clear error (e.g., <code>&quot;invalid signature&quot;</code>).</td>\n</tr>\n<tr>\n<td><strong>2. Webhook Handling &amp; Pipeline Creation</strong></td>\n<td>Integration Test</td>\n<td>Start the webhook listener HTTP server. Use <code>curl</code> to send a mock GitHub push event payload (with correct signature).</td>\n<td>The server responds with <code>202 Accepted</code>. A new <code>PipelineRun</code> is created in the store with <code>Status: STATUS_PENDING</code>. The <code>Trigger</code>, <code>CommitSHA</code>, and <code>Branch</code> fields are correctly extracted from the payload.</td>\n</tr>\n<tr>\n<td><strong>3. Queue Persistence</strong></td>\n<td>Integration Test</td>\n<td>1. Enqueue a <code>PipelineRun</code>. <br> 2. Simulate a system crash (kill the process). <br> 3. Restart and call <code>recoverPendingJobs</code>.</td>\n<td>The pending <code>PipelineRun</code> (and its <code>JobRun</code>s) are reloaded into the in-memory queue and are available for dequeuing. No work is lost.</td>\n</tr>\n<tr>\n<td><strong>4. Worker Pool &amp; Job Assignment</strong></td>\n<td>Integration Test</td>\n<td>Start a worker pool with 2 workers. Enqueue 4 jobs. Observe the logs or database.</td>\n<td>Only 2 jobs are in <code>STATUS_RUNNING</code> at any given time. As workers finish, they pick up the next job. Each job is assigned a unique <code>AssignedWorker</code> ID.</td>\n</tr>\n<tr>\n<td><strong>5. Priority Scheduling</strong></td>\n<td>Unit Test / Integration Test</td>\n<td>Enqueue jobs with different <code>Priority</code> values (e.g., 10, 5, 1). Have a single worker dequeue jobs sequentially.</td>\n<td>The jobs are dequeued in order of highest priority (10 first, then 5, then 1), regardless of insertion order.</td>\n</tr>\n<tr>\n<td><strong>6. Rate Limiting</strong></td>\n<td>Integration Test / Manual</td>\n<td>Configure a low rate limit (e.g., 2 requests per second). Send 5 webhook requests in rapid succession via a script.</td>\n<td>The first 2 requests are processed immediately. The next 3 are delayed or rejected (depending on design). The system log indicates rate limiting is active.</td>\n</tr>\n</tbody></table>\n<p><strong>Integration Checkpoint:</strong> Set up a local repository with a webhook pointing to your listener. Push a commit. Observe the system: within seconds, a webhook should be received, a pipeline run created, jobs enqueued, and workers should begin processing. The dashboard should show the new run.</p>\n<h4 id=\"milestone-4-web-dashboard\">Milestone 4: Web Dashboard</h4>\n<p>The goal is to verify that the dashboard serves data, streams logs in real-time, and generates visualizations.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Verification Method</th>\n<th>Steps to Execute</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1. API Endpoints Return Data</strong></td>\n<td>Integration Test</td>\n<td>Start the dashboard server with a populated store. Use <code>curl</code> or a browser to hit <code>GET /api/runs</code>.</td>\n<td>The endpoint returns a JSON array of <code>PipelineRun</code> objects with correct status, branch, and commit data. Pagination (<code>limit</code>, <code>offset</code>) works.</td>\n</tr>\n<tr>\n<td><strong>2. Real-Time Log Streaming (SSE)</strong></td>\n<td>Manual / E2E Test</td>\n<td>1. Start a pipeline run that produces periodic output. <br> 2. Open <code>GET /api/runs/:id/logs</code> in a browser or use <code>curl -N</code>. <br> 3. Alternatively, use the dashboard UI.</td>\n<td>Log lines appear in the browser/console as they are produced by the job, with minimal delay (seconds). The connection remains open until the job finishes.</td>\n</tr>\n<tr>\n<td><strong>3. Badge Generation</strong></td>\n<td>Unit Test / Manual</td>\n<td>Call <code>GET /badge/myproject.svg</code> for a pipeline with status <code>STATUS_SUCCEEDED</code> and another with <code>STATUS_FAILED</code>.</td>\n<td>The endpoint returns an SVG image with <code>Content-Type: image/svg+xml</code>. The badge for the successful run is green (e.g., &quot;passing&quot;), and for the failed run is red (&quot;failing&quot;).</td>\n</tr>\n<tr>\n<td><strong>4. DAG Visualization Endpoint</strong></td>\n<td>Unit Test / Manual</td>\n<td>For a pipeline run with dependencies, call <code>GET /api/runs/:id/dag</code>.</td>\n<td>Returns a JSON representation of the DAG suitable for rendering (e.g., nodes and edges). The structure correctly reflects the <code>needs</code> dependencies from the pipeline config.</td>\n</tr>\n<tr>\n<td><strong>5. Artifact Listing &amp; Download</strong></td>\n<td>Integration Test</td>\n<td>After a run with artifacts, call <code>GET /api/runs/:id/artifacts</code>. Then download one artifact via the provided link.</td>\n<td>The list endpoint returns metadata for the artifacts. The download link serves the file with correct <code>Content-Disposition</code> header.</td>\n</tr>\n<tr>\n<td><strong>6. Frontend Integration</strong></td>\n<td>Manual E2E</td>\n<td>Start the full system. Open the dashboard in a browser (<code>localhost:8080</code>). Trigger a pipeline via webhook.</td>\n<td>The build list updates automatically or on refresh. Clicking on a run shows details, a live log stream, and the DAG visualization. All interactive elements work.</td>\n</tr>\n</tbody></table>\n<p><strong>Integration Checkpoint:</strong> Deploy the dashboard and have a colleague interact with it without prior explanation. They should be able to find the status of their latest commit, see live logs, and download build artifacts without assistance. This validates the UI/UX design.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (For Learning)</th>\n<th>Advanced Option (For Production)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>Go standard <code>testing</code> package, <code>testify/assert</code> for assertions.</td>\n<td><code>github.com/stretchr/testify</code> suite (assert, mock, require).</td>\n</tr>\n<tr>\n<td>Mock Generation</td>\n<td>Hand-written interface mocks.</td>\n<td><code>go.uber.org/mock</code> (formerly <code>gomock</code>) for auto-generated mocks.</td>\n</tr>\n<tr>\n<td>Integration Testing (Docker)</td>\n<td>Direct <code>exec</code> of <code>docker</code> CLI in tests.</td>\n<td><code>testcontainers/testcontainers-go</code> for managed, ephemeral containers.</td>\n</tr>\n<tr>\n<td>Integration Testing (HTTP)</td>\n<td>Go&#39;s <code>net/http/httptest</code> package.</td>\n<td><code>golang.org/x/net/http2/h2c</code> for HTTP/2, <code>github.com/gorilla/websocket</code> for WebSocket tests.</td>\n</tr>\n<tr>\n<td>E2E Testing</td>\n<td>Bash script orchestrating binary, Docker, and <code>curl</code>.</td>\n<td><code>github.com/cucumber/godog</code> for BDD, or <code>playwright-go</code> for browser automation.</td>\n</tr>\n<tr>\n<td>Code Coverage</td>\n<td><code>go test -cover</code></td>\n<td><code>go test -coverprofile=coverage.out &amp;&amp; go tool cover -html=coverage.out</code> for HTML report.</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ci-system/\n├── internal/\n│   ├── parser/                    # Milestone 1\n│   │   ├── parser.go              # ParseConfig, ResolveEnvVars, etc.\n│   │   ├── parser_test.go         # Unit tests\n│   │   ├── matrix.go              # ExpandMatrix\n│   │   ├── dag.go                 # BuildExecutionGraph\n│   │   └── integration_test.go    # Integration tests (if any)\n│   ├── executor/                  # Milestone 2\n│   │   ├── docker/\n│   │   │   └── client.go          # Wrapper around Docker SDK\n│   │   ├── executor.go            # ExecuteJob, StreamLogs\n│   │   ├── artifacts.go           # CollectArtifacts\n│   │   ├── executor_test.go       # Unit tests (with mocks)\n│   │   └── executor_integration_test.go # Needs Docker\n│   ├── queue/                     # Milestone 3\n│   │   ├── hybrid_queue.go        # HybridQueue implementation\n│   │   ├── priority_heap.go       # PriorityQueue (heap.Interface)\n│   │   ├── worker_pool.go         # Worker pool management\n│   │   ├── queue_test.go          # Unit tests for heap logic\n│   │   └── queue_integration_test.go # With real store\n│   ├── webhook/                   # Milestone 3\n│   │   ├── handler.go             # HandleWebhook, VerifyGitHubSignature\n│   │   ├── parser.go              # ExtractEventDetails\n│   │   └── handler_test.go        # Tests with httptest\n│   ├── dashboard/                 # Milestone 4\n│   │   ├── server.go              # DashboardServer, HTTP handlers\n│   │   ├── sse.go                 # LogBroadcaster, SSEHandler\n│   │   ├── badges.go              # GetBadge SVG generation\n│   │   ├── dag_viz.go             # GetDAG data generation\n│   │   └── server_test.go         # API endpoint tests\n│   └── store/                     # Data persistence layer\n│       ├── store.go               # Store interface\n│       ├── sqlite.go              # SQLite implementation\n│       └── store_test.go          # Store interface tests\n├── cmd/\n│   ├── server/                    # Main orchestrator binary\n│   │   └── main.go\n│   ├── worker/                    # Dedicated worker binary (optional)\n│   │   └── main.go\n│   └── dashboard/                 # Dashboard binary\n│       └── main.go\n├── test/                          # E2E and integration test utilities\n│   ├── e2e/\n│   │   └── pipeline_test.go       # Full system test\n│   └── fixtures/\n│       ├── configs/               # Sample pipeline YAMLs\n│       └── webhook_payloads/      # Sample GitHub/GitLab JSON\n└── scripts/\n    └── test-e2e.sh                # Script to run E2E tests</code></pre></div>\n\n<p><strong>Infrastructure Starter Code (Test Helper with Testcontainers):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/testhelpers/docker.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testhelpers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/testcontainers/testcontainers-go</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/testcontainers/testcontainers-go/wait</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CreatePostgresContainer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">testcontainers</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Container</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> testcontainers</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ContainerRequest</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Image:        </span><span style=\"color:#9ECBFF\">\"postgres:15-alpine\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ExposedPorts: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"5432/tcp\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Env: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"POSTGRES_DB\"</span><span style=\"color:#E1E4E8\">:       </span><span style=\"color:#9ECBFF\">\"testdb\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"POSTGRES_USER\"</span><span style=\"color:#E1E4E8\">:     </span><span style=\"color:#9ECBFF\">\"testuser\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"POSTGRES_PASSWORD\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"testpass\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WaitingFor: wait.</span><span style=\"color:#B392F0\">ForLog</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"database system is ready to accept connections\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pgContainer, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> testcontainers.</span><span style=\"color:#B392F0\">GenericContainer</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#B392F0\">testcontainers</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">GenericContainerRequest</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ContainerRequest: req,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Started:          </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Fatal</span><span style=\"color:#E1E4E8\">(err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    host, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> pgContainer.</span><span style=\"color:#B392F0\">Host</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Fatal</span><span style=\"color:#E1E4E8\">(err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    port, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> pgContainer.</span><span style=\"color:#B392F0\">MappedPort</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"5432\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Fatal</span><span style=\"color:#E1E4E8\">(err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dsn </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"postgres://testuser:testpass@</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/testdb?sslmode=disable\"</span><span style=\"color:#E1E4E8\">, host, port.</span><span style=\"color:#B392F0\">Port</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> pgContainer, dsn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code (Example: Unit Test for Parser):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/parser/parser_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/assert</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/require</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestParseConfig_ValidYAML</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    yamlContent </span><span style=\"color:#F97583\">:=</span><span style=\"color:#9ECBFF\"> `</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">jobs:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  build:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    runs-on: ubuntu-latest</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    steps:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">      - name: Echo</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        command: echo \"Hello\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> ParseConfig</span><span style=\"color:#E1E4E8\">(yamlContent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    require.</span><span style=\"color:#B392F0\">NoError</span><span style=\"color:#E1E4E8\">(t, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    require.</span><span style=\"color:#B392F0\">NotNil</span><span style=\"color:#E1E4E8\">(t, config)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Assert that the Jobs map has exactly one key: \"build\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Assert that the job \"build\" has RunsOn equal to \"ubuntu-latest\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Assert that the job \"build\" has exactly one step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Assert that the step's Name is \"Echo\" and Command is `echo \"Hello\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestResolveEnvVars</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    yamlContent </span><span style=\"color:#F97583\">:=</span><span style=\"color:#9ECBFF\"> `</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">env:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  GREETING: \"Hello\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">jobs:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  test:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    runs-on: ubuntu-latest</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    steps:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">      - name: Print</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        command: echo $GREETING ${GREETING}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> ParseConfig</span><span style=\"color:#E1E4E8\">(yamlContent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    require.</span><span style=\"color:#B392F0\">NoError</span><span style=\"color:#E1E4E8\">(t, err)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overrideEnv </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"GREETING\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Hola\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Call ResolveEnvVars with config and overrideEnv</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Assert that the step's Command is resolved to `echo Hola Hola`</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Test that system env (like $HOME) is also resolved if not overridden</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestExpandMatrix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> JobConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Name: </span><span style=\"color:#9ECBFF\">\"test\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Matrix: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"version\"</span><span style=\"color:#E1E4E8\">: []</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span><span style=\"color:#9ECBFF\">\"1.19\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1.20\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"os\"</span><span style=\"color:#E1E4E8\">:      []</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span><span style=\"color:#9ECBFF\">\"linux\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Call ExpandMatrix with this job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Assert the result is a slice of length 2 (cartesian product)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each expanded job, assert the Env map contains VERSION and OS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Assert the job names are distinct (e.g., \"test (version=1.19, os=linux)\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint Commands:</strong></p>\n<ul>\n<li><strong>After Milestone 1:</strong> Run <code>go test ./internal/parser/... -v</code>. Expect: All tests pass, with green <code>PASS</code> output. For manual verification, run <code>go run cmd/testparser/main.go</code> (if you create a small test program) and see a printed execution plan.</li>\n<li><strong>After Milestone 2:</strong> Run <code>go test ./internal/executor/... -v -short</code> to run unit tests. Then run <code>go test ./internal/executor/... -v -run TestExecuteJobIntegration</code> (requires Docker). Expect: Integration test passes and cleans up containers (<code>docker ps -a</code> shows no leftover test containers).</li>\n<li><strong>After Milestone 3:</strong> Start the webhook server (<code>go run cmd/server/main.go</code>). In another terminal, use <code>curl -X POST -H &quot;X-GitHub-Event: push&quot; -H &quot;Content-Type: application/json&quot; -d @test/fixtures/webhook_payloads/push.json http://localhost:8080/webhook</code>. Expect a <code>202 Accepted</code> response. Check the server logs for &quot;Pipeline run created&quot;.</li>\n<li><strong>After Milestone 4:</strong> Run <code>go test ./internal/dashboard/... -v</code>. Start the dashboard (<code>go run cmd/dashboard/main.go</code>). Open <code>http://localhost:8081</code> in your browser. You should see the build history page. If you have a run in progress, click on it and see logs updating in real-time.</li>\n</ul>\n<p><strong>Debugging Tips for Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Integration test hangs forever.</td>\n<td>Docker container not starting, or command in container is infinite loop.</td>\n<td>Check test logs for container start errors. Run <code>docker ps</code> during the test to see container state.</td>\n<td>Ensure the container image is correct and the command exits. Use timeouts in tests (<code>t.Timeout</code>).</td>\n</tr>\n<tr>\n<td>Unit test with mocks fails: &quot;unexpected call&quot;.</td>\n<td>The test code is calling a method you didn&#39;t expect, or the mock setup is incomplete.</td>\n<td>Examine the mock error message: it shows which method was called with what arguments.</td>\n<td>Adjust the mock setup (<code>On(...).Return(...)</code>) to match the actual calls made by your code under test.</td>\n</tr>\n<tr>\n<td>Webhook test passes locally but fails in CI.</td>\n<td>Missing Docker or network environment differences.</td>\n<td>Check CI runner logs for &quot;cannot connect to Docker daemon&quot;.</td>\n<td>In CI, use Docker-in-Docker or configure the test to skip integration tests if Docker is unavailable (<code>if testing.Short()</code>).</td>\n</tr>\n<tr>\n<td>SSE log streaming test doesn&#39;t receive events.</td>\n<td>The <code>LogBroadcaster</code> is not connected to the actual log producer in the test.</td>\n<td>Add logging to <code>Broadcast</code> method to see if events are being sent. Check if the client is registered correctly.</td>\n<td>In the test, ensure you call <code>Broadcast</code> after registering the SSE client. Simulate a log event from the executor.</td>\n</tr>\n<tr>\n<td>Priority queue test fails after adding many items.</td>\n<td>The heap invariant is violated (parent priority not &gt;= child priority).</td>\n<td>Write a helper function to verify the heap property after each operation. Print the heap contents before failure.</td>\n<td>Carefully implement <code>heap.Interface</code> methods (<code>Len</code>, <code>Less</code>, <code>Swap</code>, <code>Push</code>, <code>Pop</code>). <code>Less</code> should compare priority (higher priority first).</td>\n</tr>\n</tbody></table>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>Building a CI/CD system involves orchestrating multiple moving parts—webhooks, queues, containers, and real-time data streams. When things go wrong, symptoms can be confusing because a failure in one component often manifests as a strange behavior in another. This section provides a structured approach to debugging by cataloging common symptoms, their root causes, and step-by-step fixes. Think of debugging this system like being a <strong>factory floor supervisor</strong>: when the assembly line stops, you need to walk the line, check each station (component), inspect the work orders (queue items), and examine the machinery (containers) to find the blockage.</p>\n<h3 id=\"common-bugs-symptom-cause-fix\">Common Bugs: Symptom → Cause → Fix</h3>\n<p>The following table maps observable symptoms to their likely causes, provides diagnostic steps to confirm the diagnosis, and outlines concrete fixes. Each entry assumes you&#39;re following the architecture and data models defined in previous sections.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Webhooks are ignored</strong> – No pipeline runs are created when pushing to the repository.</td>\n<td>1. <strong>Webhook signature validation failure</strong>: The <code>WebhookVerifier.VerifyGitHubSignature</code> or <code>VerifyGitLabToken</code> is rejecting the payload.<br>2. <strong>Event type mismatch</strong>: The <code>ExtractEventDetails</code> function cannot parse the event, or the pipeline configuration&#39;s <code>if</code> conditions filter it out.<br>3. <strong>Network/firewall issue</strong>: The webhook listener isn&#39;t reachable from your Git host.</td>\n<td>1. Check the webhook listener logs for errors during <code>HandleWebhook</code>. Look for <code>&quot;signature invalid&quot;</code> or <code>&quot;event type unknown&quot;</code> messages.<br>2. Use <code>curl</code> to send a test payload locally: <code>curl -X POST -H &quot;X-GitHub-Event: push&quot; -d @payload.json http://localhost:8080/webhook</code>. Inspect the <code>Store</code> for new <code>PipelineRun</code> entries.<br>3. Verify the webhook endpoint URL is correctly configured in your repository settings and that port forwarding/firewall rules allow inbound connections.</td>\n<td>1. Ensure the webhook secret in your CI system matches the one configured in your repository. Double-check the signature header name (GitHub uses <code>X-Hub-Signature-256</code>, GitLab uses <code>X-GitLab-Token</code>).<br>2. Add debug logging in <code>ExtractEventDetails</code> to print the raw event type and ref. Ensure your pipeline YAML&#39;s <code>on:</code> clause matches the event.<br>3. If using a local development environment, use a service like ngrok to expose your local server with a public URL for testing webhooks.</td>\n</tr>\n<tr>\n<td><strong>Jobs hang forever</strong> – A <code>JobRun</code> remains in <code>STATUS_RUNNING</code> long after commands should have completed.</td>\n<td>1. <strong>Container cleanup failure</strong>: The Docker container exited, but <code>ExecuteJob</code> failed to update the <code>JobRun.Status</code> to <code>STATUS_FAILED</code> or <code>STATUS_SUCCEEDED</code>.<br>2. <strong>Infinite loop in user script</strong>: A step command (e.g., <code>while true; do sleep 1; done</code>) never exits.<br>3. <strong>Deadlock in worker coordination</strong>: The worker process crashed while holding a lock, or the <code>HybridQueue.DequeueJobRun</code> is stuck waiting.</td>\n<td>1. Run <code>docker ps -a</code> to list all containers. Look for containers with the <code>JobRun.ID</code> in their name or labels. Check if they are in <code>Exited</code> state.<br>2. Check the real-time logs for the job. If logs stop abruptly, the container may have been killed. Use <code>docker logs &lt;container-id&gt;</code> to see the container&#39;s stdout/stderr.<br>3. Inspect the worker logs. If a worker panicked, there may be no log of job completion. Check the <code>Store</code> for <code>JobRun</code> records with <code>Status=STATUS_RUNNING</code> but no recent <code>UpdatedAt</code> timestamp.</td>\n<td>1. Implement a <strong>heartbeat or timeout</strong> in <code>ExecuteJob</code>. Use <code>context.WithTimeout</code> and ensure the Docker client&#39;s <code>ContainerWait</code> call respects it. Always call <code>ContainerRemove</code> in a <code>defer</code> block.<br>2. Add <strong>timeout configuration</strong> to <code>JobConfig</code>. In <code>ExecuteJob</code>, enforce a maximum duration per job, killing the container if exceeded.<br>3. Implement a <strong>stuck job recovery</strong> routine. The <code>CleanupStuckJobs</code> function should periodically scan for jobs in <code>STATUS_RUNNING</code> with an old <code>StartedAt</code> time and mark them as <code>STATUS_FAILED</code>.</td>\n</tr>\n<tr>\n<td><strong>Logs stop streaming</strong> – The dashboard shows a job is running, but the log output freezes and doesn&#39;t update.</td>\n<td>1. <strong>Backpressure in log streaming</strong>: The <code>LogBroadcaster.Broadcast</code> is blocked because a client channel is full (slow browser).<br>2. <strong>Broken pipe in container log tail</strong>: The goroutine reading from Docker&#39;s <code>ContainerLogs</code> encountered an error and stopped.<br>3. <strong>SSE connection closed</strong>: The browser&#39;s EventSource connection dropped due to network interruption or server-side timeout.</td>\n<td>1. Check the dashboard server logs for errors like <code>&quot;send on closed channel&quot;</code> or <code>&quot;client channel full&quot;</code>. Monitor the number of active clients in <code>LogBroadcaster.clients</code>.<br>2. In <code>StreamLogs</code>, verify the reader from <code>dockerClient.ContainerLogs</code> is still active. Add logging to capture <code>io.Copy</code> errors.<br>3. Open browser developer tools, check the Network tab for the SSE endpoint (<code>/api/runs/:id/logs</code>). Look for connection status and error events.</td>\n<td>1. Implement <strong>non-blocking send with buffer</strong> in <code>LogBroadcaster</code>. Use a buffered channel for each <code>ClientChannel.Send</code> and a select statement with a default case to drop old log lines if the client is too slow.<br>2. Add <strong>reconnection logic</strong> in the frontend JavaScript. When the EventSource closes, wait a few seconds and reconnect.<br>3. Ensure the <code>StreamLogs</code> method handles <code>context.Canceled</code> gracefully (when client disconnects) and stops reading container logs to free resources.</td>\n</tr>\n<tr>\n<td><strong>Artifacts missing</strong> – The job completes successfully, but no artifacts appear on the download page.</td>\n<td>1. <strong>Glob pattern mismatch</strong>: The artifact <code>patterns</code> in the pipeline YAML don&#39;t match any files in the container&#39;s workspace.<br>2. <strong>Permission error during copy</strong>: The <code>CollectArtifacts</code> function cannot read files from the container or write to persistent storage.<br>3. <strong>Container removed before collection</strong>: The <code>ExecuteJob</code> function removes the container before <code>CollectArtifacts</code> is called.</td>\n<td>1. Check the job logs for messages from <code>CollectArtifacts</code>. Look for <code>&quot;no files matched&quot;</code> warnings. Run the job locally with the same image and commands to verify file paths.<br>2. Inspect the Docker daemon logs for permission errors. Ensure the CI system&#39;s process has read/write access to the artifact storage directory.<br>3. Verify the order of operations in <code>ExecuteJob</code>: artifacts should be collected <em>before</em> <code>ContainerRemove</code> is called. Check if the <code>JobRun.Status</code> is updated before artifact collection.</td>\n<td>1. Improve <strong>artifact pattern debugging</strong>: Log the resolved absolute paths inside the container before attempting copy. Consider supporting absolute paths and relative paths from the workspace.<br>2. Run the CI system with appropriate user permissions. If using Docker, ensure the container runs with a user ID that can read the files (or run as root if necessary).<br>3. Refactor <code>ExecuteJob</code> to use a <strong>deferred cleanup</strong> pattern: schedule container removal only after all steps (including artifact collection) are complete or have failed.</td>\n</tr>\n<tr>\n<td><strong>Matrix builds explode</strong> – Creating a pipeline run with a matrix causes hundreds of jobs, overwhelming the system.</td>\n<td>1. <strong>Combinatorial explosion</strong>: A matrix with multiple axes, each with many values, produces too many job permutations (e.g., <code>os: [ubuntu, alpine]</code> × <code>node: [14, 16, 18, 20]</code> × <code>python: [3.8, 3.9, 3.10]</code> = 24 jobs).<br>2. <strong>Infinite expansion loop</strong>: A bug in <code>ExpandMatrix</code> causes recursive expansion or incorrect handling of nested arrays.</td>\n<td>1. Check the pipeline run detail in the dashboard. Count the number of <code>JobRun</code> entries created. Examine the <code>MatrixAxes</code> field in the <code>PipelineConfig</code>.<br>2. Add logging to <code>ExpandMatrix</code> to print each axis and its values. Verify the Cartesian product logic is correct.</td>\n<td>1. Implement <strong>matrix limits</strong> in the parser: reject matrix definitions that would produce more than a configured maximum (e.g., 100) jobs.<br>2. Add <strong>exclude clauses</strong> to the matrix configuration (like GitHub Actions) to filter out unwanted combinations.<br>3. Ensure <code>ExpandMatrix</code> correctly handles scalar values and empty arrays. Use unit tests with edge cases.</td>\n</tr>\n<tr>\n<td><strong>Dependency deadlock</strong> – Jobs are stuck in <code>STATUS_PENDING</code> even though their dependencies have completed.</td>\n<td>1. <strong>Circular dependency</strong>: The pipeline configuration has a cycle in job <code>needs</code> (e.g., job A needs B, B needs C, C needs A).<br>2. <strong>Missing dependency update</strong>: <code>MarkJobRunComplete</code> fails to notify or enqueue dependent jobs.<br>3. <strong>Silent dependency failure</strong>: A dependent job was skipped (<code>STATUS_SKIPPED</code>) due to an <code>if</code> condition, but downstream jobs are waiting for it to succeed.</td>\n<td>1. Use the pipeline DAG visualization (<code>GetDAG</code> endpoint) to inspect the graph. Look for cycles.<br>2. Check the <code>HybridQueue</code> after a job completes. Are the dependent jobs added to the heap? Inspect the <code>Store</code> for <code>JobRun</code> records whose <code>Needs</code> list contains only completed jobs.<br>3. Examine the <code>JobRun.Status</code> of all jobs in the pipeline. Look for <code>STATUS_SKIPPED</code> jobs that other jobs depend on.</td>\n<td>1. Add <strong>cycle detection</strong> in <code>BuildExecutionGraph</code>. Use topological sort (Kahn&#39;s algorithm) and fail early if a cycle is detected.<br>2. In <code>MarkJobRunComplete</code>, after updating the job status, query all jobs that list this job in their <code>Needs</code>. If <em>all</em> dependencies of a downstream job are satisfied (complete or skipped), call <code>EnqueueJobRun</code> for it.<br>3. Adjust dependency resolution logic: a job should run if its dependencies are <code>STATUS_SUCCEEDED</code> <em>or</em> <code>STATUS_SKIPPED</code>. Update the condition in <code>MarkJobRunComplete</code>.</td>\n</tr>\n<tr>\n<td><strong>Priority scheduling not working</strong> – High-priority jobs (like production deploys) are stuck behind low-priority test jobs.</td>\n<td>1. <strong>Priority field not set</strong>: The <code>PipelineRun</code> or <code>JobRun</code> created from the webhook has <code>Priority=0</code> (default).<br>2. <strong>Heap invariant broken</strong>: The <code>PriorityQueue</code> implementation (a min-heap) is not correctly maintaining order during <code>push</code>/<code>pop</code>.<br>3. <strong>Worker not respecting priority</strong>: Workers are dequeuing jobs in a different order (e.g., FIFO) because they&#39;re accessing the queue incorrectly.</td>\n<td>1. Inspect the <code>Item</code> entries in the <code>HybridQueue.heap</code>. Log the <code>Priority</code> of each item when enqueuing. Check the webhook handler logic that sets priority based on branch name (e.g., <code>main</code> gets higher priority).<br>2. Write a unit test for <code>PriorityQueue</code> that inserts items with random priorities and verifies they pop in correct order.<br>3. Check that all workers use the same <code>DequeueJobRun</code> method, which should atomically fetch the highest-priority item.</td>\n<td>1. Ensure <code>HandleWebhook</code> assigns a meaningful priority. For example: <code>main</code> branch → priority 10, feature branches → priority 1. Store this in the <code>PipelineRun</code> and propagate to <code>JobRun</code>.<br>2. Verify the <code>heap.Interface</code> implementation (Len, Less, Swap, Push, Pop). The <code>Less</code> method should compare <code>Priority</code> (higher priority = smaller number if min-heap).<br>3. Use <strong>transactional dequeue</strong>: <code>DequeueJobRun</code> must lock the heap, take the top item, remove it, and update the store, all in one atomic operation.</td>\n</tr>\n<tr>\n<td><strong>Real-time dashboard shows stale data</strong> – The list of pipeline runs doesn&#39;t update until the page is refreshed.</td>\n<td>1. <strong>No reactive updates</strong>: The dashboard frontend uses periodic polling instead of live updates for the run list.<br>2. <strong>Caching in API layer</strong>: The <code>GetRuns</code> handler serves a cached response or doesn&#39;t reflect the latest <code>Store</code> state.<br>3. <strong>Database isolation level</strong>: If using SQLite with multiple goroutines, reads may not see writes from other connections without proper transaction handling.</td>\n<td>1. Check the browser&#39;s network tab. Are there repeated calls to <code>/api/runs</code>? If not, the frontend is not polling.<br>2. Add a timestamp log in <code>GetRuns</code>. Verify it&#39;s called when new runs are created. Check if the <code>Store.ListPipelineRuns</code> uses a cache with TTL.<br>3. In SQLite, enable WAL mode (<code>journal_mode=WAL</code>) to allow reads and writes to happen concurrently. Check for <code>BEGIN EXCLUSIVE</code> transactions that block reads.</td>\n<td>1. Implement <strong>Server-Sent Events (SSE) for run list updates</strong> or use WebSocket to push new run events to the dashboard. Alternatively, use simple HTTP polling (every 5 seconds).<br>2. Ensure the <code>Store</code> interface methods are called with a context that has a short timeout. Avoid caching in the API layer for real-time data.<br>3. Use a connection pool for the database and set appropriate transaction isolation levels. For SQLite, ensure writes are committed before reads are attempted.</td>\n</tr>\n</tbody></table>\n<h3 id=\"debugging-techniques-and-tools\">Debugging Techniques and Tools</h3>\n<p>Effective debugging requires both systematic thinking and familiarity with the tools at your disposal. When faced with an issue, adopt the mindset of a <strong>detective following clues</strong>: start from the symptom, gather evidence from logs and system state, form a hypothesis, test it, and then apply the fix. Below are specialized techniques and tools for this CI/CD system.</p>\n<h4 id=\"1-container-forensics-with-docker-commands\">1. Container Forensics with Docker Commands</h4>\n<p>Since jobs run in Docker containers, your first stop when a job behaves oddly is the Docker daemon.</p>\n<ul>\n<li><strong>List all containers</strong>: <code>docker ps -a</code> shows containers from all runs, including exited ones. Look for containers with names or labels containing the <code>JobRun.ID</code>. If a container is still running when it should be finished, you might have a hanging process.</li>\n<li><strong>Inspect container details</strong>: <code>docker inspect &lt;container-id&gt;</code> gives you a JSON dump of the container&#39;s configuration, including environment variables, mounted volumes, and the exact command that was run. Compare this with what you expect from the <code>JobConfig</code>.</li>\n<li><strong>Check container logs</strong>: <code>docker logs &lt;container-id&gt;</code> shows the stdout/stderr of the container, which might have more details than what was captured by <code>StreamLogs</code> (especially if the log streaming broke). Use <code>--tail</code> and <code>--follow</code> to watch in real time.</li>\n<li><strong>Examine filesystem</strong>: <code>docker exec -it &lt;container-id&gt; /bin/sh</code> (if the container is still running) lets you explore the workspace and verify that files were created as expected. For stopped containers, you can <code>docker export</code> the filesystem to a tar archive.</li>\n<li><strong>Clean up orphaned containers</strong>: If your CI system crashes, containers might be left behind. <code>docker container prune</code> removes all stopped containers. Be careful not to remove containers belonging to active jobs.</li>\n</ul>\n<h4 id=\"2-queue-and-state-inspection\">2. Queue and State Inspection</h4>\n<p>The queue (<code>HybridQueue</code>) and the <code>Store</code> hold the authoritative state of what&#39;s supposed to happen.</p>\n<ul>\n<li><strong>Dump the queue contents</strong>: Add a debug endpoint (or temporary code) that logs the contents of the <code>PriorityQueue</code> heap. Print each <code>Item</code>&#39;s <code>JobRunID</code>, <code>Priority</code>, and <code>CreatedAt</code>. This reveals if jobs are stuck in the queue or ordered incorrectly.</li>\n<li><strong>Query the database directly</strong>: If using SQLite, open the database file with <code>sqlite3 ci.db</code> and run queries:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">  SELECT</span><span style=\"color:#F97583\"> *</span><span style=\"color:#F97583\"> FROM</span><span style=\"color:#E1E4E8\"> pipeline_runs </span><span style=\"color:#F97583\">ORDER BY</span><span style=\"color:#E1E4E8\"> created_at </span><span style=\"color:#F97583\">DESC</span><span style=\"color:#F97583\"> LIMIT</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  SELECT</span><span style=\"color:#F97583\"> *</span><span style=\"color:#F97583\"> FROM</span><span style=\"color:#E1E4E8\"> job_runs </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#F97583\"> status</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'RUNNING'</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  SELECT</span><span style=\"color:#E1E4E8\"> job_name, </span><span style=\"color:#F97583\">status</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">COUNT</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> job_runs </span><span style=\"color:#F97583\">GROUP BY</span><span style=\"color:#E1E4E8\"> job_name, </span><span style=\"color:#F97583\">status</span><span style=\"color:#E1E4E8\">;</span></span></code></pre></div>\n<p>  This gives you a ground truth independent of the application&#39;s memory state.</p>\n<ul>\n<li><strong>Check for blocked dependencies</strong>: Write a query to find jobs that are <code>STATUS_PENDING</code> but have all their dependencies satisfied (i.e., all jobs in their <code>needs</code> list are <code>STATUS_SUCCEEDED</code> or <code>STATUS_SKIPPED</code>). This uncovers bugs in the dependency resolution logic.</li>\n</ul>\n<h4 id=\"3-structured-logging-and-correlation-ids\">3. Structured Logging and Correlation IDs</h4>\n<p>The system produces many logs; without proper structure, it&#39;s hard to trace a single pipeline run across components.</p>\n<ul>\n<li><strong>Enable correlation IDs</strong>: Ensure every webhook request generates a unique correlation ID (e.g., <code>X-Correlation-ID</code>) that is passed through all components and included in every log entry. The <code>Logger</code> should have a <code>WithCorrelationID</code> method that adds this field to the <code>LogEntry.Fields</code>.</li>\n<li><strong>Search logs by run ID</strong>: When investigating a specific pipeline run, grep your log files for its <code>PipelineRun.ID</code>. For example: <code>grep &quot;run_abc123&quot; ci.log | tail -20</code>. This shows you the entire lifecycle across webhook, queue, worker, and dashboard.</li>\n<li><strong>Log at key decision points</strong>: Add debug-level logs at critical junctures:<ul>\n<li>When a webhook is received and validated (<code>HandleWebhook</code>)</li>\n<li>When a job is enqueued (<code>EnqueueJobRun</code>)</li>\n<li>When a worker picks up a job (<code>DequeueJobRun</code>)</li>\n<li>When a container starts and stops (<code>ExecuteJob</code>)</li>\n<li>When artifacts are collected (<code>CollectArtifacts</code>)</li>\n</ul>\n</li>\n<li><strong>Use log levels wisely</strong>: <code>ERROR</code> for conditions that require intervention (e.g., signature mismatch, container failure). <code>WARN</code> for unusual but recoverable situations (e.g., artifact pattern matched no files). <code>INFO</code> for normal business logic (e.g., job started). <code>DEBUG</code> for detailed step-by-step flow.</li>\n</ul>\n<h4 id=\"4-network-and-endpoint-testing\">4. Network and Endpoint Testing</h4>\n<p>Many failures are due to misconfigured networking or HTTP errors.</p>\n<ul>\n<li><strong>Test webhook endpoint manually</strong>: Use <code>curl</code> or Postman to simulate a webhook payload. This bypasses Git host complexities and lets you control exactly what&#39;s sent. Example for GitHub:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">  curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       -H</span><span style=\"color:#9ECBFF\"> \"X-GitHub-Event: push\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       -H</span><span style=\"color:#9ECBFF\"> \"X-Hub-Signature-256: sha256=$(</span><span style=\"color:#B392F0\">openssl</span><span style=\"color:#9ECBFF\"> dgst </span><span style=\"color:#79B8FF\">-sha256</span><span style=\"color:#79B8FF\"> -hmac</span><span style=\"color:#9ECBFF\"> \"your-secret\" payload.json </span><span style=\"color:#F97583\">|</span><span style=\"color:#B392F0\"> cut</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\">' ' </span><span style=\"color:#79B8FF\">-f2</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       -d</span><span style=\"color:#9ECBFF\"> @payload.json</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       http://localhost:8080/webhook</span></span></code></pre></div>\n<ul>\n<li><strong>Verify dashboard API responses</strong>: The dashboard endpoints (<code>/api/runs</code>, <code>/api/runs/:id/logs</code>) should return appropriate HTTP status codes. Use <code>curl</code> to check:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">  curl</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/runs</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  curl</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/runs/run_abc123/logs</span></span></code></pre></div>\n<p>  Look for <code>200 OK</code>, correct <code>Content-Type</code> (e.g., <code>text/event-stream</code> for logs), and CORS headers if needed.</p>\n<ul>\n<li><strong>Monitor open connections</strong>: Use <code>netstat</code> or <code>lsof</code> to see if your server is accumulating many open connections (indicating a leak). For example: <code>lsof -i :8080</code> lists processes using port 8080.</li>\n</ul>\n<h4 id=\"5-performance-profiling-and-resource-monitoring\">5. Performance Profiling and Resource Monitoring</h4>\n<p>When the system slows down or becomes unresponsive, resource exhaustion might be the cause.</p>\n<ul>\n<li><strong>Monitor Docker daemon resources</strong>: <code>docker stats</code> shows real-time CPU, memory, and network usage of all containers. A job with a memory leak can starve other jobs.</li>\n<li><strong>Check system resource usage</strong>: Use <code>top</code> or <code>htop</code> to see if the CI server process itself is using excessive CPU or memory. The log broadcaster holding many client channels in memory can cause bloat.</li>\n<li><strong>Profile Go code</strong>: If you suspect a performance bug in the Go code, use the built-in profiler. Add <code>import _ &quot;net/http/pprof&quot;</code> and start an HTTP server on a debug port. Then use <code>go tool pprof</code> to examine CPU and memory profiles.</li>\n<li><strong>Set resource limits</strong>: In <code>ExecuteJob</code>, use Docker&#39;s <code>--memory</code>, <code>--cpus</code>, and <code>--ulimit</code> flags to prevent a single job from consuming all host resources.</li>\n</ul>\n<h4 id=\"6-incremental-and-isolated-testing\">6. Incremental and Isolated Testing</h4>\n<p>When you add a new feature or fix a bug, test it in isolation before integrating.</p>\n<ul>\n<li><strong>Unit test each component</strong>: Run <code>go test ./internal/parser/...</code> to ensure the parser still works after changes. Write table-driven tests for edge cases in <code>ExpandMatrix</code> and <code>BuildExecutionGraph</code>.</li>\n<li><strong>Integration test with a mock Docker daemon</strong>: Use a library like <code>testcontainers</code> or a mock Docker client to test <code>ExecuteJob</code> without a real Docker daemon. This speeds up tests and avoids side effects.</li>\n<li><strong>End-to-end test with a local Git server</strong>: Set up a local Git server (e.g., Gitea) or use file:// URLs to simulate pushes. Trigger a webhook and watch the pipeline run through completion. This tests the entire chain.</li>\n<li><strong>Use a staging environment</strong>: If possible, run a second instance of the CI system against a test repository. This lets you experiment with configuration changes without affecting production pipelines.</li>\n</ul>\n<blockquote>\n<p><strong>Key Insight</strong>: The most effective debugging combines <strong>observability</strong> (logs, metrics, traces) with <strong>reproducibility</strong> (being able to recreate the failure on demand). Invest time in building good logging and making the system testable; it pays off when things go wrong at 3 AM.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While debugging is primarily a thinking skill, having the right tools and code patterns makes it easier. Below are some implementation snippets that help with debugging.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logging</td>\n<td>Structured JSON logs to stdout (using <code>log/slog</code> or <code>zap</code>)</td>\n<td>Centralized log aggregation (ELK stack, Grafana Loki) with correlation IDs</td>\n</tr>\n<tr>\n<td>Metrics</td>\n<td>Basic counters and gauges exposed via <code>/metrics</code> (Prometheus format)</td>\n<td>Full tracing with OpenTelemetry to track requests across components</td>\n</tr>\n<tr>\n<td>Debug Endpoints</td>\n<td>HTTP <code>/debug/pprof</code> for profiling, <code>/debug/queue</code> for queue inspection</td>\n<td>Admin dashboard with real-time system health and manual intervention buttons</td>\n</tr>\n<tr>\n<td>Container Inspection</td>\n<td>Direct <code>docker</code> command-line calls</td>\n<td>Container monitoring tools (cAdvisor, Portainer)</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Add debugging utilities within a dedicated package:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ci-system/\n  cmd/\n    server/main.go\n    worker/main.go\n  internal/\n    debug/              # Debugging utilities\n      inspector.go     # Functions to dump queue state, list stuck jobs\n      profiler.go      # pprof HTTP server setup\n    store/\n      store.go         # Store interface\n      sqlite.go        # SQLite implementation\n    queue/\n      hybrid_queue.go  # HybridQueue with debug methods\n    logger/\n      logger.go        # Structured Logger with correlation ID support</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-structured-logger-with-correlation-id\">C. Infrastructure Starter Code: Structured Logger with Correlation ID</h4>\n<p>Here&#39;s a complete, ready-to-use structured logger that supports correlation IDs and log levels:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/logger/logger.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> logger</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ctxKey</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#79B8FF\"> correlationIDKey</span><span style=\"color:#B392F0\"> ctxKey</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"correlation_id\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Logger wraps slog.Logger and adds correlation ID support.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// New creates a new Logger with JSON output.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Logger: slog.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(slog.</span><span style=\"color:#B392F0\">NewJSONHandler</span><span style=\"color:#E1E4E8\">(os.Stdout, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerOptions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Level: slog.LevelDebug,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })).</span><span style=\"color:#B392F0\">With</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"component\"</span><span style=\"color:#E1E4E8\">, component),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WithCorrelationID returns a new Logger with the correlation ID added to all log entries.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WithCorrelationID</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">cid</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">{l.Logger.</span><span style=\"color:#B392F0\">With</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"correlation_id\"</span><span style=\"color:#E1E4E8\">, cid)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FromContext extracts the correlation ID from context and returns a Logger with it.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">FromContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> cid, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">(correlationIDKey).(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">WithCorrelationID</span><span style=\"color:#E1E4E8\">(cid)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> l</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewContextWithCorrelationID creates a new context with the given correlation ID.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewContextWithCorrelationID</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cid</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithValue</span><span style=\"color:#E1E4E8\">(ctx, correlationIDKey, cid)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogEntry is the structured log entry (for reference, not used directly).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Level       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"component\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"error,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorType   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"error_type,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Fields      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"fields,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Caller      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"caller,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code-debug-inspector-for-queue\">D. Core Logic Skeleton Code: Debug Inspector for Queue</h4>\n<p>Add a debug method to <code>HybridQueue</code> to inspect its contents safely:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/queue/hybrid_queue.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> queue</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DebugItems returns a slice of all items in the heap, sorted by priority (highest first).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This is safe for debugging but not for production use as it holds the lock.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DebugItems</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#B392F0\">Item</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hq.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hq.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Copy the heap slice</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    items </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">Item</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hq.heap))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, item </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hq.heap {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        items[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">item</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sort by priority (ascending for min-heap, so highest priority = smallest number)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sort.</span><span style=\"color:#B392F0\">Slice</span><span style=\"color:#E1E4E8\">(items, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">j</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> items[i].Priority </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> items[j].Priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> items</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PrintDebugInfo logs the current state of the queue.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HybridQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">PrintDebugInfo</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    items </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hq.</span><span style=\"color:#B392F0\">DebugItems</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=== Queue Debug Info ===</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Total items: </span><span style=\"color:#79B8FF\">%d\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(items))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, item </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> items {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"  JobRunID: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, Priority: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">, CreatedAt: </span><span style=\"color:#79B8FF\">%v\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            item.JobRunID, item.Priority, item.CreatedAt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Go&#39;s pprof</strong>: Import <code>_ &quot;net/http/pprof&quot;</code> and start an HTTP server on a debug port (e.g., <code>:6060</code>). Then run <code>go tool pprof http://localhost:6060/debug/pprof/heap</code> to analyze memory usage.</li>\n<li><strong>Docker SDK</strong>: Use <code>github.com/docker/docker/client</code> to interact with Docker. When debugging, set <code>DOCKER_LOG_LEVEL=debug</code> to see verbose output from the Docker client.</li>\n<li><strong>SQLite browser</strong>: Use a GUI like DB Browser for SQLite to visually inspect tables and run ad-hoc queries during debugging.</li>\n<li><strong>Context timeouts</strong>: Always pass a context with timeout to potentially blocking operations (like <code>ContainerWait</code>). Use <code>context.WithTimeout</code> and handle <code>context.DeadlineExceeded</code> errors appropriately.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the system, verify debugging capabilities:</p>\n<ol>\n<li><strong>Trigger a pipeline run</strong> and check that correlation IDs flow through all logs:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/webhook</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> @test_payload.json</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   tail</span><span style=\"color:#79B8FF\"> -f</span><span style=\"color:#9ECBFF\"> ci.log</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -E</span><span style=\"color:#9ECBFF\"> \"correlation_id.*&#x3C;id>\"</span></span></code></pre></div>\n<p>   You should see logs from webhook handler, queue, worker, and dashboard with the same correlation ID.</p>\n<ol start=\"2\">\n<li><strong>Simulate a stuck job</strong> by adding a <code>sleep 1000</code> command in a pipeline step. Then use the debug inspector:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/debug/queue</span><span style=\"color:#6A737D\">   # if you implement such endpoint</span></span></code></pre></div>\n<p>   You should see the job in the queue with its priority.</p>\n<ol start=\"3\">\n<li><strong>Test artifact collection debugging</strong> by intentionally using a wrong glob pattern. Check the logs for a clear warning message indicating which pattern matched zero files.</li>\n</ol>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dashboard shows &quot;Connection lost&quot; repeatedly</td>\n<td>SSE stream keeps disconnecting</td>\n<td>Check browser console for errors. Look for server logs with &quot;client channel full&quot; or &quot;write timeout&quot;.</td>\n<td>Increase the <code>Send</code> channel buffer size in <code>ClientChannel</code>. Implement ping/pong keepalive in SSE.</td>\n</tr>\n<tr>\n<td>Worker crashes with &quot;container not found&quot;</td>\n<td>Race condition: container removed before logs are fully read</td>\n<td>Check timestamps: when does <code>ContainerRemove</code> happen vs when does <code>StreamLogs</code> finish?</td>\n<td>Use a waitgroup or channel to ensure <code>StreamLogs</code> goroutine completes before container removal.</td>\n</tr>\n<tr>\n<td>SQLite database locked</td>\n<td>Multiple goroutines writing without proper locking</td>\n<td>Use <code>sqlite3 ci.db &quot;PRAGMA journal_mode=WAL;&quot;</code> to enable WAL mode. Check for long-running transactions.</td>\n<td>Use a connection pool with max open connections = 1, or switch to a proper database like PostgreSQL.</td>\n</tr>\n<tr>\n<td>Webhook handler returns 500 but logs show nothing</td>\n<td>Panic recovered by HTTP server</td>\n<td>Look for <code>http: panic serving</code> in stdout. Add a defer with recover in <code>HandleWebhook</code>.</td>\n<td>Wrap the handler logic in a defer to catch panics and log them with the correlation ID.</td>\n</tr>\n</tbody></table>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>The initial CI/CD Pipeline Orchestrator is designed as a <strong>minimal viable product</strong> that delivers the core value proposition: running automated pipelines in response to code changes. However, the architecture is intentionally <strong>extensible</strong> at key interfaces to accommodate future enhancements without requiring a complete rewrite. This section explores potential improvements that could be built upon the existing foundation, categorized by implementation effort and architectural impact.</p>\n<p>Think of the current system as a <strong>modular factory</strong> with standardized connection points: just as a real factory can add new assembly lines or upgrade individual workstations without shutting down the entire plant, our CI system can evolve by extending components that follow clear contracts.</p>\n<h3 id=\"low-effort-extensions\">Low-Effort Extensions</h3>\n<p>Low-effort extensions are features that require <strong>minimal architectural changes</strong> and can often be implemented by adding new configuration options, extending existing data structures, or introducing small new modules that plug into existing interfaces. These are ideal for learners who want to practice incremental enhancement after completing the core milestones.</p>\n<table>\n<thead>\n<tr>\n<th>Extension Idea</th>\n<th>Current Design Support</th>\n<th>Required Changes</th>\n<th>Effort Estimate</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Additional Git Providers (Bitbucket, Gitea)</strong></td>\n<td>The <code>WebhookVerifier</code> interface and <code>ExtractEventDetails</code> function are already abstracted to handle multiple providers. The <code>HandleWebhook</code> method uses a generic payload structure.</td>\n<td>1. Add new secret/token validation methods to <code>WebhookVerifier</code>.<br>2. Extend <code>ExtractEventDetails</code> to parse provider-specific payload schemas.<br>3. Add provider-specific event type constants (e.g., <code>EVENT_BITBUCKET_PUSH</code>).</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Manual Pipeline Triggers via API</strong></td>\n<td>The <code>EnqueueRun</code> function and <code>PipelineRun</code> creation logic are already decoupled from webhooks. The <code>Store</code> interface supports creating arbitrary runs.</td>\n<td>1. Add a new HTTP endpoint (e.g., <code>POST /api/runs/trigger</code>) that accepts a pipeline config and branch/commit parameters.<br>2. Reuse the existing <code>HandleWebhook</code> logic to construct a <code>PipelineRun</code> from manual parameters.</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Per-Job Timeout Configuration</strong></td>\n<td>The <code>JobConfig</code> structure already includes a <code>RunsOn</code> field for image specification; timeouts are a similar job-level property. The <code>ExecuteJob</code> function already has a context for cancellation.</td>\n<td>1. Add a <code>TimeoutSeconds int</code> field to <code>JobConfig</code>.<br>2. In <code>ExecuteJob</code>, create a context with a deadline derived from this timeout.<br>3. Propagate timeout errors through the job status (<code>STATUS_FAILED</code> with a timeout reason).</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Job Artifacts as Pipeline Dependencies</strong></td>\n<td>The <code>ArtifactKeys</code> field in <code>JobRun</code> already stores references to collected artifacts. The dependency graph (<code>BuildExecutionGraph</code>) currently only uses the <code>Needs</code> field.</td>\n<td>1. Extend <code>JobConfig</code> with an <code>ArtifactDependencies []string</code> field that lists artifact patterns from upstream jobs.<br>2. During job execution, download these artifacts into the container workspace before running steps.<br>3. Modify the <code>CollectArtifacts</code> function to also handle uploading for downstream consumption.</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Pipeline-Level Environment Variables File</strong></td>\n<td>The <code>PipelineConfig</code> already has an <code>Environment map[string]string</code> field. The <code>ResolveEnvVars</code> function merges pipeline, system, and secret variables.</td>\n<td>1. Add a <code>EnvFile string</code> field to <code>PipelineConfig</code> that specifies a path (e.g., <code>.ci-env</code>) in the repository.<br>2. During parsing, read this file from the source code and merge its key-value pairs into the pipeline environment.<br>3. Ensure the file is fetched as part of the source checkout before variable resolution.</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Basic Notification Webhooks</strong></td>\n<td>The <code>Store</code> interface can be extended to emit events when a pipeline run changes state. The <code>MarkJobRunComplete</code> function already updates status.</td>\n<td>1. Define a simple event bus interface (e.g., <code>Notify(stateChange)</code>).<br>2. In <code>MarkJobRunComplete</code>, fire a notification event with run details.<br>3. Implement a webhook notifier that posts JSON to a configurable URL on failure/success.</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h4 id=\"deep-dive-supporting-additional-git-providers\">Deep Dive: Supporting Additional Git Providers</h4>\n<blockquote>\n<p><strong>Decision: Extensible Webhook Parser Design</strong></p>\n<ul>\n<li><strong>Context</strong>: The initial implementation supports GitHub and GitLab, but teams may use other Git hosting services like Bitbucket, Gitea, or self-hosted solutions.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Hardcoded per-provider logic</strong>: Write separate handler functions for each provider.</li>\n<li><strong>Plugin architecture</strong>: Define a <code>WebhookProvider</code> interface that each provider implements, with dynamic registration.</li>\n<li><strong>Unified payload normalization</strong>: Parse all provider payloads into a common internal event format using a configurable mapping.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use a <strong>unified payload normalization</strong> approach, extending the existing <code>ExtractEventDetails</code> function with provider detection and mapping tables.</li>\n<li><strong>Rationale</strong>: This keeps the webhook handling simple and linear, avoiding the complexity of a plugin system while still allowing new providers to be added by editing a single mapping file. The current <code>WebhookVerifier</code> already supports multiple secret mechanisms.</li>\n<li><strong>Consequences</strong>: Adding a new provider requires updating the payload mapping and adding a verification method, but no changes to the core queue or execution logic.</li>\n</ul>\n</blockquote>\n<p><strong>How the current design accommodates this</strong>:\nThe <code>HandleWebhook</code> method currently calls <code>ExtractEventDetails</code> which returns generic fields (<code>eventType</code>, <code>repo</code>, <code>branch</code>, <code>commitSHA</code>). This abstraction means the rest of the system doesn&#39;t care whether the event came from GitHub or GitLab. To add Bitbucket:</p>\n<ol>\n<li><strong>Verification</strong>: Add a <code>VerifyBitbucketSignature</code> method to <code>WebhookVerifier</code>.</li>\n<li><strong>Parsing</strong>: Extend <code>ExtractEventDetails</code> to detect Bitbucket webhooks (via <code>User-Agent</code> header or <code>X-Event-Key</code>) and map its JSON payload to the same internal fields.</li>\n<li><strong>Configuration</strong>: Allow users to set a Bitbucket secret in the system configuration.</li>\n</ol>\n<p>The <code>PipelineRun</code> created will be identical, and the rest of the pipeline execution proceeds unchanged.</p>\n<h4 id=\"deep-dive-manual-pipeline-triggers\">Deep Dive: Manual Pipeline Triggers</h4>\n<blockquote>\n<p><strong>Decision: Reusing Webhook Pipeline Creation Logic</strong></p>\n<ul>\n<li><strong>Context</strong>: Developers may want to trigger a pipeline run manually (e.g., for a specific branch, with custom parameters) without waiting for a Git event.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>New standalone pipeline creation service</strong>: Build a separate module that constructs runs from scratch.</li>\n<li><strong>Parameterized webhook handler</strong>: Reuse the <code>HandleWebhook</code> function but pass it a synthetic webhook payload constructed from manual parameters.</li>\n<li><strong>Direct store invocation</strong>: Call <code>CreatePipelineRun</code> directly with a fully formed <code>PipelineRun</code> object.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: <strong>Parameterized webhook handler</strong> – extend <code>HandleWebhook</code> to accept an optional manually constructed payload.</li>\n<li><strong>Rationale</strong>: This maximizes code reuse: the same validation, pipeline matching, and run creation logic applies. The only difference is the source of the event data. By adding a <code>manualPayload</code> parameter, we avoid duplicating complex logic.</li>\n<li><strong>Consequences</strong>: The <code>HandleWebhook</code> function becomes slightly more complex, but the manual trigger endpoint becomes trivial to implement.</li>\n</ul>\n</blockquote>\n<p><strong>Implementation path</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Extended HandleWebhook signature</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> HandleWebhook</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">payload</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">signature</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">manualEvent</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ManualEvent</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">PipelineRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> manualEvent </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Construct synthetic webhook payload from manualEvent</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        payload </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> createSyntheticPayload</span><span style=\"color:#E1E4E8\">(manualEvent)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Skip signature verification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Normal verification flow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Existing parsing and run creation...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>A new <code>ManualEvent</code> struct would contain fields like <code>Repo</code>, <code>Branch</code>, <code>CommitSHA</code>, <code>EventType</code>, and optional <code>Parameters</code> (for custom environment variables).</p>\n<h3 id=\"major-enhancements\">Major Enhancements</h3>\n<p>Major enhancements require <strong>significant architectural changes</strong>, potentially introducing new components, changing core data flows, or requiring a redesign of existing interfaces. These represent the evolution of the system from a single-node educational tool towards a production-grade distributed CI/CD platform.</p>\n<table>\n<thead>\n<tr>\n<th>Extension Idea</th>\n<th>Architectural Impact</th>\n<th>Required Changes</th>\n<th>Effort Estimate</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Distributed Workers Across Machines</strong></td>\n<td>High - Changes the fundamental execution model from a local worker pool to a cluster.</td>\n<td>1. Replace the in-memory <code>PriorityQueue</code> with a distributed queue (e.g., Redis Streams, RabbitMQ).<br>2. Introduce a worker registration/discovery mechanism.<br>3. Modify the job execution engine to support remote Docker daemons or Kubernetes pods.<br>4. Add a resource requirement system (CPU, memory) and scheduling constraints.</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Plugin Ecosystem for Steps and Actions</strong></td>\n<td>Medium - Requires a new plugin runtime and extension points in the job execution engine.</td>\n<td>1. Define a <code>StepPlugin</code> interface with a standard invocation method.<br>2. Create a plugin registry that loads external binaries or scripts.<br>3. Extend <code>StepConfig</code> to support <code>plugin: name</code> instead of just <code>command</code>.<br>4. Implement secure plugin execution with isolated permissions.</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Persistent Workflow Caches</strong></td>\n<td>Medium - Introduces a shared cache service and integration points in the job execution engine.</td>\n<td>1. Design a cache service with storage backends (local disk, S3).<br>2. Define cache keys based on job context (e.g., <code>hash(dependencies, command)</code>).<br>3. Integrate cache restore/save into <code>ExecuteJob</code> for specific directories (e.g., <code>node_modules</code>, <code>go/pkg</code>).<br>4. Add cache configuration to <code>JobConfig</code> (paths, key generation).</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Pipeline-as-Code with Dynamic Generation</strong></td>\n<td>High - Changes how pipeline configurations are discovered and evaluated.</td>\n<td>1. Introduce a scriptable pipeline generator (e.g., a <code>generate-pipeline</code> script that outputs YAML).<br>2. Modify the parser to run this generator before parsing.<br>3. Support multiple configuration files and inheritance (e.g., base pipeline with overrides).</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Advanced Security with Secret Management</strong></td>\n<td>Medium - Requires a secure secret injection system beyond environment variables.</td>\n<td>1. Integrate with external secret managers (Hashicorp Vault, AWS Secrets Manager).<br>2. Add secret references in pipeline config (e.g., <code>secret: DB_PASSWORD</code>).<br>3. Implement just-in-time secret retrieval and secure temporary storage in containers.<br>4. Audit logging for secret access.</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Pipeline Approval Gates and Manual Interventions</strong></td>\n<td>Medium - Introduces a new paused state and user interaction mechanism.</td>\n<td>1. Add a new job status <code>STATUS_WAITING_FOR_APPROVAL</code>.<br>2. Extend the dashboard with an approval UI.<br>3. Create an API for resuming paused jobs.<br>4. Integrate with notification systems to alert approvers.</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h4 id=\"deep-dive-distributed-workers-across-machines\">Deep Dive: Distributed Workers Across Machines</h4>\n<blockquote>\n<p><strong>Decision: Decentralized Job Scheduling Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: As pipeline volume grows, a single machine becomes a bottleneck. We need to scale horizontally by adding worker nodes across multiple machines, potentially in different data centers.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Centralized scheduler with remote execution</strong>: Keep a single orchestrator that schedules jobs and uses SSH or Docker API to execute on remote nodes.</li>\n<li><strong>Distributed work queue with worker pull</strong>: Use a persistent distributed queue (Redis, PostgreSQL) where workers pull jobs independently.</li>\n<li><strong>Peer-to-peer job distribution</strong>: Workers discover each other and distribute jobs via gossip protocol.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: <strong>Distributed work queue with worker pull</strong> – replace the <code>HybridQueue</code> with a Redis-backed queue and implement worker nodes as independent processes.</li>\n<li><strong>Rationale</strong>: This pattern is battle-tested in production CI systems (Jenkins, GitLab Runner). It provides clear separation of concerns, scales horizontally, and allows workers to be added/removed dynamically. Redis provides persistence, pub/sub for coordination, and atomic operations for job assignment.</li>\n<li><strong>Consequences</strong>: Introduces a new infrastructure dependency (Redis). Workers become stateful and must handle network partitions. The <code>JobRun</code>&#39;s <code>AssignedWorker</code> field becomes critical for tracking and recovery.</li>\n</ul>\n</blockquote>\n<p><strong>Architectural changes required</strong>:</p>\n<ol>\n<li><p><strong>Queue Replacement</strong>: The current <code>HybridQueue</code> (in-memory heap + SQLite persistence) would be replaced with a <code>RedisQueue</code> that implements the same <code>Store</code> interface methods (<code>EnqueueJobRun</code>, <code>DequeueJobRun</code>, <code>MarkJobRunComplete</code>). Jobs would be stored as Redis streams or sorted sets with priority scores.</p>\n</li>\n<li><p><strong>Worker Node Daemon</strong>: A new <code>worker-node</code> binary would be created, containing:</p>\n<ul>\n<li>Connection to the Redis queue</li>\n<li>Docker client for execution</li>\n<li>Health reporting and metrics</li>\n<li>Configuration for resource limits (concurrency, CPU/memory reservations)</li>\n</ul>\n</li>\n<li><p><strong>Orchestrator Role Changes</strong>: The main orchestrator becomes a <strong>pure scheduler</strong> that only creates pipeline runs and pushes jobs to Redis. It no longer manages a local worker pool.</p>\n</li>\n<li><p><strong>Discovery and Registration</strong>: Workers register themselves in Redis when they start, allowing the dashboard to show available capacity.</p>\n</li>\n<li><p><strong>Artifact Storage</strong>: Artifacts can no longer be stored on the orchestrator&#39;s local filesystem. They must be uploaded to a shared storage service (S3, MinIO) with <code>ArtifactKeys</code> becoming URLs.</p>\n</li>\n</ol>\n<p>The current <code>JobRun</code> data model already supports this: the <code>AssignedWorker</code> field would store the worker node ID, and <code>ContainerID</code> would still reference the Docker container on that specific worker.</p>\n<h4 id=\"deep-dive-plugin-ecosystem-for-steps-and-actions\">Deep Dive: Plugin Ecosystem for Steps and Actions</h4>\n<blockquote>\n<p><strong>Decision: Binary Plugin Interface with gRPC</strong></p>\n<ul>\n<li><strong>Context</strong>: Hardcoded shell commands limit pipeline flexibility. Teams want reusable, versioned actions (like GitHub Actions) that can abstract complex operations.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Shell script wrappers</strong>: Continue using shell commands but package them in versioned scripts fetched from a repository.</li>\n<li><strong>Docker container actions</strong>: Each step runs a specified Docker container with entrypoint and arguments.</li>\n<li><strong>Binary plugins with gRPC</strong>: Define a gRPC service interface that plugins must implement, allowing any language.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: <strong>Docker container actions</strong> as the initial plugin mechanism, with potential evolution to gRPC.</li>\n<li><strong>Rationale</strong>: Docker containers provide the strongest isolation, versioning, and dependency packaging. They align with the existing execution model (we already run jobs in containers). This approach mirrors GitHub Actions&#39; &quot;composite actions&quot; and &quot;Docker container actions&quot;. gRPC adds unnecessary complexity for most use cases initially.</li>\n<li><strong>Consequences</strong>: Plugin execution becomes launching a container instead of running a shell command. The plugin must follow conventions for input (environment variables) and output (files). Artifact collection becomes the primary communication channel between plugins.</li>\n</ul>\n</blockquote>\n<p><strong>How this extends the current design</strong>:</p>\n<p>The <code>StepConfig</code> would gain a new field <code>Action</code> alongside <code>Command</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#85E89D\">steps</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">Run tests</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    action</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">docker://mytestframework:1.0</span><span style=\"color:#6A737D\">  # Special URI scheme</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    with</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># Parameters passed as environment variables</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      test_pattern</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"**/*_test.go\"</span></span></code></pre></div>\n\n<p>During job execution, <code>ExecuteJob</code> would:</p>\n<ol>\n<li>Detect if a step uses <code>action:</code> instead of <code>command:</code></li>\n<li>Pull the specified Docker image (if not cached)</li>\n<li>Run the container with the specified environment variables (<code>with:</code> mapping)</li>\n<li>Capture logs from the container&#39;s stdout/stderr</li>\n<li>After completion, collect artifacts from a predefined output directory in the container</li>\n</ol>\n<p>The existing container execution infrastructure (<code>ExecuteJob</code>) can handle this with minimal modification—instead of executing a shell command, it runs the container with the appropriate entrypoint.</p>\n<p>For advanced use cases, the system could later add a <code>grpc://</code> action type that connects to a local gRPC service implementing a <code>StepPlugin</code> protocol, allowing long-running plugins that maintain state across steps.</p>\n<h4 id=\"deep-dive-persistent-workflow-caches\">Deep Dive: Persistent Workflow Caches</h4>\n<blockquote>\n<p><strong>Decision: Content-Addressable Cache with S3 Backend</strong></p>\n<ul>\n<li><strong>Context</strong>: Many build and test jobs download dependencies (npm packages, Go modules) repeatedly, wasting time and bandwidth. A shared cache between pipeline runs can dramatically speed up execution.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Local per-worker cache</strong>: Each worker node maintains its own cache directory.</li>\n<li><strong>Network filesystem (NFS)</strong>: Mount a shared drive to all workers.</li>\n<li><strong>Content-addressable object store (S3)</strong>: Cache entries keyed by content hash, stored in S3-compatible storage.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: <strong>Content-addressable object store with S3 backend</strong> for the initial implementation.</li>\n<li><strong>Rationale</strong>: Content addressing ensures cache integrity (same hash = same content). S3 provides durability, scalability, and works across distributed workers. It also allows cache pruning by age or size. The implementation can start with a local filesystem backend for simplicity, then switch to S3.</li>\n<li><strong>Consequences</strong>: Introduces a new service dependency (object storage). Cache keys must be carefully designed to avoid collisions while maximizing hit rates. Requires integration into the job execution flow (restore before steps, save after).</li>\n</ul>\n</blockquote>\n<p><strong>Integration points in the current system</strong>:</p>\n<ol>\n<li><strong>Cache Configuration</strong>: Extend <code>JobConfig</code> with:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   type</span><span style=\"color:#B392F0\"> CacheConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       Key     </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">   // Template string like \"node-modules-{{ hashFiles('package-lock.json') }}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       Paths   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Directories to cache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       When    </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">   // \"on_success\" or \"always\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Cache Service Interface</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   type</span><span style=\"color:#B392F0\"> CacheStore</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">       Restore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">destPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// returns hit/miss</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">       Save</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">srcPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Job Execution Integration</strong>: In <code>ExecuteJob</code>, after creating the container but before running steps:<ul>\n<li>Generate cache keys by evaluating templates with job context</li>\n<li>Call <code>cache.Restore()</code> for each cache entry</li>\n<li>After successful job completion, call <code>cache.Save()</code> for each cache entry</li>\n</ul>\n</li>\n</ol>\n<p>The current artifact collection mechanism (<code>CollectArtifacts</code>) could be extended to also handle cache saving, since both involve copying files from the container to persistent storage.</p>\n<h3 id=\"architectural-evolution-guidance\">Architectural Evolution Guidance</h3>\n<p>When considering these enhancements, follow these principles to maintain system coherence:</p>\n<ol>\n<li><p><strong>Preserve Core Data Model</strong>: The <code>PipelineRun</code>, <code>JobRun</code>, and <code>StepRun</code> structures should remain the central execution records. Enhance them with optional fields (e.g., <code>CacheHits []string</code>) rather than creating parallel data structures.</p>\n</li>\n<li><p><strong>Extend Interfaces, Don&#39;t Break Them</strong>: When adding new capabilities, extend existing interfaces with new methods rather than changing method signatures. Use optional parameters (struct options pattern) where possible.</p>\n</li>\n<li><p><strong>Maintain Single-Node Simplicity</strong>: The default configuration should remain a single-node system that runs everything locally. Distributed features should be opt-in via configuration.</p>\n</li>\n<li><p><strong>Progressive Enhancement</strong>: Start with the simplest implementation that works for the 80% use case. For example, a persistent cache could first be implemented as a local directory shared between jobs, then upgraded to S3.</p>\n</li>\n</ol>\n<p>The current architecture, with its clear separation between <strong>orchestration</strong>, <strong>execution</strong>, <strong>queuing</strong>, and <strong>presentation</strong>, provides natural boundaries for these extensions. Most enhancements will live within one component or at the interface between two components, minimizing ripple effects throughout the system.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While full implementation of these extensions is beyond the scope of the core milestones, here are starting points for the most impactful enhancements.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Additional Git Providers</td>\n<td>Extend <code>ExtractEventDetails</code> with hardcoded mappings</td>\n<td>Provider plugin system with dynamic registration</td>\n</tr>\n<tr>\n<td>Distributed Workers</td>\n<td>Redis as queue backend with single consumer group</td>\n<td>Kubernetes Custom Resource Definitions (CRDs) and operator pattern</td>\n</tr>\n<tr>\n<td>Plugin Ecosystem</td>\n<td>Docker container actions with predefined entrypoints</td>\n<td>gRPC plugin protocol with versioned SDKs</td>\n</tr>\n<tr>\n<td>Persistent Caches</td>\n<td>Local directory shared via volume mounts</td>\n<td>S3-compatible object storage with content-addressable keys</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure for Extensions:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    orchestrator/          # Existing orchestrator binary\n    worker-node/           # New: distributed worker daemon\n      main.go\n    cache-service/         # New: optional cache service\n      main.go\n  \n  internal/\n    webhook/\n      providers/           # New: provider-specific parsers\n        github.go\n        gitlab.go\n        bitbucket.go       # New provider\n      verifier.go          # Extended with new methods\n    \n    queue/\n      redis_queue.go       # New: Redis implementation of Store interface\n      hybrid_queue.go      # Existing in-memory + SQLite queue\n    \n    cache/                 # New: cache abstraction\n      cache.go            # CacheStore interface\n      s3_cache.go         # S3 implementation\n      local_cache.go      # Local filesystem implementation\n    \n    plugin/               # New: plugin runtime\n      docker_action.go    # Docker container action executor\n      grpc_client.go      # gRPC plugin client (future)\n    \n    execution/\n      engine.go           # Extended to support cache restore/save\n      docker_client.go    # Extended to support plugin containers</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code for Redis Queue:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/queue/redis_queue.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> queue</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/go-redis/redis/v8</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/your-org/ci-system/internal/store</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisQueue</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stream </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisQueue</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">addr</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">password</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">db</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisQueue</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rdb </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Options</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:     addr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Password: password,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DB:       db,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Test connection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rdb.</span><span style=\"color:#B392F0\">Ping</span><span style=\"color:#E1E4E8\">(ctx).</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to connect to Redis: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create consumer group if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rdb.</span><span style=\"color:#B392F0\">XGroupCreateMkStream</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"jobs\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"ci-workers\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"BUSYGROUP Consumer Group name already exists\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create consumer group: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RedisQueue</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client: rdb,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stream: </span><span style=\"color:#9ECBFF\">\"jobs\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        group:  </span><span style=\"color:#9ECBFF\">\"ci-workers\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rq </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisQueue</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EnqueueJobRun</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Convert job to JSON</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(job)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to marshal job: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add to stream with priority as score in sorted set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use Redis Streams for ordered consumption with consumer groups</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rq.client.</span><span style=\"color:#B392F0\">XAdd</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">XAddArgs</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Stream: rq.stream,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Values: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"job\"</span><span style=\"color:#E1E4E8\">:      </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(data),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"priority\"</span><span style=\"color:#E1E4E8\">: job.Priority, </span><span style=\"color:#6A737D\">// Assume Priority field added to JobRun</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"time\"</span><span style=\"color:#E1E4E8\">:     time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UnixNano</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }).</span><span style=\"color:#B392F0\">Result</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 1: Implement DequeueJobRun using XReadGroup for consumer group pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 2: Implement MarkJobRunComplete using XACK to acknowledge processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 3: Implement recovery of pending jobs on startup using XPENDING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 4: Add retry logic for failed jobs (move to dead letter stream after N attempts)</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton for Cache Integration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/execution/engine.go (extensions)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Engine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteJob</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">env</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JobRun</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... existing setup code (create container, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // NEW: Restore caches before executing steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, cache </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> job.Config.Caches {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Generate cache key by evaluating template with job context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   Example: Replace {{ hashFiles('package-lock.json') }} with actual hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Call cacheStore.Restore(ctx, key, destPathInContainer)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: If cache hit, record in job metadata for dashboard display</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: If cache miss, continue (will save after job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Execute steps (existing logic)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, step </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> job.Config.Steps {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // ... step execution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // NEW: Save caches after successful execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> job.Status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> STATUS_SUCCEEDED {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _, cache </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> job.Config.Caches {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Skip if cache.When == \"on_failure\" and job succeeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 6: Call cacheStore.Save(ctx, key, srcPathInContainer)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 7: Handle cache save errors (log but don't fail job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... existing cleanup code</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> job, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints for Extensions:</strong></p>\n<ul>\n<li><strong>Redis Integration</strong>: Use <code>github.com/go-redis/redis/v8</code> for Go. Remember to handle connection pooling and timeouts. Use Redis Streams for ordered job processing with consumer groups.</li>\n<li><strong>S3 Cache Storage</strong>: Use <code>github.com/aws/aws-sdk-go-v2/service/s3</code> for Go. Implement multipart uploads for large caches. Set appropriate cache-control headers for expiration.</li>\n<li><strong>Docker Plugin Actions</strong>: Use the existing Docker client (<code>github.com/docker/docker/client</code>) but instead of <code>ExecCreate</code> for shell commands, use <code>ContainerCreate</code> with the plugin image and appropriate entrypoint.</li>\n<li><strong>gRPC Plugins</strong>: Use <code>google.golang.org/grpc</code> to define a <code>StepPlugin</code> service protocol buffer. Generate server and client code. Plugins can be compiled as binaries that the orchestrator launches and connects to via local socket.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint for Distributed Workers:</strong></p>\n<p>After implementing the Redis queue and worker node:</p>\n<ol>\n<li><strong>Start Redis</strong>: <code>docker run -p 6379:6379 redis</code></li>\n<li><strong>Start Orchestrator</strong>: Configure it to use Redis queue instead of hybrid queue</li>\n<li><strong>Start Worker Node</strong>: <code>go run cmd/worker-node/main.go --redis-addr localhost:6379</code></li>\n<li><strong>Trigger a Pipeline</strong>: Send a webhook or manual trigger</li>\n<li><strong>Verify</strong>: Check Redis with <code>redis-cli XINFO GROUPS jobs</code> to see pending jobs. Worker logs should show job execution. Dashboard should show job assigned to worker node ID.</li>\n</ol>\n<p><strong>Expected Behavior</strong>: Jobs are distributed to available workers. Multiple workers can run jobs concurrently. If a worker dies, its unacknowledged jobs are reassigned to other workers after timeout.</p>\n<p><strong>G. Debugging Tips for Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Cache hits are always 0%</strong></td>\n<td>Cache key generation doesn&#39;t match between runs</td>\n<td>Compare generated cache keys in job logs. Check template variable resolution.</td>\n<td>Ensure hash functions use same input files. Standardize key generation algorithm.</td>\n</tr>\n<tr>\n<td><strong>Distributed workers pick up jobs but don&#39;t execute</strong></td>\n<td>Container runtime not accessible on worker node</td>\n<td>Check worker logs for Docker API errors. Run <code>docker ps</code> on worker node.</td>\n<td>Ensure Docker socket is mounted/accessible. Configure DOCKER_HOST environment variable.</td>\n</tr>\n<tr>\n<td><strong>Plugin containers exit immediately with code 0</strong></td>\n<td>Entrypoint expects different arguments</td>\n<td>Inspect plugin container logs. Check plugin documentation for required env vars.</td>\n<td>Pass required environment variables from <code>step.with</code> mapping.</td>\n</tr>\n<tr>\n<td><strong>Redis queue grows indefinitely</strong></td>\n<td>Workers not acknowledging completed jobs</td>\n<td>Check <code>XPENDING</code> count in Redis. Look for missing <code>XACK</code> calls in worker code.</td>\n<td>Ensure <code>MarkJobRunComplete</code> calls <code>XACK</code>. Add dead-letter stream for failed jobs after retries.</td>\n</tr>\n<tr>\n<td><strong>Manual trigger API returns 400</strong></td>\n<td>Missing required parameters</td>\n<td>Check API endpoint validation logic. Review request payload structure.</td>\n<td>Ensure all required fields (repo, branch, commit) are provided. Add better validation error messages.</td>\n</tr>\n</tbody></table>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3, Milestone 4</p>\n</blockquote>\n<p>This glossary provides definitive explanations of key terms, acronyms, and domain-specific vocabulary used throughout this design document. Understanding this terminology is essential for building a shared mental model of the CI/CD Pipeline Orchestrator system. The terms are organized alphabetically, with each entry providing a clear definition and, where helpful, an illustrative example or connection to system components.</p>\n<h3 id=\"terminology-reference\">Terminology Reference</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Example / Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Artifact</strong></td>\n<td>A file or directory produced by a job that is saved for later use, such as compiled binaries, test reports, or deployment packages. Artifacts are specified in the pipeline configuration with glob patterns and uploaded to persistent storage after job completion.</td>\n<td>A job might produce artifacts: <code>dist/*.zip</code> and <code>test-results.xml</code>. These are stored and can be downloaded from the dashboard.</td>\n</tr>\n<tr>\n<td><strong>Atomic Operation</strong></td>\n<td>An operation that completes entirely or not at all, crucial for maintaining data consistency during concurrent access. In our system, this applies to queue operations (like <code>DequeueJobRun</code>) and database updates to prevent race conditions.</td>\n<td>Marking a job as <code>STATUS_RUNNING</code> and assigning it to a worker must be atomic; otherwise, two workers might execute the same job.</td>\n</tr>\n<tr>\n<td><strong>Backpressure</strong></td>\n<td>Resistance or force opposing data flow when a consumer is slower than a producer. In log streaming, if the browser cannot keep up with log events, the server must apply backpressure to avoid overwhelming memory or network buffers.</td>\n<td>The <code>LogBroadcaster</code> drops old log lines for slow clients to prevent unbounded memory growth, a form of backpressure.</td>\n</tr>\n<tr>\n<td><strong>CI/CD</strong></td>\n<td><strong>Continuous Integration and Continuous Delivery/Deployment</strong>. A set of practices and tools that automate the process of integrating code changes, running tests, and delivering/deploying software. Our system focuses on the CI aspect (integration and testing).</td>\n<td>The entire purpose of the Pipeline Orchestrator is to enable CI/CD by automating pipeline execution on code changes.</td>\n</tr>\n<tr>\n<td><strong>Consumer Group</strong></td>\n<td>A Redis Streams feature for distributing messages among multiple consumers (workers) in a load-balanced fashion, ensuring each message is processed by exactly one consumer in the group.</td>\n<td>The <code>RedisQueue</code> can use consumer groups to allow multiple workers to share the load of processing jobs from a single stream.</td>\n</tr>\n<tr>\n<td><strong>Content-Addressable Cache</strong></td>\n<td>A cache where keys are derived from a cryptographic hash (e.g., SHA-256) of the content being stored. This ensures identical content generates the same key, enabling efficient deduplication and verification.</td>\n<td>Used in persistent workflow caches: the key for <code>node_modules</code> might be a hash of the <code>package-lock.json</code> file.</td>\n</tr>\n<tr>\n<td><strong>Correlation IDs</strong></td>\n<td>Unique identifiers included in log messages and event payloads to trace a single request or pipeline run across multiple system components. Essential for debugging distributed workflows.</td>\n<td>A <code>PipelineRunID</code> is used as a correlation ID, appearing in logs from the webhook handler, queue, worker, and dashboard.</td>\n</tr>\n<tr>\n<td><strong>CORS (Cross-Origin Resource Sharing)</strong></td>\n<td>A mechanism that uses additional HTTP headers to tell a browser to allow a web application running at one origin (domain) to access selected resources from a server at a different origin.</td>\n<td>The dashboard API endpoints may need CORS headers if the frontend is served from a different domain than the backend.</td>\n</tr>\n<tr>\n<td><strong>DAG (Directed Acyclic Graph)</strong></td>\n<td>A graph structure consisting of nodes (jobs or stages) connected by directed edges (dependencies), with no cycles. Used to model the execution order and parallelism within a pipeline.</td>\n<td>The <code>BuildExecutionGraph</code> function constructs a DAG from job <code>needs</code> dependencies. <img src=\"/api/project/build-ci-system/architecture-doc/asset?path=diagrams%2Fdiag-parser-flowchart.svg\" alt=\"Pipeline Configuration Parsing and DAG Construction\"></td>\n</tr>\n<tr>\n<td><strong>DAG Visualization</strong></td>\n<td>A graphical representation of a Directed Acyclic Graph, showing nodes (jobs/stages) and arrows (dependencies). The dashboard component generates this to help developers understand pipeline structure.</td>\n<td>The <code>GetDAG</code> endpoint returns data for rendering a DAG visualization of a pipeline run&#39;s jobs and their dependencies.</td>\n</tr>\n<tr>\n<td><strong>Distributed Work Queue</strong></td>\n<td>A queue implementation using an external data store like Redis or a database that allows multiple worker processes across different machines to coordinate job processing. Enables horizontal scaling.</td>\n<td>The <code>RedisQueue</code> is a distributed work queue alternative to the in-memory <code>HybridQueue</code>.</td>\n</tr>\n<tr>\n<td><strong>Matrix Build</strong></td>\n<td>A feature that runs multiple variations of a job by computing the cartesian product of defined axis values (e.g., operating systems and Python versions). Creates parallel job instances for comprehensive testing.</td>\n<td>A job with matrix: <code>{os: [ubuntu-latest, windows-latest], python: [&#39;3.9&#39;, &#39;3.10&#39;]}</code> expands to 4 separate <code>JobRun</code> instances.</td>\n</tr>\n<tr>\n<td><strong>Persistent Failures</strong></td>\n<td>Failures that are not expected to resolve themselves if retried immediately (e.g., a syntax error in the code). Require different handling (e.g., immediate failure) than transient failures.</td>\n<td>A job step with <code>exit code 1</code> (compilation error) is a persistent failure; retrying won&#39;t help.</td>\n</tr>\n<tr>\n<td><strong>Persistent Workflow Caches</strong></td>\n<td>Shared cache storage across pipeline runs to speed up execution by reusing expensive-to-compute data (e.g., dependency downloads). Implemented with a <code>CacheStore</code> interface.</td>\n<td>A <code>CacheConfig</code> might define a cache for <code>node_modules</code> keyed by <code>package-lock.json</code> hash, restored on subsequent runs.</td>\n</tr>\n<tr>\n<td><strong>Pipeline</strong></td>\n<td>The entire automated process defined to build, test, and deploy code. Represented by a <code>PipelineConfig</code> (blueprint) and instantiated as a <code>PipelineRun</code> (execution instance).</td>\n<td>A pipeline might consist of stages: &quot;lint&quot;, &quot;test&quot;, &quot;build&quot;, &quot;deploy&quot;.</td>\n</tr>\n<tr>\n<td><strong>Plugin Ecosystem</strong></td>\n<td>A system for extending pipeline steps with reusable, packaged actions (plugins) that can be invoked in pipeline YAML. Allows community contributions and complex functionality beyond shell commands.</td>\n<td>A future extension could allow steps like <code>uses: actions/checkout@v3</code> similar to GitHub Actions.</td>\n</tr>\n<tr>\n<td><strong>Priority Scheduling</strong></td>\n<td>A method of ordering jobs in the queue based on importance, allowing critical pipelines (e.g., production deploys) to be processed before lower-priority jobs (e.g., feature branch tests).</td>\n<td>The <code>PriorityQueue</code> orders <code>Item</code> structs by <code>Priority</code> field (higher number = higher priority).</td>\n</tr>\n<tr>\n<td><strong>Queue</strong></td>\n<td>A data structure that holds pending jobs for execution in a specific order (FIFO or priority). The system uses a queue (like <code>HybridQueue</code> or <code>RedisQueue</code>) to decouple webhook reception from job execution.</td>\n<td>The <code>EnqueueRun</code> and <code>DequeueJob</code> functions manage the flow of jobs through the queue.</td>\n</tr>\n<tr>\n<td><strong>S3 Backend</strong></td>\n<td>Amazon S3 or compatible object storage service, often used for storing build artifacts or persistent workflow caches due to its durability, scalability, and cost-effectiveness.</td>\n<td>An implementation of <code>CacheStore</code> or artifact storage might use an S3 backend.</td>\n</tr>\n<tr>\n<td><strong>Server-Sent Events (SSE)</strong></td>\n<td>A unidirectional server-to-client streaming technology over standard HTTP, where the server can push text events to the browser. Used for real-time log streaming in the dashboard.</td>\n<td>The <code>SSEHandler</code> sets up an HTTP connection that streams <code>LogEvent</code> data as server-sent events.</td>\n</tr>\n<tr>\n<td><strong>Stage</strong></td>\n<td>A logical grouping of jobs within a pipeline (e.g., &#39;test&#39;, &#39;build&#39;). Stages often run sequentially, with all jobs in a stage completing before the next stage begins. Represented implicitly via job dependencies in our system.</td>\n<td>A pipeline might have stages: &quot;Build&quot; (jobs: compile, docker-build), then &quot;Test&quot; (jobs: unit, integration).</td>\n</tr>\n<tr>\n<td><strong>Step</strong></td>\n<td>An individual shell command or action within a job. A job consists of a sequence of steps executed in order. Represented by <code>StepConfig</code> (definition) and <code>StepRun</code> (execution record).</td>\n<td>A step: <code>- name: Run tests   command: go test ./...</code></td>\n</tr>\n<tr>\n<td><strong>Structured Logging</strong></td>\n<td>Logging with machine-parsable key-value pairs (e.g., JSON) instead of plain text, enabling aggregation, filtering, and analysis by log management systems. Our <code>LogEntry</code> struct supports structured logging.</td>\n<td>A log entry: <code>{&quot;timestamp&quot;: &quot;...&quot;, &quot;level&quot;: &quot;ERROR&quot;, &quot;component&quot;: &quot;worker&quot;, &quot;message&quot;: &quot;container start failed&quot;, &quot;error&quot;: &quot;no such image&quot;}</code></td>\n</tr>\n<tr>\n<td><strong>Transient Failures</strong></td>\n<td>Failures that often resolve themselves and warrant retries (e.g., network timeouts, temporary resource unavailability). The system uses <code>RetryWithBackoff</code> for such failures.</td>\n<td>A Docker daemon connection timeout is transient; retrying after a delay may succeed.</td>\n</tr>\n<tr>\n<td><strong>Webhook</strong></td>\n<td>An HTTP callback triggered by an event in a Git repository (e.g., push, pull request). Our webhook listener receives these POST requests, validates signatures, and triggers pipeline runs.</td>\n<td>GitHub sends a webhook payload with <code>event: push</code> to our <code>/webhook/github</code> endpoint.</td>\n</tr>\n<tr>\n<td><strong>Worker</strong></td>\n<td>A process that executes jobs by pulling them from the queue. Multiple workers can run concurrently, limited by a configurable pool size. Each worker calls <code>DequeueJob</code> and <code>ExecuteJob</code>.</td>\n<td>The worker pool consists of 5 worker goroutines, each processing <code>JobRun</code> items from the <code>HybridQueue</code>.</td>\n</tr>\n</tbody></table>\n<hr>\n","toc":[{"level":1,"text":"CI/CD Pipeline Orchestrator: Design Document","id":"cicd-pipeline-orchestrator-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Automated Factory Floor","id":"mental-model-the-automated-factory-floor"},{"level":3,"text":"The Core Integration Problem","id":"the-core-integration-problem"},{"level":3,"text":"Survey of Existing CI Systems","id":"survey-of-existing-ci-systems"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Goals (Must Have)","id":"goals-must-have"},{"level":3,"text":"Non-Goals (Out of Scope)","id":"non-goals-out-of-scope"},{"level":4,"text":"Architecture Decision Record: Scope Definition Strategy","id":"architecture-decision-record-scope-definition-strategy"},{"level":4,"text":"Common Pitfalls in Scope Definition","id":"common-pitfalls-in-scope-definition"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Map and Responsibilities","id":"component-map-and-responsibilities"},{"level":3,"text":"How Components Connect","id":"how-components-connect"},{"level":3,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":3,"text":"Architecture Decision Record: Monolithic vs. Microservices Deployment","id":"architecture-decision-record-monolithic-vs-microservices-deployment"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Core Types and Structures","id":"core-types-and-structures"},{"level":4,"text":"Configuration Entities (Blueprint)","id":"configuration-entities-blueprint"},{"level":4,"text":"Run Entities (Execution Records)","id":"run-entities-execution-records"},{"level":3,"text":"Relationships and Lifecycle","id":"relationships-and-lifecycle"},{"level":4,"text":"Entity Relationships","id":"entity-relationships"},{"level":4,"text":"State Lifecycles","id":"state-lifecycles"},{"level":3,"text":"Storage Strategy ADR","id":"storage-strategy-adr"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Component: Pipeline Configuration Parser (Milestone 1)","id":"component-pipeline-configuration-parser-milestone-1"},{"level":3,"text":"Mental Model: The Blueprint Interpreter","id":"mental-model-the-blueprint-interpreter"},{"level":3,"text":"Interface and API","id":"interface-and-api"},{"level":3,"text":"Internal Behavior and Algorithm","id":"internal-behavior-and-algorithm"},{"level":3,"text":"ADR: YAML vs. Custom DSL for Configuration","id":"adr-yaml-vs-custom-dsl-for-configuration"},{"level":3,"text":"Common Pitfalls: Parser","id":"common-pitfalls-parser"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Component: Job Execution Engine (Milestone 2)","id":"component-job-execution-engine-milestone-2"},{"level":3,"text":"Mental Model: The Sandboxed Playground","id":"mental-model-the-sandboxed-playground"},{"level":3,"text":"Interface and API","id":"interface-and-api"},{"level":3,"text":"Internal Behavior and Algorithm","id":"internal-behavior-and-algorithm"},{"level":3,"text":"ADR: Container vs. Process Isolation","id":"adr-container-vs-process-isolation"},{"level":3,"text":"Common Pitfalls: Execution Engine","id":"common-pitfalls-execution-engine"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Component: Webhook &amp; Queue System (Milestone 3)","id":"component-webhook-amp-queue-system-milestone-3"},{"level":3,"text":"Mental Model: The Restaurant Host and Kitchen Ticket Rail","id":"mental-model-the-restaurant-host-and-kitchen-ticket-rail"},{"level":3,"text":"Interface and API","id":"interface-and-api"},{"level":3,"text":"Internal Behavior and Algorithm","id":"internal-behavior-and-algorithm"},{"level":3,"text":"ADR: In-Memory vs. Persistent Queue Backend","id":"adr-in-memory-vs-persistent-queue-backend"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":2,"text":"Component: Web Dashboard (Milestone 4)","id":"component-web-dashboard-milestone-4"},{"level":3,"text":"Mental Model: The Airport Flight Display Board","id":"mental-model-the-airport-flight-display-board"},{"level":3,"text":"Interface and API","id":"interface-and-api"},{"level":3,"text":"Internal Behavior and Algorithm","id":"internal-behavior-and-algorithm"},{"level":3,"text":"ADR: Real-time Log Streaming: WebSockets vs. Server-Sent Events","id":"adr-real-time-log-streaming-websockets-vs-server-sent-events"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Happy Path: From Push to Pipeline Completion","id":"happy-path-from-push-to-pipeline-completion"},{"level":3,"text":"Message and Event Formats","id":"message-and-event-formats"},{"level":4,"text":"Webhook Payload (GitHub Push Event Example)","id":"webhook-payload-github-push-event-example"},{"level":4,"text":"Internal Queue Item (Item)","id":"internal-queue-item-item"},{"level":4,"text":"Log Event (LogEvent)","id":"log-event-logevent"},{"level":3,"text":"Concurrency and Parallelism Patterns","id":"concurrency-and-parallelism-patterns"},{"level":4,"text":"1. Parallel Job Execution via Worker Pool","id":"1-parallel-job-execution-via-worker-pool"},{"level":4,"text":"2. DAG-Based Dependency Scheduling","id":"2-dag-based-dependency-scheduling"},{"level":4,"text":"3. Matrix Build Parallelism","id":"3-matrix-build-parallelism"},{"level":4,"text":"4. Real-Time Log Streaming Concurrency","id":"4-real-time-log-streaming-concurrency"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: LogBroadcaster","id":"c-infrastructure-starter-code-logbroadcaster"},{"level":4,"text":"D. Core Logic Skeleton Code: HybridQueue Dequeue","id":"d-core-logic-skeleton-code-hybridqueue-dequeue"},{"level":4,"text":"E. Language-Specific Hints (Go)","id":"e-language-specific-hints-go"},{"level":4,"text":"F. Milestone Checkpoint (Milestone 3 &amp; 4 Integration)","id":"f-milestone-checkpoint-milestone-3-amp-4-integration"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Categorized Failure Modes","id":"categorized-failure-modes"},{"level":4,"text":"Parsing and Configuration Failures","id":"parsing-and-configuration-failures"},{"level":4,"text":"Execution Engine Failures","id":"execution-engine-failures"},{"level":4,"text":"Queue and Webhook System Failures","id":"queue-and-webhook-system-failures"},{"level":4,"text":"Dashboard and Network Failures","id":"dashboard-and-network-failures"},{"level":3,"text":"Recovery and Retry Strategy","id":"recovery-and-retry-strategy"},{"level":4,"text":"Retry Policies for Transient Failures","id":"retry-policies-for-transient-failures"},{"level":4,"text":"Cleanup Procedures for Stuck Resources","id":"cleanup-procedures-for-stuck-resources"},{"level":3,"text":"User-Facing Error Reporting","id":"user-facing-error-reporting"},{"level":4,"text":"Dashboard Error Display","id":"dashboard-error-display"},{"level":4,"text":"Structured Error Logging","id":"structured-error-logging"},{"level":4,"text":"Build Status Badges","id":"build-status-badges"},{"level":4,"text":"Error Communication Guidelines","id":"error-communication-guidelines"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Testing Pyramid for a CI System","id":"testing-pyramid-for-a-ci-system"},{"level":3,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":4,"text":"Milestone 1: Pipeline Configuration Parser","id":"milestone-1-pipeline-configuration-parser"},{"level":4,"text":"Milestone 2: Job Execution Engine","id":"milestone-2-job-execution-engine"},{"level":4,"text":"Milestone 3: Webhook &amp; Queue System","id":"milestone-3-webhook-amp-queue-system"},{"level":4,"text":"Milestone 4: Web Dashboard","id":"milestone-4-web-dashboard"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Common Bugs: Symptom → Cause → Fix","id":"common-bugs-symptom-cause-fix"},{"level":3,"text":"Debugging Techniques and Tools","id":"debugging-techniques-and-tools"},{"level":4,"text":"1. Container Forensics with Docker Commands","id":"1-container-forensics-with-docker-commands"},{"level":4,"text":"2. Queue and State Inspection","id":"2-queue-and-state-inspection"},{"level":4,"text":"3. Structured Logging and Correlation IDs","id":"3-structured-logging-and-correlation-ids"},{"level":4,"text":"4. Network and Endpoint Testing","id":"4-network-and-endpoint-testing"},{"level":4,"text":"5. Performance Profiling and Resource Monitoring","id":"5-performance-profiling-and-resource-monitoring"},{"level":4,"text":"6. Incremental and Isolated Testing","id":"6-incremental-and-isolated-testing"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: Structured Logger with Correlation ID","id":"c-infrastructure-starter-code-structured-logger-with-correlation-id"},{"level":4,"text":"D. Core Logic Skeleton Code: Debug Inspector for Queue","id":"d-core-logic-skeleton-code-debug-inspector-for-queue"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Low-Effort Extensions","id":"low-effort-extensions"},{"level":4,"text":"Deep Dive: Supporting Additional Git Providers","id":"deep-dive-supporting-additional-git-providers"},{"level":4,"text":"Deep Dive: Manual Pipeline Triggers","id":"deep-dive-manual-pipeline-triggers"},{"level":3,"text":"Major Enhancements","id":"major-enhancements"},{"level":4,"text":"Deep Dive: Distributed Workers Across Machines","id":"deep-dive-distributed-workers-across-machines"},{"level":4,"text":"Deep Dive: Plugin Ecosystem for Steps and Actions","id":"deep-dive-plugin-ecosystem-for-steps-and-actions"},{"level":4,"text":"Deep Dive: Persistent Workflow Caches","id":"deep-dive-persistent-workflow-caches"},{"level":3,"text":"Architectural Evolution Guidance","id":"architectural-evolution-guidance"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Terminology Reference","id":"terminology-reference"}],"title":"CI/CD Pipeline Orchestrator: Design Document","markdown":"# CI/CD Pipeline Orchestrator: Design Document\n\n\n## Overview\n\nThis document outlines the design for a custom Continuous Integration (CI) system that automatically runs defined test and build pipelines in response to code changes. The key architectural challenge is orchestrating isolated, parallel job executions reliably while providing real-time feedback to developers, balancing simplicity for educational purposes with the core patterns used in production systems like GitHub Actions.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nThis section establishes the foundational problem our CI/CD Pipeline Orchestrator aims to solve. We begin by constructing an intuitive mental model, then formalize the core integration challenges, and finally survey the landscape of existing solutions to understand where our design fits.\n\n### Mental Model: The Automated Factory Floor\n\nImagine a modern, fully automated factory that produces software. In this factory:\n\n*   **Code changes** (a `git push`, a pull request) are the **work orders** arriving at the receiving dock.\n*   Each **pipeline configuration file** (e.g., `.ci.yml`) is an **assembly line blueprint**, detailing the specific sequence of operations required to validate and package a particular product (your application).\n*   A **pipeline run** is an **active assembly line** instantiated to process a single work order.\n*   **Stages** (like `test`, `build`, `deploy`) are **logical workstations** along the assembly line, grouping related tasks. Some workstations can operate in parallel, while others must wait for upstream stations to finish.\n*   **Jobs** within a stage are the **robotic stations** at each workstation. Each job is a self-contained unit of work, like running a linter, executing a test suite, or compiling a binary.\n*   **Steps** are the **individual commands** the robot executes, such as `npm install` or `go test ./...`.\n*   **Docker containers** provide **identical, sterile workbenches** for each robotic station. Every job gets a fresh, predictable environment, ensuring no oil (state) from assembling a car (web service) contaminates the bench for assembling a motorcycle (CLI tool).\n*   The **Orchestrator** is the **factory floor manager**. It receives work orders, consults the blueprints, schedules tasks on available stations, and monitors progress.\n*   The **Dashboard** is the **factory control room's giant display**, showing the status of every active assembly line, streaming live video from each station (logs), and flashing alerts when a robot malfunctions (a job fails).\n\nThis mental model frames the CI system not as a monolithic application, but as a **coordination engine for isolated, parallelizable tasks**. The primary architectural challenge becomes reliably managing this coordination—dispatching work, isolating execution, capturing output, and reporting results—without creating a tangled mess of dependencies or becoming a single point of failure.\n\n### The Core Integration Problem\n\nBefore automated CI, integrating code changes was a manual, error-prone, and time-consuming process. A developer would:\n1.  Pull the latest `main` branch.\n2.  Run the test suite locally, hoping their environment (OS, dependency versions, environment variables) matches what other developers and production use.\n3.  Manually build artifacts, often forgetting a step documented in a wiki.\n4.  Deploy to a staging environment, crossing fingers that it works.\n\nThis approach suffers from several critical flaws:\n\n1.  **The \"It Works on My Machine\" Syndrome:** Inconsistent environments lead to bugs that appear only in production or on a colleague's computer. The lack of **isolation and reproducibility** makes diagnosing issues a nightmare.\n2.  **Feedback Delay:** Manual steps mean integration happens infrequently (perhaps once a day in a \"daily build\"), allowing bugs to accumulate and making it hard to pinpoint which change introduced a regression. Developers need **immediate feedback** on whether their change breaks the system.\n3.  **Resource Contention and Scaling:** Running long test suites on a developer's laptop ties up their machine. As the team grows, coordinating who runs what test when becomes impossible. The system needs to **manage shared resources** (compute, memory) and **scale execution** horizontally.\n4.  **Process Consistency:** Humans forget steps. An automated process defined in code (**Pipeline as Code**) ensures that every change undergoes the exact same rigorous validation, every single time.\n\nA CI system directly addresses these flaws by providing:\n*   **Environment Isolation:** Each job runs in a fresh, containerized environment defined by a Docker image, guaranteeing consistency.\n*   **Automation and Triggering:** Code changes automatically trigger the defined pipeline via webhooks, removing manual intervention.\n*   **Parallel Execution:** Independent jobs and stages run concurrently, slashing total feedback time.\n*   **Centralized Logging and Artifacts:** All output and build products are captured in a single place, accessible to the entire team.\n*   **Visibility:** Everyone can see the health of the codebase via the build status dashboard and real-time logs.\n\n### Survey of Existing CI Systems\n\nTo inform our design, we examine how popular CI systems architecturally address the core problem. They generally fall along two axes: **deployment model** (hosted vs. self-hosted) and **execution architecture** (monolithic vs. distributed).\n\n| System | Deployment Model | Execution Architecture | Key Architectural Insight | Primary Use Case |\n| :--- | :--- | :--- | :--- | :--- |\n| **GitHub Actions** | Hosted (SaaS) | Distributed, event-driven with managed runners. | Tight integration with the GitHub platform. Uses a YAML-based workflow file stored in the repository. Jobs are dispatched to scalable, managed (or self-hosted) runner machines. Employs a rich marketplace of reusable actions. | Teams deeply embedded in the GitHub ecosystem seeking a seamless, low-maintenance CI/CD solution. |\n| **Jenkins** | Self-hosted | Traditionally monolithic master/agent, plugin-based. | Extreme extensibility via plugins. The Jenkins master server handles webhooks, scheduling, and the UI. Build execution is delegated to agent nodes (which can be on different machines). Configuration is often managed via a web UI or Jenkinsfile (Pipeline as Code). | Organizations requiring maximum flexibility, control over infrastructure, and support for a wide variety of tools and languages. |\n| **GitLab CI/CD** | Hybrid (SaaS & Self-hosted) | Integrated, distributed with GitLab Runners. | Deeply integrated into the GitLab DevOps platform. Configuration is via `.gitlab-ci.yml`. Runners (which can be shared or project-specific) pick up jobs from a central coordinator. Supports auto-scaling runners with Kubernetes. | Teams using GitLab who want a unified experience from source control to deployment. |\n| **CircleCI** | Hosted (SaaS) | Distributed, container-based. | Strong focus on parallelism and test splitting to optimize speed. Uses a declarative `config.yml`. Jobs run in isolated containers on managed infrastructure or on self-hosted runner machines. | Teams prioritizing fast build times and a robust, cloud-native CI service. |\n| **Buildkite** | Hybrid (Agent-based) | Distributed, with self-hosted agents. | The control plane is hosted, but all build execution happens on customer-managed agents. This provides the reliability of a SaaS dashboard with the security and flexibility of controlling your own compute. | Organizations that need the simplicity of SaaS but must run builds on their own infrastructure due to compliance, network, or resource needs. |\n\n**Architectural Patterns Comparison:**\n\n> **Decision: Monolithic vs. Distributed Orchestration**\n>\n> *   **Context:** We must decide how to coordinate and execute pipeline jobs. A monolithic design runs everything within a single process/machine, while a distributed design separates coordination from execution.\n> *   **Options Considered:**\n>     1.  **Monolithic Server:** A single binary that handles webhooks, queuing, scheduling, and job execution (e.g., spawning Docker containers directly).\n>     2.  **Distributed with Central Queue:** A coordinator handles webhooks and scheduling, placing jobs into a persistent queue. Separate worker processes (on the same or different machines) consume jobs from the queue and execute them.\n> *   **Decision:** **Distributed with Central Queue**.\n> *   **Rationale:** Even for an educational project, this pattern is fundamental to reliable, scalable systems. It decouples components, allowing them to fail and restart independently. The webhook handler and dashboard remain responsive even if workers are bogged down with long jobs. It also cleanly maps to our mental model (factory manager vs. robotic stations).\n> *   **Consequences:** Introduces complexity of managing a queue (persistence, delivery semantics) and worker coordination. However, it provides a clear path for scaling horizontally by adding more workers and makes the system more resilient to partial failures.\n\n| Option | Pros | Cons | Our Choice? |\n| :--- | :--- | :--- | :--- |\n| **Monolithic Server** | Simpler to deploy (one binary). Easier to reason about data flow (in-process). No network overhead for job dispatch. | Components are tightly coupled. A long-running job can block the entire system (UI, webhooks). Scaling requires running entire monolith copies. Harder to isolate failures. | ❌ |\n| **Distributed with Central Queue** | Loose coupling, high availability. UI and webhooks remain responsive under load. Workers can be scaled independently. Clear separation of concerns. | More moving parts (queue, workers). Requires job serialization. Must handle network failures and queue semantics (at-least-once vs. exactly-once delivery). | ✅ |\n\nOur design takes inspiration from the distributed, queue-based pattern used by GitHub Actions (with its job queue and runner model) and Jenkins (with its master/agent separation), but aims for a **simplified, self-contained architecture** suitable for learning and deployment on a single machine initially. We will use a **persistent queue** (like Redis or an embedded database) to allow components to be restarted without losing jobs, and a **pool of worker goroutines** (or processes) that pull jobs and execute them in Docker containers.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nThis section defines the concrete boundaries of our CI/CD Pipeline Orchestrator project. Establishing clear goals and non-goals is critical for maintaining focus, managing complexity, and ensuring the implementation remains achievable within the educational context. Think of this as the **project requirements document** for our automated factory floor—it specifies what features our factory must include (the assembly lines, robotic stations, and control panels) and explicitly states what's outside our current construction plans (like building multiple factories worldwide or manufacturing custom robot parts).\n\n### Goals (Must Have)\n\nThese are the core capabilities our CI system must implement to be considered functionally complete. Each goal corresponds directly to one of the four project milestones and represents a fundamental building block of a production-ready CI system, scaled down to an educational implementation.\n\n| Goal | Corresponding Milestone | Description | Rationale & Key Capabilities |\n|------|------------------------|-------------|------------------------------|\n| **1. Pipeline Configuration Parser** | Milestone 1 | Parse YAML configuration files into executable pipeline definitions with dependency resolution. | Every CI system needs a way to define workflows. YAML is industry-standard (GitHub Actions, GitLab CI). The parser must handle: <br>• **Structured Pipeline Objects**: Convert YAML into `PipelineConfig`, `Stage`, `Job`, and `Step` representations <br>• **Environment Variable Substitution**: Resolve `$VAR` and `${VAR}` references from pipeline config, system env, and secrets <br>• **Conditional Execution**: Support `if` expressions for branch/event filtering <br>• **Matrix Build Expansion**: Generate cartesian product of axis values (e.g., `os: [linux, macos]`) into parallel job configurations <br>• **DAG Construction**: Build dependency graphs from explicit `needs:` declarations or implicit stage ordering |\n| **2. Containerized Job Execution Engine** | Milestone 2 | Execute pipeline jobs in isolated Docker containers with real-time log capture and artifact collection. | Reliable, repeatable execution is the heart of CI. Containerization provides: <br>• **Isolation**: Each job runs in a fresh container, preventing state contamination <br>• **Consistency**: Same environment across all runs (specified by Docker image) <br>• **Resource Control**: Memory/CPU limits via Docker <br>• **Step Execution**: Sequential shell command execution with exit code checking <br>• **Output Capture**: Real-time streaming of stdout/stderr to persistent storage <br>• **Artifact Collection**: Copying files matching glob patterns from container to storage |\n| **3. Webhook Handling & Job Queue System** | Milestone 3 | Receive Git webhooks, validate signatures, and manage queued pipeline runs with worker coordination. | The system must respond automatically to code changes and handle multiple concurrent executions: <br>• **Webhook Processing**: HTTP endpoint for GitHub/GitLab push, PR, and tag events with signature verification <br>• **Event Parsing**: Extract commit SHA, branch, author, and changed files from webhook payloads <br>• **Queue Management**: Reliable job queuing with at-least-once delivery semantics <br>• **Worker Pool**: Concurrent job execution with configurable parallelism limits <br>• **Priority Scheduling**: Higher priority for production deployment pipelines vs. feature branch tests |\n| **4. Basic Web Dashboard** | Milestone 4 | Provide a web interface for viewing pipeline history, real-time logs, and visualizations. | Developers need visibility into CI runs: <br>• **Build List View**: Paginated table of `PipelineRun` history with status, trigger, branch, and duration <br>• **Real-time Log Streaming**: Live tail of job output as it executes (via Server-Sent Events or WebSockets) <br>• **Status Badges**: SVG images showing pass/fail status for embedding in repository READMEs <br>• **Pipeline Visualization**: DAG rendering of stages and jobs showing dependencies and current status <br>• **Artifact Access**: Download links for artifacts generated by completed jobs |\n| **5. Single-Machine Deployment** | Cross-cutting | All components run on a single machine using local Docker and a file-based or SQLite database. | For educational simplicity and minimal infrastructure requirements: <br>• **Local Execution**: No need for cloud VMs or Kubernetes clusters <br>• **Simplified Networking**: All communication happens via localhost or Unix sockets <br>• **Unified Logging**: All logs accessible from the same machine <br>• **Easy Reset**: Clear all state by stopping processes and deleting local files |\n| **6. Core Data Model Persistence** | Cross-cutting | Store pipeline definitions, run history, job logs, and artifacts in persistent storage. | The system must retain history across restarts: <br>• **Structured Storage**: SQL tables for `PipelineConfig`, `PipelineRun`, `JobRun`, and `StepRun` entities <br>• **Log Storage**: Job output stored in files or database blobs with efficient retrieval <br>• **Artifact Storage**: File system directory structure organized by run/job identifiers <br>• **State Recovery**: Resume monitoring of in-progress runs after system restart |\n\n> **Design Insight:** The goals follow a **vertical slice architecture**—each milestone delivers a complete end-to-end flow for one aspect of the system (parsing → execution → triggering → visualization). This approach provides working software at each stage rather than implementing all layers of one component before moving to the next.\n\n### Non-Goals (Out of Scope)\n\nThese are features explicitly excluded from the current implementation. Listing non-goals prevents scope creep and clarifies that while these capabilities exist in production CI systems, they are not required for our educational version. Each exclusion is accompanied by reasoning and suggestions for how they could be added as future extensions.\n\n| Non-Goal | Reasoning | What We're Doing Instead |\n|----------|-----------|--------------------------|\n| **Distributed Workers Across Multiple Machines** | Adds significant complexity in networking, service discovery, artifact synchronization, and fault tolerance. The educational focus is on job execution logic, not distributed systems. | All workers run on the same machine as the orchestrator, sharing local Docker daemon and file system. |\n| **Complex Secret Management** | Production secret management involves encryption, key rotation, access controls, and integration with Vault/Key Management Services. This is security-focused rather than CI-core logic. | Environment variables (including \"secrets\") are passed as plaintext to containers. For educational purposes, we assume secrets are stored in the pipeline configuration or system environment variables without encryption. |\n| **Full-Featured SaaS UI** | A production-grade UI with user accounts, team management, analytics dashboards, and complex filtering would dwarf the core CI logic in implementation effort. | Basic dashboard with essential views (run list, logs, DAG visualization) using server-rendered HTML or minimal JavaScript. No authentication required. |\n| **Custom Docker Image Building** | Building Docker images as part of pipelines requires Docker-in-Docker, privileged containers, and layer caching—complex concerns orthogonal to job execution. | Jobs use pre-existing Docker images from public registries (e.g., `alpine`, `golang:latest`). The pipeline configuration specifies which image to use. |\n| **Advanced Caching Mechanisms** | Dependency caching (e.g., npm packages, Go modules) across pipeline runs requires distributed storage, cache key generation, and invalidation logic. | Each job starts with a clean container without any cross-run caching. Users can implement caching within their own scripts if needed. |\n| **Multi-Repository Pipeline Triggers** | Pipelines that trigger based on changes across multiple repositories require complex event correlation and permission models. | Pipelines are triggered only by events in the repository where the pipeline configuration file resides (single-repo scope). |\n| **Manual Approval Gates & Human Intervention** | Waiting for human approval adds stateful pause/resume logic, notification systems, and UI for approval actions. | All pipeline steps run automatically once triggered. Conditional `if` expressions can skip jobs but not pause for manual review. |\n| **Extensive Plugin Ecosystem** | Supporting third-party plugins requires plugin loading, versioning, sandboxing, and a marketplace infrastructure. | Core functionality is built directly into the system. Extensibility is limited to shell commands within job steps. |\n| **Multi-tenant Isolation** | Running pipelines for multiple organizations/teams requires resource quotas, network isolation, and authentication/authorization. | The system assumes a single tenant (e.g., one development team or individual user). |\n| **Advanced Notification Integrations** | Slack, email, webhook notifications require configurable templates, retry logic, and integration credential management. | Pipeline status is only visible via the web dashboard and status badges. No external notifications are sent. |\n\n> **Design Insight:** By explicitly declaring these non-goals, we create a **constrained design space** that allows for deep implementation of core CI concepts without being overwhelmed by peripheral features. Each non-goal represents a potential future extension point that could be built upon the solid foundation established here.\n\n#### Architecture Decision Record: Scope Definition Strategy\n\n> **Decision: Vertical Slice with Clear Boundaries**\n> - **Context**: We need to build a functional CI system for educational purposes within limited time/complexity constraints. The system must demonstrate core CI concepts while remaining implementable by learners.\n> - **Options Considered**:\n>     1. **Horizontal Layering**: Implement all data access logic first, then all business logic, then all UI logic. Provides clean separation but delays end-to-end functionality.\n>     2. **Vertical Slicing (Chosen)**: Build complete feature slices (parser → executor → webhook → dashboard) that each deliver visible value and can be tested independently.\n>     3. **Minimal Viable Product then Expand**: Build the absolute simplest CI (single job, no parallelism) then incrementally add features, risking significant refactoring at each stage.\n> - **Decision**: Adopt vertical slicing organized by milestones, with each milestone delivering a complete, testable subsystem.\n> - **Rationale**: Vertical slicing provides early feedback, maintains learner motivation with visible progress, and aligns with the project's milestone structure. Each slice can be designed with clean interfaces that allow for later expansion without breaking existing functionality.\n> - **Consequences**: \n>     - **Positive**: Working software after each milestone; natural integration testing as slices connect; clear boundaries between components.\n>     - **Negative**: Some duplication of effort (e.g., data models may need refinement across slices); later milestones may reveal early design limitations.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Horizontal Layering | Clean separation of concerns; each layer can be optimized independently; easier to swap implementations (e.g., database) | No working end-to-end system until all layers complete; harder to test integration early; less motivating for learners | No |\n| **Vertical Slicing** | **Delivers user-visible value each milestone; natural integration testing; aligns with project structure; maintains motivation** | **May require refactoring as understanding deepens; some duplication across slices** | **Yes** |\n| MVP then Expand | Starts with simplest possible system; forces prioritization of essentials | Significant refactoring as features added; early design may not accommodate later complexity well | No |\n\n#### Common Pitfalls in Scope Definition\n\n⚠️ **Pitfall: Implementing Production-Grade Features Prematurely**\n- **Description**: Learners often try to implement distributed workers, Kubernetes integration, or OAuth authentication before completing core job execution.\n- **Why it's wrong**: These features introduce massive complexity early, diverting focus from the fundamental CI algorithms (parsing, execution, queuing). The system becomes over-engineered and may never reach basic functionality.\n- **How to avoid**: Strictly adhere to the non-goals list. When tempted to add a feature, ask: \"Is this required for any of the four milestones?\" If not, document it as a future extension and move on.\n\n⚠️ **Pitfall: Neglecting the \"Basic\" in Basic Web Dashboard**\n- **Description**: Spending excessive time on polished UI animations, complex filtering, or real-time updates across all views instead of focusing on core log streaming and status display.\n- **Why it's wrong**: The dashboard's primary educational value is demonstrating real-time log streaming and pipeline visualization—not UI polish. Time spent on cosmetic features detracts from implementing core streaming mechanisms.\n- **How to avoid**: Implement the dashboard as server-rendered HTML with minimal JavaScript. Use Server-Sent Events (SSE) for log streaming as it's simpler than WebSockets. Accept that the UI will be functional but not beautiful.\n\n⚠️ **Pitfall: Over-Engineering the Parser**\n- **Description**: Creating an overly complex YAML parser with custom DSL extensions, template inheritance, or complex validation beyond what's needed for the milestone acceptance criteria.\n- **Why it's wrong**: The parser should be robust but not exhaustive. Production CI systems have evolved complex syntax over years; our educational version needs only the specified features (matrix, env vars, conditionals).\n- **How to avoid**: Implement exactly what the Milestone 1 acceptance criteria specify. Use existing YAML parsing libraries rather than writing custom parsers. Validate only required fields.\n\n### Implementation Guidance\n\n> **Technology Stack Recommendations**: The following table recommends specific technologies for each component, balancing simplicity for learners against production relevance.\n\n| Component | Simple Option (Recommended) | Advanced Option (Alternative) |\n|-----------|----------------------------|-------------------------------|\n| **Language & Runtime** | Go (static binary, excellent concurrency primitives) | Python (faster prototyping) or Rust (maximum performance) |\n| **Configuration Parser** | `gopkg.in/yaml.v3` (stable YAML parsing) | Custom parser with `github.com/goccy/go-yaml` for advanced features |\n| **Container Runtime** | Docker Engine via `github.com/docker/docker/client` | `containerd` via direct API or `podman` compatibility layer |\n| **Queue Backend** | In-memory channel with persistent SQLite backup | Redis via `github.com/go-redis/redis` for distributed scenarios |\n| **Database** | SQLite with `github.com/mattn/go-sqlite3` | PostgreSQL via `github.com/lib/pq` for concurrent access |\n| **Web Framework** | Standard library `net/http` with minimal routing | `github.com/gin-gonic/gin` for productivity features |\n| **Real-time Streaming** | Server-Sent Events (SSE) using `net/http` | WebSockets via `github.com/gorilla/websocket` |\n| **Frontend** | Server-rendered HTML with vanilla JavaScript | React/Vite SPA with TypeScript |\n\n> **File/Module Structure**: Organize the codebase by component boundaries aligned with the milestones.\n\n```\nbuild-your-own-ci/\n├── cmd/\n│   ├── server/                 # Main orchestrator server\n│   │   └── main.go\n│   ├── worker/                 # Worker process (can be same binary with flag)\n│   │   └── main.go\n│   └── dashboard/              # Dashboard web server\n│       └── main.go\n├── internal/                   # Private application code\n│   ├── config/\n│   │   ├── parser.go           # PipelineConfig parsing (Milestone 1)\n│   │   ├── dag.go              # DAG construction\n│   │   ├── matrix.go           # Matrix expansion\n│   │   └── validator.go        # YAML validation\n│   ├── executor/\n│   │   ├── docker.go           # Container execution (Milestone 2)\n│   │   ├── artifacts.go        # Artifact collection\n│   │   └── logger.go           # Log capture and streaming\n│   ├── orchestrator/\n│   │   ├── queue.go            # Job queue (Milestone 3)\n│   │   ├── scheduler.go        # Worker scheduling\n│   │   └── webhook.go          # Webhook handler\n│   ├── dashboard/\n│   │   ├── server.go           # HTTP handlers (Milestone 4)\n│   │   ├── sse.go              # Server-Sent Events for logs\n│   │   └── badges.go           # SVG badge generation\n│   ├── storage/\n│   │   ├── database.go         # SQLite interface\n│   │   ├── models.go           # PipelineRun, JobRun structs\n│   │   └── artifacts.go        # File system artifact storage\n│   └── types/                  # Shared data types\n│       ├── pipeline.go         # PipelineConfig, Stage, Job, Step\n│       └── runs.go             # PipelineRun, JobRun, StepRun\n├── pkg/                        # Public libraries (if any)\n├── web/                        # Frontend assets\n│   ├── static/\n│   │   ├── css/\n│   │   └── js/\n│   └── templates/              # Go HTML templates\n│       ├── index.html\n│       └── run_detail.html\n├── migrations/                 # SQL schema migrations\n│   └── 001_initial.sql\n├── docker-compose.yml          # For optional Redis/PostgreSQL\n├── go.mod\n└── README.md\n```\n\n> **Infrastructure Starter Code**: Basic SQLite setup and Docker client wrapper.\n\n```go\n// internal/storage/database.go\npackage storage\n\nimport (\n    \"database/sql\"\n    \"fmt\"\n    \"log\"\n    _ \"github.com/mattn/go-sqlite3\"\n)\n\ntype DB struct {\n    *sql.DB\n}\n\nfunc NewSQLiteDB(path string) (*DB, error) {\n    db, err := sql.Open(\"sqlite3\", path)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to open database: %w\", err)\n    }\n    \n    // Enable foreign keys and journaling for data integrity\n    _, err = db.Exec(`PRAGMA foreign_keys = ON; PRAGMA journal_mode = WAL;`)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to set pragmas: %w\", err)\n    }\n    \n    // Run migrations\n    if err := runMigrations(db); err != nil {\n        return nil, fmt.Errorf(\"migrations failed: %w\", err)\n    }\n    \n    return &DB{db}, nil\n}\n\nfunc runMigrations(db *sql.DB) error {\n    migrations := []string{\n        `CREATE TABLE IF NOT EXISTS pipeline_configs (\n            id TEXT PRIMARY KEY,\n            repo TEXT NOT NULL,\n            path TEXT NOT NULL,\n            raw_yaml TEXT NOT NULL,\n            parsed_config BLOB,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )`,\n        `CREATE TABLE IF NOT EXISTS pipeline_runs (\n            id TEXT PRIMARY KEY,\n            pipeline_config_id TEXT REFERENCES pipeline_configs(id),\n            status TEXT CHECK(status IN ('pending', 'running', 'succeeded', 'failed', 'cancelled', 'skipped')),\n            trigger_event TEXT,\n            commit_sha TEXT,\n            branch TEXT,\n            started_at TIMESTAMP,\n            finished_at TIMESTAMP,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )`,\n        `CREATE TABLE IF NOT EXISTS job_runs (\n            id TEXT PRIMARY KEY,\n            pipeline_run_id TEXT REFERENCES pipeline_runs(id) ON DELETE CASCADE,\n            job_name TEXT NOT NULL,\n            status TEXT CHECK(status IN ('pending', 'running', 'succeeded', 'failed', 'cancelled', 'skipped')),\n            container_id TEXT,\n            worker_id TEXT,\n            log_path TEXT,\n            started_at TIMESTAMP,\n            finished_at TIMESTAMP,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )`,\n        `CREATE TABLE IF NOT EXISTS step_runs (\n            id TEXT PRIMARY KEY,\n            job_run_id TEXT REFERENCES job_runs(id) ON DELETE CASCADE,\n            step_name TEXT NOT NULL,\n            command TEXT,\n            exit_code INTEGER,\n            output TEXT,\n            started_at TIMESTAMP,\n            finished_at TIMESTAMP\n        )`,\n        `CREATE INDEX IF NOT EXISTS idx_pipeline_runs_status ON pipeline_runs(status)`,\n        `CREATE INDEX IF NOT EXISTS idx_job_runs_pipeline_id ON job_runs(pipeline_run_id)`,\n    }\n    \n    for _, migration := range migrations {\n        _, err := db.Exec(migration)\n        if err != nil {\n            return fmt.Errorf(\"migration failed: %w\\nQuery: %s\", err, migration)\n        }\n    }\n    return nil\n}\n\n// Helper for paginated queries\nfunc (db *DB) GetPipelineRuns(limit, offset int) ([]PipelineRun, error) {\n    // TODO: Implement query with JOIN to pipeline_configs for repo info\n    rows, err := db.Query(`\n        SELECT id, status, trigger_event, commit_sha, branch, started_at, finished_at\n        FROM pipeline_runs \n        ORDER BY created_at DESC \n        LIMIT ? OFFSET ?\n    `, limit, offset)\n    if err != nil {\n        return nil, err\n    }\n    defer rows.Close()\n    \n    var runs []PipelineRun\n    for rows.Next() {\n        var run PipelineRun\n        // TODO: Scan row into struct fields\n        runs = append(runs, run)\n    }\n    return runs, nil\n}\n```\n\n```go\n// internal/executor/docker.go\npackage executor\n\nimport (\n    \"context\"\n    \"io\"\n    \"github.com/docker/docker/api/types\"\n    \"github.com/docker/docker/api/types/container\"\n    \"github.com/docker/docker/client\"\n)\n\ntype DockerExecutor struct {\n    client *client.Client\n}\n\nfunc NewDockerExecutor() (*DockerExecutor, error) {\n    cli, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n    if err != nil {\n        return nil, err\n    }\n    return &DockerExecutor{client: cli}, nil\n}\n\nfunc (d *DockerExecutor) PullImage(ctx context.Context, image string) error {\n    // TODO 1: Check if image exists locally with d.client.ImageList\n    // TODO 2: If not present, pull with d.client.ImagePull\n    // TODO 3: Stream pull progress to logs\n    return nil\n}\n\nfunc (d *DockerExecutor) CreateContainer(ctx context.Context, config *ContainerConfig) (string, error) {\n    // TODO 1: Define container.Config with Image, Env, Cmd, WorkingDir\n    // TODO 2: Define container.HostConfig with Binds (for workspace), NetworkMode\n    // TODO 3: Call d.client.ContainerCreate and return container ID\n    return \"\", nil\n}\n\nfunc (d *DockerExecutor) RunContainer(ctx context.Context, containerID string, output io.Writer) (int64, error) {\n    // TODO 1: Start container with d.client.ContainerStart\n    // TODO 2: Attach to container logs with d.client.ContainerLogs (follow=true)\n    // TODO 3: Stream logs to output writer in real-time\n    // TODO 4: Wait for container completion with d.client.ContainerWait\n    // TODO 5: Return exit code and any error\n    return 0, nil\n}\n\nfunc (d *DockerExecutor) CleanupContainer(ctx context.Context, containerID string) error {\n    // TODO 1: Force remove container with d.client.ContainerRemove\n    // TODO 2: Ignore \"container not found\" errors\n    return nil\n}\n```\n\n> **Core Logic Skeleton**: Key functions with TODO comments mapping to algorithm steps.\n\n```go\n// internal/config/parser.go\npackage config\n\n// ParseConfig parses and validates a YAML pipeline configuration file.\n// Algorithm steps correspond to the parsing flowchart (diag-parser-flowchart).\nfunc ParseConfig(yamlContent string) (PipelineConfig, error) {\n    var config PipelineConfig\n    \n    // TODO 1: Parse raw YAML into a temporary map structure using yaml.Unmarshal\n    // TODO 2: Validate required top-level fields exist (at least one job)\n    // TODO 3: Parse environment variables section (if present) into map\n    // TODO 4: Parse matrix definitions (if any) for later expansion\n    // TODO 5: Parse jobs section, for each job:\n    //   - Extract steps as []Step\n    //   - Parse job-level environment variables\n    //   - Parse 'if' conditions for conditional execution\n    //   - Parse 'needs' dependencies for DAG construction\n    // TODO 6: Validate no circular dependencies in 'needs' graph\n    // TODO 7: Expand matrix jobs by computing cartesian product of axis values\n    // TODO 8: Build DAG of job dependencies based on 'needs' or stage ordering\n    // TODO 9: Return populated PipelineConfig struct with all parsed data\n    \n    return config, nil\n}\n\n// ExpandMatrix takes a job definition with matrix axes and returns multiple job configurations.\nfunc ExpandMatrix(baseJob Job, matrixAxes map[string][]interface{}) ([]Job, error) {\n    // TODO 1: Validate matrix axes have at least one dimension with non-empty values\n    // TODO 2: Compute cartesian product of all axis values\n    // TODO 3: For each combination in the product:\n    //   - Clone the base job\n    //   - Substitute matrix values into job name (e.g., \"test-${{matrix.os}}\")\n    //   - Add matrix values as environment variables\n    //   - Apply axis-specific overrides if defined in matrix.include/exclude\n    // TODO 4: Return the list of expanded jobs\n    return []Job{}, nil\n}\n```\n\n```go\n// internal/orchestrator/queue.go  \npackage orchestrator\n\n// EnqueueRun places a pipeline run into the job queue for execution.\n// Implements priority scheduling and rate limiting.\nfunc (q *Queue) EnqueueRun(run PipelineRun) (string, error) {\n    // TODO 1: Determine priority based on branch/event (production = high priority)\n    // TODO 2: Check rate limits for the repository to prevent overload\n    // TODO 3: Generate unique queue ID for the run\n    // TODO 4: Serialize run to JSON for storage\n    // TODO 5: Store in database with status = STATUS_PENDING\n    // TODO 6: If using in-memory queue, also add to priority queue structure\n    // TODO 7: Notify available workers via channel or condition variable\n    // TODO 8: Return queue ID\n    return \"\", nil\n}\n\n// DequeueJob retrieves the next available job from the queue for a worker to process.\nfunc (q *Queue) DequeueJob(workerID string) (JobRun, error) {\n    // TODO 1: Acquire queue lock for thread-safe access\n    // TODO 2: Find highest priority run that has pending jobs\n    // TODO 3: Select a job from that run that has all dependencies satisfied\n    // TODO 4: Update job status to STATUS_RUNNING and assign workerID\n    // TODO 5: Update database record for the job\n    // TODO 6: Return the JobRun for execution\n    var job JobRun\n    return job, nil\n}\n```\n\n> **Language-Specific Hints**:\n\n- **Go Modules**: Use `go mod init build-your-own-ci` to initialize the project. Dependencies will be automatically managed.\n- **Concurrency**: Use `sync.Mutex` for protecting shared queue state, and `goroutines` with `channels` for worker coordination.\n- **Context Propagation**: Pass `context.Context` through all execution paths to enable timeouts and cancellation.\n- **Error Handling**: Use Go's multi-value returns with `error`. Wrap errors with `fmt.Errorf(\"... %w\", err)` to preserve stack traces.\n- **Docker Client**: The official Docker client library is thread-safe; create one instance and reuse it across goroutines.\n- **SQLite Concurrency**: SQLite supports concurrent reads but only one write at a time. Use `PRAGMA journal_mode=WAL` to improve concurrency.\n\n> **Milestone Checkpoint**: After completing the Goals and Non-Goals section, learners should:\n\n1. **Create the project structure** with the recommended directory layout.\n2. **Set up the database** by running the provided SQLite initialization code.\n3. **Verify Docker is accessible** by running a simple test:\n   ```bash\n   docker run --rm alpine echo \"Hello from CI\"\n   ```\n4. **Create a simple webhook endpoint** that logs incoming requests:\n   ```bash\n   curl -X POST http://localhost:8080/webhook -d '{\"test\": \"payload\"}'\n   ```\n   Should see log output in the server console.\n\n> **Debugging Tips** for scope-related issues:\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"I'm implementing Kubernetes pod scheduling instead of Docker containers\" | Scope creep into advanced orchestration | Check if feature is in Milestone 2 acceptance criteria | Revert to simple Docker client; Kubernetes is a non-goal |\n| \"Dashboard has real-time updates for everything but logs aren't streaming\" | Over-investment in UI polish before core functionality | Verify Server-Sent Events endpoint returns logs | Focus on log streaming first; add other real-time updates later |\n| \"Parser supports Jinja2 templating but matrix builds don't work\" | Implementing nice-to-have features before required ones | Compare implemented features against Milestone 1 checklist | Remove templating; implement matrix expansion per acceptance criteria |\n\n\n## High-Level Architecture\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nThis section paints the architectural blueprint of our CI/CD Pipeline Orchestrator. Before diving into the gritty details of each component, we establish a shared mental model of the system as a whole. Understanding the **responsibilities** of each major subsystem, **how they connect**, and **where code should live** is crucial for building a coherent system that scales from a single binary to a distributed service.\n\nThink of the architecture as a **modern restaurant kitchen**. The `Webhook Listener` is the host who greets customers (Git events) and takes their orders (code changes). The `Orchestrator` is the head chef who breaks down the order into individual dishes (jobs) and posts tickets (queue messages) on the kitchen rail. The `Worker Pool` comprises line cooks who each grab a ticket and prepare a dish in their own isolated workstation (container). Finally, the `Dashboard` is the expo station and dining room TV, showing the status of every order and allowing the customer (developer) to watch the cooking process in real-time. Each station is specialized, communicates through clear protocols, and together they deliver a consistent dining experience.\n\n### Component Map and Responsibilities\n\nThe system is decomposed into four core subsystems, each with a distinct responsibility and clear boundaries. This separation of concerns allows for independent development, testing, and scaling.\n\n| Component | Responsibility | Key Functions | Owned Data (In-Memory/Persistent) |\n| :--- | :--- | :--- | :--- |\n| **Webhook Listener** | Acts as the secure public gateway for Git hosting services (GitHub, GitLab). It validates incoming requests and translates external events into internal pipeline triggers. | 1. Validate webhook signatures.<br>2. Parse event payloads (push, pull request, tag).<br>3. Match events to pipeline configurations (e.g., via branch filters).<br>4. Create an initial `PipelineRun` record. | Webhook secret keys (configuration). Temporary in-memory cache for rate-limiting per repository. |\n| **Orchestrator** | The brain of the operation. It manages the lifecycle of pipeline runs, coordinates job execution order based on dependencies, and serves as the interface between the queue and the workers. | 1. Parse pipeline configuration (`ParseConfig`).<br>2. Manage the job queue (`EnqueueRun`, `DequeueJob`).<br>3. Expand matrix builds and build the execution DAG.<br>4. Dispatch `JobRun` instances to workers.<br>5. Update overall `PipelineRun` status based on job outcomes. | `PipelineConfig` cache. In-memory or persistent job queue. The authoritative state of all `PipelineRun`, `JobRun`, and `StepRun` entities (persisted in a database). |\n| **Worker Pool** | A fleet of stateless executors that carry out the actual work. Each worker claims a job and runs its steps in an isolated, containerized environment. | 1. Poll for or receive `JobRun` assignments.<br>2. Execute job steps sequentially in a Docker container (`ExecuteJob`).<br>3. Stream real-time log output (`StreamLogs`).<br>4. Collect and upload artifacts.<br>5. Report job status (success/failure) back to the Orchestrator. | Ephemeral container IDs and local log buffers during job execution. No long-term state; workers are disposable and horizontally scalable. |\n| **Dashboard** | The user-facing control panel and observation deck. It provides visibility into pipeline execution history and real-time progress, and serves status badges. | 1. Display a paginated list of pipeline runs (`GetRunHistory`).<br>2. Stream live logs from running jobs to the browser.<br>3. Generate and serve status badge SVGs.<br>4. Visualize pipeline DAGs for each run.<br>5. Provide download links for artifacts. | Cached badge SVGs and session data for active log streams. It primarily reads data owned by the Orchestrator. |\n\n![High-Level System Component Diagram](./diagrams/diag-system-component.svg)\n\n### How Components Connect\n\nThe flow of data through the system follows a unidirectional pipeline, reminiscent of an assembly line. This design minimizes coupling and makes the system easy to reason about. The primary connections are HTTP for external communication, an internal job queue for work dispatch, and a shared database for state persistence.\n\n**Primary Data Flow: The Happy Path of a Git Push**\n\n1.  **Event Ingestion:** A developer pushes code to a repository on GitHub. GitHub's servers send an HTTP POST request (a webhook) to the publicly accessible endpoint of our **Webhook Listener**.\n2.  **Trigger & Creation:** The **Webhook Listener** validates the request's signature using a shared secret, parses the JSON payload, and determines which pipeline configuration (e.g., `.ci/pipeline.yml`) should be triggered. It then calls the **Orchestrator** to create a new `PipelineRun` record, passing along the commit SHA, branch, and event type.\n3.  **Orchestration & Queuing:** The **Orchestrator** loads the relevant `PipelineConfig`, resolves environment variables, expands any matrix definitions, and constructs the execution DAG. It then creates individual `JobRun` records for each job and places references to them into the **Job Queue** via `EnqueueRun`. The `PipelineRun` status is set to `STATUS_PENDING`.\n4.  **Job Execution:** An idle **Worker** from the **Worker Pool** calls `DequeueJob` on the **Orchestrator** (or polls the queue directly). Upon receiving a `JobRun`, the worker executes it by calling `ExecuteJob`. This involves pulling the specified Docker image, creating a container, injecting environment variables and secrets, and running the job's shell commands sequentially.\n5.  **Real-Time Observation:** As the job runs, the **Worker** captures `stdout` and `stderr`. The **Dashboard** opens a Server-Sent Events (SSE) or WebSocket connection to stream logs. The worker pushes log chunks to a channel that the **Dashboard** subscribes to (or writes to a location the Dashboard can read), enabling the developer to see output in their browser in real-time.\n6.  **Status Propagation & Completion:** Upon job completion (success or failure), the **Worker** updates the `JobRun` status (`STATUS_SUCCEEDED` or `STATUS_FAILED`) and uploads any artifacts to persistent storage. The **Orchestrator** is notified and updates the overall `PipelineRun` status accordingly (e.g., if all jobs succeed, the run succeeds; if any job fails, the run fails).\n7.  **History & Inspection:** The **Dashboard** uses `GetRunHistory` to fetch completed runs from the database (via the **Orchestrator** or directly) and displays them in a list. The developer can click on any run to see its detailed DAG visualization, archived logs, and download artifacts.\n\n**Key Communication Channels:**\n\n| From → To | Protocol/Mechanism | Data Exchanged | Direction |\n| :--- | :--- | :--- | :--- |\n| Git Host → Webhook Listener | HTTP(S) POST | Webhook payload (JSON) | Inbound |\n| Webhook Listener → Orchestrator | Internal Function Call / RPC | `PipelineRun` creation request | Internal |\n| Orchestrator → Job Queue | Database insert / Pub-Sub | `JobRun` reference (ID, metadata) | Internal |\n| Worker Pool → Job Queue | Database query / Pub-Sub consume | Claimed `JobRun` reference | Internal |\n| Worker → Dashboard (Logs) | SSE / WebSocket / Shared Bus | Real-time log lines | Internal |\n| Dashboard → Orchestrator / DB | HTTP API / Direct SQL | Queries for `PipelineRun` history | Internal |\n| Worker → Docker Daemon | Docker Socket (Unix/HTTP) | Container lifecycle commands | Outbound (Infra) |\n\n> The critical insight here is the **separation of the control plane (Orchestrator) from the data plane (Worker Pool)**. The Orchestrator makes all scheduling and state decisions, while Workers are dumb executors. This allows you to scale workers horizontally without complex coordination, and even run workers on different machines, as long as they can communicate with the central Orchestrator and Docker daemon.\n\n### Recommended File/Module Structure\n\nA well-organized codebase mirrors the architectural boundaries. For our Go implementation, we adopt a standard project layout centered around an `internal` package, which prevents external imports of our core logic. Each major component gets its own subpackage.\n\n```\nbuild-your-own-ci/\n├── cmd/                           # Application entry points\n│   ├── ci-server/                 # Main server binary (Orchestrator + Webhook Listener + Dashboard)\n│   │   └── main.go\n│   └── ci-worker/                 # Optional separate worker binary\n│       └── main.go\n├── internal/                      # Private application code\n│   ├── config/                    # Pipeline configuration parsing (Milestone 1)\n│   │   ├── parser.go              # `ParseConfig`, YAML unmarshaling\n│   │   ├── dag.go                 # DAG construction, dependency resolution\n│   │   ├── matrix.go              # Matrix expansion logic\n│   │   ├── envsubst.go            # Environment variable substitution\n│   │   └── config_test.go\n│   ├── orchestrator/              # Core orchestration logic (Milestone 1, 3)\n│   │   ├── orchestrator.go        # Manages runs, queue interface\n│   │   ├── queue/                 * Abstract queue interface and implementations\n│   │   │   ├── queue.go           # Interface (EnqueueRun, DequeueJob)\n│   │   │   ├── memory.go          # In-memory channel-based queue\n│   │   │   └── redis.go           # Redis-backed queue\n│   │   └── store/                 * Abstract storage interface and implementations\n│   │       ├── store.go           # Interface for PipelineRun, JobRun, StepRun\n│   │       ├── memory.go          # In-memory store (for dev)\n│   │       └── sqlite.go          # SQLite persistence\n│   ├── worker/                    # Job execution engine (Milestone 2)\n│   │   ├── executor.go            # `ExecuteJob`, container management\n│   │   ├── docker_client.go       # Wrapper around Docker SDK\n│   │   ├── logstream.go           # `StreamLogs`, real-time output handling\n│   │   ├── artifact.go            # Artifact collection and upload\n│   │   └── pool.go                * Worker pool management\n│   ├── webhook/                   # Webhook handling (Milestone 3)\n│   │   ├── handler.go             # `HandleWebhook`, HTTP handler\n│   │   ├── github.go              # GitHub payload parsing & validation\n│   │   ├── gitlab.go              # GitLab payload parsing & validation\n│   │   └── verifier.go            # Signature verification\n│   └── dashboard/                 # Web dashboard (Milestone 4)\n│       ├── server.go              # HTTP server, route definitions\n│       ├── handlers.go            # `GetRunHistory`, badge handler, DAG data endpoint\n│       ├── sse.go                 # Server-Sent Events for log streaming\n│       └── assets/                # Static HTML, JS, CSS\n│           ├── index.html\n│           └── app.js\n├── pkg/                           # Public, reusable libraries (if any)\n│   └── ciutils/                   # E.g., shared logging, constants\n├── go.mod\n├── go.sum\n└── docker-compose.yml             # For local Redis, Docker-in-Docker setup\n```\n\n**Package Dependencies Flow:** The `cmd/ci-server` imports from `internal/webhook`, `internal/orchestrator`, and `internal/dashboard`. The `internal/orchestrator` imports `internal/config` and `internal/worker` (for job definitions and execution interface). The `internal/worker` is the only package that directly interacts with the Docker SDK. This structure enforces a clear dependency hierarchy and prevents circular imports.\n\n### Architecture Decision Record: Monolithic vs. Microservices Deployment\n\n> **Decision: Deploy as a Single Monolithic Binary with Optional Worker Separation**\n>\n> - **Context**: We are building a learning-focused CI system that must be simple to run locally and deploy. We need to balance conceptual clarity with the ability to scale components independently.\n> - **Options Considered**:\n>     1.  **Pure Monolith**: A single binary containing the Webhook Listener, Orchestrator, Worker Pool, and Dashboard. All components run in one process.\n>     2.  **Full Microservices**: Separate binaries (and possibly containers) for each major component, communicating via network calls (gRPC/HTTP).\n>     3.  **Hybrid Monolith with Extractable Workers**: A primary server binary (Orchestrator, Listener, Dashboard) and a separate, optional worker binary. The worker connects back to the main server's queue and API.\n> - **Decision**: We choose the **Hybrid** approach (Option 3). The primary `ci-server` will embed the Webhook Listener, Orchestrator, and Dashboard. The Worker Pool can be run either as goroutines within the same process (simple mode) or as separate `ci-worker` processes (scalable mode).\n> - **Rationale**: A pure monolith is the simplest for learners to grasp and run, but it conflates the very different concerns of job scheduling and job execution. The hybrid model maintains logical separation in the code (`internal/worker` package) while offering deployment flexibility. Learners can start with the all-in-one server and later extract workers without rewriting core logic. It mirrors the pattern of real-world systems like Jenkins (controller/agent) or GitHub Actions (runner).\n> - **Consequences**:\n>     - **Positive**: Simplifies initial development and debugging (single binary). Clear component boundaries in code. Allows scaling workers horizontally by launching more `ci-worker` processes.\n>     - **Negative**: Introduces a slight operational complexity for the multi-process deployment. Requires the queue and storage to be accessible from separate processes (i.e., cannot use simple in-memory channels).\n\n| Option | Pros | Cons | Chosen? |\n| :--- | :--- | :--- | :--- |\n| **Pure Monolith** | Ultimate simplicity for running and debugging. No inter-process communication. | Components cannot scale independently. A long-running job can block the webhook handler. Blurs architectural boundaries. | No |\n| **Full Microservices** | Maximum scalability and fault isolation. Clear deployment boundaries. | High operational and development complexity. Network latency and failure modes add significant learning overhead. | No |\n| **Hybrid Monolith** | Good separation of concerns in code. Flexible deployment: all-in-one or scaled workers. Mimics production patterns. | Slightly more complex than a pure monolith. Requires shared, persistent queue/storage for multi-process mode. | **Yes** |\n\n### Implementation Guidance\n\nThis section provides concrete starting points for organizing and implementing the high-level architecture in Go.\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option (Starting Point) | Advanced Option (For Extension) |\n| :--- | :--- | :--- |\n| **Web Transport** | `net/http` with Gorilla Mux or `http.ServeMux` | `chi` router for middleware, `gRPC` for internal RPC |\n| **Queue Backend** | In-memory channel (`chan interface{}`) with a wrapper | `Redis` (go-redis), `PostgreSQL` with SKIP LOCKED, `RabbitMQ` |\n| **Data Persistence** | In-memory map with `sync.RWMutex` (for dev) | `SQLite` (go-sqlite3) for single-binary persistence, `PostgreSQL` |\n| **Docker Client** | Official Docker SDK for Go (`github.com/docker/docker/client`) | Direct use of `containerd` API for lower-level control |\n| **Real-time Logs** | Server-Sent Events (SSE) via `http.ResponseWriter` | `WebSocket` (gorilla/websocket) for bidirectional communication |\n| **Frontend** | Plain HTML/JavaScript with vanilla JS or Alpine.js | React/Vite SPA with a separate build process |\n\n**B. Recommended File/Module Structure Starter**\n\nThe provided structure above can be initialized with the following commands and stub files.\n\nFirst, create the module and base directories:\n```bash\nmkdir -p build-your-own-ci/cmd/ci-server build-your-own-ci/cmd/ci-worker\nmkdir -p build-your-own-ci/internal/config internal/orchestrator/queue internal/orchestrator/store\nmkdir -p build-your-own-ci/internal/worker internal/webhook internal/dashboard/assets\ncd build-your-own-ci\ngo mod init github.com/yourusername/build-your-own-ci\n```\n\n**C. Infrastructure Starter Code: Core Data Types and Store Interface**\n\nTo ensure consistency across components, we define the core data structures and a generic storage interface in a shared internal package. Place this in `internal/types/types.go`.\n\n```go\npackage types\n\nimport (\n    \"time\"\n)\n\n// PipelineConfig holds the parsed and validated pipeline configuration.\ntype PipelineConfig struct {\n    // Raw YAML source (or file path). Useful for debugging.\n    Source         string\n    // Map of job names to their definition.\n    Jobs           map[string]JobConfig\n    // Global environment variables defined in the pipeline.\n    Environment    map[string]string\n    // Matrix definitions for parallel job expansion.\n    MatrixAxes     map[string][]interface{}\n}\n\n// JobConfig defines a single job within a pipeline.\ntype JobConfig struct {\n    Name         string\n    RunsOn       string   // Docker image, e.g., \"ubuntu:latest\"\n    Steps        []StepConfig\n    Needs        []string // Dependencies on other jobs\n    Environment  map[string]string\n    Matrix       map[string]interface{} // Per-job matrix overrides\n}\n\n// StepConfig defines a single shell command or action.\ntype StepConfig struct {\n    Name    string\n    Command string\n    Env     map[string]string\n    If      string // Conditional expression\n}\n\n// PipelineRun represents a single execution instance of a pipeline.\ntype PipelineRun struct {\n    ID        string\n    Status    string // e.g., STATUS_PENDING\n    Trigger   string // EVENT_PUSH, EVENT_PULL_REQUEST, EVENT_TAG\n    CommitSHA string\n    Branch    string\n    CreatedAt time.Time\n    UpdatedAt time.Time\n    StartedAt *time.Time\n    FinishedAt *time.Time\n    Config    *PipelineConfig // Reference to the pipeline config\n}\n\n// JobRun represents a single execution instance of a job.\ntype JobRun struct {\n    ID           string\n    PipelineRunID string\n    JobName      string\n    Status       string\n    AssignedWorker string // Worker ID that claimed this job\n    ContainerID  string\n    LogKey       string // Reference to where logs are stored (e.g., file path, object storage key)\n    ArtifactKeys []string\n    StartedAt    *time.Time\n    FinishedAt   *time.Time\n    Env          map[string]string // Resolved environment for this specific run\n}\n\n// StepRun represents the execution of a single step.\ntype StepRun struct {\n    ID        string\n    JobRunID  string\n    StepName  string\n    Command   string\n    ExitCode  int\n    Output    string // May be truncated; full logs via LogKey\n    StartedAt time.Time\n    Duration  time.Duration\n}\n\n// Store defines the interface for persistent storage of runs.\n// Implementations can be in-memory, SQLite, PostgreSQL, etc.\ntype Store interface {\n    // PipelineRun operations\n    CreatePipelineRun(run *PipelineRun) error\n    GetPipelineRun(id string) (*PipelineRun, error)\n    UpdatePipelineRunStatus(id string, status string) error\n    ListPipelineRuns(limit, offset int) ([]*PipelineRun, error)\n\n    // JobRun operations\n    CreateJobRun(job *JobRun) error\n    GetJobRun(id string) (*JobRun, error)\n    UpdateJobRunStatus(id string, status string) error\n    UpdateJobRunLogKey(id string, logKey string) error\n    ListJobRunsForPipeline(pipelineRunID string) ([]*JobRun, error)\n\n    // StepRun operations\n    CreateStepRun(step *StepRun) error\n    UpdateStepRunOutput(id string, output string, exitCode int, duration time.Duration) error\n    ListStepRunsForJob(jobRunID string) ([]*StepRun, error)\n\n    Close() error\n}\n```\n\n**D. Core Logic Skeleton: Main Server Entry Point**\n\nThe `cmd/ci-server/main.go` ties all components together. This is a skeleton showing the orchestration.\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"log\"\n    \"net/http\"\n    \"os\"\n    \"os/signal\"\n    \"syscall\"\n\n    \"github.com/yourusername/build-your-own-ci/internal/config\"\n    \"github.com/yourusername/build-your-own-ci/internal/dashboard\"\n    \"github.com/yourusername/build-your-own-ci/internal/orchestrator\"\n    \"github.com/yourusername/build-your-own-ci/internal/orchestrator/queue/memory\"\n    \"github.com/yourusername/build-your-own-ci/internal/orchestrator/store/sqlite\"\n    \"github.com/yourusername/build-your-own-ci/internal/webhook\"\n    \"github.com/yourusername/build-your-own-ci/internal/worker\"\n)\n\nfunc main() {\n    ctx, cancel := context.WithCancel(context.Background())\n    defer cancel()\n\n    // 1. Initialize storage (using SQLite for simplicity)\n    store, err := sqlite.NewStore(\"ci_pipeline.db\")\n    if err != nil {\n        log.Fatalf(\"Failed to initialize store: %v\", err)\n    }\n    defer store.Close()\n\n    // 2. Initialize the job queue (in-memory for single process)\n    jobQueue := memory.NewQueue()\n\n    // 3. Create the Orchestrator, injecting dependencies\n    orch := orchestrator.NewOrchestrator(store, jobQueue)\n\n    // 4. Create the Webhook Handler, passing a callback to the Orchestrator\n    webhookHandler := webhook.NewHandler(store, func(event webhook.Event) error {\n        // TODO 1: Load the pipeline config for the repository/event from a file (e.g., .ci/pipeline.yml)\n        // TODO 2: Call orch.CreatePipelineRun with the config and event details\n        return nil\n    })\n\n    // 5. Create the Dashboard server\n    dashboardServer := dashboard.NewServer(store, orch)\n\n    // 6. Optionally, start an in-process worker pool\n    workerPool := worker.NewPool(2, jobQueue, store) // 2 concurrent workers\n    go workerPool.Run(ctx)\n\n    // 7. Set up HTTP routing\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/webhook/github\", webhookHandler.HandleGitHubWebhook)\n    mux.HandleFunc(\"/api/runs\", dashboardServer.HandleListRuns)\n    mux.HandleFunc(\"/api/runs/\", dashboardServer.HandleGetRun)\n    mux.HandleFunc(\"/logs/\", dashboardServer.HandleStreamLogs)\n    mux.Handle(\"/\", http.FileServer(http.Dir(\"./internal/dashboard/assets\")))\n\n    srv := &http.Server{\n        Addr:    \":8080\",\n        Handler: mux,\n    }\n\n    // 8. Graceful shutdown\n    go func() {\n        sig := make(chan os.Signal, 1)\n        signal.Notify(sig, syscall.SIGINT, syscall.SIGTERM)\n        <-sig\n        log.Println(\"Shutting down server...\")\n        srv.Shutdown(ctx)\n        cancel()\n    }()\n\n    log.Println(\"Server starting on :8080\")\n    if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {\n        log.Fatalf(\"Server failed: %v\", err)\n    }\n}\n```\n\n**E. Language-Specific Hints**\n\n*   **Dependency Injection**: Use interfaces (like `Store` and `Queue`) liberally. Pass dependencies as parameters to constructors (as shown above). This makes components testable and interchangeable.\n*   **Context Propagation**: Use `context.Context` throughout for cancellation and timeouts, especially in long-running operations like job execution and log streaming.\n*   **Error Handling**: In Go, handle errors immediately where they occur. For the orchestrator, consider defining custom error types (e.g., `ErrConfigNotFound`, `ErrJobDependencyCycle`) for better error reporting.\n*   **Concurrency**: Use `sync.WaitGroup` to coordinate goroutines, like waiting for all workers to finish on shutdown. Use `sync.RWMutex` to protect in-memory data structures in the simple queue and store implementations.\n\n\n## Data Model\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nThis section defines the core data structures that represent the **blueprint**, **execution records**, and **runtime state** of our CI/CD Pipeline Orchestrator. Just as a construction project requires architectural drawings (blueprints), work orders (execution plans), and daily progress reports (runtime logs), our system needs three fundamental categories of data: configuration models that define what to do, run models that track ongoing and completed executions, and the supporting infrastructure that persists this information reliably. This data model serves as the shared language between all system components, ensuring the Webhook Listener, Orchestrator, Worker Pool, and Dashboard all understand the same concepts of pipelines, jobs, and steps.\n\n### Core Types and Structures\n\nThink of the data model as a **family tree of execution**. The `PipelineConfig` is the founding ancestor—a static definition. Each time it's triggered, it spawns a `PipelineRun` (a child), which then gives birth to multiple `JobRun` grandchildren. Each `JobRun` further produces `StepRun` great-grandchildren, representing the finest granularity of work. This hierarchy ensures traceability from a single shell command back to the original code change that initiated it.\n\nThe following tables define each entity with all its fields, their types, and their purpose. These exact names and structures must be used consistently across the codebase.\n\n#### Configuration Entities (Blueprint)\n\nThese structures represent the parsed and validated pipeline definition. They are **immutable** after parsing and define the \"what\" and \"in what order\" of the work.\n\n**Table: `PipelineConfig` (The Master Blueprint)**\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `Source` | `string` | The source location of the original configuration file (e.g., repository URL and path). Useful for debugging and auditing. |\n| `Jobs` | `map[string]JobConfig` | A dictionary of all jobs defined in the pipeline, keyed by the job's unique name. This map is the primary definition of work units. |\n| `Environment` | `map[string]string` | Global environment variables that should be available to all jobs in the pipeline, unless overridden at the job or step level. |\n| `MatrixAxes` | `map[string][]interface{}` | **Optional.** Defines axes for matrix builds. Each key is an axis name (e.g., `go_version`), and the value is a list of possible values (e.g., `[\"1.19\", \"1.20\"]`). The orchestrator will compute the cartesian product to expand into multiple job instances. |\n\n**Table: `JobConfig` (An Individual Work Unit Blueprint)**\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `Name` | `string` | The unique identifier for this job within the pipeline (e.g., `\"test-linux\"`). |\n| `RunsOn` | `string` | The Docker image or a runner label specifying the execution environment (e.g., `\"ubuntu:22.04\"` or `\"linux-arm64\"`). |\n| `Steps` | `[]StepConfig` | An ordered list of shell commands or actions to execute within the job's container. Execution stops if any step fails. |\n| `Needs` | `[]string` | **Optional.** A list of job names that must complete successfully before this job can start. This defines the dependency graph (DAG). If empty, the job can start immediately. |\n| `Environment` | `map[string]string` | **Optional.** Job-specific environment variables, which merge with and override pipeline-level environment variables. |\n| `Matrix` | `map[string]interface{}` | **Optional.** A job-specific matrix definition. If present, this job will be expanded into multiple instances using the cartesian product of the defined axes. This overrides any pipeline-level `MatrixAxes`. |\n\n**Table: `StepConfig` (A Single Command Instruction)**\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `Name` | `string` | A human-readable label for this step (e.g., `\"Run unit tests\"`). Used for logging and UI display. |\n| `Command` | `string` | The shell command to execute (e.g., `\"go test ./...\"`). |\n| `Env` | `map[string]string` | **Optional.** Step-specific environment variables, which merge with and override job and pipeline-level variables. |\n| `If` | `string` | **Optional.** A conditional expression that determines if the step should run. Evaluated at runtime. Common expressions check branch name (`github.ref == 'refs/heads/main'`) or event type. |\n\n#### Run Entities (Execution Records)\n\nThese structures represent a **specific instance** of pipeline execution. They are **mutable**, tracking the live state, timings, and results as work progresses.\n\n**Table: `PipelineRun` (A Single Pipeline Execution Instance)**\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `ID` | `string` | A globally unique identifier for this run (e.g., a UUID). Used in URLs and for cross-referencing. |\n| `Status` | `string` | The current high-level state of the pipeline run. Must be one of the `STATUS_` constants (e.g., `STATUS_RUNNING`). |\n| `Trigger` | `string` | The webhook event type that initiated this run (e.g., `EVENT_PUSH`, `EVENT_PULL_REQUEST`). |\n| `CommitSHA` | `string` | The full Git commit hash that triggered the pipeline (e.g., `\"a1b2c3d...\"`). |\n| `Branch` | `string` | The Git branch name from the triggering event (e.g., `\"feature/login\"`). |\n| `CreatedAt` | `time.Time` | The timestamp when the `PipelineRun` was first created (when the webhook was processed). |\n| `UpdatedAt` | `time.Time` | The timestamp of the last status update. Useful for detecting stalled runs. |\n| `StartedAt` | `*time.Time` | **Optional.** The timestamp when the first job in the pipeline began execution. `nil` if the pipeline hasn't started. |\n| `FinishedAt` | `*time.Time` | **Optional.** The timestamp when the final job in the pipeline completed (successfully or not). `nil` if the pipeline is still running. |\n| `Config` | `*PipelineConfig` | A **pointer** to the immutable `PipelineConfig` that defines this run's structure. This is a denormalization for convenience; the config could be stored separately and linked by a foreign key. |\n\n**Table: `JobRun` (An Instance of a Job Execution)**\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `ID` | `string` | A unique identifier for this job execution (e.g., a UUID). |\n| `PipelineRunID` | `string` | The ID of the parent `PipelineRun`. Establishes the \"belongs to\" relationship. |\n| `JobName` | `string` | The name of the job as defined in the original `JobConfig` (e.g., `\"test-go-1.19\"`). This is the key to look up the configuration in `PipelineConfig.Jobs`. |\n| `Status` | `string` | The current state of the job. Must be one of the `STATUS_` constants. |\n| `AssignedWorker` | `string` | **Optional.** The identifier of the worker process that picked up this job from the queue. Used for monitoring and cleanup. |\n| `ContainerID` | `string` | **Optional.** The Docker container ID where the job is executing (or executed). Crucial for log retrieval and forced cleanup. |\n| `LogKey` | `string` | **Optional.** A reference to where the job's combined stdout/stderr logs are stored (e.g., a file path or an object storage key). |\n| `ArtifactKeys` | `[]string` | **Optional.** A list of references to artifacts produced by this job (e.g., paths in persistent storage). |\n| `StartedAt` | `*time.Time` | **Optional.** The timestamp when the job's container started and the first step began. |\n| `FinishedAt` | `*time.Time` | **Optional.** The timestamp when the job's final step completed. |\n| `Env` | `map[string]string` | The final, fully-resolved environment variables for this specific job run, including pipeline, job, matrix, and system variables. This is a snapshot used for execution. |\n\n**Table: `StepRun` (A Record of a Single Command Execution)**\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `ID` | `string` | A unique identifier for this step execution. |\n| `JobRunID` | `string` | The ID of the parent `JobRun`. |\n| `StepName` | `string` | The name of the step from the `StepConfig`. |\n| `Command` | `string` | The actual shell command that was executed, after any environment variable substitution. |\n| `ExitCode` | `int` | The exit code returned by the shell after executing the command. `0` indicates success; non-zero indicates failure. |\n| `Output` | `string` | **Optional.** The combined stdout and stderr output of the command. For very large outputs, this may be truncated, and the full logs should be read from the `JobRun.LogKey`. |\n| `StartedAt` | `time.Time` | The timestamp when the command began execution. |\n| `Duration` | `time.Duration` | How long the command took to execute. Can be calculated as `FinishedAt - StartedAt`. |\n\n### Relationships and Lifecycle\n\n> The data model relationships form a **composition hierarchy**: a `PipelineRun` **has many** `JobRun`s, and a `JobRun` **has many** `StepRun`s. This is a classic one-to-many relationship that can be efficiently represented in a relational database with foreign keys or in a document store with nested documents. The configuration (`PipelineConfig`, `JobConfig`, `StepConfig`) is referenced but not owned by the runs, as the same configuration can spawn many runs over time.\n\n![Data Model Relationship Diagram](./diagrams/diag-data-model.svg)\n\n#### Entity Relationships\n\nThe diagram above illustrates the following key relationships:\n\n1.  **`PipelineRun` to `JobRun` (One-to-Many):** A single pipeline execution consists of multiple job executions. The `JobRun.PipelineRunID` field is the foreign key linking back. When a pipeline is triggered, the orchestrator creates one `JobRun` for each job defined in the `PipelineConfig.Jobs` map, after applying matrix expansion. The `JobRun.JobName` field matches a key in that map.\n2.  **`JobRun` to `StepRun` (One-to-Many):** Each job execution sequences through its defined steps. The `StepRun.JobRunID` links to its parent. The worker creates a `StepRun` record just before executing each command in `JobConfig.Steps`.\n3.  **`PipelineRun` references `PipelineConfig` (One-to-One):** The `PipelineRun.Config` field points to the full configuration used for that run. This is a **denormalization** for performance and simplicity, allowing any component to inspect the run's definition without a separate lookup. In a persistent storage strategy, we might store the config as a serialized JSON blob alongside the run record or in a separate table linked by ID.\n4.  **`JobRun` references `JobConfig` (Indirect):** `JobRun` does not directly store a `JobConfig`. Instead, it stores the `JobName`, which is used to look up the configuration within the parent `PipelineRun.Config.Jobs` map. This ensures consistency—the run always uses the configuration that was active at the time of its creation.\n\n#### State Lifecycles\n\nEach run entity has a well-defined lifecycle, moving through a series of states. These states are not just for display; they drive the system's logic (e.g., a job with `STATUS_SUCCEEDED` allows its dependent jobs to start).\n\n**PipelineRun State Flow:**\nThe pipeline's overall status is typically derived from the states of its constituent jobs. A common aggregation logic is:\n- If any job has `STATUS_FAILED`, the pipeline is `STATUS_FAILED`.\n- If all jobs are `STATUS_SUCCEEDED`, the pipeline is `STATUS_SUCCEEDED`.\n- If all jobs are either `STATUS_SUCCEEDED` or `STATUS_SKIPPED`, the pipeline is `STATUS_SUCCEEDED`.\n- If any job is `STATUS_RUNNING`, the pipeline is `STATUS_RUNNING`.\n- Otherwise, it's `STATUS_PENDING`.\n\n**JobRun State Machine:**\nThe job state machine is more explicit and is managed by the orchestrator and worker components.\n\n![Job Run State Machine](./diagrams/diag-job-state-machine.svg)\n\n**Table: JobRun State Transitions**\n| Current State | Event | Next State | Action Taken by System |\n|---------------|-------|------------|------------------------|\n| `STATUS_PENDING` | `start` (job dequeued by a worker) | `STATUS_RUNNING` | Worker updates `JobRun.Status`, sets `StartedAt`, and begins container setup. |\n| `STATUS_RUNNING` | `finish(success)` (all steps exited with code 0) | `STATUS_SUCCEEDED` | Worker updates status, sets `FinishedAt`, collects artifacts, and cleans up the container. |\n| `STATUS_RUNNING` | `finish(error)` (any step failed, or container error) | `STATUS_FAILED` | Worker updates status, sets `FinishedAt`, saves logs, and cleans up the container. |\n| `STATUS_PENDING` or `STATUS_RUNNING` | `cancel` (user request or timeout) | `STATUS_CANCELLED` | Orchestrator or worker sends SIGKILL to container, updates status, and performs cleanup. |\n| `STATUS_PENDING` | `skip` (conditional `if` expression evaluated to false) | `STATUS_SKIPPED` | Orchestrator evaluates condition before enqueuing job; marks it skipped without worker assignment. |\n| Any state | `retry` (manual user action) | `STATUS_PENDING` | A new `JobRun` is typically created, but the old one retains its final state for history. |\n\n> **Key Insight:** The `STATUS_SKIPPED` state is special. It is determined during the pipeline's initial \"planning\" phase by the orchestrator, based on the `StepConfig.If` or `JobConfig` conditions. A skipped job never consumes a worker or a container, but it remains in the run history for a complete audit trail.\n\n### Storage Strategy ADR\n\n> **Decision: Use SQLite as the Primary Persistent Store for Run History and Job Queue**\n>\n> - **Context**: We need a storage mechanism for pipeline and job run records, their statuses, logs, and artifacts metadata. This storage must support concurrent access from multiple components (Webhook Listener, Orchestrator, Workers, Dashboard) and persist across system restarts. The project emphasizes simplicity for educational purposes and ease of setup, but also requires reliability for core queue operations to prevent job loss.\n> - **Options Considered**:\n>     1.  **SQLite (Single-file relational database)**\n>     2.  **PostgreSQL (Dedicated relational database server)**\n>     3.  **In-memory storage with periodic file backup (e.g., gob encoding)**\n> - **Decision**: Use SQLite as the primary store for all run-related entities (`PipelineRun`, `JobRun`, `StepRun`). The job queue will also be backed by a SQLite table, using transactional `SELECT ... FOR UPDATE SKIP LOCKED` patterns for reliable, concurrent job dequeuing.\n> - **Rationale**:\n>     1.  **Simplicity and Zero Dependencies**: SQLite requires no separate server process or setup. The entire database is a single file, making deployment and backup trivial. This aligns perfectly with the project's educational goal of focusing on CI/CD logic rather than infrastructure.\n>     2.  **Adequate Concurrency for Learning Scale**: While not designed for high-write, distributed scenarios, SQLite's write-ahead log (WAL) mode supports multiple concurrent readers and one writer efficiently. For a learning project with a modest number of concurrent jobs (e.g., <10 workers), this is more than sufficient. It correctly handles the transactional semantics needed for the job queue.\n>     3.  **Relational Model Fit**: Our data model is inherently relational (run -> job -> step). SQLite allows us to define schemas, foreign keys, and perform efficient joins (e.g., fetching all jobs for a pipeline run for the dashboard), which would be more complex in a simple file-based store.\n>     4.  **Persistence Guarantees**: Unlike a pure in-memory store, SQLite provides ACID transactions. This ensures that once a webhook is processed and a `PipelineRun` is committed, it survives a system crash. This is a non-negotiable requirement for a reliable CI system.\n> - **Consequences**:\n>     - **Positive**: Drastically simplifies the development and testing environment. Learners can run the entire system with a single binary and a database file. The system becomes self-contained.\n>     - **Negative**: Becomes a bottleneck at very high scale (hundreds of concurrent job starts per minute). The single-writer limitation can cause contention. This is an acceptable trade-off for the project's scope.\n>     - **Mitigation**: The design isolates storage behind the `Store` interface. If scaling is needed later, the implementation can be swapped for PostgreSQL or a distributed queue like Redis without changing the core application logic.\n\n**Table: Storage Options Comparison**\n| Option | Pros | Cons | Suitability for Project |\n|--------|------|------|-------------------------|\n| **SQLite** | Zero setup, single file, ACID transactions, good enough concurrency in WAL mode. | Single-writer bottleneck, not distributed, less performant under extreme write load. | **CHOSEN.** Excellent fit for simplicity, reliability, and educational goals. |\n| **PostgreSQL** | High concurrency, robust, scalable, full-featured SQL. | Requires running a separate database server, adds deployment complexity. | Overkill for the initial learning project. A good choice for a \"Phase 2\" scaling enhancement. |\n| **In-Memory + File Backup** | Extremely fast, simple to implement for basic reads/writes. | Data loss on crash unless complex flushing logic is added. Poor support for concurrent access and complex queries. | Unreliable for a core system component. Could be used for a non-persistent cache, but not for primary storage. |\n\n**Common Pitfalls: Data Storage**\n\n⚠️ **Pitfall: Ignoring Database Migrations**\n- **Description**: Hardcoding the database schema creation (`CREATE TABLE ...`) in the application startup logic. When you need to add a new field (like `Branch` to `PipelineRun`), you either drop all existing data or face complex manual upgrade steps.\n- **Why it's wrong**: It makes the application brittle and destroys valuable historical run data during development and updates.\n- **Fix**: Use a simple migration system from day one. This can be as basic as a `migrations/` folder containing sequential SQL files (e.g., `001_initial_schema.sql`, `002_add_branch_column.sql`). The application checks a `schema_version` table on startup and applies any missing migrations.\n\n⚠️ **Pitfall: Storing Large Logs in the Database**\n- **Description**: Storing the full stdout/stderr log text (which can be megabytes) directly in the `StepRun.Output` or `JobRun` table.\n- **Why it's wrong**: Bloating the database table makes queries slower, backups huge, and can hit size limits. It also makes real-time streaming less efficient.\n- **Fix**: Follow the model defined above: store logs externally (e.g., in a rotating file on disk, or in an object store like S3/MinIO) and keep only a small reference (`LogKey`) in the database. The `StepRun.Output` should contain only a truncated preview (last few hundred lines) for quick UI display.\n\n⚠️ **Pitfall: Not Using Transactions for Queue Operations**\n- **Description**: Implementing the job queue with a simple `SELECT` followed by an `UPDATE` to mark a job as \"taken,\" without wrapping it in a transaction.\n- **Why it's wrong**: Under concurrency, two workers might `SELECT` the same `STATUS_PENDING` job simultaneously, leading to duplicate execution—a critical bug in a CI system.\n- **Fix**: Use a transactional pattern. For SQLite, this looks like:\n    ```sql\n    BEGIN IMMEDIATE;\n    SELECT id, ... FROM job_run WHERE status = 'pending' ORDER BY created_at LIMIT 1 FOR UPDATE SKIP LOCKED;\n    -- Application logic: if a row is found, update its status to 'running' and set assigned_worker.\n    UPDATE job_run SET status = 'running', assigned_worker = ? WHERE id = ?;\n    COMMIT;\n    ```\n    The `FOR UPDATE SKIP LOCKED` clause (or equivalent with `BEGIN IMMEDIATE` and careful locking in SQLite) ensures safe concurrent access.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table**\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Storage & ORM** | SQLite with `database/sql` and manual SQL queries. | Use a lightweight ORM like `sqlc` (generate type-safe Go from SQL) or `entgo`. |\n| **Migrations** | Custom migration runner reading SQL files from an embedded filesystem (`//go:embed`). | Use a dedicated library like `golang-migrate`. |\n| **Log Storage** | Rotating files in a local directory (e.g., `/var/log/ciserver/jobs/`). | Object storage (S3-compatible like MinIO for local development, AWS S3 for production). |\n\n**B. Recommended File/Module Structure**\n```\nbuild-your-own-ci/\n  cmd/\n    server/                 # Main application entry point\n      main.go\n  internal/\n    store/                 # Data model persistence layer\n      store.go             # Store interface definition\n      sqlite/              # SQLite implementation of Store\n        sqlite.go\n        migrations/        # SQL migration files\n          001_initial.sql\n          002_add_artifact_keys.sql\n        queries/           # Optional: if using sqlc\n          jobs.sql\n          runs.sql\n    models/                # Core data type definitions (structs)\n      pipeline.go          # PipelineConfig, JobConfig, StepConfig\n      run.go               # PipelineRun, JobRun, StepRun\n      constants.go         # STATUS_*, EVENT_* constants\n  ... (other components like orchestrator, worker, dashboard)\n```\n\n**C. Infrastructure Starter Code**\nThe following is a complete, ready-to-use foundation for the SQLite store with a basic migration mechanism.\n\n**File: `internal/store/sqlite/sqlite.go`**\n```go\npackage sqlite\n\nimport (\n    \"database/sql\"\n    \"embed\"\n    \"fmt\"\n    \"log\"\n\n    // SQLite driver\n    _ \"modernc.org/sqlite\"\n)\n\n//go:embed migrations/*.sql\nvar migrationsFS embed.FS\n\n// DB wraps a sql.DB connection and provides application-specific methods.\ntype DB struct {\n    conn *sql.DB\n}\n\n// NewDB opens a connection to the SQLite database at the given path,\n// applies any pending migrations, and returns a ready-to-use DB instance.\nfunc NewDB(dataSourceName string) (*DB, error) {\n    // Enable WAL mode for better concurrency\n    dsn := fmt.Sprintf(\"%s?_journal=WAL&_timeout=5000\", dataSourceName)\n    conn, err := sql.Open(\"sqlite\", dsn)\n    if err != nil {\n        return nil, fmt.Errorf(\"open sqlite db: %w\", err)\n    }\n\n    // Test connection\n    if err := conn.Ping(); err != nil {\n        conn.Close()\n        return nil, fmt.Errorf(\"ping sqlite db: %w\", err)\n    }\n\n    db := &DB{conn: conn}\n\n    // Apply migrations\n    if err := db.migrate(); err != nil {\n        conn.Close()\n        return nil, fmt.Errorf(\"migrate database: %w\", err)\n    }\n\n    return db, nil\n}\n\n// migrate ensures the database schema is up-to-date.\nfunc (db *DB) migrate() error {\n    // Create migrations table if it doesn't exist\n    _, err := db.conn.Exec(`\n        CREATE TABLE IF NOT EXISTS schema_migrations (\n            version INTEGER PRIMARY KEY,\n            applied_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        );\n    `)\n    if err != nil {\n        return err\n    }\n\n    // Read migration files from embedded filesystem\n    files, err := migrationsFS.ReadDir(\"migrations\")\n    if err != nil {\n        return err\n    }\n\n    // Get the last applied migration version\n    var currentVersion int\n    row := db.conn.QueryRow(\"SELECT MAX(version) FROM schema_migrations\")\n    row.Scan(¤tVersion) // If NULL, currentVersion stays 0.\n\n    // Apply migrations in order\n    for _, file := range files {\n        // Simple numeric prefix: 001_initial.sql -> version 1\n        var version int\n        _, err := fmt.Sscanf(file.Name(), \"%d_\", &version)\n        if err != nil {\n            return fmt.Errorf(\"invalid migration filename %q: %w\", file.Name(), err)\n        }\n\n        if version > currentVersion {\n            log.Printf(\"Applying migration %d: %s\", version, file.Name())\n            content, err := migrationsFS.ReadFile(\"migrations/\" + file.Name())\n            if err != nil {\n                return err\n            }\n\n            // Execute migration within a transaction\n            tx, err := db.conn.Begin()\n            if err != nil {\n                return err\n            }\n\n            if _, err := tx.Exec(string(content)); err != nil {\n                tx.Rollback()\n                return fmt.Errorf(\"failed to execute migration %d: %w\", version, err)\n            }\n\n            // Record the migration version\n            _, err = tx.Exec(\"INSERT INTO schema_migrations (version) VALUES (?)\", version)\n            if err != nil {\n                tx.Rollback()\n                return err\n            }\n\n            if err := tx.Commit(); err != nil {\n                return err\n            }\n            log.Printf(\"Migration %d applied successfully\", version)\n        }\n    }\n    return nil\n}\n\n// Close closes the database connection.\nfunc (db *DB) Close() error {\n    return db.conn.Close()\n}\n\n// Conn returns the underlying *sql.DB (use for implementing Store methods).\nfunc (db *DB) Conn() *sql.DB {\n    return db.conn\n}\n```\n\n**File: `internal/store/sqlite/migrations/001_initial.sql`**\n```sql\n-- Initial schema for CI/CD Pipeline Orchestrator\n-- Includes tables for pipeline runs, job runs, step runs, and a simple queue mechanism.\n\nPRAGMA foreign_keys = ON;\n\n-- Pipeline runs represent a single execution of a pipeline config.\nCREATE TABLE pipeline_runs (\n    id TEXT PRIMARY KEY,\n    status TEXT NOT NULL DEFAULT 'pending',\n    trigger TEXT NOT NULL,\n    commit_sha TEXT NOT NULL,\n    branch TEXT NOT NULL,\n    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    started_at DATETIME,\n    finished_at DATETIME,\n    config_json TEXT NOT NULL  -- Serialized PipelineConfig as JSON\n);\n\n-- Job runs represent an instance of a job within a pipeline run.\nCREATE TABLE job_runs (\n    id TEXT PRIMARY KEY,\n    pipeline_run_id TEXT NOT NULL,\n    job_name TEXT NOT NULL,\n    status TEXT NOT NULL DEFAULT 'pending',\n    assigned_worker TEXT,\n    container_id TEXT,\n    log_key TEXT,\n    artifact_keys TEXT,  -- JSON array of strings\n    started_at DATETIME,\n    finished_at DATETIME,\n    env_json TEXT NOT NULL,  -- Serialized map[string]string as JSON\n    FOREIGN KEY (pipeline_run_id) REFERENCES pipeline_runs(id) ON DELETE CASCADE\n);\n\n-- Step runs represent the execution of a single command within a job.\nCREATE TABLE step_runs (\n    id TEXT PRIMARY KEY,\n    job_run_id TEXT NOT NULL,\n    step_name TEXT NOT NULL,\n    command TEXT NOT NULL,\n    exit_code INTEGER,\n    output TEXT,\n    started_at DATETIME NOT NULL,\n    duration_ns INTEGER,  -- Stored as nanoseconds for precision\n    FOREIGN KEY (job_run_id) REFERENCES job_runs(id) ON DELETE CASCADE\n);\n\n-- Indexes for efficient querying\nCREATE INDEX idx_job_runs_pipeline_run_id ON job_runs(pipeline_run_id);\nCREATE INDEX idx_job_runs_status ON job_runs(status) WHERE status = 'pending';\nCREATE INDEX idx_step_runs_job_run_id ON step_runs(job_run_id);\nCREATE INDEX idx_pipeline_runs_created_at ON pipeline_runs(created_at DESC);\n\n-- Trigger to automatically update `updated_at` on pipeline_runs\nCREATE TRIGGER update_pipeline_runs_timestamp \nAFTER UPDATE ON pipeline_runs\nBEGIN\n    UPDATE pipeline_runs SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;\nEND;\n```\n\n**D. Core Logic Skeleton Code**\nThe `Store` interface defines the persistence contract. Here is its definition and a skeleton for its SQLite implementation.\n\n**File: `internal/store/store.go`**\n```go\npackage store\n\nimport (\n    \"context\"\n    \"time\"\n\n    \"github.com/yourusername/build-your-own-ci/internal/models\"\n)\n\n// Store defines the interface for all persistence operations.\ntype Store interface {\n    // PipelineRun operations\n    CreatePipelineRun(ctx context.Context, run *models.PipelineRun) error\n    GetPipelineRun(ctx context.Context, id string) (*models.PipelineRun, error)\n    UpdatePipelineRunStatus(ctx context.Context, id string, status string) error\n    ListPipelineRuns(ctx context.Context, limit, offset int) ([]*models.PipelineRun, error)\n\n    // JobRun operations\n    CreateJobRun(ctx context.Context, job *models.JobRun) error\n    GetJobRun(ctx context.Context, id string) (*models.JobRun, error)\n    UpdateJobRunStatus(ctx context.Context, id string, status string) error\n    UpdateJobRunAssignedWorker(ctx context.Context, id, workerID string) error\n    UpdateJobRunWithContainerInfo(ctx context.Context, id, containerID, logKey string) error\n    MarkJobRunFinished(ctx context.Context, id, status string, finishedAt time.Time) error\n    AddArtifactKeyToJobRun(ctx context.Context, jobRunID, artifactKey string) error\n\n    // StepRun operations\n    CreateStepRun(ctx context.Context, step *models.StepRun) error\n    UpdateStepRunOutput(ctx context.Context, id string, output string) error\n    MarkStepRunFinished(ctx context.Context, id string, exitCode int, duration time.Duration) error\n\n    // Queue operations (implemented using the job_runs table)\n    EnqueueJobRun(ctx context.Context, jobRunID string) error\n    DequeueJobRun(ctx context.Context, workerID string) (*models.JobRun, error)\n\n    // Close releases any resources held by the store.\n    Close() error\n}\n```\n\n**File: `internal/store/sqlite/queries.go`**\n```go\npackage sqlite\n\nimport (\n    \"context\"\n    \"database/sql\"\n    \"encoding/json\"\n    \"fmt\"\n    \"time\"\n\n    \"github.com/yourusername/build-your-own-ci/internal/models\"\n    \"github.com/yourusername/build-your-own-ci/internal/store\"\n)\n\n// Ensure *DB implements store.Store\nvar _ store.Store = (*DB)(nil)\n\n// CreatePipelineRun implements store.Store.\nfunc (db *DB) CreatePipelineRun(ctx context.Context, run *models.PipelineRun) error {\n    // TODO 1: Serialize run.Config to JSON (use json.Marshal)\n    // TODO 2: Prepare SQL INSERT statement for pipeline_runs table with all fields\n    // TODO 3: Execute the INSERT, using the generated ID from the run parameter\n    // TODO 4: Handle potential errors (e.g., duplicate ID) and return appropriately\n    // Hint: Use db.conn.ExecContext or db.conn.PrepareContext\n    return fmt.Errorf(\"not implemented\")\n}\n\n// DequeueJobRun implements store.Store.\n// This is a critical method that must use a transaction to avoid double-processing.\nfunc (db *DB) DequeueJobRun(ctx context.Context, workerID string) (*models.JobRun, error) {\n    // TODO 1: Begin an immediate transaction (db.conn.BeginTx with isolation level sql.LevelSerializable)\n    // TODO 2: Query for a single pending job: SELECT ... FROM job_runs WHERE status = 'pending' ORDER BY created_at LIMIT 1\n    // TODO 3: If a row is found, update its status to 'running' and set assigned_worker to workerID\n    // TODO 4: Commit the transaction\n    // TODO 5: If the transaction succeeded and a row was updated, deserialize the env_json and artifact_keys fields\n    // TODO 6: Construct and return a models.JobRun object\n    // TODO 7: If no row was found, return (nil, nil) — no error\n    // TODO 8: Ensure proper rollback on any error\n    // Important: Use FOR UPDATE SKIP LOCKED style locking if your SQLite driver supports it, or rely on the transaction isolation.\n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// ListPipelineRuns implements store.Store.\nfunc (db *DB) ListPipelineRuns(ctx context.Context, limit, offset int) ([]*models.PipelineRun, error) {\n    // TODO 1: Query pipeline_runs ordered by created_at DESC with LIMIT and OFFSET\n    // TODO 2: Iterate through rows, deserialize config_json for each run\n    // TODO 3: Build a slice of *models.PipelineRun\n    // TODO 4: Return the slice and any error\n    return nil, fmt.Errorf(\"not implemented\")\n}\n```\n\n**E. Language-Specific Hints (Go)**\n- Use `encoding/json` to serialize `PipelineConfig` and `Env` maps for storage. Remember to handle `time.Time` fields correctly (they serialize to RFC3339 by default).\n- Use `sql.NullString` and `sql.NullTime` for nullable database fields if you don't want to use pointers. The starter code uses pointers (`*time.Time`) for clarity in the models.\n- The `modernc.org/sqlite` driver is a pure-Go alternative to CGO-based drivers, simplifying cross-compilation.\n- For the `Duration` field in `StepRun`, store it as an integer number of nanoseconds in the database for precision and easy conversion (`time.Duration` is an alias for `int64` nanoseconds in Go).\n\n**F. Milestone Checkpoint**\nAfter implementing the data model and basic store methods, verify your setup:\n1.  Run the SQLite migrations: Start your application; it should create a `.db` file and the tables without errors.\n2.  Write a small test program in `cmd/teststore/main.go` that:\n    - Creates a `DB` instance.\n    - Creates a simple `PipelineRun` with a minimal `PipelineConfig` and inserts it.\n    - Queries it back and prints the result.\n    - Runs `DequeueJobRun` (should return `nil, nil` as no pending jobs exist).\n3.  Use the SQLite CLI to inspect the database: `sqlite3 ciserver.db \"SELECT * FROM pipeline_runs;\"`\n\n**G. Debugging Tips**\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"Database is locked\" errors under concurrency. | SQLite's default journaling mode (`DELETE`) doesn't handle concurrent writes well. | Check the journal mode: `PRAGMA journal_mode;` | Enable Write-Ahead Logging by appending `_journal=WAL` to your DSN (already done in starter code). |\n| `DequeueJobRun` gives the same job to two workers. | The dequeuing logic is not properly transactional or lacks locking. | Add debug logs before and after the transaction; check if two workers call it at nearly the same time. | Ensure you use `BEGIN IMMEDIATE` and update the row within the same transaction before committing. The `FOR UPDATE SKIP LOCKED` pattern is ideal but verify your SQLite version supports it. |\n| JSON serialization of `PipelineConfig` fails. | Circular references or unsupported types in the struct (e.g., a `map[string]interface{}` with values that `json` can't handle). | Print the struct before marshalling with `fmt.Printf(\"%+v\", config)`. | Ensure all nested types are JSON-serializable. Consider using `json.RawMessage` for the `config_json` field if you need to delay parsing. |\n\n\n## Component: Pipeline Configuration Parser (Milestone 1)\n\n> **Milestone(s):** Milestone 1\n\nThis component is the foundational translator that converts human-readable pipeline definitions into a structured, executable plan the CI system can understand and act upon. It sits at the very beginning of the pipeline lifecycle, transforming a static YAML configuration file into a dynamic **Directed Acyclic Graph (DAG)** of jobs, ready for the execution engine. Its correctness is paramount—a misparsed configuration can lead to failed builds, security vulnerabilities, or incorrect automation.\n\n### Mental Model: The Blueprint Interpreter\n\nImagine you're an architect reviewing a set of blueprint drawings for a new building. The blueprints (your YAML file) contain symbols, notes, and diagrams that specify what to build, in what order, and with what materials. Your job as the architect is to interpret these drawings and produce a precise **bill of materials** and **construction schedule** that foremen and workers can execute. You must:\n1. **Decode the symbols** (parse YAML syntax).\n2. **Check for errors and contradictions** (validate structure).\n3. **Resolve any placeholder references** (substitute environment variables).\n4. **Expand any \"optional wings\" that have multiple design choices** (expand matrix builds).\n5. **Create a task dependency chart** showing which foundations must be poured before which walls can be built (build the job DAG).\n\nThis component is that architect. It takes the creative, declarative \"what\" from the developer and converts it into the procedural, unambiguous \"how\" for the CI system's execution engine. A flawed interpretation leads to a building that collapses; a correct one ensures a smooth, automated assembly line.\n\n### Interface and API\n\nThe parser exposes a clean, functional interface centered around the `PipelineConfig` structure. Its primary job is to produce this validated configuration object from raw YAML text.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `ParseConfig` | `yamlContent string` | `(PipelineConfig, error)` | The main entry point. Accepts a string containing the entire pipeline configuration file (e.g., `.ci.yml`). It performs syntax parsing, structural validation, and initializes the core `PipelineConfig` object. |\n| `ResolveEnvVars` | `config *PipelineConfig`, `env map[string]string` | `error` | Processes the configuration, substituting all environment variable references (e.g., `$VAR`, `${VAR}`) in `command` strings, `image` names, and other fields with their actual values from the provided `env` map. It respects precedence: job-level env overrides pipeline-level env, which overrides system-provided env. |\n| `ExpandMatrix` | `jobConfig JobConfig` | `[]JobConfig, error` | Takes a single `JobConfig` that contains a `matrix` definition and returns a slice of new `JobConfig` objects, each representing one unique combination (cartesian product) of the matrix axes. For example, a job with `os: [ubuntu-latest, macos-latest]` and `node-version: [14, 16]` expands into four separate job configurations. |\n| `BuildExecutionGraph` | `config PipelineConfig` | `(map[string][]string, error)` | Analyzes the `needs` dependencies declared between jobs and constructs an adjacency list representing the execution DAG. The returned map has job names as keys and a slice of job names that depend on that key as values. This graph is used by the orchestrator to schedule jobs in the correct order. |\n| `ValidateConfig` | `config PipelineConfig` | `error` | Performs semantic validation beyond basic parsing. Checks for circular dependencies, validates that all `needs` references point to existing jobs, ensures `if` conditions have valid syntax, and warns about potentially insecure patterns (like commands constructed from unsanitized env vars). |\n\nThese functions are typically used in sequence by the orchestrator:\n```go\nrawYAML := readFile(\".ci.yml\")\nconfig, err := ParseConfig(rawYAML)\nif err != nil { ... }\nerr = ResolveEnvVars(&config, combinedEnvMap)\nif err != nil { ... }\nexecutionGraph, err := BuildExecutionGraph(config)\nif err != nil { ... }\n```\n\n### Internal Behavior and Algorithm\n\nThe transformation from raw YAML to an execution plan follows a strict, multi-stage pipeline. Each stage validates and enriches the data, catching errors as early as possible.\n\n1.  **Lexical Analysis & Syntax Parsing**: The raw YAML string is fed into a YAML parser (like `gopkg.in/yaml.v3`). This stage checks for basic YAML syntax errors—unclosed quotes, invalid indentation, malformed mappings. On success, it produces a generic nested `map[string]interface{}` and `[]interface{}` tree in memory.\n\n2.  **Unmarshaling into Structured Types**: The generic tree is unmarshaled into our strongly-typed `PipelineConfig` struct. This involves mapping YAML keys like `jobs` and `environment` to the corresponding Go struct fields. This step fails if required top-level fields (like `jobs`) are missing or if data types are incompatible (e.g., a `job.steps` is provided as a string instead of a list).\n\n3.  **Per-Job Validation and Enrichment**: Each `JobConfig` within `PipelineConfig.Jobs` is validated individually.\n    *   The `runs-on` field is checked for a non-empty value (specifying the Docker image).\n    *   The `steps` list is validated to ensure it's non-empty and each `StepConfig` has a `name` and `command`.\n    *   The `if` condition string for each step is parsed into a simple abstract syntax tree (AST) to validate its grammar (e.g., `branch == 'main' && event == 'push'`), though full evaluation happens later during runtime.\n    *   Default values are injected where appropriate (e.g., if no `environment` is specified at the job level, an empty map is created).\n\n4.  **Environment Variable Resolution**: This is a recursive, context-aware substitution process.\n    *   A combined environment map is built, merging (in order of increasing precedence): system environment, pipeline-level `environment`, job-level `environment`, and runtime-provided secrets/context (like `GIT_COMMIT_SHA`).\n    *   The parser traverses every string field in `PipelineConfig` that is designated as \"expandable\" (primarily `StepConfig.Command`, `JobConfig.RunsOn`). It identifies patterns like `$VAR` or `${VAR}`.\n    *   For each match, it looks up the variable name in the combined environment map. If found, it replaces the pattern with the value. If not found and the pattern is `${VAR}`, it may be replaced with an empty string or cause an error based on configuration; `$VAR` patterns without braces are left untouched if not followed by a valid terminator.\n    *   **Security Note**: This step must avoid recursive substitution (where a variable's value contains another variable reference) unless explicitly intended, to prevent injection or infinite loops.\n\n5.  **Matrix Expansion**: For each job that defines a `matrix` field, the expansion algorithm runs.\n    *   It iterates over each key-value pair in the `matrix` map, where the key is an axis name (e.g., `os`) and the value is a list of choices.\n    *   It computes the **cartesian product** of all axes. For a matrix with axes `os: [a, b]` and `version: [1, 2]`, the product is `[a,1], [a,2], [b,1], [b,2]`.\n    *   For each combination, it creates a new `JobConfig` clone of the original. It then injects the combination's values into the new job's environment map (e.g., adds `MATRIX_OS=a, MATRIX_VERSION=1`) and also substitutes any references to the matrix axis in the job's `name` or `runs-on` field (e.g., `runs-on: ${{ matrix.os }}-runner` becomes `runs-on: a-runner`).\n    *   The original job (if it had a matrix) is replaced by the list of expanded jobs in the pipeline's job map.\n\n6.  **Dependency Graph Construction**: The final step is to analyze job dependencies to build an execution schedule.\n    *   It initializes an adjacency list representation of a graph, where nodes are job names.\n    *   For each job, it examines its `needs` field (a list of job names that must complete successfully before this job can start).\n    *   For each dependency `dep` in `needs`, it adds a directed edge from `dep` to the current job in the adjacency list.\n    *   Once all edges are added, it runs a **cycle detection algorithm** (like Depth-First Search) on the graph. If a cycle is found (e.g., Job A needs Job B, and Job B needs Job A), it returns an error because a cyclic dependency cannot be executed.\n    *   The resulting graph is a **DAG**. The orchestrator will later use **topological sorting** on this DAG to determine a valid linear order of execution that respects all dependencies.\n\nThe entire process can be visualized in the provided flowchart: ![Pipeline Configuration Parsing and DAG Construction](./diagrams/diag-parser-flowchart.svg)\n\n### ADR: YAML vs. Custom DSL for Configuration\n\n> **Decision: Use YAML for Pipeline Configuration**\n>\n> - **Context**: We need a human-readable, writable, and maintainable format for developers to define their CI/CD pipelines. The format must support complex nested structures (jobs, steps, environment) and be easy to integrate with existing Git repository files. The learning curve for contributors writing pipelines should be minimal.\n> - **Options Considered**:\n>     1.  **YAML (YAML Ain't Markup Language)**: A superset of JSON designed for human readability, with support for comments, multiline strings, and anchors/aliases for reducing duplication.\n>     2.  **JSON (JavaScript Object Notation)**: A ubiquitous data interchange format that is simple to parse and generate but lacks comments and can be verbose for humans to write.\n>     3.  **Custom Domain-Specific Language (DSL)**: A dedicated language (e.g., using a parser generator or an embedded DSL in a language like Python or Ruby) that could offer perfect syntactic fit, validation at definition time, and powerful abstractions.\n> - **Decision**: We will use **YAML** as the primary configuration format, specifically a schema similar to GitHub Actions and GitLab CI for familiarity.\n> - **Rationale**:\n>     *   **Familiarity and Ecosystem**: YAML is the de facto standard for modern CI/CD systems (GitHub Actions, GitLab CI, CircleCI). Using it reduces cognitive load for users and leverages existing editor support (syntax highlighting, snippets).\n>     *   **Human-Friendliness**: Support for comments is critical for documenting complex pipelines. Readable multiline strings are essential for embedding shell scripts directly in the config.\n>     *   **Adequate Expressiveness**: While not a programming language, YAML's structure (maps, lists, scalars) is sufficient to represent pipelines, dependencies, and matrices. Advanced logic (conditionals) can be embedded as string expressions evaluated at runtime.\n>     *   **Implementation Simplicity**: Mature, robust YAML parsers exist for all our target languages (e.g., `go-yaml`, `PyYAML`, `serde_yaml`). We avoid the significant complexity of designing, parsing, and maintaining a custom DSL.\n> - **Consequences**:\n>     *   **Positive**: Quick onboarding for users, extensive prior art for schema design, trivial parsing logic.\n>     *   **Negative**: YAML has well-known pitfalls (ambiguous syntax, security concerns with `!!` tags). We must strictly limit the schema and disable advanced YAML features to avoid issues. Runtime validation is mandatory since the format is not type-safe.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| **YAML** | Human-readable, supports comments, wide ecosystem, familiar to developers. | Can be tricky with indentation, requires careful schema validation, potential security issues with custom tags. | **Yes** |\n| **JSON** | Extremely simple to parse, universal support, unambiguous structure. | No comments, verbose, harder for humans to write and read. | No |\n| **Custom DSL** | Maximum expressiveness, can enforce correctness at syntax level, enable powerful abstractions. | High implementation cost, steep learning curve for users, requires custom tooling (editors, linters). | No |\n\n### Common Pitfalls: Parser\n\n⚠️ **Pitfall: Insecure Environment Variable Substitution**\n*   **Description**: Directly substituting environment variables into shell commands without any sanitization can lead to **command injection**. For example, if an environment variable `BRANCH` contains `main; rm -rf /`, a command like `echo \"Building $BRANCH\"` would execute the malicious `rm` command.\n*   **Why it's wrong**: This is a critical security vulnerability that could allow an attacker who controls an environment variable (e.g., via a pull request title or comment) to execute arbitrary code on the CI worker.\n*   **How to fix**: Never directly interpolate variables into the shell command string that will be passed to `sh -c`. Instead, pass environment variables to the container as true environment variables (`os.Environ` or Docker `--env`). Then, in the shell script, they are referenced natively by the shell, which doesn't involve a parsing/expansion step by our Go code. For step commands written directly in YAML, we must ensure the command is executed as `sh -c 'your script'` where the variables are already present in the shell's environment.\n\n⚠️ **Pitfall: Circular Dependencies**\n*   **Description**: A pipeline where Job A `needs` Job B, and Job B `needs` Job A (directly or transitively) creates a cycle. The parser must detect this; otherwise, the orchestrator will deadlock trying to schedule these jobs, as neither can start without the other finishing.\n*   **Why it's wrong**: The execution graph is no longer a **DAG** (Directed *Acyclic* Graph), breaking the fundamental scheduling algorithm. The pipeline will never start or will get stuck in a logical loop.\n*   **How to fix**: Implement cycle detection as part of `BuildExecutionGraph`. Use a standard DFS-based algorithm to traverse the dependency graph. If, during traversal, you encounter a node that is already in the current recursion stack, a cycle exists. Report a clear error to the user listing the jobs involved in the cycle.\n\n⚠️ **Pitfall: Combinatorial Explosion in Matrix Builds**\n*   **Description**: A matrix definition with many axes, each with many values, can generate an enormous number of jobs. For example, `os: [a,b,c,d,e]` x `version: [1,2,3,4,5]` x `test-suite: [unit, integration, e2e]` creates 5*5*3 = 75 jobs. This can overwhelm system resources and queue capacity.\n*   **Why it's wrong**: While functionally correct, it's often not the user's intent to run *every* combination. It can lead to excessive resource consumption, long queue times, and cost overruns.\n*   **How to fix**: Implement guardrails. **1.** Set a conservative default limit (e.g., max 50 matrix combinations per job). **2.** Allow users to explicitly increase this limit via a configuration flag (accepting the risk). **3.** Provide an `exclude` keyword in the matrix schema (like GitHub Actions) to let users prune specific unwanted combinations. **4.** Log a prominent warning when a matrix expands beyond a certain threshold.\n\n⚠️ **Pitfall: Unvalidated `if` Conditions**\n*   **Description**: The `if` field in a step or job is a string expression (e.g., `github.event_name == 'push'`). If the parser only treats it as an opaque string and leaves full evaluation to the runtime, a syntax error in the expression will only surface when the step is about to run, causing confusing failures.\n*   **Why it's wrong**: Failures should be caught as early as possible (at configuration parsing time). A syntax error in an `if` condition is a configuration error, not a runtime failure.\n*   **How to fix**: During the `ValidateConfig` phase, parse each `if` string using a simple expression parser. You don't need to evaluate it (values aren't known yet), but you can validate the grammar—ensuring parentheses are balanced, operators are valid (`==`, `!=`, `&&`, `||`), and operands look like identifiers or literals. This provides immediate feedback to the user.\n\n⚠️ **Pitfall: Incorrect Variable Scoping and Precedence**\n*   **Description**: Environment variables can be defined at the pipeline level, job level, and step level. If the resolution order is not clearly defined and implemented, a step might get the wrong value (e.g., a job-level variable overwriting a pipeline-level secret).\n*   **Why it's wrong**: This leads to non-deterministic behavior and hard-to-debug pipeline failures. It can also cause secret leakage if a less-specific scope overrides a more-specific one.\n*   **How to fix**: Define and document a strict precedence order. A common and intuitive order is (from lowest to highest priority): System Environment < Pipeline `environment` < Job `environment` < Step `env`. When resolving a variable for a step, search in reverse order: first in step `env`, then job `environment`, then pipeline `environment`, then system. The first match wins.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table**\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| YAML Parsing | `gopkg.in/yaml.v3` (pure Go, stable) | `github.com/go-yaml/yaml` (same, v2/v3) |\n| Expression Parsing (for `if`) | Simple string splitting and token matching for a limited grammar (`==`, `!=`, `&&`, `||`). | Embed a JavaScript interpreter via `github.com/robertkrimen/otto` or use a dedicated expression language like `github.com/antonmedv/expr`. |\n| DAG Algorithms | Implement DFS cycle detection and topological sort manually (50-100 lines of Go). | Use a graph library like `github.com/yourbasic/graph` for more complex operations. |\n\n**B. Recommended File/Module Structure**\n```\nci-system/\n├── cmd/\n│   └── server/                 # Main application entry point\n├── internal/\n│   ├── parser/                 # This component\n│   │   ├── config.go           # Core structs: PipelineConfig, JobConfig, StepConfig\n│   │   ├── parser.go           # ParseConfig, ValidateConfig, ResolveEnvVars\n│   │   ├── matrix.go           # ExpandMatrix function\n│   │   ├── dag.go              # BuildExecutionGraph, cycle detection\n│   │   └── parser_test.go      # Unit tests\n│   ├── orchestrator/           # Uses the parsed config\n│   ├── worker/                 # Executes jobs\n│   └── store/                  # Data persistence\n├── pkg/\n│   └── yamlutils/              # (Optional) Shared YAML helpers\n└── go.mod\n```\n\n**C. Infrastructure Starter Code**\n\nHere is a complete, ready-to-use foundation for the YAML parsing and core data structures. The learner should copy this into `internal/parser/config.go`.\n\n```go\n// internal/parser/config.go\npackage parser\n\nimport (\n    \"fmt\"\n    \"regexp\"\n    \"strings\"\n    \"time\"\n\n    \"gopkg.in/yaml.v3\"\n)\n\n// PipelineConfig represents the fully parsed and validated pipeline blueprint.\ntype PipelineConfig struct {\n    Source      string                  `yaml:\"-\"` // File path or URL where config was loaded from\n    Jobs        map[string]JobConfig    `yaml:\"jobs\"`\n    Environment map[string]string       `yaml:\"environment,omitempty\"`\n    MatrixAxes  map[string][]interface{} `yaml:\"matrix,omitempty\"` // Top-level matrix (legacy style, less common)\n}\n\n// JobConfig defines a single runnable job within a pipeline.\ntype JobConfig struct {\n    Name        string            `yaml:\"name\"`\n    RunsOn      string            `yaml:\"runs-on\"` // Docker image identifier\n    Steps       []StepConfig      `yaml:\"steps\"`\n    Needs       []string          `yaml:\"needs,omitempty\"` // Job dependencies\n    Environment map[string]string `yaml:\"env,omitempty\"`   // Job-specific environment\n    Matrix      map[string]interface{} `yaml:\"matrix,omitempty\"` // Matrix definition for this job\n    // Internal fields after processing\n    Expanded    bool              `yaml:\"-\"` // True if this job came from matrix expansion\n}\n\n// StepConfig defines a single command to run within a job.\ntype StepConfig struct {\n    Name    string            `yaml:\"name\"`\n    Command string            `yaml:\"run\"` // The shell command to execute\n    Env     map[string]string `yaml:\"env,omitempty\"` // Step-specific environment\n    If      string            `yaml:\"if,omitempty\"` // Conditional expression\n}\n\n// ParseConfig is the main entry point. It unmarshals YAML and performs basic validation.\nfunc ParseConfig(yamlContent string) (PipelineConfig, error) {\n    var config PipelineConfig\n    // TODO 1: Use yaml.Unmarshal to parse yamlContent into a temporary map.\n    // TODO 2: Perform basic top-level validation: ensure \"jobs\" key exists and is a map.\n    // TODO 3: Unmarshal the YAML node for \"jobs\" directly into config.Jobs using yaml.Unmarshal.\n    // TODO 4: For each job in config.Jobs, validate required fields: name, runs-on, steps.\n    // TODO 5: Validate that each step has a 'name' and 'run' field.\n    // TODO 6: Return the populated PipelineConfig and nil error, or a descriptive error if validation fails.\n    // Hint: Use a custom YAML unmarshaler or two-pass parsing if you need more control.\n    return config, fmt.Errorf(\"ParseConfig not implemented\")\n}\n\n// Helper function to extract a string slice from a YAML node (for matrix values).\nfunc parseStringSlice(node *yaml.Node) ([]string, error) {\n    // (Implementation provided: handles YAML sequence nodes)\n    return nil, nil\n}\n```\n\n**D. Core Logic Skeleton Code**\n\nThe following skeletons guide the implementation of the core algorithms. The learner must fill in the TODOs.\n\n```go\n// internal/parser/matrix.go\npackage parser\n\n// ExpandMatrix takes a job with a matrix definition and returns a slice of jobs,\n// one for each combination of matrix axes.\nfunc ExpandMatrix(job JobConfig) ([]JobConfig, error) {\n    // TODO 1: Check if the job has a \"matrix\" field. If not, return a slice containing just the original job.\n    // TODO 2: Validate that each matrix axis value is a list (slice). Log an error if not.\n    // TODO 3: Compute the cartesian product of all axis values.\n    //    - Example: axes = {\"os\": [\"linux\", \"macos\"], \"node\": [\"14\", \"16\"]}\n    //    - Product = [{\"os\":\"linux\",\"node\":\"14\"}, {\"os\":\"linux\",\"node\":\"16\"}, ...]\n    // TODO 4: For each combination in the product:\n    //    a. Create a deep copy of the original job (important to not share references).\n    //    b. Set job.Expanded = true.\n    //    c. For each axis in the combination, add it to the job's Environment map with a prefix (e.g., \"MATRIX_OS\").\n    //    d. Replace placeholders in job.Name and job.RunsOn (e.g., \"${{ matrix.os }}\" -> \"linux\").\n    //    e. Append the new job to the result slice.\n    // TODO 5: Apply any \"exclude\" rules if your matrix schema supports them (filter out combinations).\n    // TODO 6: Enforce a maximum expansion limit (e.g., 50). Return an error if exceeded.\n    // TODO 7: Return the slice of expanded jobs.\n    return nil, fmt.Errorf(\"ExpandMatrix not implemented\")\n}\n```\n\n```go\n// internal/parser/dag.go\npackage parser\n\n// BuildExecutionGraph constructs an adjacency list of job dependencies.\n// Returns a map where key = job name, value = list of jobs that depend on the key.\nfunc BuildExecutionGraph(config PipelineConfig) (map[string][]string, error) {\n    graph := make(map[string][]string)\n    // Initialize graph with all jobs as keys.\n    for jobName := range config.Jobs {\n        graph[jobName] = []string{}\n    }\n    // TODO 1: Build the adjacency list.\n    // For each job in config.Jobs:\n    //   For each neededJobName in job.Needs:\n    //      Append the current job's name to graph[neededJobName] (edge from needed -> current).\n    // TODO 2: Detect cycles.\n    //   Implement a DFS function that tracks visited nodes and the recursion stack.\n    //   For each unvisited job node in the graph, run DFS.\n    //   If during DFS you visit a node already in the current stack, you found a cycle.\n    //   Return an error describing the cycle path.\n    // TODO 3: If no cycles, return the graph.\n    return graph, fmt.Errorf(\"BuildExecutionGraph not implemented\")\n}\n```\n\n```go\n// internal/parser/parser.go (additional functions)\npackage parser\n\nvar envVarRegex = regexp.MustCompile(`\\$\\{?([a-zA-Z_][a-zA-Z0-9_]*)\\}?`)\n\n// ResolveEnvVars performs substitution of environment variable references in a configuration.\n// It modifies the passed config in-place.\nfunc ResolveEnvVars(config *PipelineConfig, env map[string]string) error {\n    // TODO 1: Build the final environment map for each job, merging:\n    //   baseEnv (system + pipeline environment), job.Environment, and step.Env.\n    // TODO 2: Traverse all string fields in config where substitution is allowed:\n    //   - JobConfig.RunsOn\n    //   - StepConfig.Command\n    //   - Possibly JobConfig.Name for matrix placeholders\n    // TODO 3: For each string, find all matches of envVarRegex.\n    // TODO 4: For each match, look up the variable name in the appropriate environment map.\n    //   If found, replace the match with the value. If not found, either leave it as is (for $VAR)\n    //   or replace with empty string (for ${VAR}) based on your policy.\n    // TODO 5: Be careful to avoid infinite recursion if a variable's value contains another variable reference.\n    //   You may want to limit the number of substitution passes.\n    // TODO 6: Return an error if a required variable (e.g., ${SECRET_KEY}) is not defined.\n    return fmt.Errorf(\"ResolveEnvVars not implemented\")\n}\n```\n\n**E. Language-Specific Hints**\n*   Use `gopkg.in/yaml.v3` for parsing. Prefer `yaml.UnmarshalStrict` to reject unknown fields, helping catch typos in the YAML.\n*   When unmarshaling directly into structs, define YAML tags (`yaml:\"runs-on\"`) to match the common CI schema.\n*   For deep copying `JobConfig` during matrix expansion, a simple way is to marshal to JSON and unmarshal back (`json.Marshal/Unmarshal`) since the struct contains only basic types. For production, consider a dedicated copying library.\n*   Use `regexp.MustCompile` for environment variable patterns, compiling them once at package init for efficiency.\n*   Implement DFS for cycle detection using a `map[string]int` to track status: `0=unvisited, 1=visiting, 2=visited`.\n\n**F. Milestone Checkpoint**\nAfter implementing the parser component, you should be able to run a verification test.\n1.  **Run Unit Tests**: Execute `go test ./internal/parser/... -v` from the project root. You should see tests passing for parsing a simple pipeline, detecting a circular dependency, and expanding a 2x2 matrix.\n2.  **Manual Verification**:\n    *   Create a file `test.yml` with a pipeline definition containing a matrix and dependencies.\n    *   Write a small Go program in `cmd/testparser/main.go` that calls `ParseConfig`, `ExpandMatrix`, and `BuildExecutionGraph`.\n    *   Run it: `go run cmd/testparser/main.go`. It should print the parsed structure, the list of expanded jobs, and the dependency graph without errors.\n3.  **Signs of Trouble**:\n    *   **Panic on nil map**: You forgot to initialize `PipelineConfig.Jobs` map before unmarshaling into it.\n    *   **Circular dependency not detected**: Your DFS algorithm is missing the \"visiting\" state. A job depending on itself should be caught.\n    *   **Matrix expansion produces duplicate combinations**: Check your cartesian product logic; a nested loop over axes is needed.\n\n---\n\n\n## Component: Job Execution Engine (Milestone 2)\n\n> **Milestone(s):** Milestone 2\n\nThis component transforms pipeline definitions into concrete execution. It's the **factory floor** where the automated assembly line comes to life, running each job's commands in isolated, predictable environments and producing build artifacts. The core challenge is providing strong isolation without sacrificing performance, capturing real-time output, and managing resources cleanly even when jobs fail.\n\n### Mental Model: The Sandboxed Playground\n\nImagine giving each job its own **identical, fresh sandbox**—a containerized environment where it can \"play\" (execute commands) without affecting other jobs or the underlying system. This sandbox has:\n\n1. **Fresh Sand**: Every job starts with a clean workspace—no leftover files from previous jobs.\n2. **Controlled Toys**: Only specified tools and dependencies (via the Docker image) are available.\n3. **Supervised Play**: An attendant (the execution engine) watches each command, intervenes if rules are broken (non-zero exit codes), and records everything said (stdout/stderr).\n4. **Keepsake Collection**: When playtime ends, the attendant gathers designated creations (artifacts) for safekeeping before dismantling the sandbox.\n\nThis mental model emphasizes the core guarantees: **isolation, reproducibility, and observability**. Each job gets an identical starting point, runs without interference, and leaves no mess behind.\n\n### Interface and API\n\nThe execution engine exposes a clean interface for orchestrators to trigger job execution and monitor progress. The primary abstraction is the `Executor` interface, which hides the Docker implementation details.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `ExecuteJob` | `ctx context.Context`, `job JobRun`, `env map[string]string` | `(JobRun, error)` | Executes all steps of a job in sequence within a Docker container. Injects environment variables, captures logs, and updates the job status. Returns the updated `JobRun` with timestamps and exit codes. |\n| `StreamLogs` | `ctx context.Context`, `jobID string`, `writer io.Writer` | `error` | Streams the real-time log output of a currently running job to the provided writer (e.g., an HTTP response). Blocks until the job completes or the context is cancelled. |\n| `CollectArtifacts` | `ctx context.Context`, `job JobRun`, `patterns []string` | `([]string, error)` | Copies files matching the glob patterns from the completed job container to persistent storage (e.g., local disk, cloud storage). Returns the storage keys/URLs of collected artifacts. |\n| `CancelJob` | `ctx context.Context`, `jobID string` | `error` | Forcefully stops a running job, kills its container, and updates the job status to `STATUS_CANCELLED`. Performs cleanup of container resources. |\n| `GetJobStatus` | `ctx context.Context`, `jobID string` | `(string, error)` | Returns the current status (`STATUS_RUNNING`, `STATUS_SUCCEEDED`, etc.) of a job by inspecting its container and internal state. |\n\nAdditionally, the engine maintains internal state through these key data structures:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ContainerID` | `string` | Docker container identifier assigned when the job starts execution. Used for all container operations. |\n| `LogKey` | `string` | Reference to where the job's combined stdout/stderr output is stored (e.g., file path, object storage key). |\n| `ArtifactKeys` | `[]string` | List of storage references for artifacts collected from this job run. |\n| `StartedAt` | `*time.Time` | Timestamp when job execution began (set when container starts). |\n| `FinishedAt` | `*time.Time` | Timestamp when job execution completed (set when container stops). |\n\nThe state transitions for a `JobRun` follow a predictable lifecycle:\n\n![Job Run State Machine](./diagrams/diag-job-state-machine.svg)\n\n| Current State | Event | Next State | Action Taken |\n|---------------|-------|------------|--------------|\n| `STATUS_PENDING` | `start` | `STATUS_RUNNING` | Create Docker container, inject environment variables, begin first step. Set `StartedAt`. |\n| `STATUS_RUNNING` | `step_complete(exit_code=0)` | `STATUS_RUNNING` | Continue to next step in sequence. Append step output to logs. |\n| `STATUS_RUNNING` | `step_complete(exit_code≠0)` | `STATUS_FAILED` | Stop job execution, capture final step output, kill container. Set `FinishedAt`. |\n| `STATUS_RUNNING` | `all_steps_complete` | `STATUS_SUCCEEDED` | Stop container gracefully, collect artifacts. Set `FinishedAt`. |\n| `STATUS_RUNNING` | `timeout` | `STATUS_FAILED` | Force-kill container, record timeout error in logs. Set `FinishedAt`. |\n| `STATUS_RUNNING` | `cancel` | `STATUS_CANCELLED` | Force-kill container, record cancellation. Set `FinishedAt`. |\n| Any state | `cleanup` | (unchanged) | Remove container if still exists, release any held resources. |\n\n### Internal Behavior and Algorithm\n\nWhen `ExecuteJob` is called, the engine follows a precise sequence of operations to ensure reliable, isolated execution:\n\n1. **Preparation Phase**\n   - Generate a unique workspace directory on the host filesystem (e.g., `/workspaces/job-<id>`).\n   - Create a temporary directory for log aggregation (`/workspaces/job-<id>/logs`).\n   - Merge environment variables: system defaults → pipeline-level variables → job-level variables → step-level variables, with later sources overriding earlier ones.\n   - Validate the Docker image reference exists locally or is pullable from a registry.\n\n2. **Container Creation**\n   - Construct Docker `ContainerConfig` with:\n     - Image: `job.RunsOn` (e.g., `golang:1.21`)\n     - Working directory: `/workspace` (mounted from host workspace)\n     - Environment variables: The merged map from step 1\n     - Entrypoint: `[\"/bin/sh\", \"-c\"]` to execute shell commands\n     - Network: `bridge` with no extra privileges (security baseline)\n   - Create the container with a unique name (`ci-job-<job-id>`).\n   - Start the container, which immediately pauses waiting for commands.\n\n3. **Step Execution Loop**\n   For each `StepConfig` in `job.Steps`:\n   - Evaluate the `If` condition using the current environment. If false, mark step as `STATUS_SKIPPED` and continue.\n   - Prepare the command by performing environment variable substitution on `StepConfig.Command`.\n   - Create an exec instance in the running container for the command.\n   - Attach multiplexed stdout/stderr streams to both:\n     - Real-time streaming buffer (for `StreamLogs` consumers)\n     - Cumulative log file (for permanent storage)\n   - Start the exec instance and wait for completion with a timeout (default 1 hour).\n   - Capture the exit code. If non-zero, break the loop and transition job to `STATUS_FAILED`.\n\n4. **Completion Handling**\n   - If all steps succeeded:\n     - Call `CollectArtifacts` to copy files matching patterns from container `/workspace` to persistent storage.\n     - Update `JobRun` with `ArtifactKeys` and status `STATUS_SUCCEEDED`.\n   - If any step failed or timed out:\n     - Still attempt artifact collection (for debugging failed runs).\n     - Update status to `STATUS_FAILED` with the failing exit code.\n   - Stop the container (gracefully if succeeded, forcefully if failed).\n   - Set `FinishedAt` timestamp.\n\n5. **Cleanup Phase**\n   - Remove the Docker container (regardless of success/failure).\n   - Delete the temporary host workspace directory.\n   - Close all open file handles and network connections.\n   - Update the `Store` with the final `JobRun` state.\n\nFor `StreamLogs`, the algorithm is:\n1. Locate the job's running container by `ContainerID`.\n2. Attach to the container's combined stdout/stderr stream (Docker's `logs` API with `follow=true`).\n3. Stream bytes directly to the provided `io.Writer`.\n4. Continue streaming until either:\n   - The container stops (job completes)\n   - The client disconnects (context cancellation)\n   - A timeout occurs (configured keepalive)\n\n### ADR: Container vs. Process Isolation\n\n> **Decision: Use Docker Containers for Job Isolation**\n> - **Context**: We need strong isolation between concurrent job executions to prevent contamination (file system, environment variables, processes) while maintaining reasonable implementation complexity for an educational project.\n> - **Options Considered**:\n>   1. **Docker Containers**: Full OS-level isolation via namespaces, cgroups, and union filesystems.\n>   2. **Linux Namespaces/Cgroups Direct**: Using Go libraries to create namespaces directly without Docker.\n>   3. **Simple Subprocess Execution**: Running commands as child processes with environment cleanup.\n> - **Decision**: Use Docker containers via the Docker Engine API.\n> - **Rationale**:\n>   - **Standardization**: Docker provides a consistent, well-documented interface for creating isolated environments across different host systems.\n>   - **Ecosystem**: Access to thousands of pre-built images (language runtimes, tools) without manual installation.\n>   - **Safety**: Strong isolation by default (filesystem, network, process tree) prevents \"noisy neighbor\" issues.\n>   - **Implementation Simplicity**: The Docker Go SDK provides high-level abstractions that reduce boilerplate compared to raw namespace manipulation.\n> - **Consequences**:\n>   - **Requires Docker**: Users must have Docker installed and running, adding a system dependency.\n>   - **Performance Overhead**: Container startup adds ~100-500ms per job versus subprocess execution.\n>   - **Resource Management**: Need explicit cleanup logic to avoid accumulating stopped containers.\n>   - **Security Considerations**: Must run containers with minimal privileges (no `--privileged`, read-only root filesystem where possible).\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Docker Containers | Strong isolation, vast image ecosystem, well-documented API, cross-platform | Requires Docker daemon, container startup overhead, extra dependency | **Yes** |\n| Linux Namespaces/Cgroups Direct | Minimal overhead, fine-grained control, no Docker dependency | Complex implementation, requires root privileges, less portable | No |\n| Simple Subprocess Execution | Fastest startup, simplest implementation, no extra dependencies | No filesystem isolation, environment contamination risk, limited reproducibility | No |\n\n### Common Pitfalls: Execution Engine\n\n⚠️ **Pitfall: Zombie Containers Accumulation**\n- **Description**: Forgetting to remove containers after job completion (especially after failures or cancellations) leads to hundreds of stopped containers consuming disk space.\n- **Why it's wrong**: Disk exhaustion can crash the host system; container limit errors prevent new jobs from starting.\n- **Fix**: Implement a `defer` in `ExecuteJob` that always calls container removal. Add a periodic cleanup goroutine that removes containers older than a threshold.\n\n⚠️ **Pitfall: Log Stream Blocking Deadlocks**\n- **Description**: Writing log output synchronously to both the streaming buffer and persistent storage can block if one destination is slow (e.g., disk I/O congestion), causing the entire job to stall.\n- **Why it's wrong**: Jobs appear to hang; real-time logs stop updating; timeouts trigger incorrectly.\n- **Fix**: Use a buffered channel as a log queue. Start separate goroutines for streaming and disk writing that consume from this channel. Implement backpressure detection.\n\n⚠️ **Pitfall: Artifact Path Confusion**\n- **Description**: Collecting artifacts using container paths instead of host paths (or vice versa) results in empty artifact collections or \"file not found\" errors.\n- **Why it's wrong**: Developers expect build outputs but get empty archives; debugging information is lost.\n- **Fix**: Always use `docker cp` with container paths (e.g., `/workspace/dist/*.tar`) for extraction. Document clearly that patterns are evaluated inside the container's workspace.\n\n⚠️ **Pitfall: Timeout Cascade Failure**\n- **Description**: Setting a single global timeout for the entire job (e.g., 1 hour) without per-step timeouts can let a single hanging step consume all allotted time, delaying failure feedback.\n- **Why it's wrong**: Developers wait unnecessarily for hung jobs; resource utilization is inefficient.\n- **Fix**: Implement dual timeouts: (1) per-step timeout (default 30 minutes), (2) total job timeout (default 2 hours). Cancel the job immediately when either triggers.\n\n⚠️ **Pitfall: Environment Variable Injection Leakage**\n- **Description**: Printing environment variables (including secrets) to logs during debugging or command echoing exposes sensitive data in build logs.\n- **Why it's wrong**: Secret leakage violates security policies; credentials become publicly accessible.\n- **Fix**: Never log raw environment variables. Mask secrets in logs by replacing values with `***` during output redaction. Use Docker's secret mounting feature for production.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Container Runtime | Docker Engine via official Go SDK | containerd via direct Go client (more complex but Docker-independent) |\n| Log Streaming | File-based with periodic polling | In-memory ring buffer with Server-Sent Events (SSE) |\n| Artifact Storage | Local filesystem directory | S3-compatible object storage (MinIO for local testing) |\n| Concurrency Control | `sync.WaitGroup` for job parallelism | Worker pool with rate limiting and priority queue |\n\n#### Recommended File/Module Structure\n\n```\nci-system/\n├── cmd/\n│   ├── server/                 # Main orchestrator server\n│   └── worker/                 # Dedicated worker binary (optional)\n├── internal/\n│   ├── executor/              # This component\n│   │   ├── docker.go          # Docker client wrapper and container operations\n│   │   ├── executor.go        # Main ExecuteJob and StreamLogs logic\n│   │   ├── artifacts.go       # Artifact collection utilities\n│   │   ├── logging.go         # Log streaming and aggregation\n│   │   └── executor_test.go   # Integration tests with testcontainers\n│   ├── queue/                 # Job queue system\n│   ├── parser/                # Pipeline configuration parser\n│   └── store/                 # Data persistence layer\n└── pkg/\n    └── types/                 # Shared types (JobRun, StepRun, etc.)\n```\n\n#### Infrastructure Starter Code\n\n**Complete Docker Client Wrapper (`internal/executor/docker.go`):**\n\n```go\npackage executor\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"time\"\n\n    \"github.com/docker/docker/api/types\"\n    \"github.com/docker/docker/api/types/container\"\n    \"github.com/docker/docker/api/types/mount\"\n    \"github.com/docker/docker/client\"\n    \"github.com/docker/docker/pkg/stdcopy\"\n)\n\n// DockerExecutor wraps the Docker client with convenience methods.\ntype DockerExecutor struct {\n    cli *client.Client\n}\n\n// NewDockerExecutor creates a new executor with a Docker client.\nfunc NewDockerExecutor() (*DockerExecutor, error) {\n    cli, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to create Docker client: %w\", err)\n    }\n    return &DockerExecutor{cli: cli}, nil\n}\n\n// CreateContainer creates and starts a container with the given configuration.\n// Returns the container ID and a cleanup function that removes the container.\nfunc (d *DockerExecutor) CreateContainer(ctx context.Context, config *container.Config, hostConfig *container.HostConfig) (string, func(), error) {\n    // Pull image if not present locally\n    _, _, err := d.cli.ImageInspectWithRaw(ctx, config.Image)\n    if err != nil {\n        reader, err := d.cli.ImagePull(ctx, config.Image, types.ImagePullOptions{})\n        if err != nil {\n            return \"\", nil, fmt.Errorf(\"failed to pull image %s: %w\", config.Image, err)\n        }\n        io.Copy(io.Discard, reader) // Drain the response\n        reader.Close()\n    }\n\n    // Create container\n    resp, err := d.cli.ContainerCreate(ctx, config, hostConfig, nil, nil, \"\")\n    if err != nil {\n        return \"\", nil, fmt.Errorf(\"failed to create container: %w\", err)\n    }\n\n    // Start container\n    if err := d.cli.ContainerStart(ctx, resp.ID, types.ContainerStartOptions{}); err != nil {\n        d.cli.ContainerRemove(ctx, resp.ID, types.ContainerRemoveOptions{Force: true})\n        return \"\", nil, fmt.Errorf(\"failed to start container: %w\", err)\n    }\n\n    // Cleanup function that removes the container\n    cleanup := func() {\n        d.cli.ContainerRemove(context.Background(), resp.ID, types.ContainerRemoveOptions{\n            Force:         true,\n            RemoveVolumes: true,\n        })\n    }\n\n    return resp.ID, cleanup, nil\n}\n\n// ExecInContainer executes a command in a running container.\n// Returns exit code, combined output, and any error.\nfunc (d *DockerExecutor) ExecInContainer(ctx context.Context, containerID string, cmd []string, env []string) (int, string, error) {\n    execConfig := types.ExecConfig{\n        Cmd:          cmd,\n        Env:          env,\n        AttachStdout: true,\n        AttachStderr: true,\n    }\n\n    execResp, err := d.cli.ContainerExecCreate(ctx, containerID, execConfig)\n    if err != nil {\n        return -1, \"\", fmt.Errorf(\"failed to create exec instance: %w\", err)\n    }\n\n    attachResp, err := d.cli.ContainerExecAttach(ctx, execResp.ID, types.ExecStartCheck{})\n    if err != nil {\n        return -1, \"\", fmt.Errorf(\"failed to attach to exec: %w\", err)\n    }\n    defer attachResp.Close()\n\n    // Capture output\n    var outputBuf bytes.Buffer\n    outputDone := make(chan error)\n    go func() {\n        _, err = stdcopy.StdCopy(&outputBuf, &outputBuf, attachResp.Reader)\n        outputDone <- err\n    }()\n\n    // Start execution\n    if err := d.cli.ContainerExecStart(ctx, execResp.ID, types.ExecStartCheck{}); err != nil {\n        return -1, \"\", fmt.Errorf(\"failed to start exec: %w\", err)\n    }\n\n    // Wait for completion\n    select {\n    case err := <-outputDone:\n        if err != nil {\n            return -1, \"\", fmt.Errorf(\"error reading output: %w\", err)\n        }\n    case <-ctx.Done():\n        return -1, \"\", ctx.Err()\n    }\n\n    // Inspect to get exit code\n    inspectResp, err := d.cli.ContainerExecInspect(ctx, execResp.ID)\n    if err != nil {\n        return -1, outputBuf.String(), fmt.Errorf(\"failed to inspect exec: %w\", err)\n    }\n\n    return inspectResp.ExitCode, outputBuf.String(), nil\n}\n\n// StreamLogs streams container logs to the provided writer.\nfunc (d *DockerExecutor) StreamLogs(ctx context.Context, containerID string, writer io.Writer) error {\n    options := types.ContainerLogsOptions{\n        ShowStdout: true,\n        ShowStderr: true,\n        Follow:     true,\n        Timestamps: false,\n    }\n\n    reader, err := d.cli.ContainerLogs(ctx, containerID, options)\n    if err != nil {\n        return fmt.Errorf(\"failed to get container logs: %w\", err)\n    }\n    defer reader.Close()\n\n    // Demultiplex Docker's log stream (stdout/stderr are interleaved)\n    _, err = stdcopy.StdCopy(writer, writer, reader)\n    return err\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Main Execution Logic (`internal/executor/executor.go`):**\n\n```go\npackage executor\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n    \"time\"\n\n    \"ci-system/pkg/types\"\n)\n\n// Executor implements the job execution interface.\ntype Executor struct {\n    docker     *DockerExecutor\n    workspace  string          // Base directory for job workspaces\n    logStore   LogStore        // Interface for storing logs\n    artifactStore ArtifactStore // Interface for storing artifacts\n}\n\n// ExecuteJob implements the main job execution algorithm.\nfunc (e *Executor) ExecuteJob(ctx context.Context, job types.JobRun, env map[string]string) (types.JobRun, error) {\n    // TODO 1: Prepare workspace directory on host\n    //   - Create directory at filepath.Join(e.workspace, job.ID)\n    //   - Set up log file in workspace/logs/job.log\n    \n    // TODO 2: Merge environment variables\n    //   - Start with system env (os.Environ())\n    //   - Override with pipeline env (from job.PipelineRun.Config.Environment)\n    //   - Override with job env (job.Env)\n    //   - Add CI-specific variables (CI=true, JOB_ID=job.ID, etc.)\n    \n    // TODO 3: Create Docker container\n    //   - Build container config with image from job.JobName's RunsOn\n    //   - Mount host workspace directory to /workspace in container\n    //   - Set environment variables from step 2\n    //   - Use e.docker.CreateContainer() and defer the cleanup function\n    \n    // TODO 4: Update job status to STATUS_RUNNING\n    //   - Set job.StartedAt = time.Now()\n    //   - Store updated job in database\n    \n    // TODO 5: Execute steps sequentially\n    for _, step := range job.Steps {\n        // TODO 5.1: Evaluate step.If condition\n        //   - Parse condition expression\n        //   - Evaluate against current environment\n        //   - If false, mark step as STATUS_SKIPPED and continue\n        \n        // TODO 5.2: Prepare command with env var substitution\n        //   - Replace $VAR and ${VAR} patterns in step.Command\n        //   - Handle default values ${VAR:-default} syntax\n        \n        // TODO 5.3: Execute command in container\n        //   - Use e.docker.ExecInContainer() with timeout\n        //   - Capture exit code and output\n        \n        // TODO 5.4: Stream output in real-time\n        //   - Write to log file\n        //   - Notify log streaming clients via channel\n        \n        // TODO 5.5: Handle step completion\n        //   - If exit code != 0, break loop and mark job as STATUS_FAILED\n        //   - Record step execution in StepRun structure\n    }\n    \n    // TODO 6: Handle job completion\n    //   - If all steps succeeded:\n    //     - Collect artifacts using e.CollectArtifacts()\n    //     - Update job.Status = STATUS_SUCCEEDED\n    //   - If any step failed:\n    //     - Still attempt artifact collection (for debugging)\n    //     - Update job.Status = STATUS_FAILED\n    \n    // TODO 7: Cleanup\n    //   - Stop container (already handled by defer)\n    //   - Remove temporary workspace directory\n    //   - Set job.FinishedAt = time.Now()\n    //   - Store final job state\n    \n    return job, nil\n}\n\n// StreamLogs streams job logs to the writer in real-time.\nfunc (e *Executor) StreamLogs(ctx context.Context, jobID string, writer io.Writer) error {\n    // TODO 1: Look up job by ID to get ContainerID\n    //   - Query database for job.ContainerID\n    \n    // TODO 2: If container is still running, attach to logs\n    //   - Use e.docker.StreamLogs() with the container ID\n    //   - Stream directly to writer\n    \n    // TODO 3: If container has stopped, stream from stored logs\n    //   - Read from log file in workspace\n    //   - Write to writer\n    \n    // TODO 4: Handle client disconnection\n    //   - Watch for context cancellation\n    //   - Clean up resources when client disconnects\n    \n    return nil\n}\n```\n\n**Artifact Collection (`internal/executor/artifacts.go`):**\n\n```go\npackage executor\n\n// CollectArtifacts copies artifacts from container to storage.\nfunc (e *Executor) CollectArtifacts(ctx context.Context, job types.JobRun, patterns []string) ([]string, error) {\n    var artifactKeys []string\n    \n    // TODO 1: For each pattern in patterns:\n    //   - Execute `find` command in container to locate matching files\n    //   - Parse output to get list of files\n    \n    // TODO 2: For each matching file:\n    //   - Use docker cp to copy from container to temporary host location\n    //   - Compress if file is large (>10MB)\n    //   - Upload to artifact storage (e.artifactStore)\n    //   - Record the storage key in artifactKeys\n    \n    // TODO 3: Create artifact manifest\n    //   - JSON file listing all artifacts with metadata\n    //   - Upload manifest as additional artifact\n    \n    return artifactKeys, nil\n}\n```\n\n#### Language-Specific Hints\n\n- **Docker SDK**: Use `github.com/docker/docker/client` version `v20.10.17+` for stable API. Always call `client.WithAPIVersionNegotiation()` to handle different Docker daemon versions.\n- **Context Propagation**: Pass `context.Context` through all Docker operations to enable proper cancellation and timeouts. Use `context.WithTimeout` for step execution.\n- **Concurrent Log Handling**: Use `io.MultiWriter` to send output to both file and streaming buffer simultaneously. Consider a ring buffer implementation for memory-efficient log retention.\n- **Resource Cleanup**: Implement `ContainerRemove` with `Force: true` in a deferred function to ensure containers are always cleaned up, even on panic.\n- **Error Wrapping**: Use `fmt.Errorf(\"... %w\", err)` throughout to create actionable error chains that include Docker API errors.\n\n#### Milestone Checkpoint\n\nAfter implementing the execution engine, verify functionality:\n\n1. **Run unit tests**: `go test ./internal/executor/... -v` should pass tests for:\n   - Environment variable merging\n   - Command substitution\n   - Step execution ordering\n\n2. **Integration test with Docker**:\n   ```bash\n   # Start a test job manually\n   curl -X POST http://localhost:8080/api/test/execute \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n       \"image\": \"alpine:latest\",\n       \"commands\": [\"echo 'Hello CI'\", \"ls -la\"],\n       \"env\": {\"TEST_VAR\": \"value\"}\n     }'\n   ```\n   Expected: Returns a job ID immediately, creates an Alpine container, executes commands, streams output.\n\n3. **Verify container cleanup**:\n   ```bash\n   docker ps -a | grep ci-job-  # Should show no containers after job completes\n   ```\n\n4. **Check artifact collection**:\n   Create a test job that produces a file (`touch output.txt`), specify artifact pattern `output.txt`, and verify the file appears in the artifact storage directory.\n\nSigns of problems:\n- Containers remain running after job completion → Check cleanup deferred function.\n- Logs appear truncated → Check buffer sizes in `StreamLogs`.\n- Environment variables not available in commands → Verify merging order and substitution logic.\n- \"Permission denied\" when copying artifacts → Check Docker volume mount permissions.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Job hangs at \"Creating container\" | Docker daemon not running or unreachable | Run `docker ps` to check daemon status; check `e.docker.cli.Ping()` in code | Start Docker daemon; ensure client uses correct socket path |\n| Logs stop streaming but job continues | Writer blocking (e.g., slow HTTP client) | Check if `StreamLogs` goroutine is stuck; monitor buffer sizes | Implement non-blocking writes with channel buffering; add write timeouts |\n| \"No space left on device\" error | Workspace directories not cleaned up | Check disk usage `df -h`; list workspace directories | Add workspace cleanup in defer; implement periodic cleanup job |\n| Artifact patterns match no files | Paths relative to wrong directory | Log container working directory; run `pwd` command in container | Ensure patterns are relative to `/workspace`; document this requirement |\n| Environment variables with special characters break commands | Improper shell escaping | Log the actual command sent to Docker exec | Use `sh -c` with proper quoting; consider `exec.Command` with explicit args |\n| Job succeeds but status remains \"RUNNING\" | Database update failed after execution | Check for panics after container cleanup; verify transaction commits | Wrap entire ExecuteJob in transaction; add final status update in defer |\n\n\n## Component: Webhook & Queue System (Milestone 3)\n\n> **Milestone(s):** Milestone 3\n\nThis component serves as the **entry point and traffic controller** for the entire CI/CD system. Its primary responsibility is to reliably accept events from external Git services, validate their authenticity, transform them into executable pipeline runs, and manage the orderly dispatch of work to available workers. This dual role—acting as both a secure gateway and a fair scheduler—makes it a critical subsystem for system reliability and performance.\n\n### Mental Model: The Restaurant Host and Kitchen Ticket Rail\n\nImagine a busy restaurant. **Customers** (Git events like pushes or pull requests) arrive at the door. The **host** (webhook handler) greets them, checks their reservation (validates the webhook signature), and understands their order (parses the payload to determine which pipeline to run). The host then creates a **ticket** (a `PipelineRun`) that describes the complete meal (the series of jobs to execute) and places it on the **kitchen ticket rail** (the job queue).\n\nThe ticket rail has a specific order: priority tickets (like production deployments) go to the front. Multiple **chefs** (workers) watch the rail. When a chef becomes free, they take the next available ticket from the rail (`DequeueJob`), prepare that specific dish (execute the job in a container), and mark the ticket as done when finished. The host never directly interrupts the chefs; all coordination happens through the ticket rail. This separation ensures that even during a dinner rush (a flood of commits), orders are processed fairly and no single order blocks the entire kitchen.\n\n### Interface and API\n\nThis component exposes two primary interfaces: an **HTTP handler** for external Git services and a **programmatic queue API** for internal system components. The following table details the key methods and their contracts.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `HandleWebhook` | `payload []byte`, `signature string` | `(PipelineRun, error)` | The primary public HTTP handler. Validates the incoming webhook signature, parses the JSON payload, determines the appropriate pipeline configuration from the repository, creates a new `PipelineRun`, and enqueues it. Returns the created run or an error if validation fails. |\n| `EnqueueRun` | `run PipelineRun` | `(string, error)` | Internal method that inserts a complete pipeline run (with all its constituent `JobRun` records in `STATUS_PENDING`) into the queue for execution. Returns a queue identifier or an error if the queue is unavailable. |\n| `DequeueJob` | `workerID string` | `(JobRun, error)` | Called by worker processes to request the next available job. Atomically marks a pending `JobRun` as `STATUS_RUNNING` and assigns it to the requesting worker. Returns an error if no jobs are available or if the worker fails to check in. |\n| `MarkJobComplete` | `jobID string`, `status string` | `error` | Updates the status of a `JobRun` (to `STATUS_SUCCEEDED`, `STATUS_FAILED`, etc.) and triggers the evaluation of dependent jobs. If all jobs in a `PipelineRun` are complete, updates the pipeline run status accordingly. |\n| `GetQueueStats` | - | `(pending int, running int)` | Returns operational metrics: the count of jobs in `STATUS_PENDING` and `STATUS_RUNNING`. Used for monitoring and dashboard display. |\n\n### Internal Behavior and Algorithm\n\nThe component orchestrates a multi-step process from an incoming HTTP request to a job being picked up by a worker. The sequence diagram ![Sequence: Webhook to Job Execution](./diagrams/diag-webhook-sequence.svg) illustrates this flow. The following numbered steps detail the internal logic.\n\n1.  **HTTP Request Reception & Signature Verification**\n    The HTTP server receives a `POST` request at the `/webhook` endpoint. The handler immediately extracts the `X-Hub-Signature-256` header (for GitHub) or `X-GitLab-Token` header and the raw request body.\n    > **Security Principle:** Never trust the network. The signature is an HMAC hex digest of the request body using a shared secret token. The handler must compute the HMAC of the received payload and perform a **constant-time comparison** with the provided signature to prevent timing attacks. If the signature is invalid or missing, the request is rejected with HTTP 403.\n\n2.  **Payload Parsing and Event Classification**\n    The valid JSON payload is unmarshaled into a generic event structure. The handler inspects key fields to classify the event type:\n    *   For GitHub: The `X-GitHub-Event` header indicates `push`, `pull_request`, or `create` (for tags).\n    *   For GitLab: The `object_kind` field indicates `push`, `merge_request`, or `tag_push`.\n    The handler extracts the critical context: **repository SSH URL**, **branch or tag name**, **commit SHA**, and **event type** (`EVENT_PUSH`, `EVENT_PULL_REQUEST`, `EVENT_TAG`).\n\n3.  **Pipeline Configuration Matching**\n    Using the repository URL and branch, the system locates the pipeline configuration file (e.g., `.ci/pipeline.yaml`) from the source code. In a real-world scenario, this would involve cloning the repository at the specific commit. For our system's scope, we assume the configuration is already present or fetched via a lightweight API call to the Git provider. The configuration's `on:` block (or equivalent) is evaluated against the event context. For example, a pipeline configured with `on: push: branches: [main]` will only trigger for push events to the `main` branch.\n\n4.  **PipelineRun and JobRun Creation**\n    If the event matches, the handler instantiates a new `PipelineRun`. It generates unique IDs for the run and for each `JobRun` derived from the parsed `PipelineConfig`. The `JobRun` statuses are set to `STATUS_PENDING`. The `Trigger` field is set to the event type, and the `CommitSHA`, `Branch`, and repository metadata are recorded. The complete `PipelineRun` is persisted via the `Store` interface (`CreatePipelineRun`).\n\n5.  **Queue Insertion with Priority**\n    The `EnqueueRun` method is called with the new `PipelineRun`. The queue backend stores a reference to the run. **Priority scheduling** is applied here: runs triggered by tags or the main branch may be assigned a higher priority score than those from feature branches. The queue orders items by this score (and secondarily by creation time).\n\n6.  **Worker Polling and Job Dispatch**\n    Worker processes, started as part of the worker pool, continuously call `DequeueJob`, providing their unique `workerID`. The queue logic performs an atomic transaction:\n    a. Find the highest-priority `PipelineRun` with at least one `STATUS_PENDING` job where the job's dependencies (the `Needs` field) are all satisfied (i.e., in `STATUS_SUCCEEDED`).\n    b. Select a pending `JobRun` from that pipeline.\n    c. Update the `JobRun`: set `Status` to `STATUS_RUNNING`, `AssignedWorker` to `workerID`, and `StartedAt` to the current time.\n    d. Return the updated `JobRun` object to the worker.\n    This atomicity prevents the same job from being assigned to two workers.\n\n7.  **Job Completion and State Propagation**\n    When a worker finishes executing a job, it calls `MarkJobComplete` with the final status. The queue component updates the `JobRun` (`FinishedAt`, `Status`) and then **re-evaluates the dependency graph**. It checks all jobs that `Need` the now-completed job. If all dependencies for a pending job are met, that job becomes eligible for dequeuing. This process continues until the pipeline run reaches a terminal state (all jobs succeeded, failed, or were cancelled).\n\n### ADR: In-Memory vs. Persistent Queue Backend\n\n> **Decision: Use a hybrid queue model with SQLite for persistence and an in-memory priority heap for dispatch.**\n>\n> *   **Context**: The job queue must be persistent to survive server restarts but also support efficient priority-based dequeue operations and concurrent worker access. Given the educational nature of the project, we want to minimize external dependencies while demonstrating production-like queue semantics.\n> *   **Options Considered**:\n>     1.  Pure in-memory channel (Go `chan`): Simple and fast for a single process, but jobs are lost on crash and scaling to multiple processes is impossible.\n>     2.  External Redis: Excellent for persistence, high performance, and built-in support for priority queues and pub/sub. However, it introduces an additional moving part and operational complexity for learners.\n>     3.  Database (PostgreSQL/SQLite) with advisory locks: Uses the existing persistence layer. Provides durability and transactional safety. Requires careful design to avoid table-level contention and to implement efficient priority ordering.\n> *   **Decision**: Use SQLite (via the `Store` interface) as the system of record for all `JobRun` states, combined with an **in-memory priority heap** within the orchestrator process that holds references to pending jobs. The heap is rebuilt from the database on startup.\n> *   **Rationale**: This hybrid approach balances simplicity with required durability. SQLite is already a project dependency for the `Store`. By storing the canonical job state there, we guarantee no job is lost. The in-memory heap provides O(log n) dequeue performance. On startup, a simple query (`SELECT * FROM job_runs WHERE status = 'PENDING'`) rebuilds the heap. For a single-server educational project, this is sufficient and avoids introducing Redis.\n> *   **Consequences**:\n>     *   **Positive**: No new external dependencies. Job state is durable. Priority scheduling is straightforward to implement with `container/heap`.\n>     *   **Negative**: The queue is not distributed. Multiple orchestrator processes would have separate, unsynchronized heaps leading to job duplication. This aligns with our non-goal of distributed workers. If the orchestrator process crashes, the in-memory heap is lost, but can be safely rebuilt from the database, though any jobs `STATUS_RUNNING` at the time of the crash will need a recovery mechanism (e.g., a timeout-based reset to `STATUS_PENDING`).\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| In-Memory Channel (`chan`) | Extremely simple, native concurrency, very fast. | No persistence (jobs lost on crash), single-process only, no priority support. | No |\n| Redis (External) | Persistent, high performance, supports pub/sub and priority queues, battle-tested for distributed systems. | Adds operational complexity, requires running another service, network dependency. | No |\n| Hybrid (SQLite + In-Memory Heap) | Durable (via SQLite), good single-process performance, leverages existing store, supports priority. | Not distributed, requires startup recovery logic, potential state drift if not carefully managed. | **Yes** |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Insecure Webhook Signature Verification**\n*   **Description**: Implementing signature verification using a simple string equality (`==`) operator, which is vulnerable to timing attacks. An attacker could theoretically deduce the secret by measuring tiny differences in response time.\n*   **Why it's wrong**: It compromises the entire system's security. An attacker could forge webhook events, triggering arbitrary pipeline executions, potentially leading to unauthorized code deployment or resource exhaustion.\n*   **Fix**: Always use a constant-time comparison function. In Go, use `crypto/hmac`'s `hmac.Equal`. Never log the secret or the computed signature.\n\n⚠️ **Pitfall: Queue Starvation**\n*   **Description**: Low-priority jobs (e.g., from long-running feature branches) are never executed because higher-priority jobs (from `main`) continuously enter the queue.\n*   **Why it's wrong**: It violates fairness. Developers on feature branches receive no feedback, undermining the purpose of CI. The queue backlog grows indefinitely.\n*   **Fix**: Implement a **fair priority queue**. Instead of strict priority, use a weighted round-robin or aging strategy. For example, increase a job's effective priority the longer it waits in the queue. A simple method is to periodically promote a batch of old, low-priority jobs.\n\n⚠️ **Pitfall: Lost Jobs on Orchestrator Crash**\n*   **Description**: If the orchestrator process crashes while a job is `STATUS_RUNNING`, the corresponding worker may complete the job, but there is no process to call `MarkJobComplete`. The job remains `STATUS_RUNNING` forever in the database, and dependent jobs are blocked.\n*   **Why it's wrong**: It causes pipeline hangs and requires manual database intervention to fix.\n*   **Fix**: Implement a **dead man's switch** or **heartbeat**. Workers must periodically update a `last_heartbeat` timestamp on their assigned `JobRun`. A recovery goroutine (or startup routine) scans for jobs where `status = 'RUNNING'` and `last_heartbeat` is older than a threshold (e.g., 5 minutes) and resets them to `STATUS_PENDING`. This allows another worker to retry the job.\n\n⚠️ **Pitfall: Concurrent Worker Coordination Race Condition**\n*   **Description**: Two workers call `DequeueJob` at nearly the same time. The naive implementation reads the same pending job, and both proceed to execute it, leading to duplicate execution, resource contention, and potential artifact corruption.\n*   **Why it's wrong**: Breaks the core queue guarantee of \"exactly once\" delivery (within the context of a single job run). Wastes resources and produces confusing, non-deterministic results.\n*   **Fix**: The dequeue operation **must be atomic**. Implement it within a database transaction that includes a `SELECT ... FOR UPDATE` (or SQLite's `BEGIN EXCLUSIVE` transaction) to lock the row, check its status, and update it. The in-memory heap should only be used as a fast index; the final assignment must be committed via the database.\n\n### Implementation Guidance\n\nThis section provides concrete starter code and skeletons to implement the Webhook & Queue system in Go.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | `net/http` with Gorilla Mux or `http.ServeMux` | `chi` router for middleware and structured routes |\n| Queue Backend | SQLite (`modernc.org/sqlite`) + `container/heap` | Redis (`go-redis/redis`) with Sorted Sets (`ZSET`) |\n| Webhook Verification | `crypto/hmac` for manual signing | Use provider SDKs (e.g., `google/go-github`) |\n| Worker Pool | Goroutines + channels managed in-process | External worker binaries coordinated via RPC |\n\n#### B. Recommended File/Module Structure\n\n```\nyour-ci-system/\n├── cmd/\n│   ├── orchestrator/          # Main orchestrator process (webhook + queue)\n│   │   └── main.go\n│   └── worker/                # Worker process (job execution)\n│       └── main.go\n├── internal/\n│   ├── queue/                 # This component\n│   │   ├── queue.go           # Core Queue interface and hybrid implementation\n│   │   ├── webhook.go         # HTTP handlers and signature verification\n│   │   ├── worker_pool.go     # In-process worker pool management\n│   │   └── priority_heap.go   # In-memory heap implementation\n│   ├── store/                 # Data persistence (from Data Model section)\n│   │   └── sqlite.go\n│   └── executor/              # Job Execution Engine (from Milestone 2)\n│       └── docker_executor.go\n└── webhook_payloads/          # Example JSON payloads for testing\n    ├── github_push.json\n    └── gitlab_merge_request.json\n```\n\n#### C. Infrastructure Starter Code\n\n**Complete HTTP Webhook Handler Helper (`internal/queue/webhook.go`):**\n\n```go\npackage queue\n\nimport (\n    \"crypto/hmac\"\n    \"crypto/sha256\"\n    \"encoding/hex\"\n    \"errors\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\n// WebhookVerifier handles signature verification for different Git providers.\ntype WebhookVerifier struct {\n    gitHubSecret []byte\n    gitLabToken  string\n}\n\n// NewWebhookVerifier creates a verifier with provided secrets.\nfunc NewWebhookVerifier(gitHubSecret, gitLabToken string) *WebhookVerifier {\n    return &WebhookVerifier{\n        gitHubSecret: []byte(gitHubSecret),\n        gitLabToken:  gitLabToken,\n    }\n}\n\n// VerifyGitHubSignature validates a GitHub webhook signature.\n// signatureHeader is the value of the \"X-Hub-Signature-256\" header.\n// body is the raw HTTP request body.\nfunc (v *WebhookVerifier) VerifyGitHubSignature(signatureHeader string, body []byte) error {\n    if len(v.gitHubSecret) == 0 {\n        return errors.New(\"github secret not configured\")\n    }\n\n    const signaturePrefix = \"sha256=\"\n    if len(signatureHeader) < len(signaturePrefix) {\n        return errors.New(\"invalid signature format\")\n    }\n    actualSignature := signatureHeader[len(signaturePrefix):]\n\n    mac := hmac.New(sha256.New, v.gitHubSecret)\n    mac.Write(body)\n    expectedSignature := hex.EncodeToString(mac.Sum(nil))\n\n    // Use constant-time comparison to prevent timing attacks\n    if !hmac.Equal([]byte(expectedSignature), []byte(actualSignature)) {\n        return errors.New(\"invalid signature\")\n    }\n    return nil\n}\n\n// VerifyGitLabToken validates a GitLab webhook token.\n// tokenHeader is the value of the \"X-GitLab-Token\" header.\nfunc (v *WebhookVerifier) VerifyGitLabToken(tokenHeader string) error {\n    if v.gitLabToken == \"\" {\n        return errors.New(\"gitlab token not configured\")\n    }\n    if !hmac.Equal([]byte(tokenHeader), []byte(v.gitLabToken)) {\n        return errors.New(\"invalid token\")\n    }\n    return nil\n}\n\n// ExtractEventDetails parses common fields from GitHub and GitLab payloads.\n// Returns repoSSHUrl, branch, commitSHA, eventType.\nfunc ExtractEventDetails(r *http.Request, body []byte) (string, string, string, string, error) {\n    // This is a simplified example. A real implementation would fully unmarshal\n    // into provider-specific structs.\n    provider := r.Header.Get(\"User-Agent\")\n    var repo, branch, sha, event string\n\n    if contains(provider, \"GitHub\") {\n        event = r.Header.Get(\"X-GitHub-Event\")\n        // Parse GitHub JSON to extract repo, ref, and sha...\n        // (Use github.com/google/go-github/v50/github for full parsing)\n    } else if contains(provider, \"GitLab\") {\n        event = r.Header.Get(\"X-Gitlab-Event\")\n        // Parse GitLab JSON...\n    } else {\n        return \"\", \"\", \"\", \"\", errors.New(\"unknown Git provider\")\n    }\n    // Return extracted values or placeholders\n    return repo, branch, sha, event, nil\n}\n\nfunc contains(s, substr string) bool {\n    //... helper implementation\n}\n\n// Middleware to read and restore the request body for signature verification.\nfunc readBodyMiddleware(next http.HandlerFunc) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        body, err := io.ReadAll(r.Body)\n        if err != nil {\n            http.Error(w, \"Bad request\", http.StatusBadRequest)\n            return\n        }\n        r.Body.Close()\n        // Restore the body for subsequent handlers\n        r.Body = io.NopCloser(bytes.NewBuffer(body))\n        // Store the raw body in the request context\n        ctx := context.WithValue(r.Context(), \"raw_body\", body)\n        next.ServeHTTP(w, r.WithContext(ctx))\n    }\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**Priority Heap for In-Memory Queue (`internal/queue/priority_heap.go`):**\n\n```go\npackage queue\n\nimport (\n    \"container/heap\"\n    \"time\"\n)\n\n// Item represents an entry in the priority queue.\ntype Item struct {\n    JobRunID      string\n    PipelineRunID string\n    Priority      int    // Higher number = higher priority\n    CreatedAt     time.Time\n    index         int // Internal index for heap.Interface\n}\n\n// PriorityQueue implements heap.Interface.\ntype PriorityQueue []*Item\n\nfunc (pq PriorityQueue) Len() int { return len(pq) }\n\nfunc (pq PriorityQueue) Less(i, j int) bool {\n    // We want Pop to give us the highest priority, so use greater than.\n    if pq[i].Priority == pq[j].Priority {\n        // If priorities equal, older items (smaller CreatedAt) have higher priority.\n        return pq[i].CreatedAt.Before(pq[j].CreatedAt)\n    }\n    return pq[i].Priority > pq[j].Priority\n}\n\nfunc (pq PriorityQueue) Swap(i, j int) {\n    pq[i], pq[j] = pq[j], pq[i]\n    pq[i].index = i\n    pq[j].index = j\n}\n\nfunc (pq *PriorityQueue) Push(x interface{}) {\n    n := len(*pq)\n    item := x.(*Item)\n    item.index = n\n    *pq = append(*pq, item)\n}\n\nfunc (pq *PriorityQueue) Pop() interface{} {\n    old := *pq\n    n := len(old)\n    item := old[n-1]\n    old[n-1] = nil  // avoid memory leak\n    item.index = -1 // for safety\n    *pq = old[0 : n-1]\n    return item\n}\n```\n\n**Hybrid Queue Manager (`internal/queue/queue.go`):**\n\n```go\npackage queue\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// Queue defines the interface for enqueuing and dequeuing jobs.\ntype Queue interface {\n    EnqueueJobRun(ctx context.Context, job *JobRun) error\n    DequeueJobRun(ctx context.Context, workerID string) (*JobRun, error)\n    MarkJobRunComplete(ctx context.Context, jobID string, status string) error\n}\n\n// HybridQueue implements Queue using SQLite for persistence and an in-memory heap for priority.\ntype HybridQueue struct {\n    store Store\n    heap  *PriorityQueue\n    mu    sync.Mutex\n}\n\nfunc NewHybridQueue(store Store) *HybridQueue {\n    hq := &HybridQueue{\n        store: store,\n        heap:  &PriorityQueue{},\n    }\n    heap.Init(hq.heap)\n    go hq.recoverPendingJobs()\n    return hq\n}\n\n// EnqueueJobRun persists the job and pushes it onto the in-memory heap.\nfunc (hq *HybridQueue) EnqueueJobRun(ctx context.Context, job *JobRun) error {\n    // TODO 1: Validate job status is STATUS_PENDING.\n    // TODO 2: Persist the job via hq.store.UpdateJobRun (ensure it's marked PENDING).\n    // TODO 3: Calculate priority: e.g., priority=10 for main branch, priority=5 for tags, priority=1 for other branches.\n    // TODO 4: Acquire mutex lock (hq.mu.Lock()), push a new *Item onto the heap, release lock.\n    // TODO 5: Log the enqueue operation.\n    return nil\n}\n\n// DequeueJobRun atomically assigns the highest-priority eligible job to a worker.\nfunc (hq *HybridQueue) DequeueJobRun(ctx context.Context, workerID string) (*JobRun, error) {\n    // TODO 1: Acquire mutex lock (hq.mu.Lock()).\n    // TODO 2: If heap is empty, release lock and return an error indicating no jobs.\n    // TODO 3: Start a database transaction (hq.store.BeginTx).\n    // TODO 4: Peek at the highest-priority item from the heap (do not pop yet).\n    // TODO 5: In the transaction, SELECT the corresponding JobRun FOR UPDATE where status = STATUS_PENDING.\n    // TODO 6: Verify the job's dependencies are met by checking the status of all jobs in its Needs list.\n    // TODO 7: If job is eligible, update its status to STATUS_RUNNING, set AssignedWorker and StartedAt.\n    // TODO 8: Commit the transaction. On success, pop the item from the heap and return the updated JobRun.\n    // TODO 9: If the job is not eligible (dependencies not met), remove it from the heap temporarily, continue to the next item. Consider re-adding it later.\n    // TODO 10: On any error, rollback the transaction, release the lock, and return the error.\n    return nil, nil\n}\n\n// MarkJobRunComplete updates job status and potentially unblocks dependent jobs.\nfunc (hq *HybridQueue) MarkJobRunComplete(ctx context.Context, jobID string, status string) error {\n    // TODO 1: Update the JobRun in the store: set status, FinishedAt.\n    // TODO 2: Query for all JobRuns in the same PipelineRun that have a 'Needs' field containing this completed job's name.\n    // TODO 3: For each dependent job, check if ALL of its needed jobs are now complete (STATUS_SUCCEEDED or STATUS_FAILED/SKIPPED depending on semantics).\n    // TODO 4: If a dependent job's dependencies are now satisfied and it's still PENDING, ensure it has an Item in the heap (it may have been skipped earlier in Dequeue). You may need to re-add it.\n    // TODO 5: If all jobs in the PipelineRun are complete, update the PipelineRun status accordingly.\n    return nil\n}\n\n// recoverPendingJobs runs on startup to rebuild the heap from persistent storage.\nfunc (hq *HybridQueue) recoverPendingJobs() {\n    // TODO 1: Query hq.store for all JobRuns with status = STATUS_PENDING.\n    // TODO 2: For each, calculate priority and push onto the heap.\n    // TODO 3: Also query for JobRuns with status = STATUS_RUNNING but with old heartbeat. Reset them to PENDING and push onto heap.\n}\n```\n\n#### E. Language-Specific Hints\n\n*   **Concurrency**: Use `sync.Mutex` to protect the in-memory heap. For database operations, rely on SQL transactions (`BEGIN EXCLUSIVE` in SQLite) for atomicity.\n*   **Context Propagation**: Pass `context.Context` through all queue methods to enable cancellation and timeouts, especially important for long-running database queries.\n*   **Error Handling**: Distinguish between transient errors (e.g., database temporary unavailability) and permanent errors (e.g., invalid job data). For transient errors, implement retry with exponential backoff in the worker.\n*   **Logging**: Use structured logging (e.g., `log/slog`) with fields like `job_id`, `pipeline_run_id`, and `worker_id` to trace execution across components.\n\n#### F. Milestone Checkpoint\n\nTo verify your Webhook & Queue system is functioning:\n\n1.  **Start the Orchestrator**: Run `go run cmd/orchestrator/main.go`. It should start an HTTP server on port 8080.\n2.  **Send a Test Webhook**: Use `curl` to simulate a GitHub push event:\n    ```bash\n    curl -X POST http://localhost:8080/webhook \\\n      -H \"Content-Type: application/json\" \\\n      -H \"X-GitHub-Event: push\" \\\n      -H \"X-Hub-Signature-256: sha256=<compute-using-secret>\" \\\n      -d @webhook_payloads/github_push.json\n    ```\n    You should see logs indicating the webhook was received, validated, and a pipeline run was created.\n3.  **Check the Queue**: A second log line should indicate the job was enqueued. You can also expose a simple `GET /queue/stats` endpoint that uses `GetQueueStats` to display pending/running counts.\n4.  **Start a Worker**: In another terminal, run `go run cmd/worker/main.go`. The worker should log that it's polling for jobs, then almost immediately dequeue and start executing the test job.\n5.  **Verify Database State**: Query the SQLite database: `sqlite3 ci.db \"SELECT id, status FROM job_runs;\"`. You should see the job's status transition from `PENDING` to `RUNNING` to `SUCCEEDED`.\n\n**Signs of Trouble**:\n*   **Webhook returns 403**: Check your secret token and signature calculation. Ensure the middleware reads the body correctly.\n*   **Job stays in PENDING**: The worker may not be running, or the `DequeueJobRun` logic has a bug (e.g., dependency check failing incorrectly).\n*   **Duplicate job execution**: The atomicity in `DequeueJobRun` is broken. Verify your database transaction and row-level locking.\n\n\n## Component: Web Dashboard (Milestone 4)\n\n> **Milestone(s):** Milestone 4\n\nThis component serves as the **visual command center** for the CI/CD Pipeline Orchestrator, providing developers with real-time visibility into pipeline execution, historical trends, and diagnostic information. It transforms raw execution data—status updates, log streams, and dependency graphs—into an intuitive, actionable interface. The dashboard acts as the primary feedback mechanism, closing the loop between automated systems and human operators by delivering immediate, contextualized insights about build health and execution progress.\n\n![Web Dashboard Component Breakdown](./diagrams/diag-dashboard-component.svg)\n\n### Mental Model: The Airport Flight Display Board\n\nThink of the Web Dashboard as an **airport flight information display system**. Each pipeline run is a *flight* with a unique identifier (like a flight number). The overall status (pending, running, succeeded, failed) corresponds to a flight's status (scheduled, boarding, in-air, landed, cancelled). Individual jobs within the pipeline are like *gates* or *boarding areas*—subcomponents of the flight operation with their own status and timing. The real-time log streaming is the *live announcements and status updates* broadcast to passengers (developers) waiting at the gate, providing immediate feedback about delays or progress. The dependency graph visualization is the *airport terminal map* showing how gates connect via walkways (dependencies), helping you understand the overall flow. Finally, the status badges are the *flight status displays on external airport websites*—simple, embeddable indicators for external consumers. This mental model clarifies the dashboard's dual role: providing detailed operational data for engineers while offering high-level status indicators for external systems and quick glances.\n\n### Interface and API\n\nThe dashboard exposes a RESTful HTTP API alongside serving static web assets. The API follows resource-oriented design, with primary resources being `PipelineRun` and `JobRun` entities. Real-time updates are delivered via Server-Sent Events (SSE), providing efficient, unidirectional streaming from server to client.\n\n| Method | Endpoint | Parameters | Returns (Content-Type) | Description |\n|--------|----------|------------|------------------------|-------------|\n| `GET`  | `/` | None | `text/html` | Serves the main dashboard Single Page Application (SPA) HTML |\n| `GET`  | `/api/runs` | `limit` (query, default 50), `offset` (query, default 0) | `application/json` (`[]PipelineRun`) | Returns paginated list of pipeline runs, sorted by `CreatedAt` descending |\n| `GET`  | `/api/runs/:id` | `id` (path parameter) | `application/json` (`PipelineRun` with nested `JobRun`s) | Returns detailed information for a specific pipeline run |\n| `GET`  | `/api/runs/:id/logs` | `id` (path), `job` (query, optional), `step` (query, optional) | `text/event-stream` | Server-Sent Events stream of log output for a run, optionally filtered by job or step |\n| `GET`  | `/api/runs/:id/dag` | `id` (path) | `application/json` (DAG structure) | Returns dependency graph data for visualization of a specific pipeline run |\n| `GET`  | `/api/runs/:id/artifacts` | `id` (path) | `application/json` (`[]ArtifactMetadata`) | Lists artifact metadata (name, size, path) available for download |\n| `GET`  | `/api/artifacts/:key/download` | `key` (path) | `application/octet-stream` (file) | Streams the artifact file for download; requires authentication in production |\n| `GET`  | `/badge/:repo.svg` | `repo` (path), `branch` (query, default \"main\") | `image/svg+xml` | Returns an SVG status badge showing pipeline status for a repository/branch |\n| `GET`  | `/assets/*` | Wildcard path | Static files (JS, CSS, images) | Serves static assets for the dashboard SPA |\n| `POST` | `/api/runs/:id/cancel` | `id` (path) | `application/json` (`{\"success\": boolean}`) | Attempts to cancel a running pipeline; marks as `STATUS_CANCELLED` |\n\nThe API is designed for simplicity and leverages the existing `Store` interface for data persistence. The `PipelineRun` returned from `/api/runs/:id` includes its nested `JobRun` slices, each potentially containing `StepRun` data for comprehensive inspection. The DAG endpoint returns a specialized structure optimized for frontend visualization libraries.\n\n### Internal Behavior and Algorithm\n\nThe dashboard component orchestrates multiple subsystems: an HTTP server, real-time event broadcasting, SVG generation, and data aggregation. Its operation follows these coordinated processes:\n\n1. **HTTP Server Initialization and Static Asset Serving**\n   - The component starts an HTTP server listening on a configured port (e.g., `:8080`)\n   - It registers route handlers for API endpoints and static file serving\n   - For the root path `/`, it serves a pre-built `index.html` that loads the JavaScript SPA\n   - Static assets (JS, CSS, images) are served from an embedded filesystem or directory with appropriate cache headers (1-hour max-age for immutable assets)\n\n2. **Pipeline Run Listing and Detail Retrieval**\n   - When `/api/runs` is requested, the handler calls `ListPipelineRuns` on the `Store` with `limit` and `offset` parsed from query parameters\n   - The results are serialized to JSON, with timestamps formatted as RFC3339 strings for consistency\n   - For `/api/runs/:id`, the handler retrieves the `PipelineRun` by ID, then fetches all associated `JobRun`s using a `GetJobRunsForPipeline` method (not defined in conventions but implied)\n   - The nested structure is assembled and returned as a single JSON object\n\n3. **Real-Time Log Streaming via Server-Sent Events**\n   - When a client connects to `/api/runs/:id/logs`, the server establishes an SSE connection by setting headers: `Content-Type: text/event-stream`, `Cache-Control: no-cache`, `Connection: keep-alive`\n   - The handler registers the client's connection channel with a central `LogBroadcaster` component, keyed by pipeline run ID (and optionally job/step filters)\n   - As the job execution engine writes log lines (via `StreamLogs`), it publishes them to the `LogBroadcaster`\n   - The broadcaster formats each log line as an SSE event: `data: <JSON-encoded log chunk>\\n\\n` and sends to all registered clients\n   - The connection is kept alive with periodic heartbeat comments (`: ping\\n\\n`) every 30 seconds to prevent timeout\n   - On client disconnect or pipeline completion, the handler unregisters the client channel\n\n4. **SVG Badge Generation**\n   - The badge endpoint `/badge/:repo.svg` extracts repository identifier and branch from request parameters\n   - It queries the `Store` for the most recent `PipelineRun` for that repository/branch with status in `[STATUS_SUCCEEDED, STATUS_FAILED, STATUS_CANCELLED]`\n   - Based on the status, it selects a color (green for success, red for failure, orange for cancelled, gray for pending/running)\n   - It renders an inline SVG using a template with dynamic text (`{repo} | {branch} | {status}`), color, and optional animation for running builds\n   - The response includes caching headers: `Cache-Control: max-age=60` for dynamic status, `max-age=3600` for completed builds\n\n5. **Dependency Graph Visualization Data Preparation**\n   - The DAG endpoint `/api/runs/:id/dag` retrieves the pipeline run and its configuration\n   - It reconstructs the execution graph using the original `BuildExecutionGraph` algorithm from the parsed `PipelineConfig`\n   - It transforms this into a frontend-friendly format: nodes with `id`, `label`, `status`, `type` (\"stage\" or \"job\"), and edges with `source`, `target`, `type` (\"depends_on\")\n   - The graph is enriched with runtime data: actual start/end times, durations, and status colors from the `JobRun` records\n   - The resulting JSON structure is optimized for libraries like Cytoscape.js or D3.js\n\n6. **Artifact Listing and Secure Download**\n   - The artifacts endpoint lists files stored during `CollectArtifacts` execution\n   - Each artifact is represented with metadata: `name`, `size_bytes`, `storage_key`, `content_type` (inferred from extension)\n   - The download endpoint streams files from persistent storage (e.g., local filesystem, S3-compatible storage) with proper content-disposition headers\n   - In a production system, this endpoint would validate authentication tokens; for the educational implementation, it may be omitted or use a simple shared secret\n\n### ADR: Real-time Log Streaming: WebSockets vs. Server-Sent Events\n\n> **Decision: Use Server-Sent Events (SSE) for real-time log streaming**\n> - **Context**: The dashboard needs to push real-time log output from running jobs to connected browsers with minimal latency. The solution must be simple to implement, reliable, and handle potentially long-lived connections (jobs running 30+ minutes).\n> - **Options Considered**: \n>   1. **WebSockets**: Full-duplex communication channel over a single TCP connection. Provides bidirectional messaging.\n>   2. **Server-Sent Events (SSE)**: Unidirectional server-to-client streaming over standard HTTP, automatically reconnects.\n>   3. **Long-polling**: Client repeatedly requests updates, server holds connection until new data arrives.\n> - **Decision**: Implement Server-Sent Events for log streaming.\n> - **Rationale**: \n>   - **Simplicity**: SSE uses plain HTTP with minimal protocol overhead; no special framing or handshake needed. The browser's `EventSource` API is trivial to use.\n>   - **Unidirectional nature fits the use case**: Log streaming is purely server→client; we don't need client→server messages during streaming.\n>   - **Automatic reconnection**: The `EventSource` API automatically reconnects on failure, providing resilience to network interruptions.\n>   - **HTTP compatibility**: Works seamlessly with existing HTTP infrastructure (load balancers, firewalls) and doesn't require special WebSocket support.\n>   - **Resource efficiency**: Less overhead than maintaining full WebSocket connections when only unidirectional streaming is needed.\n> - **Consequences**:\n>   - **Positive**: Easier implementation and debugging (plain text protocol), better browser compatibility for our needs, automatic reconnection handling.\n>   - **Negative**: Limited to UTF-8 text data (sufficient for logs), maximum concurrent connections per browser (6 per domain, but sufficient for our use case), no bidirectional communication without separate HTTP requests.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| **WebSockets** | Full-duplex, low latency, efficient for high-frequency messages | More complex protocol, requires connection upgrade, manual reconnection logic | No |\n| **Server-Sent Events (SSE)** | Simple HTTP-based, automatic reconnection, perfect for server→client streaming | Unidirectional only, limited to text data, browser connection limits | **Yes** |\n| **Long-polling** | Works with any HTTP server, simple to implement | Higher latency, repeated connection overhead, less efficient | No |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Browser Connection Limits Exhaustion**\n*Description*: Modern browsers limit concurrent connections to a single domain (typically 6). If the dashboard opens multiple SSE connections for different pipeline runs simultaneously, it can exhaust this limit, blocking other resources (CSS, JS, API calls).\n*Why it's wrong*: This causes the dashboard to freeze or fail to load additional runs while streaming logs from multiple active pipelines.\n*Fix*: Implement connection management: 1) Close SSE connections when users navigate away from the log view; 2) Use a single multiplexed SSE connection that streams all logs with filtering on the client side; 3) Implement exponential backoff for reconnection attempts.\n\n⚠️ **Pitfall: Log Streaming Backpressure and Memory Buildup**\n*Description*: When a job produces logs faster than the browser can consume them (slow network, busy client), unbuffered writes to the SSE connection can cause memory buildup on the server as it buffers unsent data.\n*Why it's wrong*: This can lead to memory exhaustion, crashing the dashboard server during high-output jobs.\n*Fix*: Implement backpressure control: 1) Use buffered channels with limited capacity for log broadcasting; 2) Drop old log lines if buffer exceeds limit (with warning); 3) Monitor client readiness using the underlying TCP flow control.\n\n⚠️ **Pitfall: Missing Cache Headers on SVG Badges**\n*Description*: SVG badges served without proper `Cache-Control` headers cause every page load (e.g., README display) to request a fresh badge, overwhelming the server.\n*Why it's wrong*: This creates unnecessary load and delays badge display. GitHub caches external images aggressively, but without proper headers, requests still reach the server.\n*Fix*: Set appropriate cache headers based on build status: `max-age=60` for running/pending builds (frequent updates), `max-age=3600` for completed builds (stable status). Use `ETag` or `Last-Modified` headers for conditional requests.\n\n⚠️ **Pitfall: Cross-Origin Requests Blocked for Embedded Badges**\n*Description*: When badges are embedded in external sites (GitHub README), the browser's same-origin policy may block the SVG image request if CORS headers aren't set.\n*Why it's wrong*: Badges appear as broken images on GitHub or other external sites.\n*Fix*: Add CORS headers to the badge endpoint: `Access-Control-Allow-Origin: *` (or specific origins). Also ensure the endpoint responds correctly to `OPTIONS` preflight requests if needed.\n\n⚠️ **Pitfall: Inefficient DAG Data Structure for Large Pipelines**\n*Description*: Returning the entire execution graph with all job details for pipelines with 100+ jobs creates massive JSON payloads (MBs), slowing page load.\n*Why it's wrong*: The dashboard becomes unresponsive while loading, and the visualization may render poorly with too many nodes.\n*Fix*: Implement pagination or progressive loading for the DAG: 1) Return only top-level stages initially; 2) Load job details on demand when nodes are expanded; 3) Use server-side filtering to return only visible portions of the graph.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | Standard library `net/http` with `gorilla/mux` for routing | `chi` router for lightweight routing with middleware |\n| Static File Serving | `embed` package (Go 1.16+) for embedding built frontend | CDN serving with cache invalidation |\n| Real-time Streaming | Server-Sent Events via `http.ResponseWriter` | `github.com/r3labs/sse` library for SSE utilities |\n| Frontend Framework | Vanilla JavaScript with minimal dependencies | React/Vue.js SPA with state management |\n| SVG Generation | Go template with inline SVG XML | `github.com/ajstarks/svgo` for programmatic SVG |\n| DAG Visualization | Preprocess JSON for `cytoscape.js` | Custom Canvas rendering with `d3.js` |\n\n**B. Recommended File/Module Structure**\n\n```\nproject-root/\n├── cmd/\n│   └── dashboard/\n│       └── main.go              # Dashboard entry point\n├── internal/\n│   ├── dashboard/\n│   │   ├── server.go            # HTTP server setup and routing\n│   │   ├── handlers.go          # HTTP route handlers\n│   │   ├── broadcaster.go       # Log broadcasting for SSE\n│   │   ├── badges.go            # SVG badge generation\n│   │   ├── dag.go               # DAG data preparation\n│   │   └── artifacts.go         # Artifact serving logic\n│   ├── store/                   # Storage interface and implementations\n│   └── models.go                # Shared data structures (PipelineRun, etc.)\n├── web/\n│   ├── public/                  # Static assets (built frontend)\n│   │   ├── index.html\n│   │   ├── app.js\n│   │   └── style.css\n│   └── src/                     # Frontend source (optional)\n│       └── ...                  # JavaScript/TypeScript source files\n└── go.mod\n```\n\n**C. Infrastructure Starter Code (COMPLETE, ready to use)**\n\n```go\n// internal/dashboard/broadcaster.go\npackage dashboard\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\n// LogEvent represents a single log line or chunk for broadcasting\ntype LogEvent struct {\n\tPipelineRunID string    `json:\"pipeline_run_id\"`\n\tJobRunID      string    `json:\"job_run_id,omitempty\"`\n\tStepName      string    `json:\"step_name,omitempty\"`\n\tTimestamp     time.Time `json:\"timestamp\"`\n\tLine          string    `json:\"line\"`\n\tStream        string    `json:\"stream\"` // \"stdout\" or \"stderr\"\n}\n\n// ClientChannel represents a connected SSE client\ntype ClientChannel struct {\n\tID       string\n\tPipelineRunID string\n\tJobFilter    string // optional\n\tStepFilter   string // optional\n\tSend         chan []byte\n}\n\n// LogBroadcaster manages real-time log distribution\ntype LogBroadcaster struct {\n\tmu            sync.RWMutex\n\tclients       map[string]map[string]*ClientChannel // pipelineID -> clientID -> client\n\tclientCounter int\n}\n\n// NewLogBroadcaster creates a new broadcaster instance\nfunc NewLogBroadcaster() *LogBroadcaster {\n\treturn &LogBroadcaster{\n\t\tclients: make(map[string]map[string]*ClientChannel),\n\t}\n}\n\n// RegisterClient adds a new client for a specific pipeline run\nfunc (b *LogBroadcaster) RegisterClient(pipelineRunID, jobFilter, stepFilter string) *ClientChannel {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\t\n\tb.clientCounter++\n\tclientID := fmt.Sprintf(\"client-%d\", b.clientCounter)\n\tclient := &ClientChannel{\n\t\tID:            clientID,\n\t\tPipelineRunID: pipelineRunID,\n\t\tJobFilter:     jobFilter,\n\t\tStepFilter:    stepFilter,\n\t\tSend:          make(chan []byte, 100), // Buffered channel for backpressure\n\t}\n\t\n\tif _, exists := b.clients[pipelineRunID]; !exists {\n\t\tb.clients[pipelineRunID] = make(map[string]*ClientChannel)\n\t}\n\tb.clients[pipelineRunID][clientID] = client\n\t\n\treturn client\n}\n\n// UnregisterClient removes a client\nfunc (b *LogBroadcaster) UnregisterClient(pipelineRunID, clientID string) {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\t\n\tif pipelineClients, ok := b.clients[pipelineRunID]; ok {\n\t\tif client, ok := pipelineClients[clientID]; ok {\n\t\t\tclose(client.Send)\n\t\t\tdelete(pipelineClients, clientID)\n\t\t}\n\t\tif len(pipelineClients) == 0 {\n\t\t\tdelete(b.clients, pipelineRunID)\n\t\t}\n\t}\n}\n\n// Broadcast sends a log event to all interested clients\nfunc (b *LogBroadcaster) Broadcast(event LogEvent) {\n\tb.mu.RLock()\n\tdefer b.mu.RUnlock()\n\t\n\tclients, ok := b.clients[event.PipelineRunID]\n\tif !ok {\n\t\treturn\n\t}\n\t\n\tjsonData, err := json.Marshal(event)\n\tif err != nil {\n\t\treturn\n\t}\n\tsseData := fmt.Sprintf(\"data: %s\\n\\n\", jsonData)\n\t\n\tfor _, client := range clients {\n\t\t// Apply filters\n\t\tif client.JobFilter != \"\" && client.JobFilter != event.JobRunID {\n\t\t\tcontinue\n\t\t}\n\t\tif client.StepFilter != \"\" && client.StepFilter != event.StepName {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\t// Non-blocking send with buffer\n\t\tselect {\n\t\tcase client.Send <- []byte(sseData):\n\t\t\t// Successfully sent\n\t\tdefault:\n\t\t\t// Buffer full, drop event for this client\n\t\t\t// Optionally log or implement backpressure strategy\n\t\t}\n\t}\n}\n\n// SSEHandler sets up Server-Sent Events response\nfunc (b *LogBroadcaster) SSEHandler(w http.ResponseWriter, r *http.Request, pipelineRunID, jobFilter, stepFilter string) {\n\t// Set SSE headers\n\tw.Header().Set(\"Content-Type\", \"text/event-stream\")\n\tw.Header().Set(\"Cache-Control\", \"no-cache\")\n\tw.Header().Set(\"Connection\", \"keep-alive\")\n\tw.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n\t\n\t// Create flusher\n\tflusher, ok := w.(http.Flusher)\n\tif !ok {\n\t\thttp.Error(w, \"Streaming unsupported\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\t\n\t// Register client\n\tclient := b.RegisterClient(pipelineRunID, jobFilter, stepFilter)\n\tdefer b.UnregisterClient(pipelineRunID, client.ID)\n\t\n\t// Send initial comment to establish connection\n\tfmt.Fprintf(w, \": connected\\n\\n\")\n\tflusher.Flush()\n\t\n\t// Heartbeat ticker\n\theartbeat := time.NewTicker(30 * time.Second)\n\tdefer heartbeat.Stop()\n\t\n\t// Handle client close\n\tctx := r.Context()\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\t// Client disconnected\n\t\t\treturn\n\t\tcase <-heartbeat.C:\n\t\t\t// Send heartbeat comment\n\t\t\tfmt.Fprintf(w, \": ping\\n\\n\")\n\t\t\tflusher.Flush()\n\t\tcase data, ok := <-client.Send:\n\t\t\tif !ok {\n\t\t\t\t// Channel closed\n\t\t\t\treturn\n\t\t\t}\n\t\t\t_, err := w.Write(data)\n\t\t\tif err != nil {\n\t\t\t\t// Client likely disconnected\n\t\t\t\treturn\n\t\t\t}\n\t\t\tflusher.Flush()\n\t\t}\n\t}\n}\n```\n\n**D. Core Logic Skeleton Code (signature + TODOs only)**\n\n```go\n// internal/dashboard/handlers.go\npackage dashboard\n\nimport (\n\t\"encoding/json\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\t\n\t\"yourproject/internal/store\"\n)\n\ntype DashboardServer struct {\n\tStore         store.Store\n\tBroadcaster   *LogBroadcaster\n\t// ... other dependencies\n}\n\n// GetRuns handles GET /api/runs\nfunc (s *DashboardServer) GetRuns(w http.ResponseWriter, r *http.Request) {\n\t// TODO 1: Parse limit and offset query parameters with defaults (limit=50, offset=0)\n\t// TODO 2: Call store.ListPipelineRuns with the parsed parameters\n\t// TODO 3: Handle potential error from store (return HTTP 500 with appropriate message)\n\t// TODO 4: Set Content-Type header to application/json\n\t// TODO 5: Encode the runs slice as JSON and write to response\n\t// TODO 6: Consider adding pagination metadata (total count, next/prev links) in response\n}\n\n// GetRunDetail handles GET /api/runs/:id\nfunc (s *DashboardServer) GetRunDetail(w http.ResponseWriter, r *http.Request) {\n\t// TODO 1: Extract pipeline run ID from URL path (use gorilla/mux or chi)\n\t// TODO 2: Call store.GetPipelineRun to retrieve the main run object\n\t// TODO 3: Call store.GetJobRunsForPipeline to retrieve all associated job runs\n\t// TODO 4: Assemble a response DTO that nests job runs within the pipeline run\n\t// TODO 5: For each job run, optionally fetch step details if needed for the UI\n\t// TODO 6: Encode and return the combined response as JSON\n\t// TODO 7: Handle not found errors (return HTTP 404) and other errors appropriately\n}\n\n// StreamLogs handles GET /api/runs/:id/logs\nfunc (s *DashboardServer) StreamLogs(w http.ResponseWriter, r *http.Request) {\n\t// TODO 1: Extract pipeline run ID from URL path\n\t// TODO 2: Extract optional job and step filters from query parameters\n\t// TODO 3: Validate that the pipeline run exists and is accessible\n\t// TODO 4: Call broadcaster.SSEHandler with the appropriate parameters\n\t// TODO 5: Note: This method should not return until the SSE connection closes\n}\n\n// GetBadge handles GET /badge/:repo.svg\nfunc (s *DashboardServer) GetBadge(w http.ResponseWriter, r *http.Request) {\n\t// TODO 1: Extract repository identifier from URL path\n\t// TODO 2: Extract branch from query parameter (default to \"main\")\n\t// TODO 3: Query store for most recent completed pipeline run for repo/branch\n\t// TODO 4: Determine badge color based on status: success=#4CAF50, failure=#F44336, running=#2196F3, pending=#FF9800\n\t// TODO 5: Generate SVG using template or programmatic generation\n\t// TODO 6: Set appropriate Cache-Control headers based on status (running: max-age=60, completed: max-age=3600)\n\t// TODO 7: Set Content-Type to image/svg+xml and CORS headers (Access-Control-Allow-Origin: *)\n\t// TODO 8: Write SVG to response body\n}\n\n// GetDAG handles GET /api/runs/:id/dag\nfunc (s *DashboardServer) GetDAG(w http.ResponseWriter, r *http.Request) {\n\t// TODO 1: Extract pipeline run ID from URL path\n\t// TODO 2: Retrieve pipeline run and its configuration from store\n\t// TODO 3: If config not stored with run, you may need to retrieve from original source\n\t// TODO 4: Reconstruct execution graph using BuildExecutionGraph on the config\n\t// TODO 5: Transform graph into frontend-friendly format with nodes and edges\n\t// TODO 6: Enrich nodes with runtime data (status, timestamps) from job runs\n\t// TODO 7: For large pipelines, consider implementing pagination or level-of-detail\n\t// TODO 8: Return JSON structure optimized for visualization library consumption\n}\n\n// ListArtifacts handles GET /api/runs/:id/artifacts\nfunc (s *DashboardServer) ListArtifacts(w http.ResponseWriter, r *http.Request) {\n\t// TODO 1: Extract pipeline run ID from URL path\n\t// TODO 2: Query store for artifact metadata associated with this run\n\t// TODO 3: Format response as array of objects with name, size, storage_key, content_type\n\t// TODO 4: Return JSON response\n\t// TODO 5: Consider implementing authentication/authorization for production use\n}\n```\n\n**E. Language-Specific Hints**\n\n- Use Go's `embed` package (Go 1.16+) to bundle static frontend files directly into the binary: `//go:embed web/public/*` then `http.FS` to serve.\n- For SSE, ensure you call `http.Flusher.Flush()` after writing each event to immediately send data to the client.\n- Use `context.Context` to detect client disconnection in streaming handlers via `r.Context().Done()`.\n- For JSON serialization of time fields, use `json:\",omitempty\"` for nullable timestamps (`*time.Time`) and consider custom marshaler for formatting.\n- Implement middleware for CORS headers: `w.Header().Set(\"Access-Control-Allow-Origin\", \"*\")` for development; restrict in production.\n- Use `sync.RWMutex` in the `LogBroadcaster` for concurrent client map access—read lock for broadcasting, write lock for registration/deregistration.\n\n**F. Milestone Checkpoint**\n\nAfter implementing the dashboard component, verify functionality:\n\n1. **Start the dashboard server**: `go run cmd/dashboard/main.go` should start on port 8080 (or configured port).\n2. **Access the dashboard**: Navigate to `http://localhost:8080/` in a browser. You should see the dashboard interface (even if empty).\n3. **Trigger a pipeline run** via webhook (from Milestone 3) or manually via API.\n4. **Verify real-time updates**: Open the dashboard while a pipeline is running. You should see:\n   - The run appear in the list with `STATUS_RUNNING`\n   - Clicking on the run should show detailed view with job statuses\n   - Clicking \"View Logs\" should open a streaming log view that updates in real-time\n5. **Check badge generation**: Visit `http://localhost:8080/badge/my-repo.svg?branch=main`. You should see an SVG badge with status.\n6. **Verify artifact listing**: After a job with artifacts completes, the artifacts should be listed and downloadable.\n\n**Expected signs of correct implementation**:\n- No browser errors in developer console\n- Logs stream without requiring page refresh\n- Badge images render correctly in `<img>` tags\n- DAG visualization shows correct node relationships\n\n**Common issues and checks**:\n- *Logs not streaming*: Check browser network tab for SSE connection status (200). Verify `LogBroadcaster.Broadcast` is being called from the job execution engine.\n- *Badge not updating*: Check cache headers; force refresh with Ctrl+F5. Verify the store query returns the correct latest run.\n- *CORS errors for badges*: Check response headers include `Access-Control-Allow-Origin: *`.\n- *High memory usage during log streaming*: Check the buffered channel size in `LogBroadcaster` and implement backpressure strategy.\n\n**G. Debugging Tips**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Dashboard shows \"Loading...\" forever | JavaScript bundle not loading or API endpoints failing | Open browser dev tools → Network tab, check for failed requests (404/500). Check console for errors. | Ensure static files are correctly embedded/served. Verify API endpoints return valid JSON. |\n| Logs stop updating after a few seconds | SSE connection dropping due to timeout or network issue | Check browser network tab → SSE connection status (disconnected). Check server logs for errors. | Implement heartbeat in SSE handler. Increase server timeout settings. Check for proxy/firewall killing idle connections. |\n| Badge shows wrong status (e.g., success when failed) | Cache returning old badge or query fetching wrong run | Check response headers for `Cache-Control`. Verify store query orders by `CreatedAt DESC` and filters by status. | Adjust cache headers. Debug the database query for latest run. |\n| DAG visualization renders nodes overlapping | Frontend graph layout algorithm issue or missing layout config | Check browser console for visualization library errors. Inspect the JSON data structure from `/api/runs/:id/dag`. | Ensure DAG endpoint returns proper node/edge structure. Configure layout options in frontend (e.g., `dagre` layout for Cytoscape). |\n| Artifact download fails with 404 | Storage key not found or path mapping incorrect | Check server logs for error when accessing artifact. Verify the storage key exists in the artifact storage. | Ensure `CollectArtifacts` correctly stores artifacts and records keys in `JobRun.ArtifactKeys`. Verify download handler reads from correct storage backend. |\n| High CPU when multiple users view logs | Unoptimized broadcast to many clients or inefficient filtering | Profile server CPU. Check number of registered clients in `LogBroadcaster`. | Implement client filtering at broadcast time. Consider connection pooling or multiplexing. |\n\n\n## Interactions and Data Flow\n> **Milestone(s):** Milestone 2, Milestone 3, Milestone 4\n\nThis section traces the journey of a single code change through the entire CI/CD system, from the initial Git push to the final pipeline completion notification. Understanding these interactions is critical for debugging and for appreciating how the isolated components defined in previous sections collaborate to deliver a seamless automation experience. We will follow a concrete example, define the precise data formats exchanged between components, and explain the sophisticated concurrency patterns that enable efficient parallel execution.\n\n### Happy Path: From Push to Pipeline Completion\n\nImagine a developer pushes a commit to the `main` branch of their repository. This event triggers a cascade of actions across our system, transforming a simple HTTP payload into a fully executed pipeline with logs, artifacts, and a status report. The journey can be visualized in the sequence diagram:\n\n![Sequence: Webhook to Job Execution](./diagrams/diag-webhook-sequence.svg)\n\nLet's walk through each step in detail, following the life of a hypothetical pipeline run with ID `run_abc123`.\n\n1.  **Git Host Fires the Webhook:** Upon the push event, the Git hosting service (e.g., GitHub) packages event details into a JSON payload. It signs this payload with a secret key known only to itself and our CI system, then dispatches an HTTP POST request to our configured webhook endpoint URL (e.g., `https://ci.example.com/webhooks/github`).\n\n2.  **Webhook Listener Validates and Parses:** Our `DashboardServer`'s HTTP handler receives the request. The `WebhookVerifier` is invoked to `VerifyGitHubSignature`, ensuring the payload is authentic and untampered. The `ExtractEventDetails` function parses the JSON to extract the **repository URL**, **commit SHA** (`abc123`), **branch name** (`main`), and **event type** (`EVENT_PUSH`).\n\n3.  **Pipeline Configuration Retrieval:** The system clones the repository at the given commit SHA into a temporary workspace. It looks for the pipeline configuration file (e.g., `.ci/pipeline.yaml`) in the root of the repository. The raw YAML content is read from the cloned code.\n\n4.  **Configuration Parsing and Enrichment:** The YAML string is passed to `ParseConfig`. This function validates the syntax and structure, returning a `PipelineConfig` object. `ResolveEnvVars` is called to substitute any `$VARIABLE` references within the config, using a combined map of system environment variables and pipeline-defined defaults. If any jobs use the `matrix` keyword, `ExpandMatrix` is invoked, transforming a single `JobConfig` into multiple `JobConfig` instances (e.g., `test-go-1.19`, `test-go-1.20`). Finally, `BuildExecutionGraph` analyzes the `Needs` clauses in each job to produce a dependency graph (a DAG).\n\n5.  **Pipeline Run Creation:** The orchestrator (logic within the webhook handler) creates a new `PipelineRun` instance. It populates the struct with the `Trigger` (`EVENT_PUSH`), `CommitSHA`, `Branch`, `Status` (`STATUS_PENDING`), and a pointer to the fully resolved `PipelineConfig`. This run is persisted via `CreatePipelineRun`.\n\n6.  **Job Runs Generation and Enqueueing:** For each job defined in the expanded pipeline configuration, a corresponding `JobRun` is created. Its `Status` is set to `STATUS_PENDING`, and its `Needs` dependencies are recorded. Jobs with no dependencies (the entry points of the DAG) are immediately eligible for execution. The orchestrator calls `EnqueueJobRun` for each eligible job. This method does two things: it persists the `JobRun` to the `Store` (e.g., SQLite), and then pushes a reference (an `Item` containing the `JobRunID` and `Priority`) onto the in-memory `PriorityQueue` within the `HybridQueue`.\n\n7.  **Worker Polls for Work:** A worker process, which is part of a configured pool, is idle and continuously calling `DequeueJobRun`. This method performs an atomic operation: it locks the queue, pops the highest-priority `Item` from the heap, loads the full `JobRun` data from the `Store`, updates the job's `Status` to `STATUS_RUNNING` and `AssignedWorker` to the worker's ID, and finally returns the `JobRun` struct to the worker.\n\n8.  **Job Execution in Container:** The worker receives the `JobRun` and calls `ExecuteJob`. This function interacts with the Docker daemon: it pulls the required image (if not cached), creates a container with a fresh workspace volume, and injects the resolved environment variables (including any secrets). It then iterates through the job's `Steps`, executing each shell command sequentially via `StreamLogs`. The output (stdout/stderr) is captured in real-time. Each line of output is packaged into a `LogEvent` and sent to the `LogBroadcaster`.\n\n9.  **Real-Time Log Streaming:** The `LogBroadcaster` receives the `LogEvent` for `run_abc123`. It holds a registry of all connected dashboard clients (browsers) that have subscribed to logs for this run via the `SSEHandler`. The broadcaster iterates through these clients and sends the log line as a Server-Sent Event (SSE) formatted string (`data: {\"job\": \"...\", \"line\": \"...\"}\\n\\n`). The user's dashboard page receives this event and appends the line to the relevant job's log viewer.\n\n10. **Step Completion and Artifact Collection:** After a step finishes, its `ExitCode` is recorded. If non-zero, the job is marked as `STATUS_FAILED`, and execution stops. If all steps succeed, `CollectArtifacts` is called. This function uses `docker cp` to copy files matching the job's artifact patterns (e.g., `./bin/*`) from the container's workspace to a persistent storage location (e.g., a local directory or cloud bucket). The resulting file paths are stored as `ArtifactKeys` in the `JobRun`.\n\n11. **Job Completion and Dependency Unblocking:** The worker calls `MarkJobRunComplete` to update the job's `Status` to `STATUS_SUCCEEDED` and its `FinishedAt` timestamp. Crucially, this function also queries the `Store` to find all `JobRun` records whose `Needs` dependencies are now satisfied (i.e., all the jobs they depend on have succeeded). For each of these now-eligible jobs, it calls `EnqueueJobRun` to push them onto the queue. This is the mechanism that drives the DAG forward.\n\n12. **Pipeline Run Completion:** Steps 7-11 repeat for all jobs in the DAG. The orchestrator (or a separate monitoring process) periodically checks if all `JobRun` records for a `PipelineRun` have reached a terminal state (`STATUS_SUCCEEDED`, `STATUS_FAILED`, or `STATUS_CANCELLED`). Once this is true, it updates the `PipelineRun`'s overall `Status` to reflect the aggregate outcome (e.g., if any job failed, the pipeline run fails).\n\n13. **Dashboard Reflection:** Throughout this process, the user has the dashboard open on the build detail page for `run_abc123`. They see:\n    *   The pipeline run status change from \"Pending\" to \"Running\" to \"Succeeded\".\n    *   The DAG visualization update in real-time as jobs move from pending to running to succeeded.\n    *   A live, streaming log for each job as it executes.\n    *   Upon completion, an \"Artifacts\" section appears with download links.\n\nThis end-to-end flow demonstrates the seamless integration of parsing, isolation, queueing, and visualization that defines a robust CI/CD system.\n\n### Message and Event Formats\n\nThe components communicate using well-defined data structures. The primary \"messages\" are the core entities (`PipelineRun`, `JobRun`) stored in the database, and the `LogEvent` streamed to the dashboard. Additionally, webhooks and internal queue items have specific formats.\n\n#### Webhook Payload (GitHub Push Event Example)\nThe CI system must handle a subset of fields from provider-specific payloads. The `ExtractEventDetails` function abstracts these differences.\n\n| Field | Type | Description | Source (GitHub Example) |\n| :--- | :--- | :--- | :--- |\n| `repository.full_name` | `string` | Repository identifier in \"owner/name\" format. Used to match pipeline configs and clone. | `\"octocat/Hello-World\"` |\n| `ref` | `string` | Full Git ref that was pushed. Parsed to extract branch/tag name. | `\"refs/heads/main\"` |\n| `head_commit.id` | `string` | The SHA of the most recent commit in the push. The commit that will be built. | `\"abc123...\"` |\n| `head_commit.message` | `string` | Commit message. Could be used for conditional logic or display. | `\"Fix login bug\"` |\n| `action` / `event` | `string` | Type of event: `push`, `pull_request.opened`, `tag`. Mapped to `EVENT_PUSH`, etc. | `\"push\"` |\n| `pull_request` (optional) | `object` | For PR events, contains base/head refs and SHA. Used for building the merge commit. | `{\"base\": {\"ref\": \"main\", \"sha\": \"...\"}, ...}` |\n| `signature` | `string` | The value of the `X-Hub-Signature-256` HTTP header. Used for verification. | `\"sha256=...\"` |\n\n#### Internal Queue Item (`Item`)\nThe `PriorityQueue` manages `Item` structs, which are lightweight references to persisted `JobRun` records.\n\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `JobRunID` | `string` | Primary key of the `JobRun` in the `Store`. The worker fetches the full details using this ID. |\n| `PipelineRunID` | `string` | Foreign key to the parent `PipelineRun`. Useful for priority grouping or cleanup. |\n| `Priority` | `int` | Numerical priority. **Higher** numbers are dequeued first. Critical deployments might have priority 100, while regular tests have 10. |\n| `CreatedAt` | `time.Time` | Timestamp of when the item was enqueued. Used as a tie-breaker for equal priority (FIFO). |\n| `index` | `int` | Internal field used by the heap implementation (`container/heap`) to track position. |\n\n#### Log Event (`LogEvent`)\nThis is the data structure broadcast to all connected dashboard clients whenever a job produces a line of output.\n\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `PipelineRunID` | `string` | The pipeline run this log line belongs to. Allows clients to filter subscriptions. |\n| `JobRunID` | `string` | The specific job producing the output. |\n| `StepName` | `string` | The name of the step (e.g., `\"Install dependencies\"`) that emitted the line. |\n| `Timestamp` | `time.Time` | The exact time the line was captured on the worker. Used for ordering and display. |\n| `Line` | `string` | The raw text of the log line (without newline). |\n| `Stream` | `string` | Either `\"stdout\"` or `\"stderr\"`. Allows the UI to color-code output. |\n\n> **Design Insight:** The `LogEvent` is kept minimal. It does not contain the full `JobRun` or `StepRun` data. The dashboard client is responsible for fetching static metadata (job names, pipeline structure) via separate REST API calls and then correlating streaming log lines using the IDs.\n\n### Concurrency and Parallelism Patterns\n\nThe system employs several layers of concurrency to maximize resource utilization and provide fast feedback. Coordination is crucial to prevent race conditions and ensure deterministic outcomes.\n\n#### 1. Parallel Job Execution via Worker Pool\nThis is the most fundamental concurrency pattern. The `HybridQueue` and worker pool enable multiple jobs to run simultaneously on a single host.\n\n*   **Pattern:** Producer-Consumer with Priority Scheduling.\n*   **Producers:** The webhook handler (`EnqueueJobRun`) and the job completion logic (`MarkJobRunComplete`) that unblocks dependent jobs.\n*   **Consumers:** The worker goroutines, which call `DequeueJobRun` in a loop.\n*   **Coordination:** The `HybridQueue` uses a mutex (`mu sync.Mutex`) to ensure that the `PriorityQueue` (a heap) and the `Store` updates are modified atomically. The `DequeueJobRun` method is the critical section: it must atomically \"claim\" a job so that no two workers receive the same `JobRun`. This is implemented by marking the job as `STATUS_RUNNING` within the same transaction that removes the `Item` from the heap.\n*   **Limit:** The `workerPool` configuration (e.g., `maxConcurrency: 4`) limits the number of concurrent jobs. This is enforced by the pool manager, which only spins up a fixed number of worker goroutines.\n\n#### 2. DAG-Based Dependency Scheduling\nJobs within a pipeline are not all independent; they form a Directed Acyclic Graph (DAG). The system must enforce these dependencies while maximizing parallelism where possible.\n\n*   **Pattern:** Reactive Fan-out. Jobs are enqueued only when **all** of their upstream dependencies (listed in `Needs`) have completed successfully.\n*   **Algorithm:** When a job finishes (`MarkJobRunComplete`), the system performs a query: `SELECT * FROM job_runs WHERE pipeline_run_id = ? AND status = ? AND needs_met = FALSE`. For each candidate job, it checks if all jobs in its `Needs` list have `Status == STATUS_SUCCEEDED`. If true, it sets `needs_met = TRUE` and calls `EnqueueJobRun`. This check can be optimized with a materialized view or additional indexing.\n*   **Consequence:** This pattern allows multiple independent chains of jobs to run in parallel. For example, a `lint` job and a `test-unit` job with no `Needs` can start immediately and concurrently.\n\n#### 3. Matrix Build Parallelism\nA single job definition can be expanded into multiple parallel job instances, each with a different combination of parameters (e.g., Go version, operating system).\n\n*   **Pattern:** Parametric Fan-out.\n*   **Expansion:** During parsing (`ExpandMatrix`), the cartesian product of all matrix axes is computed. For a matrix defining `go: [1.19, 1.20]` and `os: [linux, macos]`, four distinct `JobConfig` objects are created: `{go:1.19, os:linux}`, `{go:1.19, os:macos}`, etc. Each gets a unique `JobName` suffix (e.g., `test-go-1.19-linux`).\n*   **Execution:** These expanded jobs are treated as independent nodes in the DAG. If the original job had `Needs: [build]`, all four expanded jobs inherit that dependency. Once `build` succeeds, all four matrix jobs can be enqueued **and run in parallel**, subject to worker pool limits.\n*   **Synchronization (Fan-in):** If a downstream job `deploy` `Needs: [test]`, it must wait for **all** expanded matrix jobs named `test-*` to succeed. This is a fan-in synchronization point. The dependency resolution logic in `MarkJobRunComplete` handles this naturally because `deploy`'s `Needs` list would contain the original job name `test`, and the system must interpret this as \"wait for all jobs where the base name is `test`\".\n\n> **ADR: Fan-in Synchronization Semantics for Matrix Jobs**\n> *   **Context:** When a job `C` depends on a job `B` that uses a matrix, we must define what \"job `B` is complete\" means. Does `C` start after the first `B` variant succeeds, after all succeed, or after any succeed?\n> *   **Options Considered:**\n>     1.  **All must succeed:** `C` waits for every expanded variant of `B` (e.g., `B-go1.19`, `B-go1.20`) to reach `STATUS_SUCCEEDED`.\n>     2.  **Any succeeds:** `C` starts as soon as any single variant of `B` succeeds. This is risky and rarely desired.\n>     3.  **First succeeds:** `C` starts after the first variant succeeds and cancels the others. This requires complex coordination.\n> *   **Decision:** **Option 1 (All must succeed).** This is the standard semantics in GitHub Actions and Jenkins. It ensures the downstream job (like a deployment) only proceeds if the test matrix passes **entirely**.\n> *   **Rationale:** Provides the safest and most intuitive behavior for CI/CD pipelines. A deployment should not proceed if tests for any configuration are failing.\n> *   **Consequences:** Requires the dependency resolution logic to treat a dependency on a matrix job name as a dependency on all its expanded children. The system must track the mapping from the original logical job name to its physical expanded `JobRun` IDs.\n\n#### 4. Real-Time Log Streaming Concurrency\nThe `LogBroadcaster` must efficiently deliver log events to many potentially idle dashboard clients.\n\n*   **Pattern:** Publish-Subscribe with Client Registration.\n*   **Registration:** When a browser connects to `/api/runs/run_abc123/logs`, the `SSEHandler` calls `RegisterClient`, creating a `ClientChannel` with a buffered `Send chan []byte`. The channel is stored in a nested map: `clients[pipelineRunID][clientID]`.\n*   **Broadcasting:** When `Broadcast(event)` is called, it acquires a read lock (`mu.RLock`), iterates over the clients subscribed to that `pipelineRunID`, and attempts to send the formatted log line into each client's `Send` channel.\n*   **Non-Blocking Send:** The send operation must **never block** the broadcaster if a client is slow (e.g., a user on a slow network). This is achieved by using a buffered channel for `Send` and a `select` with a `default` case that drops the log line if the buffer is full. This implements **backpressure** protection.\n*   **Cleanup:** If the send fails (channel full), or if the client's HTTP connection closes, the `UnregisterClient` function is called to remove the channel from the map and close it, preventing memory leaks.\n\n### Implementation Guidance\n\nThis section provides starter code for the core coordination mechanisms: the hybrid queue and the log broadcaster. The queue ensures reliable, prioritized job dispatch, while the broadcaster enables the real-time log streaming that brings the dashboard to life.\n\n#### A. Technology Recommendations Table\n| Component | Simple Option | Advanced Option |\n| :--- | :--- | :--- |\n| **Queue Persistence** | **SQLite (`database/sql` + `mattn/go-sqlite3`)**: Embedded, zero-config, ACID transactions. | PostgreSQL (`lib/pq`): Better for high concurrency and scaling to multiple orchestrate instances. |\n| **Priority Queue** | **`container/heap`**: Standard library implementation, perfect for in-memory priority queue. | Custom heap based on slice with manual `heap.Interface`. |\n| **Real-time Transport** | **Server-Sent Events (SSE)**: Simpler, works over standard HTTP, automatic reconnection. | WebSockets (`gorilla/websocket`): Full-duplex, more overhead but enables bidirectional communication. |\n| **Concurrency Control** | **`sync.Mutex` & `sync.RWMutex`**: Straightforward for coordinating access to shared in-memory structures. | Channels: Can be used for a more Go-idiomatic worker pool design. |\n\n#### B. Recommended File/Module Structure\n```\nproject-root/\n  cmd/\n    server/                 # Main application entry point\n      main.go\n    worker/                 # Optional separate worker binary\n      main.go\n  internal/\n    queue/                  # Hybrid queue implementation\n      queue.go              # HybridQueue, PriorityQueue, Item\n      store.go              # Store interface and SQLite implementation\n    orchestrator/           # Logic for DAG scheduling & run creation\n      orchestrator.go       # Calls parser, creates runs, triggers queue\n    dashboard/              # Web dashboard server\n      server.go             # DashboardServer, HTTP handlers\n      broadcaster.go        # LogBroadcaster, ClientChannel\n    executor/               # Job execution engine (Milestone 2)\n      docker.go             # Docker client wrapper\n    webhook/                # Webhook handling (Milestone 3)\n      verifier.go           # WebhookVerifier, signature validation\n    parser/                 # Pipeline configuration parser (Milestone 1)\n      parser.go\n```\n\n#### C. Infrastructure Starter Code: LogBroadcaster\nThis is a complete, ready-to-use component for managing real-time log subscriptions.\n\n```go\n// internal/dashboard/broadcaster.go\npackage dashboard\n\nimport (\n    \"sync\"\n    \"time\"\n)\n\ntype LogEvent struct {\n    PipelineRunID string\n    JobRunID      string\n    StepName      string\n    Timestamp     time.Time\n    Line          string\n    Stream        string // \"stdout\" or \"stderr\"\n}\n\ntype ClientChannel struct {\n    ID            string\n    PipelineRunID string\n    JobFilter     string // If empty, receive all jobs for the pipeline\n    StepFilter    string // If empty, receive all steps\n    Send          chan []byte // Buffered channel for SSE data\n}\n\ntype LogBroadcaster struct {\n    mu            sync.RWMutex\n    clients       map[string]map[string]*ClientChannel // pipelineRunID -> clientID -> ClientChannel\n    clientCounter int\n}\n\nfunc NewLogBroadcaster() *LogBroadcaster {\n    return &LogBroadcaster{\n        clients: make(map[string]map[string]*ClientChannel),\n    }\n}\n\nfunc (b *LogBroadcaster) RegisterClient(pipelineRunID, jobFilter, stepFilter string) *ClientChannel {\n    b.mu.Lock()\n    defer b.mu.Unlock()\n\n    if _, exists := b.clients[pipelineRunID]; !exists {\n        b.clients[pipelineRunID] = make(map[string]*ClientChannel)\n    }\n\n    clientID := fmt.Sprintf(\"client-%d\", b.clientCounter)\n    b.clientCounter++\n\n    ch := &ClientChannel{\n        ID:            clientID,\n        PipelineRunID: pipelineRunID,\n        JobFilter:     jobFilter,\n        StepFilter:    stepFilter,\n        Send:          make(chan []byte, 100), // Buffer 100 log lines\n    }\n    b.clients[pipelineRunID][clientID] = ch\n    return ch\n}\n\nfunc (b *LogBroadcaster) UnregisterClient(pipelineRunID, clientID string) {\n    b.mu.Lock()\n    defer b.mu.Unlock()\n\n    if pipelineClients, ok := b.clients[pipelineRunID]; ok {\n        if ch, ok := pipelineClients[clientID]; ok {\n            close(ch.Send)\n            delete(pipelineClients, clientID)\n        }\n        if len(pipelineClients) == 0 {\n            delete(b.clients, pipelineRunID)\n        }\n    }\n}\n\nfunc (b *LogBroadcaster) Broadcast(event LogEvent) {\n    b.mu.RLock()\n    defer b.mu.RUnlock()\n\n    pipelineClients, ok := b.clients[event.PipelineRunID]\n    if !ok {\n        return\n    }\n\n    data := fmt.Sprintf(`{\"job\":\"%s\",\"step\":\"%s\",\"stream\":\"%s\",\"line\":\"%s\"}`,\n        event.JobRunID, event.StepName, event.Stream, escapeJSONString(event.Line))\n    sseMsg := []byte(\"data: \" + data + \"\\n\\n\")\n\n    for _, client := range pipelineClients {\n        // Apply client-specific filters\n        if client.JobFilter != \"\" && client.JobFilter != event.JobRunID {\n            continue\n        }\n        if client.StepFilter != \"\" && client.StepFilter != event.StepName {\n            continue\n        }\n        // Non-blocking send to prevent broadcaster from stalling on slow clients\n        select {\n        case client.Send <- sseMsg:\n            // Sent successfully\n        default:\n            // Channel buffer full; drop the log line for this client\n            // Optionally log or increment a metric\n        }\n    }\n}\n\nfunc escapeJSONString(s string) string {\n    // Simple escape for quotes and newlines. Use encoding/json for robust escaping in production.\n    return strings.ReplaceAll(strings.ReplaceAll(s, `\"`, `\\\"`), \"\\n\", \"\\\\n\")\n}\n```\n\n#### D. Core Logic Skeleton Code: HybridQueue Dequeue\nThe `DequeueJobRun` method is the heart of the worker coordination, requiring careful atomic operations.\n\n```go\n// internal/queue/queue.go\npackage queue\n\nimport (\n    \"container/heap\"\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\ntype HybridQueue struct {\n    store Store\n    heap  *PriorityQueue\n    mu    sync.Mutex\n}\n\n// DequeueJobRun atomically assigns the highest-priority eligible job to a worker.\nfunc (q *HybridQueue) DequeueJobRun(ctx context.Context, workerID string) (*JobRun, error) {\n    q.mu.Lock()\n    defer q.mu.Unlock()\n\n    // TODO 1: Check if the in-memory heap is empty. If so, return a sentinel error (e.g., ErrNoJobs).\n    // TODO 2: Pop the highest-priority Item from the heap using heap.Pop(q.heap).\n    // TODO 3: Use the Item.JobRunID to fetch the full JobRun from the persistent store (q.store.GetJobRun).\n    // TODO 4: Validate the job is still in a PENDING state. If not (e.g., already taken by another worker), discard this item and loop back to step 1 (or return error).\n    // TODO 5: Update the JobRun in the store: set Status = STATUS_RUNNING, AssignedWorker = workerID, StartedAt = time.Now().\n    // TODO 6: Persist the updated JobRun using q.store.UpdateJobRun.\n    // TODO 7: Return the updated JobRun to the caller (the worker).\n    // Hint: Wrap steps 3-6 in a database transaction if your store supports it to ensure atomicity.\n}\n```\n\n#### E. Language-Specific Hints (Go)\n*   **Atomic Updates:** Use `database/sql` transactions (`BEGIN`, `COMMIT`) in the `Store` implementation to ensure the job state transition from `PENDING` to `RUNNING` is atomic.\n*   **Heap Management:** Implement `heap.Interface` (`Len`, `Less`, `Swap`, `Push`, `Pop`) on your `PriorityQueue` type. The `Less` method should define priority ordering: higher `Priority` first, with `CreatedAt` as a tie-breaker for FIFO behavior.\n*   **Graceful Shutdown:** Implement a context cancellation mechanism in `DequeueJobRun` so worker goroutines can be shut down cleanly when the application stops.\n*   **Log Streaming:** In your `SSEHandler`, after registering the client, run a loop that reads from `client.Send` and writes to the `http.ResponseWriter`. Remember to set the correct SSE headers (`Content-Type: text/event-stream`, `Cache-Control: no-cache`).\n\n#### F. Milestone Checkpoint (Milestone 3 & 4 Integration)\nAfter implementing the queue and broadcaster, verify the happy path works end-to-end.\n\n1.  **Start the system:** `go run cmd/server/main.go`\n2.  **Simulate a webhook:** Use `curl` to send a mock GitHub push payload to `http://localhost:8080/webhooks/github`. Include a valid signature header if you have verification enabled.\n    ```bash\n    curl -X POST http://localhost:8080/webhooks/github \\\n      -H \"Content-Type: application/json\" \\\n      -H \"X-GitHub-Event: push\" \\\n      -d '{\"ref\":\"refs/heads/main\",\"repository\":{\"full_name\":\"test/repo\"},\"head_commit\":{\"id\":\"abc123\"}}'\n    ```\n3.  **Observe the dashboard:** Open `http://localhost:8080` in your browser. You should see a new pipeline run appear in the list within a few seconds.\n4.  **Check worker activity:** If you have a separate worker process, start it (`go run cmd/worker/main.go`). Observe logs showing it dequeuing and executing jobs.\n5.  **Watch real-time logs:** Click on the new pipeline run in the dashboard. Navigate to the logs tab. As the job executes (runs `echo \"Hello\"` in a container), you should see log lines appear in the browser in real-time, without needing to refresh.\n6.  **Verify completion:** The pipeline run should eventually reach a terminal status (`Succeeded` or `Failed`). Artifacts, if defined, should be listed on the run detail page.\n\n**Signs of Trouble:**\n*   **Webhook ignored:** Check server logs for signature verification errors or parsing failures.\n*   **Jobs stuck in \"Pending\":** Verify the worker is running and connected to the same queue/store. Check for deadlocks in the `DequeueJobRun` logic.\n*   **Logs not streaming:** Ensure the `LogBroadcaster.Broadcast` is being called from `StreamLogs` in the executor. Use browser developer tools to check the SSE connection status and any JavaScript errors.\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nIn any automated system, things will fail. The true test of a CI/CD system's robustness lies not in preventing failures (which is impossible), but in how it detects, recovers from, and communicates about those failures. This section catalogs the potential failure modes across our CI/CD Pipeline Orchestrator components and defines comprehensive strategies for recovery and user-facing error reporting.\n\n**Mental Model: The Factory Incident Response Team**  \nImagine our CI/CD system as an automated factory. When something goes wrong—a conveyor belt jams, a robotic arm malfunctions, or a parts delivery is delayed—the factory doesn't just shut down. Instead, an incident response team springs into action: they **detect** the problem (alarms, sensors), **contain** it (isolate the affected area), **recover** (repair or work around), and **communicate** (alert supervisors, update status boards). This section defines our digital incident response team: the patterns and mechanisms that keep the factory running even when individual components fail.\n\n### Categorized Failure Modes\n\nFailures can originate from various sources: malformed user input, infrastructure issues, network problems, or bugs in our own code. Categorizing them helps us apply appropriate handling strategies. The following tables detail failure modes across our four main component categories, their detection mechanisms, and immediate mitigation actions.\n\n#### Parsing and Configuration Failures\nThese occur during the interpretation of pipeline configuration files, environment variable resolution, and dependency graph construction.\n\n| Failure Mode | Detection Mechanism | Immediate Mitigation |\n|--------------|---------------------|----------------------|\n| **Malformed YAML syntax** | `ParseConfig()` returns a YAML parsing error with line/column information | Return error immediately without proceeding; log with context (file path, problematic line) |\n| **Missing required fields** (e.g., `JobConfig` without `Steps`) | Validation step after unmarshaling checks each required field | Return validation error with specific field name; suggest correction in error message |\n| **Circular job dependencies** | `BuildExecutionGraph()` detects cycles during topological sort | Return error listing the circular chain (e.g., \"Job A → Job B → Job A\"); reject entire pipeline |\n| **Invalid matrix axis combinations** | `ExpandMatrix()` validates each axis value type and compatibility | Return error specifying invalid axis and value; skip expansion for that job |\n| **Environment variable substitution errors** | `ResolveEnvVars()` detects undefined variables when strict mode enabled | Option 1 (default): Leave unresolved `${VAR}` as literal. Option 2 (strict): Return error listing undefined variables |\n| **Invalid conditional expressions** in `If` fields | Syntax validation of `If` expression during parsing | Return error with expression and parsing failure; mark step/job as `STATUS_SKIPPED` with error reason |\n| **Combinatorial explosion** from large matrix | `ExpandMatrix()` computes total combinations and compares against configurable limit (e.g., 100) | Return error exceeding limit; suggest reducing matrix dimensions or increasing limit |\n| **Invalid glob patterns** for artifacts | Pattern syntax validation during configuration parsing | Return error with invalid pattern; continue without artifact collection for that pattern |\n\n#### Execution Engine Failures\nThese occur during container lifecycle management, command execution, and artifact collection.\n\n| Failure Mode | Detection Mechanism | Immediate Mitigation |\n|--------------|---------------------|----------------------|\n| **Docker daemon unavailable** | Docker client connection fails during `ExecuteJob()` | Mark job as `STATUS_FAILED` with \"Infrastructure unavailable\" error; implement retry at queue level |\n| **Container image pull failure** (network, authentication, missing image) | Docker image pull returns non-zero exit code or timeout | Mark job as `STATUS_FAILED` with specific pull error; cache error to avoid repeated pulls for same job |\n| **Container startup failure** (insufficient resources, port conflicts) | Container create/start returns error from Docker API | Mark job as `STATUS_FAILED` with startup error; ensure cleanup of partially created containers |\n| **Step command non-zero exit code** | Command execution returns exit code ≠ 0 | Stop job execution immediately; mark job as `STATUS_FAILED`; continue to artifact collection if configured |\n| **Step timeout exceeded** | Per-step or per-job timeout timer fires | Kill container process; mark step as failed with timeout error; proceed to job cleanup |\n| **Log stream blocking/disconnection** | `StreamLogs()` detects write failures or broken pipe | Close log stream; continue job execution but log warning; store partial logs up to disconnection point |\n| **Artifact collection failures** (file not found, permission denied) | `CollectArtifacts()` returns error for specific patterns | Log warning for failed patterns; continue collecting other patterns; store partial artifact list |\n| **Container cleanup failure** (zombie containers) | Periodic health check scans for containers older than max job duration | Force-kill containers; remove with Docker API; log cleanup event for audit |\n| **Resource exhaustion** (memory, disk, containers) | Docker daemon returns resource errors; system monitoring | Reject new jobs with \"Resource unavailable\"; implement backpressure to queue |\n\n#### Queue and Webhook System Failures\nThese occur during webhook processing, queue operations, and worker coordination.\n\n| Failure Mode | Detection Mechanism | Immediate Mitigation |\n|--------------|---------------------|----------------------|\n| **Invalid webhook signature/token** | `VerifyGitHubSignature()` or `VerifyGitLabToken()` returns validation error | Respond with HTTP 401 Unauthorized; log attempt with source IP for security monitoring |\n| **Malformed webhook payload** | `ExtractEventDetails()` fails to parse required fields (SHA, branch, event type) | Respond with HTTP 400 Bad Request; log payload snippet for debugging |\n| **Webhook replay attack** (duplicate delivery ID) | Track webhook delivery IDs in short-term cache (5 minutes) | Ignore duplicate; respond with HTTP 200 OK but skip processing |\n| **Queue persistence failure** | `EnqueueJobRun()` returns error when writing to database | Retry with exponential backoff (3 attempts); if all fail, respond with HTTP 503 Service Unavailable |\n| **Worker crash during job execution** | Heartbeat timeout: worker fails to send periodic \"alive\" signal | Re-queue job with original priority; mark original `JobRun` as orphaned; spawn new worker if needed |\n| **Queue starvation** (low-priority jobs never run) | Monitor queue age metrics; alert when low-priority jobs exceed age threshold | Temporarily elevate priority of oldest jobs; implement fair scheduling algorithm |\n| **Lost jobs on system crash** (in-memory heap only) | `recoverPendingJobs()` on startup rebuilds heap from persistent storage | Design: Use hybrid queue with persistence; recovery scans for `STATUS_PENDING` jobs in database |\n| **Concurrent dequeue race condition** | Database optimistic locking or `SELECT FOR UPDATE` in `DequeueJobRun()` | Retry dequeue on version conflict; ensure exactly-once delivery through atomic operations |\n\n#### Dashboard and Network Failures\nThese occur during real-time streaming, data retrieval, and user interface interactions.\n\n| Failure Mode | Detection Mechanism | Immediate Mitigation |\n|--------------|---------------------|----------------------|\n| **SSE/WebSocket connection drop** | `Broadcast()` detects closed client channel | `UnregisterClient()` automatically; log disconnection; client can reconnect |\n| **Log backpressure** (client slower than log production) | Channel send with timeout; buffer fills | Drop oldest log lines when buffer exceeds limit; log warning about backpressure |\n| **Database query timeout** | Context timeout in `ListPipelineRuns()` or `GetRunDetail()` | Return HTTP 504 Gateway Timeout; implement query pagination and indexes |\n| **SVG badge generation failure** | Template rendering error in `GetBadge()` | Return fallback \"unknown\" status badge; log rendering error |\n| **CORS errors** (dashboard hosted separately) | Browser console errors; preflight request failures | Configure appropriate CORS headers in dashboard endpoints |\n| **Large log file rendering freeze** | Browser memory exhaustion when loading multi-MB logs | Implement client-side virtual scrolling; server-side log truncation for initial load |\n| **Real-time log staleness** (logs stop updating) | Client-side timer detects no new events for >30 seconds | Auto-reconnect logic in JavaScript; server-side connection health checks |\n\n### Recovery and Retry Strategy\n\nNot all failures are equal. **Transient failures** (network blips, temporary resource constraints) often resolve themselves and warrant retries. **Persistent failures** (buggy user code, configuration errors) require different handling. Our strategy distinguishes between these and defines clear recovery paths.\n\n#### Retry Policies for Transient Failures\n\nWe implement a layered retry strategy with increasing delays to avoid overwhelming recovering systems:\n\n| Failure Category | Retryable? | Retry Policy | Max Attempts | Backoff Strategy | Notes |\n|------------------|------------|--------------|--------------|------------------|-------|\n| Docker image pull failure | Yes (network timeouts) | Exponential backoff | 3 | 2s, 4s, 8s | Reset after successful pull of same image |\n| Webhook processing (queue full) | Yes | Immediate retry with jitter | 5 | 100ms ± 50ms jitter | After max attempts, respond with HTTP 503 |\n| Worker dequeue (temporary DB issue) | Yes | Linear backoff | 10 | 1s increments | Worker sleeps between attempts; continues other jobs |\n| Artifact upload (network) | Yes | Exponential backoff | 4 | 1s, 2s, 4s, 8s | Store artifacts locally first, async upload |\n| Step execution (timeout due to load) | Conditional | Configurable per step | 1 (default) | N/A | Only retry if `retry_on_timeout: true` in config |\n| SSE connection drop | Yes (client-side) | Exponential backoff | ∞ | 1s, 2s, 4s, 8s, 16s, 30s (cap) | Implemented in browser JavaScript |\n\n**Mental Model: The Gradual Knock**  \nImagine knocking on a door when someone might be temporarily unavailable. You don't pound relentlessly—you knock, wait a moment, knock again a bit louder, wait longer, and eventually give up. Our retry policies follow this intuition: gradually increasing delays prevent overwhelming a recovering service while persistently trying to complete the operation.\n\n#### Cleanup Procedures for Stuck Resources\n\nSome failures leave behind resources that need cleanup to prevent resource leaks and system degradation:\n\n| Resource Type | Detection Method | Cleanup Procedure | Trigger Frequency |\n|---------------|------------------|-------------------|-------------------|\n| **Orphaned containers** (no corresponding active `JobRun`) | Periodic scan: Docker containers with CI labels older than max job duration | Force stop (`docker stop`), then remove (`docker rm`) | Every 5 minutes |\n| **Stuck `STATUS_RUNNING` jobs** (no heartbeat updates) | Worker heartbeat timeout (e.g., 5 minutes no update) | Mark job as `STATUS_FAILED` with \"timeout\"; trigger container cleanup | On heartbeat check (every 30 seconds) |\n| **Old pipeline run artifacts** | Scan artifact storage for runs older than retention period (default 30 days) | Delete files from storage; update database records | Daily cron job |\n| **Expired webhook delivery ID cache** | TTL-based cache eviction | Automatic removal from cache | On cache insert with TTL |\n| **Dead worker entries** in worker registry | Worker heartbeat timeout | Remove from registry; reassign any jobs assigned to that worker | On heartbeat check |\n| **Partial queue items** (persisted but not in heap) | `recoverPendingJobs()` on startup | Re-add to in-memory heap; log recovery count | System startup |\n\n> **Architecture Decision: Retry at Queue Level vs. Execution Level**\n> - **Context**: When a job fails due to transient infrastructure issues, we need to decide where retry logic resides.\n> - **Options Considered**:\n>   1. **Execution-level retry**: Each step/job retries itself immediately within `ExecuteJob()`.\n>   2. **Queue-level retry**: Failed jobs return to the queue with incremented attempt count.\n>   3. **Hybrid approach**: Transient failures (network) retry at execution level; persistent failures (test failures) don't retry.\n> - **Decision**: Queue-level retry for infrastructure failures only.\n> - **Rationale**: Queue retry maintains proper priority ordering and respects worker concurrency limits. Immediate execution retry could bypass queue fairness and cause cascading failures. We distinguish failure types: exit code ≠ 0 (user error) never retries; Docker daemon errors (infrastructure) retry via queue.\n> - **Consequences**: Simpler execution engine logic, but requires accurate failure classification. May delay retry due to queue position.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Execution-level retry | Faster retry, no queue overhead | Bypasses priority, may overload recovering system | No |\n| Queue-level retry | Maintains fairness, respects limits | Slower retry, more complex failure classification | **Yes** |\n| Hybrid approach | Optimized for different failure types | Most complex implementation, hard to debug | No |\n\n### User-Facing Error Reporting\n\nWhen failures occur, developers need clear, actionable information to understand what went wrong and how to fix it. Our error reporting strategy spans multiple channels with consistent information.\n\n#### Dashboard Error Display\n\nThe web dashboard surfaces errors through structured UI components:\n\n| Error Context | Dashboard Presentation | Example Content |\n|---------------|------------------------|-----------------|\n| **Pipeline configuration error** | Red banner at top of pipeline run view with parse error details | \"YAML parse error at line 12, column 5: unexpected ':'\" with link to config file |\n| **Job failure** | Job node turns red in DAG; click reveals failure reason and step details | \"Job 'test' failed at step 'run-tests': Exit code 1\" with collapsed log snippet |\n| **Step failure** | Expanded step view shows exit code, duration, and error output highlighted | \"Command 'npm test' failed with exit code 1. Output: [last 10 lines of stderr]\" |\n| **Infrastructure error** | Orange warning icon with generic message to avoid exposing internals | \"Infrastructure issue: Job could not start. Our team has been notified.\" |\n| **Real-time streaming failure** | Connection status indicator in log viewer; auto-reconnect countdown | \"Disconnected from log stream. Reconnecting in 5...4...3...\" |\n\n#### Structured Error Logging\n\nAll components log errors with consistent structured fields for debugging:\n\n```log\n[ERROR] 2023-10-05T14:30:22Z component=execution_engine job_run_id=job_abc123 \nerror=\"container start failed\" error_type=infrastructure \ndetail=\"Docker API error: port 8080 already allocated\" \ncontainer_id=container_xyz recovery_action=requeue attempt=2/3\n```\n\nKey logging principles:\n1. **Structured fields**: Machine-parsable key-value pairs for aggregation\n2. **Error categorization**: `error_type` = {`user_error`, `infrastructure`, `system_bug`}\n3. **Correlation IDs**: Include `pipeline_run_id`, `job_run_id` in all related logs\n4. **Sensitive data redaction**: Never log secrets or environment variables in plain text\n5. **Recovery actions logged**: Document what the system did in response\n\n#### Build Status Badges\n\nEven in failure states, badges provide immediate visual feedback:\n\n| Status | Badge Color | Text | Hover Tooltip |\n|--------|-------------|------|---------------|\n| `STATUS_FAILED` (user error) | Bright red | failed | \"Tests failed: click for details\" |\n| `STATUS_FAILED` (infrastructure) | Orange | error | \"System error: retrying\" |\n| `STATUS_CANCELLED` | Grey | cancelled | \"Manually cancelled\" |\n| `STATUS_SKIPPED` | Light grey | skipped | \"Skipped due to condition\" |\n\nBadges include a timestamp to indicate freshness and prevent caching of stale status.\n\n#### Error Communication Guidelines\n\n> **Design Principle: The Three Layers of Error Information**\n> 1. **User Actionable**: What the developer should do (e.g., \"Fix syntax in .ci.yml line 12\")\n> 2. **Debug Context**: Technical details for investigation (e.g., \"Exit code 127: command not found\")\n> 3. **System Internal**: Infrastructure details for administrators (e.g., \"Docker daemon OOM kill\")\n\nWe follow these communication patterns:\n\n| Audience | Information Level | Delivery Channel |\n|----------|------------------|------------------|\n| **Developer** | User-actionable + limited debug context | Dashboard UI, badge status, PR comments (future) |\n| **System Admin** | Debug context + system internal | Structured logs, monitoring alerts |\n| **Support Team** | All three layers | Centralized log aggregation with correlation IDs |\n\n**Common Reporting Anti-Patterns to Avoid**:\n- ⚠️ **Exposing internal infrastructure details** to end-users (security risk)\n- ⚠️ **Generic error messages** like \"Something went wrong\" (not actionable)\n- ⚠️ **Logging secrets** in error messages (security breach)\n- ⚠️ **Silent failures** where jobs disappear without trace (debugging nightmare)\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error categorization | String constants in code | Structured error types with Go 1.13+ `errors.As/Is` |\n| Retry logic | Simple for-loop with `time.Sleep` | `github.com/cenkalti/backoff/v4` exponential backoff |\n| Structured logging | `log.Printf` with fmt strings | `github.com/sirupsen/logrus` or `go.uber.org/zap` |\n| Health checks | Basic endpoint returning 200 OK | Comprehensive checks (DB, Docker, queue depth) |\n| Metrics collection | Count errors in log analysis | Prometheus counters and histograms |\n\n#### B. Recommended File/Module Structure\n\n```\nproject-root/\n  internal/\n    errors/                    # Error types and categorization\n      types.go                 # Error constants and types\n      categorization.go        # Error classification logic\n    recovery/                  # Retry and cleanup logic\n      retry.go                 # Retry policies with backoff\n      cleanup.go               # Stuck resource cleanup\n    logging/                   # Structured logging setup\n      structured.go            # Logger configuration\n      redaction.go             # Secret redaction in logs\n  cmd/\n    cleanup-agent/             # Optional: separate cleanup daemon\n      main.go\n```\n\n#### C. Infrastructure Starter Code\n\n**Complete structured logging wrapper** (ready to use):\n\n```go\n// internal/logging/structured.go\npackage logging\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"runtime\"\n    \"strings\"\n    \"time\"\n)\n\n// ErrorType categorizes errors for appropriate handling\ntype ErrorType string\n\nconst (\n    ErrorTypeUser      ErrorType = \"user_error\"      // Malformed config, test failures\n    ErrorTypeInfra     ErrorType = \"infrastructure\"  // Docker, network, disk issues\n    ErrorTypeSystem    ErrorType = \"system_bug\"      // Bugs in our code\n)\n\n// LogEntry represents a structured log entry\ntype LogEntry struct {\n    Timestamp   time.Time              `json:\"timestamp\"`\n    Level       string                 `json:\"level\"`\n    Component   string                 `json:\"component\"`\n    Message     string                 `json:\"message\"`\n    Error       string                 `json:\"error,omitempty\"`\n    ErrorType   ErrorType              `json:\"error_type,omitempty\"`\n    Fields      map[string]interface{} `json:\"fields\"`\n    Caller      string                 `json:\"caller,omitempty\"`\n}\n\n// Logger provides structured logging methods\ntype Logger struct {\n    component string\n}\n\n// NewLogger creates a new logger for a component\nfunc NewLogger(component string) *Logger {\n    return &Logger{component: component}\n}\n\n// Error logs an error with structured fields\nfunc (l *Logger) Error(ctx context.Context, err error, errorType ErrorType, fields map[string]interface{}) {\n    entry := LogEntry{\n        Timestamp: time.Now().UTC(),\n        Level:     \"ERROR\",\n        Component: l.component,\n        Message:   err.Error(),\n        Error:     err.Error(),\n        ErrorType: errorType,\n        Fields:    make(map[string]interface{}),\n    }\n    \n    // Add caller information (file:line)\n    if _, file, line, ok := runtime.Caller(1); ok {\n        // Shorten file path\n        parts := strings.Split(file, \"/\")\n        if len(parts) > 3 {\n            file = strings.Join(parts[len(parts)-3:], \"/\")\n        }\n        entry.Caller = fmt.Sprintf(\"%s:%d\", file, line)\n    }\n    \n    // Copy fields, redacting secrets\n    for k, v := range fields {\n        if strings.Contains(strings.ToLower(k), \"secret\") || \n           strings.Contains(strings.ToLower(k), \"token\") ||\n           strings.Contains(strings.ToLower(k), \"password\") {\n            entry.Fields[k] = \"[REDACTED]\"\n        } else {\n            entry.Fields[k] = v\n        }\n    }\n    \n    // Add correlation IDs from context if available\n    if runID, ok := ctx.Value(\"pipeline_run_id\").(string); ok {\n        entry.Fields[\"pipeline_run_id\"] = runID\n    }\n    if jobID, ok := ctx.Value(\"job_run_id\").(string); ok {\n        entry.Fields[\"job_run_id\"] = jobID\n    }\n    \n    // In simple implementation, output as JSON\n    // For production, integrate with logrus/zap\n    fmt.Printf(\"%s\\n\", toJSON(entry))\n}\n\n// toJSON is a simple JSON marshaller (use encoding/json in production)\nfunc toJSON(entry LogEntry) string {\n    // Simplified for example\n    return fmt.Sprintf(\n        `{\"timestamp\":\"%s\",\"level\":\"%s\",\"component\":\"%s\",\"message\":\"%s\"}`,\n        entry.Timestamp.Format(time.RFC3339),\n        entry.Level,\n        entry.Component,\n        strings.ReplaceAll(entry.Message, `\"`, `\\\"`),\n    )\n}\n\n// Errorf logs a formatted error message\nfunc (l *Logger) Errorf(ctx context.Context, errorType ErrorType, format string, args ...interface{}) {\n    err := fmt.Errorf(format, args...)\n    l.Error(ctx, err, errorType, nil)\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**Error categorization helper** (learner implements):\n\n```go\n// internal/errors/categorization.go\npackage errors\n\nimport (\n    \"strings\"\n)\n\n// CategorizeError determines the ErrorType from an error message and context\nfunc CategorizeError(err error, component string) ErrorType {\n    // TODO 1: Check if error is nil - return empty string\n    \n    // TODO 2: Check error message for infrastructure patterns\n    // Patterns indicating infrastructure: \"connection refused\", \"timeout\", \n    // \"no space left\", \"port already in use\", \"image not found\"\n    \n    // TODO 3: Check component-specific categorization\n    // Parser errors are user_error, execution engine Docker errors are infrastructure\n    \n    // TODO 4: Default to system_bug if not clearly infrastructure or user_error\n    // (assuming our code is correct, but handle unknown patterns)\n    \n    // TODO 5: Return the determined ErrorType\n    return ErrorTypeSystem // Placeholder\n}\n```\n\n**Retry with exponential backoff** (learner implements):\n\n```go\n// internal/recovery/retry.go\npackage recovery\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// RetryWithBackoff executes f with exponential backoff until success or max attempts\nfunc RetryWithBackoff(\n    ctx context.Context,\n    maxAttempts int,\n    initialDelay time.Duration,\n    f func() error,\n    shouldRetry func(error) bool,\n) error {\n    // TODO 1: Initialize attempt counter\n    \n    // TODO 2: Calculate delay with exponential backoff: delay = initialDelay * 2^(attempt-1)\n    \n    // TODO 3: Loop while attempts < maxAttempts\n    \n    // TODO 4: Execute the function f\n    \n    // TODO 5: If no error, return success immediately\n    \n    // TODO 6: Check if error should be retried using shouldRetry function\n    // If not retryable, return error immediately\n    \n    // TODO 7: Wait for delay with context cancellation support\n    // Use time.Sleep or better, select with context.Done()\n    \n    // TODO 8: Increment attempt counter and recalculate delay\n    \n    // TODO 9: After max attempts, return the last error\n    return nil // Placeholder\n}\n```\n\n**Stuck job detection and cleanup** (learner implements):\n\n```go\n// internal/recovery/cleanup.go\npackage recovery\n\nimport (\n    \"context\"\n    \"time\"\n    \n    \"github.com/yourproject/internal/store\"\n)\n\n// CleanupStuckJobs finds and marks jobs stuck in RUNNING state\nfunc CleanupStuckJobs(ctx context.Context, s store.Store, timeout time.Duration) (int, error) {\n    cleanedCount := 0\n    \n    // TODO 1: Calculate cutoff time: current time - timeout\n    \n    // TODO 2: Query database for jobs with Status = STATUS_RUNNING \n    // AND StartedAt < cutoff\n    \n    // TODO 3: For each stuck job:\n    //   - Mark as STATUS_FAILED with reason \"timeout\"\n    //   - Log cleanup action with job ID\n    //   - Attempt to stop Docker container (if ContainerID exists)\n    //   - Increment cleanedCount\n    \n    // TODO 4: Return count of cleaned jobs and any error\n    return cleanedCount, nil\n}\n```\n\n#### E. Language-Specific Hints\n\n- **Error wrapping**: Use `fmt.Errorf(\"... %w\", err)` in Go 1.13+ to wrap errors while preserving original error for `errors.Is/As`\n- **Context cancellation**: Always check `ctx.Done()` in retry loops to allow graceful shutdown\n- **Structured logging keys**: Use consistent naming like `snake_case` for log fields\n- **Docker client errors**: Check for `errdefs.IsNotFound(err)` to distinguish missing images from other Docker errors\n- **Timeout handling**: Use `context.WithTimeout` for any external calls (DB, Docker API, network)\n\n#### F. Milestone Checkpoint\n\nAfter implementing error handling, verify with:\n\n```bash\n# Test error categorization\ngo test ./internal/errors/... -v\n\n# Test retry logic with mock failing function\ngo test ./internal/recovery/... -v\n\n# Integration test: Trigger a pipeline with malformed YAML\ncurl -X POST http://localhost:8080/webhooks/github \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ref\": \"refs/heads/main\", \"repository\": {\"default_branch\": \"main\"}}'\n# Expected: HTTP 400 with clear error about missing fields\n\n# Check structured logs appear\ntail -f ci-server.log | grep ERROR\n# Should see structured JSON logs with component, error_type fields\n```\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Jobs stuck in PENDING forever** | Queue worker crashed or deadlock | Check worker logs for panics; run `SELECT * FROM job_runs WHERE status = 'pending'` | Restart workers; implement worker health checks |\n| **Logs stop streaming mid-job** | SSE connection dropped due to network or backpressure | Check browser console for errors; server logs for \"broken pipe\" | Implement client reconnection; increase log buffer size |\n| **\"Infrastructure error\" with no details** | Docker daemon down or resource exhausted | Run `docker ps` and `docker info`; check system metrics | Restart Docker; add resource limits; implement health checks |\n| **Webhooks ignored silently** | Signature validation failing | Check webhook logs for \"invalid signature\"; verify secret matches | Update webhook secret in both Git provider and config |\n| **Artifacts missing after successful job** | Globbing pattern doesn't match or permission issues | Check job logs for artifact collection errors; inspect container filesystem | Fix glob patterns; ensure artifacts are in workspace |\n\n\n## Testing Strategy\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nBuilding a CI/CD system that reliably orchestrates complex workflows requires a robust testing strategy for the system *itself*. The core challenge lies in testing a distributed, stateful system that interacts with external services (Docker, Git hosts) and manages concurrent execution. This section outlines a pragmatic, multi-layered approach to verifying each component and their integration, ensuring the system behaves correctly under expected and unexpected conditions.\n\nA well-structured testing strategy not only catches bugs early but also serves as living documentation of the system's expected behavior. Given the educational nature of this project, the focus is on **practical verification** that a learner can execute after each milestone to gain confidence in their implementation.\n\n### Testing Pyramid for a CI System\n\nThe classic testing pyramid—emphasizing many unit tests, fewer integration tests, and even fewer end-to-end (E2E) tests—applies well to our CI orchestrator. However, the nature of a CI system, which heavily relies on external infrastructure (containers, queues, HTTP), shifts the weight more towards integration tests.\n\n> **Mental Model: The Construction Site Inspection**\n> Think of testing the CI system like inspecting a construction site at different levels of detail. **Unit tests** are like checking individual tools (a hammer, a saw) to ensure they work correctly in isolation. **Integration tests** are like verifying that teams of workers (plumbers and electricians) can coordinate when the water and power are connected to the building. **End-to-end tests** are the final building inspection, where you flip the light switch and expect the bulb to illuminate, verifying the entire system from foundation to finish.\n\nThe following table outlines the three testing tiers, their scope, tools, and what they validate for our CI system:\n\n| Test Tier | Scope & Responsibility | Primary Tools/Mocks | Key Verification Points | Recommended Coverage |\n|-----------|------------------------|---------------------|-------------------------|----------------------|\n| **Unit Tests** | Isolated logic of a single function, struct, or algorithm. No external dependencies. | Standard library testing, `testify` for assertions, `gomock` for generating mocks. | - YAML parsing into `PipelineConfig` <br> - Environment variable substitution logic <br> - Matrix expansion algorithm <br> - DAG dependency sorting <br> - Priority queue ordering <br> - Log event broadcasting logic | High (70-80%). Focus on pure business logic. |\n| **Integration Tests** | Interaction between two or more components, or with a lightweight version of an external service. | `testcontainers` for Docker, an in-memory SQL database, a real HTTP server for webhook testing. | - `ParseConfig` + `BuildExecutionGraph` produces correct job order <br> - `ExecuteJob` successfully runs a command in a real container <br> - `HybridQueue` persists and recovers jobs <br> - Webhook handler validates signatures and creates `PipelineRun` <br> - Dashboard API returns correct run history | Medium (20-25%). Critical for core workflows. |\n| **End-to-End (E2E) Tests** | The entire system working together, from webhook to dashboard update, using production-like configurations. | Full system binary, real Docker daemon, mock Git server (e.g., `gitea` in testcontainers), headless browser for UI. | - A git push triggers a pipeline via webhook <br> - Jobs execute in parallel as defined <br> - Logs stream to the dashboard in real-time <br> - Artifacts are saved and downloadable <br> - The system recovers from a restart mid-pipeline | Low (5-10%). Complex and slow, but essential for confidence. |\n\n**Testing External Dependencies:** A key decision is how to handle tests for components that interact with Docker, a database, or a Git host.\n\n> **Decision: Use Test Doubles for Unit Tests, Lightweight Real Services for Integration**\n> - **Context**: Unit tests must be fast and deterministic. Spinning up a Docker daemon or a PostgreSQL instance is slow and flaky.\n> - **Options Considered**:\n>     1. **Mock everything**: Use interfaces and generated mocks for all external interactions (Docker client, store, queue).\n>     2. **Use lightweight fakes**: Implement in-memory fakes (e.g., a `FakeDockerClient` that records calls).\n>     3. **Use real services in containers**: Leverage `testcontainers` to spin up real Docker, Redis, or Postgres for integration tests.\n> - **Decision**: Use **Option 1 (mocks) for unit tests** and **Option 3 (testcontainers) for integration tests**. Option 2 is avoided as it requires maintaining fake implementations that must mimic complex external behavior.\n> - **Rationale**: Mocks keep unit tests fast and focused on the component's logic, not the external service's behavior. Using real services in integration tests provides the highest fidelity, catching issues like API incompatibility or connection handling. `testcontainers` provides a clean, programmatic way to manage these dependencies.\n> - **Consequences**: Integration test suites will be slower and require a working Docker environment. This is an acceptable trade-off for the confidence gained.\n\n**Structured Logging for Testability:** The system's use of structured logging with correlation IDs (as defined in the `LogEntry` type) is a powerful testing tool. During integration and E2E tests, logs can be captured and assertions can be made about the sequence and content of log messages, providing a trace of the system's internal state.\n\n### Milestone Verification Checkpoints\n\nAfter completing each milestone, learners should run a series of verification steps to confirm their implementation aligns with the design. These checkpoints mix automated tests, manual commands, and visual inspection.\n\n#### Milestone 1: Pipeline Configuration Parser\n\nThe goal is to verify that YAML configuration is correctly parsed, validated, and transformed into an executable plan.\n\n| Checkpoint | Verification Method | Steps to Execute | Expected Outcome |\n|------------|---------------------|------------------|------------------|\n| **1. Basic Parsing** | Unit Test | Run `go test ./internal/parser/... -v -run TestParseConfig` | All tests pass. A test parsing a simple pipeline YAML returns a `PipelineConfig` struct with correctly populated `Jobs` map and `Environment`. |\n| **2. Environment Variable Resolution** | Unit Test / Manual CLI | 1. Write a test YAML with `command: \"echo $USER ${HOME}\"`. <br> 2. Call `ParseConfig` and then `ResolveEnvVars` with a provided env map. <br> 3. Inspect the resolved command in the resulting `StepConfig`. | The `Command` field in the `StepConfig` has the variable references replaced with values from the provided map (e.g., `\"echo alice /home/alice\"`). System environment variables are also substituted when no override is provided. |\n| **3. Matrix Expansion** | Unit Test | Run a test for `ExpandMatrix` with a job containing `matrix: { go: [\"1.19\", \"1.20\"], os: [\"linux\"] }`. | The function returns a slice of `JobConfig` structs with length 2. Each job's `Env` map should contain `GO_VERSION=1.19` and `OS=linux` (or the equivalent for the second combination). The `Name` field should be updated to reflect the combination (e.g., `\"build (go=1.19, os=linux)\"`). |\n| **4. DAG Construction** | Unit Test & Visual Inspection | 1. Write a test pipeline with 3 jobs: `A`, `B` (needs: `A`), `C` (needs: `A`). <br> 2. Call `BuildExecutionGraph`. <br> 3. Assert the adjacency list: `A` has no dependencies, `B` and `C` depend on `A`. <br> 4. Print the topological order. | The graph is constructed without cycles. A topological sort yields `[A, B, C]` (or `[A, C, B]`), confirming `A` must run before `B` and `C`, which can run in parallel. |\n| **5. Validation & Error Cases** | Unit Test | Run tests for invalid YAML, circular dependencies, and invalid matrix axes. | The parser returns descriptive, actionable error messages (e.g., `\"job 'B' forms a circular dependency with job 'A'\"`). Tests for invalid input pass (i.e., they expect an error). |\n\n**Integration Checkpoint:** Create a small Go program that uses your parser package to read a file `.ci/pipeline.yaml` and print the execution plan (job order and expanded matrix jobs). Run it against a sample configuration. You should see a human-readable list of jobs and their dependencies.\n\n#### Milestone 2: Job Execution Engine\n\nThe goal is to verify that jobs can be run in isolated containers, with output capture and artifact collection.\n\n| Checkpoint | Verification Method | Steps to Execute | Expected Outcome |\n|------------|---------------------|------------------|------------------|\n| **1. Container Lifecycle** | Integration Test | Write a test that calls `ExecuteJob` for a job with a single step: `command: \"echo 'Hello World'\"`. Use a simple image like `alpine:latest`. | The test passes. The `JobRun` returned has `Status: STATUS_SUCCEEDED`. The `LogKey` is populated. The container is created, runs, and is removed (verified by checking `docker ps -a` after test completion). |\n| **2. Step Execution & Exit Codes** | Integration Test | Test two jobs: one with `command: \"exit 0\"` and one with `command: \"exit 1\"`. | The first job returns `STATUS_SUCCEEDED` with `ExitCode: 0`. The second job returns `STATUS_FAILED` with `ExitCode: 1`. The execution stops after the failing step. |\n| **3. Log Streaming** | Integration Test / Manual | In a test, call `StreamLogs` while a job is running (e.g., a job with `command: \"for i in 1 2 3; do echo $i; sleep 0.5; done\"`). Capture the output to a buffer. | The buffer contains the lines `\"1\"`, `\"2\"`, `\"3\"` in real-time, as they are produced. The output is captured via stdout/stderr. |\n| **4. Environment Injection** | Integration Test | Execute a job with an `Env` map `{\"GREETING\": \"Hello\"}` and a step command `\"echo $GREETING\"`. Capture the logs. | The log output contains the line `\"Hello\"`. The environment variable was successfully injected into the container. |\n| **5. Artifact Collection** | Integration Test | Run a job that creates a file (`touch output.txt`) and specify artifact patterns `[\"output.txt\", \"*.txt\"]`. Call `CollectArtifacts` after job completion. | The function returns a list of artifact keys (e.g., file paths in persistent storage). The specified file is copied from the container's workspace and can be retrieved from the storage layer. |\n| **6. Parallel Execution** | Integration Test / Manual | Use the `WorkerPool` to enqueue two independent jobs that each run for 2 seconds. Measure total execution time. | Both jobs start nearly simultaneously. The total time is just over 2 seconds (not 4 seconds), confirming parallel execution. |\n\n**Integration Checkpoint:** Start a standalone worker process, and manually enqueue a `JobRun` (by inserting directly into the queue/store). Observe the worker picks it up, runs the container, and updates the status. Check the database: the `JobRun` record should progress from `STATUS_PENDING` to `STATUS_RUNNING` to `STATUS_SUCCEEDED`.\n\n#### Milestone 3: Webhook & Queue System\n\nThe goal is to verify that the system correctly receives events, queues work, and coordinates workers.\n\n| Checkpoint | Verification Method | Steps to Execute | Expected Outcome |\n|------------|---------------------|------------------|------------------|\n| **1. Webhook Signature Verification** | Unit Test | Use the `WebhookVerifier` to test `VerifyGitHubSignature` with a known secret, payload, and signature. Test both valid and invalid signatures. | The valid signature passes (no error). The invalid signature returns a clear error (e.g., `\"invalid signature\"`). |\n| **2. Webhook Handling & Pipeline Creation** | Integration Test | Start the webhook listener HTTP server. Use `curl` to send a mock GitHub push event payload (with correct signature). | The server responds with `202 Accepted`. A new `PipelineRun` is created in the store with `Status: STATUS_PENDING`. The `Trigger`, `CommitSHA`, and `Branch` fields are correctly extracted from the payload. |\n| **3. Queue Persistence** | Integration Test | 1. Enqueue a `PipelineRun`. <br> 2. Simulate a system crash (kill the process). <br> 3. Restart and call `recoverPendingJobs`. | The pending `PipelineRun` (and its `JobRun`s) are reloaded into the in-memory queue and are available for dequeuing. No work is lost. |\n| **4. Worker Pool & Job Assignment** | Integration Test | Start a worker pool with 2 workers. Enqueue 4 jobs. Observe the logs or database. | Only 2 jobs are in `STATUS_RUNNING` at any given time. As workers finish, they pick up the next job. Each job is assigned a unique `AssignedWorker` ID. |\n| **5. Priority Scheduling** | Unit Test / Integration Test | Enqueue jobs with different `Priority` values (e.g., 10, 5, 1). Have a single worker dequeue jobs sequentially. | The jobs are dequeued in order of highest priority (10 first, then 5, then 1), regardless of insertion order. |\n| **6. Rate Limiting** | Integration Test / Manual | Configure a low rate limit (e.g., 2 requests per second). Send 5 webhook requests in rapid succession via a script. | The first 2 requests are processed immediately. The next 3 are delayed or rejected (depending on design). The system log indicates rate limiting is active. |\n\n**Integration Checkpoint:** Set up a local repository with a webhook pointing to your listener. Push a commit. Observe the system: within seconds, a webhook should be received, a pipeline run created, jobs enqueued, and workers should begin processing. The dashboard should show the new run.\n\n#### Milestone 4: Web Dashboard\n\nThe goal is to verify that the dashboard serves data, streams logs in real-time, and generates visualizations.\n\n| Checkpoint | Verification Method | Steps to Execute | Expected Outcome |\n|------------|---------------------|------------------|------------------|\n| **1. API Endpoints Return Data** | Integration Test | Start the dashboard server with a populated store. Use `curl` or a browser to hit `GET /api/runs`. | The endpoint returns a JSON array of `PipelineRun` objects with correct status, branch, and commit data. Pagination (`limit`, `offset`) works. |\n| **2. Real-Time Log Streaming (SSE)** | Manual / E2E Test | 1. Start a pipeline run that produces periodic output. <br> 2. Open `GET /api/runs/:id/logs` in a browser or use `curl -N`. <br> 3. Alternatively, use the dashboard UI. | Log lines appear in the browser/console as they are produced by the job, with minimal delay (seconds). The connection remains open until the job finishes. |\n| **3. Badge Generation** | Unit Test / Manual | Call `GET /badge/myproject.svg` for a pipeline with status `STATUS_SUCCEEDED` and another with `STATUS_FAILED`. | The endpoint returns an SVG image with `Content-Type: image/svg+xml`. The badge for the successful run is green (e.g., \"passing\"), and for the failed run is red (\"failing\"). |\n| **4. DAG Visualization Endpoint** | Unit Test / Manual | For a pipeline run with dependencies, call `GET /api/runs/:id/dag`. | Returns a JSON representation of the DAG suitable for rendering (e.g., nodes and edges). The structure correctly reflects the `needs` dependencies from the pipeline config. |\n| **5. Artifact Listing & Download** | Integration Test | After a run with artifacts, call `GET /api/runs/:id/artifacts`. Then download one artifact via the provided link. | The list endpoint returns metadata for the artifacts. The download link serves the file with correct `Content-Disposition` header. |\n| **6. Frontend Integration** | Manual E2E | Start the full system. Open the dashboard in a browser (`localhost:8080`). Trigger a pipeline via webhook. | The build list updates automatically or on refresh. Clicking on a run shows details, a live log stream, and the DAG visualization. All interactive elements work. |\n\n**Integration Checkpoint:** Deploy the dashboard and have a colleague interact with it without prior explanation. They should be able to find the status of their latest commit, see live logs, and download build artifacts without assistance. This validates the UI/UX design.\n\n### Implementation Guidance\n\n**Technology Recommendations Table:**\n\n| Component | Simple Option (For Learning) | Advanced Option (For Production) |\n|-----------|------------------------------|----------------------------------|\n| Unit Testing | Go standard `testing` package, `testify/assert` for assertions. | `github.com/stretchr/testify` suite (assert, mock, require). |\n| Mock Generation | Hand-written interface mocks. | `go.uber.org/mock` (formerly `gomock`) for auto-generated mocks. |\n| Integration Testing (Docker) | Direct `exec` of `docker` CLI in tests. | `testcontainers/testcontainers-go` for managed, ephemeral containers. |\n| Integration Testing (HTTP) | Go's `net/http/httptest` package. | `golang.org/x/net/http2/h2c` for HTTP/2, `github.com/gorilla/websocket` for WebSocket tests. |\n| E2E Testing | Bash script orchestrating binary, Docker, and `curl`. | `github.com/cucumber/godog` for BDD, or `playwright-go` for browser automation. |\n| Code Coverage | `go test -cover` | `go test -coverprofile=coverage.out && go tool cover -html=coverage.out` for HTML report. |\n\n**Recommended File/Module Structure:**\n```\nci-system/\n├── internal/\n│   ├── parser/                    # Milestone 1\n│   │   ├── parser.go              # ParseConfig, ResolveEnvVars, etc.\n│   │   ├── parser_test.go         # Unit tests\n│   │   ├── matrix.go              # ExpandMatrix\n│   │   ├── dag.go                 # BuildExecutionGraph\n│   │   └── integration_test.go    # Integration tests (if any)\n│   ├── executor/                  # Milestone 2\n│   │   ├── docker/\n│   │   │   └── client.go          # Wrapper around Docker SDK\n│   │   ├── executor.go            # ExecuteJob, StreamLogs\n│   │   ├── artifacts.go           # CollectArtifacts\n│   │   ├── executor_test.go       # Unit tests (with mocks)\n│   │   └── executor_integration_test.go # Needs Docker\n│   ├── queue/                     # Milestone 3\n│   │   ├── hybrid_queue.go        # HybridQueue implementation\n│   │   ├── priority_heap.go       # PriorityQueue (heap.Interface)\n│   │   ├── worker_pool.go         # Worker pool management\n│   │   ├── queue_test.go          # Unit tests for heap logic\n│   │   └── queue_integration_test.go # With real store\n│   ├── webhook/                   # Milestone 3\n│   │   ├── handler.go             # HandleWebhook, VerifyGitHubSignature\n│   │   ├── parser.go              # ExtractEventDetails\n│   │   └── handler_test.go        # Tests with httptest\n│   ├── dashboard/                 # Milestone 4\n│   │   ├── server.go              # DashboardServer, HTTP handlers\n│   │   ├── sse.go                 # LogBroadcaster, SSEHandler\n│   │   ├── badges.go              # GetBadge SVG generation\n│   │   ├── dag_viz.go             # GetDAG data generation\n│   │   └── server_test.go         # API endpoint tests\n│   └── store/                     # Data persistence layer\n│       ├── store.go               # Store interface\n│       ├── sqlite.go              # SQLite implementation\n│       └── store_test.go          # Store interface tests\n├── cmd/\n│   ├── server/                    # Main orchestrator binary\n│   │   └── main.go\n│   ├── worker/                    # Dedicated worker binary (optional)\n│   │   └── main.go\n│   └── dashboard/                 # Dashboard binary\n│       └── main.go\n├── test/                          # E2E and integration test utilities\n│   ├── e2e/\n│   │   └── pipeline_test.go       # Full system test\n│   └── fixtures/\n│       ├── configs/               # Sample pipeline YAMLs\n│       └── webhook_payloads/      # Sample GitHub/GitLab JSON\n└── scripts/\n    └── test-e2e.sh                # Script to run E2E tests\n```\n\n**Infrastructure Starter Code (Test Helper with Testcontainers):**\n```go\n// test/testhelpers/docker.go\npackage testhelpers\n\nimport (\n    \"context\"\n    \"testing\"\n    \"github.com/testcontainers/testcontainers-go\"\n    \"github.com/testcontainers/testcontainers-go/wait\"\n)\n\nfunc CreatePostgresContainer(ctx context.Context, t *testing.T) (testcontainers.Container, string) {\n    req := testcontainers.ContainerRequest{\n        Image:        \"postgres:15-alpine\",\n        ExposedPorts: []string{\"5432/tcp\"},\n        Env: map[string]string{\n            \"POSTGRES_DB\":       \"testdb\",\n            \"POSTGRES_USER\":     \"testuser\",\n            \"POSTGRES_PASSWORD\": \"testpass\",\n        },\n        WaitingFor: wait.ForLog(\"database system is ready to accept connections\"),\n    }\n    pgContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{\n        ContainerRequest: req,\n        Started:          true,\n    })\n    if err != nil {\n        t.Fatal(err)\n    }\n\n    host, err := pgContainer.Host(ctx)\n    if err != nil {\n        t.Fatal(err)\n    }\n    port, err := pgContainer.MappedPort(ctx, \"5432\")\n    if err != nil {\n        t.Fatal(err)\n    }\n    dsn := fmt.Sprintf(\"postgres://testuser:testpass@%s:%s/testdb?sslmode=disable\", host, port.Port())\n    return pgContainer, dsn\n}\n```\n\n**Core Logic Skeleton Code (Example: Unit Test for Parser):**\n```go\n// internal/parser/parser_test.go\npackage parser\n\nimport (\n    \"testing\"\n    \"github.com/stretchr/testify/assert\"\n    \"github.com/stretchr/testify/require\"\n)\n\nfunc TestParseConfig_ValidYAML(t *testing.T) {\n    yamlContent := `\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Echo\n        command: echo \"Hello\"\n`\n    config, err := ParseConfig(yamlContent)\n    require.NoError(t, err)\n    require.NotNil(t, config)\n\n    // TODO 1: Assert that the Jobs map has exactly one key: \"build\"\n    // TODO 2: Assert that the job \"build\" has RunsOn equal to \"ubuntu-latest\"\n    // TODO 3: Assert that the job \"build\" has exactly one step\n    // TODO 4: Assert that the step's Name is \"Echo\" and Command is `echo \"Hello\"`\n}\n\nfunc TestResolveEnvVars(t *testing.T) {\n    yamlContent := `\nenv:\n  GREETING: \"Hello\"\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Print\n        command: echo $GREETING ${GREETING}\n`\n    config, err := ParseConfig(yamlContent)\n    require.NoError(t, err)\n\n    overrideEnv := map[string]string{\"GREETING\": \"Hola\"}\n    // TODO 1: Call ResolveEnvVars with config and overrideEnv\n    // TODO 2: Assert that the step's Command is resolved to `echo Hola Hola`\n    // TODO 3: Test that system env (like $HOME) is also resolved if not overridden\n}\n\nfunc TestExpandMatrix(t *testing.T) {\n    job := JobConfig{\n        Name: \"test\",\n        Matrix: map[string]interface{}{\n            \"version\": []interface{}{\"1.19\", \"1.20\"},\n            \"os\":      []interface{}{\"linux\"},\n        },\n    }\n    // TODO 1: Call ExpandMatrix with this job\n    // TODO 2: Assert the result is a slice of length 2 (cartesian product)\n    // TODO 3: For each expanded job, assert the Env map contains VERSION and OS\n    // TODO 4: Assert the job names are distinct (e.g., \"test (version=1.19, os=linux)\")\n}\n```\n\n**Milestone Checkpoint Commands:**\n- **After Milestone 1:** Run `go test ./internal/parser/... -v`. Expect: All tests pass, with green `PASS` output. For manual verification, run `go run cmd/testparser/main.go` (if you create a small test program) and see a printed execution plan.\n- **After Milestone 2:** Run `go test ./internal/executor/... -v -short` to run unit tests. Then run `go test ./internal/executor/... -v -run TestExecuteJobIntegration` (requires Docker). Expect: Integration test passes and cleans up containers (`docker ps -a` shows no leftover test containers).\n- **After Milestone 3:** Start the webhook server (`go run cmd/server/main.go`). In another terminal, use `curl -X POST -H \"X-GitHub-Event: push\" -H \"Content-Type: application/json\" -d @test/fixtures/webhook_payloads/push.json http://localhost:8080/webhook`. Expect a `202 Accepted` response. Check the server logs for \"Pipeline run created\".\n- **After Milestone 4:** Run `go test ./internal/dashboard/... -v`. Start the dashboard (`go run cmd/dashboard/main.go`). Open `http://localhost:8081` in your browser. You should see the build history page. If you have a run in progress, click on it and see logs updating in real-time.\n\n**Debugging Tips for Tests:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Integration test hangs forever. | Docker container not starting, or command in container is infinite loop. | Check test logs for container start errors. Run `docker ps` during the test to see container state. | Ensure the container image is correct and the command exits. Use timeouts in tests (`t.Timeout`). |\n| Unit test with mocks fails: \"unexpected call\". | The test code is calling a method you didn't expect, or the mock setup is incomplete. | Examine the mock error message: it shows which method was called with what arguments. | Adjust the mock setup (`On(...).Return(...)`) to match the actual calls made by your code under test. |\n| Webhook test passes locally but fails in CI. | Missing Docker or network environment differences. | Check CI runner logs for \"cannot connect to Docker daemon\". | In CI, use Docker-in-Docker or configure the test to skip integration tests if Docker is unavailable (`if testing.Short()`). |\n| SSE log streaming test doesn't receive events. | The `LogBroadcaster` is not connected to the actual log producer in the test. | Add logging to `Broadcast` method to see if events are being sent. Check if the client is registered correctly. | In the test, ensure you call `Broadcast` after registering the SSE client. Simulate a log event from the executor. |\n| Priority queue test fails after adding many items. | The heap invariant is violated (parent priority not >= child priority). | Write a helper function to verify the heap property after each operation. Print the heap contents before failure. | Carefully implement `heap.Interface` methods (`Len`, `Less`, `Swap`, `Push`, `Pop`). `Less` should compare priority (higher priority first). |\n\n\n## Debugging Guide\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nBuilding a CI/CD system involves orchestrating multiple moving parts—webhooks, queues, containers, and real-time data streams. When things go wrong, symptoms can be confusing because a failure in one component often manifests as a strange behavior in another. This section provides a structured approach to debugging by cataloging common symptoms, their root causes, and step-by-step fixes. Think of debugging this system like being a **factory floor supervisor**: when the assembly line stops, you need to walk the line, check each station (component), inspect the work orders (queue items), and examine the machinery (containers) to find the blockage.\n\n### Common Bugs: Symptom → Cause → Fix\n\nThe following table maps observable symptoms to their likely causes, provides diagnostic steps to confirm the diagnosis, and outlines concrete fixes. Each entry assumes you're following the architecture and data models defined in previous sections.\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| **Webhooks are ignored** – No pipeline runs are created when pushing to the repository. | 1. **Webhook signature validation failure**: The `WebhookVerifier.VerifyGitHubSignature` or `VerifyGitLabToken` is rejecting the payload.<br>2. **Event type mismatch**: The `ExtractEventDetails` function cannot parse the event, or the pipeline configuration's `if` conditions filter it out.<br>3. **Network/firewall issue**: The webhook listener isn't reachable from your Git host. | 1. Check the webhook listener logs for errors during `HandleWebhook`. Look for `\"signature invalid\"` or `\"event type unknown\"` messages.<br>2. Use `curl` to send a test payload locally: `curl -X POST -H \"X-GitHub-Event: push\" -d @payload.json http://localhost:8080/webhook`. Inspect the `Store` for new `PipelineRun` entries.<br>3. Verify the webhook endpoint URL is correctly configured in your repository settings and that port forwarding/firewall rules allow inbound connections. | 1. Ensure the webhook secret in your CI system matches the one configured in your repository. Double-check the signature header name (GitHub uses `X-Hub-Signature-256`, GitLab uses `X-GitLab-Token`).<br>2. Add debug logging in `ExtractEventDetails` to print the raw event type and ref. Ensure your pipeline YAML's `on:` clause matches the event.<br>3. If using a local development environment, use a service like ngrok to expose your local server with a public URL for testing webhooks. |\n| **Jobs hang forever** – A `JobRun` remains in `STATUS_RUNNING` long after commands should have completed. | 1. **Container cleanup failure**: The Docker container exited, but `ExecuteJob` failed to update the `JobRun.Status` to `STATUS_FAILED` or `STATUS_SUCCEEDED`.<br>2. **Infinite loop in user script**: A step command (e.g., `while true; do sleep 1; done`) never exits.<br>3. **Deadlock in worker coordination**: The worker process crashed while holding a lock, or the `HybridQueue.DequeueJobRun` is stuck waiting. | 1. Run `docker ps -a` to list all containers. Look for containers with the `JobRun.ID` in their name or labels. Check if they are in `Exited` state.<br>2. Check the real-time logs for the job. If logs stop abruptly, the container may have been killed. Use `docker logs <container-id>` to see the container's stdout/stderr.<br>3. Inspect the worker logs. If a worker panicked, there may be no log of job completion. Check the `Store` for `JobRun` records with `Status=STATUS_RUNNING` but no recent `UpdatedAt` timestamp. | 1. Implement a **heartbeat or timeout** in `ExecuteJob`. Use `context.WithTimeout` and ensure the Docker client's `ContainerWait` call respects it. Always call `ContainerRemove` in a `defer` block.<br>2. Add **timeout configuration** to `JobConfig`. In `ExecuteJob`, enforce a maximum duration per job, killing the container if exceeded.<br>3. Implement a **stuck job recovery** routine. The `CleanupStuckJobs` function should periodically scan for jobs in `STATUS_RUNNING` with an old `StartedAt` time and mark them as `STATUS_FAILED`. |\n| **Logs stop streaming** – The dashboard shows a job is running, but the log output freezes and doesn't update. | 1. **Backpressure in log streaming**: The `LogBroadcaster.Broadcast` is blocked because a client channel is full (slow browser).<br>2. **Broken pipe in container log tail**: The goroutine reading from Docker's `ContainerLogs` encountered an error and stopped.<br>3. **SSE connection closed**: The browser's EventSource connection dropped due to network interruption or server-side timeout. | 1. Check the dashboard server logs for errors like `\"send on closed channel\"` or `\"client channel full\"`. Monitor the number of active clients in `LogBroadcaster.clients`.<br>2. In `StreamLogs`, verify the reader from `dockerClient.ContainerLogs` is still active. Add logging to capture `io.Copy` errors.<br>3. Open browser developer tools, check the Network tab for the SSE endpoint (`/api/runs/:id/logs`). Look for connection status and error events. | 1. Implement **non-blocking send with buffer** in `LogBroadcaster`. Use a buffered channel for each `ClientChannel.Send` and a select statement with a default case to drop old log lines if the client is too slow.<br>2. Add **reconnection logic** in the frontend JavaScript. When the EventSource closes, wait a few seconds and reconnect.<br>3. Ensure the `StreamLogs` method handles `context.Canceled` gracefully (when client disconnects) and stops reading container logs to free resources. |\n| **Artifacts missing** – The job completes successfully, but no artifacts appear on the download page. | 1. **Glob pattern mismatch**: The artifact `patterns` in the pipeline YAML don't match any files in the container's workspace.<br>2. **Permission error during copy**: The `CollectArtifacts` function cannot read files from the container or write to persistent storage.<br>3. **Container removed before collection**: The `ExecuteJob` function removes the container before `CollectArtifacts` is called. | 1. Check the job logs for messages from `CollectArtifacts`. Look for `\"no files matched\"` warnings. Run the job locally with the same image and commands to verify file paths.<br>2. Inspect the Docker daemon logs for permission errors. Ensure the CI system's process has read/write access to the artifact storage directory.<br>3. Verify the order of operations in `ExecuteJob`: artifacts should be collected *before* `ContainerRemove` is called. Check if the `JobRun.Status` is updated before artifact collection. | 1. Improve **artifact pattern debugging**: Log the resolved absolute paths inside the container before attempting copy. Consider supporting absolute paths and relative paths from the workspace.<br>2. Run the CI system with appropriate user permissions. If using Docker, ensure the container runs with a user ID that can read the files (or run as root if necessary).<br>3. Refactor `ExecuteJob` to use a **deferred cleanup** pattern: schedule container removal only after all steps (including artifact collection) are complete or have failed. |\n| **Matrix builds explode** – Creating a pipeline run with a matrix causes hundreds of jobs, overwhelming the system. | 1. **Combinatorial explosion**: A matrix with multiple axes, each with many values, produces too many job permutations (e.g., `os: [ubuntu, alpine]` × `node: [14, 16, 18, 20]` × `python: [3.8, 3.9, 3.10]` = 24 jobs).<br>2. **Infinite expansion loop**: A bug in `ExpandMatrix` causes recursive expansion or incorrect handling of nested arrays. | 1. Check the pipeline run detail in the dashboard. Count the number of `JobRun` entries created. Examine the `MatrixAxes` field in the `PipelineConfig`.<br>2. Add logging to `ExpandMatrix` to print each axis and its values. Verify the Cartesian product logic is correct. | 1. Implement **matrix limits** in the parser: reject matrix definitions that would produce more than a configured maximum (e.g., 100) jobs.<br>2. Add **exclude clauses** to the matrix configuration (like GitHub Actions) to filter out unwanted combinations.<br>3. Ensure `ExpandMatrix` correctly handles scalar values and empty arrays. Use unit tests with edge cases. |\n| **Dependency deadlock** – Jobs are stuck in `STATUS_PENDING` even though their dependencies have completed. | 1. **Circular dependency**: The pipeline configuration has a cycle in job `needs` (e.g., job A needs B, B needs C, C needs A).<br>2. **Missing dependency update**: `MarkJobRunComplete` fails to notify or enqueue dependent jobs.<br>3. **Silent dependency failure**: A dependent job was skipped (`STATUS_SKIPPED`) due to an `if` condition, but downstream jobs are waiting for it to succeed. | 1. Use the pipeline DAG visualization (`GetDAG` endpoint) to inspect the graph. Look for cycles.<br>2. Check the `HybridQueue` after a job completes. Are the dependent jobs added to the heap? Inspect the `Store` for `JobRun` records whose `Needs` list contains only completed jobs.<br>3. Examine the `JobRun.Status` of all jobs in the pipeline. Look for `STATUS_SKIPPED` jobs that other jobs depend on. | 1. Add **cycle detection** in `BuildExecutionGraph`. Use topological sort (Kahn's algorithm) and fail early if a cycle is detected.<br>2. In `MarkJobRunComplete`, after updating the job status, query all jobs that list this job in their `Needs`. If *all* dependencies of a downstream job are satisfied (complete or skipped), call `EnqueueJobRun` for it.<br>3. Adjust dependency resolution logic: a job should run if its dependencies are `STATUS_SUCCEEDED` *or* `STATUS_SKIPPED`. Update the condition in `MarkJobRunComplete`. |\n| **Priority scheduling not working** – High-priority jobs (like production deploys) are stuck behind low-priority test jobs. | 1. **Priority field not set**: The `PipelineRun` or `JobRun` created from the webhook has `Priority=0` (default).<br>2. **Heap invariant broken**: The `PriorityQueue` implementation (a min-heap) is not correctly maintaining order during `push`/`pop`.<br>3. **Worker not respecting priority**: Workers are dequeuing jobs in a different order (e.g., FIFO) because they're accessing the queue incorrectly. | 1. Inspect the `Item` entries in the `HybridQueue.heap`. Log the `Priority` of each item when enqueuing. Check the webhook handler logic that sets priority based on branch name (e.g., `main` gets higher priority).<br>2. Write a unit test for `PriorityQueue` that inserts items with random priorities and verifies they pop in correct order.<br>3. Check that all workers use the same `DequeueJobRun` method, which should atomically fetch the highest-priority item. | 1. Ensure `HandleWebhook` assigns a meaningful priority. For example: `main` branch → priority 10, feature branches → priority 1. Store this in the `PipelineRun` and propagate to `JobRun`.<br>2. Verify the `heap.Interface` implementation (Len, Less, Swap, Push, Pop). The `Less` method should compare `Priority` (higher priority = smaller number if min-heap).<br>3. Use **transactional dequeue**: `DequeueJobRun` must lock the heap, take the top item, remove it, and update the store, all in one atomic operation. |\n| **Real-time dashboard shows stale data** – The list of pipeline runs doesn't update until the page is refreshed. | 1. **No reactive updates**: The dashboard frontend uses periodic polling instead of live updates for the run list.<br>2. **Caching in API layer**: The `GetRuns` handler serves a cached response or doesn't reflect the latest `Store` state.<br>3. **Database isolation level**: If using SQLite with multiple goroutines, reads may not see writes from other connections without proper transaction handling. | 1. Check the browser's network tab. Are there repeated calls to `/api/runs`? If not, the frontend is not polling.<br>2. Add a timestamp log in `GetRuns`. Verify it's called when new runs are created. Check if the `Store.ListPipelineRuns` uses a cache with TTL.<br>3. In SQLite, enable WAL mode (`journal_mode=WAL`) to allow reads and writes to happen concurrently. Check for `BEGIN EXCLUSIVE` transactions that block reads. | 1. Implement **Server-Sent Events (SSE) for run list updates** or use WebSocket to push new run events to the dashboard. Alternatively, use simple HTTP polling (every 5 seconds).<br>2. Ensure the `Store` interface methods are called with a context that has a short timeout. Avoid caching in the API layer for real-time data.<br>3. Use a connection pool for the database and set appropriate transaction isolation levels. For SQLite, ensure writes are committed before reads are attempted. |\n\n### Debugging Techniques and Tools\n\nEffective debugging requires both systematic thinking and familiarity with the tools at your disposal. When faced with an issue, adopt the mindset of a **detective following clues**: start from the symptom, gather evidence from logs and system state, form a hypothesis, test it, and then apply the fix. Below are specialized techniques and tools for this CI/CD system.\n\n#### 1. Container Forensics with Docker Commands\n\nSince jobs run in Docker containers, your first stop when a job behaves oddly is the Docker daemon.\n\n- **List all containers**: `docker ps -a` shows containers from all runs, including exited ones. Look for containers with names or labels containing the `JobRun.ID`. If a container is still running when it should be finished, you might have a hanging process.\n- **Inspect container details**: `docker inspect <container-id>` gives you a JSON dump of the container's configuration, including environment variables, mounted volumes, and the exact command that was run. Compare this with what you expect from the `JobConfig`.\n- **Check container logs**: `docker logs <container-id>` shows the stdout/stderr of the container, which might have more details than what was captured by `StreamLogs` (especially if the log streaming broke). Use `--tail` and `--follow` to watch in real time.\n- **Examine filesystem**: `docker exec -it <container-id> /bin/sh` (if the container is still running) lets you explore the workspace and verify that files were created as expected. For stopped containers, you can `docker export` the filesystem to a tar archive.\n- **Clean up orphaned containers**: If your CI system crashes, containers might be left behind. `docker container prune` removes all stopped containers. Be careful not to remove containers belonging to active jobs.\n\n#### 2. Queue and State Inspection\n\nThe queue (`HybridQueue`) and the `Store` hold the authoritative state of what's supposed to happen.\n\n- **Dump the queue contents**: Add a debug endpoint (or temporary code) that logs the contents of the `PriorityQueue` heap. Print each `Item`'s `JobRunID`, `Priority`, and `CreatedAt`. This reveals if jobs are stuck in the queue or ordered incorrectly.\n- **Query the database directly**: If using SQLite, open the database file with `sqlite3 ci.db` and run queries:\n  ```sql\n  SELECT * FROM pipeline_runs ORDER BY created_at DESC LIMIT 5;\n  SELECT * FROM job_runs WHERE status = 'RUNNING';\n  SELECT job_name, status, COUNT(*) FROM job_runs GROUP BY job_name, status;\n  ```\n  This gives you a ground truth independent of the application's memory state.\n- **Check for blocked dependencies**: Write a query to find jobs that are `STATUS_PENDING` but have all their dependencies satisfied (i.e., all jobs in their `needs` list are `STATUS_SUCCEEDED` or `STATUS_SKIPPED`). This uncovers bugs in the dependency resolution logic.\n\n#### 3. Structured Logging and Correlation IDs\n\nThe system produces many logs; without proper structure, it's hard to trace a single pipeline run across components.\n\n- **Enable correlation IDs**: Ensure every webhook request generates a unique correlation ID (e.g., `X-Correlation-ID`) that is passed through all components and included in every log entry. The `Logger` should have a `WithCorrelationID` method that adds this field to the `LogEntry.Fields`.\n- **Search logs by run ID**: When investigating a specific pipeline run, grep your log files for its `PipelineRun.ID`. For example: `grep \"run_abc123\" ci.log | tail -20`. This shows you the entire lifecycle across webhook, queue, worker, and dashboard.\n- **Log at key decision points**: Add debug-level logs at critical junctures:\n  - When a webhook is received and validated (`HandleWebhook`)\n  - When a job is enqueued (`EnqueueJobRun`)\n  - When a worker picks up a job (`DequeueJobRun`)\n  - When a container starts and stops (`ExecuteJob`)\n  - When artifacts are collected (`CollectArtifacts`)\n- **Use log levels wisely**: `ERROR` for conditions that require intervention (e.g., signature mismatch, container failure). `WARN` for unusual but recoverable situations (e.g., artifact pattern matched no files). `INFO` for normal business logic (e.g., job started). `DEBUG` for detailed step-by-step flow.\n\n#### 4. Network and Endpoint Testing\n\nMany failures are due to misconfigured networking or HTTP errors.\n\n- **Test webhook endpoint manually**: Use `curl` or Postman to simulate a webhook payload. This bypasses Git host complexities and lets you control exactly what's sent. Example for GitHub:\n  ```bash\n  curl -X POST -H \"Content-Type: application/json\" \\\n       -H \"X-GitHub-Event: push\" \\\n       -H \"X-Hub-Signature-256: sha256=$(openssl dgst -sha256 -hmac \"your-secret\" payload.json | cut -d' ' -f2)\" \\\n       -d @payload.json \\\n       http://localhost:8080/webhook\n  ```\n- **Verify dashboard API responses**: The dashboard endpoints (`/api/runs`, `/api/runs/:id/logs`) should return appropriate HTTP status codes. Use `curl` to check:\n  ```bash\n  curl -v http://localhost:8080/api/runs\n  curl -v http://localhost:8080/api/runs/run_abc123/logs\n  ```\n  Look for `200 OK`, correct `Content-Type` (e.g., `text/event-stream` for logs), and CORS headers if needed.\n- **Monitor open connections**: Use `netstat` or `lsof` to see if your server is accumulating many open connections (indicating a leak). For example: `lsof -i :8080` lists processes using port 8080.\n\n#### 5. Performance Profiling and Resource Monitoring\n\nWhen the system slows down or becomes unresponsive, resource exhaustion might be the cause.\n\n- **Monitor Docker daemon resources**: `docker stats` shows real-time CPU, memory, and network usage of all containers. A job with a memory leak can starve other jobs.\n- **Check system resource usage**: Use `top` or `htop` to see if the CI server process itself is using excessive CPU or memory. The log broadcaster holding many client channels in memory can cause bloat.\n- **Profile Go code**: If you suspect a performance bug in the Go code, use the built-in profiler. Add `import _ \"net/http/pprof\"` and start an HTTP server on a debug port. Then use `go tool pprof` to examine CPU and memory profiles.\n- **Set resource limits**: In `ExecuteJob`, use Docker's `--memory`, `--cpus`, and `--ulimit` flags to prevent a single job from consuming all host resources.\n\n#### 6. Incremental and Isolated Testing\n\nWhen you add a new feature or fix a bug, test it in isolation before integrating.\n\n- **Unit test each component**: Run `go test ./internal/parser/...` to ensure the parser still works after changes. Write table-driven tests for edge cases in `ExpandMatrix` and `BuildExecutionGraph`.\n- **Integration test with a mock Docker daemon**: Use a library like `testcontainers` or a mock Docker client to test `ExecuteJob` without a real Docker daemon. This speeds up tests and avoids side effects.\n- **End-to-end test with a local Git server**: Set up a local Git server (e.g., Gitea) or use file:// URLs to simulate pushes. Trigger a webhook and watch the pipeline run through completion. This tests the entire chain.\n- **Use a staging environment**: If possible, run a second instance of the CI system against a test repository. This lets you experiment with configuration changes without affecting production pipelines.\n\n> **Key Insight**: The most effective debugging combines **observability** (logs, metrics, traces) with **reproducibility** (being able to recreate the failure on demand). Invest time in building good logging and making the system testable; it pays off when things go wrong at 3 AM.\n\n### Implementation Guidance\n\nWhile debugging is primarily a thinking skill, having the right tools and code patterns makes it easier. Below are some implementation snippets that help with debugging.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Logging | Structured JSON logs to stdout (using `log/slog` or `zap`) | Centralized log aggregation (ELK stack, Grafana Loki) with correlation IDs |\n| Metrics | Basic counters and gauges exposed via `/metrics` (Prometheus format) | Full tracing with OpenTelemetry to track requests across components |\n| Debug Endpoints | HTTP `/debug/pprof` for profiling, `/debug/queue` for queue inspection | Admin dashboard with real-time system health and manual intervention buttons |\n| Container Inspection | Direct `docker` command-line calls | Container monitoring tools (cAdvisor, Portainer) |\n\n#### B. Recommended File/Module Structure\n\nAdd debugging utilities within a dedicated package:\n\n```\nci-system/\n  cmd/\n    server/main.go\n    worker/main.go\n  internal/\n    debug/              # Debugging utilities\n      inspector.go     # Functions to dump queue state, list stuck jobs\n      profiler.go      # pprof HTTP server setup\n    store/\n      store.go         # Store interface\n      sqlite.go        # SQLite implementation\n    queue/\n      hybrid_queue.go  # HybridQueue with debug methods\n    logger/\n      logger.go        # Structured Logger with correlation ID support\n```\n\n#### C. Infrastructure Starter Code: Structured Logger with Correlation ID\n\nHere's a complete, ready-to-use structured logger that supports correlation IDs and log levels:\n\n```go\n// internal/logger/logger.go\npackage logger\n\nimport (\n    \"context\"\n    \"log/slog\"\n    \"os\"\n)\n\ntype ctxKey string\n\nconst correlationIDKey ctxKey = \"correlation_id\"\n\n// Logger wraps slog.Logger and adds correlation ID support.\ntype Logger struct {\n    *slog.Logger\n}\n\n// New creates a new Logger with JSON output.\nfunc New(component string) *Logger {\n    return &Logger{\n        Logger: slog.New(slog.NewJSONHandler(os.Stdout, &slog.HandlerOptions{\n            Level: slog.LevelDebug,\n        })).With(\"component\", component),\n    }\n}\n\n// WithCorrelationID returns a new Logger with the correlation ID added to all log entries.\nfunc (l *Logger) WithCorrelationID(cid string) *Logger {\n    return &Logger{l.Logger.With(\"correlation_id\", cid)}\n}\n\n// FromContext extracts the correlation ID from context and returns a Logger with it.\nfunc (l *Logger) FromContext(ctx context.Context) *Logger {\n    if cid, ok := ctx.Value(correlationIDKey).(string); ok {\n        return l.WithCorrelationID(cid)\n    }\n    return l\n}\n\n// NewContextWithCorrelationID creates a new context with the given correlation ID.\nfunc NewContextWithCorrelationID(ctx context.Context, cid string) context.Context {\n    return context.WithValue(ctx, correlationIDKey, cid)\n}\n\n// LogEntry is the structured log entry (for reference, not used directly).\ntype LogEntry struct {\n    Timestamp   string                 `json:\"timestamp\"`\n    Level       string                 `json:\"level\"`\n    Component   string                 `json:\"component\"`\n    Message     string                 `json:\"message\"`\n    Error       string                 `json:\"error,omitempty\"`\n    ErrorType   string                 `json:\"error_type,omitempty\"`\n    Fields      map[string]interface{} `json:\"fields,omitempty\"`\n    Caller      string                 `json:\"caller,omitempty\"`\n}\n```\n\n#### D. Core Logic Skeleton Code: Debug Inspector for Queue\n\nAdd a debug method to `HybridQueue` to inspect its contents safely:\n\n```go\n// internal/queue/hybrid_queue.go\npackage queue\n\nimport (\n    \"fmt\"\n    \"sort\"\n)\n\n// DebugItems returns a slice of all items in the heap, sorted by priority (highest first).\n// This is safe for debugging but not for production use as it holds the lock.\nfunc (hq *HybridQueue) DebugItems() []Item {\n    hq.mu.Lock()\n    defer hq.mu.Unlock()\n    \n    // Copy the heap slice\n    items := make([]Item, len(hq.heap))\n    for i, item := range hq.heap {\n        items[i] = *item\n    }\n    // Sort by priority (ascending for min-heap, so highest priority = smallest number)\n    sort.Slice(items, func(i, j int) bool {\n        return items[i].Priority < items[j].Priority\n    })\n    return items\n}\n\n// PrintDebugInfo logs the current state of the queue.\nfunc (hq *HybridQueue) PrintDebugInfo() {\n    items := hq.DebugItems()\n    fmt.Printf(\"=== Queue Debug Info ===\\n\")\n    fmt.Printf(\"Total items: %d\\n\", len(items))\n    for _, item := range items {\n        fmt.Printf(\"  JobRunID: %s, Priority: %d, CreatedAt: %v\\n\",\n            item.JobRunID, item.Priority, item.CreatedAt)\n    }\n}\n```\n\n#### E. Language-Specific Hints\n\n- **Go's pprof**: Import `_ \"net/http/pprof\"` and start an HTTP server on a debug port (e.g., `:6060`). Then run `go tool pprof http://localhost:6060/debug/pprof/heap` to analyze memory usage.\n- **Docker SDK**: Use `github.com/docker/docker/client` to interact with Docker. When debugging, set `DOCKER_LOG_LEVEL=debug` to see verbose output from the Docker client.\n- **SQLite browser**: Use a GUI like DB Browser for SQLite to visually inspect tables and run ad-hoc queries during debugging.\n- **Context timeouts**: Always pass a context with timeout to potentially blocking operations (like `ContainerWait`). Use `context.WithTimeout` and handle `context.DeadlineExceeded` errors appropriately.\n\n#### F. Milestone Checkpoint\n\nAfter implementing the system, verify debugging capabilities:\n\n1. **Trigger a pipeline run** and check that correlation IDs flow through all logs:\n   ```bash\n   curl -X POST http://localhost:8080/webhook -d @test_payload.json\n   tail -f ci.log | grep -E \"correlation_id.*<id>\"\n   ```\n   You should see logs from webhook handler, queue, worker, and dashboard with the same correlation ID.\n\n2. **Simulate a stuck job** by adding a `sleep 1000` command in a pipeline step. Then use the debug inspector:\n   ```bash\n   curl http://localhost:8080/debug/queue   # if you implement such endpoint\n   ```\n   You should see the job in the queue with its priority.\n\n3. **Test artifact collection debugging** by intentionally using a wrong glob pattern. Check the logs for a clear warning message indicating which pattern matched zero files.\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Dashboard shows \"Connection lost\" repeatedly | SSE stream keeps disconnecting | Check browser console for errors. Look for server logs with \"client channel full\" or \"write timeout\". | Increase the `Send` channel buffer size in `ClientChannel`. Implement ping/pong keepalive in SSE. |\n| Worker crashes with \"container not found\" | Race condition: container removed before logs are fully read | Check timestamps: when does `ContainerRemove` happen vs when does `StreamLogs` finish? | Use a waitgroup or channel to ensure `StreamLogs` goroutine completes before container removal. |\n| SQLite database locked | Multiple goroutines writing without proper locking | Use `sqlite3 ci.db \"PRAGMA journal_mode=WAL;\"` to enable WAL mode. Check for long-running transactions. | Use a connection pool with max open connections = 1, or switch to a proper database like PostgreSQL. |\n| Webhook handler returns 500 but logs show nothing | Panic recovered by HTTP server | Look for `http: panic serving` in stdout. Add a defer with recover in `HandleWebhook`. | Wrap the handler logic in a defer to catch panics and log them with the correlation ID. |\n\n\n## Future Extensions\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nThe initial CI/CD Pipeline Orchestrator is designed as a **minimal viable product** that delivers the core value proposition: running automated pipelines in response to code changes. However, the architecture is intentionally **extensible** at key interfaces to accommodate future enhancements without requiring a complete rewrite. This section explores potential improvements that could be built upon the existing foundation, categorized by implementation effort and architectural impact.\n\nThink of the current system as a **modular factory** with standardized connection points: just as a real factory can add new assembly lines or upgrade individual workstations without shutting down the entire plant, our CI system can evolve by extending components that follow clear contracts.\n\n### Low-Effort Extensions\n\nLow-effort extensions are features that require **minimal architectural changes** and can often be implemented by adding new configuration options, extending existing data structures, or introducing small new modules that plug into existing interfaces. These are ideal for learners who want to practice incremental enhancement after completing the core milestones.\n\n| Extension Idea | Current Design Support | Required Changes | Effort Estimate |\n|----------------|------------------------|------------------|-----------------|\n| **Additional Git Providers (Bitbucket, Gitea)** | The `WebhookVerifier` interface and `ExtractEventDetails` function are already abstracted to handle multiple providers. The `HandleWebhook` method uses a generic payload structure. | 1. Add new secret/token validation methods to `WebhookVerifier`.<br>2. Extend `ExtractEventDetails` to parse provider-specific payload schemas.<br>3. Add provider-specific event type constants (e.g., `EVENT_BITBUCKET_PUSH`). | Low |\n| **Manual Pipeline Triggers via API** | The `EnqueueRun` function and `PipelineRun` creation logic are already decoupled from webhooks. The `Store` interface supports creating arbitrary runs. | 1. Add a new HTTP endpoint (e.g., `POST /api/runs/trigger`) that accepts a pipeline config and branch/commit parameters.<br>2. Reuse the existing `HandleWebhook` logic to construct a `PipelineRun` from manual parameters. | Low |\n| **Per-Job Timeout Configuration** | The `JobConfig` structure already includes a `RunsOn` field for image specification; timeouts are a similar job-level property. The `ExecuteJob` function already has a context for cancellation. | 1. Add a `TimeoutSeconds int` field to `JobConfig`.<br>2. In `ExecuteJob`, create a context with a deadline derived from this timeout.<br>3. Propagate timeout errors through the job status (`STATUS_FAILED` with a timeout reason). | Low |\n| **Job Artifacts as Pipeline Dependencies** | The `ArtifactKeys` field in `JobRun` already stores references to collected artifacts. The dependency graph (`BuildExecutionGraph`) currently only uses the `Needs` field. | 1. Extend `JobConfig` with an `ArtifactDependencies []string` field that lists artifact patterns from upstream jobs.<br>2. During job execution, download these artifacts into the container workspace before running steps.<br>3. Modify the `CollectArtifacts` function to also handle uploading for downstream consumption. | Medium |\n| **Pipeline-Level Environment Variables File** | The `PipelineConfig` already has an `Environment map[string]string` field. The `ResolveEnvVars` function merges pipeline, system, and secret variables. | 1. Add a `EnvFile string` field to `PipelineConfig` that specifies a path (e.g., `.ci-env`) in the repository.<br>2. During parsing, read this file from the source code and merge its key-value pairs into the pipeline environment.<br>3. Ensure the file is fetched as part of the source checkout before variable resolution. | Low |\n| **Basic Notification Webhooks** | The `Store` interface can be extended to emit events when a pipeline run changes state. The `MarkJobRunComplete` function already updates status. | 1. Define a simple event bus interface (e.g., `Notify(stateChange)`).<br>2. In `MarkJobRunComplete`, fire a notification event with run details.<br>3. Implement a webhook notifier that posts JSON to a configurable URL on failure/success. | Medium |\n\n#### Deep Dive: Supporting Additional Git Providers\n\n> **Decision: Extensible Webhook Parser Design**\n> - **Context**: The initial implementation supports GitHub and GitLab, but teams may use other Git hosting services like Bitbucket, Gitea, or self-hosted solutions.\n> - **Options Considered**:\n>   1. **Hardcoded per-provider logic**: Write separate handler functions for each provider.\n>   2. **Plugin architecture**: Define a `WebhookProvider` interface that each provider implements, with dynamic registration.\n>   3. **Unified payload normalization**: Parse all provider payloads into a common internal event format using a configurable mapping.\n> - **Decision**: Use a **unified payload normalization** approach, extending the existing `ExtractEventDetails` function with provider detection and mapping tables.\n> - **Rationale**: This keeps the webhook handling simple and linear, avoiding the complexity of a plugin system while still allowing new providers to be added by editing a single mapping file. The current `WebhookVerifier` already supports multiple secret mechanisms.\n> - **Consequences**: Adding a new provider requires updating the payload mapping and adding a verification method, but no changes to the core queue or execution logic.\n\n**How the current design accommodates this**:\nThe `HandleWebhook` method currently calls `ExtractEventDetails` which returns generic fields (`eventType`, `repo`, `branch`, `commitSHA`). This abstraction means the rest of the system doesn't care whether the event came from GitHub or GitLab. To add Bitbucket:\n\n1. **Verification**: Add a `VerifyBitbucketSignature` method to `WebhookVerifier`.\n2. **Parsing**: Extend `ExtractEventDetails` to detect Bitbucket webhooks (via `User-Agent` header or `X-Event-Key`) and map its JSON payload to the same internal fields.\n3. **Configuration**: Allow users to set a Bitbucket secret in the system configuration.\n\nThe `PipelineRun` created will be identical, and the rest of the pipeline execution proceeds unchanged.\n\n#### Deep Dive: Manual Pipeline Triggers\n\n> **Decision: Reusing Webhook Pipeline Creation Logic**\n> - **Context**: Developers may want to trigger a pipeline run manually (e.g., for a specific branch, with custom parameters) without waiting for a Git event.\n> - **Options Considered**:\n>   1. **New standalone pipeline creation service**: Build a separate module that constructs runs from scratch.\n>   2. **Parameterized webhook handler**: Reuse the `HandleWebhook` function but pass it a synthetic webhook payload constructed from manual parameters.\n>   3. **Direct store invocation**: Call `CreatePipelineRun` directly with a fully formed `PipelineRun` object.\n> - **Decision**: **Parameterized webhook handler** – extend `HandleWebhook` to accept an optional manually constructed payload.\n> - **Rationale**: This maximizes code reuse: the same validation, pipeline matching, and run creation logic applies. The only difference is the source of the event data. By adding a `manualPayload` parameter, we avoid duplicating complex logic.\n> - **Consequences**: The `HandleWebhook` function becomes slightly more complex, but the manual trigger endpoint becomes trivial to implement.\n\n**Implementation path**:\n```go\n// Extended HandleWebhook signature\nfunc HandleWebhook(payload []byte, signature string, manualEvent *ManualEvent) (PipelineRun, error) {\n    if manualEvent != nil {\n        // Construct synthetic webhook payload from manualEvent\n        payload = createSyntheticPayload(manualEvent)\n        // Skip signature verification\n    } else {\n        // Normal verification flow\n    }\n    // Existing parsing and run creation...\n}\n```\n\nA new `ManualEvent` struct would contain fields like `Repo`, `Branch`, `CommitSHA`, `EventType`, and optional `Parameters` (for custom environment variables).\n\n### Major Enhancements\n\nMajor enhancements require **significant architectural changes**, potentially introducing new components, changing core data flows, or requiring a redesign of existing interfaces. These represent the evolution of the system from a single-node educational tool towards a production-grade distributed CI/CD platform.\n\n| Extension Idea | Architectural Impact | Required Changes | Effort Estimate |\n|----------------|----------------------|------------------|-----------------|\n| **Distributed Workers Across Machines** | High - Changes the fundamental execution model from a local worker pool to a cluster. | 1. Replace the in-memory `PriorityQueue` with a distributed queue (e.g., Redis Streams, RabbitMQ).<br>2. Introduce a worker registration/discovery mechanism.<br>3. Modify the job execution engine to support remote Docker daemons or Kubernetes pods.<br>4. Add a resource requirement system (CPU, memory) and scheduling constraints. | High |\n| **Plugin Ecosystem for Steps and Actions** | Medium - Requires a new plugin runtime and extension points in the job execution engine. | 1. Define a `StepPlugin` interface with a standard invocation method.<br>2. Create a plugin registry that loads external binaries or scripts.<br>3. Extend `StepConfig` to support `plugin: name` instead of just `command`.<br>4. Implement secure plugin execution with isolated permissions. | High |\n| **Persistent Workflow Caches** | Medium - Introduces a shared cache service and integration points in the job execution engine. | 1. Design a cache service with storage backends (local disk, S3).<br>2. Define cache keys based on job context (e.g., `hash(dependencies, command)`).<br>3. Integrate cache restore/save into `ExecuteJob` for specific directories (e.g., `node_modules`, `go/pkg`).<br>4. Add cache configuration to `JobConfig` (paths, key generation). | Medium |\n| **Pipeline-as-Code with Dynamic Generation** | High - Changes how pipeline configurations are discovered and evaluated. | 1. Introduce a scriptable pipeline generator (e.g., a `generate-pipeline` script that outputs YAML).<br>2. Modify the parser to run this generator before parsing.<br>3. Support multiple configuration files and inheritance (e.g., base pipeline with overrides). | Medium |\n| **Advanced Security with Secret Management** | Medium - Requires a secure secret injection system beyond environment variables. | 1. Integrate with external secret managers (Hashicorp Vault, AWS Secrets Manager).<br>2. Add secret references in pipeline config (e.g., `secret: DB_PASSWORD`).<br>3. Implement just-in-time secret retrieval and secure temporary storage in containers.<br>4. Audit logging for secret access. | High |\n| **Pipeline Approval Gates and Manual Interventions** | Medium - Introduces a new paused state and user interaction mechanism. | 1. Add a new job status `STATUS_WAITING_FOR_APPROVAL`.<br>2. Extend the dashboard with an approval UI.<br>3. Create an API for resuming paused jobs.<br>4. Integrate with notification systems to alert approvers. | Medium |\n\n#### Deep Dive: Distributed Workers Across Machines\n\n> **Decision: Decentralized Job Scheduling Architecture**\n> - **Context**: As pipeline volume grows, a single machine becomes a bottleneck. We need to scale horizontally by adding worker nodes across multiple machines, potentially in different data centers.\n> - **Options Considered**:\n>   1. **Centralized scheduler with remote execution**: Keep a single orchestrator that schedules jobs and uses SSH or Docker API to execute on remote nodes.\n>   2. **Distributed work queue with worker pull**: Use a persistent distributed queue (Redis, PostgreSQL) where workers pull jobs independently.\n>   3. **Peer-to-peer job distribution**: Workers discover each other and distribute jobs via gossip protocol.\n> - **Decision**: **Distributed work queue with worker pull** – replace the `HybridQueue` with a Redis-backed queue and implement worker nodes as independent processes.\n> - **Rationale**: This pattern is battle-tested in production CI systems (Jenkins, GitLab Runner). It provides clear separation of concerns, scales horizontally, and allows workers to be added/removed dynamically. Redis provides persistence, pub/sub for coordination, and atomic operations for job assignment.\n> - **Consequences**: Introduces a new infrastructure dependency (Redis). Workers become stateful and must handle network partitions. The `JobRun`'s `AssignedWorker` field becomes critical for tracking and recovery.\n\n**Architectural changes required**:\n\n1. **Queue Replacement**: The current `HybridQueue` (in-memory heap + SQLite persistence) would be replaced with a `RedisQueue` that implements the same `Store` interface methods (`EnqueueJobRun`, `DequeueJobRun`, `MarkJobRunComplete`). Jobs would be stored as Redis streams or sorted sets with priority scores.\n\n2. **Worker Node Daemon**: A new `worker-node` binary would be created, containing:\n   - Connection to the Redis queue\n   - Docker client for execution\n   - Health reporting and metrics\n   - Configuration for resource limits (concurrency, CPU/memory reservations)\n\n3. **Orchestrator Role Changes**: The main orchestrator becomes a **pure scheduler** that only creates pipeline runs and pushes jobs to Redis. It no longer manages a local worker pool.\n\n4. **Discovery and Registration**: Workers register themselves in Redis when they start, allowing the dashboard to show available capacity.\n\n5. **Artifact Storage**: Artifacts can no longer be stored on the orchestrator's local filesystem. They must be uploaded to a shared storage service (S3, MinIO) with `ArtifactKeys` becoming URLs.\n\nThe current `JobRun` data model already supports this: the `AssignedWorker` field would store the worker node ID, and `ContainerID` would still reference the Docker container on that specific worker.\n\n#### Deep Dive: Plugin Ecosystem for Steps and Actions\n\n> **Decision: Binary Plugin Interface with gRPC**\n> - **Context**: Hardcoded shell commands limit pipeline flexibility. Teams want reusable, versioned actions (like GitHub Actions) that can abstract complex operations.\n> - **Options Considered**:\n>   1. **Shell script wrappers**: Continue using shell commands but package them in versioned scripts fetched from a repository.\n>   2. **Docker container actions**: Each step runs a specified Docker container with entrypoint and arguments.\n>   3. **Binary plugins with gRPC**: Define a gRPC service interface that plugins must implement, allowing any language.\n> - **Decision**: **Docker container actions** as the initial plugin mechanism, with potential evolution to gRPC.\n> - **Rationale**: Docker containers provide the strongest isolation, versioning, and dependency packaging. They align with the existing execution model (we already run jobs in containers). This approach mirrors GitHub Actions' \"composite actions\" and \"Docker container actions\". gRPC adds unnecessary complexity for most use cases initially.\n> - **Consequences**: Plugin execution becomes launching a container instead of running a shell command. The plugin must follow conventions for input (environment variables) and output (files). Artifact collection becomes the primary communication channel between plugins.\n\n**How this extends the current design**:\n\nThe `StepConfig` would gain a new field `Action` alongside `Command`:\n\n```yaml\nsteps:\n  - name: Run tests\n    action: docker://mytestframework:1.0  # Special URI scheme\n    with:  # Parameters passed as environment variables\n      test_pattern: \"**/*_test.go\"\n```\n\nDuring job execution, `ExecuteJob` would:\n1. Detect if a step uses `action:` instead of `command:`\n2. Pull the specified Docker image (if not cached)\n3. Run the container with the specified environment variables (`with:` mapping)\n4. Capture logs from the container's stdout/stderr\n5. After completion, collect artifacts from a predefined output directory in the container\n\nThe existing container execution infrastructure (`ExecuteJob`) can handle this with minimal modification—instead of executing a shell command, it runs the container with the appropriate entrypoint.\n\nFor advanced use cases, the system could later add a `grpc://` action type that connects to a local gRPC service implementing a `StepPlugin` protocol, allowing long-running plugins that maintain state across steps.\n\n#### Deep Dive: Persistent Workflow Caches\n\n> **Decision: Content-Addressable Cache with S3 Backend**\n> - **Context**: Many build and test jobs download dependencies (npm packages, Go modules) repeatedly, wasting time and bandwidth. A shared cache between pipeline runs can dramatically speed up execution.\n> - **Options Considered**:\n>   1. **Local per-worker cache**: Each worker node maintains its own cache directory.\n>   2. **Network filesystem (NFS)**: Mount a shared drive to all workers.\n>   3. **Content-addressable object store (S3)**: Cache entries keyed by content hash, stored in S3-compatible storage.\n> - **Decision**: **Content-addressable object store with S3 backend** for the initial implementation.\n> - **Rationale**: Content addressing ensures cache integrity (same hash = same content). S3 provides durability, scalability, and works across distributed workers. It also allows cache pruning by age or size. The implementation can start with a local filesystem backend for simplicity, then switch to S3.\n> - **Consequences**: Introduces a new service dependency (object storage). Cache keys must be carefully designed to avoid collisions while maximizing hit rates. Requires integration into the job execution flow (restore before steps, save after).\n\n**Integration points in the current system**:\n\n1. **Cache Configuration**: Extend `JobConfig` with:\n   ```go\n   type CacheConfig struct {\n       Key     string   // Template string like \"node-modules-{{ hashFiles('package-lock.json') }}\"\n       Paths   []string // Directories to cache\n       When    string   // \"on_success\" or \"always\"\n   }\n   ```\n\n2. **Cache Service Interface**:\n   ```go\n   type CacheStore interface {\n       Restore(ctx context.Context, key string, destPath string) (bool, error) // returns hit/miss\n       Save(ctx context.Context, key string, srcPath string) error\n   }\n   ```\n\n3. **Job Execution Integration**: In `ExecuteJob`, after creating the container but before running steps:\n   - Generate cache keys by evaluating templates with job context\n   - Call `cache.Restore()` for each cache entry\n   - After successful job completion, call `cache.Save()` for each cache entry\n\nThe current artifact collection mechanism (`CollectArtifacts`) could be extended to also handle cache saving, since both involve copying files from the container to persistent storage.\n\n### Architectural Evolution Guidance\n\nWhen considering these enhancements, follow these principles to maintain system coherence:\n\n1. **Preserve Core Data Model**: The `PipelineRun`, `JobRun`, and `StepRun` structures should remain the central execution records. Enhance them with optional fields (e.g., `CacheHits []string`) rather than creating parallel data structures.\n\n2. **Extend Interfaces, Don't Break Them**: When adding new capabilities, extend existing interfaces with new methods rather than changing method signatures. Use optional parameters (struct options pattern) where possible.\n\n3. **Maintain Single-Node Simplicity**: The default configuration should remain a single-node system that runs everything locally. Distributed features should be opt-in via configuration.\n\n4. **Progressive Enhancement**: Start with the simplest implementation that works for the 80% use case. For example, a persistent cache could first be implemented as a local directory shared between jobs, then upgraded to S3.\n\nThe current architecture, with its clear separation between **orchestration**, **execution**, **queuing**, and **presentation**, provides natural boundaries for these extensions. Most enhancements will live within one component or at the interface between two components, minimizing ripple effects throughout the system.\n\n### Implementation Guidance\n\nWhile full implementation of these extensions is beyond the scope of the core milestones, here are starting points for the most impactful enhancements.\n\n**A. Technology Recommendations Table:**\n\n| Extension | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Additional Git Providers | Extend `ExtractEventDetails` with hardcoded mappings | Provider plugin system with dynamic registration |\n| Distributed Workers | Redis as queue backend with single consumer group | Kubernetes Custom Resource Definitions (CRDs) and operator pattern |\n| Plugin Ecosystem | Docker container actions with predefined entrypoints | gRPC plugin protocol with versioned SDKs |\n| Persistent Caches | Local directory shared via volume mounts | S3-compatible object storage with content-addressable keys |\n\n**B. Recommended File/Module Structure for Extensions:**\n\n```\nproject-root/\n  cmd/\n    orchestrator/          # Existing orchestrator binary\n    worker-node/           # New: distributed worker daemon\n      main.go\n    cache-service/         # New: optional cache service\n      main.go\n  \n  internal/\n    webhook/\n      providers/           # New: provider-specific parsers\n        github.go\n        gitlab.go\n        bitbucket.go       # New provider\n      verifier.go          # Extended with new methods\n    \n    queue/\n      redis_queue.go       # New: Redis implementation of Store interface\n      hybrid_queue.go      # Existing in-memory + SQLite queue\n    \n    cache/                 # New: cache abstraction\n      cache.go            # CacheStore interface\n      s3_cache.go         # S3 implementation\n      local_cache.go      # Local filesystem implementation\n    \n    plugin/               # New: plugin runtime\n      docker_action.go    # Docker container action executor\n      grpc_client.go      # gRPC plugin client (future)\n    \n    execution/\n      engine.go           # Extended to support cache restore/save\n      docker_client.go    # Extended to support plugin containers\n```\n\n**C. Infrastructure Starter Code for Redis Queue:**\n\n```go\n// internal/queue/redis_queue.go\npackage queue\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"time\"\n\n    \"github.com/go-redis/redis/v8\"\n    \"github.com/your-org/ci-system/internal/store\"\n)\n\ntype RedisQueue struct {\n    client *redis.Client\n    stream string\n    group  string\n}\n\nfunc NewRedisQueue(addr, password string, db int) (*RedisQueue, error) {\n    rdb := redis.NewClient(&redis.Options{\n        Addr:     addr,\n        Password: password,\n        DB:       db,\n    })\n    \n    // Test connection\n    ctx := context.Background()\n    if err := rdb.Ping(ctx).Err(); err != nil {\n        return nil, fmt.Errorf(\"failed to connect to Redis: %w\", err)\n    }\n    \n    // Create consumer group if it doesn't exist\n    err := rdb.XGroupCreateMkStream(ctx, \"jobs\", \"ci-workers\", \"0\").Err()\n    if err != nil && err.Error() != \"BUSYGROUP Consumer Group name already exists\" {\n        return nil, fmt.Errorf(\"failed to create consumer group: %w\", err)\n    }\n    \n    return &RedisQueue{\n        client: rdb,\n        stream: \"jobs\",\n        group:  \"ci-workers\",\n    }, nil\n}\n\nfunc (rq *RedisQueue) EnqueueJobRun(ctx context.Context, job *store.JobRun) error {\n    // Convert job to JSON\n    data, err := json.Marshal(job)\n    if err != nil {\n        return fmt.Errorf(\"failed to marshal job: %w\", err)\n    }\n    \n    // Add to stream with priority as score in sorted set\n    // Use Redis Streams for ordered consumption with consumer groups\n    _, err = rq.client.XAdd(ctx, &redis.XAddArgs{\n        Stream: rq.stream,\n        Values: map[string]interface{}{\n            \"job\":      string(data),\n            \"priority\": job.Priority, // Assume Priority field added to JobRun\n            \"time\":     time.Now().UnixNano(),\n        },\n    }).Result()\n    \n    return err\n}\n\n// TODO 1: Implement DequeueJobRun using XReadGroup for consumer group pattern\n// TODO 2: Implement MarkJobRunComplete using XACK to acknowledge processing\n// TODO 3: Implement recovery of pending jobs on startup using XPENDING\n// TODO 4: Add retry logic for failed jobs (move to dead letter stream after N attempts)\n```\n\n**D. Core Logic Skeleton for Cache Integration:**\n\n```go\n// internal/execution/engine.go (extensions)\nfunc (e *Engine) ExecuteJob(ctx context.Context, job store.JobRun, env map[string]string) (store.JobRun, error) {\n    // ... existing setup code (create container, etc.)\n    \n    // NEW: Restore caches before executing steps\n    for _, cache := range job.Config.Caches {\n        // TODO 1: Generate cache key by evaluating template with job context\n        //   Example: Replace {{ hashFiles('package-lock.json') }} with actual hash\n        // TODO 2: Call cacheStore.Restore(ctx, key, destPathInContainer)\n        // TODO 3: If cache hit, record in job metadata for dashboard display\n        // TODO 4: If cache miss, continue (will save after job)\n    }\n    \n    // Execute steps (existing logic)\n    for _, step := range job.Config.Steps {\n        // ... step execution\n    }\n    \n    // NEW: Save caches after successful execution\n    if job.Status == STATUS_SUCCEEDED {\n        for _, cache := range job.Config.Caches {\n            // TODO 5: Skip if cache.When == \"on_failure\" and job succeeded\n            // TODO 6: Call cacheStore.Save(ctx, key, srcPathInContainer)\n            // TODO 7: Handle cache save errors (log but don't fail job)\n        }\n    }\n    \n    // ... existing cleanup code\n    return job, nil\n}\n```\n\n**E. Language-Specific Hints for Extensions:**\n\n- **Redis Integration**: Use `github.com/go-redis/redis/v8` for Go. Remember to handle connection pooling and timeouts. Use Redis Streams for ordered job processing with consumer groups.\n- **S3 Cache Storage**: Use `github.com/aws/aws-sdk-go-v2/service/s3` for Go. Implement multipart uploads for large caches. Set appropriate cache-control headers for expiration.\n- **Docker Plugin Actions**: Use the existing Docker client (`github.com/docker/docker/client`) but instead of `ExecCreate` for shell commands, use `ContainerCreate` with the plugin image and appropriate entrypoint.\n- **gRPC Plugins**: Use `google.golang.org/grpc` to define a `StepPlugin` service protocol buffer. Generate server and client code. Plugins can be compiled as binaries that the orchestrator launches and connects to via local socket.\n\n**F. Milestone Checkpoint for Distributed Workers:**\n\nAfter implementing the Redis queue and worker node:\n1. **Start Redis**: `docker run -p 6379:6379 redis`\n2. **Start Orchestrator**: Configure it to use Redis queue instead of hybrid queue\n3. **Start Worker Node**: `go run cmd/worker-node/main.go --redis-addr localhost:6379`\n4. **Trigger a Pipeline**: Send a webhook or manual trigger\n5. **Verify**: Check Redis with `redis-cli XINFO GROUPS jobs` to see pending jobs. Worker logs should show job execution. Dashboard should show job assigned to worker node ID.\n\n**Expected Behavior**: Jobs are distributed to available workers. Multiple workers can run jobs concurrently. If a worker dies, its unacknowledged jobs are reassigned to other workers after timeout.\n\n**G. Debugging Tips for Extensions:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Cache hits are always 0%** | Cache key generation doesn't match between runs | Compare generated cache keys in job logs. Check template variable resolution. | Ensure hash functions use same input files. Standardize key generation algorithm. |\n| **Distributed workers pick up jobs but don't execute** | Container runtime not accessible on worker node | Check worker logs for Docker API errors. Run `docker ps` on worker node. | Ensure Docker socket is mounted/accessible. Configure DOCKER_HOST environment variable. |\n| **Plugin containers exit immediately with code 0** | Entrypoint expects different arguments | Inspect plugin container logs. Check plugin documentation for required env vars. | Pass required environment variables from `step.with` mapping. |\n| **Redis queue grows indefinitely** | Workers not acknowledging completed jobs | Check `XPENDING` count in Redis. Look for missing `XACK` calls in worker code. | Ensure `MarkJobRunComplete` calls `XACK`. Add dead-letter stream for failed jobs after retries. |\n| **Manual trigger API returns 400** | Missing required parameters | Check API endpoint validation logic. Review request payload structure. | Ensure all required fields (repo, branch, commit) are provided. Add better validation error messages. |\n\n\n## Glossary\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3, Milestone 4\n\nThis glossary provides definitive explanations of key terms, acronyms, and domain-specific vocabulary used throughout this design document. Understanding this terminology is essential for building a shared mental model of the CI/CD Pipeline Orchestrator system. The terms are organized alphabetically, with each entry providing a clear definition and, where helpful, an illustrative example or connection to system components.\n\n### Terminology Reference\n\n| Term | Definition | Example / Notes |\n|------|------------|-----------------|\n| **Artifact** | A file or directory produced by a job that is saved for later use, such as compiled binaries, test reports, or deployment packages. Artifacts are specified in the pipeline configuration with glob patterns and uploaded to persistent storage after job completion. | A job might produce artifacts: `dist/*.zip` and `test-results.xml`. These are stored and can be downloaded from the dashboard. |\n| **Atomic Operation** | An operation that completes entirely or not at all, crucial for maintaining data consistency during concurrent access. In our system, this applies to queue operations (like `DequeueJobRun`) and database updates to prevent race conditions. | Marking a job as `STATUS_RUNNING` and assigning it to a worker must be atomic; otherwise, two workers might execute the same job. |\n| **Backpressure** | Resistance or force opposing data flow when a consumer is slower than a producer. In log streaming, if the browser cannot keep up with log events, the server must apply backpressure to avoid overwhelming memory or network buffers. | The `LogBroadcaster` drops old log lines for slow clients to prevent unbounded memory growth, a form of backpressure. |\n| **CI/CD** | **Continuous Integration and Continuous Delivery/Deployment**. A set of practices and tools that automate the process of integrating code changes, running tests, and delivering/deploying software. Our system focuses on the CI aspect (integration and testing). | The entire purpose of the Pipeline Orchestrator is to enable CI/CD by automating pipeline execution on code changes. |\n| **Consumer Group** | A Redis Streams feature for distributing messages among multiple consumers (workers) in a load-balanced fashion, ensuring each message is processed by exactly one consumer in the group. | The `RedisQueue` can use consumer groups to allow multiple workers to share the load of processing jobs from a single stream. |\n| **Content-Addressable Cache** | A cache where keys are derived from a cryptographic hash (e.g., SHA-256) of the content being stored. This ensures identical content generates the same key, enabling efficient deduplication and verification. | Used in persistent workflow caches: the key for `node_modules` might be a hash of the `package-lock.json` file. |\n| **Correlation IDs** | Unique identifiers included in log messages and event payloads to trace a single request or pipeline run across multiple system components. Essential for debugging distributed workflows. | A `PipelineRunID` is used as a correlation ID, appearing in logs from the webhook handler, queue, worker, and dashboard. |\n| **CORS (Cross-Origin Resource Sharing)** | A mechanism that uses additional HTTP headers to tell a browser to allow a web application running at one origin (domain) to access selected resources from a server at a different origin. | The dashboard API endpoints may need CORS headers if the frontend is served from a different domain than the backend. |\n| **DAG (Directed Acyclic Graph)** | A graph structure consisting of nodes (jobs or stages) connected by directed edges (dependencies), with no cycles. Used to model the execution order and parallelism within a pipeline. | The `BuildExecutionGraph` function constructs a DAG from job `needs` dependencies. ![Pipeline Configuration Parsing and DAG Construction](./diagrams/diag-parser-flowchart.svg) |\n| **DAG Visualization** | A graphical representation of a Directed Acyclic Graph, showing nodes (jobs/stages) and arrows (dependencies). The dashboard component generates this to help developers understand pipeline structure. | The `GetDAG` endpoint returns data for rendering a DAG visualization of a pipeline run's jobs and their dependencies. |\n| **Distributed Work Queue** | A queue implementation using an external data store like Redis or a database that allows multiple worker processes across different machines to coordinate job processing. Enables horizontal scaling. | The `RedisQueue` is a distributed work queue alternative to the in-memory `HybridQueue`. |\n| **Matrix Build** | A feature that runs multiple variations of a job by computing the cartesian product of defined axis values (e.g., operating systems and Python versions). Creates parallel job instances for comprehensive testing. | A job with matrix: `{os: [ubuntu-latest, windows-latest], python: ['3.9', '3.10']}` expands to 4 separate `JobRun` instances. |\n| **Persistent Failures** | Failures that are not expected to resolve themselves if retried immediately (e.g., a syntax error in the code). Require different handling (e.g., immediate failure) than transient failures. | A job step with `exit code 1` (compilation error) is a persistent failure; retrying won't help. |\n| **Persistent Workflow Caches** | Shared cache storage across pipeline runs to speed up execution by reusing expensive-to-compute data (e.g., dependency downloads). Implemented with a `CacheStore` interface. | A `CacheConfig` might define a cache for `node_modules` keyed by `package-lock.json` hash, restored on subsequent runs. |\n| **Pipeline** | The entire automated process defined to build, test, and deploy code. Represented by a `PipelineConfig` (blueprint) and instantiated as a `PipelineRun` (execution instance). | A pipeline might consist of stages: \"lint\", \"test\", \"build\", \"deploy\". |\n| **Plugin Ecosystem** | A system for extending pipeline steps with reusable, packaged actions (plugins) that can be invoked in pipeline YAML. Allows community contributions and complex functionality beyond shell commands. | A future extension could allow steps like `uses: actions/checkout@v3` similar to GitHub Actions. |\n| **Priority Scheduling** | A method of ordering jobs in the queue based on importance, allowing critical pipelines (e.g., production deploys) to be processed before lower-priority jobs (e.g., feature branch tests). | The `PriorityQueue` orders `Item` structs by `Priority` field (higher number = higher priority). |\n| **Queue** | A data structure that holds pending jobs for execution in a specific order (FIFO or priority). The system uses a queue (like `HybridQueue` or `RedisQueue`) to decouple webhook reception from job execution. | The `EnqueueRun` and `DequeueJob` functions manage the flow of jobs through the queue. |\n| **S3 Backend** | Amazon S3 or compatible object storage service, often used for storing build artifacts or persistent workflow caches due to its durability, scalability, and cost-effectiveness. | An implementation of `CacheStore` or artifact storage might use an S3 backend. |\n| **Server-Sent Events (SSE)** | A unidirectional server-to-client streaming technology over standard HTTP, where the server can push text events to the browser. Used for real-time log streaming in the dashboard. | The `SSEHandler` sets up an HTTP connection that streams `LogEvent` data as server-sent events. |\n| **Stage** | A logical grouping of jobs within a pipeline (e.g., 'test', 'build'). Stages often run sequentially, with all jobs in a stage completing before the next stage begins. Represented implicitly via job dependencies in our system. | A pipeline might have stages: \"Build\" (jobs: compile, docker-build), then \"Test\" (jobs: unit, integration). |\n| **Step** | An individual shell command or action within a job. A job consists of a sequence of steps executed in order. Represented by `StepConfig` (definition) and `StepRun` (execution record). | A step: `- name: Run tests   command: go test ./...` |\n| **Structured Logging** | Logging with machine-parsable key-value pairs (e.g., JSON) instead of plain text, enabling aggregation, filtering, and analysis by log management systems. Our `LogEntry` struct supports structured logging. | A log entry: `{\"timestamp\": \"...\", \"level\": \"ERROR\", \"component\": \"worker\", \"message\": \"container start failed\", \"error\": \"no such image\"}` |\n| **Transient Failures** | Failures that often resolve themselves and warrant retries (e.g., network timeouts, temporary resource unavailability). The system uses `RetryWithBackoff` for such failures. | A Docker daemon connection timeout is transient; retrying after a delay may succeed. |\n| **Webhook** | An HTTP callback triggered by an event in a Git repository (e.g., push, pull request). Our webhook listener receives these POST requests, validates signatures, and triggers pipeline runs. | GitHub sends a webhook payload with `event: push` to our `/webhook/github` endpoint. |\n| **Worker** | A process that executes jobs by pulling them from the queue. Multiple workers can run concurrently, limited by a configurable pool size. Each worker calls `DequeueJob` and `ExecuteJob`. | The worker pool consists of 5 worker goroutines, each processing `JobRun` items from the `HybridQueue`. |\n\n---\n"}