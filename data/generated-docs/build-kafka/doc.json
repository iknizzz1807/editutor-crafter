{"html":"<h1 id=\"build-your-own-kafka-design-document\">Build Your Own Kafka: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This document describes the design and implementation of a distributed message queue supporting partitioned topics, consumer groups, and replication. The key architectural challenge is maintaining ordered, durable message delivery across a scalable and fault-tolerant cluster of brokers, while navigating the inherent trade-offs between latency, throughput, and consistency.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<blockquote>\n<p><strong>Milestone(s):</strong> All (foundational context)</p>\n</blockquote>\n<h2 id=\"1-context-and-problem-statement\">1. Context and Problem Statement</h2>\n<p>At the heart of modern software architecture lies a fundamental tension: how do we reliably and efficiently move streams of events—user actions, sensor readings, database updates, payment transactions—between dozens or hundreds of independent services? This is the <strong>scalable event streaming problem</strong>. Traditional request/response communication creates a tightly coupled web of dependencies, where the failure or slowness of one service can cascade through the entire system. Message queues emerged to decouple services by introducing an asynchronous buffer, but early designs often traded off one critical property for another: throughput for ordering, durability for latency, or simplicity for scale.</p>\n<p>Apache Kafka, introduced by LinkedIn in 2011, revolutionized this space by adopting a surprisingly simple core abstraction: the <strong>immutable, partitioned, append-only log</strong>. Instead of treating messages as transient envelopes to be delivered and deleted, Kafka treats them as permanent records in a ledger. This design shifts the focus from complex routing and delivery semantics to durable storage and high-speed sequential I/O. This document outlines the design and implementation of a simplified, educational version of such a system—&quot;Build Your Own Kafka.&quot; We will explore how to construct a distributed message queue that provides strong ordering guarantees, horizontal scalability, and fault tolerance through replication, while navigating the inherent trade-offs between latency, throughput, and consistency.</p>\n<h3 id=\"11-the-centralized-ledger-analogy\">1.1 The Centralized Ledger Analogy</h3>\n<p>To build intuition, imagine a traditional bank&#39;s <strong>centralized ledger</strong>. This ledger records every transaction—deposits, withdrawals, transfers—in strict chronological order. Each account&#39;s balance is not a stored number but a derived state computed by replaying all transactions for that account from the beginning of time. This model provides an <strong>authoritative, immutable history</strong> of all activity.</p>\n<p>Now, scale this bank to serve millions of customers. A single ledger book becomes a bottleneck; clerks cannot write fast enough, and customers wait in long queues to record their transactions. The bank&#39;s solution is <strong>partitioning</strong>: they create multiple identical ledger books, each responsible for a specific subset of accounts (e.g., accounts A-L in Ledger 1, M-Z in Ledger 2). This is the core mental model for a Kafka <strong>topic</strong>:</p>\n<ul>\n<li><strong>Topic</strong>: The overall stream of related events (e.g., <code>bank-transactions</code>). Analogous to the <em>concept</em> of the transaction log.</li>\n<li><strong>Partition</strong>: A single, ordered, immutable sequence of records within a topic. Analogous to one physical ledger book. Ordering is guaranteed <em>only within a single partition</em>.</li>\n<li><strong>Record</strong>: A single entry in the ledger (e.g., &quot;Account #123: Deposit $100 at 14:30&quot;). It consists of a key, a value, and a timestamp.</li>\n<li><strong>Offset</strong>: The sequential, monotonically increasing index number assigned to each record as it is appended to its partition (e.g., record #0, #1, #2...). This is the page and line number in the ledger book.</li>\n</ul>\n<p>The critical insight of the key is its role in <strong>deterministic partitioning</strong>. When a transaction (record) arrives, its account number (key) is hashed to consistently route it to the same ledger (partition). All transactions for account #123 always go to the same partition, preserving their chronological order. If the key is null, the record is assigned to partitions in a round-robin fashion for load distribution.</p>\n<blockquote>\n<p><strong>Design Insight:</strong> The shift from a &quot;message queue&quot; to a &quot;log&quot; is profound. A queue is about delivery and removal; a log is about durable recording and subscription. Consumers read from the log at their own pace, and the log persists independently of them. This enables replayability, auditing, and the ability to spawn new consumers for historical data analysis—patterns that are cumbersome or impossible with traditional queues.</p>\n</blockquote>\n<p>This ledger analogy extends to the cluster:</p>\n<ul>\n<li><strong>Broker</strong>: A bank branch office that safely stores a set of ledger books (partitions). A cluster consists of multiple brokers for redundancy and capacity.</li>\n<li><strong>Producer</strong>: A teller or ATM that accepts new transactions and writes them into the correct ledger book.</li>\n<li><strong>Consumer</strong>: An auditor or accountant who reads sequentially from one or more ledger books to compute balances or detect fraud.</li>\n<li><strong>Consumer Group</strong>: A team of auditors, where each member is assigned a non-overlapping subset of the ledger books to parallelize the audit work.</li>\n</ul>\n<h3 id=\"12-the-scalable-event-streaming-problem\">1.2 The Scalable Event Streaming Problem</h3>\n<p>Building a system that realizes this ledger analogy at internet scale requires satisfying a interconnected set of core requirements. These requirements often exist in tension, and the design choices we make will prioritize certain qualities over others.</p>\n<p><strong>Core Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Requirement</th>\n<th align=\"left\">Description</th>\n<th align=\"left\">Implication for Design</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>High-Throughput &amp; Low Latency</strong></td>\n<td align=\"left\">The system must ingest and deliver hundreds of thousands—or millions—of events per second with minimal delay.</td>\n<td align=\"left\">Demands efficient serialization, batching, and heavy reliance on sequential disk I/O over random access. Network and disk overhead must be minimized.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Ordered Delivery</strong></td>\n<td align=\"left\">Events with a causal relationship (e.g., updates to the same entity) must be processed in the order they occurred.</td>\n<td align=\"left\">Requires a strong ordering guarantee <em>within a partition</em>. The system must ensure all records for a given key go to the same partition and are appended in arrival order.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Durability &amp; Fault Tolerance</strong></td>\n<td align=\"left\">Once acknowledged, messages must not be lost, even in the face of individual server failures, power outages, or network partitions.</td>\n<td align=\"left\">Necessitates replication of data across multiple brokers (servers) and durable writes to disk (fsync). The system must define what &quot;acknowledged&quot; means (e.g., written to leader? written to all replicas?).</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Scalability &amp; Elasticity</strong></td>\n<td align=\"left\">The system&#39;s capacity should increase linearly by adding more brokers. It should also allow consumers and producers to scale independently.</td>\n<td align=\"left\">Requires stateless producers/consumers, data partitioning, and dynamic reassignment of partition ownership. No single component (like a central router) should become a bottleneck.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Decoupling of Producers &amp; Consumers</strong></td>\n<td align=\"left\">Producers should not need to know about consumers, and vice versa. Consumers should be able to process data at their own pace.</td>\n<td align=\"left\">Implies a publish-subscribe model with persistent storage. The log acts as a buffer, allowing producers to write as fast as they can and consumers to read when they are ready.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Consumer Group Semantics</strong></td>\n<td align=\"left\">Multiple consumers can work together to process a topic, with each partition being consumed by exactly one group member at a time. This provides parallel processing and fault tolerance for the consumption side.</td>\n<td align=\"left\">Requires a coordination service to manage group membership, assign partitions, and track progress (offsets).</td>\n</tr>\n</tbody></table>\n<p>The primary challenge is that these requirements conflict. For example:</p>\n<ul>\n<li><strong>Durability vs. Latency:</strong> Ensuring a message is written to disk on multiple replicas (high durability) increases latency compared to acknowledging once the leader receives it.</li>\n<li><strong>Ordering vs. Throughput:</strong> Maintaining strict global order requires a single sequencing point (a bottleneck). Partitioning sacrifices global order for parallelism and throughput.</li>\n<li><strong>Scalability vs. Simplicity:</strong> A simple, single-broker design is easy to reason about but doesn&#39;t scale. A distributed design introduces complexity around consistency, failure detection, and recovery.</li>\n</ul>\n<p><strong>The Log-Based Solution:</strong>\nThe partitioned log model directly addresses these tensions:</p>\n<ol>\n<li><strong>Throughput &amp; Scalability:</strong> Partitioning allows parallel ingestion and consumption across many brokers and disks. Sequential appends maximize disk I/O efficiency.</li>\n<li><strong>Ordered Delivery:</strong> Order is guaranteed per-partition, which is sufficient for most use cases when keys are chosen wisely (e.g., <code>user_id</code>).</li>\n<li><strong>Durability:</strong> The log is an immutable, append-only file. Replication provides durability against node failure, while periodic fsync provides durability against process crashes.</li>\n<li><strong>Decoupling:</strong> The log is the persistent interface. Producers append; consumers subscribe to positions (offsets) in the log.</li>\n</ol>\n<blockquote>\n<p><strong>Architecture Decision Record (ADR): Log as the Primary Abstraction</strong></p>\n<ul>\n<li><strong>Context:</strong> We need a data structure that serves as the durable, ordered backbone for event streaming, balancing throughput, ordering, and simplicity.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Traditional Message Queue (e.g., AMQP model):</strong> Messages are stored in in-memory or disk-backed queues, deleted upon acknowledgment. Supports complex routing (exchanges, bindings).</li>\n<li><strong>Distributed Commit Log:</strong> Messages are immutable records appended to partitioned, replicated log segments. Simple append/read semantics.</li>\n<li><strong>Event Sourcing Database:</strong> Treats the log as a system of record, with built-in query capabilities and state derivation.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use a <strong>Distributed Commit Log</strong> as the primary abstraction.</li>\n<li><strong>Rationale:</strong> The log model provides the optimal balance for our core requirements. Its simplicity (append/read) enables extremely high throughput. Immutability simplifies replication and failure recovery. Partitioning provides horizontal scalability. The retention-based cleanup model is simpler than per-message deletion and enables valuable replay capabilities.</li>\n<li><strong>Consequences:</strong><ul>\n<li><strong>Enables:</strong> High throughput, simple replication, replayability, and natural support for multiple consumers.</li>\n<li><strong>Trade-offs:</strong> Requires consumers to manage their own position (offset). Storage management shifts from message TTL to log segment retention/compaction. No built-in complex routing; filtering must happen on the consumer side.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Option</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Why Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Traditional Message Queue</strong></td>\n<td align=\"left\">Rich routing semantics, immediate deletion saves space, mature protocols (AMQP).</td>\n<td align=\"left\">Deletion complicates replication and replay, often relies on slower random I/O, complex to scale horizontally.</td>\n<td align=\"left\">Better for complex routing needs, but our primary goals are throughput and scalability.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Distributed Commit Log</strong></td>\n<td align=\"left\">Maximizes sequential I/O, simple replication, natural replay, excellent horizontal scalability.</td>\n<td align=\"left\">Consumers must manage offset, no built-in message routing, storage grows until retention/compaction.</td>\n<td align=\"left\">Directly aligns with high-throughput, ordered, durable event streaming. Simplicity is a feature.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Event Sourcing Database</strong></td>\n<td align=\"left\">Powerful query abilities, strong consistency models, integrated state derivation.</td>\n<td align=\"left\">Higher overhead per message, more complex operational model, often lower write throughput.</td>\n<td align=\"left\">Overkill for a messaging backbone; we want a transport layer, not a system of record.</td>\n</tr>\n</tbody></table>\n<h3 id=\"13-existing-approaches-amp-trade-offs\">1.3 Existing Approaches &amp; Trade-offs</h3>\n<p>Kafka&#39;s log-centric design did not emerge in a vacuum. It sits within a landscape of messaging and streaming systems, each making different architectural trade-offs. Understanding these alternatives clarifies <em>why</em> Kafka&#39;s choices are particularly well-suited for high-volume event streaming.</p>\n<p><strong>1. Traditional Enterprise Message Brokers (e.g., RabbitMQ, ActiveMQ):</strong>\nThese systems are built around the <strong>smart broker/dumb consumer</strong> model. The broker is responsible for complex message routing, delivery guarantees, and transaction management.</p>\n<ul>\n<li><strong>Typical Architecture:</strong> Centralized or clustered brokers hold messages in queues. They support advanced patterns like publish/subscribe, request/reply, and point-to-point via exchanges, bindings, and queues.</li>\n<li><strong>Trade-offs:</strong><ul>\n<li><strong>Pros:</strong> Rich feature set (priority, TTL, dead-letter queues), strong delivery guarantees with transactions, flexible routing.</li>\n<li><strong>Cons:</strong> The broker is a complex, stateful monolith that can become a performance bottleneck. Scaling often involves partitioning (federation/shovel) which is complex. Persistence models (often B-trees) can be slower than sequential logs. Consumers are generally stateful connections to the broker.</li>\n</ul>\n</li>\n<li><strong>Comparison to Our Design:</strong> Our system adopts a <strong>dumb broker/smart consumer</strong> model. The broker&#39;s job is simple: durably store logs and serve reads/writes. All intelligence (partition assignment, offset management, consumer state) is pushed to the client libraries. This shifts complexity but allows the broker to be simpler, more robust, and easier to scale horizontally.</li>\n</ul>\n<p><strong>2. Distributed Log Systems (e.g., Apache BookKeeper, NATS Streaming):</strong>\nThese share Kafka&#39;s log-oriented philosophy but differ in implementation details.</p>\n<ul>\n<li><strong>Apache BookKeeper:</strong> Provides a reliable storage service for log segments. Kafka historically used BookKeeper for tiered storage. It offers strong durability guarantees via a quorum-write protocol.</li>\n<li><strong>NATS Streaming:</strong> Provides a similar log abstraction but often with simpler clustering models and a focus on the Go ecosystem.</li>\n<li><strong>Trade-offs:</strong> BookKeeper separates storage and serving, which can offer flexibility. NATS Streaming offers simplicity and integration with the NATS core. Kafka&#39;s integrated design (managing its own storage) provides end-to-end control over performance and semantics.</li>\n</ul>\n<p><strong>3. Streaming Platforms &amp; Databases (e.g., Apache Pulsar, AWS Kinesis, Apache Flink):</strong>\nThese systems extend the core log model with additional capabilities.</p>\n<ul>\n<li><strong>Apache Pulsar:</strong> Uses a similar log abstraction but with a clearer separation of compute (brokers) and storage (Apache BookKeeper). Supports both queuing and streaming semantics natively.</li>\n<li><strong>AWS Kinesis:</strong> A managed service offering a Kafka-like shard (partition) model, with tight integration into the AWS ecosystem. Often has stricter limits on retention and throughput per shard.</li>\n<li><strong>Apache Flink:</strong> A true stream processing engine that can also act as a persistent event store with its Stateful Functions API, blurring the line between transport and processing.</li>\n<li><strong>Trade-offs:</strong> Pulsar&#39;s separation can aid in independent scaling and operational flexibility. Kinesis offers simplicity as a service. Flink integrates processing and storage. Our educational design follows Kafka&#39;s more monolithic broker model for simplicity of understanding, acknowledging that separation of concerns is a valid advanced architectural pattern.</li>\n</ul>\n<p><strong>4. General-Purpose Distributed Databases (e.g., etcd, CockroachDB):</strong>\nThese can be (ab)used as message queues by treating a table as a queue.</p>\n<ul>\n<li><strong>Trade-offs:</strong><ul>\n<li><strong>Pros:</strong> Strong consistency, transactions, and rich querying.</li>\n<li><strong>Cons:</strong> Write amplification, lower throughput for streaming workloads, and inefficient tail reads. They are optimized for random access and point queries, not sequential appends and reads.</li>\n</ul>\n</li>\n<li><strong>Key Distinction:</strong> Databases are optimized for <strong>point-in-time queries</strong> over a dataset. Logs are optimized for <strong>continuous ingestion and sequential reads</strong> of an unbounded stream. The access patterns are fundamentally different.</li>\n</ul>\n<p>The following table summarizes the key architectural trade-offs:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">System Category</th>\n<th align=\"left\">Primary Data Model</th>\n<th align=\"left\">Scaling Model</th>\n<th align=\"left\">Durability Mechanism</th>\n<th align=\"left\">Typical Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Traditional Broker (RabbitMQ)</strong></td>\n<td align=\"left\">Queue/Exchange</td>\n<td align=\"left\">Vertical scaling or complex federation</td>\n<td align=\"left\">Persistent messages in broker (often B-tree)</td>\n<td align=\"left\">Enterprise application integration, complex routing.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Our Design / Kafka</strong></td>\n<td align=\"left\">Partitioned, Append-Only Log</td>\n<td align=\"left\">Horizontal partitioning across brokers</td>\n<td align=\"left\">Replicated log segments, sequential disk I/O</td>\n<td align=\"left\">High-throughput event streaming, log aggregation, activity tracking.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Pulsar</strong></td>\n<td align=\"left\">Segmented Log (with separate storage)</td>\n<td align=\"left\">Independent scaling of brokers &amp; storage</td>\n<td align=\"left\">BookKeeper ledger replication</td>\n<td align=\"left\">Streaming and queuing with cloud-native operations.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Database-as-Queue</strong></td>\n<td align=\"left\">Table/Key-Value</td>\n<td align=\"left\">Horizontal sharding (for distributed DBs)</td>\n<td align=\"left\">Replicated write-ahead log (WAL)</td>\n<td align=\"left\">When strong consistency and queueing are both required (generally an anti-pattern for high-volume streams).</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>The Central Trade-Off: Simplicity vs. Control.</strong> Our educational design, following Kafka&#39;s early architecture, chooses to keep storage management within the broker. This creates a more monolithic but simpler-to-understand component. In production systems at scale, the trend is toward disaggregating storage (as seen in Pulsar/Kafka Tiered Storage) to improve elasticity and cost efficiency. However, for learning the fundamentals of replication, partitioning, and consensus, the integrated model provides a more coherent mental model.</p>\n</blockquote>\n<p>By grounding our design in the immutable log analogy and understanding the landscape of alternatives, we establish a clear foundation. The subsequent sections will delve into how to concretely build a system that embodies these principles, starting with the fundamental unit of storage: the topic and its partitions.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section is purely conceptual and foundational; there is no code to implement. However, it is critical to internalize the mental models and trade-offs discussed here before writing any code. A strong conceptual understanding will prevent fundamental architectural missteps.</p>\n<p><strong>Next Steps for the Learner:</strong></p>\n<ol>\n<li><strong>Internalize the Ledger Analogy:</strong> Sketch out how a topic named <code>page-views</code> with 3 partitions would store records for <code>user_id</code> keys 1, 2, and 3. Determine which partition each record would land on using a simple hash modulo partition count strategy.</li>\n<li><strong>Contrast with a Queue:</strong> Write down two key differences in behavior between a traditional queue (message deleted after acknowledgment) and a log (message retained until retention expires) from the perspective of a consumer that crashes and restarts.</li>\n<li><strong>Anticipate Trade-offs:</strong> Before starting Milestone 1, consider: If you prioritize very low latency for producers, what durability setting (<code>acks</code>) might you choose, and what is the potential risk?</li>\n</ol>\n<p>Proceed to <strong>Section 5: Component Design: Broker and Topic Partitions</strong> to begin the implementation of the ledger (log) itself.</p>\n<h2 id=\"2-goals-and-non-goals\">2. Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All (foundational requirements)</p>\n</blockquote>\n<p>This section establishes the bounded scope of our educational system. In any engineering endeavor, particularly when learning complex distributed systems, explicitly defining what we <strong>will</strong> and <strong>will not</strong> build is crucial. It prevents scope creep, focuses effort on core learning objectives, and sets realistic expectations about the system&#39;s capabilities and limitations.</p>\n<p>The design philosophy for &quot;Build Your Own Kafka&quot; prioritizes <strong>educational clarity over feature completeness</strong> and <strong>conceptual implementation over production robustness</strong>. We aim to build a <em>functional prototype</em> that illustrates the architectural patterns and trade-offs of a distributed log, not a production-ready message broker.</p>\n<h3 id=\"21-functional-goals\">2.1 Functional Goals</h3>\n<p>Our system must implement the core abstractions and protocols that make a Kafka-like message queue work. Think of these as the <strong>minimum viable pillars</strong> that, when combined, deliver the fundamental value proposition: scalable, ordered, and durable event streaming.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Functional Area</th>\n<th align=\"left\">Required Capability</th>\n<th align=\"left\">Description &amp; Learning Objective</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Topic &amp; Partition Management</strong></td>\n<td align=\"left\">Create topics with configurable partitions</td>\n<td align=\"left\">Learn how logical data streams (topics) are physically sharded across partitions for horizontal scaling. Each partition is an independent, ordered sequence.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Consistent key-based routing</td>\n<td align=\"left\">Understand how message keys determine partition assignment, enabling per-key ordering guarantees while distributing load.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Sequential offset assignment</td>\n<td align=\"left\">Implement the immutable, append-only log abstraction where each message gets a unique, monotonic offset within its partition.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Producer</strong></td>\n<td align=\"left\">Configurable acknowledgment levels (0, 1, all)</td>\n<td align=\"left\">Experience the durability vs. latency trade-off firsthand by implementing fire-and-forget, leader-acknowledged, and fully-replicated write semantics.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Client-side batching</td>\n<td align=\"left\">Learn how accumulating messages into batches drastically improves network and disk I/O efficiency, a key technique for high-throughput systems.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Retry with backoff</td>\n<td align=\"left\">Handle transient failures (network timeouts, leader elections) gracefully without requiring application intervention.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Consumer Groups</strong></td>\n<td align=\"left\">Dynamic group membership</td>\n<td align=\"left\">Implement the &quot;team reading a book&quot; metaphor where consumers in a group coordinate to share the work of consuming a topic&#39;s partitions.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Partition assignment strategies (Range, RoundRobin)</td>\n<td align=\"left\">Explore different algorithms for distributing partitions among group members, balancing fairness and locality.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Offset commit &amp; fetch</td>\n<td align=\"left\">Provide at-least-once delivery semantics by allowing consumers to periodically save their read position, enabling resume-after-restart.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Group rebalancing</td>\n<td align=\"left\">Handle the complex distributed coordination problem of redistributing partitions when group membership changes (joins, leaves, failures).</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Replication</strong></td>\n<td align=\"left\">Leader-follower log replication</td>\n<td align=\"left\">Build fault tolerance by having followers asynchronously copy data from the partition leader, ensuring data survives single-broker failures.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">In-Sync Replica (ISR) management</td>\n<td align=\"left\">Maintain a dynamic set of &quot;caught-up&quot; followers and understand how this set defines durability guarantees (<code>acks=all</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">High Watermark advancement</td>\n<td align=\"left\">Implement the safety mechanism that prevents consumers from reading data that could be lost on leader failure.</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">Leader election (from ISR)</td>\n<td align=\"left\">Recover from leader failure by promoting a fully caught-up follower, ensuring no data loss during failover (when configured).</td>\n</tr>\n</tbody></table>\n<p>These goals map directly to the four project milestones. Successfully implementing them yields a system where:</p>\n<ol>\n<li>A <strong>producer</strong> can send a message with a key to a topic.</li>\n<li>The system <strong>hashes the key</strong> to select a specific partition for that topic.</li>\n<li>The <strong>partition leader</strong> appends the message to its local log, assigns an offset, and (based on <code>acks</code>) waits for replication to followers.</li>\n<li>A <strong>consumer group</strong> with multiple members subscribes to the topic.</li>\n<li>The <strong>group coordinator</strong> assigns each partition to exactly one group member.</li>\n<li>Each <strong>consumer</strong> fetches messages from its assigned partitions, processes them, and commits its offsets.</li>\n<li>If a <strong>broker fails</strong>, the system elects a new leader from the in-sync replicas for its hosted partitions, and consumer groups rebalance to reassign the affected partitions.</li>\n</ol>\n<h3 id=\"22-non-functional-goals\">2.2 Non-Functional Goals</h3>\n<p>Beyond the feature checklist, our system should exhibit certain qualitative properties. These are not explicit user-facing features but essential characteristics of a well-designed educational prototype.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Property</th>\n<th align=\"left\">Goal &amp; Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Durability (within reason)</strong></td>\n<td align=\"left\">Messages acknowledged with <code>acks=all</code> should survive a single broker failure without loss. This is achieved via on-disk storage (fsync configurable) and replication. We trade off absolute durability (e.g., synchronous disk writes for every message) for implementation simplicity and performance that illustrates the concept.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Scalability (demonstrable)</strong></td>\n<td align=\"left\">The architecture should allow horizontal scaling: adding more partitions to a topic should increase write throughput; adding more consumers to a group should increase read throughput. The implementation should not have central bottlenecks (like a global lock) that would prevent this in a multi-broker setup, even if our test cluster is small.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Fault Tolerance (basic)</strong></td>\n<td align=\"left\">The system must handle predictable failures gracefully: broker crashes trigger leader election and consumer rebalancing; network timeouts trigger producer retries. We aim for <em>clean failure modes</em> (clear error messages, no silent data corruption) over complex recovery from arbitrary faults (e.g., Byzantine failures).</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Understandability (paramount)</strong></td>\n<td align=\"left\">Code and design must prioritize clarity for learners. This means: <br>• Choosing simpler, more explicit algorithms over optimized but obscure ones.<br>• Using clear naming and modular separation of concerns.<br>• Providing extensive logging to visualize internal state (e.g., &quot;Leader for partition <code>orders-0</code> changed from broker 1 to broker 2&quot;).<br>• Avoiding premature optimization that obfuscates the core logic.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Operability (for learning)</strong></td>\n<td align=\"left\">The system should be easy to run and observe locally. This includes simple startup/shutdown procedures, exposed metrics/logs for debugging, and the ability to simulate failures (e.g., killing a broker process) to see recovery in action.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The primary non-functional goal is <strong>pedagogical value</strong>. Every design decision should be evaluated against the question: &quot;Does this make the core concepts easier or harder to understand?&quot; This often means choosing the more explicit, verbose, or modular approach over the most performant or idiomatic one.</p>\n</blockquote>\n<h3 id=\"23-explicit-non-goals\">2.3 Explicit Non-Goals</h3>\n<p>To maintain focus, we deliberately exclude several features that are important in production Kafka but are considered advanced or tangential to our core learning objectives. Stating these upfront prevents &quot;feature creep&quot; and clarifies the system&#39;s boundaries.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Area</th>\n<th align=\"left\">What We Are <strong>NOT</strong> Building</th>\n<th align=\"left\">Rationale &amp; Learning Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Protocol Compatibility</strong></td>\n<td align=\"left\">Full Apache Kafka wire protocol (binary) compatibility.</td>\n<td align=\"left\">Implementing the official protocol is complex and adds significant serialization/deserialization overhead. We will define a <strong>simplified, explicit protocol</strong> (e.g., JSON over HTTP or a trivial binary format) to keep the focus on system logic, not wire-format intricacies.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Transactional Messaging</strong></td>\n<td align=\"left\">Exactly-once semantics (EOS), idempotent producers, or cross-partition transactions.</td>\n<td align=\"left\">Transactions introduce significant complexity (transaction coordinator, write-ahead logs for transaction state, two-phase commit). They are a valuable <em>advanced topic</em> but would double the project scope. We focus on at-least-once delivery as the foundational model.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Storage Efficiency</strong></td>\n<td align=\"left\">Tiered storage (offloading to S3), log compaction, or sophisticated compression (Zstandard, LZ4).</td>\n<td align=\"left\">While critical for production cost and performance, these are optimizations built <em>on top of</em> the core log abstraction. Implementing them would distract from understanding the log itself. Our log segments will be simple on-disk files.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Production-Grade Security</strong></td>\n<td align=\"left\">Authentication (SASL), Authorization (ACLs), or Encryption (TLS).</td>\n<td align=\"left\">Security is vital but largely orthogonal to the distributed systems algorithms we&#39;re studying. Adding it would complicate every network call and configuration without deepening understanding of replication, consensus, or partitioning.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Advanced Cluster Management</strong></td>\n<td align=\"left\">Dynamic configuration, partition reassignment tools, rack-aware replica placement, or multi-region replication.</td>\n<td align=\"left\">These are operations and reliability features for managing large clusters. Our educational cluster will be static-configured and likely run on a single machine.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Monitoring &amp; Metrics</strong></td>\n<td align=\"left\">A comprehensive metrics subsystem (JMX, Prometheus endpoints) or detailed health checks.</td>\n<td align=\"left\">While we value observability for learning, we&#39;ll achieve it through structured logging rather than a full metrics pipeline to keep the codebase lean.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>High-Performance Networking</strong></td>\n<td align=\"left\">Custom RPC layer with zero-copy, kernel-bypass, or sophisticated connection pooling.</td>\n<td align=\"left\">We&#39;ll use the standard library&#39;s networking (<code>net/http</code> or <code>net</code>) to keep I/O models simple and understandable. Performance tuning is a separate, deep discipline.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Client Libraries</strong></td>\n<td align=\"left\">Idiomatic, feature-complete client libraries for multiple languages.</td>\n<td align=\"left\">We&#39;ll build a minimal client (producer/consumer) in Go for testing and demonstration. The focus is on the server-side broker logic.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>ADR Summary: Scope Limitation for Educational Focus</strong>\n<strong>Context:</strong> The project could easily expand to encompass many production features of Apache Kafka.\n<strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Minimal Core:</strong> Build only the four milestone features with simplified protocols and storage.</li>\n<li><strong>Extended Feature Set:</strong> Include 1-2 advanced features like idempotent producers or log compaction.</li>\n<li><strong>Protocol-First:</strong> Focus on implementing the full Kafka binary protocol with a minimal feature set.\n<strong>Decision:</strong> Adopt Option 1 (Minimal Core).\n<strong>Rationale:</strong> The primary objective is to learn distributed systems concepts—replication, partitioning, consensus, and fault tolerance—not to build a production system. Every additional feature dilutes focus, increases complexity, and extends the timeline, risking that learners never complete the core educational journey. A working minimal system that demonstrates the key architectural patterns is more valuable than a half-finished system with many partially implemented features.\n<strong>Consequences:</strong> The built system will be unsuitable for production use. Learners will need to understand its limitations. However, they will gain a crystal-clear understanding of the foundational mechanics, which is the best preparation for subsequently studying advanced features or contributing to real systems.</li>\n</ol>\n</blockquote>\n<p>Understanding these non-goals is as important as the goals themselves. It defines the &quot;finished line&quot; for the project and allows learners to celebrate a complete, coherent system that, while limited, fundamentally works and teaches the intended lessons.</p>\n<hr>\n<hr>\n<blockquote>\n<p><strong>Milestone(s):</strong> All (foundational architecture)</p>\n</blockquote>\n<h2 id=\"3-high-level-architecture\">3. High-Level Architecture</h2>\n<p>This section presents the architectural blueprint of our distributed message queue system. Before diving into individual components, we establish a <strong>mental model</strong> of the entire system as a <strong>global library system</strong>. Imagine a vast library (the cluster) with multiple branches (brokers). Authors (producers) submit new books (messages) to the library catalog (topics), where each book is placed on a specific shelf (partition). Reading clubs (consumer groups) visit the library, with each club member (consumer) assigned specific shelves to read from. A head librarian (group coordinator) tracks which club members are present and which shelves they&#39;re reading. This analogy helps ground the abstract distributed components in tangible, real-world concepts before we formalize their technical specifications.</p>\n<p><img src=\"/api/project/build-kafka/architecture-doc/asset?path=diagrams%2Fdiagram-system-component.svg\" alt=\"System Component Overview\"></p>\n<h3 id=\"31-component-overview-amp-responsibilities\">3.1 Component Overview &amp; Responsibilities</h3>\n<p>The system comprises four primary component types that work in concert to deliver durable, ordered message streaming. Each component has a distinct responsibility domain and maintains specific state.</p>\n<h4 id=\"311-core-components\">3.1.1 Core Components</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Key Data Held</th>\n<th>Key Operations Performed</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Broker</strong></td>\n<td>Stores partition replicas, handles client read/write requests, and replicates data to other brokers.</td>\n<td>- Local partition logs (<code>Partition.Log</code>)<br>- Topic metadata<br>- Replica state (leader/follower)<br>- Consumer offsets (cached)</td>\n<td>- Append records to partition logs<br>- Serve fetch requests to consumers<br>- Replicate data to follower brokers<br>- Elect partition leaders<br>- Track In-Sync Replicas (ISR)</td>\n</tr>\n<tr>\n<td><strong>Producer</strong></td>\n<td>Publishes records to topics, handling batching, reliability guarantees, and error recovery.</td>\n<td>- Unsent message batches per partition<br>- Metadata about cluster (topic/partition leaders)<br>- Sequence numbers (for idempotency)</td>\n<td>- Route messages to correct partitions<br>- Accumulate messages into batches<br>- Send batches to broker leaders<br>- Handle acknowledgments and retries</td>\n</tr>\n<tr>\n<td><strong>Consumer</strong></td>\n<td>Reads records from topics, maintains consumption position, and participates in consumer groups.</td>\n<td>- Assigned partitions and their current offsets<br>- Fetched but unprocessed records<br>- Group membership metadata</td>\n<td>- Subscribe to topics<br>- Fetch records from broker leaders<br>- Process records and advance offset<br>- Commit offsets to broker<br>- Participate in group rebalancing</td>\n</tr>\n<tr>\n<td><strong>Group Coordinator</strong></td>\n<td>Manages consumer group membership, partition assignment, and offset persistence. Typically co-located with a broker.</td>\n<td>- Group membership lists<br>- Partition assignments per group<br>- Committed offsets (durable storage)<br>- Generation IDs (for rebalancing)</td>\n<td>- Process join/leave requests<br>- Trigger and manage rebalancing<br>- Distribute partition assignments<br>- Store and serve committed offsets</td>\n</tr>\n</tbody></table>\n<h4 id=\"312-component-interactions\">3.1.2 Component Interactions</h4>\n<p>The components interact through well-defined request/response patterns over network connections:</p>\n<ol>\n<li><p><strong>Producer → Broker (Data Plane)</strong>: Producers send <code>Produce</code> requests to broker leaders for specific partitions. These requests contain batched messages that the broker appends to its local log.</p>\n</li>\n<li><p><strong>Consumer → Broker (Data Plane)</strong>: Consumers send <code>Fetch</code> requests to broker leaders to retrieve records from partitions, starting from a specific offset.</p>\n</li>\n<li><p><strong>Consumer ↔ Coordinator (Control Plane)</strong>: Consumers communicate with the group coordinator for membership management:</p>\n<ul>\n<li><code>JoinGroup</code>: Register with a consumer group</li>\n<li><code>SyncGroup</code>: Receive partition assignments</li>\n<li><code>Heartbeat</code>: Maintain active membership</li>\n<li><code>OffsetCommit</code>: Persist consumption progress</li>\n<li><code>OffsetFetch</code>: Retrieve last committed offset</li>\n</ul>\n</li>\n<li><p><strong>Broker ↔ Broker (Replication Plane)</strong>: Brokers replicate data among themselves:</p>\n<ul>\n<li>Followers fetch new records from partition leaders</li>\n<li>Leaders track follower replication progress (ISR)</li>\n<li>Brokers elect new leaders when current leaders fail</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p><strong>Key Insight</strong>: The separation between data plane (message flow) and control plane (coordination) is fundamental. The data plane must be optimized for high throughput and low latency, while the control plane prioritizes consistency and correctness, even at the cost of some latency.</p>\n</blockquote>\n<h4 id=\"313-deployment-topology\">3.1.3 Deployment Topology</h4>\n<p>In a typical deployment:</p>\n<ul>\n<li>Multiple <strong>brokers</strong> form a cluster, with each broker running on a separate physical or virtual machine.</li>\n<li><strong>Producers</strong> and <strong>consumers</strong> run as client libraries within application processes.</li>\n<li>The <strong>group coordinator</strong> is typically implemented as a special role assumed by one of the brokers (often the first broker or one elected via consensus).</li>\n<li>All components communicate over TCP/IP networks, with the system designed to tolerate network partitions and broker failures.</li>\n</ul>\n<h3 id=\"32-recommended-filemodule-structure\">3.2 Recommended File/Module Structure</h3>\n<p>For the Go implementation, we recommend a modular package structure that separates concerns, facilitates testing, and mirrors the architectural components. This structure balances simplicity for learners with good software engineering practices.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>build-your-own-kafka/\n├── cmd/                          # Application entry points\n│   ├── broker/                   # Broker server executable\n│   │   └── main.go               # Broker startup and configuration\n│   ├── producer-cli/             # CLI tool for testing producers\n│   │   └── main.go\n│   └── consumer-cli/             # CLI tool for testing consumers\n│       └── main.go\n├── internal/                     # Private application code (not for external use)\n│   ├── broker/                   # Broker core logic\n│   │   ├── server.go             # Main broker server struct and lifecycle\n│   │   ├── api_handler.go        # Request/response handling (Produce, Fetch, etc.)\n│   │   ├── log_manager.go        # Partition log management\n│   │   ├── replica_manager.go    # Leader/follower replication\n│   │   └── coordinator.go        # Group coordination (if embedded)\n│   ├── client/                   # Client libraries\n│   │   ├── producer/             # Producer client implementation\n│   │   │   ├── producer.go       # Public Producer interface\n│   │   │   ├── accumulator.go    # Batch accumulation logic\n│   │   │   ├── partitioner.go    # Partition selection strategies\n│   │   │   └── sender.go         # Network sender with retries\n│   │   └── consumer/             # Consumer client implementation\n│   │       ├── consumer.go       # Public Consumer interface\n│   │       ├── fetcher.go        # Record fetching logic\n│   │       ├── group_member.go   # Consumer group membership\n│   │       └── offset_tracker.go # Offset commit/fetch management\n│   ├── protocol/                 # Wire protocol definitions\n│   │   ├── messages.go           # Request/response structs (ProduceRequest, etc.)\n│   │   ├── serialization.go      # Binary encoding/decoding\n│   │   └── api_keys.go           # API key constants\n│   ├── storage/                  # Persistent storage layer\n│   │   ├── log_segment.go        # Log segment with index file\n│   │   ├── wal.go                # Write-ahead log abstraction\n│   │   └── offset_store.go       # Consumer offset persistence\n│   ├── types/                    # Core domain types\n│   │   ├── topic.go              # Topic and Partition types\n│   │   ├── record.go             # Record and RecordBatch types\n│   │   └── metadata.go           # Cluster metadata types\n│   └── network/                  # Network abstraction\n│       ├── connection.go         # TCP connection management\n│       ├── request_handler.go    # Request routing\n│       └── server.go             # Generic TCP server\n├── pkg/                          # Public libraries (if any)\n│   └── publicapi/                # Stable public APIs for external use\n└── test/                         # Integration tests and utilities\n    ├── integration/              # End-to-end integration tests\n    └── helpers.go                # Test utilities</code></pre></div>\n\n<p><strong>Package Rationale:</strong></p>\n<ul>\n<li><p><strong><code>cmd/</code></strong>: Follows Go conventions for executable applications. Separating broker, producer-cli, and consumer-cli allows independent development and testing.</p>\n</li>\n<li><p><strong><code>internal/</code></strong>: Contains implementation details that should not be imported by external code. This prevents dependency entanglement and maintains clean API boundaries.</p>\n</li>\n<li><p><strong><code>internal/broker/</code></strong>: Houses the server-side logic. Separating <code>log_manager</code>, <code>replica_manager</code>, and <code>coordinator</code> allows each to evolve independently and be tested in isolation.</p>\n</li>\n<li><p><strong><code>internal/client/</code></strong>: Contains client libraries. Producers and consumers are separate packages as they have distinct concerns, though they share some protocol code.</p>\n</li>\n<li><p><strong><code>internal/protocol/</code></strong>: Centralizes wire format definitions. This is critical because brokers and clients must agree exactly on binary formats.</p>\n</li>\n<li><p><strong><code>internal/storage/</code></strong>: Abstracts persistence concerns. This separation allows swapping storage backends (e.g., file system vs. embedded database) without affecting higher layers.</p>\n</li>\n<li><p><strong><code>internal/types/</code></strong>: Defines core domain objects shared across packages. Keeping these in a central location prevents circular dependencies.</p>\n</li>\n</ul>\n<blockquote>\n<p><strong>Design Principle</strong>: Each package should have a single, clear responsibility. Dependencies should flow downward: higher-level packages (<code>broker</code>, <code>client</code>) depend on lower-level ones (<code>protocol</code>, <code>storage</code>, <code>types</code>), but not vice versa.</p>\n</blockquote>\n<h3 id=\"33-end-to-end-message-flow\">3.3 End-to-End Message Flow</h3>\n<p>To understand how components interact, let&#39;s trace the journey of a single message through the system. We&#39;ll follow a concrete example: a weather service (producer) publishing a temperature reading to a <code>weather-updates</code> topic, which is then consumed by a dashboard application (consumer).</p>\n<p><img src=\"/api/project/build-kafka/architecture-doc/asset?path=diagrams%2Fdiagram-replication-flow.svg\" alt=\"Leader-Follower Replication Flow\"></p>\n<h4 id=\"scenario-setup\">Scenario Setup:</h4>\n<ul>\n<li><strong>Topic</strong>: <code>weather-updates</code> with 3 partitions</li>\n<li><strong>Message Key</strong>: <code>station-42</code> (identifies the weather station)</li>\n<li><strong>Message Value</strong>: <code>{&quot;temp&quot;: 22.5, &quot;humidity&quot;: 65}</code></li>\n<li><strong>Producer</strong>: Configured with <code>acks=all</code> (wait for all ISR replicas)</li>\n<li><strong>Consumer Group</strong>: <code>dashboard-group</code> with 2 consumer instances</li>\n</ul>\n<h4 id=\"step-by-step-flow\">Step-by-Step Flow:</h4>\n<ol>\n<li><p><strong>Producer Initialization and Metadata Fetch</strong></p>\n<ul>\n<li>The producer starts with a bootstrap broker address (e.g., <code>broker1:9092</code>).</li>\n<li>It connects to the bootstrap broker and sends a <code>MetadataRequest</code> for the <code>weather-updates</code> topic.</li>\n<li>The broker responds with metadata including:<ul>\n<li>Partition count (3)</li>\n<li>Leader broker for each partition (e.g., partition 0 → broker2, partition 1 → broker1, partition 2 → broker3)</li>\n<li>Replica brokers for each partition</li>\n</ul>\n</li>\n<li>The producer caches this metadata for future requests (refreshing periodically or on errors).</li>\n</ul>\n</li>\n<li><p><strong>Message Routing to Partition</strong></p>\n<ul>\n<li>The producer receives the message with key <code>station-42</code>.</li>\n<li>It applies the partitioner logic: hash the key, modulo partition count.</li>\n<li><code>hash(&quot;station-42&quot;) % 3 = 1</code> → message routes to partition 1.</li>\n<li>The producer knows broker1 leads partition 1 (from cached metadata).</li>\n</ul>\n</li>\n<li><p><strong>Batch Accumulation</strong></p>\n<ul>\n<li>The producer maintains a per-partition batch accumulator.</li>\n<li>The message is added to the batch for partition 1.</li>\n<li>If the batch is not yet full and the linger time hasn&#39;t expired, the producer waits for more messages to batch.</li>\n<li>Once batch is full or timer fires, the batch is marked ready for sending.</li>\n</ul>\n</li>\n<li><p><strong>Produce Request to Leader</strong></p>\n<ul>\n<li>The producer sends a <code>ProduceRequest</code> to broker1 (leader for partition 1).</li>\n<li>The request contains:<ul>\n<li>Topic: <code>weather-updates</code></li>\n<li>Partition: 1</li>\n<li>Record batch with our message (and potentially other batched messages)</li>\n<li>Required acks: <code>all</code> (wait for all ISR replicas)</li>\n</ul>\n</li>\n<li>The producer starts a retry timer and stores the batch in an &quot;in-flight&quot; buffer.</li>\n</ul>\n</li>\n<li><p><strong>Leader Log Append</strong></p>\n<ul>\n<li>Broker1 receives the <code>ProduceRequest</code> and validates permissions, topic existence, etc.</li>\n<li>It appends the record batch to its local log for partition 1.</li>\n<li>The log manager assigns the next sequential offset (e.g., offset 142) to the first record in the batch.</li>\n<li>The leader writes the batch to its write-ahead log and calls <code>fsync()</code> based on durability configuration.</li>\n<li>The leader updates its <strong>log end offset</strong> (LEO) for partition 1 to 143 (142 + 1 record).</li>\n</ul>\n</li>\n<li><p><strong>Replication to Followers</strong></p>\n<ul>\n<li>Partition 1 has replicas on broker2 and broker3 (followers).</li>\n<li>Followers periodically send <code>FetchRequest</code> to the leader (or the leader pushes, depending on design).</li>\n<li>The leader sends the new record batch to followers in response to their fetch requests.</li>\n<li>Each follower appends the batch to its own log and acknowledges to the leader.</li>\n<li>The leader tracks which replicas have caught up. Once all ISR replicas have the record, the leader advances the <strong>high watermark</strong> to offset 143.</li>\n</ul>\n</li>\n<li><p><strong>Acknowledgment to Producer</strong></p>\n<ul>\n<li>With <code>acks=all</code>, the leader waits until all ISR replicas have replicated the record.</li>\n<li>Once the high watermark advances past our record&#39;s offset, the leader sends a successful <code>ProduceResponse</code> to the producer.</li>\n<li>The response includes the base offset assigned (142) for the batch.</li>\n<li>The producer removes the batch from its in-flight buffer and can consider the message durably stored.</li>\n</ul>\n</li>\n<li><p><strong>Consumer Subscription and Assignment</strong></p>\n<ul>\n<li>Meanwhile, two consumers (C1 and C2) in group <code>dashboard-group</code> subscribe to topic <code>weather-updates</code>.</li>\n<li>Each consumer sends <code>JoinGroup</code> request to the group coordinator (which may be broker1).</li>\n<li>The coordinator designates one consumer as the group leader and collects subscription preferences.</li>\n<li>The group leader runs the partition assignment strategy (e.g., range assignment):<ul>\n<li>Partitions 0 and 1 → Consumer C1</li>\n<li>Partition 2 → Consumer C2</li>\n</ul>\n</li>\n<li>The coordinator sends assignments via <code>SyncGroup</code> responses.</li>\n</ul>\n</li>\n<li><p><strong>Consumer Fetch and Delivery</strong></p>\n<ul>\n<li>Consumer C1, now assigned partition 1, fetches records starting from its last committed offset (initially 0 or from previous session).</li>\n<li>It sends a <code>FetchRequest</code> to broker1 (leader for partition 1), requesting records from offset 142 onward.</li>\n<li>Broker1 returns records starting at offset 142, which includes our temperature reading.</li>\n<li>Consumer C1 delivers the record to the application callback for processing.</li>\n<li>The consumer periodically commits its offset (e.g., offset 143) via <code>OffsetCommit</code> requests.</li>\n</ul>\n</li>\n<li><p><strong>Offset Commitment</strong></p>\n<ul>\n<li>Consumer C1 sends an <code>OffsetCommit</code> request to the group coordinator.</li>\n<li>The coordinator durably stores <code>{group: dashboard-group, topic: weather-updates, partition: 1, offset: 143}</code>.</li>\n<li>On restart, consumer C1 can fetch this committed offset and resume from where it left off.</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"failure-scenario-handling\">Failure Scenario Handling</h4>\n<p>If broker1 fails <strong>after</strong> appending the record but <strong>before</strong> replicating to followers:</p>\n<ol>\n<li>The producer&#39;s retry timer expires (no response received).</li>\n<li>The producer refreshes metadata and discovers broker1 is down.</li>\n<li>A new leader election occurs for partition 1 (from ISR set).</li>\n<li>The producer resends the batch to the new leader.</li>\n<li>Because the original write may or may not have persisted, we rely on <strong>idempotent sequence numbers</strong> (optional) or accept <strong>at-least-once</strong> delivery semantics.</li>\n</ol>\n<p>This end-to-end flow illustrates the coordinated dance between components to deliver durable, ordered messaging while handling failures transparently.</p>\n<hr>\n<h2 id=\"implementation-guidance\">Implementation Guidance</h2>\n<h3 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h3>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Recommended for Learning)</th>\n<th>Advanced Option (For Extension)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Network Transport</strong></td>\n<td>Plain TCP with custom binary protocol using Go&#39;s <code>net</code> package</td>\n<td>gRPC with Protocol Buffers for type-safe RPCs</td>\n</tr>\n<tr>\n<td><strong>Serialization</strong></td>\n<td>Manual binary encoding using <code>encoding/binary</code></td>\n<td>Apache Avro with schema registry</td>\n</tr>\n<tr>\n<td><strong>Storage Backend</strong></td>\n<td>Direct filesystem I/O with <code>os.File</code></td>\n<td>Embedded key-value store (BadgerDB, BoltDB)</td>\n</tr>\n<tr>\n<td><strong>Coordination</strong></td>\n<td>In-memory coordinator on designated broker with periodic persistence</td>\n<td>External coordination service (etcd, ZooKeeper)</td>\n</tr>\n<tr>\n<td><strong>Concurrency</strong></td>\n<td>Goroutines + channels + <code>sync.Mutex</code>/<code>sync.RWMutex</code></td>\n<td>Actor model with third-party library (protoactor-go)</td>\n</tr>\n</tbody></table>\n<h3 id=\"b-recommended-starter-code-structure\">B. Recommended Starter Code Structure</h3>\n<p>Here&#39;s a complete, working foundation for the network and storage layers that learners can build upon:</p>\n<p><strong>File: <code>internal/network/server.go</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> network</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TCPServer is a generic TCP server that accepts connections and delegates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// handling to a RequestHandler.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TCPServer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    addr     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    listener </span><span style=\"color:#B392F0\">net</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Listener</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler  </span><span style=\"color:#B392F0\">RequestHandler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx      </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancel   </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RequestHandler processes incoming requests from a connection.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RequestHandler</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Handle</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">conn</span><span style=\"color:#B392F0\"> net</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Conn</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTCPServer creates a new TCP server.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTCPServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">addr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#B392F0\"> RequestHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TCPServer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TCPServer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        addr:    addr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        handler: handler,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins accepting connections.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TCPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    listener, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> net.</span><span style=\"color:#B392F0\">Listen</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tcp\"</span><span style=\"color:#E1E4E8\">, s.addr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to listen on </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, s.addr, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.listener </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> listener</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.ctx, s.cancel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithCancel</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">acceptLoop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// acceptLoop accepts incoming connections and spawns goroutines to handle them.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TCPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">acceptLoop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.listener.</span><span style=\"color:#B392F0\">Accept</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // Log error but continue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">c</span><span style=\"color:#B392F0\"> net</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Conn</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                defer</span><span style=\"color:#E1E4E8\"> s.wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                defer</span><span style=\"color:#E1E4E8\"> c.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                s.handler.</span><span style=\"color:#B392F0\">Handle</span><span style=\"color:#E1E4E8\">(c)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }(conn)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down the server.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TCPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.cancel </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.</span><span style=\"color:#B392F0\">cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.listener </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.listener.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>File: <code>internal/storage/wal.go</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WAL implements a simple write-ahead log for durable writes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WAL</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filePath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\"> // Current write offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OpenWAL opens or creates a WAL file at the given path.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> OpenWAL</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filePath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure directory exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Dir</span><span style=\"color:#E1E4E8\">(filePath)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(dir, </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create directory </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, dir, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Open file in append mode, create if doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">OpenFile</span><span style=\"color:#E1E4E8\">(filePath, os.O_RDWR</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_CREATE</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_APPEND, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to open WAL file </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, filePath, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get current file size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stat, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to stat WAL file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file:     file,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filePath: filePath,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        offset:   stat.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Append writes data to the WAL and optionally fsyncs.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sync</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write length prefix (8 bytes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    length </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    header </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint64</span><span style=\"color:#E1E4E8\">(header, length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write header</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(header); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to write header: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(data); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to write data: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sync if requested</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> sync {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Sync</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to sync: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startOffset </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.offset </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data)) </span><span style=\"color:#6A737D\">// Update offset for next write</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> startOffset, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadAll reads all entries from the WAL.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">() ([][]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Seek to beginning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Seek</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, io.SeekStart); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to seek: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> entries [][]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Read length prefix</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> length </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(w.file, binary.BigEndian, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">length); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> io.EOF {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read length: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Read data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, length)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadFull</span><span style=\"color:#E1E4E8\">(w.file, data); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read data: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entries </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(entries, data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entries, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close closes the WAL file.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CurrentOffset returns the current write offset.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CurrentOffset</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> w.offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>File: <code>internal/types/topic.go</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Topic represents a named stream of records divided into partitions.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Topic</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Partitions []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Partition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Config     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Partition represents a single ordered, immutable sequence of records.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Partition</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID              </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LeaderBrokerID  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicaBrokerIDs []</span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Log             </span><span style=\"color:#B392F0\">Log</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Additional fields for replication</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HighWatermark   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ISR             []</span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\"> // Broker IDs in the in-sync replica set</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Log represents the storage for a partition.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Log</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Segments      []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogSegment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\"> // Next offset to assign</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogSegment represents a physical file segment of the log.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogSegment</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BaseOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // First offset in this segment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataFile   </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Path to data file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IndexFile  </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Path to index file (offset → position)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"c-core-logic-skeleton\">C. Core Logic Skeleton</h3>\n<p><strong>File: <code>internal/broker/server.go</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> broker</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/your-org/build-your-own-kafka/internal/network</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/your-org/build-your-own-kafka/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/your-org/build-your-own-kafka/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Server is the main broker server.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config    </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">network</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TCPServer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logDir    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // State protected by mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    topics      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Topic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add other state fields: consumer groups, replica state, etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Dependencies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logManager  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    coordinator </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Coordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config holds broker configuration.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Host </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Port </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataDir </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewServer creates a new broker server.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config: config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        topics: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Topic</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logDir: filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(config.DataDir, </span><span style=\"color:#9ECBFF\">\"logs\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start starts the broker server.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create log directory if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize log manager to handle partition logs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Load existing topics and partitions from disk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Initialize coordinator for consumer groups</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Create and start TCP server with request handler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Start background goroutines: replica fetcher, leader election, etc.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateTopic creates a new topic with the given partition count.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateTopic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partitionCount</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate topic name doesn't already exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate partition count is positive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create partition objects with empty logs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Assign partition leaders (initially this broker)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Initialize log segments for each partition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Store topic in topics map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Persist topic metadata to disk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleProduce handles a produce request from a client.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleProduce</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ProduceRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ProduceResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate request: topic exists, partition exists, acks valid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each partition in request:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check if this broker is leader for the partition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Append records to partition log</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Update log end offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Based on acks level:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - acks=0: Return success immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - acks=1: Wait for leader write, then return</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - acks=all: Wait for all ISR replicas, update high watermark, then return</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Build response with offsets for each partition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleFetch handles a fetch request from a consumer.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleFetch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FetchRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FetchResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate request: topic exists, partition exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each requested partition:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check if this broker is leader for the partition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Read records starting from requested offset up to high watermark</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If offset is beyond high watermark, wait up to max wait time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Build response with records for each partition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"d-language-specific-hints\">D. Language-Specific Hints</h3>\n<ol>\n<li><p><strong>Concurrency</strong>: Use <code>sync.RWMutex</code> for protecting shared broker state (topics, partitions). Readers can acquire read locks concurrently, while writers need exclusive locks.</p>\n</li>\n<li><p><strong>File I/O</strong>: Always use <code>file.Sync()</code> after critical writes to ensure durability. For performance, batch sync operations when possible.</p>\n</li>\n<li><p><strong>Network Connections</strong>: Use <code>SetDeadline()</code> on connections to prevent hung connections from consuming resources.</p>\n</li>\n<li><p><strong>Error Handling</strong>: In Go, return errors rather than panicking. Use <code>fmt.Errorf(&quot;%w&quot;, err)</code> to wrap errors with context.</p>\n</li>\n<li><p><strong>Memory Management</strong>: For high-throughput scenarios, reuse byte buffers with <code>sync.Pool</code> to reduce garbage collection pressure.</p>\n</li>\n<li><p><strong>Testing</strong>: Use <code>net.Listen(&quot;tcp&quot;, &quot;localhost:0&quot;)</code> to get random ports for integration tests, avoiding port conflicts.</p>\n</li>\n</ol>\n<h3 id=\"e-milestone-checkpoint\">E. Milestone Checkpoint</h3>\n<p>After implementing the high-level architecture, you should be able to:</p>\n<ol>\n<li><p><strong>Start a broker</strong>: Run <code>go run cmd/broker/main.go</code> and see it listen on the configured port (e.g., <code>:9092</code>).</p>\n</li>\n<li><p><strong>Create a topic via API</strong>: Use a simple curl command or test program to call <code>CreateTopic</code> and verify the topic appears in the broker&#39;s topic list.</p>\n</li>\n<li><p><strong>Verify directory structure</strong>: Check that the data directory contains subdirectories for each topic partition.</p>\n</li>\n<li><p><strong>Basic connectivity</strong>: Write a simple test that connects to the broker, sends a ping, and receives a response.</p>\n</li>\n</ol>\n<p><strong>Expected directory structure after creating a topic:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>data/\n└── logs/\n    └── weather-updates-0/\n        ├── 00000000000000000000.log\n        └── 00000000000000000000.index</code></pre></div>\n\n<p><strong>Signs something is wrong:</strong></p>\n<ul>\n<li>Broker fails to start: Check port availability and directory permissions.</li>\n<li>Topic creation fails: Verify partition count is positive and topic name follows naming rules.</li>\n<li>No log files created: Ensure the broker has write permissions to the data directory.</li>\n</ul>\n<h2 id=\"4-data-model\">4. Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1, 2, 3, 4 (foundational structures)\nThis section defines the foundational data structures that give our message queue its shape, both in memory and on persistent storage. Before diving into component interactions and algorithms, we must establish a precise vocabulary of types and their relationships. These structures embody the core concepts of topics, partitions, logs, and offsets, forming the skeleton upon which all system behavior is built.</p>\n</blockquote>\n<h3 id=\"41-core-types-and-relationships\">4.1 Core Types and Relationships</h3>\n<p>Think of the data model as a <strong>library system for events</strong>. A <code>Topic</code> is like a book series (e.g., &quot;Financial Transactions&quot;). Each book in the series is a <code>Partition</code> (e.g., &quot;Volume 1: NYSE&quot;, &quot;Volume 2: NASDAQ&quot;). Inside each partition, <code>Record</code>s are individual pages, numbered sequentially by <code>Offset</code> (page number). <code>LogSegment</code>s are the physical bindings that group a contiguous set of pages for storage efficiency. Finally, <code>ConsumerGroup</code>s are reading clubs; each member (<code>Consumer</code>) checks out specific volumes and keeps a bookmark (<code>OffsetMetadata</code>) to remember where they left off.</p>\n<p>This mental model clarifies the hierarchy and ownership: a topic <em>has</em> partitions, a partition <em>is</em> a log, a log <em>contains</em> segments, and segments <em>hold</em> records. Consumer groups <em>track</em> progress per partition. The following tables define each type&#39;s fields and purpose.</p>\n<h4 id=\"in-memory-core-types\">In-Memory Core Types</h4>\n<p>These structures are primarily held in memory on brokers and clients to manage runtime state.</p>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong><code>Topic</code></strong></td>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Unique identifier for the topic (e.g., <code>&quot;user-events&quot;</code>).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Partitions</code></td>\n<td><code>[]Partition</code></td>\n<td>The list of partitions that constitute this topic. The length defines the partition count.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Config</code></td>\n<td><code>map[string]string</code></td>\n<td>Key-value settings for the topic (e.g., <code>&quot;retention.ms&quot;: &quot;604800000&quot;</code>).</td>\n</tr>\n<tr>\n<td><strong><code>Partition</code></strong></td>\n<td><code>Topic</code></td>\n<td><code>string</code></td>\n<td>The name of the parent topic.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ID</code></td>\n<td><code>int</code></td>\n<td>The zero-based identifier of this partition within the topic.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>LeaderBrokerID</code></td>\n<td><code>int</code></td>\n<td>The broker ID currently acting as the leader for this partition, handling all reads and writes.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ReplicaBrokerIDs</code></td>\n<td><code>[]int</code></td>\n<td>The ordered list of broker IDs hosting replicas of this partition. The first element is typically the leader.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Log</code></td>\n<td><code>Log</code></td>\n<td>The append-only log instance managing the physical storage of records for this partition.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>HighWatermark</code></td>\n<td><code>int64</code></td>\n<td>The offset of the last record that is known to be replicated to all In-Sync Replicas (ISR). Consumers cannot read beyond this point.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>LogEndOffset</code></td>\n<td><code>int64</code></td>\n<td>The offset that will be assigned to the next record appended to the log (also called LEO).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ISR</code></td>\n<td><code>[]int</code></td>\n<td>The list of broker IDs currently considered &quot;in-sync&quot; (replicas caught up within a configurable lag threshold).</td>\n</tr>\n<tr>\n<td><strong><code>Record</code></strong></td>\n<td><code>Key</code></td>\n<td><code>[]byte</code></td>\n<td>Optional key used for partitioning and log compaction. <code>nil</code> keys are allowed and handled by a default partitioner.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Value</code></td>\n<td><code>[]byte</code></td>\n<td>The message payload. Can be <code>nil</code> for tombstones in log compaction.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Timestamp</code></td>\n<td><code>int64</code></td>\n<td>The time the record was created, in milliseconds since the Unix epoch. Used for retention and client-side ordering.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Headers</code></td>\n<td><code>map[string][]byte</code></td>\n<td>Optional application-specific key-value pairs attached to the record (e.g., tracing IDs, content-type).</td>\n</tr>\n<tr>\n<td><strong><code>Log</code></strong></td>\n<td><code>Segments</code></td>\n<td><code>[]LogSegment</code></td>\n<td>The ordered list of segment files comprising the log. The last segment is the active one for appends.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>CurrentOffset</code></td>\n<td><code>int64</code></td>\n<td>The next offset to be assigned (identical to <code>Partition.LogEndOffset</code>, maintained for convenience).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>BaseDir</code></td>\n<td><code>string</code></td>\n<td>The directory path where segment files for this partition are stored (e.g., <code>&quot;/data/topic-0&quot;</code>).</td>\n</tr>\n<tr>\n<td><strong><code>LogSegment</code></strong></td>\n<td><code>BaseOffset</code></td>\n<td><code>int64</code></td>\n<td>The offset of the first record contained in this segment. Used for filename generation and binary search.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>DataFile</code></td>\n<td><code>string</code></td>\n<td>Path to the primary data file (e.g., <code>&quot;00000000000000000000.log&quot;</code>) holding the record batches.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>IndexFile</code></td>\n<td><code>string</code></td>\n<td>Path to the sparse index file (e.g., <code>&quot;00000000000000000000.index&quot;</code>) mapping record offsets to byte positions in the data file.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>SizeBytes</code></td>\n<td><code>int64</code></td>\n<td>Current size of the data file in bytes. Used to trigger rolling to a new segment.</td>\n</tr>\n<tr>\n<td><strong><code>ConsumerGroup</code></strong></td>\n<td><code>GroupID</code></td>\n<td><code>string</code></td>\n<td>Unique identifier for the consumer group (e.g., <code>&quot;email-processors&quot;</code>).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Members</code></td>\n<td><code>map[string]ConsumerMetadata</code></td>\n<td>Map of consumer member IDs to their metadata (subscriptions, assigned partitions).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>State</code></td>\n<td><code>GroupState</code></td>\n<td>Current group state: <code>Stable</code>, <code>PreparingRebalance</code>, <code>Dead</code>.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>GenerationID</code></td>\n<td><code>int32</code></td>\n<td>Monotonically increasing integer incremented on each successful rebalancing. Used to identify obsolete requests.</td>\n</tr>\n<tr>\n<td><strong><code>ConsumerMetadata</code></strong></td>\n<td><code>MemberID</code></td>\n<td><code>string</code></td>\n<td>Unique ID assigned by the coordinator for this group member session.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ClientID</code></td>\n<td><code>string</code></td>\n<td>Client-provided identifier for debugging.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>SubscribedTopics</code></td>\n<td><code>[]string</code></td>\n<td>List of topics this consumer has subscribed to.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>AssignedPartitions</code></td>\n<td><code>map[string][]int32</code></td>\n<td>Map from topic to list of partition IDs assigned to this member after rebalancing.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>LastHeartbeat</code></td>\n<td><code>int64</code></td>\n<td>Unix timestamp (ms) of the last received heartbeat. Used for failure detection.</td>\n</tr>\n<tr>\n<td><strong><code>OffsetMetadata</code></strong></td>\n<td><code>Partition</code></td>\n<td><code>PartitionID</code></td>\n<td>Composite identifier (Topic, PartitionID) for the partition.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>GroupID</code></td>\n<td><code>string</code></td>\n<td>The consumer group this offset belongs to.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Offset</code></td>\n<td><code>int64</code></td>\n<td>The last successfully processed record offset. The consumer will resume from <code>Offset + 1</code>.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Metadata</code></td>\n<td><code>string</code></td>\n<td>Optional client-provided string (e.g., a commit message).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>CommitTimestamp</code></td>\n<td><code>int64</code></td>\n<td>When this offset was committed (ms since epoch).</td>\n</tr>\n</tbody></table>\n<h4 id=\"type-relationships-and-lifetime\">Type Relationships and Lifetime</h4>\n<p>The relationships between these types form a directed graph. A <code>Topic</code> aggregates <code>Partition</code> objects. Each <code>Partition</code> owns a single <code>Log</code> instance. The <code>Log</code> manages a list of <code>LogSegment</code> objects, which are created, rolled, and deleted over time. <code>Record</code> objects are ephemeral; they are created by producers, serialized into batches, and written to segments. Once written, they are primarily accessed via byte offsets, not as in-memory objects.</p>\n<p><code>ConsumerGroup</code> and <code>OffsetMetadata</code> have a separate lifecycle. Groups are created when the first consumer joins and persist until all members leave or a session timeout expires. Offsets are committed by consumers and stored durably, separate from the log data, to survive broker restarts.</p>\n<blockquote>\n<p><strong>Design Insight:</strong> The separation of <em>message data</em> (in partition logs) from <em>consumption progress</em> (in offset storage) is critical. It allows multiple independent consumer groups to read the same topic at their own pace, and enables replay by resetting offsets without modifying the underlying log.</p>\n</blockquote>\n<p>The class diagram below visualizes these relationships:\n<img src=\"/api/project/build-kafka/architecture-doc/asset?path=diagrams%2Fdiagram-data-model.svg\" alt=\"Data Model Relationships\"></p>\n<h3 id=\"42-on-disk-storage-format\">4.2 On-Disk Storage Format</h3>\n<p>Our system&#39;s durability guarantee hinges on reliably persisting two types of data: <strong>log records</strong> (the messages themselves) and <strong>consumer offsets</strong> (progress bookmarks). The design of the on-disk format must balance read efficiency, write performance, crash consistency, and operational simplicity.</p>\n<blockquote>\n<p><strong>Mental Model: The Ship&#39;s Logbook and Its Index.</strong> Imagine a captain&#39;s logbook (<code>*.log</code> file) where entries are written sequentially, day by day. A separate index (<code>*.index</code> file) records that &quot;Day 50 starts on page 120.&quot; The logbook is append-only; we never modify past entries. The index is sparse—we don&#39;t index every entry, only every few pages—to keep it small and fast to search.</p>\n</blockquote>\n<h4 id=\"log-segment-files\">Log Segment Files</h4>\n<p>Each partition&#39;s log is split into multiple <strong>segment files</strong> for manageability. Segments are rolled when the active data file reaches a configured size (e.g., 1 GB) or time threshold. The naming convention uses the <code>BaseOffset</code> (the offset of the first record in the segment) padded to 20 digits: <code>{base_offset}.log</code> and <code>{base_offset}.index</code>.</p>\n<p><strong>Record Batch Format (<code>.log</code> file)</strong>\nMessages are grouped into <strong>batches</strong> for efficiency. The batch is the unit of writing and reading from disk. The binary format is as follows:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Value/Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>base_offset</code></td>\n<td>int64</td>\n<td>The offset of the first record in this batch. Used for recovery and validation.</td>\n</tr>\n<tr>\n<td><code>batch_length</code></td>\n<td>int32</td>\n<td>Length in bytes of the batch data (from <code>partition_leader_epoch</code> to the end of <code>records</code>).</td>\n</tr>\n<tr>\n<td><code>partition_leader_epoch</code></td>\n<td>int32</td>\n<td>For leader fencing; can be set to 0 in initial implementation.</td>\n</tr>\n<tr>\n<td><code>magic</code></td>\n<td>int8</td>\n<td>Protocol version (set to <code>2</code> for our system, indicating v2 record format).</td>\n</tr>\n<tr>\n<td><code>crc</code></td>\n<td>uint32</td>\n<td>CRC32C checksum of the batch data (from <code>magic</code> to end of <code>records</code>). Validated on read.</td>\n</tr>\n<tr>\n<td><code>attributes</code></td>\n<td>int16</td>\n<td>Bit flags for compression (<code>0x00</code> = none, <code>0x01</code> = gzip, <code>0x02</code> = snappy), timestamp type, and transactional control.</td>\n</tr>\n<tr>\n<td><code>last_offset_delta</code></td>\n<td>int32</td>\n<td>The difference between the last and first offset in the batch. Allows quick calculation of the last offset without parsing all records.</td>\n</tr>\n<tr>\n<td><code>first_timestamp</code></td>\n<td>int64</td>\n<td>The timestamp of the first record in the batch.</td>\n</tr>\n<tr>\n<td><code>max_timestamp</code></td>\n<td>int64</td>\n<td>The maximum timestamp among records in the batch.</td>\n</tr>\n<tr>\n<td><code>producer_id</code></td>\n<td>int64</td>\n<td>For idempotent producers; set to <code>-1</code> if not used.</td>\n</tr>\n<tr>\n<td><code>producer_epoch</code></td>\n<td>int16</td>\n<td>For idempotent producers; set to <code>0</code> if not used.</td>\n</tr>\n<tr>\n<td><code>base_sequence</code></td>\n<td>int32</td>\n<td>For idempotent producers; set to <code>0</code> if not used.</td>\n</tr>\n<tr>\n<td><code>record_count</code></td>\n<td>int32</td>\n<td>Number of records in the batch.</td>\n</tr>\n<tr>\n<td><code>records</code></td>\n<td>[]Record</td>\n<td>Array of individual records, each encoded as below.</td>\n</tr>\n</tbody></table>\n<p><strong>Individual Record Format (inside batch)</strong>\nRecords are encoded within a batch to avoid repeating common metadata.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>length</code></td>\n<td>varint</td>\n<td>Length of the record data in bytes (following this field).</td>\n</tr>\n<tr>\n<td><code>attributes</code></td>\n<td>int8</td>\n<td>Currently unused (set to <code>0</code>).</td>\n</tr>\n<tr>\n<td><code>timestamp_delta</code></td>\n<td>varint</td>\n<td>Difference from <code>first_timestamp</code> in milliseconds.</td>\n</tr>\n<tr>\n<td><code>offset_delta</code></td>\n<td>varint</td>\n<td>Difference from <code>base_offset</code>.</td>\n</tr>\n<tr>\n<td><code>key_length</code></td>\n<td>varint</td>\n<td>Length of the key bytes, or <code>-1</code> for null key.</td>\n</tr>\n<tr>\n<td><code>key</code></td>\n<td>bytes</td>\n<td>The key bytes (if <code>key_length &gt;= 0</code>).</td>\n</tr>\n<tr>\n<td><code>value_length</code></td>\n<td>varint</td>\n<td>Length of the value bytes, or <code>-1</code> for null value.</td>\n</tr>\n<tr>\n<td><code>value</code></td>\n<td>bytes</td>\n<td>The value bytes (if <code>value_length &gt;= 0</code>).</td>\n</tr>\n<tr>\n<td><code>headers_count</code></td>\n<td>varint</td>\n<td>Number of headers.</td>\n</tr>\n<tr>\n<td><code>headers</code></td>\n<td>[]Header</td>\n<td>Array of headers, each with a <code>header_key</code> (string varint) and <code>header_value</code> (bytes varint).</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Why Batches?</strong> Writing individual records would incur massive overhead per I/O operation. Batching amortizes the cost of the disk seek and filesystem sync across many messages, dramatically increasing throughput. The batch header also allows readers to skip batches (e.g., by offset or timestamp) without parsing individual records.</p>\n</blockquote>\n<p><strong>Sparse Index Format (<code>.index</code> file)</strong>\nThe index file provides a mapping from <strong>offset</strong> to <strong>byte position</strong> in the corresponding <code>.log</code> file. It is sparse to remain small; we index only every <code>index.interval.bytes</code> (e.g., 4KB) of log data. Each entry is two 8-byte values:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>relative_offset</code></td>\n<td>int32</td>\n<td>The offset relative to the segment&#39;s <code>base_offset</code> (<code>actual_offset - base_offset</code>). This allows using a 4-byte integer, saving space.</td>\n</tr>\n<tr>\n<td><code>position</code></td>\n<td>int32</td>\n<td>The physical byte position of the start of the log batch containing this offset, within the <code>.log</code> file.</td>\n</tr>\n</tbody></table>\n<p>The index is memory-mapped for fast binary search. To find the position for a target offset <code>O</code>: 1) binary search the index for the largest <code>relative_offset &lt;= (O - base_offset)</code>, 2) read the <code>.log</code> file from the corresponding <code>position</code>, and 3) scan forward until reaching offset <code>O</code>.</p>\n<h4 id=\"consumer-offset-storage-format\">Consumer Offset Storage Format</h4>\n<p>Consumer offsets must be persisted durably and support fast updates and queries. We have two primary design options:</p>\n<blockquote>\n<p><strong>Decision: Store Offsets in a Special Internal Topic</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a durable, replicated, and scalable storage for consumer group offsets that aligns with the system&#39;s existing replication and fault-tolerance mechanisms.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Special internal topic</strong> (<code>__consumer_offsets</code>): A compacted topic where each message key is <code>(GroupID, Topic, Partition)</code> and the value is the offset metadata.</li>\n<li><strong>Dedicated file per broker</strong>: A local file (e.g., <code>offsets.json</code>) storing all offsets for groups managed by that broker&#39;s coordinator.</li>\n<li><strong>External key-value store</strong>: Using an embedded DB (like RocksDB/LevelDB) for offset storage.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use a special internal, compacted topic (<code>__consumer_offsets</code>).</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Consistency</strong>: Leverages the same replication, durability, and recovery mechanisms as regular topics, ensuring offsets survive broker failures.</li>\n<li><strong>Scalability</strong>: Partitions of the internal topic distribute the load across the cluster.</li>\n<li><strong>Operational Simplicity</strong>: No separate storage subsystem to manage; log compaction automatically removes obsolete offset commits.</li>\n<li><strong>Educational Value</strong>: Reinforces the power of the log as a universal storage abstraction.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>Offsets are replicated and fault-tolerant.</li>\n<li>Requires implementing log compaction (or a simplified version) for cleanup.</li>\n<li>Introduces a bootstrap dependency: the broker must be able to create and write to this topic before handling consumer groups.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Internal Topic</td>\n<td>Unified storage, replication, scalability</td>\n<td>Requires log compaction; circular dependency risk</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Dedicated File</td>\n<td>Simple, no extra features needed</td>\n<td>Not replicated, single point of failure, scales poorly</td>\n<td>No</td>\n</tr>\n<tr>\n<td>External KV Store</td>\n<td>Fast random updates, mature</td>\n<td>Adds a new dependency, operational complexity</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p><strong>Offset Topic Record Format</strong>\nThe <code>__consumer_offsets</code> topic uses the same log segment format, but with a specific key and value schema for its records.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Key</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>group_id</code></td>\n<td>string</td>\n<td>The consumer group identifier.</td>\n</tr>\n<tr>\n<td><code>topic</code></td>\n<td>string</td>\n<td>The topic name.</td>\n</tr>\n<tr>\n<td><code>partition</code></td>\n<td>int32</td>\n<td>The partition ID.</td>\n</tr>\n<tr>\n<td><strong>Value</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>offset</code></td>\n<td>int64</td>\n<td>The committed offset.</td>\n</tr>\n<tr>\n<td><code>metadata</code></td>\n<td>string</td>\n<td>Optional client metadata string.</td>\n</tr>\n<tr>\n<td><code>commit_timestamp</code></td>\n<td>int64</td>\n<td>Timestamp of the commit (ms).</td>\n</tr>\n<tr>\n<td><code>expire_timestamp</code></td>\n<td>int64</td>\n<td>For session timeouts; can be omitted initially.</td>\n</tr>\n</tbody></table>\n<p>This topic should be <strong>compacted</strong> so that only the latest offset for each <code>(group, topic, partition)</code> key is retained, saving space.</p>\n<h3 id=\"43-cluster-metadata\">4.3 Cluster Metadata</h3>\n<p>Cluster metadata is the <strong>directory of the library</strong>—it tells clients and brokers where to find things. It includes the list of live brokers, which broker leads each partition, and topic configurations. This metadata must be consistently available to all participants and updated dynamically as the cluster changes (brokers join/leave, leadership changes).</p>\n<h4 id=\"metadata-structures\">Metadata Structures</h4>\n<p>The following structures are maintained by a <strong>metadata coordinator</strong> (which could be a dedicated broker or a consensus service like Raft) and cached by all brokers and clients.</p>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong><code>Broker</code></strong></td>\n<td><code>ID</code></td>\n<td><code>int</code></td>\n<td>Unique broker identifier (must be stable across restarts).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Host</code></td>\n<td><code>string</code></td>\n<td>Network hostname or IP address for client connections.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Port</code></td>\n<td><code>int</code></td>\n<td>TCP port for client connections.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Rack</code></td>\n<td><code>string</code></td>\n<td>Optional rack identifier for rack-aware replica placement.</td>\n</tr>\n<tr>\n<td><strong><code>TopicMetadata</code></strong></td>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Topic name.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Partitions</code></td>\n<td><code>[]PartitionMetadata</code></td>\n<td>Metadata for each partition in the topic.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Config</code></td>\n<td><code>map[string]string</code></td>\n<td>Topic-level configuration overrides.</td>\n</tr>\n<tr>\n<td><strong><code>PartitionMetadata</code></strong></td>\n<td><code>Topic</code></td>\n<td><code>string</code></td>\n<td>Topic name.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>PartitionID</code></td>\n<td><code>int</code></td>\n<td>Partition identifier.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>LeaderID</code></td>\n<td><code>int</code></td>\n<td>Broker ID of the current partition leader. <code>-1</code> if leader unknown.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ReplicaIDs</code></td>\n<td><code>[]int</code></td>\n<td>Ordered list of broker IDs hosting replicas.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ISR</code></td>\n<td><code>[]int</code></td>\n<td>Subset of <code>ReplicaIDs</code> currently in-sync.</td>\n</tr>\n<tr>\n<td><strong><code>ClusterMetadata</code></strong></td>\n<td><code>Brokers</code></td>\n<td><code>map[int]Broker</code></td>\n<td>Map of broker ID to broker details.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Topics</code></td>\n<td><code>map[string]TopicMetadata</code></td>\n<td>Map of topic name to topic metadata.</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ControllerID</code></td>\n<td><code>int</code></td>\n<td>Broker ID of the current controller (manages leadership elections and partition assignments).</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ClusterID</code></td>\n<td><code>string</code></td>\n<td>Unique string identifying the cluster (for multi-cluster routing).</td>\n</tr>\n</tbody></table>\n<h4 id=\"metadata-propagation-and-consistency\">Metadata Propagation and Consistency</h4>\n<p>Metadata is disseminated through a <strong>gossip-like protocol</strong> or via direct responses to client requests. When a client (producer/consumer) requests metadata (e.g., because of an unknown topic or a <code>NOT_LEADER</code> error), it contacts any broker, which returns the current <code>ClusterMetadata</code>. Brokers update their metadata upon leadership changes, broker failures, or topic creation.</p>\n<blockquote>\n<p><strong>Design Insight:</strong> Eventually consistent metadata is acceptable for our system. Clients may temporarily have stale metadata, leading them to send produce/fetch requests to the wrong broker. The receiving broker must reject such requests with a <code>NOT_LEADER</code> error, prompting the client to refresh its metadata. This pattern ensures correctness while avoiding the complexity of a strongly consistent metadata store in the initial implementation.</p>\n</blockquote>\n<h4 id=\"metadata-storage\">Metadata Storage</h4>\n<p>Cluster metadata itself must be persisted to survive controller failures. The controller (a designated broker) stores critical metadata in a durable <strong>metadata log</strong>, which could be a special internal topic (<code>__cluster_metadata</code>) or a replicated state machine (like Raft). For simplicity in early milestones, we can store metadata in a file on the controller broker, but this becomes a single point of failure.</p>\n<blockquote>\n<p><strong>ADR: Metadata Storage for Milestone 1-3</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a simple way to persist topic and partition assignments that is sufficient for the educational project before implementing full replication.</li>\n<li><strong>Options</strong>:<ol>\n<li><strong>In-memory only</strong>: No persistence; topics and assignments are lost on broker restart.</li>\n<li><strong>File on controller</strong>: Write metadata to a JSON or binary file on the controller broker&#39;s disk.</li>\n<li><strong>Replicated log</strong>: Use the same WAL/replication mechanism as data topics.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use a file on the controller broker (<code>metadata.json</code>).</li>\n<li><strong>Rationale</strong>: Simplicity and focus. The primary learning goal in early milestones is the data path (produce/consume), not metadata high availability. A file is easy to implement and provides basic durability for development.</li>\n<li><strong>Consequences</strong>: The controller is a single point of failure. If the controller crashes and its disk is lost, metadata is lost. This is acceptable for learning and can be upgraded to a replicated log in later milestones.</li>\n</ul>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete starter code and structure for implementing the data model in Go.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>On-Disk Record Format</td>\n<td>Custom binary format with <code>encoding/binary</code></td>\n<td>Use Protobuf/FlatBuffers for schema evolution</td>\n</tr>\n<tr>\n<td>Offset Storage</td>\n<td>File-per-broker (<code>offsets.json</code>)</td>\n<td>Internal compacted topic (<code>__consumer_offsets</code>)</td>\n</tr>\n<tr>\n<td>Metadata Storage</td>\n<td>JSON file on controller (<code>metadata.json</code>)</td>\n<td>Embedded Raft (hashicorp/raft) for consensus</td>\n</tr>\n<tr>\n<td>Index Access</td>\n<td>Read entire index into <code>[]byte</code>, binary search</td>\n<td>Memory-map index file (<code>mmap</code>) for zero-copy</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>build-your-own-kafka/\n├── internal/\n│   ├── types/                    # Core data structures\n│   │   ├── topic.go\n│   │   ├── partition.go\n│   │   ├── record.go\n│   │   ├── log.go\n│   │   └── metadata.go\n│   ├── storage/                  # On-disk formats and access\n│   │   ├── segment.go\n│   │   ├── index.go\n│   │   ├── wal.go               # Provided WAL wrapper\n│   │   └── offset_store.go\n│   └── protocol/                # Wire format encoding/decoding\n│       ├── record_batch.go\n│       ├── request_response.go\n│       └── encoding.go\n└── pkg/\n    └── server/                  # Broker server (uses types and storage)</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p>Here is a complete, ready-to-use Write-Ahead Log (WAL) wrapper for segment operations. This handles the low-level file appends and syncs, allowing the learner to focus on higher-level log management.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/wal.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WAL represents an append-only log file with optional fsync.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WAL</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filePath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\"> // current write offset in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OpenWAL opens or creates a WAL file at the given path.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> OpenWAL</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Dir</span><span style=\"color:#E1E4E8\">(path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(dir, </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"create wal dir: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">OpenFile</span><span style=\"color:#E1E4E8\">(path, os.O_CREATE</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_RDWR</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_APPEND, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"open wal file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stat, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"stat wal file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file:     file,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filePath: path,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        offset:   stat.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Append writes data to the WAL and optionally syncs to disk.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns the file offset at which data was written.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sync</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write length prefix (8 bytes) followed by data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> buf [</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint64</span><span style=\"color:#E1E4E8\">(buf[:], </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(data)))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf[:]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"write length: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(data); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"write data: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> sync {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Sync</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"sync: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    writtenAt </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.offset </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> writtenAt, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadAll reads all entries from the WAL.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">() ([][]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Seek</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"seek to start: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> entries [][]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> lengthBuf [</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(lengthBuf[:]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"EOF\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read length: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        length </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint64</span><span style=\"color:#E1E4E8\">(lengthBuf[:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, length)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(data); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read data: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entries </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(entries, data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entries, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CurrentOffset returns the current write offset in bytes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CurrentOffset</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> w.offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close closes the WAL file.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p>Below are skeleton implementations for the key data model types with TODOs that map to the design described above.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/types/record.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Record</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Headers   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRecord creates a new Record with the current timestamp if not provided.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRecord</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">headers</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Key:       key,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Value:     value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UnixMilli</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Headers:   headers,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/types/log.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Log</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Segments      []</span><span style=\"color:#B392F0\">LogSegment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BaseDir       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#B392F0\">LogConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SegmentMaxBytes </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IndexInterval   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Append writes a batch of records to the log, returning the starting offset.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">records</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if active segment exists and has space (l.Segments[len(l.Segments)-1].SizeBytes &#x3C; config.SegmentMaxBytes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If no space, roll a new segment (create new data and index files with BaseOffset = l.CurrentOffset)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Serialize records into a RecordBatch using the binary format (see protocol/record_batch.go)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write the batch to the active segment's data file (using WAL.Append with sync=true for durability)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If written bytes exceed IndexInterval, add an index entry (relative_offset, position) to the index file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update active segment's SizeBytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update l.CurrentOffset by len(records)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return the starting offset (previous CurrentOffset)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Read fetches records starting from the given offset, up to maxBytes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">startOffset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">maxBytes</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Locate the segment containing startOffset (binary search on Segments[i].BaseOffset)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Load the segment's index into memory (if not already cached)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use index to find the approximate byte position in the data file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Open data file, seek to position, and scan forward to find the batch with startOffset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Deserialize batches, collect records whose offset >= startOffset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Stop when accumulated size exceeds maxBytes or end of log</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/segment.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogSegment</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BaseOffset </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataFile   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IndexFile  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SizeBytes  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Index</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Index manages the sparse offset-to-position mapping.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Index</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entries []</span><span style=\"color:#B392F0\">IndexEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> IndexEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RelativeOffset </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Position       </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddEntry adds a new index entry when a batch is written at the given position.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">idx </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Index</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">offsetDelta</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">position</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Append entry to in-memory slice, periodically flush to index file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FindEntry returns the index entry with the largest relativeOffset &#x3C;= targetDelta.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">idx </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Index</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">FindEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">targetDelta</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">IndexEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Binary search on idx.entries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> IndexEntry</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Serialization</strong>: Use <code>encoding/binary</code> with <code>binary.BigEndian</code> for portability across networks. For variable-length integers (varint), implement a simple version or use <code>binary.PutVarint</code> (though note it&#39;s little-endian; you may need to adapt).</li>\n<li><strong>File I/O</strong>: Use <code>os.File</code> with <code>O_APPEND</code> for log segments. For fsync, call <code>file.Sync()</code> after writes when <code>acks=all</code>. Use <code>bufio.Writer</code> for batching file writes, but be careful to flush before sync.</li>\n<li><strong>Concurrency</strong>: Protect in-memory structures like <code>Log.Segments</code> and <code>Index.entries</code> with <code>sync.RWMutex</code>. Multiple goroutines can read concurrently, but writes must be exclusive.</li>\n<li><strong>Memory Management</strong>: Avoid loading entire segment files into memory. Use <code>io.ReaderAt</code> and <code>io.WriterAt</code> for random access. Consider memory-mapping index files (<code>mmap</code>) for efficient binary search.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint-data-model\">F. Milestone Checkpoint (Data Model)</h4>\n<p>To verify your data model implementation works:</p>\n<ol>\n<li>Run the unit tests for the <code>types</code> and <code>storage</code> packages:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/types/...</span><span style=\"color:#9ECBFF\"> ./internal/storage/...</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<ol start=\"2\">\n<li>Create a simple test that creates a <code>Log</code>, appends a few records, reads them back, and verifies offsets are sequential and content matches.</li>\n<li>Inspect the on-disk files: after appending, you should see <code>00000000000000000000.log</code> and <code>00000000000000000000.index</code> in your data directory. The <code>.log</code> file should contain the binary batch format; you can inspect it with a hex dump (<code>hexdump -C file.log</code>).</li>\n</ol>\n<h4 id=\"g-common-pitfalls\">G. Common Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Incorrect offset arithmetic leading to gaps or duplicates</strong></p>\n<ul>\n<li><strong>Description</strong>: When appending a batch of 5 records starting at offset 100, the next offset must be 105. A common mistake is incrementing by 1 instead of the batch size.</li>\n<li><strong>Why it&#39;s wrong</strong>: Consumers expect contiguous offsets. A gap causes them to stall waiting for the missing offset; duplicates cause reprocessing.</li>\n<li><strong>Fix</strong>: Maintain <code>CurrentOffset</code> as the <em>next</em> offset to assign. When a batch of N records is appended, the starting offset is <code>CurrentOffset</code>, and after append, <code>CurrentOffset += N</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not fsync&#39;ing when required by acknowledgment level</strong></p>\n<ul>\n<li><strong>Description</strong>: Writing to the file buffer without calling <code>Sync()</code> when <code>acks=all</code> can lose data if the OS crashes.</li>\n<li><strong>Why it&#39;s wrong</strong>: The producer receives an acknowledgment, but the data may not be on durable storage, violating durability guarantees.</li>\n<li><strong>Fix</strong>: In <code>WAL.Append</code>, conditionally call <code>file.Sync()</code> based on the <code>sync</code> parameter, which should be set according to the produce request&#39;s <code>acks</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Storing absolute offsets in index files</strong></p>\n<ul>\n<li><strong>Description</strong>: Using the full 8-byte offset in index entries instead of a 4-byte relative offset wastes space and limits segment size.</li>\n<li><strong>Why it&#39;s wrong</strong>: Index files become unnecessarily large, reducing the effectiveness of caching. Also, segment base offsets can be very large (e.g., after many writes), making 4-byte absolute offsets insufficient.</li>\n<li><strong>Fix</strong>: Store <code>relative_offset = actual_offset - segment_base_offset</code> as int32. Ensure segment rolling happens before <code>relative_offset</code> exceeds <code>math.MaxInt32</code> (i.e., segment size limit).</li>\n</ul>\n<h2 id=\"5-component-design-broker-and-topic-partitions\">5. Component Design: Broker and Topic Partitions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1 (Topic and Partitions)</p>\n</blockquote>\n<h3 id=\"51-responsibility-and-scope\">5.1 Responsibility and Scope</h3>\n<p>The <strong>Broker</strong> is the foundational server node that stores data and handles client requests. Think of it as a <strong>library branch</strong> in a distributed library system: it houses specific shelves (partitions) of books (messages), accepts new donations (producer writes), and allows patrons (consumers) to check out books for reading. Each broker operates independently but coordinates with others to form a complete, fault-tolerant cluster.</p>\n<p>The broker&#39;s primary responsibilities are:</p>\n<ol>\n<li><strong>Log Storage</strong>: Maintain an immutable, append-only log for each partition it hosts, physically persisted to disk with configurable durability guarantees.</li>\n<li><strong>Request Handling</strong>: Serve <strong>Produce</strong> and <strong>Fetch</strong> requests from producers and consumers over a binary TCP protocol, ensuring correct ordering and offset tracking.</li>\n<li><strong>Topic Management</strong>: Create and manage topics with their partitions, including partition assignment to brokers and metadata distribution.</li>\n<li><strong>Local Offset Tracking</strong>: Store and retrieve consumer group offsets for partitions hosted locally (before replication, this is simply local file storage).</li>\n<li><strong>Coordination Participation</strong>: Work with the cluster coordinator (which may be a designated broker) for leader election and metadata synchronization.</li>\n</ol>\n<p>The scope of this component in Milestone 1 is intentionally bounded: we implement a <strong>single broker</strong> that can manage multiple topics and partitions locally, without yet handling replication or distributed coordination. This allows learners to solidify the core log storage and request handling patterns before adding complexity.</p>\n<h3 id=\"52-mental-model-the-immutable-ledger\">5.2 Mental Model: The Immutable Ledger</h3>\n<p>Imagine a traditional <strong>bank ledger book</strong> used to record transactions in chronological order. Each page represents a <strong>segment</strong> of the log, and each line entry is a <strong>record</strong> with a sequential number (offset). The ledger has these key properties:</p>\n<ul>\n<li><strong>Append-Only</strong>: You never erase or modify past entries; you only add new entries at the end. If a mistake occurs, you add a correction entry rather than changing the original.</li>\n<li><strong>Immutable History</strong>: Once written, entries become permanent history. This simplifies concurrency—readers can access any part of the ledger without blocking writers.</li>\n<li><strong>Sequential Offsets</strong>: Each entry gets a monotonically increasing number (0, 1, 2...), which serves as a unique identifier and position indicator within that specific ledger.</li>\n<li><strong>Partitioned Ledgers</strong>: Instead of one giant ledger for all transactions, the bank maintains separate ledger books for different account types (checking, savings). This is <strong>partitioning</strong>—each ledger (partition) maintains its own independent sequence.</li>\n</ul>\n<p>In our system, each <strong>partition</strong> is exactly such a ledger. The broker&#39;s primary job is to <strong>guard these ledgers</strong>: ensure writes are appended correctly in order, assign proper offsets, and allow readers to efficiently locate entries by their offset number.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The immutable log abstraction is the core innovation of Kafka-like systems. By treating messages as an ordered, append-only sequence rather than transient queues, we gain durability, replayability, and strong ordering guarantees that enable event sourcing and stream processing.</p>\n</blockquote>\n<h3 id=\"53-public-interface-apis\">5.3 Public Interface (APIs)</h3>\n<p>The broker exposes its functionality through a binary RPC protocol over TCP. While the full wire protocol will be detailed in Section 9, we define the logical API methods here. Clients (producers, consumers, administrative tools) interact with the broker through these request/response pairs.</p>\n<h4 id=\"core-broker-rpc-methods\">Core Broker RPC Methods</h4>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CreateTopic</code></td>\n<td><code>name</code> (string), <code>partitionCount</code> (int), <code>replicationFactor</code> (int), <code>config</code> (map[string]string)</td>\n<td><code>error</code></td>\n<td>Creates a new topic with the specified number of partitions. In Milestone 1, <code>replicationFactor</code> is ignored (set to 1). Initializes empty logs for each partition and updates cluster metadata.</td>\n</tr>\n<tr>\n<td><code>Produce</code></td>\n<td><code>topic</code> (string), <code>partition</code> (int), <code>records</code> ([]<code>Record</code>), <code>requiredAcks</code> (int), <code>timeout</code> (int32)</td>\n<td><code>baseOffset</code> (int64), <code>error</code></td>\n<td>Appends a batch of records to the specified partition&#39;s log. Returns the offset assigned to the first record in the batch. <code>requiredAcks</code> controls durability (0=no ack, 1=leader ack, -1=all ISR ack).</td>\n</tr>\n<tr>\n<td><code>Fetch</code></td>\n<td><code>topic</code> (string), <code>partition</code> (int), <code>offset</code> (int64), <code>maxBytes</code> (int32)</td>\n<td><code>records</code> ([]<code>Record</code>), <code>error</code></td>\n<td>Retrieves records from the partition log starting at <code>offset</code>. Returns up to <code>maxBytes</code> worth of records. If the requested offset is beyond the log end, waits for new data up to a timeout.</td>\n</tr>\n<tr>\n<td><code>GetOffsets</code></td>\n<td><code>topic</code> (string), <code>partition</code> (int), <code>time</code> (int64)</td>\n<td><code>offsets</code> ([]int64), <code>error</code></td>\n<td>Returns offset metadata for a partition. <code>time</code> can be -1 (earliest), -2 (latest), or a timestamp. Used by consumers to find starting positions.</td>\n</tr>\n<tr>\n<td><code>CommitOffsets</code></td>\n<td><code>group</code> (string), <code>topic</code> (string), <code>partition</code> (int), <code>offset</code> (int64), <code>metadata</code> (string)</td>\n<td><code>error</code></td>\n<td>Commits a consumer group&#39;s offset for a specific partition. Stores the offset locally (later, in an internal <code>__consumer_offsets</code> topic).</td>\n</tr>\n<tr>\n<td><code>FetchOffsets</code></td>\n<td><code>group</code> (string), <code>topic</code> (string), <code>partition</code> (int)</td>\n<td><code>offset</code> (int64), <code>metadata</code> (string), <code>error</code></td>\n<td>Retrieves the last committed offset for a consumer group and partition.</td>\n</tr>\n</tbody></table>\n<h4 id=\"internal-management-apis-used-by-other-brokerscoordinators\">Internal Management APIs (Used by Other Brokers/Coordinators)</h4>\n<p>These methods are not directly called by external clients but are used during replication and coordination (Milestone 4).</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>LeaderForPartition</code></td>\n<td><code>topic</code> (string), <code>partition</code> (int)</td>\n<td><code>brokerID</code> (int), <code>error</code></td>\n<td>Returns the broker ID currently acting as leader for the partition. Used by clients for request routing.</td>\n</tr>\n<tr>\n<td><code>UpdateISR</code></td>\n<td><code>topic</code> (string), <code>partition</code> (int), <code>isr</code> ([]int)</td>\n<td><code>error</code></td>\n<td>Updates the In-Sync Replica set for a partition. Called by the controller after replication changes.</td>\n</tr>\n<tr>\n<td><code>FetchReplica</code></td>\n<td><code>topic</code> (string), <code>partition</code> (int), <code>offset</code> (int64), <code>maxBytes</code> (int32)</td>\n<td><code>records</code> ([]<code>Record</code>), <code>error</code></td>\n<td>Similar to <code>Fetch</code> but used by follower brokers to replicate data from the leader. May include additional replication metadata.</td>\n</tr>\n</tbody></table>\n<h4 id=\"state-transition-table-for-partition-leadership\">State Transition Table for Partition Leadership</h4>\n<p>As the broker manages partitions, each partition can be in different states regarding leadership and replication. This state machine becomes relevant in Milestone 4 but is introduced here for completeness.</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Offline</code></td>\n<td><code>CreateTopic</code> command received</td>\n<td><code>Online</code> (Follower)</td>\n<td>Initialize empty log, set <code>LeaderBrokerID</code> to self, set <code>ISR</code> = [self], start log appender</td>\n</tr>\n<tr>\n<td><code>Online</code> (Leader)</td>\n<td>Broker receives <code>LeaderAndIsr</code> request with leader = self</td>\n<td><code>Online</code> (Leader)</td>\n<td>Begin accepting produce requests, start tracking ISR, initialize high watermark to LEO</td>\n</tr>\n<tr>\n<td><code>Online</code> (Leader)</td>\n<td>Broker receives <code>LeaderAndIsr</code> request with leader != self</td>\n<td><code>Online</code> (Follower)</td>\n<td>Stop accepting produce requests, start fetching from new leader, clear ISR list</td>\n</tr>\n<tr>\n<td><code>Online</code> (Follower)</td>\n<td>Fetched offset catches up to leader&#39;s LEO</td>\n<td><code>Online</code> (Follower in ISR)</td>\n<td>Broker adds itself to leader&#39;s ISR via <code>UpdateISR</code> request</td>\n</tr>\n<tr>\n<td>Any <code>Online</code> state</td>\n<td>Broker shutdown initiated</td>\n<td><code>Offline</code></td>\n<td>Close log files, persist final offsets, notify coordinator</td>\n</tr>\n</tbody></table>\n<h3 id=\"54-internal-behavior-log-management\">5.4 Internal Behavior: Log Management</h3>\n<p>The broker&#39;s most critical internal subsystem is the <strong>log manager</strong>, which orchestrates the physical storage and retrieval of records for all partitions. Its algorithm ensures atomic appends, efficient reads, and proper log rolling.</p>\n<h4 id=\"data-structures-for-log-management\">Data Structures for Log Management</h4>\n<p>The broker maintains the following core in-memory structures (as defined in Section 4):</p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Server</code></td>\n<td><code>topics</code></td>\n<td><code>map[string]*Topic</code></td>\n<td>Registry of all topics known to this broker, keyed by topic name.</td>\n</tr>\n<tr>\n<td><code>Server</code></td>\n<td><code>logManager</code></td>\n<td><code>*LogManager</code></td>\n<td>Central coordinator for all partition logs, handling segment rolling and cleanup.</td>\n</tr>\n<tr>\n<td><code>Topic</code></td>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Unique topic identifier.</td>\n</tr>\n<tr>\n<td><code>Topic</code></td>\n<td><code>Partitions</code></td>\n<td><code>[]Partition</code></td>\n<td>List of partitions belonging to this topic.</td>\n</tr>\n<tr>\n<td><code>Partition</code></td>\n<td><code>Topic</code></td>\n<td><code>string</code></td>\n<td>Topic name this partition belongs to.</td>\n</tr>\n<tr>\n<td><code>Partition</code></td>\n<td><code>ID</code></td>\n<td><code>int</code></td>\n<td>Partition number within the topic (0-indexed).</td>\n</tr>\n<tr>\n<td><code>Partition</code></td>\n<td><code>LeaderBrokerID</code></td>\n<td><code>int</code></td>\n<td>ID of broker currently leading this partition (self for local partitions).</td>\n</tr>\n<tr>\n<td><code>Partition</code></td>\n<td><code>Log</code></td>\n<td><code>Log</code></td>\n<td>The actual log instance containing segments and current offset.</td>\n</tr>\n<tr>\n<td><code>Partition</code></td>\n<td><code>HighWatermark</code></td>\n<td><code>int64</code></td>\n<td>Offset up to which all ISR replicas have replicated (for consumers).</td>\n</tr>\n<tr>\n<td><code>Partition</code></td>\n<td><code>LogEndOffset</code></td>\n<td><code>int64</code></td>\n<td>The offset of the next message to be appended (LEO).</td>\n</tr>\n<tr>\n<td><code>Log</code></td>\n<td><code>Segments</code></td>\n<td><code>[]LogSegment</code></td>\n<td>Ordered list of log segment files, from oldest to newest.</td>\n</tr>\n<tr>\n<td><code>Log</code></td>\n<td><code>CurrentOffset</code></td>\n<td><code>int64</code></td>\n<td>The next offset to be assigned (equal to LEO).</td>\n</tr>\n<tr>\n<td><code>Log</code></td>\n<td><code>BaseDir</code></td>\n<td><code>string</code></td>\n<td>Directory path where segment files are stored.</td>\n</tr>\n<tr>\n<td><code>LogSegment</code></td>\n<td><code>BaseOffset</code></td>\n<td><code>int64</code></td>\n<td>Offset of the first record in this segment.</td>\n</tr>\n<tr>\n<td><code>LogSegment</code></td>\n<td><code>DataFile</code></td>\n<td><code>string</code></td>\n<td>Path to the <code>.log</code> file containing record batches.</td>\n</tr>\n<tr>\n<td><code>LogSegment</code></td>\n<td><code>IndexFile</code></td>\n<td><code>string</code></td>\n<td>Path to the <code>.index</code> file mapping offsets to byte positions.</td>\n</tr>\n<tr>\n<td><code>LogSegment</code></td>\n<td><code>SizeBytes</code></td>\n<td><code>int64</code></td>\n<td>Current size of the data file in bytes.</td>\n</tr>\n</tbody></table>\n<h4 id=\"algorithm-appending-records-to-a-partition-log\">Algorithm: Appending Records to a Partition Log</h4>\n<p>When a <code>Produce</code> request arrives for a valid partition, the broker executes this append algorithm:</p>\n<ol>\n<li><strong>Validate Request</strong>: Check that the partition exists, the broker is the leader (for now, assume true), and the request size is within configured limits.</li>\n<li><strong>Assign Offsets</strong>: For each record in the incoming batch, assign a sequential offset starting from the partition&#39;s <code>LogEndOffset</code>. The first record gets offset = <code>LogEndOffset</code>, the second gets <code>LogEndOffset + 1</code>, etc.</li>\n<li><strong>Serialize Batch</strong>: Encode the records along with their assigned offsets, timestamps, and headers into the binary <strong>RecordBatch</strong> format (see Section 9.2).</li>\n<li><strong>Append to Active Segment</strong>: Write the serialized bytes to the current active segment&#39;s data file. The write is performed using the <code>WAL.Append</code> method which ensures atomic appends.</li>\n<li><strong>Update Index</strong>: For every N records (configurable <code>IndexInterval</code>), add an entry to the segment&#39;s sparse index mapping the offset delta to the byte position in the data file.</li>\n<li><strong>Update In-Memory State</strong>: Increment the partition&#39;s <code>LogEndOffset</code> by the number of records appended. If <code>requiredAcks</code> is 1 or -1, also update the <code>HighWatermark</code> (for now, set equal to <code>LogEndOffset</code>).</li>\n<li><strong>Conditional Sync</strong>: If <code>requiredAcks</code> is -1 (all ISR), call <code>fsync</code> on the data file to flush OS buffers to disk. For acks=1, we may defer sync for performance.</li>\n<li><strong>Roll Segment if Needed</strong>: If the active segment&#39;s <code>SizeBytes</code> exceeds <code>SegmentMaxBytes</code> (e.g., 1 GB), close it and create a new segment with <code>BaseOffset</code> = the new <code>LogEndOffset</code>.</li>\n<li><strong>Return Response</strong>: Send the <code>baseOffset</code> (the offset assigned to the first record) back to the producer.</li>\n</ol>\n<h4 id=\"algorithm-reading-records-from-a-partition-log\">Algorithm: Reading Records from a Partition Log</h4>\n<p>When a <code>Fetch</code> request arrives, the broker must efficiently locate and return records:</p>\n<ol>\n<li><strong>Validate Offset</strong>: Check that the requested offset is between the partition&#39;s earliest offset (base offset of oldest segment) and <code>LogEndOffset</code>. If offset = <code>LogEndOffset</code>, optionally wait for new data.</li>\n<li><strong>Locate Segment</strong>: Perform binary search on the partition&#39;s <code>Segments</code> slice to find the segment where <code>segment.BaseOffset &lt;= targetOffset &lt; nextSegment.BaseOffset</code>.</li>\n<li><strong>Consult Index</strong>: Load the segment&#39;s index (if not already in memory). Use <code>Index.FindEntry</code> to find the largest index entry with offset &lt;= targetOffset, giving a byte position in the data file.</li>\n<li><strong>Seek and Scan</strong>: Open the data file, seek to the byte position, and read forward until reaching <code>targetOffset</code>. Since records are stored in batches, we may need to scan through a batch to find the exact starting record.</li>\n<li><strong>Collect Records</strong>: Read sequential records until either hitting <code>maxBytes</code> limit, reaching end of segment, or exceeding a maximum record count.</li>\n<li><strong>Return Results</strong>: Package the records into a <code>FetchResponse</code>. Include the <code>HighWatermark</code> so consumers know the safe read limit.</li>\n</ol>\n<h4 id=\"log-segment-rolling-and-cleanup\">Log Segment Rolling and Cleanup</h4>\n<p>The log manager periodically checks segments and performs maintenance:</p>\n<ol>\n<li><strong>Size-Based Rolling</strong>: After each append, if the active segment exceeds <code>SegmentMaxBytes</code>, it is rolled.</li>\n<li><strong>Time-Based Rolling</strong>: A background thread checks segments older than <code>SegmentRollTime</code> (e.g., 7 days) and rolls the active segment if it&#39;s too old.</li>\n<li><strong>Deletion Policy</strong>: For simplicity, we implement a <strong>retention-based cleanup</strong>. A background task runs every 5 minutes:<ul>\n<li>For each partition, list segments sorted by <code>BaseOffset</code>.</li>\n<li>If total log size exceeds <code>RetentionBytes</code> or the oldest segment&#39;s timestamp is older than <code>RetentionTime</code>, delete the oldest segment file and its index.</li>\n<li>Update the <code>Segments</code> slice accordingly.</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p><strong>Design Insight</strong>: Segment-based storage (rather than one giant file per partition) enables efficient old data deletion and parallel I/O. The sparse index trades minor CPU overhead during reads for significant disk space savings, as we only index every N records.</p>\n</blockquote>\n<h3 id=\"55-adr-log-storage-backend\">5.5 ADR: Log Storage Backend</h3>\n<blockquote>\n<p><strong>Decision: File-Based Append-Only Segments with Sparse Indexing</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a persistent storage engine for partition logs that provides high-throughput sequential writes, efficient random reads by offset, and easy deletion of old data. The solution must be simple to implement and understand for educational purposes while being performant enough for the core use case.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Simple file-based append-only logs with custom sparse indexing</strong> (Kafka&#39;s approach)</li>\n<li><strong>Embedded key-value store</strong> (e.g., LevelDB/RocksDB, SQLite)</li>\n<li><strong>Memory-mapped files with byte-addressable offset indexing</strong></li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Option 1 — file-based append-only logs with sparse indexing.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Conceptual Simplicity</strong>: The mental model of &quot;segments as files&quot; maps directly to the immutable ledger analogy, making it easier for learners to debug and reason about.</li>\n<li><strong>Predictable Performance</strong>: Sequential disk writes are the fastest possible I/O pattern, and reads are mostly sequential scans after index lookup. This matches Kafka&#39;s proven performance profile.</li>\n<li><strong>Explicit Control</strong>: We avoid the complexity and black-box nature of an embedded database. Learners understand exactly when data is persisted, indexed, and deleted.</li>\n<li><strong>Ease of Inspection</strong>: Log segments are plain files viewable with hex editors or custom tools, aiding debugging and education.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Excellent write throughput, straightforward implementation, easy to add compression or encryption per segment.</li>\n<li><strong>Negative</strong>: Manual management of file handles and indexes required. Need to implement our own recovery logic after crashes. No built-in transactions—we must ensure atomic batch writes ourselves.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>File-based segments</strong></td>\n<td>Maximum write throughput, simple mental model, easy inspection and debugging, matches Kafka&#39;s design for direct learning</td>\n<td>Manual file management, need to implement own indexing and recovery, concurrent access requires careful locking</td>\n<td><strong>Yes</strong> - best for educational goals and performance</td>\n</tr>\n<tr>\n<td><strong>Embedded KV store</strong></td>\n<td>Built-in recovery, concurrent access handled, potentially simpler API, good for prototyping</td>\n<td>Black-box behavior, harder to debug, often optimized for random writes not sequential appends, adds dependency</td>\n<td>No - obscures learning objectives</td>\n</tr>\n<tr>\n<td><strong>Memory-mapped files</strong></td>\n<td>Excellent read performance, OS handles paging, simple offset-to-address arithmetic</td>\n<td>Complex to handle file growth, portability concerns, risk of SIGBUS on truncated files, still need custom index</td>\n<td>No - increased complexity without sufficient benefit</td>\n</tr>\n</tbody></table>\n<h3 id=\"56-common-pitfalls\">5.6 Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Forgetting to fsync or syncing too often</strong></p>\n<p><strong>Description</strong>: After writing records to a file, developers may omit calling <code>file.Sync()</code> (or equivalent) to flush OS buffers to disk. Conversely, calling sync after every single write destroys throughput.</p>\n<p><strong>Why it&#39;s wrong</strong>: Without fsync, a broker crash can lose recently &quot;acknowledged&quot; messages (if acks=1 or all), violating durability guarantees. With excessive fsync, throughput drops to disk seek speed (often &lt; 1000 writes/sec).</p>\n<p><strong>How to fix</strong>: Implement a <strong>sync policy</strong> based on <code>requiredAcks</code> and batch size. For acks=1, sync periodically (e.g., every 100ms or 1MB of data). For acks=-1 (all ISR), sync immediately after each write. Use the <code>WAL.Append</code> method&#39;s <code>sync</code> parameter to control this.</p>\n<hr>\n<p>⚠️ <strong>Pitfall: Incorrect offset gaps or reuse</strong></p>\n<p><strong>Description</strong>: Assigning non-sequential offsets (e.g., skipping numbers) or reusing offsets after a crash recovery.</p>\n<p><strong>Why it&#39;s wrong</strong>: Consumers rely on strictly monotonic offsets for progress tracking. Gaps confuse offset commit logic and may cause infinite loops. Reused offsets cause duplicate processing and break idempotency.</p>\n<p><strong>How to fix</strong>: Always derive new offsets from <code>LogEndOffset</code>, which must be persisted (in the segment file names or a checkpoint file). On startup, scan segments to reconstruct the exact <code>LogEndOffset</code>. Never decrement it.</p>\n<hr>\n<p>⚠️ <strong>Pitfall: Poor key hashing leading to partition skew</strong></p>\n<p><strong>Description</strong>: Using a naive hash function (like Java&#39;s <code>Object.hashCode()</code> equivalent) or not handling null keys properly, causing uneven distribution across partitions.</p>\n<p><strong>Why it&#39;s wrong</strong>: Skewed partition distribution defeats the purpose of partitioning—one partition becomes a hotspot while others are underutilized, limiting scalability.</p>\n<p><strong>How to fix</strong>: Use a proven hash function (e.g., MurmurHash2/3) and always apply a modulo operation with the number of partitions. For null keys, use a round-robin or random partition assignment to spread load.</p>\n<hr>\n<p>⚠️ <strong>Pitfall: Not handling concurrent reads and writes</strong></p>\n<p><strong>Description</strong>: Allowing simultaneous read and write operations on the same log segment without proper synchronization, leading to corrupted reads or panics.</p>\n<p><strong>Why it&#39;s wrong</strong>: Concurrent reads may see partially written records or incorrect offsets. File descriptors may be closed while in use.</p>\n<p><strong>How to fix</strong>: Use a <strong>reader-writer lock</strong> per partition. Multiple fetches can read concurrently, but an append acquires an exclusive lock. For higher throughput, consider copy-on-write semantics for the active segment.</p>\n<hr>\n<p>⚠️ <strong>Pitfall: Infinite memory growth from unclosed segments</strong></p>\n<p><strong>Description</strong>: Keeping all segment file handles and index data in memory forever, causing memory exhaustion as the log grows.</p>\n<p><strong>Why it&#39;s wrong</strong>: A long-running broker will eventually run out of memory and crash, especially with many partitions.</p>\n<p><strong>How to fix</strong>: Implement an <strong>LRU cache</strong> for index data. Close file handles for inactive segments (not the active one). Reopen files on demand when reads target old segments.</p>\n<hr>\n<p>⚠️ <strong>Pitfall: Ignoring disk space exhaustion</strong></p>\n<p><strong>Description</strong>: Appending records without checking available disk space, causing write failures that may corrupt the log.</p>\n<p><strong>Why it&#39;s wrong</strong>: When disk is full, writes may partially succeed, leaving torn records. The broker might crash inconsistently.</p>\n<p><strong>How to fix</strong>: Before each append, check free space against a configurable threshold (e.g., 1GB). If below threshold, reject writes with an appropriate error. Implement alerting for operators.</p>\n<h3 id=\"57-implementation-guidance\">5.7 Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log Storage</td>\n<td>Plain files with <code>os.File</code>, manual indexing</td>\n<td>Memory-mapped files (<code>mmap</code>) with atomic appends</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td><code>sync.RWMutex</code> per partition</td>\n<td>Lock-free ring buffer for appends, RCU for reads</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>Custom binary format with <code>encoding/binary</code></td>\n<td>Protocol Buffers or FlatBuffers for schema evolution</td>\n</tr>\n<tr>\n<td>File Operations</td>\n<td>Direct <code>os.Open</code>/<code>Close</code> per operation</td>\n<td>Pooled file descriptors with reference counting</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>build-your-own-kafka/\n├── cmd/\n│   ├── broker/\n│   │   └── main.go                 # Broker entry point\n│   └── client/                     # Producer/consumer CLI tools\n├── internal/\n│   ├── broker/\n│   │   ├── server.go              # Main Server struct and lifecycle\n│   │   ├── handler.go             # Request handling (Produce, Fetch, etc.)\n│   │   └── log_manager.go         # LogManager orchestrating partitions\n│   ├── storage/\n│   │   ├── log.go                 # Partition Log implementation\n│   │   ├── segment.go             # LogSegment with index\n│   │   ├── wal.go                 # Write-ahead log wrapper (provided below)\n│   │   └── index.go               # Sparse index implementation\n│   ├── types/\n│   │   ├── topic.go               # Topic and Partition structs\n│   │   ├── record.go              # Record and RecordBatch\n│   │   └── metadata.go            # Broker, ClusterMetadata\n│   └── protocol/\n│       ├── request.go             # Request structs and parsing\n│       └── response.go            # Response building\n└── pkg/\n    └── wal/                       # Reusable WAL package (could be external)</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-wal-wrapper\">C. Infrastructure Starter Code: WAL Wrapper</h4>\n<p>Below is a complete, ready-to-use Write-Ahead Log wrapper. This handles atomic appends with optional fsync, which is crucial for durability. Learners can import this directly.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/wal.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WAL implements a simple write-ahead log with atomic appends.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WAL</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filePath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\"> // current write offset in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OpenWAL opens or creates a WAL file at the given path.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> OpenWAL</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">OpenFile</span><span style=\"color:#E1E4E8\">(path, os.O_RDWR</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_CREATE</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_APPEND, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to open WAL file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stat, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to stat WAL file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file:     file,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filePath: path,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        offset:   stat.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Append writes data to the WAL with optional fsync.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns the file offset at which data was written.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sync</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"write failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">!=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"short write: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> != </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, n, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.offset </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(n)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> sync {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Sync</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // We still wrote the data, but sync failed.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // The file system may have buffered it; we can't guarantee durability.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> offset, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"fsync failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> offset, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadAll reads all entries from the WAL (for recovery).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">() ([][]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Seek</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"seek failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> entries [][]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buffer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4096</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        n, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(buffer)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // In a real implementation, you'd need to parse record boundaries.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // For simplicity, we assume each write is a complete entry.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, n)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            copy</span><span style=\"color:#E1E4E8\">(entry, buffer[:n])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entries </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(entries, entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"EOF\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> entries, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read error: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entries, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CurrentOffset returns the current write offset in bytes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CurrentOffset</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> w.offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close closes the WAL file.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p>Below are skeleton implementations for the key components that learners should fill in. Each TODO corresponds to a step in the algorithms described in Section 5.4.</p>\n<p><strong>Log Manager</strong> (orchestrates all partition logs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/log.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Log represents a partition's log composed of multiple segments.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Log</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Segments      []</span><span style=\"color:#B392F0\">LogSegment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BaseDir       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#B392F0\">LogConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogConfig holds configuration for log behavior.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SegmentMaxBytes </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IndexInterval   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionBytes  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Append writes a batch of records to the log and returns the starting offset.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">records</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> l.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate input - ensure records slice is not empty</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Assign sequential offsets starting from CurrentOffset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         For each record, set Record.Offset = CurrentOffset + i</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Serialize records into RecordBatch binary format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         Use encodeRecordBatch() helper (to be implemented)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Get active segment (create first segment if none exists)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         activeSegment := l.getOrCreateActiveSegment()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Write batch bytes to segment data file using WAL.Append</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         Call with sync=false for now (we'll add sync policy later)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update segment size and add index entry if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         if recordsWritten % IndexInterval == 0, add index entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update CurrentOffset by adding len(records)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Check if segment should be rolled (size > SegmentMaxBytes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         if yes, call l.rollSegment()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Return the starting offset (CurrentOffset before increment)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Read fetches records starting from the given offset.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">startOffset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">maxBytes</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> l.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate startOffset is within range [oldestSegment.BaseOffset, CurrentOffset]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Locate segment containing startOffset using binary search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         segment := l.findSegment(startOffset)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Load segment index (lazy load if not in memory)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Use index to find approximate position in data file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         position := segment.Index.FindEntry(offsetDelta)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Open data file, seek to position, and scan forward to startOffset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Read records sequentially until hitting maxBytes or segment end</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Decode RecordBatch(es) into []*Record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return records (may be empty if startOffset == CurrentOffset)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// getOrCreateActiveSegment returns the last segment if it has space, else creates new.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getOrCreateActiveSegment</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogSegment</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: If Segments is empty, create first segment with BaseOffset = CurrentOffset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get last segment, if its SizeBytes &#x3C; config.SegmentMaxBytes, return it</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Otherwise, roll a new segment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">rollSegment</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// rollSegment closes the current active segment and creates a new one.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">rollSegment</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogSegment</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: If there's an active segment, close its files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create new segment with BaseOffset = CurrentOffset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Generate filenames: {BaseOffset}.log and {BaseOffset}.index</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Open data file and index file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Append new segment to l.Segments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return the new segment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Broker Request Handler</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/broker/handler.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> broker</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/byok/internal/protocol</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/byok/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/byok/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleProduce processes a Produce request from a client.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleProduce</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ProduceRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ProduceResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate topic exists (look up in s.topics map)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate partition exists and this broker is leader</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         partition := topic.Partitions[req.Partition]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Convert protocol.RecordBatch to []*types.Record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call partition.Log.Append(records)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle requiredAcks:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - If 0: return immediately with success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - If 1: wait for append to complete (already done), return</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - If -1: (future) wait for ISR replication</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Build response with base offset and any errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update partition.HighWatermark (for now = LogEndOffset)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ProduceResponse</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BaseOffset: </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleFetch processes a Fetch request from a consumer.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleFetch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FetchRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FetchResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate topic and partition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check requested offset against partition.HighWatermark</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         Consumers should not read beyond HighWatermark</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call partition.Log.Read(startOffset, maxBytes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Convert []*types.Record to protocol.RecordBatch for response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Include HighWatermark in response so consumer knows safe read limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FetchResponse</span><span style=\"color:#E1E4E8\">{}, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>File Sync</strong>: Use <code>file.Sync()</code> for durability. On Linux, this maps to the <code>fsync()</code> system call.</li>\n<li><strong>Binary Encoding</strong>: Use <code>encoding/binary</code> with <code>binary.BigEndian</code> for network-portable format (Kafka uses big-endian).</li>\n<li><strong>Concurrent Maps</strong>: Use <code>sync.RWMutex</code> to protect the <code>topics</code> map. Consider <code>sync.Map</code> for read-heavy workloads once initialized.</li>\n<li><strong>Path Operations</strong>: Always use <code>filepath.Join()</code> instead of string concatenation for cross-platform compatibility.</li>\n<li><strong>Error Handling</strong>: Use <code>fmt.Errorf(&quot;... %w&quot;, err)</code> to wrap errors with context for debugging.</li>\n<li><strong>Clean Resource Management</strong>: Implement <code>Close()</code> methods for all resources (files, network connections) and use <code>defer</code> appropriately.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the broker core and log storage:</p>\n<ol>\n<li><strong>Run the test suite</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   go test ./internal/storage/... -v\n   go test ./internal/broker/... -v</code></pre></div>\n\n<ol start=\"2\">\n<li><p><strong>Expected output</strong>: All tests should pass, demonstrating:</p>\n<ul>\n<li>Log appends return sequential offsets</li>\n<li>Reading from an offset returns correct records</li>\n<li>Segment rolling occurs when size limit reached</li>\n<li>Topic creation creates appropriate directory structure</li>\n</ul>\n</li>\n<li><p><strong>Manual verification</strong>:</p>\n<ul>\n<li>Start the broker: <code>go run cmd/broker/main.go --data-dir /tmp/byok</code></li>\n<li>Use a simple CLI producer (to be built in Milestone 2) or <code>netcat</code> to send a Produce request:</li>\n</ul>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>     echo -n &quot;test message&quot; | nc localhost 9092</code></pre></div>\n<ul>\n<li>Check that a segment file appears in <code>/tmp/byok/topics/test/0/</code></li>\n<li>Verify offset tracking by reading the log via a simple dump utility.</li>\n</ul>\n<ol start=\"4\">\n<li><strong>Signs of trouble</strong>:<ul>\n<li><strong>&quot;No such file or directory&quot;</strong>: Check that <code>BaseDir</code> is created with <code>os.MkdirAll</code>.</li>\n<li><strong>Offsets not sequential</strong>: Ensure <code>CurrentOffset</code> is updated atomically with the write.</li>\n<li><strong>High memory usage</strong>: You might be keeping all segment files open; implement LRU closing.</li>\n<li><strong>Test hangs</strong>: Check for deadlocks—use <code>go test -race</code> to detect data races.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"6-component-design-producer\">6. Component Design: Producer</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 2 (Producer)</p>\n</blockquote>\n<h3 id=\"61-responsibility-and-scope\">6.1 Responsibility and Scope</h3>\n<p>The <strong>Producer</strong> is a client library that applications use to publish (write) records to topics. Its primary responsibility is to take messages from the application, efficiently route them to the correct partition leader, and ensure they are durably written according to the configured acknowledgment semantics. It shields the application from the complexities of the distributed system, such as broker discovery, leader changes, network retries, and batching for performance.</p>\n<p>The Producer&#39;s scope encompasses:</p>\n<ol>\n<li><strong>Message Serialization:</strong> Converting application-provided keys, values, and headers into the binary <code>Record</code> and <code>RecordBatch</code> format understood by brokers.</li>\n<li><strong>Topic Metadata Management:</strong> Discovering which broker leads each partition of a topic, and refreshing this information when leadership changes.</li>\n<li><strong>Partition Selection:</strong> Determining the target partition for a message, using key-based hashing for ordering guarantees or round-robin for load distribution.</li>\n<li><strong>Batching and Accumulation:</strong> Grouping multiple messages destined for the same partition leader into single network requests to amortize overhead and achieve high throughput.</li>\n<li><strong>Reliability Semantics:</strong> Implementing configurable acknowledgment levels (0, 1, <code>all</code>) and corresponding retry logic with exponential backoff to provide at-least-once delivery guarantees.</li>\n<li><strong>Connection Management:</strong> Maintaining efficient, persistent network connections to broker leaders and handling connection failures.</li>\n</ol>\n<p>Crucially, the Producer is stateless regarding message content; it does not store messages durably itself. Its state is limited to in-flight message batches, metadata caches, and retry counters.</p>\n<h3 id=\"62-mental-model-the-postal-batch-sorter\">6.2 Mental Model: The Postal Batch Sorter</h3>\n<p>Imagine you run a large postal service. Every day, individuals (applications) bring you millions of letters (messages) to send. Each letter has a destination city and street (topic and key). Your goal is to deliver them reliably and quickly, but sending each letter individually by courier is prohibitively expensive.</p>\n<p>You set up a system of sorting bins:</p>\n<ol>\n<li><strong>Sorting by Destination:</strong> You first look up the correct regional sorting facility for each city (broker leader for a partition). Letters for the same facility go into the same large bin (<code>RecordBatch</code>).</li>\n<li><strong>Batch Dispatch:</strong> Once a bin is full or a timer expires, you seal it and send the entire bin via a single truck (network request) to the regional facility. This is far more efficient than sending individual letters.</li>\n<li><strong>Delivery Receipts:</strong> For important letters, you request a signed receipt (<code>ack</code>). If the receipt doesn&#39;t arrive, you put a copy of the letter in a new bin and try again (retry). For less critical mail, you might just trust the postal system (<code>acks=0</code>).</li>\n<li><strong>Route Changes:</strong> If a regional facility tells you they&#39;re no longer handling a certain street, you update your destination map (metadata) and re-route future letters accordingly.</li>\n</ol>\n<p>This model captures the essence of the producer: <strong>accumulation by destination, batch dispatch, and managed reliability.</strong> The &quot;postal service&quot; (producer) handles the complexity so the &quot;individual&quot; (application) has a simple interface: just hand over the letter.</p>\n<h3 id=\"63-public-interface\">6.3 Public Interface</h3>\n<p>The Producer exposes a simple, synchronous or asynchronous API for sending messages, along with configuration structs to control its behavior.</p>\n<p><strong>Configuration (<code>ProducerConfig</code>):</strong>\nThe behavior of the producer is finely tuned via a configuration struct passed at creation time.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Description</th>\n<th align=\"left\">Default</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>BootstrapServers</code></td>\n<td align=\"left\"><code>[]string</code></td>\n<td align=\"left\">Initial list of broker addresses (host:port) for metadata discovery.</td>\n<td align=\"left\">(required)</td>\n</tr>\n<tr>\n<td align=\"left\"><code>ClientID</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">Logical name for this producer client, used in logs and server-side quotas.</td>\n<td align=\"left\"><code>&quot;byok-producer&quot;</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>Acks</code></td>\n<td align=\"left\"><code>AcksLevel</code></td>\n<td align=\"left\">Durability guarantee: <code>AcksNone</code> (0), <code>AcksLeader</code> (1), or <code>AcksAll</code> (-1).</td>\n<td align=\"left\"><code>AcksLeader</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>Retries</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Maximum number of retries for a failed batch before giving up.</td>\n<td align=\"left\"><code>3</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>RetryBackoffMs</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Base milliseconds to wait before the first retry. Exponential backoff applies.</td>\n<td align=\"left\"><code>100</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>BatchSize</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Maximum number of bytes to include in a batch before sending.</td>\n<td align=\"left\"><code>16384</code> (16KB)</td>\n</tr>\n<tr>\n<td align=\"left\"><code>LingerMs</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Milliseconds to wait for additional messages to fill a batch before sending.</td>\n<td align=\"left\"><code>5</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>BufferMemory</code></td>\n<td align=\"left\"><code>int64</code></td>\n<td align=\"left\">Total bytes of memory the producer can use for unsent batches.</td>\n<td align=\"left\"><code>33554432</code> (32MB)</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Partitioner</code></td>\n<td align=\"left\"><code>Partitioner</code></td>\n<td align=\"left\">Class implementing partition selection logic (e.g., <code>HashPartitioner</code>, <code>RoundRobinPartitioner</code>).</td>\n<td align=\"left\"><code>HashPartitioner</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>CompressionType</code></td>\n<td align=\"left\"><code>CompressionType</code></td>\n<td align=\"left\">Compression algorithm for record batches (<code>none</code>, <code>gzip</code>, <code>snappy</code>).</td>\n<td align=\"left\"><code>CompressionNone</code></td>\n</tr>\n</tbody></table>\n<p><strong>Core API Methods:</strong>\nThe primary interaction is through a <code>Producer</code> struct with the following methods.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Method</th>\n<th align=\"left\">Parameters</th>\n<th align=\"left\">Returns</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>Send</code></td>\n<td align=\"left\"><code>ctx context.Context</code>, <code>record *Record</code></td>\n<td align=\"left\"><code>(int64, error)</code></td>\n<td align=\"left\">Synchronously sends a single record. Blocks until the record is acknowledged according to the <code>Acks</code> setting, or until <code>ctx</code> times out/cancels. Returns the partition and offset the record was assigned, or an error.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>SendAsync</code></td>\n<td align=\"left\"><code>record *Record</code>, <code>callback func(int64, error)</code></td>\n<td align=\"left\"><code>error</code></td>\n<td align=\"left\">Asynchronously sends a record. The callback is invoked when the send succeeds or fails. Returns immediately unless the internal buffers are full (<code>ErrBufferFull</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Flush</code></td>\n<td align=\"left\"><code>ctx context.Context</code></td>\n<td align=\"left\"><code>error</code></td>\n<td align=\"left\">Blocks until all currently buffered (unsent) records are transmitted and acknowledged. Used for graceful shutdown or to ensure delivery at specific points.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Close</code></td>\n<td align=\"left\"><code>ctx context.Context</code></td>\n<td align=\"left\"><code>error</code></td>\n<td align=\"left\">Gracefully shuts down the producer. Flushes any buffered records, waits for pending callbacks, and closes all network connections.</td>\n</tr>\n</tbody></table>\n<p><strong>Example Usage:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Creating a producer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ProducerConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BootstrapServers: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"broker1:9092\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"broker2:9092\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClientID:         </span><span style=\"color:#9ECBFF\">\"my-app\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Acks:             AcksAll,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Retries:          </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">producer, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewProducer</span><span style=\"color:#E1E4E8\">(config)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">defer</span><span style=\"color:#E1E4E8\"> producer.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Synchronous send</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">offset, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> producer.</span><span style=\"color:#B392F0\">Send</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic: </span><span style=\"color:#9ECBFF\">\"orders\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key:   []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"order-123\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value: []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`{\"status\": \"shipped\"}`</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Record written to partition </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> at offset </span><span style=\"color:#79B8FF\">%d\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, partition, offset)</span></span></code></pre></div>\n\n<h3 id=\"64-internal-behavior-batching-and-sending\">6.4 Internal Behavior: Batching and Sending</h3>\n<p>Internally, the producer is a complex state machine orchestrating metadata, accumulation, and network I/O across multiple goroutines. Its core algorithm can be broken down into the following steps for a synchronous <code>Send</code>.</p>\n<p><strong>1. Metadata Fetch and Partition Selection:</strong>\n   When a record arrives for a topic, the producer first checks its local metadata cache.</p>\n<ol>\n<li>If the topic metadata is unknown or stale (e.g., last refresh &gt; <code>metadata.max.age.ms</code>), it sends a <code>MetadataRequest</code> to one of the bootstrap brokers. The response populates the cache with the list of partitions and their current leader brokers.</li>\n<li>Using the configured <code>Partitioner</code> (e.g., <code>HashPartitioner</code>), the producer computes the target partition ID. For a non-nil key, it hashes the key and applies modulus to the partition count. For a nil key, it may use a sticky partition strategy or round-robin.</li>\n<li>The metadata cache provides the <code>BrokerID</code> (and network address) of the leader for that partition.</li>\n</ol>\n<p><strong>2. Record Accumulation (Batching):</strong>\n   The producer maintains a map of in-memory buffers keyed by <code>TopicPartition</code> (Topic + Partition ID). Each buffer holds a <code>RecordBatch</code> under construction.</p>\n<ol>\n<li>The serialized record is appended to the <code>RecordBatch</code> for its target <code>TopicPartition</code>.</li>\n<li>The batch is not immediately sent. It waits until either:<ul>\n<li>The total byte size of the batch exceeds <code>BatchSize</code>.</li>\n<li>The <code>LingerMs</code> timer for that batch expires.</li>\n<li>The <code>Flush</code> or <code>Close</code> method is called.\n This batching is critical for throughput, as it amortizes the fixed cost of a network round-trip and disk I/O over many records.</li>\n</ul>\n</li>\n</ol>\n<p><strong>3. Batch Dispatch and In-Flight Management:</strong>\n   When a batch is ready to send, it is handed off to a <strong>Sender</strong> goroutine.</p>\n<ol>\n<li>The Sender manages a pool of network connections, one per broker leader. It retrieves or creates a connection to the leader broker for the batch&#39;s partition.</li>\n<li>It wraps the <code>RecordBatch</code> in a <code>ProduceRequest</code> and sends it over the network.</li>\n<li>The batch is moved to an &quot;in-flight&quot; map, keyed by <code>TopicPartition</code>. This tracks batches awaiting acknowledgment.</li>\n<li>The Sender can multiplex batches for the same broker into a single network request for further efficiency.</li>\n</ol>\n<p><strong>4. Acknowledgment Handling and Retry:</strong>\n   The producer awaits the broker&#39;s <code>ProduceResponse</code>.</p>\n<ol>\n<li><strong>Success:</strong> If the response indicates success (and meets the <code>Acks</code> level), the in-flight batch is removed. For a synchronous <code>Send</code>, the calling goroutine is unblocked with the assigned offset. For <code>SendAsync</code>, the user&#39;s callback is invoked with the offset.</li>\n<li><strong>Retriable Error:</strong> Errors like <code>NotLeaderForPartition</code>, <code>NetworkException</code>, or <code>Timeout</code> trigger a retry.\n  a. The batch is re-queued to the accumulator for the same <code>TopicPartition</code>.\n  b. The producer&#39;s metadata for that topic is marked as stale, forcing a refresh before the next send attempt (to discover the new leader).\n  c. A retry delay is calculated using exponential backoff: <code>delay = RetryBackoffMs * 2^(attempt)</code>. The batch is scheduled for re-dispatch after this delay.</li>\n<li><strong>Fatal Error:</strong> Errors like <code>InvalidTopic</code>, <code>RecordTooLarge</code>, or exceeding <code>Retries</code> cause the batch to fail permanently. The error is reported to the application.</li>\n</ol>\n<p><strong>5. Ordering and Idempotence (Optional):</strong>\n   To guarantee exactly-once semantics and preserve ordering across retries, an idempotent producer assigns a monotonic sequence number per <code>TopicPartition</code>. The broker rejects duplicates based on this number. For our educational project, this is an advanced extension, but the design should consider leaving room for it (e.g., a <code>ProducerID</code> and <code>SequenceNumber</code> field in the <code>RecordBatch</code>).</p>\n<p>The following sequence diagram illustrates this flow for a successful send:\n<img src=\"/api/project/build-kafka/architecture-doc/asset?path=diagrams%2Fdiagram-producer-flow.svg\" alt=\"Producer Send Sequence\"></p>\n<h3 id=\"65-adr-acknowledgment-semantics\">6.5 ADR: Acknowledgment Semantics</h3>\n<blockquote>\n<p><strong>Decision: Support Three Acknowledgement Levels (0, 1, all)</strong></p>\n<ul>\n<li><strong>Context:</strong> The producer must offer trade-offs between durability and latency. Different applications have different needs: a logging system may tolerate some data loss for maximum speed, while a financial transaction processor cannot.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Fire-and-forget (<code>acks=0</code>):</strong> The producer sends the message and does not wait for any acknowledgment from the broker.</li>\n<li><strong>Leader acknowledgment (<code>acks=1</code>):</strong> The producer waits for the partition leader to have written the record to its local log before considering the send successful.</li>\n<li><strong>Full ISR acknowledgment (<code>acks=all</code>):</strong> The producer waits for the record to be written to the local log of <em>all</em> in-sync replicas (ISR) before success.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Implement all three levels (<code>AcksNone</code>, <code>AcksLeader</code>, <code>AcksAll</code>) as configurable options.</li>\n<li><strong>Rationale:</strong> This mirrors Apache Kafka&#39;s approach and provides a clear, practical spectrum of durability guarantees for learners to experiment with. Implementing all three demonstrates the incremental complexity: <code>acks=0</code> is trivial, <code>acks=1</code> introduces waiting for a network response, and <code>acks=all</code> requires understanding replication and the High Watermark. This graduated complexity is ideal for learning.</li>\n<li><strong>Consequences:</strong> The broker&#39;s <code>HandleProduce</code> method must implement logic for each <code>acks</code> level. <code>AcksAll</code> requires the broker to track the High Watermark and may introduce higher latency. The producer must handle potential timeouts for <code>acks=all</code> if an ISR replica is slow.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Option</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong><code>acks=0</code> (Fire-and-forget)</strong></td>\n<td align=\"left\">Lowest latency, maximum throughput. Simple to implement.</td>\n<td align=\"left\">Possible data loss if broker fails before writing. No backpressure signal.</td>\n<td align=\"left\"><strong>Yes</strong> – for throughput-critical, loss-tolerant use cases.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>acks=1</code> (Leader ack)</strong></td>\n<td align=\"left\">Good balance. Protects against leader process crash (record is on disk). Moderate latency.</td>\n<td align=\"left\">Data loss possible if leader crashes <em>after</em> ack but before replicas copy the data (failover to a non-replica).</td>\n<td align=\"left\"><strong>Yes</strong> – the default balance for many applications.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>acks=all</code> (ISR ack)</strong></td>\n<td align=\"left\">Highest durability. Survives <code>f</code> broker failures if replication factor &gt; <code>f+1</code>.</td>\n<td align=\"left\">Highest latency (waits for slowest ISR). Throughput limited by slow replica. Can block if ISR shrinks.</td>\n<td align=\"left\"><strong>Yes</strong> – for critical data where loss is unacceptable.</td>\n</tr>\n</tbody></table>\n<h3 id=\"66-common-pitfalls\">6.6 Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Blocking on a Full Buffer</strong></p>\n<ul>\n<li><strong>Description:</strong> The <code>Send</code> method blocks indefinitely if the total size of unsent batches (<code>BufferMemory</code>) is exceeded and the producer cannot drain batches fast enough (e.g., due to slow network or broker).</li>\n<li><strong>Why it&#39;s wrong:</strong> This can cause application threads to hang, leading to a cascading failure. It violates the principle of graceful degradation.</li>\n<li><strong>Fix:</strong> Implement a bounded wait with a timeout in <code>Send</code>, or design <code>SendAsync</code> to return an <code>ErrBufferFull</code> immediately if the buffer is full, allowing the application to apply backpressure or shed load.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring Leader Changes During Send</strong></p>\n<ul>\n<li><strong>Description:</strong> A producer caches partition leader metadata. If a leader fails and a new election occurs after the producer has chosen a partition but before it sends the batch, the batch will be sent to the wrong (old leader) broker.</li>\n<li><strong>Why it&#39;s wrong:</strong> The send fails with <code>NotLeaderForPartition</code>, incurring a retry delay and unnecessary load on the old leader. In a volatile cluster, this can cause a storm of misdirected requests.</li>\n<li><strong>Fix:</strong> On receiving <code>NotLeaderForPartition</code> or <code>UnknownTopicPartition</code> errors, immediately invalidate the cached metadata for that topic and refresh it before the next retry. The metadata should also be refreshed periodically.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Duplicate Messages on Retry</strong></p>\n<ul>\n<li><strong>Description:</strong> When a <code>ProduceRequest</code> times out at the producer, it&#39;s ambiguous whether the broker processed it. A retry may cause the same record to be appended twice to the log.</li>\n<li><strong>Why it&#39;s wrong:</strong> Consumers will process duplicate records, breaking at-least-once semantics and potentially causing incorrect application state (e.g., double-charging).</li>\n<li><strong>Fix (Basic):</strong> For <code>acks=all</code>, duplicate writes are less likely but still possible on timeout. Educate learners that with retries, the system provides <strong>at-least-once</strong> semantics. Application logic must be idempotent.</li>\n<li><strong>Fix (Advanced):</strong> Implement the idempotent producer with sequence numbers to allow the broker to deduplicate, enabling <strong>exactly-once</strong> semantics in the log.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Poor Key Hashing Leading to Skewed Partitions</strong></p>\n<ul>\n<li><strong>Description:</strong> Using a naive hash function (like Java&#39;s <code>Object.hashCode()</code> or Go&#39;s default for strings) or applying modulus to a non-power-of-two partition count can lead to uneven distribution of records across partitions.</li>\n<li><strong>Why it&#39;s wrong:</strong> Some partitions become hotspots, limiting the overall throughput of the topic and causing uneven load on brokers.</li>\n<li><strong>Fix:</strong> Use a robust, deterministic hash function (like MurmurHash2/3) on the key bytes. When performing modulus, ensure the hash value is non-negative. For nil keys, use a sticky random partitioner that batches records to the same partition for a short time to improve batching efficiency.</li>\n</ul>\n<h3 id=\"67-implementation-guidance\">6.7 Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option</th>\n<th align=\"left\">Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Network I/O</strong></td>\n<td align=\"left\">Synchronous I/O with <code>net.Dial</code> and <code>io.Read/Write</code>. Simple to understand.</td>\n<td align=\"left\">Asynchronous I/O using <code>goroutines</code> per connection and channels for batching. Better performance.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Serialization</strong></td>\n<td align=\"left\">Manual byte slice construction using <code>binary.Write</code> and <code>append</code>. Clear and direct.</td>\n<td align=\"left\">Protocol Buffer definitions (<code>protobuf</code>) for request/response formats. More maintainable and extensible.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Partitioner</strong></td>\n<td align=\"left\">Hash-based using <code>crc32</code> or <code>fnv</code>. Round-robin with a counter.</td>\n<td align=\"left\">Implement the &quot;sticky partitioner&quot; from Kafka: fills one batch per partition before moving to the next for better batching.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Compression</strong></td>\n<td align=\"left\">None initially.</td>\n<td align=\"left\">Integrate <code>compress/gzip</code> or a third-party <code>snappy</code> library for record batches.</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<p>Add the following files for the producer client library. It should be a separate package that applications can import.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/producer-cli/           # Optional: example CLI tool for testing\n    main.go\n  internal/producer/          # Producer client library\n    producer.go               # Main Producer struct and public API\n    config.go                 # ProducerConfig and constants\n    accumulator.go            # Batch accumulation logic\n    sender.go                 # Network sender and in-flight management\n    partitioner.go            # HashPartitioner, RoundRobinPartitioner\n    record_batch.go           # RecordBatch serialization format\n    errors.go                 # Producer-specific errors (e.g., ErrBufferFull)\n  internal/protocol/          # Shared request/response structs and serialization\n    produce.go                # ProduceRequest, ProduceResponse\n    metadata.go               # MetadataRequest, MetadataResponse\n    types.go                  # Common types (Record, RecordBatch)</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here is a complete, ready-to-use <code>RecordBatch</code> serialization helper. This handles the binary format for a batch of records, which is the unit of writing for both the producer and the broker&#39;s log.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/protocol/record_batch.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordBatch represents a batch of records as written to the log.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This is a simplified version. A full implementation includes crc, attributes, etc.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecordBatch</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BaseOffset      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PartitionLeaderEpoch </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\"> // Used for leader epoch tracking (advanced)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MagicByte       </span><span style=\"color:#F97583\">int8</span><span style=\"color:#6A737D\">   // Version of the batch format (set to 2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CRC             </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\"> // CRC of the batch data (after this field)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Attributes      </span><span style=\"color:#F97583\">int16</span><span style=\"color:#6A737D\">  // Bitmask for compression, timestamp type, etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastOffsetDelta </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\">  // Delta from BaseOffset for the last record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FirstTimestamp  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // Timestamp of the first record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxTimestamp    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // Max timestamp in the batch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProducerID      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // For idempotence (-1 for none)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProducerEpoch   </span><span style=\"color:#F97583\">int16</span><span style=\"color:#6A737D\">  // For idempotence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BaseSequence    </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\">  // For idempotence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Records         []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Record is an individual message.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Record</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Length     </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\">  // Delta from the start of the batch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Attributes </span><span style=\"color:#F97583\">int8</span><span style=\"color:#6A737D\">   // Currently unused</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimestampDelta </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\"> // Delta from FirstTimestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    OffsetDelta </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\"> // Delta from BaseOffset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key        []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value      []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Headers    []</span><span style=\"color:#B392F0\">Header</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Header</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Encode converts the RecordBatch to its on-wire bytes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buf </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> new</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 1. Write the fixed-size header fields.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    Note: We write a placeholder for CRC, compute it later, then overwrite.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.BaseOffset)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.PartitionLeaderEpoch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.MagicByte)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    crcPos </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> buf.</span><span style=\"color:#B392F0\">Len</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#6A737D\">// Placeholder CRC</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.Attributes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.LastOffsetDelta)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.FirstTimestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.MaxTimestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.ProducerID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.ProducerEpoch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, rb.BaseSequence)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf, binary.BigEndian, </span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(rb.Records)))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 2. Encode each record.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, rec </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> rb.Records {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // ... (Encode record length, attributes, deltas, key, value, headers)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // This is detailed serialization logic.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 3. Compute CRC over the bytes starting from Attributes to the end.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> buf.</span><span style=\"color:#B392F0\">Bytes</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    crc </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> computeCRC</span><span style=\"color:#E1E4E8\">(data[crcPos</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">:]) </span><span style=\"color:#6A737D\">// Skip placeholder CRC</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Overwrite the placeholder CRC.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint32</span><span style=\"color:#E1E4E8\">(data[crcPos:], crc)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> data, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DecodeRecordBatch parses bytes into a RecordBatch.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> DecodeRecordBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation omitted for brevity.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Reads fields in order, validates CRC, decodes records.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> computeCRC</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use a CRC32 implementation (e.g., github.com/klauspost/crc32)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\"> // Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p><strong>1. Accumulator (<code>accumulator.go</code>):</strong> Manages batching per TopicPartition.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/producer/accumulator.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> producer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/byok/internal/protocol</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Partition </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Accumulator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ProducerConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batches </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecordBatch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cond    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Cond</span><span style=\"color:#6A737D\"> // Used to signal sender when a batch is ready</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    closed  </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewAccumulator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ProducerConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Accumulator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    acc </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Accumulator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config: config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batches: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    acc.cond </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sync.</span><span style=\"color:#B392F0\">NewCond</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">acc.mu)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> acc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Append adds a record to the batch for the given TopicPartition.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It returns the batch if it's ready to send (size or linger), otherwise nil.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Accumulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tp</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">record</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> a.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> a.batches[tp]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            FirstTimestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UnixMilli</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Records:        []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a.batches[tp] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Start a linger timer for this batch in a goroutine.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //         When the timer fires, call a.readyBatch(tp).</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Serialize the record to estimate its size in bytes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add the record to batch.Records.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update batch.LastOffsetDelta and batch.MaxTimestamp.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If the batch's estimated size >= config.BatchSize, mark it ready.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         Call a.readyBatch(tp) and return the batch.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // Return nil if batch not ready yet.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// readyBatch marks a batch as ready to send and notifies the sender.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Accumulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">readyBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tp</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> a.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> a.batches[tp]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Remove the batch from the `batches` map.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Signal the waiting sender goroutine via a.cond.Signal().</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetReadyBatch blocks until a batch is ready or the accumulator is closed.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Called by the sender goroutine.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Accumulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetReadyBatch</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> a.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">a.closed {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 8: Iterate through `batches` and find one marked as ready.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //         If found, remove it and return it.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 9: If none ready, wait on a.cond.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Accumulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> a.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.closed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.cond.</span><span style=\"color:#B392F0\">Broadcast</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>2. Partitioner (<code>partitioner.go</code>):</strong> Selects a partition for a record.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/producer/partitioner.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> producer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash/fnv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Partitioner</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Partition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">numPartitions</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashPartitioner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher </span><span style=\"color:#B392F0\">hash</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Hash32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashPartitioner</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashPartitioner</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HashPartitioner</span><span style=\"color:#E1E4E8\">{hasher: fnv.</span><span style=\"color:#B392F0\">New32a</span><span style=\"color:#E1E4E8\">()}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashPartitioner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Partition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">numPartitions</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> numPartitions </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, ErrInvalidPartitionCount</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Handle nil key. Common strategy: round-robin across partitions.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //         You may need state (a counter) per topic. Use sync/atomic.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.hasher.</span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.hasher.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">(p.hasher.</span><span style=\"color:#B392F0\">Sum32</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure non-negative partition.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    partition </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> (hash </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#F97583\"> 0x</span><span style=\"color:#79B8FF\">7FFFFFFF</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> numPartitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> partition, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>3. Sender (<code>sender.go</code>):</strong> Manages network communication and retries.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/producer/sender.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> producer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/byok/internal/protocol</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Sender</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ProducerConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    accumulator   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Accumulator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadataCache </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetadataCache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inFlight      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InFlightBatch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    connPool      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConnectionPool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retryQueue    </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RetryItem</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    done          </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSender</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ProducerConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">acc</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Accumulator</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">meta</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetadataCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:        config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulator:   acc,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadataCache: meta,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inFlight:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InFlightBatch</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connPool:      </span><span style=\"color:#B392F0\">NewConnectionPool</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        retryQueue:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RetryItem</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        done:          </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">run</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">retryLoop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">run</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.done:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tp, batch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.accumulator.</span><span style=\"color:#B392F0\">GetReadyBatch</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> batch </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span><span style=\"color:#6A737D\"> // Accumulator closed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">sendBatch</span><span style=\"color:#E1E4E8\">(tp, batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">sendBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tp</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; attempt </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> s.config.Retries; attempt</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Look up the leader broker for tp using metadataCache.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //         If metadata is stale, refresh it.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        leaderID, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.metadataCache.</span><span style=\"color:#B392F0\">Leader</span><span style=\"color:#E1E4E8\">(tp.Topic, tp.Partition)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO: Handle error, maybe schedule metadata refresh.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Get a connection to the leader from connPool.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.connPool.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(leaderID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO: Handle connection error, maybe mark broker as dead.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Encode the batch into a ProduceRequest.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        req </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ProduceRequest</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Topic:      tp.Topic,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Partition:  tp.Partition,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            RecordBatch: batch,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Acks:       s.config.Acks,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Send the request and receive a response.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">SendProduceRequest</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Network error. Schedule retry.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">scheduleRetry</span><span style=\"color:#E1E4E8\">(tp, batch, attempt)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Check the response error code.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> resp.ErrorCode </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> protocol.ErrNone {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Success! Notify the original caller (via callback or channel).</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">completeBatch</span><span style=\"color:#E1E4E8\">(tp, batch, resp.Offset)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#B392F0\"> isRetriableError</span><span style=\"color:#E1E4E8\">(resp.ErrorCode) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Retriable error (e.g., NotLeaderForPartition).</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.metadataCache.</span><span style=\"color:#B392F0\">MarkStale</span><span style=\"color:#E1E4E8\">(tp.Topic)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">scheduleRetry</span><span style=\"color:#E1E4E8\">(tp, batch, attempt)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Fatal error. Fail the batch permanently.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">failBatch</span><span style=\"color:#E1E4E8\">(tp, batch, resp.ErrorCode)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Exhausted retries.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.</span><span style=\"color:#B392F0\">failBatch</span><span style=\"color:#E1E4E8\">(tp, batch, protocol.ErrRetriesExhausted)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">scheduleRetry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tp</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">attempt</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    delay </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(s.config.RetryBackoffMs) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> time.Millisecond </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#E1E4E8\"> attempt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    item </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RetryItem</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tp:    tp,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch: batch,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time:  time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(delay),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement a priority queue based on `time` instead of a simple channel.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(delay)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> s.retryQueue </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> item:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.done:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">retryLoop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.done:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> item </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.retryQueue:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO: Re-insert the batch into the accumulator for the same tp.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // This will cause it to be picked up by the sender again.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">completeBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tp</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Invoke the user's callback or unblock the synchronous Send.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    delete</span><span style=\"color:#E1E4E8\">(s.inFlight, tp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">failBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tp</span><span style=\"color:#B392F0\"> TopicPartition</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#B392F0\"> protocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ErrorCode</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Invoke the user's callback with error or return error for synchronous Send.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    delete</span><span style=\"color:#E1E4E8\">(s.inFlight, tp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sender</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    close</span><span style=\"color:#E1E4E8\">(s.done)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.connPool.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints (Go)</strong></p>\n<ol>\n<li><strong>Concurrency:</strong> Use a separate goroutine for the main sender loop (<code>run</code>) and the retry loop. Use <code>sync.Cond</code> for efficient waiting between the accumulator and sender. Protect shared maps (<code>batches</code>, <code>inFlight</code>) with <code>sync.RWMutex</code>.</li>\n<li><strong>Network Connections:</strong> Implement a <code>ConnectionPool</code> that reuses <code>net.Conn</code> for each broker. Set <code>TCPKeepAlive</code> and reasonable read/write deadlines. Handle connection errors gracefully by evicting the broken connection from the pool.</li>\n<li><strong>Context Propagation:</strong> Use <code>context.Context</code> in the public <code>Send</code> and <code>Flush</code> methods to allow cancellation and timeouts from the application. Propagate this context through the call chain to network requests.</li>\n<li><strong>Error Types:</strong> Define specific error types (e.g., <code>ErrBufferFull</code>, <code>ErrTopicNotFound</code>) in <code>errors.go</code> for clear error handling.</li>\n</ol>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>To verify your producer implementation works with the broker from Milestone 1:</p>\n<ol>\n<li><strong>Test Setup:</strong> Start a single broker with a topic <code>test-topic</code> (2 partitions).</li>\n<li><strong>Write a Test Producer:</strong> Create a simple program that uses your producer library to send 100 messages with sequential keys.</li>\n<li><strong>Expected Behavior:</strong><ul>\n<li>Messages should be written to the broker&#39;s log segments. Check the data directory for new <code>.log</code> files.</li>\n<li>Use the broker&#39;s <code>HandleFetch</code> API (or a simple test consumer) to read back the messages. They should be present in order within each partition.</li>\n<li>For <code>acks=all</code>, test by killing the broker leader after a send but before acknowledgment; the producer should retry and eventually succeed once a new leader is elected (Milestone 4).</li>\n</ul>\n</li>\n<li><strong>Signs of Trouble:</strong><ul>\n<li><strong>No messages appear:</strong> Check network connectivity, broker logs for errors, and that the producer is looking up the correct leader.</li>\n<li><strong>Messages are duplicated:</strong> Your retry logic is likely not handling timeouts correctly. Ensure you are not retrying on ambiguous errors without idempotence.</li>\n<li><strong>Producer hangs:</strong> The <code>BufferMemory</code> might be too small, or the sender goroutine may be deadlocked. Add debug logs to trace the flow.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"7-component-design-consumer-and-consumer-groups\">7. Component Design: Consumer and Consumer Groups</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 3 (Consumer Groups)</p>\n</blockquote>\n<h3 id=\"71-responsibility-and-scope\">7.1 Responsibility and Scope</h3>\n<p>The <strong>Consumer</strong> is a client application that subscribes to topics and reads messages from partitions in a controlled, scalable manner. Its primary responsibility is to fetch batches of records from broker partitions, track its consumption progress via offsets, and participate in a <strong>Consumer Group</strong> for coordinated parallel consumption. The <strong>Group Coordinator</strong> (typically a designated broker) manages the metadata and lifecycle of consumer groups, including membership, partition assignment, and offset persistence.</p>\n<p><strong>Key Scope Boundaries:</strong></p>\n<ul>\n<li><p><strong>Consumer Responsibilities:</strong></p>\n<ul>\n<li>Discover topic partition leaders via metadata requests</li>\n<li>Join/leave consumer groups and maintain heartbeat with the coordinator</li>\n<li>Execute partition assignment strategies (when acting as group leader)</li>\n<li>Fetch records from assigned partitions, respecting the high watermark</li>\n<li>Periodically commit consumed offsets for durability and resume capability</li>\n<li>Handle partition reassignment during group rebalancing</li>\n</ul>\n</li>\n<li><p><strong>Coordinator Responsibilities:</strong></p>\n<ul>\n<li>Maintain group membership state (active members, generation ID)</li>\n<li>Trigger and orchestrate rebalancing when membership changes</li>\n<li>Store and serve committed offsets for each group-partition pair</li>\n<li>Detect and evict dead consumers via heartbeat timeout</li>\n<li>Act as the single source of truth for group assignment</li>\n</ul>\n</li>\n<li><p><strong>Out of Scope for This Implementation:</strong></p>\n<ul>\n<li>Dynamic partition addition to existing topics (would trigger rebalance)</li>\n<li>Transactional offset commits (exactly-once semantics)</li>\n<li>Custom assignment strategies beyond range and round-robin</li>\n<li>Automatic offset reset policies (earliest/latest) — defaults to committed offset</li>\n<li>Standalone consumers (non-group) reading from explicit partitions</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"72-mental-model-the-team-reading-a-shared-book\">7.2 Mental Model: The Team Reading a Shared Book</h3>\n<p>Imagine a team assigned to read and summarize a multi-volume encyclopedia (the <strong>topic</strong>). Each volume is a <strong>partition</strong> — an independent, ordered sequence of pages. The team&#39;s goal is to divide the work evenly and ensure every page is read exactly once. </p>\n<p><strong>The Initial Division:</strong> When the team first gathers, they elect a <strong>leader</strong> who inspects all volumes and assigns each volume to exactly one team member. This is the <strong>partition assignment</strong>. Each member notes their starting page number (<strong>offset</strong>) in their assigned volumes.</p>\n<p><strong>Parallel Reading with Coordination:</strong> Each member reads pages sequentially from their assigned volumes, periodically checking in with the team coordinator to confirm they&#39;re still active (<strong>heartbeat</strong>). If a member leaves (goes for coffee) or joins the team, the coordinator calls everyone back to re-divide the volumes — a <strong>rebalance</strong>. During rebalance, everyone stops reading, waits for new assignments, then resumes from their last noted page.</p>\n<p><strong>Progress Tracking:</strong> Each member writes down the page number they&#39;ve finished reading in a shared notebook (<strong>offset commit</strong>). If a member restarts (takes a nap), they can consult the notebook to resume from where they left off, avoiding re-reading or skipping pages.</p>\n<p>This model illustrates the core consumer group concepts: <strong>shared workload</strong> (partitions assigned to one consumer), <strong>coordination overhead</strong> (rebalances pause consumption), <strong>fault tolerance</strong> (heartbeats detect failures), and <strong>progress persistence</strong> (offset commits enable resume).</p>\n<h3 id=\"73-public-interface\">7.3 Public Interface</h3>\n<p>The consumer and coordinator expose APIs for applications and internal communication. These interfaces are expressed as RPC methods the broker implements and the consumer client calls.</p>\n<h4 id=\"consumer-client-api-application-facing\">Consumer Client API (Application-Facing)</h4>\n<p>The consumer provides a pull-based interface where the application explicitly requests records.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Subscribe(topics []string)</code></td>\n<td><code>topics</code>: List of topic names to subscribe to</td>\n<td><code>error</code></td>\n<td>Joins the consumer group and begins partition assignment for the given topics. Triggers initial rebalance.</td>\n</tr>\n<tr>\n<td><code>Poll(timeout time.Duration)</code></td>\n<td><code>timeout</code>: Maximum time to wait for records</td>\n<td><code>[]*Record</code> (and internal offset update)</td>\n<td>Fetches records from assigned partitions. Blocks until records arrive or timeout expires. Automatically handles heartbeats in background.</td>\n</tr>\n<tr>\n<td><code>CommitSync(offsets map[TopicPartition]int64)</code></td>\n<td><code>offsets</code>: Map from partition to offset to commit</td>\n<td><code>error</code></td>\n<td>Synchronously commits specific offsets for the consumer&#39;s group. Waits for coordinator acknowledgment.</td>\n</tr>\n<tr>\n<td><code>CommitAsync(offsets map[TopicPartition]int64, callback func(error))</code></td>\n<td><code>offsets</code>: Map to commit, <code>callback</code>: Invoked on completion</td>\n<td>-</td>\n<td>Asynchronously commits offsets. Callback receives error if commit fails.</td>\n</tr>\n<tr>\n<td><code>Close()</code></td>\n<td>-</td>\n<td><code>error</code></td>\n<td>Leaves consumer group, commits final offsets, and releases network resources.</td>\n</tr>\n</tbody></table>\n<p><strong>Configuration Knobs:</strong></p>\n<ul>\n<li><code>group.id</code>: Unique string identifying the consumer group</li>\n<li><code>session.timeout.ms</code>: Timeout after which coordinator considers consumer dead (triggers rebalance)</li>\n<li><code>heartbeat.interval.ms</code>: Frequency of heartbeat signals to coordinator</li>\n<li><code>max.poll.interval.ms</code>: Maximum time between <code>Poll()</code> calls before consumer considered stalled</li>\n<li><code>auto.offset.reset</code>: What to do when no committed offset exists (<code>earliest</code>, <code>latest</code>, <code>none</code> – error)</li>\n<li><code>enable.auto.commit</code>: Whether to automatically commit offsets periodically (simplified: we implement manual commit)</li>\n</ul>\n<h4 id=\"coordinator-internal-api-broker-to-consumer\">Coordinator Internal API (Broker-to-Consumer)</h4>\n<p>The coordinator implements these RPC endpoints that consumers call for group management.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Request Parameters</th>\n<th>Response Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>JoinGroup(groupID, memberID, protocolType, protocols)</code></td>\n<td><code>groupID</code>: Group identifier<br><code>memberID</code>: Current member ID (empty on first join)<br><code>protocolType</code>: Always &quot;consumer&quot;<br><code>protocols</code>: List of supported assignment strategies</td>\n<td><code>memberID</code>: Assigned member ID<br><code>generationID</code>: Current group generation<br><code>leaderID</code>: ID of elected leader<br><code>protocol</code>: Chosen assignment strategy<br><code>members</code>: List of group members (only returned to leader)</td>\n<td>Registers consumer with group. If first member or rebalance needed, triggers new generation. Returns member list to leader for assignment.</td>\n</tr>\n<tr>\n<td><code>SyncGroup(groupID, generationID, memberID, assignments)</code></td>\n<td><code>groupID</code>, <code>generationID</code>, <code>memberID</code>: Identity<br><code>assignments</code>: Partition assignments (from leader only)</td>\n<td><code>assignments</code>: Partition assignments for this member</td>\n<td>After JoinGroup, members call this to receive their assigned partitions. Leader provides assignments for all members.</td>\n</tr>\n<tr>\n<td><code>Heartbeat(groupID, generationID, memberID)</code></td>\n<td><code>groupID</code>, <code>generationID</code>, <code>memberID</code>: Identity</td>\n<td><code>error</code></td>\n<td>Periodic keep-alive. Coordinator resets session timeout. If generation mismatch or member unknown, signals rejoin.</td>\n</tr>\n<tr>\n<td><code>OffsetCommit(groupID, offsets)</code></td>\n<td><code>groupID</code>: Group identifier<br><code>offsets</code>: Map from partition to offset + metadata</td>\n<td><code>error</code></td>\n<td>Persists offsets for partitions. Used for manual commit and auto-commit.</td>\n</tr>\n<tr>\n<td><code>OffsetFetch(groupID, partitions)</code></td>\n<td><code>groupID</code>: Group identifier<br><code>partitions</code>: List of partitions to fetch offsets for</td>\n<td><code>offsets</code>: Map from partition to committed offset + metadata</td>\n<td>Retrieves previously committed offsets. Called on consumer startup to resume.</td>\n</tr>\n</tbody></table>\n<h3 id=\"74-internal-behavior-group-membership-amp-rebalancing\">7.4 Internal Behavior: Group Membership &amp; Rebalancing</h3>\n<p>Consumer group operation is a distributed state machine coordinated by the group coordinator. Each consumer transitions through states, and the group as a whole undergoes phases.</p>\n<h4 id=\"consumer-member-state-machine\">Consumer Member State Machine</h4>\n<p>Each group member follows the state transitions below. Reference diagram: <img src=\"/api/project/build-kafka/architecture-doc/asset?path=diagrams%2Fdiagram-consumer-group-state.svg\" alt=\"Consumer Group State Machine\"></p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>UNJOINED</strong></td>\n<td>Application calls <code>Subscribe()</code></td>\n<td><strong>JOINING</strong></td>\n<td>Generate temporary <code>memberID</code> (empty), send <code>JoinGroup</code> request to coordinator.</td>\n</tr>\n<tr>\n<td><strong>JOINING</strong></td>\n<td>Coordinator responds with <code>memberID</code> and <code>generationID</code></td>\n<td><strong>AWAITING_ASSIGNMENT</strong></td>\n<td>Store assigned <code>memberID</code>. If elected leader, run partition assignment algorithm. Send <code>SyncGroup</code> with assignments (if leader) or empty assignments.</td>\n</tr>\n<tr>\n<td><strong>AWAITING_ASSIGNMENT</strong></td>\n<td>Coordinator responds to <code>SyncGroup</code> with partition assignments</td>\n<td><strong>STABLE</strong></td>\n<td>Update local assignment map. Start fetch loop for assigned partitions. Begin periodic heartbeats.</td>\n</tr>\n<tr>\n<td><strong>STABLE</strong></td>\n<td><code>Poll()</code> called</td>\n<td><strong>STABLE</strong></td>\n<td>Fetch records from assigned partitions. Reset <code>max.poll.interval</code> timer.</td>\n</tr>\n<tr>\n<td><strong>STABLE</strong></td>\n<td>Heartbeat response indicates <code>REBALANCE_IN_PROGRESS</code></td>\n<td><strong>JOINING</strong></td>\n<td>Stop fetching. Re-join group with current <code>memberID</code>.</td>\n</tr>\n<tr>\n<td><strong>STABLE</strong></td>\n<td>Session timeout (no heartbeat response)</td>\n<td><strong>UNJOINED</strong></td>\n<td>Assume coordinator dead. Re-discover coordinator and rejoin.</td>\n</tr>\n<tr>\n<td><strong>STABLE</strong></td>\n<td>Coordinator fails heartbeat (generation mismatch)</td>\n<td><strong>JOINING</strong></td>\n<td>Stop fetching. Re-join group with current <code>memberID</code>.</td>\n</tr>\n<tr>\n<td><strong>ANY</strong></td>\n<td>Application calls <code>Close()</code></td>\n<td><strong>UNJOINED</strong></td>\n<td>Send <code>LeaveGroup</code> (optional), stop heartbeats, close network connections.</td>\n</tr>\n</tbody></table>\n<h4 id=\"group-rebalancing-protocol\">Group Rebalancing Protocol</h4>\n<p>Rebalancing ensures partitions are redistributed when members join or leave. The coordinator orchestrates this process:</p>\n<ol>\n<li><p><strong>Trigger Detection:</strong> Coordinator detects rebalance trigger:</p>\n<ul>\n<li>New <code>JoinGroup</code> request for a group in <code>Stable</code> state</li>\n<li>Existing member&#39;s heartbeat times out (<code>session.timeout.ms</code>)</li>\n<li>Explicit member leave (TCP disconnect, <code>LeaveGroup</code> request)</li>\n</ul>\n</li>\n<li><p><strong>Generation Bump:</strong> Coordinator increments <code>generationID</code> for the group, transitions group state to <code>PreparingRebalance</code>. It delays responding to new <code>JoinGroup</code> requests for up to <code>rebalance.timeout.ms</code> to allow existing members to rejoin.</p>\n</li>\n<li><p><strong>Member Joining:</strong> Coordinator collects <code>JoinGroup</code> requests from all (existing and new) members. It chooses a leader (first member to join) and a partition assignment protocol (intersection of all members&#39; supported protocols).</p>\n</li>\n<li><p><strong>Leader Assignment:</strong> Coordinator responds to <code>JoinGroup</code> with member list <strong>only to the leader</strong>. Other members receive empty member list.</p>\n</li>\n<li><p><strong>Assignment Calculation:</strong> Leader consumer runs partition assignment algorithm (range or round-robin) and sends assignments for all members in its <code>SyncGroup</code> request.</p>\n</li>\n<li><p><strong>Distribution:</strong> Coordinator stores leader&#39;s assignments. When each member calls <code>SyncGroup</code>, it receives its specific assignment.</p>\n</li>\n<li><p><strong>Completion:</strong> Once all members have called <code>SyncGroup</code>, coordinator transitions group to <code>Stable</code> state. Members receive assignments and begin fetching.</p>\n</li>\n</ol>\n<p><strong>Critical Behavior:</strong> During rebalance (between steps 2-7), the coordinator responds to <code>Heartbeat</code> requests with <code>REBALANCE_IN_PROGRESS</code> error, prompting consumers to re-join. This ensures all members synchronize to the new generation.</p>\n<h4 id=\"partition-assignment-algorithm-leader-side\">Partition Assignment Algorithm (Leader Side)</h4>\n<p>When elected leader, the consumer runs one of the following algorithms to assign partitions to group members.</p>\n<p><strong>Range Assignment (Default):</strong></p>\n<ol>\n<li>Sort topics lexicographically, sort partitions within each topic numerically.</li>\n<li>Sort consumer members lexicographically by <code>memberID</code>.</li>\n<li>For each topic:<ul>\n<li>Calculate <code>partitionsPerConsumer = floor(totalPartitions / totalConsumers)</code></li>\n<li>Calculate <code>consumersWithExtra = totalPartitions % totalConsumers</code></li>\n<li>Iterate through sorted consumers: assign <code>partitionsPerConsumer + 1</code> partitions to the first <code>consumersWithExtra</code> consumers, then <code>partitionsPerConsumer</code> to the rest.</li>\n</ul>\n</li>\n<li>Output mapping: <code>memberID -&gt; list of TopicPartition</code>.</li>\n</ol>\n<p><em>Example:</em> Topic <code>orders</code> with partitions 0-5, consumers C1, C2, C3.</p>\n<ul>\n<li><code>partitionsPerConsumer = floor(6/3)=2</code></li>\n<li><code>consumersWithExtra = 6%3=0</code></li>\n<li>Assignment: C1 gets [0,1], C2 gets [2,3], C3 gets [4,5].</li>\n</ul>\n<p><strong>RoundRobin Assignment:</strong></p>\n<ol>\n<li>Flatten all partitions across subscribed topics into a single list, sorted by topic then partition.</li>\n<li>Sort consumer members lexicographically by <code>memberID</code>.</li>\n<li>Distribute partitions in circular fashion: partition i goes to consumer at position <code>(i mod totalConsumers)</code>.</li>\n<li>Output mapping.</li>\n</ol>\n<p><em>Example:</em> Same topic and consumers. Sorted partitions: [orders-0, orders-1, orders-2, orders-3, orders-4, orders-5].</p>\n<ul>\n<li>Assignment: C1 gets [orders-0, orders-3], C2 gets [orders-1, orders-4], C3 gets [orders-2, orders-5].</li>\n</ul>\n<blockquote>\n<p><strong>Design Insight:</strong> Range assignment tends to preserve per-topic partition ordering and is simpler, but can lead to imbalance when subscriptions differ across members. RoundRobin distributes more evenly but scatters partitions of the same topic across consumers. For our educational implementation, we implement both but default to Range for its conceptual clarity.</p>\n</blockquote>\n<h4 id=\"offset-commit-and-fetch\">Offset Commit and Fetch</h4>\n<p>Offset management is crucial for resume-on-restart semantics. The coordinator stores offsets in a durable store (simplified: in-memory map persisted to a file).</p>\n<p><strong>Commit Flow:</strong></p>\n<ol>\n<li>Consumer periodically (or manually) calls <code>OffsetCommit</code> RPC with a map of <code>TopicPartition -&gt; offset</code>.</li>\n<li>Coordinator validates the consumer is still a member of the group and generation matches.</li>\n<li>Coordinator persists offset to storage (fsync for durability).</li>\n<li>Coordinator responds success; consumer may discard locally cached commits.</li>\n</ol>\n<p><strong>Fetch Flow (on startup):</strong></p>\n<ol>\n<li>Consumer calls <code>OffsetFetch</code> for all partitions it is assigned (or potentially subscribes to).</li>\n<li>Coordinator returns stored offset for each partition, or <code>-1</code> if none exists.</li>\n<li>Consumer uses returned offset as its starting position. If offset is <code>-1</code>, it uses <code>auto.offset.reset</code> policy (default to earliest).</li>\n</ol>\n<p><strong>Storage Format:</strong> Offsets are stored per <code>(groupID, topic, partition)</code> tuple. Each entry includes offset, metadata (optional string), and timestamp.</p>\n<h3 id=\"75-adr-partition-assignment-strategy\">7.5 ADR: Partition Assignment Strategy</h3>\n<blockquote>\n<p><strong>Decision: Implement Both Range and RoundRobin, Default to Range</strong></p>\n</blockquote>\n<ul>\n<li><strong>Context:</strong> Consumer groups need a deterministic algorithm to map partitions to members. The algorithm must produce balanced assignments while being understandable for learners. Real Kafka supports pluggable strategies; we must choose which to implement first.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Range Assignment:</strong> Assign contiguous ranges of partitions per topic to each consumer.</li>\n<li><strong>RoundRobin Assignment:</strong> Distribute partitions in circular order across all consumers.</li>\n<li><strong>Sticky Assignment:</strong> Advanced algorithm that minimizes partition movement during rebalance (out of scope).</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Implement both Range and RoundRobin, with Range as the default. The coordinator will choose the protocol based on what all members support (intersection).</li>\n<li><strong>Rationale:</strong><ul>\n<li><strong>Range</strong> is conceptually simpler to understand and implement — it mirrors how humans naturally divide ordered lists. This aligns with our educational goal.</li>\n<li><strong>RoundRobin</strong> introduces the concept of cross-topic balancing and is slightly more complex, but provides a clear contrast in behavior.</li>\n<li>Implementing both allows learners to experiment with trade-offs and understand protocol negotiation.</li>\n<li>Sticky assignment is omitted because its optimization (minimal reassignment) adds complexity disproportionate to learning value for Milestone 3.</li>\n</ul>\n</li>\n<li><strong>Consequences:</strong><ul>\n<li>Learners must implement protocol negotiation in <code>JoinGroup</code>.</li>\n<li>Range may cause workload imbalance when members subscribe to different topic subsets (but our simplified implementation assumes uniform subscription).</li>\n<li>The system can be extended later with custom assignment strategies.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Comparison Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Suitable For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Range</strong></td>\n<td>Simple to implement and reason about; preserves partition ordering per consumer</td>\n<td>Can create imbalance when partitions per topic not divisible by consumers; uneven if subscriptions differ</td>\n<td>Educational default; good for understanding basic assignment</td>\n</tr>\n<tr>\n<td><strong>RoundRobin</strong></td>\n<td>More even distribution across all partitions; balances load better</td>\n<td>Scatters partitions of same topic across consumers; may hurt locality</td>\n<td>Demonstrating alternative strategy; better balance</td>\n</tr>\n<tr>\n<td><strong>Sticky</strong></td>\n<td>Minimizes partition movement during rebalance; reduces temporary unavailability</td>\n<td>Complex algorithm; stateful coordination required</td>\n<td>Production systems where rebalance cost is high (out of scope)</td>\n</tr>\n</tbody></table>\n<h3 id=\"76-common-pitfalls\">7.6 Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Rebalance Storms (Frequent Unnecessary Rebalances)</strong><br><strong>Description:</strong> Consumers repeatedly trigger rebalances due to aggressive timeout settings or misconfigured heartbeats, causing constant consumption pauses.<br><strong>Why Wrong:</strong> System spends more time rebalancing than processing messages; throughput plummets.<br><strong>Fix:</strong> Set <code>session.timeout.ms</code> appropriately (e.g., 10-30 seconds). Ensure heartbeat thread runs independently of poll loop. Implement <code>max.poll.interval.ms</code> separately for slow processing detection.</p>\n<p>⚠️ <strong>Pitfall: Zombie Consumers (Double Assignment)</strong><br><strong>Description:</strong> A consumer thought dead (heartbeat timeout) but actually just slow, continues fetching from its assigned partitions while new consumer gets same assignment.<br><strong>Why Wrong:</strong> Two consumers read same partitions → duplicate processing and offset commit conflicts.<br><strong>Fix:</strong> Coordinator must increment <code>generationID</code> on rebalance; include generation in fetch requests; broker should reject fetches from stale generations.</p>\n<p>⚠️ <strong>Pitfall: Offset Commit Races During Rebalance</strong><br><strong>Description:</strong> Consumer commits offsets after being assigned partitions in rebalance, but commit may be applied after new generation is active, corrupting offset for new member.<br><strong>Why Wrong:</strong> New consumer starts from wrong offset, causing missed or repeated messages.<br><strong>Fix:</strong> Coordinator must reject offset commits with stale <code>generationID</code>. Consumer should commit offsets <em>before</em> joining new rebalance (implement <code>onPartitionsRevoked</code> callback in real Kafka).</p>\n<p>⚠️ <strong>Pitfall: Missing Heartbeats Due to Blocking Poll</strong><br><strong>Description:</strong> Consumer&#39;s <code>Poll()</code> blocks longer than <code>session.timeout.ms</code> (e.g., processing large batch), preventing heartbeat thread from sending keep-alives.<br><strong>Why Wrong:</strong> Coordinator evicts consumer, triggering rebalance even though consumer is active.<br><strong>Fix:</strong> Run heartbeat thread independently with its own timer. Ensure <code>Poll()</code> returns quickly; process messages in application thread after fetch.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Assignment on Heterogeneous Subscriptions</strong><br><strong>Description:</strong> If group members subscribe to different topic lists, naive assignment algorithms may assign partitions for topics a consumer didn&#39;t subscribe to.<br><strong>Why Wrong:</strong> Consumer receives partitions it cannot consume; some partitions may be unassigned.<br><strong>Fix:</strong> In <code>JoinGroup</code>, each member must list its subscribed topics. Leader should assign only partitions of topics a member subscribes to. Filter assignments before distribution.</p>\n<h3 id=\"77-implementation-guidance\">7.7 Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Group State Storage</td>\n<td>In-memory map with periodic file snapshot (JSON)</td>\n<td>Embedded KV store (BadgerDB) with WAL</td>\n</tr>\n<tr>\n<td>Network Protocol</td>\n<td>Plain TCP with custom binary format (same as produce/fetch)</td>\n<td>gRPC with Protocol Buffers for type safety</td>\n</tr>\n<tr>\n<td>Heartbeat Scheduler</td>\n<td>Goroutine with <code>time.Ticker</code></td>\n<td>Dedcheduled timer wheel for efficiency</td>\n</tr>\n<tr>\n<td>Offset Storage</td>\n<td>Separate file per group (<code>offsets/groupID.json</code>)</td>\n<td>Internal <code>__consumer_offsets</code> topic (like Kafka)</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    server/                 # Broker main\n      main.go\n    consumer/               # Consumer client main\n      main.go\n  internal/\n    coordinator/            # Group coordinator logic\n      coordinator.go        # Group membership, rebalance orchestration\n      assignment.go         # Range and RoundRobin assignment strategies\n      offsets.go            # Offset commit/fetch storage\n      coordinator_test.go\n    consumer/               # Consumer client library\n      client.go             # Public Consumer type (Subscribe, Poll, Commit)\n      fetcher.go            # Background fetch loop for assigned partitions\n      heartbeater.go        # Heartbeat management\n      member.go             # Group membership state machine\n    protocol/               # Shared wire format\n      join_group.go         # JoinGroup request/response structs\n      sync_group.go         # SyncGroup request/response\n      heartbeat.go          # Heartbeat request/response\n      offset_commit.go      # Offset commit/fetch structs\n    storage/\n      offset_store.go       # Offset persistence abstraction</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code: Offset Storage</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/coordinator/offset_store.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OffsetStore persists consumer group offsets to disk.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Simplified: one JSON file per group. Not optimized for performance.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> OffsetStore</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseDir </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // in-memory cache: group -> topic -> partition -> offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewOffsetStore creates or loads offset store from directory.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewOffsetStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">baseDir</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OffsetStore</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(baseDir, </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">OffsetStore</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        baseDir: baseDir,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cache:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Load existing offsets from disk (optional)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // store.loadAll()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> store, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Commit writes offsets for a group.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OffsetStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Commit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">groupID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partition</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.cache[groupID]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.cache[groupID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.cache[groupID][topic]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.cache[groupID][topic] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.cache[groupID][topic][partition] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Persist to disk (simplified: write entire group's offsets)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">persistGroup</span><span style=\"color:#E1E4E8\">(groupID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Fetch retrieves offset for a group-topic-partition.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OffsetStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Fetch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">groupID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partition</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> groupMap, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.cache[groupID]; ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> topicMap, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> groupMap[topic]; ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            offset, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> topicMap[partition]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> offset, ok</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// persistGroup writes group's offsets to JSON file.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OffsetStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">persistGroup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">groupID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    groupFile </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(s.baseDir, groupID</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">\".json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">MarshalIndent</span><span style=\"color:#E1E4E8\">(s.cache[groupID], </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"  \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Atomic write: write to temp then rename</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tmpFile </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> groupFile </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \".tmp\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">WriteFile</span><span style=\"color:#E1E4E8\">(tmpFile, data, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Rename</span><span style=\"color:#E1E4E8\">(tmpFile, groupFile)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadGroup reads offsets from disk (call on startup).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OffsetStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadGroup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">groupID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    groupFile </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(s.baseDir, groupID</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">\".json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(groupFile)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">IsNotExist</span><span style=\"color:#E1E4E8\">(err) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // No existing offsets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> groupMap </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">groupMap); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.cache[groupID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> groupMap</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p><strong>Group Coordinator Main Logic:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/coordinator/coordinator.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GroupState</span><span style=\"color:#F97583\"> int32</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GroupStateStable</span><span style=\"color:#B392F0\"> GroupState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GroupStatePreparingRebalance</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GroupStateDead</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ConsumerGroup</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GroupID      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    State        </span><span style=\"color:#B392F0\">GroupState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GenerationID </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LeaderID     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Protocol     </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // e.g., \"range\", \"roundrobin\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Members      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConsumerMetadata</span><span style=\"color:#6A737D\"> // keyed by memberID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeoutMs    </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Tracks when rebalance started for timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rebalanceStartedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ConsumerMetadata</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemberID           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClientID           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SubscribedTopics   []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AssignedPartitions </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\"> // topic -> partition list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastHeartbeat      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JoinWait           </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#6A737D\">// signals member that join is complete</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Coordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    groups   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConsumerGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offsetStore </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OffsetStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleJoinGroup processes a consumer's request to join a group.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns memberID, generationID, leaderID, protocol, and member list (if leader).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Coordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleJoinGroup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">groupID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">memberID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">protocolType</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    protocols</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeoutMs</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConsumerMetadata</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Lookup or create group for groupID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If memberID is empty, generate a unique ID (e.g., \"consumer-&#x3C;uuid>\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate protocolType == \"consumer\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If group is Stable and this is first member joining (new consumer), trigger rebalance:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Set group state to PreparingRebalance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Increment GenerationID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Record rebalance start time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If group is already PreparingRebalance, add member to Members map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If this member is first to join in this generation, set as LeaderID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Choose protocol from intersection of all members' protocols (prefer \"range\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: If all expected members have joined or timeout reached, move to next step:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For leader: return full member list so it can compute assignments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For followers: return empty member list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Return assigned memberID, current GenerationID, LeaderID, chosen Protocol</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleSyncGroup distributes partition assignments to members.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Coordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleSyncGroup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">groupID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">generationID</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    memberID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">assignments</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Assignment</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate group exists, generation matches, member is in group</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If member is leader, store assignments for all members</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If assignments already stored (by leader), return this member's assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If all members have retrieved assignments, transition group to Stable state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return assigned partitions for this member: topic -> []partition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleHeartbeat validates member liveness.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Coordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleHeartbeat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">groupID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">generationID</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    memberID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Lookup group, validate generation matches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Update member's LastHeartbeat timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If group is PreparingRebalance, return error to signal consumer to rejoin</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return nil if successful</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// checkRebalanceTimeout runs periodically to complete rebalance if members don't all join.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Coordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkRebalanceTimeout</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Iterate groups in PreparingRebalance state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If rebalance started more than TimeoutMs ago, proceed with current members</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Mark missing members as dead, remove from group</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Trigger assignment with remaining members</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Consumer Client Membership State Machine:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/consumer/member.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> consumer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MemberState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateUnjoined</span><span style=\"color:#B392F0\"> MemberState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateJoining</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateAwaitingAssignment</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateStable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GroupMember</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    groupID        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memberID       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    generationID   </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state          </span><span style=\"color:#B392F0\">MemberState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assigned       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    coordinator    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BrokerConnection</span><span style=\"color:#6A737D\"> // connection to group coordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    heartbeatInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastHeartbeat   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancelHeartbeat </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu             </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// joinGroup performs the JoinGroup/SyncGroup protocol.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GroupMember</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">joinGroup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">topics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set state to Joining</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Send JoinGroup request to coordinator with empty memberID (if first time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: On response, store assigned memberID, generationID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If elected leader, compute assignments for all members using chosen protocol</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send SyncGroup request with assignments (if leader) or empty map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: On SyncGroup response, update assigned partitions map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Start heartbeat goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Transition state to Stable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Start fetcher for assigned partitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// startHeartbeat begins periodic heartbeat to coordinator.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GroupMember</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">startHeartbeat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(m.heartbeatInterval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 1: Send Heartbeat request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 2: If error indicates rebalance needed, call joinGroup again</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 3: Update lastHeartbeat timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Partition Assignment Strategies:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/coordinator/assignment.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Assignment represents partitions assigned to a consumer member.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Assignment</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TopicPartitions </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\"> // topic -> list of partition IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RangeAssigner implements range assignment strategy.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RangeAssigner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RangeAssigner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Assign</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">members</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConsumerMetadata</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    topics</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Assignment</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Sort members by MemberID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each topic, sort partitions numerically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate partitionsPerConsumer and consumersWithExtra</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Assign contiguous ranges to each consumer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Build Assignment map for each member</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RoundRobinAssigner implements round-robin assignment.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RoundRobinAssigner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RoundRobinAssigner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Assign</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">members</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConsumerMetadata</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    topics</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Assignment</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Sort members by MemberID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Flatten all partitions into single list: [topic-partition, ...]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Distribute in round-robin fashion: partition i -> member i % len(members)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Build Assignment map for each member</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints (Go)</strong></p>\n<ol>\n<li><strong>Concurrency:</strong> Use <code>sync.RWMutex</code> for <code>ConsumerGroup</code> and <code>Coordinator</code> maps. Heartbeat and fetch loops should run in separate goroutines; coordinate shutdown with <code>context.Context</code>.</li>\n<li><strong>ID Generation:</strong> Use <code>crypto/rand</code> for unique <code>memberID</code> (e.g., <code>&quot;consumer-&quot; + hex.EncodeToString(uuid[:])</code>). Avoid using client IP/port as it may change.</li>\n<li><strong>Timers:</strong> For session timeout, store <code>LastHeartbeat</code> as <code>time.Time</code> and periodically check <code>time.Since(last) &gt; sessionTimeout</code>. Use <code>time.Ticker</code> for heartbeat loop.</li>\n<li><strong>Network Errors:</strong> When TCP connection to coordinator fails, consumer should rediscover coordinator via metadata request and rejoin group.</li>\n<li><strong>State Persistence:</strong> Write offset commits with <code>os.WriteFile</code> to temporary file then <code>os.Rename</code> for atomicity. Use JSON for simplicity; in production you&#39;d use binary format.</li>\n</ol>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing consumer groups:</p>\n<ol>\n<li><strong>Start a broker:</strong> <code>go run cmd/server/main.go --port 9092</code></li>\n<li><strong>Create a topic with 3 partitions:</strong> Use your admin tool or implement a simple <code>CreateTopic</code> RPC.</li>\n<li><strong>Start two consumers in same group:</strong></li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Terminal 1</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/consumer/main.go</span><span style=\"color:#79B8FF\"> --group</span><span style=\"color:#9ECBFF\"> my-group</span><span style=\"color:#79B8FF\"> --topic</span><span style=\"color:#9ECBFF\"> test-topic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Terminal 2  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">   go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/consumer/main.go</span><span style=\"color:#79B8FF\"> --group</span><span style=\"color:#9ECBFF\"> my-group</span><span style=\"color:#79B8FF\"> --topic</span><span style=\"color:#9ECBFF\"> test-topic</span></span></code></pre></div>\n<ol start=\"4\">\n<li><strong>Start a producer and send 100 messages:</strong> Messages should be divided between the two consumers.</li>\n<li><strong>Kill one consumer (Ctrl+C):</strong> Observe logs: surviving consumer should get reassigned all partitions after rebalance timeout.</li>\n<li><strong>Restart killed consumer:</strong> It should rejoin and partitions should be redistributed.</li>\n<li><strong>Verify no duplicates:</strong> Count messages received across all consumers; should equal 100 exactly.</li>\n<li><strong>Test offset persistence:</strong> Stop all consumers, restart, produce more messages. New consumers should resume from committed offsets.</li>\n</ol>\n<p><strong>Expected Observations:</strong></p>\n<ul>\n<li>Consumers log &quot;Assigned partitions: [...]&quot; on each rebalance.</li>\n<li>During rebalance, message consumption pauses briefly.</li>\n<li>If you implement auto-commit, offsets are saved periodically.</li>\n<li>Heartbeat logs (if enabled) show periodic &quot;Heartbeat sent&quot; messages.</li>\n</ul>\n<p><strong>Signs of Trouble:</strong></p>\n<ul>\n<li><strong>Messages duplicated:</strong> Likely generation ID not validated in fetch requests, or offset commits race.</li>\n<li><strong>Consumer gets no messages:</strong> Check assignment algorithm; verify partitions actually have data.</li>\n<li><strong>Rebalance loops continuously:</strong> Session timeout too short or heartbeat not sent.</li>\n<li><strong>Offsets not persisted:</strong> Offset store file not being written (check permissions).</li>\n</ul>\n<hr>\n<h2 id=\"8-component-design-replication\">8. Component Design: Replication</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 4 (Replication)</p>\n</blockquote>\n<h3 id=\"81-responsibility-and-scope\">8.1 Responsibility and Scope</h3>\n<p>The <strong>Replication module</strong> is the core subsystem responsible for providing <strong>fault tolerance</strong> and <strong>data durability</strong> in our distributed message queue. Its primary responsibility is to maintain <strong>identical copies</strong> (replicas) of each partition&#39;s log across multiple broker nodes, ensuring that if one broker fails, another can seamlessly take over without data loss or unavailability. This module operates within the <strong>replication plane</strong> of the cluster—the internal network path over which brokers communicate to synchronize data.</p>\n<p>The scope of the replication module encompasses four critical functions:</p>\n<ol>\n<li><strong>Leader-Follower Replication Protocol:</strong> Continuously copying newly appended records from the <strong>leader replica</strong> (the broker responsible for handling all read/write requests for a partition) to one or more <strong>follower replicas</strong>.</li>\n<li><strong>In-Sync Replica (ISR) Management:</strong> Dynamically tracking which follower replicas are sufficiently caught up with the leader to be considered &quot;in-sync,&quot; and managing the membership of this set based on configurable lag thresholds.</li>\n<li><strong>High Watermark Advancement:</strong> Determining the offset up to which all in-sync replicas have replicated data, thereby defining the <strong>durable frontier</strong> that consumers can safely read without risking exposure to data that could be lost during a failover.</li>\n<li><strong>Leader Election:</strong> Selecting a new leader replica from the current ISR when the existing leader becomes unavailable, ensuring continuous availability of the partition.</li>\n</ol>\n<p>This module directly interacts with the <code>Log</code> and <code>Partition</code> components within each broker, and communicates with other brokers via internal RPCs. It is a foundational requirement for achieving the <strong>exactly-once semantics</strong> and <strong>guaranteed delivery</strong> that characterize production-grade message systems.</p>\n<h3 id=\"82-mental-model-the-ship39s-log-replica\">8.2 Mental Model: The Ship&#39;s Log Replica</h3>\n<p>To build intuition, imagine a <strong>captain&#39;s logbook</strong> on a sailing ship (the partition log). The captain (the <strong>leader</strong>) records every important event (messages) in chronological order. To protect against the logbook being lost overboard, the captain employs several <strong>first mates</strong> (followers) who each maintain their own copy.</p>\n<ul>\n<li><strong>Copying Process:</strong> After each entry, the captain calls out the new lines, and the first mates write them into their own logbooks. Some mates write quickly and stay current; others might lag behind if distracted.</li>\n<li><strong>In-Sync Crew (ISR):</strong> The captain periodically checks which mates have fully copied the latest entries. Only those who are completely up-to-date are considered part of the &quot;in-sync crew.&quot; If a mate falls too far behind (beyond a tolerance threshold), they are temporarily removed from this trusted set until they catch up.</li>\n<li><strong>Safe Reading Point (High Watermark):</strong> When a sailor (consumer) wants to read the log, the captain only allows them to read entries that <strong>every in-sync mate</strong> has successfully copied. This ensures that even if the captain is suddenly lost at sea, the new captain (elected from the in-sync crew) will have a complete copy of everything the sailor has read, guaranteeing no data loss.</li>\n<li><strong>Captain Election:</strong> If the captain falls ill, the crew holds a quick vote among the in-sync mates to choose a new captain. They select the mate whose log is most complete (has the highest offset). This new captain immediately assumes responsibility for recording new events.</li>\n</ul>\n<p>This model illustrates the core trade-off: <strong>durability versus latency</strong>. Waiting for all mates to acknowledge each entry (synchronous replication) maximizes safety but slows down the recording process. The ISR model allows the captain to proceed once a <strong>quorum</strong> of mates (often just the leader itself, or a majority) has acknowledged, striking a practical balance.</p>\n<h3 id=\"83-internal-broker-to-broker-api\">8.3 Internal Broker-to-Broker API</h3>\n<p>Replication is driven by a set of internal Remote Procedure Calls (RPCs) between brokers. These are separate from the client-facing <code>Produce</code> and <code>Fetch</code> APIs, though they may share the same transport layer. The following table details the key RPCs.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Initiator</th>\n<th>Target</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>LeaderForPartition</code></td>\n<td>Follower/Coordinator</td>\n<td>Any Broker</td>\n<td><code>Topic string</code>, <code>PartitionID int</code></td>\n<td><code>LeaderBrokerID int</code>, <code>LeaderEpoch int32</code>, <code>Error error</code></td>\n<td>Discovers the current leader broker for a given partition. Used by followers during startup or after a leader change.</td>\n</tr>\n<tr>\n<td><code>FetchReplica</code></td>\n<td>Follower</td>\n<td>Leader</td>\n<td><code>Topic string</code>, <code>PartitionID int</code>, <code>FollowerBrokerID int</code>, <code>FetchOffset int64</code>, <code>MaxBytes int32</code></td>\n<td><code>Records []*Record</code>, <code>HighWatermark int64</code>, <code>LeaderEpoch int32</code>, <code>Error error</code></td>\n<td>The core replication fetch request. A follower uses this to pull records from the leader&#39;s log starting at <code>FetchOffset</code>. The leader includes the current <strong>high watermark</strong> so the follower knows what is safely committed.</td>\n</tr>\n<tr>\n<td><code>UpdateISR</code></td>\n<td>Leader</td>\n<td>Metadata Coordinator / Controller</td>\n<td><code>Topic string</code>, <code>PartitionID int</code>, <code>NewISR []int</code>, <code>LeaderEpoch int32</code></td>\n<td><code>Error error</code></td>\n<td>Notifies the cluster metadata service of changes to the In-Sync Replica set (e.g., a follower is added or removed). This update must be persisted and propagated to all brokers.</td>\n</tr>\n<tr>\n<td><code>BeginLeaderElection</code></td>\n<td>Controller / Coordinator</td>\n<td>Candidate Broker</td>\n<td><code>Topic string</code>, <code>PartitionID int</code>, <code>CandidateISR []int</code></td>\n<td><code>Success bool</code>, <code>Error error</code></td>\n<td>Initiates a leader election for a partition. The controller proposes a candidate broker (from the ISR) to become the new leader. The candidate validates it is in the ISR and takes leadership.</td>\n</tr>\n<tr>\n<td><code>FollowerHeartbeat</code></td>\n<td>Follower</td>\n<td>Leader</td>\n<td><code>Topic string</code>, <code>PartitionID int</code>, <code>FollowerBrokerID int</code>, <code>FollowerLEO int64</code></td>\n<td><code>CurrentLeaderEpoch int32</code>, <code>Error error</code></td>\n<td>Optional periodic heartbeat from follower to leader, carrying the follower&#39;s latest Log End Offset (LEO). This allows the leader to track follower lag without waiting for a fetch request.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The <code>FetchReplica</code> RPC is intentionally similar to the client <code>Fetch</code> API. This symmetry simplifies the code—a follower is essentially a special consumer that reads from the leader&#39;s log. However, replication fetches are continuous, long-polling requests that wait for new data, unlike typical consumer fetches.</p>\n</blockquote>\n<p>In our simplified educational system, we may combine the roles of <strong>Metadata Coordinator</strong> and <strong>Controller</strong> into a single component (the <code>Coordinator</code> from previous sections). Therefore, <code>UpdateISR</code> and <code>BeginLeaderElection</code> would be directed to that coordinator broker.</p>\n<h3 id=\"84-internal-behavior-follower-sync-and-isr-management\">8.4 Internal Behavior: Follower Sync and ISR Management</h3>\n<p>The replication process is a continuous loop run by each follower replica for every partition it is assigned to. The leader, in parallel, monitors the progress of all followers to maintain the ISR.</p>\n<h4 id=\"follower-synchronization-algorithm\">Follower Synchronization Algorithm</h4>\n<p>For each partition where the broker is a follower:</p>\n<ol>\n<li><strong>Determine Leader:</strong> If not known, call <code>LeaderForPartition</code> to discover the current leader broker.</li>\n<li><strong>Establish Connection:</strong> Open a persistent network connection to the leader&#39;s replication endpoint.</li>\n<li><strong>Initialize Fetch Offset:</strong> Start fetching from the last offset successfully written to the follower&#39;s local log (its <strong>Log End Offset</strong> or <strong>LEO</strong>). If the log is empty, start at 0.</li>\n<li><strong>Loop:</strong><ol>\n<li>Send a <code>FetchReplica</code> request to the leader with the current <code>FetchOffset</code>.</li>\n<li>If the leader responds with records:<ol>\n<li>Append the records to the local <code>Log</code> in order, ensuring no gaps in offsets.</li>\n<li>Advance the local LEO to <code>FetchOffset + len(records)</code>.</li>\n<li>Optionally, periodically send a <code>FollowerHeartbeat</code> with the new LEO to keep the leader informed.</li>\n</ol>\n</li>\n<li>If the leader responds with an error (e.g., <code>ErrNotLeaderForPartition</code>):<ol>\n<li>Break the connection.</li>\n<li>Wait a short, randomized backoff period (to avoid thundering herds).</li>\n<li>Go to step 1 to rediscover the new leader.</li>\n</ol>\n</li>\n<li>If the leader responds with no new records (i.e., the follower is caught up):<ol>\n<li>Wait for a configurable <code>replica.fetch.wait.ms</code> interval, then repeat the fetch (long polling). This reduces busy-waiting.</li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<h4 id=\"leader-isr-management-algorithm\">Leader ISR Management Algorithm</h4>\n<p>For each partition where the broker is the leader:</p>\n<ol>\n<li><strong>Initialize ISR:</strong> The ISR starts as the set of all assigned replicas (including the leader). This is stored in the <code>Partition.ISR</code> field.</li>\n<li><strong>Track Follower State:</strong> Maintain, for each follower in the ISR, the last known <strong>last fetched offset</strong> (from <code>FetchReplica</code> requests) or <strong>last heartbeat LEO</strong>.</li>\n<li><strong>Periodic Check (every <code>replica.lag.time.max.ms</code>):</strong><ol>\n<li>Calculate the <strong>replica lag</strong> for each follower: <code>Leader&#39;s LEO - Follower&#39;s Last Fetched Offset</code>.</li>\n<li>If a follower&#39;s lag exceeds <code>replica.lag.max.messages</code> <strong>or</strong> its last fetch time is older than <code>replica.lag.time.max.ms</code>, remove it from the ISR.</li>\n<li>If a previously out-of-sync follower&#39;s lag falls to zero (it has caught up), add it back to the ISR.</li>\n</ol>\n</li>\n<li><strong>On ISR Change:</strong><ol>\n<li>Persist the new ISR set (e.g., to ZooKeeper or an internal topic).</li>\n<li>Broadcast the update via <code>UpdateISR</code> to the metadata coordinator so other brokers can update their caches.</li>\n</ol>\n</li>\n<li><strong>Advance High Watermark:</strong><ol>\n<li>The <strong>high watermark</strong> (<code>Partition.HighWatermark</code>) is the maximum offset for which <strong>all replicas in the current ISR</strong> have acknowledged replication.</li>\n<li>After each successful <code>FetchReplica</code> response acknowledgment from a follower, the leader recalculates the high watermark as the minimum LEO across all ISR members.</li>\n<li>The high watermark is included in every <code>FetchReplica</code> response, allowing followers (and consumers) to know what is safely durable.</li>\n</ol>\n</li>\n</ol>\n<blockquote>\n<p><strong>Critical Detail:</strong> The high watermark advancement is <strong>monotonic</strong>. It never moves backward, even if the ISR shrinks. If a follower is removed from the ISR because it is slow, the high watermark can still advance based on the remaining ISR members. This ensures availability at the potential cost of durability if the remaining ISR size falls below a desired minimum (a pitfall we address later).</p>\n</blockquote>\n<h3 id=\"85-adr-leader-election-trigger\">8.5 ADR: Leader Election Trigger</h3>\n<blockquote>\n<p><strong>Decision: Coordinator-Initiated Leader Election</strong></p>\n</blockquote>\n<ul>\n<li><p><strong>Context:</strong> When a partition leader fails (due to broker crash, network partition, or graceful shutdown), a new leader must be elected promptly to maintain partition availability. The election must choose a replica that is <strong>guaranteed to have all committed messages</strong> (i.e., a member of the ISR) to prevent data loss. We need a simple, deterministic mechanism that learners can implement without complex consensus protocols.</p>\n</li>\n<li><p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Explicit Coordinator-Managed Election:</strong> A dedicated controller/coordinator broker monitors leader liveness (via heartbeats). Upon detecting leader failure, it selects a new leader from the ISR and directs it to assume leadership via RPC.</li>\n<li><strong>Follower-Triggered Election:</strong> Followers detect the leader is unresponsive (via fetch timeouts) and initiate a leader election among themselves using a consensus protocol like Raft or a simple voting mechanism.</li>\n</ol>\n</li>\n<li><p><strong>Decision:</strong> We choose <strong>Option 1: Explicit Coordinator-Managed Election</strong>. The <code>Coordinator</code> component (introduced for consumer groups) will be extended to act as a <strong>controller</strong> for partition leadership.</p>\n</li>\n<li><p><strong>Rationale:</strong></p>\n<ul>\n<li><strong>Simplicity for Learners:</strong> Implementing a full distributed consensus protocol (like Raft) is a significant undertaking beyond the core learning goals of message queue replication. Coordinator-managed election centralizes the decision logic, making it easier to implement, debug, and understand.</li>\n<li><strong>Alignment with Kafka&#39;s Design:</strong> Apache Kafka uses a similar controller-based leader election, proving the pattern&#39;s effectiveness at scale. This allows learners to map concepts directly to the real system.</li>\n<li><strong>Deterministic Outcome:</strong> The coordinator has a global view of the ISR and can make an unambiguous choice (e.g., pick the replica with the highest LEO). This avoids split-brain scenarios that can occur in leaderless voting if network partitions are not properly handled.</li>\n<li><strong>Integration with Metadata:</strong> The coordinator already manages cluster metadata (brokers, topics). Extending it to manage partition leadership state is a natural cohesion of responsibilities.</li>\n</ul>\n</li>\n<li><p><strong>Consequences:</strong></p>\n<ul>\n<li><strong>Single Point of Failure:</strong> The coordinator becomes a critical component. If it crashes, leader elections cannot occur until a new coordinator is elected. We mitigate this by making the coordinator role <strong>electable</strong> among brokers (similar to Kafka&#39;s controller election), but this adds complexity.</li>\n<li><strong>Increased Latency on Failover:</strong> Detection and election involve two hops: follower detects leader failure, reports to coordinator, coordinator elects new leader. This may take slightly longer than a follower immediately taking over, but within acceptable bounds for our educational system.</li>\n<li><strong>Simplified Follower Logic:</strong> Followers only need to report leader failure and accept leadership assignments; they don&#39;t need election logic.</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Coordinator-Managed</strong></td>\n<td>Simple to implement; Deterministic; Centralized metadata consistency.</td>\n<td>Coordinator is a SPOF; Slightly slower failover.</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td><strong>Follower-Triggered</strong></td>\n<td>Faster failover; No single point of failure.</td>\n<td>Requires complex consensus protocol; Risk of split-brain; Harder to implement correctly.</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The election algorithm, managed by the coordinator, proceeds as follows:</p>\n<ol>\n<li><strong>Failure Detection:</strong> The coordinator receives heartbeat or metadata from brokers. If the leader broker for a partition fails to heartbeat within <code>session.timeout.ms</code>, it is considered dead.</li>\n<li><strong>ISR Validation:</strong> The coordinator fetches the latest ISR for the partition from its persisted metadata.</li>\n<li><strong>Leader Selection:</strong> From the ISR, select the replica with the highest <strong>Log End Offset</strong> (the most up-to-date). If multiple have the same LEO, choose the first by broker ID for determinism.</li>\n<li><strong>Leadership Transition:</strong> Send a <code>BeginLeaderElection</code> RPC to the chosen broker. The broker validates it is in the ISR, updates its <code>Partition.LeaderBrokerID</code> to itself, and begins accepting produce/fetch requests.</li>\n<li><strong>Metadata Propagation:</strong> The coordinator updates the <code>PartitionMetadata</code> for all brokers and notifies them of the new leader.</li>\n</ol>\n<h3 id=\"86-common-pitfalls\">8.6 Common Pitfalls</h3>\n<p>Implementing replication is fraught with subtle bugs that can lead to data loss or inconsistency. Here are the most common pitfalls learners encounter.</p>\n<p>⚠️ <strong>Pitfall 1: Allowing Unclean Leader Election</strong></p>\n<ul>\n<li><strong>Description:</strong> Electing a leader that is <strong>not</strong> in the ISR (i.e., a lagging follower) because the ISR has become empty. This new leader may be missing messages that were acknowledged to producers, causing <strong>data loss</strong>.</li>\n<li><strong>Why It&#39;s Wrong:</strong> The fundamental guarantee of durability is violated. Producers that received an acknowledgment (<code>acks=all</code>) believe their message is durable, but it is lost.</li>\n<li><strong>How to Fix:</strong> Implement a strict policy: <strong>never elect a non-ISR replica</strong>. If the ISR becomes empty, the partition must become unavailable (return an error to clients) until a replica recovers and rejoins the ISR. This is the &quot;unclean.leader.election.enable=false&quot; policy in Kafka.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 2: ISR Shrinking to Zero</strong></p>\n<ul>\n<li><strong>Description:</strong> If followers are consistently slower than the leader (due to network or disk issues), they may be removed from the ISR one by one until no followers remain. This leaves the leader as the only ISR member, which is risky—if it fails, no eligible successor exists.</li>\n<li><strong>Why It&#39;s Wrong:</strong> The system loses fault tolerance. While the partition remains available, it is one failure away from permanent unavailability.</li>\n<li><strong>How to Fix:</strong> Monitor ISR size and alert if it falls below a minimum threshold (e.g., <code>min.insync.replicas</code>). Producers can be configured to require a minimum ISR size (<code>acks=all</code> will fail if ISR size is below this minimum), trading availability for safety. Also, tune replica fetch parameters to reduce lag.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 3: Incorrect High Watermark Update</strong></p>\n<ul>\n<li><strong>Description:</strong> Updating the high watermark based on <strong>all replicas</strong> (including out-of-sync ones) or updating it before a follower&#39;s write is durable on disk. This can cause the watermark to advance too slowly (hindering consumers) or, worse, to advance too quickly (exposing uncommitted data).</li>\n<li><strong>Why It&#39;s Wrong:</strong> If the high watermark advances before data is durable on followers, and the leader crashes, a new leader may not have that data, yet consumers have already read it (data loss for consumers). Conversely, a stagnant watermark reduces consumer throughput.</li>\n<li><strong>How to Fix:</strong> The high watermark must be the minimum LEO <strong>only among current ISR members</strong>. Additionally, a follower should only update its LEO (and thus be counted for the leader&#39;s watermark calculation) after it has durably appended the records to its local log (fsync). The leader should wait for this acknowledgment.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 4: Not Handling Leader Epoch</strong></p>\n<ul>\n<li><strong>Description:</strong> Failing to track a <strong>leader epoch</strong>—a monotonically increasing number for each leader change—can cause <strong>message duplication</strong> or <strong>reordering</strong> after a leader failover.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Without epoch tracking, a former leader that comes back online (e.g., after a network partition) may still think it&#39;s the leader and accept writes, creating a split-brain scenario with divergent logs.</li>\n<li><strong>How to Fix:</strong> Introduce a <code>LeaderEpoch</code> field in <code>PartitionMetadata</code>. Each new leader increments the epoch. Include the epoch in every replication RPC and client request. Followers and clients reject requests from stale leaders (with older epochs). The leader also truncates its log to the last known offset for each epoch to maintain consistency.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 5: Busy-Waiting in Follower Fetch Loop</strong></p>\n<ul>\n<li><strong>Description:</strong> Followers that immediately re-send fetch requests when caught up, consuming excessive CPU and network resources.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Inefficient resource usage; can overwhelm the leader with requests.</li>\n<li><strong>How to Fix:</strong> Implement <strong>long polling</strong>: the leader holds the fetch request for a short period (<code>replica.fetch.wait.ms</code>) if there is no new data, returning immediately when data arrives or the wait time elapses. This reduces the request rate while maintaining low latency.</li>\n</ul>\n<h3 id=\"87-implementation-guidance\">8.7 Implementation Guidance</h3>\n<p>This section provides starter code and structure for the replication module in Go.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Replication Transport</strong></td>\n<td>Plain TCP with custom binary protocol (reuse <code>TCPServer</code>)</td>\n<td>gRPC with streaming for fetch requests</td>\n</tr>\n<tr>\n<td><strong>Metadata Storage</strong></td>\n<td>In-memory map in the coordinator, persisted to a local WAL file</td>\n<td>Embedded key-value store (BadgerDB) for fault-tolerant metadata</td>\n</tr>\n<tr>\n<td><strong>Leader Election</strong></td>\n<td>Centralized coordinator with manual failover</td>\n<td>Raft consensus for coordinator fault tolerance</td>\n</tr>\n<tr>\n<td><strong>Follower Sync</strong></td>\n<td>Periodic fetch with sleep interval</td>\n<td>Long-polling fetch with <code>net.Conn</code> read timeout</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Add the following directories and files to your project. This separates replication logic from the core broker and coordinator.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    broker/\n      server.go           # Main broker logic (existing)\n      log_manager.go      # Log management (existing)\n    replication/\n      replica_manager.go  # Main replication logic on each broker\n      follower_syncer.go  # Follower sync loop\n      isr_manager.go      # ISR tracking and high watermark updates\n      leader_election.go  # Leader election logic (in coordinator)\n    coordinator/\n      coordinator.go      # Extended with controller functions\n      metadata_store.go   # Persists topic/partition metadata\n    protocol/\n      replication.go      # Replication RPC request/response structs</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-replication-rpc-protocol\">C. Infrastructure Starter Code: Replication RPC Protocol</h4>\n<p>First, define the replication-specific protocol messages. These are separate from client protocol messages.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/protocol/replication.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FetchReplicaRequest is sent by follower to leader to fetch records.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FetchReplicaRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PartitionID     </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FollowerBrokerID </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FetchOffset     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxBytes        </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // LeaderEpoch for fencing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentLeaderEpoch </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FetchReplicaResponse is returned by leader to follower.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FetchReplicaResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorCode       </span><span style=\"color:#F97583\">int16</span><span style=\"color:#6A737D\"> // e.g., ErrNone, ErrNotLeaderForPartition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HighWatermark   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LeaderEpoch     </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Records         []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateISRRequest is sent by leader to coordinator.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UpdateISRRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PartitionID </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NewISR      []</span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LeaderEpoch </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ZK version or similar for optimistic concurrency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ZKVersion   </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateISRResponse is returned by coordinator.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UpdateISRResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorCode </span><span style=\"color:#F97583\">int16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LeaderForPartitionRequest asks for the current leader of a partition.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LeaderForPartitionRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PartitionID </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LeaderForPartitionResponse contains leader info.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LeaderForPartitionResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorCode       </span><span style=\"color:#F97583\">int16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LeaderBrokerID  </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LeaderEpoch     </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BeginLeaderElectionRequest instructs a broker to become leader.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BeginLeaderElectionRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PartitionID </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // The ISR at time of election, for validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ISR         []</span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LeaderEpoch </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\"> // The new epoch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BeginLeaderElectionResponse confirms leadership taken.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BeginLeaderElectionResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorCode </span><span style=\"color:#F97583\">int16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>1. Replica Manager</strong> (runs on each broker)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/replication/replica_manager.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> replication</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">yourproject/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReplicaManager manages all replica sync tasks for partitions hosted on this broker.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ReplicaManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    brokerID      </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#B392F0\">ReplicaConfig</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Maps TopicPartition -> FollowerSyncer (for partitions where we are follower)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    followers     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FollowerSyncer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Maps TopicPartition -> ISRManager (for partitions where we are leader)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    leaders       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ISRManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx           </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancel        </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ReplicaConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FetchWaitMs          </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicaFetchMinBytes </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicaFetchMaxBytes </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicaLagTimeMaxMs  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewReplicaManager creates a new manager.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewReplicaManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">brokerID</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> ReplicaConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicaManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithCancel</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ReplicaManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        brokerID: brokerID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:   config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        followers: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FollowerSyncer</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        leaders:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ISRManager</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx:       ctx,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cancel:    cancel,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins all sync loops for followers and leaders.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicaManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load partition assignments from local metadata (which partitions this broker hosts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each partition where this broker is a follower, create a FollowerSyncer and start its run loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each partition where this broker is the leader, create an ISRManager and start its monitoring loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start a background goroutine to handle partition assignment changes (e.g., after rebalance)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OnNewPartitionAssignment is called when the broker's assigned partitions change.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicaManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">OnNewPartitionAssignment</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">assignments</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PartitionMetadata</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Compare new assignments with current followers/leaders maps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Stop and remove syncers for partitions we are no longer assigned</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add new syncers for newly assigned partitions (determine role: leader or follower)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For partitions where role changed (follower->leader or vice versa), recreate the syncer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>2. Follower Syncer</strong> (per partition where broker is follower)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/replication/follower_syncer.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> replication</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">yourproject/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FollowerSyncer continuously fetches from the leader for a single partition.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FollowerSyncer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    brokerID      </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    topic         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    partitionID   </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#B392F0\">ReplicaConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log           </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#6A737D\"> // local log to append to</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Connection pool to talk to other brokers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    connPool      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConnectionPool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    leaderBrokerID </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    leaderEpoch   </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fetchOffset   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx           </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancel        </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Run is the main sync loop.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fs </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FollowerSyncer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Discover leader using LeaderForPartition RPC (retry until successful)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Loop until context is done:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Send FetchReplicaRequest to leader with current fetchOffset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. On response:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        i. If error is ErrNotLeaderForPartition, break to rediscover leader</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //       ii. If no error, append records to local log (ensure offset continuity)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //      iii. Advance fetchOffset by number of records appended</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //       iv. Update high watermark from response (store in partition metadata)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. If no records returned, sleep for config.FetchWaitMs before next fetch (long polling)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   d. If fetch failed (network error), backoff exponentially and retry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AppendToLocalLog writes records to the local log filesystem.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fs </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FollowerSyncer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AppendToLocalLog</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">records</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">leaderHW</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire lock on log to ensure sequential writes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate that the first record's offset equals the current local LEO (no gaps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call log.Append(records)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update in-memory high watermark for this partition (min(leaderHW, localLEO))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return any error from append</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>3. ISR Manager</strong> (per partition where broker is leader)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/replication/isr_manager.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> replication</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">yourproject/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ISRManager tracks followers and manages ISR for a partition where this broker is leader.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ISRManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    topic         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    partitionID   </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#B392F0\">ReplicaConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    partition     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Partition</span><span style=\"color:#6A737D\"> // reference to the partition object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Map followerBrokerID -> lastCaughtUpTime and lastFetchOffset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    followerState </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">followerStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx           </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancel        </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> followerStatus</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFetchOffset </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFetchTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins the periodic ISR check.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">im </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ISRManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(im.config.ReplicaLagTimeMaxMs) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> time.Millisecond)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">im.ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            im.</span><span style=\"color:#B392F0\">evaluateISR</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evaluateISR checks each follower's lag and updates ISR.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">im </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ISRManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateISR</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get current leader LEO from partition.Log</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each follower in partition.ReplicaBrokerIDs:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Compute lag = leaderLEO - followerState[brokerID].lastFetchOffset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Compute time since last fetch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. If lag > replica.lag.max.messages OR time since last fetch > replica.lag.time.max.ms:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        Remove follower from ISR (if present)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   d. Else if follower is not in ISR and lag == 0:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        Add follower back to ISR</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If ISR changed:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Update partition.ISR</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Persist new ISR via UpdateISR RPC to coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. Recalculate high watermark (min LEO across new ISR)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateFollowerState is called when a FetchReplica request is processed.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">im </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ISRManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateFollowerState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">followerID</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fetchOffset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fetchedBytes</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Update followerState map with new offset and current time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If fetchOffset == leaderLEO (follower is caught up), ensure follower is in ISR</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>4. Leader Election in Coordinator</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/coordinator/leader_election.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">yourproject/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LeaderElector runs in the coordinator and manages partition leader elections.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LeaderElector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadataStore </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetadataStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    brokerPool    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BrokerPool</span><span style=\"color:#6A737D\"> // to send RPCs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx           </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OnBrokerFailure triggers leader election for partitions whose leader was the failed broker.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">le </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LeaderElector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">OnBrokerFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">failedBrokerID</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Query metadataStore for all partitions where leader == failedBrokerID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each such partition:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Get current ISR from metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Remove failedBrokerID from ISR (if present)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. If ISR is empty, log error and mark partition offline (no leader)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   d. Else:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        i. Select new leader: replica in ISR with highest LEO (from metadata)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //       ii. Send BeginLeaderElection RPC to selected broker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //      iii. On success, update metadataStore with new leader and ISR</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //       iv. Propagate metadata update to all brokers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// selectNewLeader chooses a replica from ISR to become leader.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">le </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LeaderElector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">selectNewLeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">isr</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partitionInfo</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PartitionMetadata</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: If ISR is empty, return -1 (no leader possible)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Fetch LEO for each replica in ISR from metadata (may be stale)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Sort replicas by LEO descending, then by brokerID ascending</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return the first replica ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Concurrency:</strong> Use <code>sync.RWMutex</code> to protect <code>Partition.ISR</code> and <code>Partition.HighWatermark</code>. Followers update high watermark on read; leaders update it and ISR on write.</li>\n<li><strong>Disk Persistence:</strong> When a follower appends records, it must call <code>log.Sync()</code> if the leader&#39;s request had <code>acks=all</code> to ensure durability before acknowledging to leader.</li>\n<li><strong>Context for Cancellation:</strong> Pass <code>context.Context</code> to all long-running loops (follower sync, ISR monitor) to allow graceful shutdown on broker stop.</li>\n<li><strong>Time:</strong> Use <code>time.Time</code> for last fetch timestamps. <code>time.Since()</code> is convenient for calculating lag duration.</li>\n<li><strong>Network Errors:</strong> Use <code>net.Error</code> type checks to differentiate temporary vs permanent network failures. Implement exponential backoff with <code>time.Sleep</code> for temporary errors.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing replication, you should be able to verify the following scenario:</p>\n<ol>\n<li><strong>Start a 3-broker cluster</strong> (e.g., on ports 9092, 9093, 9094). Designate broker 1 as the coordinator/controller.</li>\n<li><strong>Create a topic</strong> with 1 partition and replication factor 3.</li>\n<li><strong>Produce messages</strong> with <code>acks=all</code>. Observe logs: the leader should append, and followers should fetch and append.</li>\n<li><strong>Kill the leader broker</strong> (e.g., kill -9). Wait for the session timeout (e.g., 10 seconds).</li>\n<li><strong>Produce more messages</strong> to the same topic. The new leader should be elected and accept writes.</li>\n<li><strong>Read all messages</strong> from the beginning. You should see <strong>no data loss</strong>—all messages from step 3 and step 5 should be present, in correct offset order.</li>\n</ol>\n<p><strong>Expected Logs:</strong></p>\n<ul>\n<li>Follower logs: &quot;Fetching from leader broker X at offset Y&quot;</li>\n<li>Leader logs: &quot;Updated ISR to [1,2,3]&quot;</li>\n<li>Coordinator logs: &quot;Broker 2 failed, electing new leader for topic T partition 0&quot;</li>\n<li>New leader logs: &quot;Assuming leadership for topic T partition 0, epoch Z&quot;</li>\n</ul>\n<p><strong>Debugging Tips:</strong></p>\n<ul>\n<li>If followers are not fetching, check that they know the correct leader (metadata cache).</li>\n<li>If ISR shrinks unexpectedly, check follower fetch frequency and network latency.</li>\n<li>If high watermark does not advance, ensure leader is calculating min LEO across ISR correctly.</li>\n</ul>\n<hr>\n<h2 id=\"9-interactions-and-data-flow\">9. Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 2 (Producer), 3 (Consumer Groups), 4 (Replication)</p>\n</blockquote>\n<p>Understanding how system components interact is crucial for building a correct distributed message queue. This section details the choreography between producers, brokers, consumers, and coordinators through three key perspectives: sequence diagrams showing the temporal flow of operations, the binary wire protocol that defines their communication format, and the coordination service that maintains cluster metadata. These three layers—behavioral, syntactic, and infrastructural—form the foundation of reliable distributed communication.</p>\n<h3 id=\"91-key-sequence-diagrams\">9.1 Key Sequence Diagrams</h3>\n<p>Sequence diagrams provide a visual timeline of interactions between system components, helping developers understand the causality and concurrency in distributed operations. Think of these diagrams as <strong>air traffic control logs</strong>—they show which aircraft (components) communicated with the tower (brokers) at what time, what instructions were exchanged, and how the overall flow of traffic is coordinated to avoid collisions and ensure safe delivery.</p>\n<h4 id=\"911-message-production-flow\">9.1.1 Message Production Flow</h4>\n<p>The producer-to-broker interaction follows a &quot;prepare-route-send-acknowledge&quot; pattern that balances throughput with durability guarantees. This flow resembles a <strong>courier service delivery process</strong>: a sender packages multiple items into a single shipment (batching), addresses them to specific destinations (partition selection), dispatches them via the most efficient route (network send), and waits for delivery confirmation (acknowledgment).</p>\n<p><img src=\"/api/project/build-kafka/architecture-doc/asset?path=diagrams%2Fdiagram-producer-flow.svg\" alt=\"Producer Send Sequence\"></p>\n<p>The complete production sequence involves these steps:</p>\n<ol>\n<li><p><strong>Metadata Resolution</strong>: The producer consults its local <code>MetadataCache</code> to determine which broker leads the target partition. If cache is stale or missing, it sends a metadata request to any bootstrap broker.</p>\n</li>\n<li><p><strong>Record Preparation</strong>: For each message, the producer:</p>\n<ul>\n<li>Serializes key, value, and headers into a <code>Record</code></li>\n<li>Applies optional compression to the entire batch</li>\n<li>Calculates CRC32 checksum for data integrity verification</li>\n</ul>\n</li>\n<li><p><strong>Partition Assignment</strong>: Using the configured <code>Partitioner</code> (typically <code>HashPartitioner</code>), the producer determines the target partition:</p>\n<ul>\n<li>For records with keys: <code>hash(key) % partition_count</code> ensures consistent mapping</li>\n<li>For null-key records: round-robin distribution across partitions</li>\n<li>The partition assignment determines which broker (leader) receives the batch</li>\n</ul>\n</li>\n<li><p><strong>Batch Accumulation</strong>: Records are grouped by <code>TopicPartition</code> in the <code>Accumulator</code>:</p>\n<ul>\n<li>Batches grow until reaching <code>BatchSize</code> bytes or <code>LingerMs</code> timeout</li>\n<li>Each batch becomes a <code>RecordBatch</code> with common metadata (base offset, timestamps)</li>\n<li>The accumulator maintains an in-memory buffer limited by <code>BufferMemory</code></li>\n</ul>\n</li>\n<li><p><strong>Network Dispatch</strong>: The <code>Sender</code> thread retrieves ready batches and:</p>\n<ul>\n<li>Establishes or reuses a TCP connection to the partition leader</li>\n<li>Encodes the batch using the binary protocol (Section 9.2)</li>\n<li>Sends the request with the configured <code>Acks</code> level</li>\n</ul>\n</li>\n<li><p><strong>Acknowledgement Handling</strong>: Based on the <code>Acks</code> configuration:</p>\n<ul>\n<li><code>AcksNone</code> (0): Fire-and-forget—no response expected, maximum throughput</li>\n<li><code>AcksLeader</code> (1): Wait for leader to append to its local log (fsync optional)</li>\n<li><code>AcksAll</code> (-1): Wait for all ISR replicas to acknowledge (highest durability)</li>\n<li>On timeout or error, <code>RetryItem</code> is scheduled with exponential backoff</li>\n</ul>\n</li>\n<li><p><strong>Delivery Completion</strong>: Successful acknowledgment includes:</p>\n<ul>\n<li>Partition ID and starting offset for the batch</li>\n<li>Leader epoch for fencing protection</li>\n<li>Error code (0 for success)</li>\n<li>The producer invokes any registered callback with this information</li>\n</ul>\n</li>\n</ol>\n<p>The critical insight is that batching happens at <strong>two levels</strong>: within the producer (multiple records) and within the broker (multiple batches to disk). This double batching amortizes network and disk I/O overhead, making high-throughput streaming possible.</p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Component</th>\n<th>Action</th>\n<th>Key Consideration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>Producer</td>\n<td>Fetch metadata</td>\n<td>Cache TTL vs. leader change frequency</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Producer</td>\n<td>Serialize record</td>\n<td>Compression trade-off: CPU vs. bandwidth</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Partitioner</td>\n<td>Select partition</td>\n<td>Hash consistency during partition count changes</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Accumulator</td>\n<td>Group records</td>\n<td>Memory pressure vs. latency trade-off</td>\n</tr>\n<tr>\n<td>5</td>\n<td>Sender</td>\n<td>Send batch</td>\n<td>Connection pooling and TCP congestion control</td>\n</tr>\n<tr>\n<td>6</td>\n<td>Broker</td>\n<td>Process request</td>\n<td>Durability vs. latency based on acks</td>\n</tr>\n<tr>\n<td>7</td>\n<td>Broker</td>\n<td>Send response</td>\n<td>Include leader epoch for client fencing</td>\n</tr>\n</tbody></table>\n<h4 id=\"912-consumer-group-rebalancing-flow\">9.1.2 Consumer Group Rebalancing Flow</h4>\n<p>Consumer group rebalancing is a distributed consensus protocol that redistributes partitions when membership changes. Imagine a <strong>sports team drafting players</strong>: when a new player joins or leaves, the team captain (coordinator) reassesses everyone&#39;s positions, reallocates responsibilities, and ensures each player knows exactly what territory they cover.</p>\n<p><img src=\"/api/project/build-kafka/architecture-doc/asset?path=diagrams%2Fdiagram-rebalance-sequence.svg\" alt=\"Consumer Group Rebalance Sequence\"></p>\n<p>The rebalancing protocol follows a three-phase &quot;join-sync-stabilize&quot; pattern:</p>\n<p><strong>Phase 1: Membership Change Detection</strong></p>\n<ol>\n<li>A new consumer calls <code>Subscribe()</code> or an existing consumer fails to send <code>Heartbeat</code> within <code>SessionTimeoutMs</code></li>\n<li>The <code>GroupCoordinator</code> detects membership change and transitions group state to <code>GroupStatePreparingRebalance</code></li>\n<li>All existing members receive <code>ErrRebalanceInProgress</code> on their next heartbeat, signaling them to rejoin</li>\n</ol>\n<p><strong>Phase 2: Join Group Synchronization</strong>\n4. Each consumer (including the new one) sends <code>JoinGroupRequest</code> containing:</p>\n<ul>\n<li><code>GroupID</code>: Identifier for the consumer group</li>\n<li><code>MemberID</code>: Current member ID (empty for new members)</li>\n<li><code>ProtocolType</code>: Always &quot;consumer&quot; for this system</li>\n<li><code>Protocols</code>: List of supported partition assignment strategies (range, round-robin)</li>\n<li><code>SessionTimeoutMs</code>: Maximum time without heartbeat before considered dead</li>\n</ul>\n<ol start=\"5\">\n<li>The coordinator waits up to <code>RebalanceTimeoutMs</code> for all expected members to join:<ul>\n<li>First joiner becomes the &quot;leader&quot; consumer (selected by coordinator)</li>\n<li>Coordinator records each member&#39;s subscribed topics and supported protocols</li>\n<li>If timeout expires, proceeds with currently joined members (stragglers excluded)</li>\n</ul>\n</li>\n</ol>\n<p><strong>Phase 3: Assignment Distribution</strong>\n6. The coordinator sends <code>JoinGroupResponse</code> to all members containing:</p>\n<ul>\n<li><code>GenerationID</code>: Incremented epoch number for this assignment</li>\n<li><code>LeaderID</code>: Member ID of the designated leader consumer</li>\n<li>Assigned <code>MemberID</code> for each consumer (new members receive generated IDs)</li>\n</ul>\n<ol start=\"7\">\n<li><p>The leader consumer computes partition assignments using the selected <code>PartitionAssigner</code>:</p>\n<ul>\n<li>Gathers all subscribed topics from member metadata</li>\n<li>Applies assignment strategy (range or round-robin) to distribute partitions</li>\n<li>Creates <code>Assignment</code> mapping from member ID to topic-partition list</li>\n</ul>\n</li>\n<li><p>Each member sends <code>SyncGroupRequest</code>:</p>\n<ul>\n<li>Leader includes the computed assignments in its request</li>\n<li>Followers send empty assignment payload</li>\n</ul>\n</li>\n<li><p>Coordinator distributes assignments via <code>SyncGroupResponse</code>:</p>\n<ul>\n<li>Validates leader&#39;s assignment covers all partitions</li>\n<li>Sends each member their specific assigned partitions</li>\n<li>Updates stored group metadata with new generation</li>\n</ul>\n</li>\n</ol>\n<p><strong>Phase 4: Stable Operation</strong>\n10. Upon receiving assignments, each consumer:\n    - Updates its <code>GroupMember</code> state to <code>StateStable</code>\n    - Begins fetching from assigned partitions starting from committed offsets\n    - Resumes periodic heartbeats to maintain membership</p>\n<ol start=\"11\">\n<li>The coordinator transitions group to <code>GroupStateStable</code> and monitors heartbeats</li>\n</ol>\n<p>The protocol ensures <strong>exactly-once partition ownership</strong> within a generation: while a consumer holds a generation ID, it uniquely owns its assigned partitions. If a consumer fails to heartbeat, the coordinator increments the generation, invalidating the previous owner&#39;s claims.</p>\n<table>\n<thead>\n<tr>\n<th>Rebalance Trigger</th>\n<th>Detection Mechanism</th>\n<th>Coordinator Action</th>\n<th>Consumer Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>New consumer joins</td>\n<td><code>JoinGroupRequest</code> from unknown member</td>\n<td>Transition to <code>GroupStatePreparingRebalance</code></td>\n<td>Re-send join after receiving error</td>\n</tr>\n<tr>\n<td>Consumer leaves</td>\n<td>Heartbeat timeout after <code>SessionTimeoutMs</code></td>\n<td>Mark member dead, trigger rebalance</td>\n<td>N/A (consumer is dead)</td>\n</tr>\n<tr>\n<td>Consumer crashes</td>\n<td>TCP connection drop + heartbeat timeout</td>\n<td>Same as leave</td>\n<td>N/A (consumer crashed)</td>\n</tr>\n<tr>\n<td>Topic metadata changes</td>\n<td>Admin API or broker notification</td>\n<td>Trigger rebalance if partition count changed</td>\n<td>Rejoin when notified</td>\n</tr>\n<tr>\n<td>Manual rebalance</td>\n<td>External command via API</td>\n<td>Force transition to rebalancing state</td>\n<td>All members receive error code</td>\n</tr>\n</tbody></table>\n<h4 id=\"913-message-consumption-flow\">9.1.3 Message Consumption Flow</h4>\n<p>Consumer message fetching follows a &quot;poll-fetch-advance-commit&quot; cycle that balances throughput with memory constraints. Picture a <strong>library checkout system</strong>: patrons (consumers) periodically visit the library (broker), check out several books (records) at once, read them at their own pace, and update their checkout record (commit offset) before returning for more.</p>\n<p>The consumption sequence proceeds as follows:</p>\n<ol>\n<li><p><strong>Subscription Initialization</strong>: Consumer calls <code>Subscribe()</code> which triggers the join-sync protocol (Section 9.1.2), resulting in assigned partitions.</p>\n</li>\n<li><p><strong>Offset Initialization</strong>: For each assigned partition, consumer determines starting position:</p>\n<ul>\n<li>If <code>auto.offset.reset</code> = &quot;earliest&quot;: starts at partition&#39;s lowest available offset</li>\n<li>If &quot;latest&quot;: starts at current <code>LogEndOffset</code> (only new messages)</li>\n<li>If valid committed offset exists: resumes from <code>OffsetStore</code> persisted value</li>\n<li>Consumer maintains <code>FetchOffset</code> per partition as its read position</li>\n</ul>\n</li>\n<li><p><strong>Fetch Request Preparation</strong>: Consumer builds <code>FetchRequest</code> containing:</p>\n<ul>\n<li>Maximum wait time (<code>MaxWaitMs</code>) for broker to accumulate data</li>\n<li>Minimum bytes (<code>MinBytes</code>) to return before satisfying request</li>\n<li>Maximum bytes (<code>MaxBytes</code>) per partition to prevent memory overflow</li>\n<li>Per-partition fetch state: <code>TopicPartition</code>, <code>FetchOffset</code>, <code>MaxBytes</code></li>\n</ul>\n</li>\n<li><p><strong>Broker Processing</strong>: For each requested partition, broker leader:</p>\n<ul>\n<li>Validates consumer has read permission and offset is within range</li>\n<li>Reads from <code>Log</code> starting at <code>FetchOffset</code> up to <code>MaxBytes</code> or log end</li>\n<li>Returns records with <code>HighWatermark</code> (last safely replicable offset)</li>\n<li>If <code>FetchOffset</code> ≥ <code>HighWatermark</code>, waits up to <code>MaxWaitMs</code> for new data</li>\n<li>Returns empty batch if no new data after wait period</li>\n</ul>\n</li>\n<li><p><strong>Record Delivery</strong>: Consumer receives <code>FetchResponse</code> and:</p>\n<ul>\n<li>Processes records in offset order within each partition</li>\n<li>Delivers to application via <code>Poll()</code> return or callback</li>\n<li>Advances <code>FetchOffset</code> past delivered records</li>\n<li>Updates metrics (bytes consumed, lag, throughput)</li>\n</ul>\n</li>\n<li><p><strong>Offset Commitment</strong>: Periodically or explicitly, consumer commits progress:</p>\n<ul>\n<li><code>CommitSync()</code> blocks until offset stored durably</li>\n<li><code>CommitAsync()</code> returns immediately, calls callback on completion</li>\n<li>Offset committed to <code>__consumer_offsets</code> internal topic via broker</li>\n<li>Coordinator validates generation ID to prevent stale commits</li>\n</ul>\n</li>\n<li><p><strong>Rebalance Handling</strong>: If rebalance occurs during consumption:</p>\n<ul>\n<li>Consumer completes current batch processing</li>\n<li>Commits offsets for revoked partitions</li>\n<li>Releases partition ownership (stops fetching)</li>\n<li>Rejoins group for new assignment</li>\n</ul>\n</li>\n</ol>\n<p>The <strong>fetch session</strong> optimization is crucial: consumers maintain long-lived connections to brokers, sending incremental fetch requests that only list changed partitions. This reduces request size and parsing overhead for steady-state consumption.</p>\n<h3 id=\"92-wire-protocol-specification\">9.2 Wire Protocol Specification</h3>\n<p>The wire protocol defines the binary format for all network communication between components. Think of it as the <strong>international shipping container standard</strong>: just as standardized containers enable efficient global logistics regardless of cargo type, a well-defined binary protocol enables interoperability between different client implementations and server versions while minimizing parsing overhead.</p>\n<h4 id=\"921-protocol-design-principles\">9.2.1 Protocol Design Principles</h4>\n<p>The protocol follows several key design decisions documented in this ADR:</p>\n<blockquote>\n<p><strong>Decision: Binary Protocol Over Text Protocol</strong></p>\n<ul>\n<li><strong>Context</strong>: Need for high-throughput, low-latency communication between distributed components with efficient parsing and minimal serialization overhead.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Text-based (JSON/XML)</strong>: Human-readable, easy to debug, but high parsing cost and large payload size</li>\n<li><strong>Binary with TLV (Type-Length-Value)</strong>: Self-describing, flexible evolution, but header overhead per field</li>\n<li><strong>Fixed-position binary</strong>: Compact, fast parsing, but requires versioning for schema changes</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Fixed-position binary protocol with versioned requests and explicit schema evolution rules.</li>\n<li><strong>Rationale</strong>: Throughput and latency are primary concerns for a messaging system. Binary encoding reduces serialization costs by 5-10x compared to JSON, and fixed-position parsing avoids heap allocations during deserialization. Versioning allows backward-compatible evolution.</li>\n<li><strong>Consequences</strong>: Protocol debugging requires specialized tools, but the performance benefits justify the complexity. Schema changes must follow compatibility rules (add-only fields, bump version).</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Protocol Aspect</th>\n<th>Design Choice</th>\n<th>Rationale</th>\n<th>Trade-off</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Encoding</strong></td>\n<td>Binary, big-endian</td>\n<td>Network byte order standard</td>\n<td>Not human-readable</td>\n</tr>\n<tr>\n<td><strong>Framing</strong></td>\n<td>Length-prefixed</td>\n<td>Simple parsing, no delimiters</td>\n<td>Requires length prefix overhead</td>\n</tr>\n<tr>\n<td><strong>Versioning</strong></td>\n<td>Per-request API key + version</td>\n<td>Backward compatibility</td>\n<td>Multiple code paths</td>\n</tr>\n<tr>\n<td><strong>Error handling</strong></td>\n<td>Error code per response partition</td>\n<td>Granular failure reporting</td>\n<td>Additional complexity</td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td>Per-record-batch</td>\n<td>Reduces network bandwidth</td>\n<td>CPU overhead, delayed batching</td>\n</tr>\n</tbody></table>\n<h4 id=\"922-common-requestresponse-structure\">9.2.2 Common Request/Response Structure</h4>\n<p>All protocol messages follow the same envelope structure:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Size</code></td>\n<td>int32</td>\n<td>Total message size in bytes (excluding this field)</td>\n</tr>\n<tr>\n<td><code>APIKey</code></td>\n<td>int16</td>\n<td>Numeric identifier for the request type (Produce=0, Fetch=1, etc.)</td>\n</tr>\n<tr>\n<td><code>APIVersion</code></td>\n<td>int16</td>\n<td>Version of the API for forward/backward compatibility</td>\n</tr>\n<tr>\n<td><code>CorrelationID</code></td>\n<td>int32</td>\n<td>Client-generated ID to match responses to requests</td>\n</tr>\n<tr>\n<td><code>ClientID</code></td>\n<td>string</td>\n<td>Client identifier for debugging (nullable)</td>\n</tr>\n<tr>\n<td><code>Request/Response Body</code></td>\n<td>variable</td>\n<td>API-specific structured data</td>\n</tr>\n</tbody></table>\n<p>The <strong>version negotiation</strong> process: clients send requests with their supported version; brokers respond with the same version if supported, or downgrade/error if not. This enables gradual feature rollout without breaking existing clients.</p>\n<h4 id=\"923-produce-requestresponse-format\">9.2.3 Produce Request/Response Format</h4>\n<p>The Produce API writes records to partition logs with configurable durability guarantees.</p>\n<p><strong>ProduceRequest (API Key: 0)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TransactionID</code></td>\n<td>int64</td>\n<td>For idempotent producers, 0 otherwise</td>\n</tr>\n<tr>\n<td><code>Acks</code></td>\n<td>int16</td>\n<td>Required acknowledgments: -1=all, 0=none, 1=leader</td>\n</tr>\n<tr>\n<td><code>TimeoutMs</code></td>\n<td>int32</td>\n<td>Maximum time to wait for <code>Acks</code> fulfillment</td>\n</tr>\n<tr>\n<td><code>TopicData</code></td>\n<td>array</td>\n<td>Topics to produce to (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>TopicData Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Topic</code></td>\n<td>string</td>\n<td>Topic name</td>\n</tr>\n<tr>\n<td><code>Data</code></td>\n<td>array</td>\n<td>Partition data (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>PartitionData Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Partition</code></td>\n<td>int32</td>\n<td>Partition index</td>\n</tr>\n<tr>\n<td><code>RecordSet</code></td>\n<td>bytes</td>\n<td>Serialized <code>RecordBatch</code> (Section 4.2)</td>\n</tr>\n</tbody></table>\n<p><strong>ProduceResponse</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Responses</code></td>\n<td>array</td>\n<td>Per-topic response (see below)</td>\n</tr>\n<tr>\n<td><code>ThrottleTimeMs</code></td>\n<td>int32</td>\n<td>Time client should wait before next request</td>\n</tr>\n</tbody></table>\n<p><strong>TopicResponse Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Topic</code></td>\n<td>string</td>\n<td>Topic name</td>\n</tr>\n<tr>\n<td><code>PartitionResponses</code></td>\n<td>array</td>\n<td>Per-partition response (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>PartitionResponse Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Partition</code></td>\n<td>int32</td>\n<td>Partition index</td>\n</tr>\n<tr>\n<td><code>ErrorCode</code></td>\n<td>int16</td>\n<td>0=success, non-zero error codes</td>\n</tr>\n<tr>\n<td><code>BaseOffset</code></td>\n<td>int64</td>\n<td>Offset of first record in batch</td>\n</tr>\n<tr>\n<td><code>LogAppendTime</code></td>\n<td>int64</td>\n<td>Broker timestamp when appended</td>\n</tr>\n<tr>\n<td><code>LogStartOffset</code></td>\n<td>int64</td>\n<td>New log start offset after compaction</td>\n</tr>\n</tbody></table>\n<p>The <code>RecordSet</code> field contains a complete serialized <code>RecordBatch</code> as defined in Section 4.2, including compression. Brokers validate CRC before appending to log.</p>\n<h4 id=\"924-fetch-requestresponse-format\">9.2.4 Fetch Request/Response Format</h4>\n<p>The Fetch API reads records from partition logs with configurable batching and wait semantics.</p>\n<p><strong>FetchRequest (API Key: 1)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ReplicaID</code></td>\n<td>int32</td>\n<td>-1 for consumers, broker ID for replication</td>\n</tr>\n<tr>\n<td><code>MaxWaitMs</code></td>\n<td>int32</td>\n<td>Maximum time to block waiting for <code>MinBytes</code></td>\n</tr>\n<tr>\n<td><code>MinBytes</code></td>\n<td>int32</td>\n<td>Minimum bytes to accumulate before responding</td>\n</tr>\n<tr>\n<td><code>MaxBytes</code></td>\n<td>int32</td>\n<td>Maximum bytes to return in response</td>\n</tr>\n<tr>\n<td><code>IsolationLevel</code></td>\n<td>int8</td>\n<td>0=read uncommitted, 1=read committed (≤ high watermark)</td>\n</tr>\n<tr>\n<td><code>SessionID</code></td>\n<td>int32</td>\n<td>Fetch session ID for incremental fetches</td>\n</tr>\n<tr>\n<td><code>SessionEpoch</code></td>\n<td>int32</td>\n<td>Fetch session epoch for incremental fetches</td>\n</tr>\n<tr>\n<td><code>Topics</code></td>\n<td>array</td>\n<td>Topics to fetch from (see below)</td>\n</tr>\n<tr>\n<td><code>ForgottenTopics</code></td>\n<td>array</td>\n<td>Topics to remove from session (incremental only)</td>\n</tr>\n</tbody></table>\n<p><strong>TopicFetch Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Topic</code></td>\n<td>string</td>\n<td>Topic name</td>\n</tr>\n<tr>\n<td><code>Partitions</code></td>\n<td>array</td>\n<td>Partitions to fetch (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>PartitionFetch Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Partition</code></td>\n<td>int32</td>\n<td>Partition index</td>\n</tr>\n<tr>\n<td><code>FetchOffset</code></td>\n<td>int64</td>\n<td>Starting offset to fetch from</td>\n</tr>\n<tr>\n<td><code>LogStartOffset</code></td>\n<td>int64</td>\n<td>Earliest available offset (for truncation detection)</td>\n</tr>\n<tr>\n<td><code>MaxBytes</code></td>\n<td>int32</td>\n<td>Maximum bytes to fetch for this partition</td>\n</tr>\n</tbody></table>\n<p><strong>FetchResponse</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ThrottleTimeMs</code></td>\n<td>int32</td>\n<td>Time client should wait before next request</td>\n</tr>\n<tr>\n<td><code>ErrorCode</code></td>\n<td>int16</td>\n<td>Top-level error (0=success)</td>\n</tr>\n<tr>\n<td><code>SessionID</code></td>\n<td>int32</td>\n<td>Fetch session ID (if incremental)</td>\n</tr>\n<tr>\n<td><code>Responses</code></td>\n<td>array</td>\n<td>Per-topic response (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>TopicFetchResponse Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Topic</code></td>\n<td>string</td>\n<td>Topic name</td>\n</tr>\n<tr>\n<td><code>PartitionResponses</code></td>\n<td>array</td>\n<td>Per-partition response (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>PartitionFetchResponse Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Partition</code></td>\n<td>int32</td>\n<td>Partition index</td>\n</tr>\n<tr>\n<td><code>ErrorCode</code></td>\n<td>int16</td>\n<td>Partition-level error (0=success)</td>\n</tr>\n<tr>\n<td><code>HighWatermark</code></td>\n<td>int64</td>\n<td>Last committed offset in partition</td>\n</tr>\n<tr>\n<td><code>LastStableOffset</code></td>\n<td>int64</td>\n<td>Last stable offset (for transactions)</td>\n</tr>\n<tr>\n<td><code>LogStartOffset</code></td>\n<td>int64</td>\n<td>Earliest available offset</td>\n</tr>\n<tr>\n<td><code>AbortedTransactions</code></td>\n<td>array</td>\n<td>List of aborted transactions (for read committed)</td>\n</tr>\n<tr>\n<td><code>RecordSet</code></td>\n<td>bytes</td>\n<td>Serialized <code>RecordBatch</code> (empty if no data)</td>\n</tr>\n</tbody></table>\n<p>The <strong>incremental fetch session</strong> optimization: when <code>SessionID</code> ≠ 0, broker remembers requested partitions across requests. Subsequent requests only need to list changed partitions (<code>ForgottenTopics</code> to remove), reducing request size significantly.</p>\n<h4 id=\"925-joingroup-requestresponse-format\">9.2.5 JoinGroup Request/Response Format</h4>\n<p>The JoinGroup API coordinates consumer group membership and leader election.</p>\n<p><strong>JoinGroupRequest (API Key: 11)</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>GroupID</code></td>\n<td>string</td>\n<td>Consumer group identifier</td>\n</tr>\n<tr>\n<td><code>SessionTimeoutMs</code></td>\n<td>int32</td>\n<td>Time after which inactive members are removed</td>\n</tr>\n<tr>\n<td><code>RebalanceTimeoutMs</code></td>\n<td>int32</td>\n<td>Maximum time coordinator waits for members</td>\n</tr>\n<tr>\n<td><code>MemberID</code></td>\n<td>string</td>\n<td>Current member ID (empty for new members)</td>\n</tr>\n<tr>\n<td><code>ProtocolType</code></td>\n<td>string</td>\n<td>&quot;consumer&quot; for consumer groups</td>\n</tr>\n<tr>\n<td><code>Protocols</code></td>\n<td>array</td>\n<td>Supported partition assignment protocols (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>GroupProtocol Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Name</code></td>\n<td>string</td>\n<td>Protocol name (e.g., &quot;range&quot;, &quot;roundrobin&quot;)</td>\n</tr>\n<tr>\n<td><code>Metadata</code></td>\n<td>bytes</td>\n<td>Serialized protocol metadata (see below)</td>\n</tr>\n</tbody></table>\n<p><strong>ProtocolMetadata Structure</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Version</code></td>\n<td>int16</td>\n<td>Metadata format version</td>\n</tr>\n<tr>\n<td><code>Topics</code></td>\n<td>array</td>\n<td>Subscribed topics list</td>\n</tr>\n<tr>\n<td><code>UserData</code></td>\n<td>bytes</td>\n<td>Optional custom data</td>\n</tr>\n</tbody></table>\n<p><strong>JoinGroupResponse</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ErrorCode</code></td>\n<td>int16</td>\n<td>Top-level error (0=success)</td>\n</tr>\n<tr>\n<td><code>GenerationID</code></td>\n<td>int32</td>\n<td>Group generation identifier (increments each rebalance)</td>\n</tr>\n<tr>\n<td><code>ProtocolName</code></td>\n<td>string</td>\n<td>Selected protocol by coordinator</td>\n</tr>\n<tr>\n<td><code>LeaderID</code></td>\n<td>string</td>\n<td>Member ID of group leader</td>\n</tr>\n<tr>\n<td><code>MemberID</code></td>\n<td>string</td>\n<td>Assigned member ID for this consumer</td>\n</tr>\n<tr>\n<td><code>Members</code></td>\n<td>array</td>\n<td>Group member metadata (only returned to leader)</td>\n</tr>\n</tbody></table>\n<p><strong>MemberMetadata Array Element</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MemberID</code></td>\n<td>string</td>\n<td>Member identifier</td>\n</tr>\n<tr>\n<td><code>Metadata</code></td>\n<td>bytes</td>\n<td>Serialized protocol metadata (same as request)</td>\n</tr>\n</tbody></table>\n<p>The coordinator selects the <strong>protocol</strong> based on member intersection: if all members support &quot;range&quot;, it&#39;s selected; otherwise, coordinator picks highest common protocol. This enables rolling upgrades of assignment strategies.</p>\n<h3 id=\"93-coordination-service-abstraction\">9.3 Coordination Service Abstraction</h3>\n<p>The coordination service maintains cluster metadata and enables consensus for critical operations like leader election. Think of it as the <strong>air traffic control database</strong>: it doesn&#39;t control planes directly but maintains the authoritative registry of which runways are active, which controllers are on duty, and which flight plans are approved—information all pilots and towers reference to coordinate safely.</p>\n<h4 id=\"931-minimal-coordination-interface\">9.3.1 Minimal Coordination Interface</h4>\n<p>For our educational system, we define a simplified coordination interface that can be implemented with various backends (in-memory, etcd, ZooKeeper). This abstraction follows the <strong>registry pattern</strong>: components register themselves and subscribe to changes, receiving notifications when relevant metadata updates.</p>\n<p><strong>Core Coordination Interface Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Signature</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RegisterBroker(broker *Broker) error</code></td>\n<td><code>broker</code>: Broker metadata</td>\n<td>error</td>\n<td>Registers a broker with the cluster; generates unique ID if not set</td>\n</tr>\n<tr>\n<td><code>DeregisterBroker(brokerID int) error</code></td>\n<td><code>brokerID</code>: Broker identifier</td>\n<td>error</td>\n<td>Removes broker from cluster; triggers partition reassignment</td>\n</tr>\n<tr>\n<td><code>GetBrokers() ([]*Broker, error)</code></td>\n<td>None</td>\n<td>Broker list, error</td>\n<td>Returns all currently registered brokers</td>\n</tr>\n<tr>\n<td><code>CreateTopic(topic *TopicMetadata) error</code></td>\n<td><code>topic</code>: Topic configuration</td>\n<td>error</td>\n<td>Creates topic with specified partitions and replication factor</td>\n</tr>\n<tr>\n<td><code>DeleteTopic(topicName string) error</code></td>\n<td><code>topicName</code>: Topic to delete</td>\n<td>error</td>\n<td>Marks topic for deletion; brokers clean up replicas</td>\n</tr>\n<tr>\n<td><code>GetTopicMetadata(topicName string) (*TopicMetadata, error)</code></td>\n<td><code>topicName</code>: Topic name</td>\n<td>Metadata, error</td>\n<td>Returns current partition leadership and replica assignments</td>\n</tr>\n<tr>\n<td><code>UpdatePartitionLeadership(partition *PartitionMetadata) error</code></td>\n<td><code>partition</code>: Updated metadata</td>\n<td>error</td>\n<td>Updates leader and ISR for a partition (atomic compare-and-swap)</td>\n</tr>\n<tr>\n<td><code>ElectPartitionLeader(topic string, partition int, isr []int) (int, error)</code></td>\n<td><code>topic</code>, <code>partition</code>, <code>isr</code></td>\n<td>leaderID, error</td>\n<td>Selects new leader from ISR using deterministic algorithm</td>\n</tr>\n<tr>\n<td><code>WatchBrokers(callback func([]*Broker)) (cancel func(), error)</code></td>\n<td><code>callback</code>: Change handler</td>\n<td>cancel function, error</td>\n<td>Registers for broker list changes (for client metadata updates)</td>\n</tr>\n<tr>\n<td><code>WatchTopic(topicName string, callback func(*TopicMetadata)) (cancel func(), error)</code></td>\n<td><code>topicName</code>, <code>callback</code></td>\n<td>cancel function, error</td>\n<td>Registers for topic metadata changes (for producer routing)</td>\n</tr>\n</tbody></table>\n<p><strong>State Management Guarantees:</strong></p>\n<blockquote>\n<p><strong>Design Principle: Eventual Consistency with Strong Consistency for Critical Paths</strong></p>\n<ul>\n<li><strong>Metadata reads</strong> may be slightly stale (cached for performance)</li>\n<li><strong>Leadership elections</strong> are linearizable (all observers see same leader)</li>\n<li><strong>Partition assignment</strong> uses compare-and-swap to prevent lost updates</li>\n<li><strong>Watch notifications</strong> are at-least-once (may receive duplicates)</li>\n</ul>\n</blockquote>\n<h4 id=\"932-metadata-propagation-patterns\">9.3.2 Metadata Propagation Patterns</h4>\n<p>Cluster metadata flows through the system using a <strong>publish-subscribe with cache invalidation</strong> pattern. This minimizes coordination traffic while ensuring timely updates:</p>\n<ol>\n<li><strong>Broker Registration Flow</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   Broker startup → RegisterBroker() → Coordination service stores → \n   WatchBrokers() callbacks fire → All brokers update ClusterMetadata →\n   Clients fetch metadata on next request → Updated routing tables</code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Leader Election Flow</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   Leader failure detected → ElectPartitionLeader() → Coordination service selects →\n   UpdatePartitionLeadership() → WatchTopic() callbacks fire → \n   Followers update their replica state → Clients get ErrNotLeaderForPartition →\n   Client metadata refresh → New routing established</code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Topic Creation Flow</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   Admin creates topic → CreateTopic() → Coordination service validates →\n   Assign partitions to brokers → Store TopicMetadata → \n   WatchTopic() callbacks fire → Brokers create local log directories →\n   Clients can produce/consume</code></pre></div>\n\n<p>The <strong>metadata cache lifecycle</strong> in clients:</p>\n<ul>\n<li>Cache initialized with bootstrap broker list</li>\n<li>On each request, check cache freshness (timestamp)</li>\n<li>On <code>ErrNotLeaderForPartition</code> or <code>ErrUnknownTopicOrPartition</code>, force refresh</li>\n<li>Refresh interval: min(metadata.max.age.ms, calculated based on error rate)</li>\n<li>Cache entries include generation ID to detect stale updates</li>\n</ul>\n<h4 id=\"933-fault-tolerance-considerations\">9.3.3 Fault Tolerance Considerations</h4>\n<p>The coordination service itself must be highly available. Our abstraction supports different implementations with varying consistency guarantees:</p>\n<table>\n<thead>\n<tr>\n<th>Implementation</th>\n<th>Consistency Model</th>\n<th>Failure Handling</th>\n<th>Recommended Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>In-memory with single leader</strong></td>\n<td>Strong consistency (leader)</td>\n<td>Leader election via Raft/ZooKeeper</td>\n<td>Small clusters, educational use</td>\n</tr>\n<tr>\n<td><strong>Embedded etcd</strong></td>\n<td>Linearizable via Raft</td>\n<td>Automatic failover, data replication</td>\n<td>Production-like deployment</td>\n</tr>\n<tr>\n<td><strong>External ZooKeeper</strong></td>\n<td>Sequential consistency</td>\n<td>Ensemble quorum maintenance</td>\n<td>Integration with existing infra</td>\n</tr>\n<tr>\n<td><strong>Gossip-based</strong></td>\n<td>Eventual consistency</td>\n<td>No single point of failure</td>\n<td>Large-scale, AP-focused systems</td>\n</tr>\n</tbody></table>\n<p><strong>Critical coordination paths requiring strong consistency:</strong></p>\n<ol>\n<li><strong>Partition leadership</strong>: To prevent split-brain (two brokers believing they&#39;re leader)</li>\n<li><strong>Consumer group generation</strong>: To prevent duplicate processing during rebalance</li>\n<li><strong>Transaction state</strong>: For exactly-once semantics (future extension)</li>\n<li><strong>Controller election</strong>: To ensure single active controller broker</li>\n</ol>\n<p><strong>Eventual consistency acceptable for:</strong></p>\n<ol>\n<li><strong>Broker liveness</strong>: Slight delay in detecting failures acceptable</li>\n<li><strong>Topic configuration</strong>: Temporary inconsistency doesn&#39;t cause data loss</li>\n<li><strong>Client metadata</strong>: Stale routing corrected via error retry</li>\n</ol>\n<p>The coordination service represents a <strong>single point of truth</strong> but not a <strong>single point of failure</strong> when implemented with replication and automatic failover.</p>\n<h3 id=\"94-implementation-guidance\">9.4 Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Protocol Serialization</strong></td>\n<td>Manual byte packing/unpacking</td>\n<td>Protocol Buffers with code generation</td>\n</tr>\n<tr>\n<td><strong>Connection Management</strong></td>\n<td><code>net.Conn</code> with manual pooling</td>\n<td>Custom connection pool with keep-alive</td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td>None (simplicity)</td>\n<td><code>compress/gzip</code> or third-party Snappy</td>\n</tr>\n<tr>\n<td><strong>Coordination Service</strong></td>\n<td>In-memory with file persistence</td>\n<td>Embedded etcd (<code>go.etcd.io/etcd/client/v3</code>)</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    broker/                # Broker entry point\n      main.go\n    producer/              # Producer CLI example\n      main.go\n    consumer/              # Consumer CLI example\n      main.go\n  internal/\n    protocol/              # Wire protocol definitions\n      api_keys.go          # API key constants (Produce=0, Fetch=1, etc.)\n      requests.go          # Request structs and serialization\n      responses.go         # Response structs and serialization\n      errors.go            # Error code definitions\n      framing.go           # Length-prefixed message framing\n    network/\n      connection.go        # TCP connection wrapper with timeout\n      pool.go              # Connection pool management\n      server.go            # TCPServer implementation\n    coordination/          # Coordination service abstraction\n      interface.go         # CoordinationService interface\n      memory/              # In-memory implementation\n        store.go           # In-memory metadata store\n        watcher.go         # Watch notification system\n      etcd/                # etcd implementation (optional)\n        client.go\n    metadata/              # Client metadata cache\n      cache.go             # Metadata caching with TTL\n      refresher.go         # Background metadata refresh</code></pre></div>\n\n<h4 id=\"protocol-framing-infrastructure-complete\">Protocol Framing Infrastructure (Complete)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/protocol/framing.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrMessageTooLarge </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"message size exceeds maximum allowed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidSize     </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid message size field\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MaxMessageSize</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\"> // 100 MB maximum message size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SizeFieldLen</span><span style=\"color:#F97583\">   =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">                 // int32 size prefix</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadMessage reads a length-prefixed message from the reader</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ReadMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read size prefix (4 bytes, big-endian)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> sizeBuf [</span><span style=\"color:#B392F0\">SizeFieldLen</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadFull</span><span style=\"color:#E1E4E8\">(r, sizeBuf[:]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">(binary.BigEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(sizeBuf[:]))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> size </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> size </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> MaxMessageSize {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidSize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Allocate buffer and read message body</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buf </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, size)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadFull</span><span style=\"color:#E1E4E8\">(r, buf); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> buf, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteMessage writes a length-prefixed message to the writer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> WriteMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> size </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> MaxMessageSize {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrMessageTooLarge</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write size prefix</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> sizeBuf [</span><span style=\"color:#B392F0\">SizeFieldLen</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint32</span><span style=\"color:#E1E4E8\">(sizeBuf[:], </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">(size))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(sizeBuf[:]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write message body</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MessageHeader represents the common header for all requests/responses</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MessageHeader</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Size          </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\">  // Already read by framing layer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    APIKey        </span><span style=\"color:#F97583\">int16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    APIVersion    </span><span style=\"color:#F97583\">int16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CorrelationID </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClientID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Nullable string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DecodeHeader parses the common header from a message buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> DecodeHeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">buf</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MessageHeader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(buf) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#6A737D\">// Minimum header size (without ClientID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"buffer too small for header\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MessageHeader</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // APIKey (int16)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.APIKey </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> int16</span><span style=\"color:#E1E4E8\">(binary.BigEndian.</span><span style=\"color:#B392F0\">Uint16</span><span style=\"color:#E1E4E8\">(buf[pos:]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // APIVersion (int16)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.APIVersion </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> int16</span><span style=\"color:#E1E4E8\">(binary.BigEndian.</span><span style=\"color:#B392F0\">Uint16</span><span style=\"color:#E1E4E8\">(buf[pos:]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // CorrelationID (int32)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.CorrelationID </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">(binary.BigEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(buf[pos:]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ClientID (nullable string: int16 length + bytes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clientIDLen </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int16</span><span style=\"color:#E1E4E8\">(binary.BigEndian.</span><span style=\"color:#B392F0\">Uint16</span><span style=\"color:#E1E4E8\">(buf[pos:]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> clientIDLen </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        h.ClientID </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> clientIDLen </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> pos</span><span style=\"color:#F97583\">+int</span><span style=\"color:#E1E4E8\">(clientIDLen) </span><span style=\"color:#F97583\">></span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(buf) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"buffer underflow for ClientID\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        h.ClientID </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(buf[pos : pos</span><span style=\"color:#F97583\">+int</span><span style=\"color:#E1E4E8\">(clientIDLen)])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(clientIDLen)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid ClientID length\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> h, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EncodeHeader serializes the header to bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> EncodeHeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">h</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MessageHeader</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate total size (APIKey + APIVersion + CorrelationID + ClientID length + ClientID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> +</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(h.ClientID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buf </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, size)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // APIKey</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint16</span><span style=\"color:#E1E4E8\">(buf[pos:], </span><span style=\"color:#F97583\">uint16</span><span style=\"color:#E1E4E8\">(h.APIKey))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // APIVersion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint16</span><span style=\"color:#E1E4E8\">(buf[pos:], </span><span style=\"color:#F97583\">uint16</span><span style=\"color:#E1E4E8\">(h.APIVersion))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // CorrelationID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint32</span><span style=\"color:#E1E4E8\">(buf[pos:], </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">(h.CorrelationID))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ClientID (nullable string)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> h.ClientID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint16</span><span style=\"color:#E1E4E8\">(buf[pos:], </span><span style=\"color:#F97583\">uint16</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        binary.BigEndian.</span><span style=\"color:#B392F0\">PutUint16</span><span style=\"color:#E1E4E8\">(buf[pos:], </span><span style=\"color:#F97583\">uint16</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(h.ClientID)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        copy</span><span style=\"color:#E1E4E8\">(buf[pos:], h.ClientID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pos </span><span style=\"color:#F97583\">+=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(h.ClientID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> buf, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-protocol-serialization-skeleton\">Core Protocol Serialization Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/protocol/requests.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProduceRequest represents a produce API request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ProduceRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TransactionalID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Acks           </span><span style=\"color:#F97583\">int16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeoutMs      </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TopicData      []</span><span style=\"color:#B392F0\">TopicProduceData</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TopicProduceData represents topic-level produce data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TopicProduceData</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Topic      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Partitions []</span><span style=\"color:#B392F0\">PartitionProduceData</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PartitionProduceData represents partition-level produce data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PartitionProduceData</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Partition  </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecordSet  []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#6A737D\"> // Serialized RecordBatch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DecodeProduceRequest parses a ProduceRequest from bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> DecodeProduceRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">apiVersion</span><span style=\"color:#F97583\"> int16</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">buf</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ProduceRequest</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ProduceRequest</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: For apiVersion >= 3, read TransactionalID (nullable string)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Read int16 length, if -1 set to \"\", else read that many bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read Acks (int16)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Read TimeoutMs (int32)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Read array size (int32) for TopicData</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For each topic:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a) Read topic name (nullable string)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b) Read array size (int32) for Partitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c) For each partition:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        i) Read Partition (int32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        ii) Read RecordSet size (int32) and bytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        iii) Append to Partitions slice</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate buffer boundaries (pos should equal len(buf))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> req, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EncodeProduceRequest serializes a ProduceRequest to bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> EncodeProduceRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ProduceRequest</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">apiVersion</span><span style=\"color:#F97583\"> int16</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate total buffer size needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Iterate through all nested structures counting bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Allocate buffer with correct size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write TransactionalID if apiVersion >= 3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Nullable string: length (-1 for null) then bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write Acks (int16, big-endian)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Write TimeoutMs (int32)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Write TopicData array size (int32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For each topic:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a) Write topic name (nullable string)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b) Write Partitions array size (int32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c) For each partition:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        i) Write Partition (int32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        ii) Write RecordSet length (int32) followed by bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleProduceRequest processes a produce request on the broker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> HandleProduceRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ProduceRequest</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">broker</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ProduceResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ProduceResponse</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Responses: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">TopicProduceResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(req.TopicData)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, topicData </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> req.TopicData {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        topicResp </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> TopicProduceResponse</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Topic:             topicData.Topic,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            PartitionResponses: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">PartitionProduceResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(topicData.Partitions)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _, partitionData </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> topicData.Partitions {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            partitionResp </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> PartitionProduceResponse</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                Partition: partitionData.Partition,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Validate topic exists and partition is valid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Check if this broker is leader for the partition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // If not, set ErrorCode = ErrNotLeaderForPartition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Validate RecordSet CRC and decompress if needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Append records to partition log</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // baseOffset, err := broker.logManager.Append(...)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Based on Acks, wait for replication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // - AcksNone: immediate success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // - AcksLeader: wait for local fsync</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // - AcksAll: wait for all ISR acknowledgment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 6: Set response fields: BaseOffset, LogAppendTime, LogStartOffset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            topicResp.PartitionResponses </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(topicResp.PartitionResponses, partitionResp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resp.Responses </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(resp.Responses, topicResp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> resp, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"coordination-service-skeleton\">Coordination Service Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/coordination/interface.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordination</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CoordinationService defines the minimal interface for cluster coordination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CoordinationService</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Broker management</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    RegisterBroker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">broker</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DeregisterBroker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">brokerID</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetBrokers</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Topic management</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateTopic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">TopicMetadata</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DeleteTopic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topicName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetTopicMetadata</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topicName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TopicMetadata</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListTopics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Partition leadership</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdatePartitionLeadership</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partition</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">PartitionMetadata</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ElectPartitionLeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partition</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">isr</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetPartitionMetadata</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partition</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PartitionMetadata</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Watch/notification system</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    WatchBrokers</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">callback</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">)) (</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    WatchTopic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topicName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">callback</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TopicMetadata</span><span style=\"color:#E1E4E8\">)) (</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Close releases resources</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// In-memory implementation skeleton</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// internal/coordination/memory/store.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> memory</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourproject/internal/coordination</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourproject/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MemoryCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    brokers </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Broker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    topics  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicMetadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Watch subscriptions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    brokerWatchers []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    topicWatchers  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicMetadata</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    closeChan </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMemoryCoordinator</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MemoryCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        brokers:       </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        topics:        </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicMetadata</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        topicWatchers: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TopicMetadata</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        closeChan:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterBroker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">broker</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> c.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: If broker.ID is 0, assign next available ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Store broker in map</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Notify all broker watchers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Call each callback in a goroutine to avoid blocking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WatchBrokers</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">callback</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">)) (</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> c.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Add callback to brokerWatchers slice</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return cleanup function that removes the callback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Immediately call callback with current broker list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ElectPartitionLeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partition</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">isr</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> c.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up current partition metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate ISR is not empty</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Select leader using deterministic algorithm:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Sort ISR replicas by broker ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Pick the replica that was most recently NOT leader (if any)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Otherwise, pick the first replica in sorted list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update partition metadata with new leader and generation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Notify topic watchers of the change</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints-for-go\">Language-Specific Hints for Go</h4>\n<ol>\n<li><p><strong>Binary Serialization</strong>: Use <code>encoding/binary.BigEndian.PutUint32()</code> and <code>binary.BigEndian.Uint32()</code> for protocol serialization. Create helper functions for nullable strings (int16 length prefix with -1 for null).</p>\n</li>\n<li><p><strong>Connection Management</strong>: Wrap <code>net.Conn</code> with read/write deadlines using <code>SetReadDeadline()</code> and <code>SetWriteDeadline()</code>. Implement connection pooling with <code>sync.Pool</code> for reuse.</p>\n</li>\n<li><p><strong>Watch Pattern</strong>: Implement watch callbacks using channels or function slices. Use <code>context.Context</code> for cancellation. Always call callbacks in goroutines to avoid blocking the coordinator.</p>\n</li>\n<li><p><strong>Memory Optimization</strong>: For protocol buffers, use <code>bytes.Buffer</code> with pre-allocated capacity to reduce allocations. Consider object pooling for frequently allocated structs.</p>\n</li>\n<li><p><strong>Error Propagation</strong>: Define custom error types for protocol errors with error codes. Use <code>errors.Is()</code> and <code>errors.As()</code> for error handling in client retry logic.</p>\n</li>\n</ol>\n<h4 id=\"milestone-checkpoint-protocol-implementation\">Milestone Checkpoint: Protocol Implementation</h4>\n<p>After implementing the wire protocol, test with this verification:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start a broker</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/broker/main.go</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 9092</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># In another terminal, test produce protocol</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">echo</span><span style=\"color:#9ECBFF\"> '{\"key\": \"test\", \"value\": \"hello\"}'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/producer/main.go</span><span style=\"color:#79B8FF\"> --topic</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> --brokers</span><span style=\"color:#9ECBFF\"> localhost:9092</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Message sent to partition 0 at offset 0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test consume protocol</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/consumer/main.go</span><span style=\"color:#79B8FF\"> --topic</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> --group</span><span style=\"color:#9ECBFF\"> test-group</span><span style=\"color:#79B8FF\"> --brokers</span><span style=\"color:#9ECBFF\"> localhost:9092</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output (after producing):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Received message at offset 0: key=\"test\", value=\"hello\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify with network capture</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sudo</span><span style=\"color:#9ECBFF\"> tcpdump</span><span style=\"color:#79B8FF\"> -i</span><span style=\"color:#9ECBFF\"> lo</span><span style=\"color:#79B8FF\"> -A</span><span style=\"color:#9ECBFF\"> port</span><span style=\"color:#79B8FF\"> 9092</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -A5</span><span style=\"color:#79B8FF\"> -B5</span><span style=\"color:#9ECBFF\"> \"test\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should show binary data (not human-readable)</span></span></code></pre></div>\n\n<p><strong>Debugging Tips:</strong></p>\n<ul>\n<li><p><strong>Symptom</strong>: &quot;Connection reset by peer&quot; or unexpected EOF</p>\n<ul>\n<li><strong>Cause</strong>: Incorrect message framing (size field wrong)</li>\n<li><strong>Diagnosis</strong>: Log raw bytes sent/received with hex dump</li>\n<li><strong>Fix</strong>: Verify <code>ReadMessage()</code>/<code>WriteMessage()</code> handle size prefix correctly</li>\n</ul>\n</li>\n<li><p><strong>Symptom</strong>: &quot;Invalid API key&quot; error</p>\n<ul>\n<li><strong>Cause</strong>: Wrong byte order or buffer offset</li>\n<li><strong>Diagnosis</strong>: Print decoded header fields vs. expected</li>\n<li><strong>Fix</strong>: Ensure big-endian encoding throughout</li>\n</ul>\n</li>\n<li><p><strong>Symptom</strong>: Consumer doesn&#39;t receive messages after produce</p>\n<ul>\n<li><strong>Cause</strong>: Fetch offset not advancing or incorrect high watermark</li>\n<li><strong>Diagnosis</strong>: Check FetchResponse contains correct HighWatermark</li>\n<li><strong>Fix</strong>: Ensure broker updates high watermark after replication</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"10-error-handling-and-edge-cases\">10. Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1, 2, 3, 4 (critical for all operational aspects)</p>\n</blockquote>\n<p>Distributed systems operate in a hostile environment where components fail independently and unpredictably. A message queue&#39;s reliability hinges not on preventing failures (impossible) but on gracefully recovering from them. This section catalogs the inevitable failure modes and edge cases your system will encounter, providing a comprehensive playbook for designing robust recovery mechanisms. Think of this as the system&#39;s immune system: it must detect pathogens (failures), contain damage (prevent cascading failures), and initiate healing (recovery) while maintaining core functions.</p>\n<h3 id=\"101-failure-mode-categories\">10.1 Failure Mode Categories</h3>\n<p>Failures in a distributed message queue can be categorized by their origin, duration, and observability. Each category presents distinct challenges for detection and recovery.</p>\n<h4 id=\"1011-broker-crashes-process-termination\">10.1.1 Broker Crashes (Process Termination)</h4>\n<p>Broker crashes occur when a broker process terminates abruptly due to fatal errors, <code>SIGKILL</code>, out-of-memory conditions, or administrative shutdown. The broker disappears from the network entirely, leaving its hosted partitions leaderless if they were leading replicas.</p>\n<p><strong>Mental Model: The Vanishing Bank Teller</strong>\nImagine a bank with multiple tellers (brokers) serving customers (clients). One teller suddenly collapses mid-transaction. Customers waiting at that teller&#39;s line are stuck, but the bank manager (controller/coordinator) notices the absence and redirects customers to other tellers who have duplicate ledger copies (replicas).</p>\n<table>\n<thead>\n<tr>\n<th>Failure Characteristic</th>\n<th>Description</th>\n<th>Detection Mechanism</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Sudden Termination</strong></td>\n<td>Process exits without cleanup (OOM, segmentation fault)</td>\n<td>Missing heartbeats in coordination service, TCP connection failures</td>\n</tr>\n<tr>\n<td><strong>Graceful Shutdown</strong></td>\n<td>Controlled stop with cleanup attempts</td>\n<td>Explicit deregistration request before exit</td>\n</tr>\n<tr>\n<td><strong>Partial Functionality</strong></td>\n<td>Process runs but critical subsystems fail (log full, disk error)</td>\n<td>Health checks, failed produce/fetch requests, metrics monitoring</td>\n</tr>\n</tbody></table>\n<h4 id=\"1012-network-partitions-split-brain-scenarios\">10.1.2 Network Partitions (Split-Brain Scenarios)</h4>\n<p>Network partitions occur when subsets of brokers can communicate within their group but cannot reach brokers in other groups. This creates &quot;split-brain&quot; scenarios where each partition believes the others have failed, potentially leading to multiple leaders for the same partition.</p>\n<p><strong>Mental Model: The Islanded Archipelago</strong>\nPicture an archipelago (broker cluster) where a storm severs communication links between islands. Islands can&#39;t coordinate, so each might independently declare itself the new capital (leader) for shared territories (partitions), creating conflicting governance.</p>\n<table>\n<thead>\n<tr>\n<th>Partition Type</th>\n<th>Typical Cause</th>\n<th>Duration</th>\n<th>Impact Severity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Client-Broker</strong></td>\n<td>Client network issues, firewall rules</td>\n<td>Seconds to minutes</td>\n<td>Temporary unavailability for specific clients</td>\n</tr>\n<tr>\n<td><strong>Broker-Broker</strong></td>\n<td>Switch failures, misconfigured routes</td>\n<td>Variable</td>\n<td>Replication stalls, ISR shrinkage, potential data inconsistency</td>\n</tr>\n<tr>\n<td><strong>Controller Isolation</strong></td>\n<td>Controller broker loses connection to majority</td>\n<td>Until partition heals</td>\n<td>No leadership elections, metadata stagnation</td>\n</tr>\n</tbody></table>\n<h4 id=\"1013-disk-failures-storage-corruption\">10.1.3 Disk Failures (Storage Corruption)</h4>\n<p>Disk failures range from complete device failure to silent corruption where reads return incorrect data. The log&#39;s durability guarantees depend entirely on stable storage, making disk failures particularly insidious.</p>\n<p><strong>Mental Model: The Fading Ledger</strong>\nEnvision a ledger (log) whose ink gradually fades (bit rot) or pages tear (sector errors). Transactions recorded yesterday become unreadable today, threatening the entire financial record.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Manifestation</th>\n<th>Detection Challenge</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Full Disk</strong></td>\n<td>No space for new writes</td>\n<td><code>ENOSPC</code> errors on append, monitoring alerts</td>\n</tr>\n<tr>\n<td><strong>Corrupted Files</strong></td>\n<td>CRC mismatches, invalid offsets</td>\n<td>Read validation failures, checksum verification</td>\n</tr>\n<tr>\n<td><strong>Slow I/O</strong></td>\n<td>Latency spikes beyond tolerance</td>\n<td>Timeout monitoring, percentile latency tracking</td>\n</tr>\n<tr>\n<td><strong>Controller Failure</strong></td>\n<td>RAID/HBA issues affecting all disks</td>\n<td>Health checks, SMART monitoring</td>\n</tr>\n</tbody></table>\n<h4 id=\"1014-client-timeouts-and-transient-failures\">10.1.4 Client Timeouts and Transient Failures</h4>\n<p>Clients may experience temporary connectivity issues, processing delays, or resource exhaustion. While not broker failures, these affect system semantics (message delivery guarantees, offset commits).</p>\n<p><strong>Mental Model: The Distracted Customer</strong>\nImagine a customer (consumer) who steps away from the counter (network timeout) while their transaction is being processed. The teller (broker) must decide whether to hold the transaction open or serve the next customer.</p>\n<table>\n<thead>\n<tr>\n<th>Client Issue</th>\n<th>Typical Cause</th>\n<th>System Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Network Timeout</strong></td>\n<td>Packet loss, congestion, DNS issues</td>\n<td>In-flight requests fail, retries triggered</td>\n</tr>\n<tr>\n<td><strong>Processing Stall</strong></td>\n<td>GC pause, deadlock, resource exhaustion</td>\n<td>Heartbeat failures, session timeout</td>\n</tr>\n<tr>\n<td><strong>Clock Skew</strong></td>\n<td>NTP misconfiguration, virtualization artifacts</td>\n<td>Incorrect timestamps, session expiration confusion</td>\n</tr>\n</tbody></table>\n<h3 id=\"102-recovery-strategies\">10.2 Recovery Strategies</h3>\n<p>Each failure category demands specific recovery mechanisms. The system&#39;s resilience emerges from the careful orchestration of these strategies across components.</p>\n<h4 id=\"1021-broker-crash-recovery\">10.2.1 Broker Crash Recovery</h4>\n<p>When a broker crashes, the system must restore availability without compromising consistency for partitions where the crashed broker was leader.</p>\n<blockquote>\n<p><strong>Key Insight:</strong> Recovery from broker crashes follows a two-phase approach: first <strong>detect and isolate</strong> the failed broker, then <strong>reassign and restore</strong> its responsibilities to healthy replicas.</p>\n</blockquote>\n<p><strong>Step-by-Step Recovery Algorithm:</strong></p>\n<ol>\n<li><p><strong>Failure Detection:</strong></p>\n<ul>\n<li>Coordination service (<code>MemoryCoordinator</code>) monitors broker heartbeats</li>\n<li>Missing heartbeats for <code>session.timeout.ms</code> triggers failure suspicion</li>\n<li>Controller confirms via TCP connection attempts to all broker ports</li>\n<li>After confirmation delay (configurable), broker marked as <code>DEAD</code></li>\n</ul>\n</li>\n<li><p><strong>Leadership Reassignment:</strong></p>\n<ul>\n<li>Controller (<code>LeaderElector</code>) enumerates all partitions where failed broker was leader</li>\n<li>For each partition:<ul>\n<li>Remove failed broker from ISR list in <code>PartitionMetadata</code></li>\n<li>Select new leader from remaining ISR members using deterministic algorithm (e.g., oldest replica in ISR)</li>\n<li>If ISR is empty, trigger <strong>unclean leader election</strong> decision (see ADR below)</li>\n</ul>\n</li>\n<li>Update <code>PartitionMetadata</code> across cluster via coordination service</li>\n<li>Notify all brokers of leadership changes via metadata propagation</li>\n</ul>\n</li>\n<li><p><strong>Replica Rebuilding (Post-Recovery):</strong></p>\n<ul>\n<li>When failed broker restarts, it rejoins cluster as follower for all its assigned partitions</li>\n<li>For each partition:<ul>\n<li>New leader truncates follower&#39;s log to <strong>high watermark</strong> to ensure consistency</li>\n<li>Follower fetches from leader starting at high watermark + 1</li>\n<li>Once caught up within <code>replica.lag.time.max.ms</code>, rejoins ISR</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>ADR: Unclean Leader Election Policy</strong></p>\n<blockquote>\n<p><strong>Decision: Disallow Unclean Leader Election by Default</strong></p>\n<ul>\n<li><strong>Context:</strong> When all ISR replicas for a partition are unavailable (e.g., multiple broker failures), the system faces a choice: wait for an ISR member to recover (availability impact) or elect a non-ISR follower as leader (consistency risk).</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Always wait for ISR:</strong> Never elect non-ISR replicas, guaranteeing no data loss but risking extended unavailability.</li>\n<li><strong>Allow unclean election:</strong> Elect any available replica when ISR is empty, maximizing availability but risking data loss from un-replicated messages.</li>\n<li><strong>Configurable policy:</strong> Let operators choose based on topic importance.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Implement option 1 (wait for ISR) as default for this educational project, with clear logging when availability is impacted.</li>\n<li><strong>Rationale:</strong> Simplicity and safety for learners. Understanding data loss implications requires sophisticated monitoring; better to err on the side of preserving data. Real systems (like Kafka) offer configurable <code>unclean.leader.election.enable</code>.</li>\n<li><strong>Consequences:</strong> Topics may become unavailable during multi-broker failures, but learners won&#39;t silently lose data. Forces consideration of replication factor and broker distribution.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Recovery Strategy</th>\n<th>Trigger Condition</th>\n<th>Action</th>\n<th>Consistency Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Clean Failover</strong></td>\n<td>Leader fails, ISR non-empty</td>\n<td>Elect new leader from ISR</td>\n<td>No data loss, monotonic offsets</td>\n</tr>\n<tr>\n<td><strong>ISR Shrinkage</strong></td>\n<td>Follower lags beyond threshold</td>\n<td>Remove from ISR, continue with reduced replication</td>\n<td>Potentially reduced durability</td>\n</tr>\n<tr>\n<td><strong>Unavailable Partition</strong></td>\n<td>All ISR members fail</td>\n<td>Wait for recovery, reject produce/fetch requests</td>\n<td>Strong consistency, availability loss</td>\n</tr>\n</tbody></table>\n<h4 id=\"1022-network-partition-mitigation\">10.2.2 Network Partition Mitigation</h4>\n<p>Network partitions create the most complex failure scenarios due to conflicting views of cluster state.</p>\n<p><strong>Detection and Containment Strategy:</strong></p>\n<ol>\n<li><p><strong>Dual Monitoring:</strong></p>\n<ul>\n<li><strong>Heartbeat-based:</strong> Coordination service tracks broker liveness via periodic heartbeats</li>\n<li><strong>Request-based:</strong> Brokers track peer liveness via replication fetch requests and metadata exchanges</li>\n<li>Discrepancies between these signals suggest network issues rather than true failures</li>\n</ul>\n</li>\n<li><p><strong>Fencing via Epochs:</strong></p>\n<ul>\n<li>Each partition leader has a monotonically increasing <code>leader_epoch</code> stored in <code>Partition</code></li>\n<li>All client requests include the last known leader epoch</li>\n<li>Requests with stale epoch are rejected with <code>ErrNotLeaderForPartition</code></li>\n<li>This prevents clients from accidentally writing to old leaders during partitions</li>\n</ul>\n</li>\n<li><p><strong>Controller Resilience:</strong></p>\n<ul>\n<li>Controller itself can be partitioned from the majority</li>\n<li>Use <strong>controller epoch</strong> in <code>ClusterMetadata</code> to prevent multiple active controllers</li>\n<li>Brokers reject metadata updates from controllers with stale epoch</li>\n</ul>\n</li>\n</ol>\n<p><strong>Recovery Sequence During Partition Healing:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>When network partition heals:\n1. Previously isolated brokers re-establish TCP connections\n2. They discover higher controller/leader epochs from the majority side\n3. Isolated leaders step down upon receiving higher epoch notifications\n4. Their logs are inspected for divergent writes:\n   - Compare high watermarks between old and new leaders\n   - Any messages beyond common high watermark are truncated (lost)\n5. Brokers rejoin as followers and resynchronize from current leaders</code></pre></div>\n\n<h4 id=\"1023-disk-failure-handling\">10.2.3 Disk Failure Handling</h4>\n<p>Disk failures threaten the fundamental durability guarantees of the message log.</p>\n<p><strong>Tiered Response Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Severity</th>\n<th>Detection</th>\n<th>Immediate Response</th>\n<th>Long-term Remediation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Full Disk</strong></td>\n<td><code>ENOSPC</code> on append</td>\n<td>Reject produce requests for affected partitions, continue serving fetches</td>\n<td>Add disk space, delete old segments, or migrate partitions</td>\n</tr>\n<tr>\n<td><strong>Corrupted Segment</strong></td>\n<td>CRC mismatch on read</td>\n<td>Mark segment as corrupted, skip to next segment for reads; writes continue to new segment</td>\n<td>Attempt repair from replicas, or accept data loss for that segment</td>\n</tr>\n<tr>\n<td><strong>Slow I/O</strong></td>\n<td>Write latency &gt; <code>log.flush.timeout.ms</code></td>\n<td>Log warnings, metrics alerts; continue with degraded performance</td>\n<td>Investigate disk health, redistribute load, upgrade hardware</td>\n</tr>\n<tr>\n<td><strong>Complete Disk Loss</strong></td>\n<td>All I/O operations fail</td>\n<td>Mark broker as unhealthy in coordination service, trigger partition reassignment</td>\n<td>Replace hardware, restore from replicas, recommission broker</td>\n</tr>\n</tbody></table>\n<p><strong>Segmented Isolation Approach:</strong>\nThe <code>Log</code> design with multiple <code>LogSegment</code> files provides natural isolation boundaries. A corrupted segment affects only messages within its offset range, allowing the rest of the partition to remain operational. The system should:</p>\n<ol>\n<li>Detect corruption via CRC checks in <code>Log.Read()</code></li>\n<li>Move corrupted segment to quarantine directory</li>\n<li>Update <code>Log.Segments</code> list to skip corrupted segment</li>\n<li>Log detailed error with segment metadata for admin intervention</li>\n<li>For replicated partitions, attempt to fetch missing range from other replicas</li>\n</ol>\n<h4 id=\"1024-client-failure-recovery\">10.2.4 Client Failure Recovery</h4>\n<p>Client failures primarily affect consumer progress tracking and producer message delivery semantics.</p>\n<p><strong>Consumer Session Recovery:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>When consumer fails (misses heartbeats):\n1. Group coordinator (`ConsumerGroup.State`) marks consumer as dead after `session.timeout.ms`\n2. Coordinator triggers rebalance by moving group to `PreparingRebalance` state\n3. Remaining consumers rejoin via `JoinGroup`/`SyncGroup`\n4. Partitions assigned to failed consumer are redistributed\n5. New consumers start fetching from last committed offsets</code></pre></div>\n\n<p><strong>Producer Retry with Idempotence:</strong>\nFor <code>Producer</code> with <code>AcksAll</code>, failed requests require careful retry logic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Pseudo-algorithm (not code block - prose description)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">1.</span><span style=\"color:#E1E4E8\"> On send failure (timeout, network </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">, NotLeader </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">2.</span><span style=\"color:#E1E4E8\"> Refresh metadata cache to discover new partition leader</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">3.</span><span style=\"color:#E1E4E8\"> Reassign batch to new leader </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> partition leadership changed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">4.</span><span style=\"color:#E1E4E8\"> Apply exponential backoff: wait </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> baseBackoff </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\">^</span><span style=\"color:#E1E4E8\">attempt</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">5.</span><span style=\"color:#E1E4E8\"> Retry up to </span><span style=\"color:#9ECBFF\">`Retries`</span><span style=\"color:#E1E4E8\"> configuration</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">6.</span><span style=\"color:#E1E4E8\"> If idempotent producer enabled (optional), include sequence numbers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   to allow broker to deduplicate retried batches</span></span></code></pre></div>\n\n<h3 id=\"103-edge-cases-and-race-conditions\">10.3 Edge Cases and Race Conditions</h3>\n<p>Edge cases represent improbable but possible scenarios arising from specific timing of events. Race conditions occur when multiple processes access shared data concurrently, and the outcome depends on the sequence of operations.</p>\n<h4 id=\"1031-leader-change-during-produce-request\">10.3.1 Leader Change During Produce Request</h4>\n<p><strong>Scenario:</strong> A producer sends a <code>ProduceRequest</code> to partition leader. Mid-processing, leadership changes (old leader crashes or steps down). The request might be partially processed, duplicated, or lost.</p>\n<p><strong>Mental Model: The Relaying Runner</strong>\nImagine a relay race where the baton (message) is passed between runners (brokers). If the current runner (leader) stumbles while holding the baton, and a new runner takes over, what happens to the baton? It might be dropped (lost), held by both (duplicated), or successfully passed (handled correctly).</p>\n<table>\n<thead>\n<tr>\n<th>Timing</th>\n<th>Producer Action</th>\n<th>Old Leader State</th>\n<th>New Leader State</th>\n<th>Potential Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Before send</strong></td>\n<td>Request in flight</td>\n<td>Healthy</td>\n<td>Not yet elected</td>\n<td>Connection refused, producer retries</td>\n</tr>\n<tr>\n<td><strong>During append</strong></td>\n<td>Request being written</td>\n<td>Crashes mid-write</td>\n<td>Elected</td>\n<td>Partial write: data may be corrupted or lost</td>\n</tr>\n<tr>\n<td><strong>After append, before ack</strong></td>\n<td>Waiting for response</td>\n<td>Crashes after write</td>\n<td>Elected</td>\n<td>Duplicate possible if producer retries</td>\n</tr>\n<tr>\n<td><strong>After ack, before receipt</strong></td>\n<td>Response in flight</td>\n<td>Crashes after sending ack</td>\n<td>Elected</td>\n<td>Producer sees success, consumer may not see data until recovery</td>\n</tr>\n</tbody></table>\n<p><strong>Mitigation Strategy:</strong></p>\n<ol>\n<li><strong>Leader Epoch in Requests:</strong> Include <code>leader_epoch</code> in <code>ProduceRequest</code>; new leader rejects requests with stale epoch</li>\n<li><strong>Sequence Numbers:</strong> Idempotent producers include <code>producer_id</code>, <code>epoch</code>, and <code>sequence_number</code> for deduplication</li>\n<li><strong>High Watermark Validation:</strong> New leader compares its log with old leader&#39;s before accepting writes to detect gaps</li>\n<li><strong>Client Metadata Refresh:</strong> Producer refreshes metadata on <code>ErrNotLeaderForPartition</code> and retries</li>\n</ol>\n<h4 id=\"1032-duplicate-consumer-id-during-rebalance\">10.3.2 Duplicate Consumer ID During Rebalance</h4>\n<p><strong>Scenario:</strong> A consumer crashes and restarts quickly, rejoining the group with the same <code>member_id</code> before the coordinator has marked it dead. Two instances with the same identity exist temporarily.</p>\n<p><strong>Race Condition Timeline:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>T0: Consumer C1 (member_id=&quot;A&quot;) sends heartbeat\nT1: C1 crashes\nT2: C1 restarts, reuses member_id=&quot;A&quot;, sends JoinGroup\nT3: Coordinator still has C1 as active (heartbeat within session timeout)\nT4: Coordinator now has two &quot;logical&quot; consumers with same ID</code></pre></div>\n\n<p><strong>Consequences:</strong></p>\n<ul>\n<li>Coordinator might assign partitions to both instances</li>\n<li>Both instances might commit offsets, causing overwrites</li>\n<li>Group state becomes inconsistent</li>\n</ul>\n<p><strong>Solution: Generation ID Fencing:</strong></p>\n<ul>\n<li>Each rebalance increments <code>GenerationID</code> in <code>ConsumerGroup</code></li>\n<li>Coordinator includes <code>generation_id</code> in <code>JoinGroupResponse</code></li>\n<li>Members include <code>generation_id</code> in all subsequent requests</li>\n<li>Coordinator rejects requests with stale <code>generation_id</code></li>\n<li>Restarted consumer gets new <code>member_id</code> if rejoining after generation change</li>\n</ul>\n<h4 id=\"1033-partial-write-before-crash-torn-writes\">10.3.3 Partial Write Before Crash (Torn Writes)</h4>\n<p><strong>Scenario:</strong> Broker crashes while appending a <code>RecordBatch</code> to disk, writing only part of the batch. On restart, the log contains corrupted data.</p>\n<p><strong>Detailed Failure Modes:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Corruption Type</th>\n<th>Cause</th>\n<th>Detection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Incomplete batch</strong></td>\n<td>Crash during <code>WAL.Append()</code></td>\n<td>Batch length prefix doesn&#39;t match actual bytes</td>\n</tr>\n<tr>\n<td><strong>Misaligned offset</strong></td>\n<td>Crash between writing record and updating index</td>\n<td>Index points to invalid file position</td>\n</tr>\n<tr>\n<td><strong>CRC mismatch</strong></td>\n<td>Crash during write leaves partial data</td>\n<td>CRC validation fails on read</td>\n</tr>\n</tbody></table>\n<p><strong>Recovery via Write-Ahead Log with Batch Atomicity:</strong></p>\n<ol>\n<li><strong>Write Strategy:</strong> Entire <code>RecordBatch</code> written as single <code>WAL.Append()</code> operation</li>\n<li><strong>CRC Protection:</strong> Compute CRC after serialization, include in header</li>\n<li><strong>Fsync Control:</strong> <code>sync</code> parameter controls durability trade-off</li>\n<li><strong>Restart Recovery:</strong> <code>Log</code> scans segments on startup:<ul>\n<li>Read forward until incomplete batch detected (length mismatch)</li>\n<li>Truncate file to last valid batch boundary</li>\n<li>Rebuild index from remaining valid data</li>\n<li>Log truncation offset for admin review</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"1034-zombie-consumers-in-rebalance\">10.3.4 Zombie Consumers in Rebalance</h4>\n<p><strong>Scenario:</strong> A consumer is considered dead by coordinator (missed heartbeats) but is actually alive and still fetching messages. It becomes a &quot;zombie&quot; – processing messages without the coordinator&#39;s knowledge.</p>\n<p><strong>Root Causes:</strong></p>\n<ol>\n<li><strong>Network Partition:</strong> Consumer isolated from coordinator but connected to partition leaders</li>\n<li><strong>Heartbeat Thread Stall:</strong> Consumer process alive but heartbeat goroutine blocked</li>\n<li><strong>Clock Skew:</strong> Consumer&#39;s clock ahead, causing early heartbeat timeout calculation</li>\n</ol>\n<p><strong>Dangers:</strong></p>\n<ul>\n<li>Zombie continues committing offsets, interfering with new consumer&#39;s progress</li>\n<li>Messages processed twice (by zombie and replacement consumer)</li>\n<li>Offset commits race condition</li>\n</ul>\n<p><strong>Fencing via Generation ID:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Solution implemented in HandleFetch:\n1. Each FetchRequest includes (group_id, member_id, generation_id)\n2. Broker validates against local cache of valid consumers\n3. If generation_id stale or member not in current generation:\n   - Return error to zombie\n   - Zombie must rejoin group to get current generation\n4. Partition leaders get valid member list from coordinator periodically</code></pre></div>\n\n<h4 id=\"1035-isr-shrinkage-to-empty-set\">10.3.5 ISR Shrinkage to Empty Set</h4>\n<p><strong>Scenario:</strong> All followers for a partition fall behind the leader due to network issues, broker failures, or sustained high load. The ISR shrinks to just the leader, then the leader fails, leaving no in-sync replicas.</p>\n<p><strong>Progression:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Initial: ISR = [Leader L, Followers F1, F2]\nStep 1: F1 network partition → removed from ISR after replica.lag.time.max.ms\nStep 2: F2 disk slowdown → falls behind → removed from ISR\nStep 3: ISR = [L] (single replica)\nStep 4: L crashes → No in-sync replicas remain</code></pre></div>\n\n<p><strong>Recovery Options:</strong></p>\n<ol>\n<li><strong>Wait for Recovery:</strong> Don&#39;t elect new leader until at least one replica recovers and catches up</li>\n<li><strong>Best-Effort Election:</strong> Elect the replica with most recent data (highest LEO)</li>\n<li><strong>Admin Intervention:</strong> Manual override via tooling</li>\n</ol>\n<p><strong>Implementation Guidance:</strong></p>\n<ul>\n<li>Track <code>lastCaughtUpTime</code> for each follower in <code>ISRManager</code></li>\n<li>Configurable <code>unclean.leader.election.enable</code> per topic</li>\n<li>Log warnings when ISR size drops below <code>min.insync.replicas</code></li>\n<li>Expose metrics for admin monitoring</li>\n</ul>\n<h4 id=\"1036-offset-commit-race-during-rebalance\">10.3.6 Offset Commit Race During Rebalance</h4>\n<p><strong>Scenario:</strong> Consumer commits offsets while rebalance is in progress. The commit might apply to old partition assignment, causing offsets to be associated with wrong partitions after reassignment.</p>\n<p><strong>Timeline:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>T0: Consumer C1 has partitions [P1, P2]\nT1: C1 commits offset for P2\nT2: Rebalance starts (new consumer joins)\nT3: Coordinator reassigns P2 to C2\nT4: C1's offset commit arrives (delayed network)\nT5: Offset for P2 stored under C1's member_id, but C2 now owns P2</code></pre></div>\n\n<p><strong>Solution: Generation-aware Offset Commits:</strong></p>\n<ul>\n<li><code>CommitOffset</code> request includes <code>generation_id</code></li>\n<li>Coordinator rejects commits with stale <code>generation_id</code></li>\n<li>Consumers should commit offsets <strong>before</strong> rejoining group during rebalance</li>\n<li>Alternatively, commit offsets <strong>after</strong> receiving new assignment (Kafka&#39;s approach)</li>\n</ul>\n<h4 id=\"1037-producer-retry-causing-duplicate-sequence\">10.3.7 Producer Retry Causing Duplicate Sequence</h4>\n<p><strong>Scenario:</strong> Idempotent producer sends batch with sequence number N, gets timeout, retries with same sequence number N, but first batch actually succeeded and was durably stored.</p>\n<p><strong>Without Proper Deduplication:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>P1: Send batch (producer_id=100, epoch=1, seq=5) → Timeout\nP2: Retry same batch (pid=100, epoch=1, seq=5) → Success\nP3: First batch eventually succeeds (network delay) → Duplicate in log</code></pre></div>\n\n<p><strong>Broker-side Deduplication:</strong></p>\n<ul>\n<li>Maintain <code>lastSequence</code> per <code>(producer_id, producer_epoch, partition)</code></li>\n<li>Reject batches with sequence ≤ <code>lastSequence</code></li>\n<li>Accept batches with sequence = <code>lastSequence + 1</code></li>\n<li>Gap in sequence numbers (sequence &gt; <code>lastSequence + 1</code>) indicates lost messages → return error</li>\n</ul>\n<h4 id=\"1038-log-truncation-during-follower-sync\">10.3.8 Log Truncation During Follower Sync</h4>\n<p><strong>Scenario:</strong> Leader truncates its log (e.g., after unclean leader election or admin operation) while follower is fetching. Follower may have already replicated data that no longer exists on leader.</p>\n<p><strong>Detection and Recovery:</strong></p>\n<ol>\n<li>Leader includes <code>log_start_offset</code> in <code>FetchResponse</code></li>\n<li>Follower compares its fetch offset with leader&#39;s <code>log_start_offset</code></li>\n<li>If <code>fetch_offset &lt; log_start_offset</code>, follower must truncate its log</li>\n<li>Follower rewinds to <code>log_start_offset</code> and re-fetches from there</li>\n<li>Any local messages beyond truncation point are discarded</li>\n</ol>\n<p><strong>Implementation in <code>FollowerSyncer.AppendToLocalLog()</code>:</strong></p>\n<ul>\n<li>Validate <code>leader_hw</code> and <code>log_start_offset</code> against local log</li>\n<li>If divergence detected, truncate local log to common point</li>\n<li>Log warning with offset details for admin review</li>\n</ul>\n<h3 id=\"104-failure-recovery-summary-table\">10.4 Failure Recovery Summary Table</h3>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n<th>Consistency Impact</th>\n<th>Availability Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Broker crash (leader)</strong></td>\n<td>Missed heartbeats, TCP failure</td>\n<td>Elect new leader from ISR</td>\n<td>None if ISR &gt; 1</td>\n<td>Brief unavailability during election</td>\n</tr>\n<tr>\n<td><strong>Broker crash (follower)</strong></td>\n<td>Missed heartbeats</td>\n<td>Remove from ISR, replicate to other followers</td>\n<td>Reduced durability margin</td>\n<td>None</td>\n</tr>\n<tr>\n<td><strong>Network partition</strong></td>\n<td>Divergent metadata views, heartbeat failures</td>\n<td>Fencing via epochs, wait for healing</td>\n<td>Potential divergent writes if unclean election allowed</td>\n<td>Partial availability loss</td>\n</tr>\n<tr>\n<td><strong>Disk full</strong></td>\n<td><code>ENOSPC</code> on write</td>\n<td>Reject writes, alert admin</td>\n<td>Potential data loss if buffers overflow</td>\n<td>Write unavailability for affected partitions</td>\n</tr>\n<tr>\n<td><strong>Consumer timeout</strong></td>\n<td>Missed heartbeats</td>\n<td>Rebalance, redistribute partitions</td>\n<td>At-least-once reprocessing possible</td>\n<td>Brief consumption pause during rebalance</td>\n</tr>\n<tr>\n<td><strong>Producer retry storm</strong></td>\n<td>High error rate, timeout spikes</td>\n<td>Exponential backoff, circuit breaker</td>\n<td>Potential duplicates without idempotence</td>\n<td>Increased latency, reduced throughput</td>\n</tr>\n<tr>\n<td><strong>Controller failure</strong></td>\n<td>Missed controller heartbeats</td>\n<td>Elect new controller from brokers</td>\n<td>Metadata update stall</td>\n<td>No new leader elections until new controller</td>\n</tr>\n<tr>\n<td><strong>Zombie consumer</strong></td>\n<td>Generation ID mismatch in fetch</td>\n<td>Reject fetches, force rejoin</td>\n<td>Prevents duplicate processing</td>\n<td>Brief disruption for zombie consumer</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<blockquote>\n<p><strong>Technology Note:</strong> While production systems use sophisticated monitoring (Prometheus, health checks), our educational implementation can rely on simpler timeouts and periodic checks. The key is implementing the recovery logic correctly, not building enterprise monitoring.</p>\n</blockquote>\n<h4 id=\"a-recommended-failure-detection-infrastructure\">A. Recommended Failure Detection Infrastructure</h4>\n<p><strong>Simple Health Check Endpoint:</strong>\nAdd to your <code>Server</code> a basic HTTP health endpoint (separate port) that checks:</p>\n<ul>\n<li>Disk space above threshold</li>\n<li>WAL write/read test</li>\n<li>Coordination service connectivity</li>\n<li>Memory usage</li>\n</ul>\n<p><strong>Timeout Configuration Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Configuration</th>\n<th>Default Value</th>\n<th>Purpose</th>\n<th>Override Guidance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>session.timeout.ms</code></td>\n<td>10000 (10s)</td>\n<td>Consumer heartbeat timeout</td>\n<td>Increase for GC-heavy environments</td>\n</tr>\n<tr>\n<td><code>replica.lag.time.max.ms</code></td>\n<td>30000 (30s)</td>\n<td>Max time follower can lag before ISR removal</td>\n<td>Adjust based on network reliability</td>\n</tr>\n<tr>\n<td><code>request.timeout.ms</code></td>\n<td>30000 (30s)</td>\n<td>Client request timeout</td>\n<td>Match expected network latency</td>\n</tr>\n<tr>\n<td><code>log.flush.timeout.ms</code></td>\n<td>1000 (1s)</td>\n<td>Max time for log fsync</td>\n<td>Tune based on disk performance</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-core-recovery-logic-skeletons\">B. Core Recovery Logic Skeletons</h4>\n<p><strong>Leader Election on Broker Failure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// In internal/coordinator/leader_elector.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OnBrokerFailure triggers leader election for all partitions where the failed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// broker was the leader. Called by coordination service when broker marked dead.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LeaderElector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">OnBrokerFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">failedBrokerID</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Query coordination service for all partitions where</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         failedBrokerID == PartitionMetadata.LeaderID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each affected partition:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Get current ISR from PartitionMetadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Remove failedBrokerID from ISR slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. If ISR is empty:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        - Log critical warning \"No in-sync replicas for partition\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        - Skip election (partition unavailable) OR</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        - If unclean election enabled, use all replicas as candidates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Select new leader using deterministic algorithm:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Prefer replicas in original ISR order (oldest first)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Ensure new leader is alive (check broker heartbeats)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update PartitionMetadata with new LeaderID and ISR</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Notify all brokers of metadata change via coordination service</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return any errors encountered during the process</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// selectNewLeader chooses a replica from ISR to become the new leader.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns -1 if no suitable leader found (ISR empty).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LeaderElector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">selectNewLeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">isr</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partitionInfo</span><span style=\"color:#B392F0\"> PartitionMetadata</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Filter ISR to only alive brokers (check broker heartbeats)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If filtered ISR empty, return -1 (partition unavailable)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply deterministic selection:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Sort alive ISR members by broker ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Choose first in sorted list (simplest strategy)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Alternative: choose replica with highest LEO (requires extra metadata)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return selected broker ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Consumer Zombie Detection in Fetch Handler:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// In internal/broker/fetch_handler.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// validateConsumerGeneration checks if consumer is part of current generation.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns ErrUnknownMember if consumer is zombie (stale generation).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FetchHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">validateConsumerGeneration</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    groupID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    memberID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    generationID</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get group from coordinator using groupID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If group not found, return ErrUnknownMemberId</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check if memberID exists in group.Members map</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Compare generationID with group.GenerationID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If generationID &#x3C; group.GenerationID, consumer is zombie</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         Return ErrUnknownMemberId to force rejoin</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If member not in current members list, return ErrUnknownMemberId</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleFetch now includes validation:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FetchHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleFetch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">FetchRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FetchResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: For consumer fetch requests (ReplicaID = -1):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   Call validateConsumerGeneration for each unique (group, member)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   Reject entire request if any consumer is zombie</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... rest of fetch logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Log Recovery on Broker Startup:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// In internal/log/log_manager.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecoverLogs scans all log directories and recovers from partial writes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Called during broker startup before accepting client requests.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecoverLogs</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Walk data directory to find all topic-partition directories</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each partition directory:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. List segment files sorted by base offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. For each segment file:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        i. Open data file and index file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        ii. Read from start, validating each RecordBatch CRC</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        iii. When invalid batch found (CRC mismatch or length wrong):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //              - Truncate file at last valid batch boundary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //              - Rebuild index from remaining valid data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //              - Log warning with truncation offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //        iv. Update Log.Segments list and Log.CurrentOffset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For replicated partitions, compare with leader's high watermark</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         Truncate beyond high watermark if follower is ahead</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return any unrecoverable errors (e.g., disk errors)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Network Partition Detection via Dual Heartbeats:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// In internal/coordinator/memory_coordinator.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// checkBrokerHealth performs dual health checking to detect network partitions.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkBrokerHealth</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: For each registered broker:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Check last heartbeat time (coordination plane)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Check last metadata request time (data plane proxy)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If heartbeat stale but metadata recent:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Log warning \"Possible network partition for broker X\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Don't mark as dead immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Increase suspicion level</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If both stale beyond thresholds:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Mark broker as dead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Trigger leader election for affected partitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If heartbeat recent but metadata stale:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Broker may be overloaded or have disk issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Consider removing from ISR for hosted partitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"c-debugging-tips-for-common-failure-scenarios\">C. Debugging Tips for Common Failure Scenarios</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Messages not consumed after consumer restart</strong></td>\n<td>Offset commit failed during shutdown</td>\n<td>1. Check offset store for group<br>2. Verify last commit timestamp<br>3. Check consumer logs for commit errors</td>\n<td>Ensure <code>CommitSync()</code> called before <code>Close()</code>, increase <code>request.timeout.ms</code></td>\n</tr>\n<tr>\n<td><strong>Producer hangs indefinitely</strong></td>\n<td>All replicas down for target partition</td>\n<td>1. Check partition ISR size<br>2. Verify broker health<br>3. Check producer logs for <code>ErrNotLeaderForPartition</code></td>\n<td>Configure <code>retries</code> and <code>retry.backoff.ms</code>, monitor broker health</td>\n</tr>\n<tr>\n<td><strong>Consumer group stuck in rebalance</strong></td>\n<td>One consumer not responding to JoinGroup</td>\n<td>1. Check coordinator logs for timeout members<br>2. Verify all consumers can reach coordinator<br>3. Check for clock skew between machines</td>\n<td>Increase <code>session.timeout.ms</code>, synchronize NTP, ensure network connectivity</td>\n</tr>\n<tr>\n<td><strong>High watermark not advancing</strong></td>\n<td>Followers not catching up to leader</td>\n<td>1. Check follower fetch logs<br>2. Verify network between brokers<br>3. Check disk I/O on followers</td>\n<td>Increase <code>replica.fetch.wait.ms</code>, check network links, monitor disk performance</td>\n</tr>\n<tr>\n<td><strong>Duplicate messages consumed</strong></td>\n<td>Consumer processed messages, crashed before commit</td>\n<td>1. Check offset lag before/after crash<br>2. Verify commit frequency settings<br>3. Check for zombie consumers</td>\n<td>Decrease <code>auto.commit.interval.ms</code>, implement synchronous commits, add generation fencing</td>\n</tr>\n<tr>\n<td><strong>Log segment corruption</strong></td>\n<td>Disk error or improper shutdown</td>\n<td>1. Check CRC validation logs<br>2. Verify disk SMART status<br>3. Check for OOM kill during write</td>\n<td>Implement segment CRC validation, add disk monitoring, ensure proper shutdown hooks</td>\n</tr>\n</tbody></table>\n<h4 id=\"d-testing-failure-recovery\">D. Testing Failure Recovery</h4>\n<p><strong>Integration Test Skeleton for Broker Failure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// In tests/integration/broker_failure_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestLeaderElectionOnBrokerFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start 3 brokers in test cluster</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create topic with replication factor 3, partitions 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Produce messages to establish leadership</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Kill leader broker (simulate crash)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Wait for election timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify new leader elected from ISR</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Produce more messages to new leader</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Verify all messages preserved (no data loss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Restart dead broker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Verify it rejoins as follower and catches up</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Property-based Test for Idempotent Producer:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Use gopter for property testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestProducerIdempotence</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> gopter.</span><span style=\"color:#B392F0\">DefaultTestParameters</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> gopter.</span><span style=\"color:#B392F0\">NewProperties</span><span style=\"color:#E1E4E8\">(parameters)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties.</span><span style=\"color:#B392F0\">Property</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"retried batches produce no duplicates\"</span><span style=\"color:#E1E4E8\">, prop.</span><span style=\"color:#B392F0\">ForAll</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">initialSeq</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO: Implement test that:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // 1. Sends batch with sequence N</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // 2. Simulates network timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // 3. Retries same batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // 4. Verifies only one copy in log</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gen.</span><span style=\"color:#B392F0\">Int32Range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties.</span><span style=\"color:#B392F0\">TestingRun</span><span style=\"color:#E1E4E8\">(t)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>Key Learning Point:</strong> The most robust systems aren&#39;t those that never fail, but those that fail in predictable, recoverable ways. Your implementation should prioritize clear failure modes and recovery paths over trying to prevent all failures.</p>\n</blockquote>\n<h2 id=\"11-testing-strategy\">11. Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1, 2, 3, 4 (essential for validating all system behaviors)</p>\n</blockquote>\n<p>Building a distributed message queue is an exercise in managing complexity. The system&#39;s correctness depends on subtle interactions between concurrent components, network communication, and persistent storage. A comprehensive testing strategy is therefore not merely a validation step—it is a fundamental design tool that helps you reason about the system&#39;s behavior, catch bugs early, and build confidence in the implementation. This section provides a structured approach to testing, from unit tests of individual algorithms to integration tests that simulate real-world cluster operations.</p>\n<h3 id=\"111-testing-approach\">11.1 Testing Approach</h3>\n<p>Testing a distributed system is akin to stress-testing a suspension bridge. You must first validate each cable and beam in isolation (unit tests), then assemble them into spans and verify their connections (integration tests), and finally subject the entire structure to extreme loads and unexpected forces (property-based and fault injection tests) to ensure it won&#39;t collapse under real-world conditions.</p>\n<p>Our testing strategy employs three complementary layers, each targeting different levels of abstraction and failure modes:</p>\n<p><strong>1. Unit Tests: The Foundation of Logic Verification</strong>\nUnit tests focus on isolated components—individual functions, methods, and data structures—in a controlled environment. Their primary goal is to validate algorithmic correctness and internal state transitions without the complexity of network or disk I/O. For example, testing that the <code>HashPartitioner.Partition</code> function consistently routes the same key to the same partition, or that the <code>Index.FindEntry</code> binary search returns the correct file position. These tests should be fast, deterministic, and run without external dependencies.</p>\n<blockquote>\n<p><strong>Key Insight:</strong> Mock all I/O and external dependencies in unit tests. Use dependency injection to replace network clients, file systems, and random number generators with deterministic test doubles. This ensures tests remain reliable and fast, even as the system grows.</p>\n</blockquote>\n<p><strong>2. Integration Tests: Validating Component Interactions</strong>\nIntegration tests assemble multiple components and verify they work together correctly. This layer is crucial for our system because the core value emerges from interactions—between producers and brokers, consumers and coordinators, and leaders and followers. We recommend two types of integration tests:</p>\n<ul>\n<li><strong>In-process cluster tests:</strong> Launch multiple <code>Server</code> instances within the same process, connected via loopback network interfaces. This allows testing full cluster behaviors (rebalancing, leader election) without the overhead of process spawning.</li>\n<li><strong>Component integration tests:</strong> Test specific interfaces between components, such as a <code>Producer</code> sending to a real <code>Server</code> over TCP, or a <code>ConsumerGroup</code> interacting with a <code>Coordinator</code>.</li>\n</ul>\n<p>These tests should use real network sockets and file I/O, but run within a controlled test environment (e.g., temporary directories, deterministic port assignment).</p>\n<p><strong>3. Property-Based Tests: Exploring Edge Cases Systematically</strong>\nProperty-based testing (PBT) is a powerful technique for uncovering edge cases that example-based tests might miss. Instead of writing specific test cases (e.g., &quot;send 3 messages&quot;), you define properties that should <em>always</em> hold true for any valid input, and a test framework generates thousands of random inputs to verify them. For our system, important properties include:</p>\n<ul>\n<li><strong>Idempotence:</strong> Sending the same batch of records twice (with the same producer ID and sequence) should result in exactly one set of records in the log.</li>\n<li><strong>Ordering within partitions:</strong> For any sequence of messages sent to the same partition, the offsets returned must be strictly increasing and contiguous when no failures occur.</li>\n<li><strong>No data loss:</strong> The concatenation of all messages read by a consumer (after accounting for commits) must equal the concatenation of all messages sent by producers.</li>\n</ul>\n<p>PBT frameworks like <a href=\"https://github.com/leanovate/gopter\">gopter</a> for Go can generate complex scenarios: random network partitions, broker crashes, and concurrent client operations.</p>\n<p><strong>ADR: Testing Strategy Layering</strong></p>\n<blockquote>\n<p><strong>Decision: Three-Layer Testing Pyramid with Heavy Integration Focus</strong></p>\n<ul>\n<li><strong>Context:</strong> Our educational system must validate both algorithmic correctness (unit tests) and complex distributed behaviors (integration). Learners need immediate feedback on logic errors, but also confidence that components work together.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Unit-heavy pyramid:</strong> Traditional approach with 70% unit, 20% integration, 10% end-to-end tests. Fast but misses interaction bugs.</li>\n<li><strong>Integration-heavy &quot;hourglass&quot;:</strong> Equal emphasis on unit and end-to-end, with integration as the thick middle layer. Slower but catches more distributed bugs.</li>\n<li><strong>Property-based emphasis:</strong> Rely primarily on generated tests to explore state space. Powerful but requires significant upfront investment and expertise.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Adopt an integration-heavy hourglass model, supplemented by property-based tests for core invariants.</li>\n<li><strong>Rationale:</strong> The greatest learning value and most subtle bugs in distributed systems arise from component interactions. Integration tests provide concrete, observable behaviors that learners can debug. Property-based tests complement by systematically exploring edge cases that manual test design might miss.</li>\n<li><strong>Consequences:</strong> Test suite will run slower than unit-only approaches, requiring careful use of test parallelism and short timeouts. However, it will provide higher confidence in system correctness and better prepare learners for real-world debugging.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit-heavy pyramid</td>\n<td>Fast execution, clear isolation, easy to debug</td>\n<td>Misses interaction bugs, gives false confidence</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Integration-heavy hourglass</td>\n<td>Catches distributed bugs, validates real workflows</td>\n<td>Slower, more complex test setup</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Property-based emphasis</td>\n<td>Explores vast state space, finds obscure edge cases</td>\n<td>Steep learning curve, harder to debug failures</td>\n<td>Supplementary</td>\n</tr>\n</tbody></table>\n<p><strong>Common Pitfalls in Testing Distributed Systems</strong></p>\n<p>⚠️ <strong>Pitfall: Non-deterministic tests due to timing and concurrency</strong>\nTests that rely on real-time delays (<code>time.Sleep</code>) or uncoordinated goroutines will fail intermittently, eroding trust in the test suite.</p>\n<ul>\n<li><strong>Why it&#39;s wrong:</strong> Flaky tests mask real bugs and waste debugging time. They often pass in development but fail in CI.</li>\n<li><strong>How to fix:</strong> Use synchronized channels or condition variables to signal when events occur. For time-based operations, inject a mock clock that can be advanced manually in tests.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not cleaning up resources between tests</strong>\nLeaving open files, network sockets, or goroutines from one test can affect subsequent tests.</p>\n<ul>\n<li><strong>Why it&#39;s wrong:</strong> Tests become order-dependent and may fail when run in isolation vs. as a suite.</li>\n<li><strong>How to fix:</strong> Use <code>t.Cleanup()</code> or <code>defer</code> to close resources. For goroutines, pass a <code>context.Context</code> that gets cancelled when the test ends.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Over-mocking eliminates integration value</strong>\nMocking every dependency, including the local log storage or in-memory maps, reduces integration tests to unit tests of the mock setup.</p>\n<ul>\n<li><strong>Why it&#39;s wrong:</strong> You&#39;re not testing the actual interaction between components, just your assumptions about how they <em>should</em> interact.</li>\n<li><strong>How to fix:</strong> Use real implementations for internal components (like the <code>WAL</code> or <code>Log</code>), but mock only external boundaries (network between separate processes, disk I/O if too slow).</li>\n</ul>\n<h3 id=\"112-milestone-verification-checkpoints\">11.2 Milestone Verification Checkpoints</h3>\n<p>Each milestone in the project builds upon the previous one, creating a cumulative verification challenge. The following checkpoints provide concrete, executable scenarios you can use to validate that your implementation meets the core requirements. These are not exhaustive tests, but they represent critical paths that, if working correctly, indicate fundamental correctness.</p>\n<p><strong>Checkpoint Structure:</strong>\nFor each milestone, we define:</p>\n<ol>\n<li><strong>Test Scenario:</strong> A narrative description of the operation to test.</li>\n<li><strong>Preconditions:</strong> State of the system before the test.</li>\n<li><strong>Steps:</strong> Concrete actions to perform (manually or via test code).</li>\n<li><strong>Expected Output:</strong> What should happen after each step.</li>\n<li><strong>Verification Method:</strong> How to observe and confirm the expected behavior.</li>\n</ol>\n<p><strong>Milestone 1: Topic and Partitions</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Details</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Test Scenario</strong></td>\n<td>Create a topic with multiple partitions, produce messages with and without keys, and verify consistent partitioning and offset ordering.</td>\n</tr>\n<tr>\n<td><strong>Preconditions</strong></td>\n<td>Single broker running with clean data directory.</td>\n</tr>\n<tr>\n<td><strong>Steps</strong></td>\n<td>1. Create topic &quot;orders&quot; with 3 partitions.<br>2. Produce 5 messages: two with key=&quot;user1&quot;, two with key=&quot;user2&quot;, one with null key.<br>3. Fetch metadata to see partition assignments.<br>4. Read messages from each partition individually.</td>\n</tr>\n<tr>\n<td><strong>Expected Output</strong></td>\n<td>- Topic creation succeeds.<br>- Messages with same key land in same partition.<br>- Offsets within each partition are sequential (0,1,...).<br>- Null-key message distributes via round-robin.<br>- <code>LogEndOffset</code> for each partition matches message count.</td>\n</tr>\n<tr>\n<td><strong>Verification Method</strong></td>\n<td>Inspect broker logs or use a debug API to dump partition contents. Programmatically verify offsets and key-to-partition mapping.</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 2: Producer</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Details</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Test Scenario</strong></td>\n<td>Producer with <code>acks=all</code> sends batched messages, survives a leader failover, and retries without duplicates (idempotence optional).</td>\n</tr>\n<tr>\n<td><strong>Preconditions</strong></td>\n<td>3-node cluster with topic &quot;logs&quot; (replication factor 3). Producer configured with batch size=2, linger=100ms.</td>\n</tr>\n<tr>\n<td><strong>Steps</strong></td>\n<td>1. Send 10 messages asynchronously with callbacks.<br>2. Kill the partition leader while messages are in-flight.<br>3. Wait for new leader election.<br>4. Call <code>Flush()</code> and verify all callbacks fired with success.</td>\n</tr>\n<tr>\n<td><strong>Expected Output</strong></td>\n<td>- All 10 messages are eventually acknowledged.<br>- No duplicate messages appear in the log (check offsets).<br>- Producer logs show retry attempts after leader failure.<br>- High watermark advances only after all ISRs acknowledge.</td>\n</tr>\n<tr>\n<td><strong>Verification Method</strong></td>\n<td>Compare sent message count with total records in log segments. Check producer callback invocation count.</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 3: Consumer Groups</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Details</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Test Scenario</strong></td>\n<td>Two consumers join a group, receive balanced partition assignments, commit offsets, and trigger rebalance when one leaves.</td>\n</tr>\n<tr>\n<td><strong>Preconditions</strong></td>\n<td>Topic &quot;events&quot; with 4 partitions, containing 100 pre-existing messages.</td>\n</tr>\n<tr>\n<td><strong>Steps</strong></td>\n<td>1. Start Consumer A in group &quot;grp1&quot;, subscribe to &quot;events&quot;.<br>2. Start Consumer B in same group, subscribe to &quot;events&quot;.<br>3. Let each consumer poll 20 messages and commit offsets.<br>4. Stop Consumer B gracefully.<br>5. Observe Consumer A&#39;s partition reassignment.</td>\n</tr>\n<tr>\n<td><strong>Expected Output</strong></td>\n<td>- Initial assignment: each consumer gets 2 partitions (balanced).<br>- Each consumer reads only from its assigned partitions.<br>- Offset commits are persisted and survive consumer restart.<br>- After B leaves, A gets all 4 partitions within session timeout.<br>- No messages are processed twice (thanks to committed offsets).</td>\n</tr>\n<tr>\n<td><strong>Verification Method</strong></td>\n<td>Check coordinator&#39;s assignment state. Verify committed offsets in <code>__consumer_offsets</code> internal topic. Count total unique messages processed.</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 4: Replication</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Details</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Test Scenario</strong></td>\n<td>Leader-follower replication maintains consistency during network partition and recovers via ISR management.</td>\n</tr>\n<tr>\n<td><strong>Preconditions</strong></td>\n<td>3-node cluster, topic &quot;audit&quot; with RF=3, all replicas in sync.</td>\n</tr>\n<tr>\n<td><strong>Steps</strong></td>\n<td>1. Produce 50 messages with <code>acks=all</code>. Verify all replicas have same LEO.<br>2. Isolate follower 2 from leader (simulate network partition).<br>3. Produce 20 more messages.<br>4. Wait for replica.lag.time.max.ms to expire.<br>5. Check ISR size (should shrink to 2).<br>6. Heal network partition, verify follower catches up and rejoins ISR.</td>\n</tr>\n<tr>\n<td><strong>Expected Output</strong></td>\n<td>- First 50 messages replicated to all followers.<br>- ISR shrinks after follower 2 lags beyond threshold.<br>- Messages 51-70 are only replicated to remaining ISR members.<br>- After healing, follower 2 fetches missing messages and rejoins ISR.<br>- High watermark never exceeds LEO of slowest ISR member.</td>\n</tr>\n<tr>\n<td><strong>Verification Method</strong></td>\n<td>Query each replica&#39;s log end offset. Monitor ISR changes via metadata. Verify consumer reads only up to high watermark.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> These checkpoints intentionally stress the &quot;happy path&quot; <em>and</em> failure scenarios. A system that works only when nothing goes wrong is useless in production. Each milestone&#39;s checkpoint includes at least one injected failure (leader kill, consumer stop, network partition) to validate recovery logic.</p>\n</blockquote>\n<h3 id=\"113-tools-and-libraries\">11.3 Tools and Libraries</h3>\n<p>The Go ecosystem provides excellent testing libraries that align with our layered approach. The following recommendations balance simplicity for learners with power for complex scenarios.</p>\n<p><strong>Core Testing Framework: <code>testing</code> + <code>testify</code></strong>\nGo&#39;s built-in <code>testing</code> package is sufficient for most needs, but <code>testify</code> adds valuable assertions and mocking capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Library</th>\n<th>Purpose</th>\n<th>Key Features</th>\n<th>Example Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>testing</code> (standard)</td>\n<td>Basic test framework</td>\n<td><code>t.Run()</code> for subtests, <code>t.Helper()</code>, <code>t.Cleanup()</code></td>\n<td>All unit and integration tests</td>\n</tr>\n<tr>\n<td><code>testify/assert</code></td>\n<td>Readable assertions</td>\n<td><code>assert.Equal()</code>, <code>assert.NoError()</code>, helpful failure messages</td>\n<td>Replacing <code>if err != nil { t.Fatal() }</code> patterns</td>\n</tr>\n<tr>\n<td><code>testify/require</code></td>\n<td>Assertions that stop test</td>\n<td><code>require.True()</code> stops test immediately on failure</td>\n<td>Precondition validation in tests</td>\n</tr>\n<tr>\n<td><code>testify/suite</code></td>\n<td>Test suite organization</td>\n<td><code>SetupSuite()</code>, <code>TearDownTest()</code> methods</td>\n<td>Structuring integration test lifecycle</td>\n</tr>\n<tr>\n<td><code>testify/mock</code></td>\n<td>Mock object generation</td>\n<td>Generate mocks for interfaces, verify calls</td>\n<td>Mocking <code>ConnectionPool</code> in producer tests</td>\n</tr>\n</tbody></table>\n<p><strong>Mocking and Dependency Injection</strong>\nFor unit testing components in isolation, generate mocks for their dependencies. The <code>testify/mock</code> package works well, but consider interface design that facilitates testing:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Example: Interface for network connection to allow mocking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Connection</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Send</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">request</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#FFAB70\">response</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Real implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TCPConnection</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#6A737D\">/* ... */</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Mock implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockConnection</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    mock</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockConnection</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Send</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">request</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.</span><span style=\"color:#B392F0\">Called</span><span style=\"color:#E1E4E8\">(request)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> args.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">).([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">), args.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Property-Based Testing: <code>gopter</code></strong>\n<code>gopter</code> (GO Property Tester) generates random inputs and shrinks failing cases to minimal examples.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Benefit for Our System</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Arbitrary value generation</td>\n<td>Create random <code>Record</code> objects with varied keys, values, headers</td>\n</tr>\n<tr>\n<td>Stateful command testing</td>\n<td>Model producer/consumer interactions as state transitions</td>\n</tr>\n<tr>\n<td>Shrinking</td>\n<td>Reduce failing 1000-message test to 3-message minimal case</td>\n</tr>\n</tbody></table>\n<p><strong>Concurrency and Race Detection</strong>\nGo&#39;s race detector (<code>go test -race</code>) is essential for catching data races in concurrent code. Additionally:</p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Implementation</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Synchronized test verification</td>\n<td>Use channels to collect results from goroutines</td>\n<td>Verify concurrent producer sends</td>\n</tr>\n<tr>\n<td>Context cancellation</td>\n<td><code>context.WithTimeout</code> in tests</td>\n<td>Prevent hanging tests on deadlock</td>\n</tr>\n<tr>\n<td>WaitGroups</td>\n<td><code>sync.WaitGroup</code> to await goroutine completion</td>\n<td>Ensure all test goroutines finish</td>\n</tr>\n</tbody></table>\n<p><strong>Network Testing Utilities</strong>\nFor integration tests that require real network communication:</p>\n<table>\n<thead>\n<tr>\n<th>Utility</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>net.Listen(&quot;tcp&quot;, &quot;localhost:0&quot;)</code></td>\n<td>Get free port for test servers</td>\n<td>Start broker on random port</td>\n</tr>\n<tr>\n<td><code>httptest.Server</code> (if using HTTP)</td>\n<td>In-memory HTTP server for client tests</td>\n<td>Not used in our binary protocol</td>\n</tr>\n<tr>\n<td><code>io.Pipe()</code></td>\n<td>Connect reader/writer without sockets</td>\n<td>Test protocol encoding/decoding</td>\n</tr>\n</tbody></table>\n<p><strong>Temporary Resources Management</strong>\nAlways use temporary directories for test data to avoid collisions and ensure cleanup:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestBroker_RecoversLogs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tempDir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.</span><span style=\"color:#B392F0\">TempDir</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// Automatically cleaned up after test</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    broker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewBroker</span><span style=\"color:#E1E4E8\">(tempDir)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... test logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Integration Test Helper Pattern</strong>\nCreate a test helper that spins up an in-process cluster:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TestCluster</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Brokers []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TempDirs []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTestCluster</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestCluster</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Initialize 'size' brokers with temporary directories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Set up inter-broker communication via loopback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return cluster handle with cleanup via t.Cleanup()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test Framework</td>\n<td><code>testing</code> + <code>testify</code></td>\n<td><code>ginkgo</code> + <code>gomega</code> (BDD style)</td>\n</tr>\n<tr>\n<td>Mocking</td>\n<td><code>testify/mock</code> (manual)</td>\n<td><code>gomock</code> (code generation)</td>\n</tr>\n<tr>\n<td>Property Testing</td>\n<td>Manual random generation</td>\n<td><code>gopter</code> with custom generators</td>\n</tr>\n<tr>\n<td>Concurrency Testing</td>\n<td><code>go test -race</code>, channels</td>\n<td><code>stress</code> test with goroutine profiling</td>\n</tr>\n<tr>\n<td>Network Testing</td>\n<td>Loopback TCP with real sockets</td>\n<td><code>netem</code> for network fault injection</td>\n</tr>\n<tr>\n<td>Temporary Storage</td>\n<td><code>t.TempDir()</code></td>\n<td>RAM disk for faster I/O</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure for Tests</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    broker/\n      broker.go\n      broker_test.go           # Unit tests for broker logic\n    log/\n      log.go\n      log_test.go              # Unit tests for log operations\n    protocol/\n      encode_decode.go\n      encode_decode_test.go    # Property tests for wire format\n    coordinator/\n      coordinator.go\n      coordinator_integration_test.go  # Integration with real network\n  test/\n    integration/\n      cluster_test.go          # Multi-broker integration tests\n      producer_consumer_test.go # End-to-end workflows\n    property/\n      ordering_property_test.go # PBT for ordering guarantees\n    helpers/\n      test_cluster.go          # TestCluster helper\n      test_client.go           # Test producer/consumer clients\n  cmd/\n    server/\n      main.go\n    server_integration_test.go # Integration test for main binary</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code: Test Helper Utilities</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/helpers/test_cluster.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> helpers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/require</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestCluster manages an in-process broker cluster for integration tests.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TestCluster</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    brokers []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tempDirs []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTestCluster creates a cluster of 'size' brokers.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTestCluster</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestCluster</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    require.</span><span style=\"color:#B392F0\">Greater</span><span style=\"color:#E1E4E8\">(t, size, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"cluster size must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tc </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TestCluster</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t:      t,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        size:   size,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        brokers: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">, size),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tempDirs: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, size),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create coordination service (in-memory for tests)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    coord </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewMemoryCoordinator</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Initialize each broker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> size; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tempDir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.</span><span style=\"color:#B392F0\">TempDir</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tc.tempDirs[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tempDir</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Find free port</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        listener, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> net.</span><span style=\"color:#B392F0\">Listen</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tcp\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"localhost:0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        require.</span><span style=\"color:#B392F0\">NoError</span><span style=\"color:#E1E4E8\">(t, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        addr </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> listener.</span><span style=\"color:#B392F0\">Addr</span><span style=\"color:#E1E4E8\">().(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">net</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TCPAddr</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        listener.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Host:   </span><span style=\"color:#9ECBFF\">\"localhost\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Port:   addr.Port,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DataDir: tempDir,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            BrokerID: </span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">(i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        broker, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(config, coord)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        require.</span><span style=\"color:#B392F0\">NoError</span><span style=\"color:#E1E4E8\">(t, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tc.brokers[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> broker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Start broker in background</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">b</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.</span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"broker </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> stopped: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, i, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }(broker)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Register broker with coordination service</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> coord.</span><span style=\"color:#B392F0\">RegisterBroker</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">Broker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ID:   </span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">(i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Host: </span><span style=\"color:#9ECBFF\">\"localhost\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Port: </span><span style=\"color:#F97583\">int32</span><span style=\"color:#E1E4E8\">(addr.Port),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        require.</span><span style=\"color:#B392F0\">NoError</span><span style=\"color:#E1E4E8\">(t, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Wait for brokers to be ready</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tc.</span><span style=\"color:#B392F0\">waitForBrokersReady</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Cleanup on test completion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Cleanup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tc.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// waitForBrokersReady polls each broker until it responds to metadata requests.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestCluster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">waitForBrokersReady</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    deadline </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, broker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> tc.brokers {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(deadline) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Try to connect to broker's admin port (simplified)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> net.</span><span style=\"color:#B392F0\">DialTimeout</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tcp\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, broker.config.Host, broker.config.Port), </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                100</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Millisecond)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Millisecond)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        require.</span><span style=\"color:#B392F0\">True</span><span style=\"color:#E1E4E8\">(tc.t, time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(deadline), </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"broker </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> failed to start within timeout\"</span><span style=\"color:#E1E4E8\">, i)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown gracefully stops all brokers.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestCluster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tc.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> tc.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, broker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> tc.brokers {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> broker </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            broker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// Assumes Stop() method exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetBroker returns the broker at index i (0-based).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestCluster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetBroker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tc.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> tc.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    require.</span><span style=\"color:#B392F0\">Less</span><span style=\"color:#E1E4E8\">(tc.t, i, tc.size, </span><span style=\"color:#9ECBFF\">\"broker index out of range\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tc.brokers[i]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateTopic creates a topic on the cluster with default settings.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestCluster</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateTopic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partitions</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">replicationFactor</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation creates topic metadata via coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement using coordinator API</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code: Property-Based Test Example</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/property/ordering_property_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> property</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/leanovate/gopter</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/leanovate/gopter/gen</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/leanovate/gopter/prop</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestPartitionOrderingProperty verifies that offsets within a partition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// are always strictly increasing and contiguous when no failures occur.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestPartitionOrderingProperty</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> gopter.</span><span style=\"color:#B392F0\">DefaultTestParameters</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters.MinSuccessfulTests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters.MaxSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#6A737D\"> // Max messages per test run</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> gopter.</span><span style=\"color:#B392F0\">NewProperties</span><span style=\"color:#E1E4E8\">(parameters)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Generator for a slice of records with random keys/values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recordsGen </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> gen.</span><span style=\"color:#B392F0\">SliceOf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">genRecord</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties.</span><span style=\"color:#B392F0\">Property</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"offsets are strictly increasing within partition\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        prop.</span><span style=\"color:#B392F0\">ForAll</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">records</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Create a fresh in-memory Log for testing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Append all records using Log.Append</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Read back records from offset 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Verify each record's offset increases by exactly 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Verify the sequence of records matches input sequence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#6A737D\"> // placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }, recordsGen))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties.</span><span style=\"color:#B392F0\">TestingRun</span><span style=\"color:#E1E4E8\">(t)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// genRecord generates a random Record for property testing.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> genRecord</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">gopter</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Gen</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gen.</span><span style=\"color:#B392F0\">Struct</span><span style=\"color:#E1E4E8\">(reflect.</span><span style=\"color:#B392F0\">TypeOf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">{}), </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">gopter</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Gen</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Key\"</span><span style=\"color:#E1E4E8\">:   gen.</span><span style=\"color:#B392F0\">SliceOf</span><span style=\"color:#E1E4E8\">(gen.</span><span style=\"color:#B392F0\">UInt8</span><span style=\"color:#E1E4E8\">()).</span><span style=\"color:#B392F0\">SuchThat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(v.([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\"> }),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Value\"</span><span style=\"color:#E1E4E8\">: gen.</span><span style=\"color:#B392F0\">SliceOf</span><span style=\"color:#E1E4E8\">(gen.</span><span style=\"color:#B392F0\">UInt8</span><span style=\"color:#E1E4E8\">()).</span><span style=\"color:#B392F0\">SuchThat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(v.([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\"> }),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Headers\"</span><span style=\"color:#E1E4E8\">: gen.</span><span style=\"color:#B392F0\">SliceOf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">genHeader</span><span style=\"color:#E1E4E8\">()),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// genHeader generates a random Header.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> genHeader</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">gopter</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Gen</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gen.</span><span style=\"color:#B392F0\">Struct</span><span style=\"color:#E1E4E8\">(reflect.</span><span style=\"color:#B392F0\">TypeOf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">{}), </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">gopter</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Gen</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Key\"</span><span style=\"color:#E1E4E8\">:   gen.</span><span style=\"color:#B392F0\">AlphaString</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Value\"</span><span style=\"color:#E1E4E8\">: gen.</span><span style=\"color:#B392F0\">SliceOf</span><span style=\"color:#E1E4E8\">(gen.</span><span style=\"color:#B392F0\">UInt8</span><span style=\"color:#E1E4E8\">()),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints for Go Testing</strong></p>\n<ol>\n<li><strong>Table-Driven Tests:</strong> Use this pattern extensively for unit tests with multiple input cases:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   func</span><span style=\"color:#B392F0\"> TestHashPartitioner</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">testing.T) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       tests </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">struct</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           name </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           key []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           numPartitions </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           want </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       }{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           {</span><span style=\"color:#9ECBFF\">\"nil key\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           {</span><span style=\"color:#9ECBFF\">\"key 'abc'\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"abc\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       for</span><span style=\"color:#E1E4E8\"> _, tt </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> tests {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           t.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(tt.name, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               p </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HashPartitioner</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               got, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">Partition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"test\"</span><span style=\"color:#E1E4E8\">, tt.key, tt.numPartitions)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               require.</span><span style=\"color:#B392F0\">NoError</span><span style=\"color:#E1E4E8\">(t, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               require.</span><span style=\"color:#B392F0\">Equal</span><span style=\"color:#E1E4E8\">(t, tt.want, got)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><p><strong>Test Parallelization:</strong> Use <code>t.Parallel()</code> in independent tests to speed up execution, but avoid it for tests that share resources (like a global test cluster).</p>\n</li>\n<li><p><strong>Golden Files:</strong> For protocol serialization tests, consider using golden files to compare expected binary output:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">   golden </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"testdata\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"produce_request_v2.golden\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   if</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">update {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       os.</span><span style=\"color:#B392F0\">WriteFile</span><span style=\"color:#E1E4E8\">(golden, encoded, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   expected, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(golden)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   require.</span><span style=\"color:#B392F0\">Equal</span><span style=\"color:#E1E4E8\">(t, expected, encoded)</span></span></code></pre></div>\n\n<p><strong>F. Milestone Checkpoint Verification</strong></p>\n<p>After implementing each milestone, run the corresponding verification test:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Milestone 1: Topic and Partition basics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/broker</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestBroker_CreateTopicAndProduce</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># === RUN   TestBroker_CreateTopicAndProduce</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># --- PASS: TestBroker_CreateTopicAndProduce (0.2s)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># PASS</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 2: Producer with batching and retries  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/producer</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestProducer_SendWithLeaderFailover</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 3: Consumer group rebalancing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/coordinator</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestConsumerGroup_Rebalance</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 4: Replication and ISR management</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/replication</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestReplication_ISRShrinkAndRecover</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>G. Debugging Tips for Test Failures</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test hangs indefinitely</td>\n<td>Deadlock in goroutines, missing channel receive</td>\n<td>Use <code>pprof</code> goroutine dump, add test timeouts with <code>context.WithTimeout</code></td>\n<td>Ensure all goroutines have cancellation paths, use <code>select</code> with <code>ctx.Done()</code></td>\n</tr>\n<tr>\n<td>Intermittent test failures</td>\n<td>Race condition, timing dependency</td>\n<td>Run with <code>go test -race</code>, add <code>t.Log</code> statements to trace execution order</td>\n<td>Add proper synchronization (mutexes, channels), make tests deterministic</td>\n</tr>\n<tr>\n<td>&quot;Address already in use&quot;</td>\n<td>Previous test didn&#39;t clean up sockets</td>\n<td>Check for proper <code>t.Cleanup()</code> or <code>defer server.Stop()</code></td>\n<td>Ensure all servers are stopped, use random ports with <code>:0</code></td>\n</tr>\n<tr>\n<td>Disk space errors in CI</td>\n<td>Temporary files not cleaned up</td>\n<td>Check <code>t.TempDir()</code> usage, look for leftover files</td>\n<td>Always use <code>t.TempDir()</code>, not manual <code>/tmp</code> creation</td>\n</tr>\n<tr>\n<td>Consumer group rebalance storms</td>\n<td>Heartbeat intervals too short, session timeout too low</td>\n<td>Log rebalance triggers, increase test timeouts</td>\n<td>Adjust <code>session.timeout.ms</code> and <code>heartbeat.interval.ms</code> in test config</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Final Testing Principle:</strong> Write tests that fail in the way you expect. If a bug is fixed, add a test that would have caught it. Tests are not just verification—they are executable documentation of your system&#39;s intended behavior.</p>\n</blockquote>\n<h2 id=\"12-debugging-guide\">12. Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1, 2, 3, 4 (Debugging is relevant to all milestones)</p>\n</blockquote>\n<p>Building a distributed message queue is a complex endeavor with many moving parts. Even with careful design and implementation, bugs are inevitable. This debugging guide provides a systematic approach to diagnosing and fixing common issues that arise during development. Think of debugging a distributed system like being a <strong>mechanical engineer troubleshooting a multi-engine aircraft</strong>: you need to check each subsystem (engines, fuel lines, electrical) individually while also understanding how they interact, using both instrument readings (logs) and test procedures (diagnosis techniques) to isolate the root cause.</p>\n<h3 id=\"121-common-bug-symptoms\">12.1 Common Bug Symptoms</h3>\n<p>Distributed system bugs manifest in observable ways that affect either correctness (wrong results) or performance (slow results). Learning to recognize these symptoms is the first step toward diagnosis.</p>\n<ol>\n<li><p><strong>Messages Lost (Missing Data)</strong></p>\n<ul>\n<li><em>Description</em>: Messages sent by producers never appear when consumers fetch, or only some messages are delivered. This violates the at-least-once delivery guarantee and indicates data loss.</li>\n<li><em>Example Scenario</em>: A producer sends 100 messages with <code>acks=all</code>, but a consumer reading from the beginning only receives 85 messages, with no apparent pattern to which ones are missing.</li>\n</ul>\n</li>\n<li><p><strong>Duplicate Messages (Extra Data)</strong></p>\n<ul>\n<li><em>Description</em>: The same message appears multiple times in consumer output, either immediately or after restart. This violates at-most-once semantics and can cause incorrect application behavior.</li>\n<li><em>Example Scenario</em>: After a consumer crashes and restarts, it reprocesses messages it had already processed before the crash, even though it had committed offsets.</li>\n</ul>\n</li>\n<li><p><strong>Consumers Stuck (No Progress)</strong></p>\n<ul>\n<li><em>Description</em>: One or more consumers in a group stop fetching new messages despite producers continuing to send data. The consumer appears &quot;frozen&quot; at a particular offset.</li>\n<li><em>Example Scenario</em>: In a three-consumer group, two consumers continue processing messages while the third remains stuck at offset 42, never advancing despite new messages being available in its assigned partitions.</li>\n</ul>\n</li>\n<li><p><strong>High Latency (Slow Performance)</strong></p>\n<ul>\n<li><em>Description</em>: Message delivery takes significantly longer than expected, with producers experiencing slow acknowledgment times or consumers experiencing long delays between message batches.</li>\n<li><em>Example Scenario</em>: A producer configured with 10ms <code>linger.ms</code> actually waits 500ms before sending batches, or a consumer calling <code>Poll()</code> takes seconds to return even when messages are available.</li>\n</ul>\n</li>\n<li><p><strong>Rebalance Storms (Frequent Churn)</strong></p>\n<ul>\n<li><em>Description</em>: Consumer groups undergo constant rebalancing, with partitions being reassigned frequently even without consumers joining or leaving. This wastes resources and disrupts processing.</li>\n<li><em>Example Scenario</em>: A stable consumer group suddenly starts rebalancing every 2-3 seconds, causing repeated pauses in message consumption without any apparent reason.</li>\n</ul>\n</li>\n<li><p><strong>Inconsistent Ordering (Sequence Violation)</strong></p>\n<ul>\n<li><em>Description</em>: Messages with the same key appear in different order when consumed, violating the per-key ordering guarantee within a partition.</li>\n<li><em>Example Scenario</em>: A producer sends messages with keys A, B, A (in that order), but the consumer receives them as A, A, B, even though both A messages went to the same partition.</li>\n</ul>\n</li>\n<li><p><strong>ISR Shrinkage to Empty (No Replicas Available)</strong></p>\n<ul>\n<li><em>Description</em>: The In-Sync Replica set for a partition becomes empty, making the partition unavailable for writes or unsafe reads, often after a series of failures.</li>\n<li><em>Example Scenario</em>: After a network partition isolates the leader, all followers are removed from ISR, and when the leader fails, no eligible replica exists to become the new leader.</li>\n</ul>\n</li>\n<li><p><strong>Memory Leak (Growing Resource Usage)</strong></p>\n<ul>\n<li><em>Description</em>: Broker or client memory usage grows continuously over time without stabilizing, eventually leading to out-of-memory crashes.</li>\n<li><em>Example Scenario</em>: A broker&#39;s RSS memory increases by 10MB per hour even with constant message volume, eventually crashing after 24 hours.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"122-diagnosis-techniques\">12.2 Diagnosis Techniques</h3>\n<p>Effective debugging requires systematic observation and hypothesis testing. These techniques provide the &quot;instruments&quot; to understand what&#39;s happening inside your system.</p>\n<h4 id=\"structured-logging-with-context\">Structured Logging with Context</h4>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of structured logs as a <strong>flight data recorder</strong> (black box) for your distributed system. Each component continuously records its state, decisions, and interactions with timestamps, allowing you to reconstruct events leading to a failure.</p>\n</blockquote>\n<ul>\n<li><p><strong>Implementation Approach</strong>: Instead of <code>fmt.Printf</code>, use a structured logging library that supports:</p>\n<ul>\n<li><strong>Levels</strong>: DEBUG (internal state), INFO (normal operations), WARN (potential issues), ERROR (failures)</li>\n<li><strong>Fields</strong>: Key-value pairs attached to each log message (e.g., <code>partition=3</code>, <code>offset=142</code>, <code>consumer_id=&quot;c1&quot;</code>)</li>\n<li><strong>Correlation IDs</strong>: Unique identifiers passed through request chains to trace messages across components</li>\n</ul>\n</li>\n<li><p><strong>Critical Log Points to Add</strong>:</p>\n<ul>\n<li><em>Message Flow</em>: Log when messages are appended to log (with offset), when batches are sent/received, when consumers fetch messages</li>\n<li><em>State Changes</em>: Log partition leadership changes, ISR membership updates, consumer group state transitions</li>\n<li><em>Timing</em>: Log request durations, batch accumulation times, network round-trip times</li>\n<li><em>Errors</em>: Log all errors with full context (what operation failed, with what parameters, what error code)</li>\n</ul>\n</li>\n<li><p><strong>Example Log Configuration</strong>:</p>\n</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">  // In initialization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  log.</span><span style=\"color:#B392F0\">SetLevel</span><span style=\"color:#E1E4E8\">(log.DebugLevel)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  log.</span><span style=\"color:#B392F0\">SetFormatter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JSONFormatter</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">  // In broker append logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  log.</span><span style=\"color:#B392F0\">WithFields</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Fields</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">      \"topic\"</span><span style=\"color:#E1E4E8\">: topic,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">      \"partition\"</span><span style=\"color:#E1E4E8\">: partitionID,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">      \"offset\"</span><span style=\"color:#E1E4E8\">: newOffset,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">      \"batch_size\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(batch.Records),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">      \"leader_epoch\"</span><span style=\"color:#E1E4E8\">: leaderEpoch,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }).</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Appended records to partition\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"internal-state-inspection-via-admin-apis\">Internal State Inspection via Admin APIs</h4>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of state inspection as a <strong>submarine&#39;s control panel</strong> with gauges for each subsystem. By exposing internal metrics and state through diagnostic endpoints, you can check &quot;vital signs&quot; without stopping the system.</p>\n</blockquote>\n<ul>\n<li><p><strong>What to Expose</strong>:</p>\n<ul>\n<li><em>Broker Metrics</em>: Memory usage, open file descriptors, Goroutine count, request queue depths</li>\n<li><em>Partition State</em>: Log end offset, high watermark, ISR members, leader epoch, follower lag</li>\n<li><em>Consumer Group State</em>: Member list, assigned partitions, last heartbeat time, generation ID</li>\n<li><em>Producer State</em>: Batch accumulator sizes, in-flight request counts, retry queue depth</li>\n</ul>\n</li>\n<li><p><strong>Implementation Approach</strong>: Create a simple HTTP endpoint (<code>/debug/state</code>) that returns JSON representations of key data structures. Consider thread-safe snapshot approaches:</p>\n</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">  // Example: Broker state snapshot</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  type</span><span style=\"color:#B392F0\"> BrokerDebugState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      Topics     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">TopicDebugState</span><span style=\"color:#9ECBFF\"> `json:\"topics\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      Goroutines </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">                        `json:\"goroutines\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      MemoryMB   </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">                    `json:\"memory_mb\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      Uptime     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                     `json:\"uptime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">  // Called from debug endpoint handler</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  func</span><span style=\"color:#E1E4E8\"> (b </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">Broker) </span><span style=\"color:#B392F0\">GetDebugState</span><span style=\"color:#E1E4E8\">() BrokerDebugState {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      b.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      defer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      state </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> BrokerDebugState</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          Topics: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">TopicDebugState</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          Goroutines: runtime.</span><span style=\"color:#B392F0\">NumGoroutine</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          MemoryMB: </span><span style=\"color:#B392F0\">getMemoryUsageMB</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      for</span><span style=\"color:#E1E4E8\"> name, topic </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> b.topics {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          state.Topics[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> topic.</span><span style=\"color:#B392F0\">GetDebugState</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      return</span><span style=\"color:#E1E4E8\"> state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span></code></pre></div>\n\n<h4 id=\"timeout-based-bottleneck-identification\">Timeout-Based Bottleneck Identification</h4>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of timeouts as <strong>circuit breakers</strong> in an electrical system. When a component takes too long to respond, the timeout &quot;trips&quot; and allows the system to continue with degraded functionality, while also signaling where bottlenecks exist.</p>\n</blockquote>\n<ul>\n<li><p><strong>Strategic Timeout Placement</strong>:</p>\n<ol>\n<li><strong>Network Timeouts</strong>: Set deadlines on all socket reads/writes (e.g., 30 seconds)</li>\n<li><strong>Request Timeouts</strong>: Time out entire RPC operations (produce, fetch, join group)</li>\n<li><strong>Internal Operation Timeouts</strong>: Time out log flushes, lock acquisitions, channel operations</li>\n</ol>\n</li>\n<li><p><strong>Diagnosis Technique</strong>: When timeouts occur, examine:</p>\n<ul>\n<li><em>Which operation</em> timed out (produce, fetch, heartbeat)</li>\n<li><em>Which component</em> was involved (which broker, which partition)</li>\n<li><em>What else</em> was happening at that time (other requests, garbage collection, disk I/O)</li>\n<li><em>Correlation</em> with other symptoms (high latency, stuck consumers)</li>\n</ul>\n</li>\n<li><p><strong>Timeout Configuration Table</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Timeout Type</th>\n<th>Default Value</th>\n<th>Purpose</th>\n<th>What to Check When Hit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>socket.timeout.ms</code></td>\n<td>30000</td>\n<td>Network read/write deadline</td>\n<td>Network connectivity, broker load</td>\n</tr>\n<tr>\n<td><code>request.timeout.ms</code></td>\n<td>30000</td>\n<td>Complete RPC operation</td>\n<td>Broker processing time, lock contention</td>\n</tr>\n<tr>\n<td><code>rebalance.timeout.ms</code></td>\n<td>60000</td>\n<td>Maximum rebalance duration</td>\n<td>Consumer join/leave coordination</td>\n</tr>\n<tr>\n<td><code>fetch.timeout.ms</code></td>\n<td>500</td>\n<td>Consumer poll wait</td>\n<td>Message availability, consumer lag</td>\n</tr>\n<tr>\n<td><code>heartbeat.timeout.ms</code></td>\n<td>10000</td>\n<td>Consumer liveness detection</td>\n<td>Network partitions, consumer GC pauses</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<h4 id=\"controlled-experimentation-scientific-method\">Controlled Experimentation (Scientific Method)</h4>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of debugging as conducting <strong>laboratory experiments</strong>. You formulate hypotheses about root causes, design tests to isolate variables, run experiments, and analyze results.</p>\n</blockquote>\n<ul>\n<li><p><strong>Process</strong>:</p>\n<ol>\n<li><strong>Observe</strong>: Document the symptom precisely (what, when, where, how often)</li>\n<li><strong>Hypothesize</strong>: Propose a potential root cause based on understanding of the system</li>\n<li><strong>Experiment</strong>: Design a minimal test to verify/reject the hypothesis</li>\n<li><strong>Analyze</strong>: Compare actual results with expected results</li>\n<li><strong>Iterate</strong>: Refine hypothesis based on results</li>\n</ol>\n</li>\n<li><p><strong>Example Experiment for Message Loss</strong>:</p>\n<ul>\n<li><em>Observation</em>: 15% of messages lost with <code>acks=1</code></li>\n<li><em>Hypothesis</em>: Leader is crashing after acknowledging but before replicating to disk</li>\n<li><em>Experiment</em>: Add detailed logging at leader: log immediately before and after disk sync</li>\n<li><em>Analysis</em>: Check if crash logs show unsynced writes; verify with <code>WAL.Append(..., sync=true)</code></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"concurrency-race-detection\">Concurrency Race Detection</h4>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of race conditions as <strong>traffic intersections without signals</strong>. Multiple threads (cars) approach simultaneously, and depending on timing, may collide or proceed safely. Detection tools are like traffic cameras capturing these interactions.</p>\n</blockquote>\n<ul>\n<li><p><strong>Go-Specific Tools</strong>:</p>\n<ul>\n<li><code>go test -race</code>: Run tests with race detector enabled (adds overhead but finds data races)</li>\n<li><code>sync/atomic</code> operations: For simple counters where lock overhead is unacceptable</li>\n<li><code>context.Context</code> for cancellation: Properly propagate cancellation through goroutines</li>\n</ul>\n</li>\n<li><p><strong>Common Race Patterns to Watch For</strong>:</p>\n<ul>\n<li><em>Read-Modify-Write</em>: Incrementing a counter without synchronization</li>\n<li><em>Check-Then-Act</em>: Checking a condition then acting on it without holding lock</li>\n<li><em>Publication</em>: Publishing a reference before initialization completes</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"123-symptom-cause-fix-table\">12.3 Symptom → Cause → Fix Table</h3>\n<p>This table maps observable symptoms to their most likely root causes, diagnosis steps, and specific fixes. Use it as a starting point for your debugging journey.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Messages Lost</strong></td>\n<td>1. <strong>Producer using <code>acks=0</code> (fire-and-forget)</strong> with network failures<br>2. <strong>Leader crashes</strong> after acknowledging but before persisting to disk (<code>acks=1</code>)<br>3. <strong>ISR shrinks to empty</strong>, then unclean leader election discards unreplicated data<br>4. <strong>Consumer commits offset</strong> before processing, then crashes</td>\n<td>1. Check producer configuration for <code>AcksNone</code><br>2. Examine broker logs for crashes right after produce acknowledgments<br>3. Monitor ISR size metrics; check for follower lag exceeding threshold<br>4. Check consumer commit logic (offset committed before business logic)</td>\n<td>1. Use <code>AcksLeader</code> or <code>AcksAll</code> for durability<br>2. Ensure <code>WAL.Append</code> calls <code>sync=true</code> when <code>acks&gt;=1</code><br>3. Configure <code>unclean.leader.election.enable=false</code><br>4. Commit offsets <strong>after</strong> successful message processing</td>\n</tr>\n<tr>\n<td><strong>Duplicate Messages</strong></td>\n<td>1. <strong>Producer retries</strong> without idempotency after transient errors<br>2. <strong>Consumer rebalance</strong> causes reprocessing of uncommitted offsets<br>3. <strong>Consumer commits offset</strong> after processing but crashes before commit persists<br>4. <strong>Leader epoch mismatch</strong> during leadership change causes old leader to accept writes</td>\n<td>1. Check producer logs for retry attempts with same batch<br>2. Examine consumer group generation changes; check offset commits during rebalance<br>3. Check disk sync on offset commit store (<code>OffsetStore.persistGroup</code>)<br>4. Check leader epoch validation in produce request handling</td>\n<td>1. Implement idempotent producer with sequence numbers<br>2. Implement cooperative rebalancing with offset commit before partition revocation<br>3. Use synchronous offset commits with disk sync<br>4. Validate leader epoch in all write paths; reject stale writes</td>\n</tr>\n<tr>\n<td><strong>Consumers Stuck</strong></td>\n<td>1. <strong>Heartbeat failures</strong> causing coordinator to mark consumer dead<br>2. <strong>Fetch loop blocked</strong> on slow I/O or deadlock<br>3. <strong>No messages</strong> at current offset (log compacted or truncated)<br>4. <strong>Assignment mismatch</strong> - consumer thinks it has partitions it doesn&#39;t</td>\n<td>1. Check coordinator logs for heartbeat timeouts<br>2. Add debug logs to fetch loop; check for lock acquisition<br>3. Compare consumer offset with log start offset<br>4. Verify assignment in <code>ConsumerMetadata</code> vs actual fetch requests</td>\n<td>1. Increase <code>session.timeout.ms</code> or fix long GC pauses<br>2. Add timeouts to all blocking operations; fix deadlocks<br>3. Implement auto-offset reset to <code>earliest</code> or <code>latest</code><br>4. Validate assignments in <code>Poll()</code> before fetching</td>\n</tr>\n<tr>\n<td><strong>High Latency</strong></td>\n<td>1. <strong>Small batch sizes</strong> causing excessive network round trips<br>2. <strong>Disk I/O contention</strong> from multiple partitions on same disk<br>3. <strong>Memory pressure</strong> causing excessive garbage collection<br>4. <strong>Network bottlenecks</strong> between producers and broker cluster</td>\n<td>1. Measure batch sizes sent; check <code>BatchSize</code> and <code>LingerMs</code> settings<br>2. Monitor disk I/O wait times; check if partitions share data directory<br>3. Monitor Go GC pause times and heap size<br>4. Measure network latency between client and broker nodes</td>\n<td>1. Tune <code>batch.size</code> and <code>linger.ms</code> for throughput/latency tradeoff<br>2. Separate partition data to different physical disks<br>3. Optimize memory allocations; reuse buffers; set GOGC appropriately<br>4. Ensure brokers are geographically close to producers or use compression</td>\n</tr>\n<tr>\n<td><strong>Rebalance Storms</strong></td>\n<td>1. <strong>Session timeout too short</strong> for consumers with GC pauses<br>2. <strong>Heartbeat thread blocked</strong> by long-running operations<br>3. <strong>Coordinator overload</strong> causing delayed heartbeat processing<br>4. <strong>Network flakiness</strong> causing intermittent connectivity loss</td>\n<td>1. Check consumer GC logs for long pauses (&gt; session timeout)<br>2. Verify heartbeat runs in dedicated goroutine without blocking<br>3. Monitor coordinator CPU/memory during rebalances<br>4. Check network packet loss statistics between consumers and coordinator</td>\n<td>1. Increase <code>session.timeout.ms</code> to accommodate GC pauses<br>2. Run heartbeat in separate goroutine with its own context<br>3. Add load shedding to coordinator; scale horizontally<br>4. Improve network reliability or implement exponential backoff on rejoin</td>\n</tr>\n<tr>\n<td><strong>Inconsistent Ordering</strong></td>\n<td>1. <strong>Key hash collision</strong> sending same key to different partitions<br>2. <strong>Producer retries</strong> reordering messages due to partial failures<br>3. <strong>Multiple producers</strong> writing to same partition without coordination<br>4. <strong>Consumer reading from wrong offset</strong> due to incorrect index lookup</td>\n<td>1. Test partitioner with known keys; verify consistent mapping<br>2. Examine producer logs for out-of-order retry attempts<br>3. Check if multiple producer instances use same partition<br>4. Verify <code>Index.FindEntry</code> logic returns correct position</td>\n<td>1. Ensure <code>HashPartitioner.Partition</code> uses stable hash algorithm<br>2. Implement producer idempotency with sequence numbers<br>3. Use single producer per partition or leader epoch fencing<br>4. Test index lookup with edge cases (exact match, between entries)</td>\n</tr>\n<tr>\n<td><strong>ISR Shrinkage to Empty</strong></td>\n<td>1. <strong>Follower lag threshold too small</strong> for normal operation<br>2. <strong>Network partition</strong> isolating followers from leader<br>3. <strong>Follower I/O issues</strong> causing slow replication<br>4. <strong>Leader not updating follower state</strong> in <code>ISRManager</code></td>\n<td>1. Check <code>replica.lag.time.max.ms</code> vs actual replication latency<br>2. Examine network connectivity between broker nodes<br>3. Monitor follower disk I/O metrics and replication fetch times<br>4. Verify <code>ISRManager.UpdateFollowerState</code> is called on successful fetch</td>\n<td>1. Increase <code>replica.lag.time.max.ms</code> to accommodate temporary lag<br>2. Implement network health checks and partition detection<br>3. Separate replication traffic to dedicated network interfaces<br>4. Ensure follower state updates are not silently dropped</td>\n</tr>\n<tr>\n<td><strong>Memory Leak</strong></td>\n<td>1. <strong>Goroutine leak</strong> from unbounded channel operations or missing <code>context</code> cancellation<br>2. <strong>Cache growth without eviction</strong> in metadata or connection pools<br>3. <strong>File descriptor leak</strong> from unclosed log segment files<br>4. <strong>Reference cycles</strong> preventing garbage collection</td>\n<td>1. Monitor goroutine count over time; check for <code>go</code> statements without cleanup<br>2. Track cache sizes (<code>MetadataCache</code>, <code>ConnectionPool</code>)<br>3. Check <code>lsof</code> output for growing open files in broker process<br>4. Use heap profiling to identify reference chains</td>\n<td>1. Add <code>defer cancel()</code> to all <code>context.WithCancel</code>; close all channels<br>2. Implement LRU eviction or size limits on caches<br>3. Ensure <code>LogSegment</code> files are closed when no longer needed<br>4. Break cycles using weak references or explicit cleanup methods</td>\n</tr>\n</tbody></table>\n<h4 id=\"debugging-workflow-example-diagnosing-duplicate-messages\">Debugging Workflow Example: Diagnosing Duplicate Messages</h4>\n<p>Let&#39;s walk through a concrete debugging scenario using the techniques above:</p>\n<ol>\n<li><p><strong>Observe Symptom</strong>: Consumer application reports processing the same message ID twice.</p>\n</li>\n<li><p><strong>Initial Investigation</strong>:</p>\n<ul>\n<li>Check consumer logs: &quot;Committing offset 142 for partition topic-0&quot;</li>\n<li>Check producer logs: &quot;Retrying batch for topic-0 partition 0 (attempt 2/3)&quot;</li>\n<li>Correlation: Retry occurs around same time as offset commit</li>\n</ul>\n</li>\n<li><p><strong>Form Hypothesis</strong>: Producer retry after successful send but before acknowledgment received.</p>\n</li>\n<li><p><strong>Design Experiment</strong>:</p>\n<ul>\n<li>Add logging to producer: &quot;Batch sent with sequence X&quot; and &quot;Received ack for sequence X&quot;</li>\n<li>Add logging to consumer: &quot;Processing offset Y&quot; and &quot;Committed offset Y&quot;</li>\n<li>Reproduce with network simulation: Add artificial latency between send and ack</li>\n</ul>\n</li>\n<li><p><strong>Analyze Results</strong>:</p>\n<ul>\n<li>Logs show: Send seq=5 → Network delay → Timeout → Retry seq=5 → Original ack arrives → Retry succeeds → Two copies</li>\n<li>Consumer processes offset 142 twice because it crashes between processing and commit</li>\n</ul>\n</li>\n<li><p><strong>Root Cause</strong>: Two issues combined:</p>\n<ul>\n<li>Producer: No idempotency, retries create duplicates</li>\n<li>Consumer: Offset committed asynchronously, crash causes reprocessing</li>\n</ul>\n</li>\n<li><p><strong>Implement Fix</strong>:</p>\n<ul>\n<li>Producer: Add <code>ProducerID</code> and sequence numbers to <code>RecordBatch</code>, broker deduplicates</li>\n<li>Consumer: Change <code>CommitAsync</code> to <code>CommitSync</code> after processing completes</li>\n</ul>\n</li>\n<li><p><strong>Verify Fix</strong>:</p>\n<ul>\n<li>Run integration test with injected network failures</li>\n<li>Confirm no duplicates in 10,000 message test</li>\n<li>Monitor for performance regression from synchronous commits</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of debugging infrastructure as <strong>scaffolding around a building under construction</strong>. You build temporary platforms (logging), install safety nets (metrics), and create access points (debug APIs) that help you work on the main structure, then remove or minimize them when the building is complete.</p>\n</blockquote>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Logging</strong></td>\n<td><code>logrus</code> or <code>zap</code> for structured JSON logging</td>\n<td>OpenTelemetry with distributed tracing to correlate requests across services</td>\n</tr>\n<tr>\n<td><strong>Metrics</strong></td>\n<td>Prometheus client library exposing <code>/metrics</code> endpoint</td>\n<td>StatsD/DataDog integration with dashboards and alerting</td>\n</tr>\n<tr>\n<td><strong>Profiling</strong></td>\n<td>Go&#39;s built-in <code>pprof</code> HTTP endpoints (<code>/debug/pprof/</code>)</td>\n<td>Continuous profiling with Parca or Pyroscope for production</td>\n</tr>\n<tr>\n<td><strong>Debug API</strong></td>\n<td>Simple HTTP server with JSON endpoints for internal state</td>\n<td>gRPC health checks and reflection for deeper inspection</td>\n</tr>\n<tr>\n<td><strong>Chaos Testing</strong></td>\n<td>Manual network partition simulation (<code>iptables</code>)</td>\n<td>ChaosMesh or Litmus for automated failure injection</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n├── cmd/\n│   ├── broker/                 # Broker executable\n│   │   └── main.go\n│   ├── producer-cli/           # Producer CLI tool\n│   │   └── main.go\n│   └── consumer-cli/           # Consumer CLI tool\n│       └── main.go\n├── internal/\n│   ├── debug/                  # Debugging infrastructure\n│   │   ├── debug_server.go     # HTTP server for debug endpoints\n│   │   ├── metrics.go          # Prometheus metrics collection\n│   │   ├── pprof.go            # pprof endpoint setup\n│   │   └── state_dump.go       # State snapshot utilities\n│   ├── logging/                # Structured logging setup\n│   │   ├── logger.go           # Logger initialization\n│   │   ├── context.go          # Context with correlation IDs\n│   │   └── fields.go           # Common log field definitions\n│   └── ... (other components)\n└── scripts/\n    ├── simulate-failure.sh     # Script to inject network failures\n    ├── analyze-logs.py         # Log analysis utility\n    └── memory-profile.sh       # Memory profiling script</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-structured-logger-wrapper\">C. Infrastructure Starter Code: Structured Logger Wrapper</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/logging/logger.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#B392F0\">github.com/sirupsen/logrus</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ctxKey</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    correlationIDKey</span><span style=\"color:#B392F0\"> ctxKey</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"correlation_id\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    componentKey</span><span style=\"color:#B392F0\">     ctxKey</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"component\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitLogger configures the global logger with structured JSON output</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> InitLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">level</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">enableCaller</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logLevel, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> log.</span><span style=\"color:#B392F0\">ParseLevel</span><span style=\"color:#E1E4E8\">(level)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logLevel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> log.InfoLevel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">SetLevel</span><span style=\"color:#E1E4E8\">(logLevel)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">SetFormatter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">JSONFormatter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TimestampFormat: </span><span style=\"color:#9ECBFF\">\"2006-01-02T15:04:05.999Z07:00\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        FieldMap: </span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FieldMap</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log.FieldKeyTime:  </span><span style=\"color:#9ECBFF\">\"timestamp\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log.FieldKeyLevel: </span><span style=\"color:#9ECBFF\">\"severity\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log.FieldKeyMsg:   </span><span style=\"color:#9ECBFF\">\"message\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log.FieldKeyFunc:  </span><span style=\"color:#9ECBFF\">\"caller\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">SetOutput</span><span style=\"color:#E1E4E8\">(os.Stdout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> enableCaller {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">SetReportCaller</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WithContext returns a log entry with fields from context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> WithContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Entry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> log.</span><span style=\"color:#B392F0\">NewEntry</span><span style=\"color:#E1E4E8\">(log.</span><span style=\"color:#B392F0\">StandardLogger</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ctx </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> corrID, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">(correlationIDKey).(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> corrID </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entry.</span><span style=\"color:#B392F0\">WithField</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"correlation_id\"</span><span style=\"color:#E1E4E8\">, corrID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> component, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">(componentKey).(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> component </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entry.</span><span style=\"color:#B392F0\">WithField</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"component\"</span><span style=\"color:#E1E4E8\">, component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add caller info if enabled</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> log.</span><span style=\"color:#B392F0\">StandardLogger</span><span style=\"color:#E1E4E8\">().ReportCaller {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> pc, file, line, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> runtime.</span><span style=\"color:#B392F0\">Caller</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            funcName </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> runtime.</span><span style=\"color:#B392F0\">FuncForPC</span><span style=\"color:#E1E4E8\">(pc).</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Simplify file path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> idx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">LastIndex</span><span style=\"color:#E1E4E8\">(file, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">); idx </span><span style=\"color:#F97583\">!=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                file </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> file[idx</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entry.</span><span style=\"color:#B392F0\">WithField</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"caller\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">                log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Fields</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"file\"</span><span style=\"color:#E1E4E8\">: file, </span><span style=\"color:#9ECBFF\">\"line\"</span><span style=\"color:#E1E4E8\">: line, </span><span style=\"color:#9ECBFF\">\"function\"</span><span style=\"color:#E1E4E8\">: funcName})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewContextWithCorrelation creates a new context with correlation ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewContextWithCorrelation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">parent</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">corrID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithValue</span><span style=\"color:#E1E4E8\">(parent, correlationIDKey, corrID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewContextWithComponent creates a new context with component name</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewContextWithComponent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">parent</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithValue</span><span style=\"color:#E1E4E8\">(parent, componentKey, component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogLevelFromString safely parses log level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LogLevelFromString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">level</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Level</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> log.</span><span style=\"color:#B392F0\">ParseLevel</span><span style=\"color:#E1E4E8\">(level)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> log.InfoLevel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> l</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-debug-state-collection\">D. Core Logic Skeleton: Debug State Collection</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/debug/state_dump.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debug</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourproject/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DebugStateCollector gathers state from various components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DebugStateCollector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    broker    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDebugStateCollector creates a new collector</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDebugStateCollector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">broker</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DebugStateCollector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DebugStateCollector</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        broker:    broker,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        startTime: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CollectBrokerState gathers comprehensive broker state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DebugStateCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CollectBrokerState</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> d.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Collect basic broker info (ID, host, port, uptime)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Collect memory statistics using runtime.MemStats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Collect goroutine count and thread information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Collect topic and partition state (call broker.GetDebugState())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Collect consumer group state from coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Collect replication state from replica manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Collect connection pool statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Collect request queue depths and processing times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Collect disk usage information for log directories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Collect any custom metrics stored in d.metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> state, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordMetric stores a custom metric for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DebugStateCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> d.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d.metrics[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMemoryStats returns Go memory statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DebugStateCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMemoryStats</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> memStats </span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MemStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runtime.</span><span style=\"color:#B392F0\">ReadMemStats</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">memStats)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"alloc_mb\"</span><span style=\"color:#E1E4E8\">:        </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(memStats.Alloc) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> /</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"total_alloc_mb\"</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(memStats.TotalAlloc) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> /</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"sys_mb\"</span><span style=\"color:#E1E4E8\">:          </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(memStats.Sys) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> /</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"num_gc\"</span><span style=\"color:#E1E4E8\">:          memStats.NumGC,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"last_gc_pause_ns\"</span><span style=\"color:#E1E4E8\">: memStats.PauseNs[(memStats.NumGC</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">255</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"goroutines\"</span><span style=\"color:#E1E4E8\">:      runtime.</span><span style=\"color:#B392F0\">NumGoroutine</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"cgo_calls\"</span><span style=\"color:#E1E4E8\">:       runtime.</span><span style=\"color:#B392F0\">NumCgoCall</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetUptime returns formatted uptime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DebugStateCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetUptime</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    uptime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(d.startTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    days </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(uptime.</span><span style=\"color:#B392F0\">Hours</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hours </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(uptime.</span><span style=\"color:#B392F0\">Hours</span><span style=\"color:#E1E4E8\">()) </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 24</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    minutes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(uptime.</span><span style=\"color:#B392F0\">Minutes</span><span style=\"color:#E1E4E8\">()) </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seconds </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(uptime.</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">()) </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> days </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">d </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">h </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">m </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">s\"</span><span style=\"color:#E1E4E8\">, days, hours, minutes, seconds)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">h </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">m </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">s\"</span><span style=\"color:#E1E4E8\">, hours, minutes, seconds)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// JSONString returns state as pretty-printed JSON</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DebugStateCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">JSONString</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> d.</span><span style=\"color:#B392F0\">CollectBrokerState</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">MarshalIndent</span><span style=\"color:#E1E4E8\">(state, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"  \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(data), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-go\">E. Language-Specific Hints (Go)</h4>\n<ol>\n<li><strong>Memory Profiling</strong>: Use <code>go tool pprof -alloc_space http://localhost:6060/debug/pprof/heap</code> to find memory leaks.</li>\n<li><strong>Block Profiling</strong>: Use <code>go tool pprof http://localhost:6060/debug/pprof/block</code> to find goroutine blocking.</li>\n<li><strong>Race Detection</strong>: Always run tests with <code>go test -race ./...</code>; add <code>-race</code> flag to production for critical services.</li>\n<li><strong>Context Usage</strong>: Pass <code>context.Context</code> through all async operations; use <code>context.WithTimeout</code> for operations that should complete within a deadline.</li>\n<li><strong>Structured Logging</strong>: Use <code>logrus.Fields</code> for key-value pairs rather than formatted strings.</li>\n<li><strong>Metrics Collection</strong>: Use Prometheus <code>Gauge</code>, <code>Counter</code>, and <code>Histogram</code> for different types of measurements.</li>\n<li><strong>Debug Endpoints</strong>: Use <code>net/http/pprof</code> package and register handlers: <code>import _ &quot;net/http/pprof&quot;</code>.</li>\n<li><strong>Testing Network Failures</strong>: Use <code>nettest</code> package for simulated network conditions in tests.</li>\n</ol>\n<h4 id=\"f-debugging-tips-table\">F. Debugging Tips Table</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>How to Diagnose</th>\n<th>Useful Command/Tool</th>\n<th>What to Look For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>High CPU</strong></td>\n<td>CPU profiling</td>\n<td><code>go tool pprof -seconds 30 http://localhost:6060/debug/pprof/profile</code></td>\n<td>Functions with highest <code>cum</code> (cumulative) time</td>\n</tr>\n<tr>\n<td><strong>Memory Leak</strong></td>\n<td>Heap profiling</td>\n<td><code>go tool pprof -alloc_space http://localhost:6060/debug/pprof/heap</code></td>\n<td>Objects with highest <code>inuse_space</code> retained over time</td>\n</tr>\n<tr>\n<td><strong>Goroutine Leak</strong></td>\n<td>Goroutine dump</td>\n<td><code>curl http://localhost:6060/debug/pprof/goroutine?debug=2</code></td>\n<td>Goroutines stuck in <code>chan send</code> or <code>chan receive</code></td>\n</tr>\n<tr>\n<td><strong>Slow Disk I/O</strong></td>\n<td>Disk monitoring</td>\n<td><code>iostat -x 1</code> (Linux)</td>\n<td>High <code>await</code> or <code>%util</code> on data disk</td>\n</tr>\n<tr>\n<td><strong>Network Issues</strong></td>\n<td>Packet capture</td>\n<td><code>tcpdump -i any port 9092 -w capture.pcap</code></td>\n<td>Retransmissions, zero windows, RST packets</td>\n</tr>\n<tr>\n<td><strong>Deadlock</strong></td>\n<td>Mutex profiling</td>\n<td><code>go tool pprof http://localhost:6060/debug/pprof/mutex</code></td>\n<td>Mutexes with high contention times</td>\n</tr>\n<tr>\n<td><strong>GC Pressure</strong></td>\n<td>GC trace</td>\n<td><code>GODEBUG=gctrace=1 ./broker</code></td>\n<td>High <code>STW</code> (stop-the-world) pause times</td>\n</tr>\n</tbody></table>\n<h4 id=\"g-milestone-checkpoint-debugging-verification\">G. Milestone Checkpoint: Debugging Verification</h4>\n<p>After implementing logging and debugging infrastructure, verify it works:</p>\n<ol>\n<li><strong>Start your broker</strong> with debug logging enabled:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   LOG_LEVEL=debug ./broker --port 9092 --data-dir ./data</code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Check debug endpoints</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   curl http://localhost:6060/debug/state | jq .\n   curl http://localhost:6060/debug/pprof/goroutine?debug=1 | head -50</code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Produce test messages</strong> and verify logs show the flow:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   ./producer-cli --topic test --message &quot;debug test&quot;\n   # Check broker logs for: &quot;Appended records&quot;, &quot;Handled produce request&quot;</code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Simulate a failure</strong> and verify error logging:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   # In another terminal, kill the broker process\n   pkill -9 broker\n   # Check logs for: &quot;Received signal&quot;, &quot;Shutting down&quot;, &quot;Closing WAL&quot;</code></pre></div>\n\n<ol start=\"5\">\n<li><p><strong>Expected Output</strong>: </p>\n<ul>\n<li>Structured JSON logs for all major operations</li>\n<li>Debug endpoints return valid JSON with broker state</li>\n<li>Errors include sufficient context for diagnosis</li>\n<li>No panics or data races (run with <code>-race</code> flag)</li>\n</ul>\n</li>\n<li><p><strong>Signs of Problems</strong>:</p>\n<ul>\n<li>Logs are missing key fields (partition, offset, correlation ID)</li>\n<li>Debug endpoints timeout or return incomplete data</li>\n<li>Memory usage grows continuously during idle period</li>\n<li>Goroutine count increases without bound</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"13-future-extensions\">13. Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All (post-core extension opportunities)</p>\n</blockquote>\n<p>Having successfully implemented the core distributed message queue with partitioned topics, producer batching, consumer groups, and leader-follower replication, you now possess a solid foundation in distributed systems principles. This section explores potential advanced features you could implement to deepen your understanding and extend the system&#39;s capabilities. Each extension represents real-world challenges faced by production message brokers like Apache Kafka, and implementing them will sharpen your skills in performance optimization, consistency guarantees, and resource management.</p>\n<h3 id=\"131-potential-feature-additions\">13.1 Potential Feature Additions</h3>\n<p>The core system you&#39;ve built provides reliable, ordered message delivery with basic fault tolerance. However, production systems require additional features to handle diverse workloads, optimize resource usage, and guarantee stronger semantics. Below are several advanced extensions, each with a mental model to build intuition before technical details.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Mental Model</th>\n<th>Core Benefit</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Compression</strong></td>\n<td>A vacuum-sealed package: multiple items compressed into a smaller space for efficient shipping, then decompressed at destination.</td>\n<td>Reduces network bandwidth and disk storage by 60-90% for text-based messages.</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Exactly-Once Semantics</strong></td>\n<td>A bank&#39;s transaction ledger with debit/credit pairs: each operation is atomic and idempotent, ensuring final balance reflects exactly one execution of each transfer.</td>\n<td>Eliminates duplicate processing in stream processing pipelines, critical for financial use cases.</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Quotas &amp; Throttling</strong></td>\n<td>A highway toll system with speed limits and vehicle quotas: controls throughput per client to prevent any single user from overwhelming shared infrastructure.</td>\n<td>Protects system stability from misbehaving clients and enables multi-tenancy.</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Log Compaction</strong></td>\n<td>A key-value store&#39;s &quot;last value wins&quot; semantics: older updates for the same key are periodically discarded, retaining only the latest state.</td>\n<td>Enables using the log as a durable, replayable database of current state rather than infinite history.</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Custom RPC Layer</strong></td>\n<td>Replacing postal mail with a dedicated courier service: designing your own protocol optimized for your specific communication patterns.</td>\n<td>Reduces serialization overhead, enables zero-copy transfers, and provides finer control over connection management.</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Tiered Storage</strong></td>\n<td>A library archive: frequently accessed recent books stay on main shelves, while older volumes move to deep storage, retrievable when needed.</td>\n<td>Dramatically reduces storage costs for long-retention topics while maintaining access to historical data.</td>\n<td>Very High</td>\n</tr>\n<tr>\n<td><strong>Transaction Support</strong></td>\n<td>A database&#39;s ACID transactions across multiple partitions: all writes within a transaction are committed or aborted together.</td>\n<td>Enconsistent updates across multiple partitions, crucial for event-driven microservices.</td>\n<td>Very High</td>\n</tr>\n<tr>\n<td><strong>Schema Registry</strong></td>\n<td>A central dictionary for data formats: producers and consumers agree on message structure via shared schema definitions, enabling evolution.</td>\n<td>Prevents data corruption from format mismatches and enables efficient binary serialization.</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h4 id=\"compression-the-space-saving-courier\">Compression: The Space-Saving Courier</h4>\n<p>Imagine you run a courier service shipping books between libraries. Instead of sending each book individually, you pack multiple books into a single box, vacuum-seal it to reduce volume, then label the box with its contents. At the destination, the box is opened and books returned to normal size. <strong>Compression</strong> works similarly: multiple records in a <code>RecordBatch</code> are compressed together using algorithms like GZIP, Snappy, or LZ4 before transmission and storage, then decompressed when consumers fetch them.</p>\n<p>The key insight is that compression operates at the <strong>batch level</strong>, not individual records, because compression algorithms achieve better ratios with more data. Your existing <code>RecordBatch</code> structure already has an <code>Attributes</code> field with bits for compression type—this extension involves implementing the actual compression/decompression logic in the <code>RecordBatch.Encode()</code> and <code>DecodeRecordBatch()</code> methods, plus configuring the <code>Producer</code> to compress batches meeting a minimum size threshold.</p>\n<h4 id=\"exactly-once-semantics-the-bank-teller39s-ledger\">Exactly-Once Semantics: The Bank Teller&#39;s Ledger</h4>\n<p>Consider a bank teller processing deposits. Each transaction gets a unique sequence number in the ledger. If the network fails after the deposit but before the customer receives confirmation, the customer might retry—but the teller checks the sequence number and ignores duplicates. <strong>Exactly-once semantics</strong> extends your idempotent producer (which prevents duplicates within a single partition) to span multiple partitions and consumer side effects via <strong>transactional producers</strong> and <strong>read-committed isolation</strong>.</p>\n<p>This requires three coordinated mechanisms: 1) <strong>Transactional IDs</strong> for producer fencing, 2) <strong>Transaction coordinator</strong> (a specialized broker role) managing two-phase commit across partitions, and 3) <strong>Transaction markers</strong> written to logs to indicate commit/abort boundaries. Consumers in <code>read_committed</code> mode only deliver messages after the commit marker, avoiding exposure to uncommitted data.</p>\n<h4 id=\"quotas-amp-throttling-the-highway-traffic-control\">Quotas &amp; Throttling: The Highway Traffic Control</h4>\n<p>A highway system uses toll booths, speed limits, and vehicle quotas to ensure no single route becomes congested. <strong>Quotas</strong> in messaging systems similarly limit the byte rate or request rate per client, user, or topic to prevent a single misconfigured producer from overwhelming broker network I/O or a greedy consumer from monopolizing CPU. Throttling involves measuring traffic, comparing against quotas, and delaying excess requests or adding backpressure.</p>\n<p>Your system already has natural measurement points: <code>Server.HandleProduce()</code> and <code>Server.HandleFetch()</code> can track bytes per client ID. The challenge is implementing fair, responsive throttling without introducing substantial overhead. A token bucket algorithm per client, checked before processing each request, provides smooth rate limiting.</p>\n<h4 id=\"log-compaction-the-librarian39s-archive-purge\">Log Compaction: The Librarian&#39;s Archive Purge</h4>\n<p>Imagine a librarian periodically scanning shelves, removing all but the latest edition of each book title. <strong>Log compaction</strong> does this for keyed messages: it periodically rewrites log segments, discarding older records for keys that have more recent updates, while retaining all records for keys without updates (including null-keyed records). This transforms the log from an infinite append-only history into a finite, replayable key-value store.</p>\n<p>Compaction requires background threads scanning <code>LogSegment</code> files, creating new compacted segments, and atomically swapping them. The <code>Log</code> needs to track the <strong>clean offset</strong>—the point before which all keys are guaranteed to be compacted. Consumers can then optionally read from this offset to get the latest state of all keys.</p>\n<h4 id=\"custom-rpc-layer-building-your-own-postal-service\">Custom RPC Layer: Building Your Own Postal Service</h4>\n<p>While you&#39;ve implemented a basic length-prefixed TCP protocol, production systems often optimize further with custom binary protocols supporting multiplexing, header compression, and zero-copy transfers. Designing a <strong>custom RPC layer</strong> involves creating a wire format tailored to your specific request/response patterns, potentially using frameworks like gRPC or building directly on epoll/kqueue for high throughput.</p>\n<p>This extension would replace your current <code>TCPServer</code> and request handling with a more sophisticated transport that better leverages system calls, reduces allocations, and provides better connection pooling. It&#39;s an excellent deep dive into network programming and performance optimization.</p>\n<h3 id=\"132-design-considerations-for-extensions\">13.2 Design Considerations for Extensions</h3>\n<p>Each extension presents unique design challenges and requires modifications to your existing architecture. Below we analyze each feature&#39;s impact through the lens of Architecture Decision Records (ADRs), comparing implementation approaches and highlighting integration points with your current components.</p>\n<h4 id=\"1321-compression-batch-level-vs-record-level\">13.2.1 Compression: Batch-Level vs Record-Level</h4>\n<blockquote>\n<p><strong>Decision: Implement Compression at RecordBatch Level</strong></p>\n<ul>\n<li><strong>Context</strong>: Messages often have high redundancy (JSON fields, repeated text), especially within related records produced together. We need to reduce network and storage overhead without excessive CPU cost.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Record-level compression</strong>: Each <code>Record</code> compressed individually.</li>\n<li><strong>Batch-level compression</strong>: Entire <code>RecordBatch</code> compressed as a unit.</li>\n<li><strong>Streaming compression</strong>: Continuous compression across batch boundaries.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement batch-level compression using the existing <code>RecordBatch.Attributes</code> field to indicate compression type.</li>\n<li><strong>Rationale</strong>: Batch-level achieves better compression ratios (more data for dictionary-based algorithms) and aligns with the natural unit of network transfer and disk I/O. It also simplifies implementation: compression/decompression happens at the same boundaries as existing serialization.</li>\n<li><strong>Consequences</strong>: Producers must accumulate enough records to make compression worthwhile; smaller batches gain less benefit. Consumers must handle decompression transparently. CPU overhead concentrated at producer and consumer rather than brokers.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Suitable For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Record-level</strong></td>\n<td>Fine-grained, no batching delay</td>\n<td>Poor compression ratio, high overhead</td>\n<td>Large individual messages (&gt;1MB)</td>\n</tr>\n<tr>\n<td><strong>Batch-level</strong></td>\n<td>Good ratio, aligns with existing unit</td>\n<td>Requires minimum batch size</td>\n<td>Typical throughput-optimized workloads</td>\n</tr>\n<tr>\n<td><strong>Streaming</strong></td>\n<td>Best ratio across many batches</td>\n<td>Complex state management, harder random access</td>\n<td>Very high throughput archival</td>\n</tr>\n</tbody></table>\n<p><strong>Integration with Current Design</strong>:</p>\n<ul>\n<li><strong>Producer</strong>: The <code>Sender.sendBatch()</code> method would compress the serialized batch bytes if <code>batch.Attributes</code> indicates compression and batch size exceeds <code>compression.threshold</code>.</li>\n<li><strong>Broker</strong>: The <code>Log.Append()</code> stores compressed bytes directly; brokers don&#39;t decompress except for validation. The <code>Log.Read()</code> returns compressed batches to consumers.</li>\n<li><strong>Consumer</strong>: <code>DecodeRecordBatch()</code> detects compression via <code>Attributes</code> and decompresses before parsing individual records.</li>\n<li><strong>Configuration</strong>: Add <code>CompressionType</code> and <code>CompressionThreshold</code> to <code>ProducerConfig</code>.</li>\n</ul>\n<p>The <code>RecordBatch.Encode()</code> algorithm extends to:</p>\n<ol>\n<li>Serialize records to temporary buffer</li>\n<li>If compression enabled and buffer size &gt; threshold:<ul>\n<li>Compress buffer using selected algorithm</li>\n<li>Set compression bits in <code>Attributes</code></li>\n</ul>\n</li>\n<li>Calculate CRC on compressed data (if using)</li>\n<li>Write batch header followed by compressed data</li>\n</ol>\n<h4 id=\"1322-exactly-once-semantics-transaction-coordinator-placement\">13.2.2 Exactly-Once Semantics: Transaction Coordinator Placement</h4>\n<blockquote>\n<p><strong>Decision: Embed Transaction Coordinator in Existing Brokers</strong></p>\n<ul>\n<li><strong>Context</strong>: Transactions require a coordinator to manage two-phase commit across multiple partitions, which may be hosted on different brokers. We need to decide where this coordinator runs.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Dedicated coordinator broker</strong>: Single broker elected as transaction coordinator.</li>\n<li><strong>Embedded in all brokers</strong>: Each broker can coordinate transactions for producers that choose it.</li>\n<li><strong>External service</strong>: Separate process/container managing transactions.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Embed transaction coordination capability in all brokers, with producers hashing transactional ID to select a coordinator.</li>\n<li><strong>Rationale</strong>: This follows Kafka&#39;s design, ensuring scalability (coordination load distributes across cluster) and fault tolerance (coordinator failure only affects its transactions). It leverages existing broker infrastructure for persistence and networking.</li>\n<li><strong>Consequences</strong>: Each broker needs additional transaction state; producer must discover coordinator via hash; transaction recovery requires scanning logs.</li>\n</ul>\n</blockquote>\n<p><strong>Implementation Outline</strong>:</p>\n<ol>\n<li><strong>Transaction Coordinator Component</strong>: New <code>TransactionCoordinator</code> type in each broker, managing <code>TransactionalId → ProducerIdMapping</code> and <code>TransactionLog</code>.</li>\n<li><strong>Transaction Protocol</strong>: <ul>\n<li><code>InitProducerId(TransactionalId)</code> → returns <code>ProducerId, Epoch</code> (for fencing)</li>\n<li><code>AddPartitionsToTransaction(TxnId, [TopicPartition])</code></li>\n<li><code>EndTransaction(TxnId, CommitOrAbort)</code></li>\n</ul>\n</li>\n<li><strong>Transaction Log</strong>: Special internal topic <code>__transaction_state</code> storing transaction metadata.</li>\n<li><strong>Consumer Read Committed</strong>: Modify <code>Log.Read()</code> to skip messages between <code>BEGIN</code> and <code>COMMIT/ABORT</code> markers when <code>isolation.level=read_committed</code>.</li>\n</ol>\n<p>Your existing <code>Partition</code> log would need to support <strong>transaction markers</strong>—special control records written during two-phase commit. The <code>HighWatermark</code> advancement logic must consider that messages before an uncommitted transaction marker shouldn&#39;t be exposed to <code>read_committed</code> consumers.</p>\n<h4 id=\"1323-quotas-measurement-and-enforcement-points\">13.2.3 Quotas: Measurement and Enforcement Points</h4>\n<blockquote>\n<p><strong>Decision: Enforce Quotas at Request Handler with Token Bucket</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to limit client throughput to protect cluster stability. Must decide where to measure and how to enforce limits.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Network layer enforcement</strong>: Throttle at TCP accept/read level.</li>\n<li><strong>Request handler enforcement</strong>: Delay processing after parsing request but before business logic.</li>\n<li><strong>Response throttling</strong>: Allow processing but delay response.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement token bucket algorithm in <code>Server.HandleProduce()</code> and <code>Server.HandleFetch()</code> methods, tracking bytes per client ID.</li>\n<li><strong>Rationale</strong>: Request handler has access to structured request information (client ID, topic, byte count) and can make precise decisions. Token bucket provides smooth throttling (not all-or-nothing) and accumulates credits during idle periods.</li>\n<li><strong>Consequences</strong>: Adds per-request overhead for quota checking; delayed responses may cause client timeouts.</li>\n</ul>\n</blockquote>\n<p><strong>Integration Points</strong>:</p>\n<ul>\n<li><strong>Quota Config</strong>: Add to <code>Server</code> config: <code>quota.producer.byte.rate</code>, <code>quota.consumer.byte.rate</code> (bytes/sec per client).</li>\n<li><strong>Quota Manager</strong>: New <code>QuotaManager</code> with <code>CheckQuota(clientID string, bytes int) time.Duration</code> returning delay needed.</li>\n<li><strong>Request Handling</strong>:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// In Server.HandleProduce</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">delay </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.quotaManager.</span><span style=\"color:#B392F0\">CheckQuota</span><span style=\"color:#E1E4E8\">(clientID, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(request.Data))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> delay </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(delay) </span><span style=\"color:#6A737D\">// Or implement asynchronous delaying</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n<ul>\n<li><strong>Metrics</strong>: Track throttling time per client for observability.</li>\n</ul>\n<p>A key challenge is <strong>quota distribution across broker cluster</strong>—if a client connects to multiple brokers, each sees only partial traffic. A simplified approach enforces quotas per-broker, which works for clients with stable broker connections.</p>\n<h4 id=\"1324-log-compaction-background-vs-online-compaction\">13.2.4 Log Compaction: Background vs Online Compaction</h4>\n<blockquote>\n<p><strong>Decision: Implement Background Compaction Thread per Log</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to periodically clean logs while maintaining availability for reads and writes.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Background thread</strong>: Separate goroutine periodically scans and rewrites segments.</li>\n<li><strong>Online compaction</strong>: Compact during normal writes (append-only with periodic merging).</li>\n<li><strong>Offline compaction</strong>: Stop serving partition, compact entire log, resume.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Background compaction thread per <code>Log</code> that runs when segments meet compaction criteria.</li>\n<li><strong>Rationale</strong>: Background compaction minimizes impact on read/write latency. It can be scheduled during low-load periods. The algorithm mirrors Kafka&#39;s log cleaner.</li>\n<li><strong>Consequences</strong>: Increases disk I/O during compaction; requires careful coordination to avoid compacting active segments.</li>\n</ul>\n</blockquote>\n<p><strong>Compaction Algorithm</strong>:</p>\n<ol>\n<li><strong>Select Segments</strong>: Choose log segments where <code>dirtyRatio = (segment.size - clean.size) / segment.size &gt; minDirtyRatio</code>.</li>\n<li><strong>Build Key Map</strong>: Read selected segments, track latest offset per key.</li>\n<li><strong>Write Clean Segment</strong>: Create new segment containing only latest records for each key (plus all records without keys).</li>\n<li><strong>Atomically Replace</strong>: Swap old segments with new clean segment in <code>Log.Segments</code>.</li>\n<li><strong>Update Clean Offset</strong>: Record offset before which all keys are compacted.</li>\n</ol>\n<p>Your <code>Log</code> would need new fields:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Log</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... existing fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CleanOffset      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">        // Offset before which compaction is guaranteed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompactConfig    </span><span style=\"color:#B392F0\">CompactConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    compacting       </span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">         // Guard against concurrent compaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Consumers reading with <code>isolation.level=read_committed</code> and from <code>clean.offset</code> would see a consistent snapshot of the latest value for each key—effectively turning your message queue into a persistent key-value store.</p>\n<h4 id=\"1325-custom-rpc-layer-protocol-design-trade-offs\">13.2.5 Custom RPC Layer: Protocol Design Trade-offs</h4>\n<blockquote>\n<p><strong>Decision: Implement Connection Multiplexing with Request Batching</strong></p>\n<ul>\n<li><strong>Context</strong>: Current simple TCP server handles one request per connection serially, limiting throughput.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>HTTP/2 with gRPC</strong>: Use standard multiplexed protocol.</li>\n<li><strong>Custom binary with framing</strong>: Design own length-prefixed multiplexed protocol.</li>\n<li><strong>UDP-based protocol</strong>: For lower latency but lossy transport.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Design custom binary protocol supporting request pipelining and connection multiplexing.</li>\n<li><strong>Rationale</strong>: Custom protocol allows optimization for specific patterns (small metadata requests mixed with large data transfers). Avoids HTTP overhead while maintaining reliability via TCP.</li>\n<li><strong>Consequences</strong>: Significant implementation effort; must handle backpressure, timeouts, and connection lifecycle.</li>\n</ul>\n</blockquote>\n<p><strong>Protocol Enhancements</strong>:</p>\n<ol>\n<li><strong>Multiplexing</strong>: Single connection carries multiple concurrent request/response streams with correlation IDs.</li>\n<li><strong>Zero-copy for large fetches</strong>: Use <code>sendfile()</code> or similar to transfer log segments without copying to userspace.</li>\n<li><strong>Header compression</strong>: Repeated headers (topic names, client IDs) compressed across requests.</li>\n<li><strong>Binary encoding</strong>: Replace current ad-hoc encoding with Protocol Buffers or similar for schema evolution.</li>\n</ol>\n<p>This extension would essentially replace your entire <code>network</code> package with a more sophisticated implementation, touching almost every component. It&#39;s the most invasive but educational extension for understanding high-performance network services.</p>\n<h4 id=\"1326-tiered-storage-hotwarmcold-architecture\">13.2.6 Tiered Storage: Hot/Warm/Cold Architecture</h4>\n<blockquote>\n<p><strong>Decision: Implement Transparent Tiering with Async Offload</strong></p>\n<ul>\n<li><strong>Context</strong>: Logs grow indefinitely, consuming expensive local SSD storage. Need to move older data to cheaper object storage (S3-like).</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Manual tiering</strong>: Users manually move segments between tiers.</li>\n<li><strong>Transparent offload</strong>: System automatically moves segments based on age.</li>\n<li><strong>Hierarchical storage</strong>: OS-level tiering (e.g., LVM cache).</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement background offload of closed log segments to object storage, with local LRU cache for recently accessed segments.</li>\n<li><strong>Rationale</strong>: Transparent operation requires no application changes. Async offload minimizes performance impact. Object storage provides durability and virtually unlimited capacity.</li>\n<li><strong>Consequences</strong>: Adds complexity for fetch path (check local, then remote); requires object storage integration; network latency for cold reads.</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Impact</strong>:</p>\n<ul>\n<li><strong>LogSegment</strong>: Add <code>StorageTier</code> field (<code>LOCAL</code>, <code>REMOTE</code>, <code>ARCHIVED</code>) and <code>RemoteURI</code>.</li>\n<li><strong>Log.Read()</strong>: If segment is remote, fetch to local cache (possibly with prefetch).</li>\n<li><strong>TierManager</strong>: Background goroutine scanning for segments older than <code>retention.ms</code> to offload.</li>\n<li><strong>Configuration</strong>: Add <code>tiered.storage.enabled</code>, <code>remote.storage.endpoint</code>, <code>local.cache.size</code>.</li>\n</ul>\n<p>This extension transforms your broker from a storage node to a <strong>caching layer</strong> over durable object storage—a common pattern in modern data systems like Kafka with Tiered Storage.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compression</td>\n<td>Standard library <code>compress/gzip</code></td>\n<td>C bindings to Snappy or LZ4 via cgo</td>\n</tr>\n<tr>\n<td>Transaction Coordinator</td>\n<td>In-memory state with WAL for recovery</td>\n<td>Full Paxos/Raft for coordinator fault tolerance</td>\n</tr>\n<tr>\n<td>Quota Enforcement</td>\n<td>Simple token bucket per client</td>\n<td>Distributed quota tracking via gossip protocol</td>\n</tr>\n<tr>\n<td>Log Compaction</td>\n<td>Single-pass scanner building map in memory</td>\n<td>Multi-pass with disk-based hash tables for large logs</td>\n</tr>\n<tr>\n<td>Custom RPC</td>\n<td>Connection-per-request with pipelining</td>\n<td>Reactor pattern with epoll/kqueue event loop</td>\n</tr>\n<tr>\n<td>Tiered Storage</td>\n<td>Mock object storage (local filesystem)</td>\n<td>Integration with MinIO or AWS S3 SDK</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure-for-extensions\">Recommended Module Structure for Extensions</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    compression/           # Compression algorithms\n      codec.go            # Interface: Compress(data []byte) []byte\n      gzip.go             # GZIP implementation\n      snappy.go           # Snappy implementation (optional)\n    transaction/\n      coordinator.go      # TransactionCoordinator\n      log.go              # Transaction log (internal topic)\n      producer_id.go      # Producer ID generation and fencing\n    quotas/\n      manager.go          # QuotaManager with token buckets\n      metrics.go          # Throttling metrics collection\n    compaction/\n      cleaner.go          # LogCleaner background thread\n      strategy.go         # Compaction strategy interface\n    storage/\n      tier/               # Tiered storage\n        manager.go        # TierManager for offloading\n        cache.go          # LRU cache for remote segments\n        s3.go             # S3 client (optional)\n    protocol/             # Enhanced RPC layer\n      framing.go          # Multiplexed frame encoding\n      connection.go       # Managed connection with pipelining\n      reactor.go          # Event loop (advanced)</code></pre></div>\n\n<h4 id=\"compression-starter-code\">Compression Starter Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/compression/codec.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> compression</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Codec</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Compress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">src</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Decompress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">src</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// internal/compression/gzip.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> compression</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">compress/gzip</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GzipCodec</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GzipCodec</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Compress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">src</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> buf </span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> gzip.</span><span style=\"color:#B392F0\">NewWriter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">buf)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(src); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> buf.</span><span style=\"color:#B392F0\">Bytes</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GzipCodec</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Decompress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">src</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> gzip.</span><span style=\"color:#B392F0\">NewReader</span><span style=\"color:#E1E4E8\">(bytes.</span><span style=\"color:#B392F0\">NewReader</span><span style=\"color:#E1E4E8\">(src))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> r.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> buf </span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> buf.</span><span style=\"color:#B392F0\">ReadFrom</span><span style=\"color:#E1E4E8\">(r); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> buf.</span><span style=\"color:#B392F0\">Bytes</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GzipCodec</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"gzip\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// In RecordBatch.Encode() modification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecordBatch</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 1. Serialize records to buffer (as before)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 2. Check if compression configured</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> b.Attributes </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> CompressionMask </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> CompressionNone {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // 3. Get appropriate codec</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        codec </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> compression.</span><span style=\"color:#B392F0\">GetCodec</span><span style=\"color:#E1E4E8\">(b.Attributes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        compressed, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> codec.</span><span style=\"color:#B392F0\">Compress</span><span style=\"color:#E1E4E8\">(buffer.</span><span style=\"color:#B392F0\">Bytes</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // 4. Update buffer with compressed data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buffer.</span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buffer.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(compressed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 5. Continue with CRC and final encoding</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"transaction-coordinator-skeleton\">Transaction Coordinator Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/transaction/coordinator.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transaction</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TransactionCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    brokerID      </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logManager    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pendingTxns   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TransactionMetadata</span><span style=\"color:#6A737D\"> // key: transactionalID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    producerState </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ProducerState</span><span style=\"color:#6A737D\">       // key: transactionalID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TransactionMetadata</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TransactionalID    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProducerID         </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProducerEpoch      </span><span style=\"color:#F97583\">int16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    State              </span><span style=\"color:#B392F0\">TransactionState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Partitions         []</span><span style=\"color:#B392F0\">TopicPartition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeoutMs          </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastUpdateTime     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleInitProducerId processes request for new producer ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TransactionCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleInitProducerId</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    transactionalID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    timeoutMs</span><span style=\"color:#F97583\"> int32</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#FFAB70\">producerID</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">producerEpoch</span><span style=\"color:#F97583\"> int16</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if transactionalID already exists in producerState</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If exists, validate epoch for fencing (reject if old epoch)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Generate new producerID (monotonically increasing)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Initialize new transaction metadata with state EMPTY</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Write to transaction log for durability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return producerID and epoch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleAddPartitions processes request to add partitions to transaction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TransactionCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleAddPartitions</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    transactionalID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    partitions</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">TopicPartition</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up transaction metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate producer epoch matches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure transaction is in ONGOING state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add partitions to metadata (deduplicate)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Write updated metadata to transaction log</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleEndTransaction processes commit/abort request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TransactionCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleEndTransaction</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    transactionalID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    commit</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up transaction metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate producer epoch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Prepare PREPARE_COMMIT or PREPARE_ABORT marker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write PREPARE marker to transaction log (fsync)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: For each partition in transaction:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Write transaction marker (COMMIT/ABORT) to partition log</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: After all markers written, write COMPLETE to transaction log</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Clean up transaction state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints-for-go\">Language-Specific Hints for Go</h4>\n<ul>\n<li><strong>Compression</strong>: Use <code>compress/gzip</code> for simplicity, but consider <code>github.com/golang/snappy</code> for better speed if needed.</li>\n<li><strong>Concurrent Compaction</strong>: Use <code>sync.RWMutex</code> in <code>Log</code> with <code>TryLock()</code> to skip compaction if log is busy.</li>\n<li><strong>Token Bucket Quotas</strong>: Implement with <code>time.Ticker</code> refilling tokens and <code>sync/atomic</code> for token count.</li>\n<li><strong>Transaction Recovery</strong>: On coordinator startup, scan transaction log to rebuild in-memory state of incomplete transactions.</li>\n<li><strong>Tiered Storage</strong>: Use <code>io.ReaderAt</code> interface for random access to remote objects; implement LRU cache with <code>container/list</code> and <code>map</code>.</li>\n</ul>\n<h4 id=\"milestone-checkpoint-for-compression-extension\">Milestone Checkpoint for Compression Extension</h4>\n<p><strong>Verification Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run compression integration test</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> project-root</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/compression/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestCompressionRatio</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># === RUN   TestCompressionRatio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     compression_test.go:45: GZIP ratio: 85% (1500 bytes → 225 bytes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># --- PASS: TestCompressionRatio (0.02s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 1. Start broker with compression enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 2. Use producer with compression.type=gzip</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 3. Produce 1000 messages with repetitive JSON content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 4. Check disk usage in log directory - should be ~15% of uncompressed size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 5. Consume messages - should decompress transparently</span></span></code></pre></div>\n\n<p><strong>Debugging Tips for Compression</strong>:</p>\n<ul>\n<li><p><strong>Symptom</strong>: Consumer receives garbled data or fails to decode.</p>\n<ul>\n<li><strong>Cause</strong>: Compression attributes mismatched between producer and consumer.</li>\n<li><strong>Diagnosis</strong>: Check <code>RecordBatch.Attributes</code> bits in stored log segment (hex dump).</li>\n<li><strong>Fix</strong>: Ensure <code>DecodeRecordBatch()</code> reads compression bits and uses correct decompressor.</li>\n</ul>\n</li>\n<li><p><strong>Symptom</strong>: High CPU usage on producer.</p>\n<ul>\n<li><strong>Cause</strong>: Compressing very small batches (worse than no compression).</li>\n<li><strong>Diagnosis</strong>: Log batch sizes before compression.</li>\n<li><strong>Fix</strong>: Increase <code>batch.size</code> or set <code>compression.threshold</code> higher.</li>\n</ul>\n</li>\n</ul>\n<p>Each extension represents a significant engineering challenge that will deepen your understanding of distributed systems trade-offs. Start with compression (easiest) to gain confidence, then progress to more complex features like transactions. Remember that production systems evolve incrementally—each feature builds upon a solid foundation, which you now possess.</p>\n<h2 id=\"14-glossary\">14. Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1, 2, 3, 4 (all foundational concepts)</p>\n</blockquote>\n<p>This glossary defines the key terms, acronyms, and domain-specific vocabulary used throughout this design document. Building intuition for these concepts is essential for understanding the architecture and implementation of a distributed message queue.</p>\n<h3 id=\"141-terms-and-definitions\">14.1 Terms and Definitions</h3>\n<p>The following table provides an alphabetical reference of technical terms used in this project.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Accumulator</strong></td>\n<td>A producer component that temporarily batches multiple records by their destination <code>TopicPartition</code> before sending them as a single network request. It improves throughput by amortizing the overhead of network round-trips and serialization.</td>\n</tr>\n<tr>\n<td><strong>Acks (Acknowledgments)</strong></td>\n<td>The producer&#39;s configured durability guarantee, represented by <code>AcksLevel</code>. <code>AcksNone</code> (0) means fire-and-forget; <code>AcksLeader</code> (1) waits for the partition leader to persist the record; <code>AcksAll</code> (-1) waits for all in-sync replicas (ISR) to acknowledge.</td>\n</tr>\n<tr>\n<td><strong>API Key</strong></td>\n<td>A numeric identifier in the wire protocol&#39;s <code>MessageHeader</code> that specifies the type of request (e.g., <code>ProduceRequest</code>, <code>FetchRequest</code>, <code>JoinGroupRequest</code>).</td>\n</tr>\n<tr>\n<td><strong>BaseOffset</strong></td>\n<td>The offset of the first record in a <code>LogSegment</code>. All offsets within the segment are relative to this base.</td>\n</tr>\n<tr>\n<td><strong>Broker</strong></td>\n<td>A server node in the cluster that stores partition replicas and handles client requests. A broker hosts a subset of partitions for various topics, acting as both a storage layer and a network endpoint.</td>\n</tr>\n<tr>\n<td><strong>Compaction</strong></td>\n<td>See <strong>Log Compaction</strong>.</td>\n</tr>\n<tr>\n<td><strong>Consumer</strong></td>\n<td>A client application that subscribes to and reads records from topics. Consumers belong to a <code>ConsumerGroup</code> to coordinate parallel consumption.</td>\n</tr>\n<tr>\n<td><strong>Consumer Group</strong></td>\n<td>A set of consumers that cooperate to consume a topic. The group coordinator assigns each partition of a subscribed topic to exactly one member of the group, enabling parallel consumption while preserving ordering guarantees within each partition.</td>\n</tr>\n<tr>\n<td><strong>Control Plane</strong></td>\n<td>The network path dedicated to coordination operations, such as consumer group management (<code>JoinGroup</code>, <code>SyncGroup</code>, <code>Heartbeat</code>), metadata propagation, and leadership elections. This is distinct from the <strong>Data Plane</strong> which carries the actual message flow.</td>\n</tr>\n<tr>\n<td><strong>Controller</strong></td>\n<td>A designated broker (with the lowest or elected ID) that manages partition leadership elections and replica assignments for the entire cluster. It acts as the centralized decision-maker for partition state changes.</td>\n</tr>\n<tr>\n<td><strong>Coordination Service</strong></td>\n<td>A service (like the in-memory <code>MemoryCoordinator</code> or an external system like ZooKeeper) responsible for maintaining and distributing cluster metadata (brokers, topics, partition leadership) and enabling consensus for operations like leader election.</td>\n</tr>\n<tr>\n<td><strong>Correlation ID</strong></td>\n<td>A client-generated integer included in every request&#39;s <code>MessageHeader</code>. The server echoes it back in the corresponding response, allowing the client to match asynchronous requests with their responses.</td>\n</tr>\n<tr>\n<td><strong>Data Plane</strong></td>\n<td>The network path dedicated to the actual flow of messages, including produce requests (writes) and fetch requests (reads). This is distinct from the <strong>Control Plane</strong> used for coordination.</td>\n</tr>\n<tr>\n<td><strong>Exactly-once Semantics</strong></td>\n<td>A delivery guarantee where each message is processed exactly once, even in the face of producer retries, consumer restarts, or broker failures. This typically requires idempotent producers and transactional commits.</td>\n</tr>\n<tr>\n<td><strong>Follower Syncer</strong></td>\n<td>A component running on a broker that continuously fetches new records from a partition leader (via <code>FetchReplica</code> requests) to keep its local replica synchronized. Each follower maintains its own <code>fetchOffset</code>.</td>\n</tr>\n<tr>\n<td><strong>Generation ID</strong></td>\n<td>A monotonically increasing number (<code>GenerationID</code> in <code>ConsumerGroup</code>) that identifies the current epoch of a consumer group. It is incremented on each successful rebalance and used to fence out stale members from previous generations.</td>\n</tr>\n<tr>\n<td><strong>Group Coordinator</strong></td>\n<td>A specific broker (elected per consumer group) that manages consumer group membership, heartbeats, partition assignment, and offset commits for that group. It runs the group membership protocol.</td>\n</tr>\n<tr>\n<td><strong>High Watermark</strong></td>\n<td>The offset of the last record that has been successfully replicated to all In-Sync Replicas (ISR). Consumers can only read up to the high watermark; records beyond it are considered &quot;uncommitted&quot; and could be lost if the leader fails.</td>\n</tr>\n<tr>\n<td><strong>Idempotent Producer</strong></td>\n<td>A producer configured to use sequence numbers (<code>ProducerID</code>, <code>ProducerEpoch</code>, <code>BaseSequence</code>) per partition to detect and discard duplicate batches caused by retries, enabling at-least-once semantics without duplicates.</td>\n</tr>\n<tr>\n<td><strong>Incremental Fetch</strong></td>\n<td>An optimization in the <code>FetchRequest</code> protocol where consumers only list partitions that have changed since their last request, reducing the size of requests when most partitions have no new data.</td>\n</tr>\n<tr>\n<td><strong>Index (Sparse Index)</strong></td>\n<td>A file (<code>IndexFile</code>) associated with a log segment that maps some record offset deltas to their byte positions in the data file. It is &quot;sparse&quot; because it does not index every record, trading some precision for smaller size.</td>\n</tr>\n<tr>\n<td><strong>Internal Topic</strong></td>\n<td>A special topic used by the system for its own metadata storage. In this project, <code>__consumer_offsets</code> is an example used to persistently store consumer group offset commits.</td>\n</tr>\n<tr>\n<td><strong>ISR (In-Sync Replica)</strong></td>\n<td>The set of replicas (leader and followers) for a partition that are fully caught up with the leader, defined as replicas whose <code>lastFetchOffset</code> is within a configured lag threshold (<code>replica.lag.time.max.ms</code>) of the leader&#39;s log end offset.</td>\n</tr>\n<tr>\n<td><strong>ISR Manager</strong></td>\n<td>A component running on the partition leader that tracks the progress (<code>followerStatus</code>) of each follower replica and periodically evaluates which replicas belong in the ISR, removing those that have fallen too far behind.</td>\n</tr>\n<tr>\n<td><strong>ISR Shrinkage</strong></td>\n<td>The process of removing followers from the in-sync replica set when they fail to keep up with the leader&#39;s write rate (exceeding the <code>replica.lag.time.max.ms</code> threshold). Excessive shrinkage can reduce replication factor and durability.</td>\n</tr>\n<tr>\n<td><strong>Leader Epoch</strong></td>\n<td>A monotonically increasing number (<code>LeaderEpoch</code>) associated with each leadership term for a partition. It is used by followers to detect and recover from stale or duplicate data after a leader change, serving as a fencing mechanism.</td>\n</tr>\n<tr>\n<td><strong>Leader-Follower Replication</strong></td>\n<td>The primary replication model where each partition has one designated leader that handles all produce/fetch requests, and one or more followers that asynchronously replicate data from the leader.</td>\n</tr>\n<tr>\n<td><strong>Length-prefixed</strong></td>\n<td>A message format where the first field is an integer (typically 4 bytes) specifying the total size of the remaining message bytes. This allows the receiver to read the complete message without parsing its internal structure first.</td>\n</tr>\n<tr>\n<td><strong>Log Compaction</strong></td>\n<td>A background process (<code>LogCleaner</code>) that removes older records for the same key from a log, retaining only the latest value. This reduces storage usage for keyed topics where only the current state matters (e.g., database change logs).</td>\n</tr>\n<tr>\n<td><strong>Log End Offset (LEO)</strong></td>\n<td>The offset of the next message that will be appended to the log (i.e., one greater than the offset of the last written record). For followers, this is their local <code>fetchOffset</code>.</td>\n</tr>\n<tr>\n<td><strong>Metadata Coordinator</strong></td>\n<td>The component (often the same as the <strong>Controller</strong>) responsible for maintaining and distributing cluster metadata—broker registrations, topic configurations, partition leader assignments, and ISR sets.</td>\n</tr>\n<tr>\n<td><strong>Multiplexing</strong></td>\n<td>Carrying multiple logical request/response streams over a single physical TCP connection. In this system, a producer or consumer may have one connection to a broker but send many independent requests with different <code>CorrelationID</code>s.</td>\n</tr>\n<tr>\n<td><strong>Nullable String</strong></td>\n<td>A string encoding in the wire protocol where a length prefix of <code>-1</code> indicates a <code>null</code> value, while non-negative lengths indicate a valid string of that length. This is distinct from zero-length strings.</td>\n</tr>\n<tr>\n<td><strong>Offset</strong></td>\n<td>A monotonically increasing integer assigned to each record within a partition, representing its immutable position in the ordered sequence. Offsets start at 0 and are contiguous.</td>\n</tr>\n<tr>\n<td><strong>Offset Commit</strong></td>\n<td>The action by a consumer to persistently store its current read position (offset) for each assigned partition, typically to an internal topic (<code>__consumer_offsets</code>). This allows the consumer to resume from that point after a restart.</td>\n</tr>\n<tr>\n<td><strong>Partition</strong></td>\n<td>An ordered, immutable sequence of records that is a subset of a topic. Partitions enable horizontal scaling—different partitions of the same topic can be hosted on different brokers and consumed in parallel.</td>\n</tr>\n<tr>\n<td><strong>Partition Assignment</strong></td>\n<td>The mapping of partitions to consumers within a consumer group, determined by the group coordinator using a strategy like <strong>Range</strong> or <strong>RoundRobin</strong>. Each consumer receives an <code>Assignment</code> describing the partitions it should consume.</td>\n</tr>\n<tr>\n<td><strong>Partitioner</strong></td>\n<td>A component (<code>HashPartitioner</code>, <code>RoundRobinAssigner</code>) that selects the target partition for a record based on its key (if present) or a round-robin scheme (if key is <code>nil</code>). Ensures records with the same key go to the same partition.</td>\n</tr>\n<tr>\n<td><strong>Producer</strong></td>\n<td>A client that publishes (writes) records to topics. It batches records, selects target partitions, handles retries, and respects acknowledgment configurations.</td>\n</tr>\n<tr>\n<td><strong>Quota Enforcement</strong></td>\n<td>Limiting resource usage (e.g., bytes per second) per client using algorithms like the <strong>Token Bucket</strong> to prevent a single misbehaving client from overwhelming the broker.</td>\n</tr>\n<tr>\n<td><strong>Rebalance</strong></td>\n<td>The process of redistributing partitions among the members of a consumer group when membership changes (a consumer joins, leaves, or fails). During a rebalance, the group coordinator reassigns partitions and informs all members.</td>\n</tr>\n<tr>\n<td><strong>Rebalance Storm</strong></td>\n<td>A pathological condition where consumer group members frequently join and leave, triggering continuous rebalances and preventing the group from settling into a stable state. Often caused by misconfigured <code>session.timeout.ms</code> or network issues.</td>\n</tr>\n<tr>\n<td><strong>Record</strong></td>\n<td>The fundamental unit of data, consisting of a key, value, headers, and metadata (timestamp, attributes). Records are grouped into <strong>RecordBatch</strong>es for efficient storage and transmission.</td>\n</tr>\n<tr>\n<td><strong>Record Batch</strong></td>\n<td>A group of records written together as a single unit on disk and over the network. Batches include metadata like <code>BaseOffset</code>, <code>FirstTimestamp</code>, and <code>CRC</code> for integrity. They are the atomic unit of writing and replication.</td>\n</tr>\n<tr>\n<td><strong>Replication Plane</strong></td>\n<td>The network path dedicated to data replication between brokers (leader to follower via <code>FetchReplica</code> requests). This traffic is separate from client produce/fetch traffic to avoid interference.</td>\n</tr>\n<tr>\n<td><strong>Segment (Log Segment)</strong></td>\n<td>A physical file (<code>DataFile</code>) containing a contiguous range of log records, along with its accompanying index file (<code>IndexFile</code>). Segments are rolled when they reach a configured maximum size (<code>SegmentMaxBytes</code>).</td>\n</tr>\n<tr>\n<td><strong>Sender</strong></td>\n<td>A producer component that manages network connections to brokers, sends accumulated batches, handles retries with exponential backoff, and processes acknowledgment responses.</td>\n</tr>\n<tr>\n<td><strong>Session Timeout</strong></td>\n<td>The time (<code>session.timeout.ms</code>) after which the group coordinator considers a consumer dead if it hasn&#39;t received a heartbeat. This triggers a rebalance to reassign the dead consumer&#39;s partitions.</td>\n</tr>\n<tr>\n<td><strong>Sparse Index</strong></td>\n<td>See <strong>Index</strong>.</td>\n</tr>\n<tr>\n<td><strong>Tiered Storage</strong></td>\n<td>A hierarchical storage architecture where older log segments are moved from fast, expensive local storage (<code>StorageTierLocal</code>) to slower, cheaper remote storage (<code>StorageTierRemote</code>) like object storage, managed by a <code>TierManager</code>.</td>\n</tr>\n<tr>\n<td><strong>Token Bucket</strong></td>\n<td>An algorithm (<code>TokenBucket</code>) used for rate limiting (<strong>Quota Enforcement</strong>). Tokens are added at a fixed rate (<code>refillRate</code>) up to a <code>capacity</code>. Each request consumes tokens; if insufficient tokens are available, the request is delayed.</td>\n</tr>\n<tr>\n<td><strong>Topic</strong></td>\n<td>A named stream of records, divided into one or more partitions. Topics are the primary abstraction for categorization—producers write to a topic, and consumers subscribe to one or more topics.</td>\n</tr>\n<tr>\n<td><strong>Two-phase Commit</strong></td>\n<td>A distributed transaction protocol used by <code>TransactionCoordinator</code> to ensure atomic commit across multiple partitions. It involves a prepare phase (where participants vote) and a commit/abort phase.</td>\n</tr>\n<tr>\n<td><strong>Unclean Leader Election</strong></td>\n<td>Electing a leader from outside the current ISR (when <code>allow.unclean.leader.election</code> is enabled). This risks data loss because the new leader may not have all committed records, but it improves availability when the ISR is empty.</td>\n</tr>\n<tr>\n<td><strong>Watch Pattern</strong></td>\n<td>A callback-based notification mechanism where components can register interest (<code>WatchBrokers</code>, <code>WatchTopic</code>) in changes to cluster metadata. The coordination service invokes callbacks when the watched data changes.</td>\n</tr>\n<tr>\n<td><strong>Wire Protocol</strong></td>\n<td>The binary format for network communication between components (clients and brokers, brokers and brokers). It defines how requests and responses are serialized into bytes for transmission over TCP.</td>\n</tr>\n<tr>\n<td><strong>Zero-copy</strong></td>\n<td>A technique for data transfer where data is moved between buffers (e.g., from disk to network) without CPU copying, often using OS-level features like <code>sendfile</code>. This reduces CPU overhead for high-throughput systems.</td>\n</tr>\n<tr>\n<td><strong>Zombie Consumer</strong></td>\n<td>A consumer that is considered dead by the group coordinator (due to heartbeat timeout) but is still alive and processing messages. This can lead to duplicate processing if partitions are reassigned while the zombie is still active.</td>\n</tr>\n</tbody></table>\n<hr>\n","toc":[{"level":1,"text":"Build Your Own Kafka: Design Document","id":"build-your-own-kafka-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"1. Context and Problem Statement","id":"1-context-and-problem-statement"},{"level":3,"text":"1.1 The Centralized Ledger Analogy","id":"11-the-centralized-ledger-analogy"},{"level":3,"text":"1.2 The Scalable Event Streaming Problem","id":"12-the-scalable-event-streaming-problem"},{"level":3,"text":"1.3 Existing Approaches &amp; Trade-offs","id":"13-existing-approaches-amp-trade-offs"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"2. Goals and Non-Goals","id":"2-goals-and-non-goals"},{"level":3,"text":"2.1 Functional Goals","id":"21-functional-goals"},{"level":3,"text":"2.2 Non-Functional Goals","id":"22-non-functional-goals"},{"level":3,"text":"2.3 Explicit Non-Goals","id":"23-explicit-non-goals"},{"level":2,"text":"3. High-Level Architecture","id":"3-high-level-architecture"},{"level":3,"text":"3.1 Component Overview &amp; Responsibilities","id":"31-component-overview-amp-responsibilities"},{"level":4,"text":"3.1.1 Core Components","id":"311-core-components"},{"level":4,"text":"3.1.2 Component Interactions","id":"312-component-interactions"},{"level":4,"text":"3.1.3 Deployment Topology","id":"313-deployment-topology"},{"level":3,"text":"3.2 Recommended File/Module Structure","id":"32-recommended-filemodule-structure"},{"level":3,"text":"3.3 End-to-End Message Flow","id":"33-end-to-end-message-flow"},{"level":4,"text":"Scenario Setup:","id":"scenario-setup"},{"level":4,"text":"Step-by-Step Flow:","id":"step-by-step-flow"},{"level":4,"text":"Failure Scenario Handling","id":"failure-scenario-handling"},{"level":2,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":3,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":3,"text":"B. Recommended Starter Code Structure","id":"b-recommended-starter-code-structure"},{"level":3,"text":"C. Core Logic Skeleton","id":"c-core-logic-skeleton"},{"level":3,"text":"D. Language-Specific Hints","id":"d-language-specific-hints"},{"level":3,"text":"E. Milestone Checkpoint","id":"e-milestone-checkpoint"},{"level":2,"text":"4. Data Model","id":"4-data-model"},{"level":3,"text":"4.1 Core Types and Relationships","id":"41-core-types-and-relationships"},{"level":4,"text":"In-Memory Core Types","id":"in-memory-core-types"},{"level":4,"text":"Type Relationships and Lifetime","id":"type-relationships-and-lifetime"},{"level":3,"text":"4.2 On-Disk Storage Format","id":"42-on-disk-storage-format"},{"level":4,"text":"Log Segment Files","id":"log-segment-files"},{"level":4,"text":"Consumer Offset Storage Format","id":"consumer-offset-storage-format"},{"level":3,"text":"4.3 Cluster Metadata","id":"43-cluster-metadata"},{"level":4,"text":"Metadata Structures","id":"metadata-structures"},{"level":4,"text":"Metadata Propagation and Consistency","id":"metadata-propagation-and-consistency"},{"level":4,"text":"Metadata Storage","id":"metadata-storage"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint (Data Model)","id":"f-milestone-checkpoint-data-model"},{"level":4,"text":"G. Common Pitfalls","id":"g-common-pitfalls"},{"level":2,"text":"5. Component Design: Broker and Topic Partitions","id":"5-component-design-broker-and-topic-partitions"},{"level":3,"text":"5.1 Responsibility and Scope","id":"51-responsibility-and-scope"},{"level":3,"text":"5.2 Mental Model: The Immutable Ledger","id":"52-mental-model-the-immutable-ledger"},{"level":3,"text":"5.3 Public Interface (APIs)","id":"53-public-interface-apis"},{"level":4,"text":"Core Broker RPC Methods","id":"core-broker-rpc-methods"},{"level":4,"text":"Internal Management APIs (Used by Other Brokers/Coordinators)","id":"internal-management-apis-used-by-other-brokerscoordinators"},{"level":4,"text":"State Transition Table for Partition Leadership","id":"state-transition-table-for-partition-leadership"},{"level":3,"text":"5.4 Internal Behavior: Log Management","id":"54-internal-behavior-log-management"},{"level":4,"text":"Data Structures for Log Management","id":"data-structures-for-log-management"},{"level":4,"text":"Algorithm: Appending Records to a Partition Log","id":"algorithm-appending-records-to-a-partition-log"},{"level":4,"text":"Algorithm: Reading Records from a Partition Log","id":"algorithm-reading-records-from-a-partition-log"},{"level":4,"text":"Log Segment Rolling and Cleanup","id":"log-segment-rolling-and-cleanup"},{"level":3,"text":"5.5 ADR: Log Storage Backend","id":"55-adr-log-storage-backend"},{"level":3,"text":"5.6 Common Pitfalls","id":"56-common-pitfalls"},{"level":3,"text":"5.7 Implementation Guidance","id":"57-implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: WAL Wrapper","id":"c-infrastructure-starter-code-wal-wrapper"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":2,"text":"6. Component Design: Producer","id":"6-component-design-producer"},{"level":3,"text":"6.1 Responsibility and Scope","id":"61-responsibility-and-scope"},{"level":3,"text":"6.2 Mental Model: The Postal Batch Sorter","id":"62-mental-model-the-postal-batch-sorter"},{"level":3,"text":"6.3 Public Interface","id":"63-public-interface"},{"level":3,"text":"6.4 Internal Behavior: Batching and Sending","id":"64-internal-behavior-batching-and-sending"},{"level":3,"text":"6.5 ADR: Acknowledgment Semantics","id":"65-adr-acknowledgment-semantics"},{"level":3,"text":"6.6 Common Pitfalls","id":"66-common-pitfalls"},{"level":3,"text":"6.7 Implementation Guidance","id":"67-implementation-guidance"},{"level":2,"text":"7. Component Design: Consumer and Consumer Groups","id":"7-component-design-consumer-and-consumer-groups"},{"level":3,"text":"7.1 Responsibility and Scope","id":"71-responsibility-and-scope"},{"level":3,"text":"7.2 Mental Model: The Team Reading a Shared Book","id":"72-mental-model-the-team-reading-a-shared-book"},{"level":3,"text":"7.3 Public Interface","id":"73-public-interface"},{"level":4,"text":"Consumer Client API (Application-Facing)","id":"consumer-client-api-application-facing"},{"level":4,"text":"Coordinator Internal API (Broker-to-Consumer)","id":"coordinator-internal-api-broker-to-consumer"},{"level":3,"text":"7.4 Internal Behavior: Group Membership &amp; Rebalancing","id":"74-internal-behavior-group-membership-amp-rebalancing"},{"level":4,"text":"Consumer Member State Machine","id":"consumer-member-state-machine"},{"level":4,"text":"Group Rebalancing Protocol","id":"group-rebalancing-protocol"},{"level":4,"text":"Partition Assignment Algorithm (Leader Side)","id":"partition-assignment-algorithm-leader-side"},{"level":4,"text":"Offset Commit and Fetch","id":"offset-commit-and-fetch"},{"level":3,"text":"7.5 ADR: Partition Assignment Strategy","id":"75-adr-partition-assignment-strategy"},{"level":3,"text":"7.6 Common Pitfalls","id":"76-common-pitfalls"},{"level":3,"text":"7.7 Implementation Guidance","id":"77-implementation-guidance"},{"level":2,"text":"8. Component Design: Replication","id":"8-component-design-replication"},{"level":3,"text":"8.1 Responsibility and Scope","id":"81-responsibility-and-scope"},{"level":3,"text":"8.2 Mental Model: The Ship&#39;s Log Replica","id":"82-mental-model-the-ship39s-log-replica"},{"level":3,"text":"8.3 Internal Broker-to-Broker API","id":"83-internal-broker-to-broker-api"},{"level":3,"text":"8.4 Internal Behavior: Follower Sync and ISR Management","id":"84-internal-behavior-follower-sync-and-isr-management"},{"level":4,"text":"Follower Synchronization Algorithm","id":"follower-synchronization-algorithm"},{"level":4,"text":"Leader ISR Management Algorithm","id":"leader-isr-management-algorithm"},{"level":3,"text":"8.5 ADR: Leader Election Trigger","id":"85-adr-leader-election-trigger"},{"level":3,"text":"8.6 Common Pitfalls","id":"86-common-pitfalls"},{"level":3,"text":"8.7 Implementation Guidance","id":"87-implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: Replication RPC Protocol","id":"c-infrastructure-starter-code-replication-rpc-protocol"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":2,"text":"9. Interactions and Data Flow","id":"9-interactions-and-data-flow"},{"level":3,"text":"9.1 Key Sequence Diagrams","id":"91-key-sequence-diagrams"},{"level":4,"text":"9.1.1 Message Production Flow","id":"911-message-production-flow"},{"level":4,"text":"9.1.2 Consumer Group Rebalancing Flow","id":"912-consumer-group-rebalancing-flow"},{"level":4,"text":"9.1.3 Message Consumption Flow","id":"913-message-consumption-flow"},{"level":3,"text":"9.2 Wire Protocol Specification","id":"92-wire-protocol-specification"},{"level":4,"text":"9.2.1 Protocol Design Principles","id":"921-protocol-design-principles"},{"level":4,"text":"9.2.2 Common Request/Response Structure","id":"922-common-requestresponse-structure"},{"level":4,"text":"9.2.3 Produce Request/Response Format","id":"923-produce-requestresponse-format"},{"level":4,"text":"9.2.4 Fetch Request/Response Format","id":"924-fetch-requestresponse-format"},{"level":4,"text":"9.2.5 JoinGroup Request/Response Format","id":"925-joingroup-requestresponse-format"},{"level":3,"text":"9.3 Coordination Service Abstraction","id":"93-coordination-service-abstraction"},{"level":4,"text":"9.3.1 Minimal Coordination Interface","id":"931-minimal-coordination-interface"},{"level":4,"text":"9.3.2 Metadata Propagation Patterns","id":"932-metadata-propagation-patterns"},{"level":4,"text":"9.3.3 Fault Tolerance Considerations","id":"933-fault-tolerance-considerations"},{"level":3,"text":"9.4 Implementation Guidance","id":"94-implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Protocol Framing Infrastructure (Complete)","id":"protocol-framing-infrastructure-complete"},{"level":4,"text":"Core Protocol Serialization Skeleton","id":"core-protocol-serialization-skeleton"},{"level":4,"text":"Coordination Service Skeleton","id":"coordination-service-skeleton"},{"level":4,"text":"Language-Specific Hints for Go","id":"language-specific-hints-for-go"},{"level":4,"text":"Milestone Checkpoint: Protocol Implementation","id":"milestone-checkpoint-protocol-implementation"},{"level":2,"text":"10. Error Handling and Edge Cases","id":"10-error-handling-and-edge-cases"},{"level":3,"text":"10.1 Failure Mode Categories","id":"101-failure-mode-categories"},{"level":4,"text":"10.1.1 Broker Crashes (Process Termination)","id":"1011-broker-crashes-process-termination"},{"level":4,"text":"10.1.2 Network Partitions (Split-Brain Scenarios)","id":"1012-network-partitions-split-brain-scenarios"},{"level":4,"text":"10.1.3 Disk Failures (Storage Corruption)","id":"1013-disk-failures-storage-corruption"},{"level":4,"text":"10.1.4 Client Timeouts and Transient Failures","id":"1014-client-timeouts-and-transient-failures"},{"level":3,"text":"10.2 Recovery Strategies","id":"102-recovery-strategies"},{"level":4,"text":"10.2.1 Broker Crash Recovery","id":"1021-broker-crash-recovery"},{"level":4,"text":"10.2.2 Network Partition Mitigation","id":"1022-network-partition-mitigation"},{"level":4,"text":"10.2.3 Disk Failure Handling","id":"1023-disk-failure-handling"},{"level":4,"text":"10.2.4 Client Failure Recovery","id":"1024-client-failure-recovery"},{"level":3,"text":"10.3 Edge Cases and Race Conditions","id":"103-edge-cases-and-race-conditions"},{"level":4,"text":"10.3.1 Leader Change During Produce Request","id":"1031-leader-change-during-produce-request"},{"level":4,"text":"10.3.2 Duplicate Consumer ID During Rebalance","id":"1032-duplicate-consumer-id-during-rebalance"},{"level":4,"text":"10.3.3 Partial Write Before Crash (Torn Writes)","id":"1033-partial-write-before-crash-torn-writes"},{"level":4,"text":"10.3.4 Zombie Consumers in Rebalance","id":"1034-zombie-consumers-in-rebalance"},{"level":4,"text":"10.3.5 ISR Shrinkage to Empty Set","id":"1035-isr-shrinkage-to-empty-set"},{"level":4,"text":"10.3.6 Offset Commit Race During Rebalance","id":"1036-offset-commit-race-during-rebalance"},{"level":4,"text":"10.3.7 Producer Retry Causing Duplicate Sequence","id":"1037-producer-retry-causing-duplicate-sequence"},{"level":4,"text":"10.3.8 Log Truncation During Follower Sync","id":"1038-log-truncation-during-follower-sync"},{"level":3,"text":"10.4 Failure Recovery Summary Table","id":"104-failure-recovery-summary-table"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Recommended Failure Detection Infrastructure","id":"a-recommended-failure-detection-infrastructure"},{"level":4,"text":"B. Core Recovery Logic Skeletons","id":"b-core-recovery-logic-skeletons"},{"level":4,"text":"C. Debugging Tips for Common Failure Scenarios","id":"c-debugging-tips-for-common-failure-scenarios"},{"level":4,"text":"D. Testing Failure Recovery","id":"d-testing-failure-recovery"},{"level":2,"text":"11. Testing Strategy","id":"11-testing-strategy"},{"level":3,"text":"11.1 Testing Approach","id":"111-testing-approach"},{"level":3,"text":"11.2 Milestone Verification Checkpoints","id":"112-milestone-verification-checkpoints"},{"level":3,"text":"11.3 Tools and Libraries","id":"113-tools-and-libraries"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"12. Debugging Guide","id":"12-debugging-guide"},{"level":3,"text":"12.1 Common Bug Symptoms","id":"121-common-bug-symptoms"},{"level":3,"text":"12.2 Diagnosis Techniques","id":"122-diagnosis-techniques"},{"level":4,"text":"Structured Logging with Context","id":"structured-logging-with-context"},{"level":4,"text":"Internal State Inspection via Admin APIs","id":"internal-state-inspection-via-admin-apis"},{"level":4,"text":"Timeout-Based Bottleneck Identification","id":"timeout-based-bottleneck-identification"},{"level":4,"text":"Controlled Experimentation (Scientific Method)","id":"controlled-experimentation-scientific-method"},{"level":4,"text":"Concurrency Race Detection","id":"concurrency-race-detection"},{"level":3,"text":"12.3 Symptom → Cause → Fix Table","id":"123-symptom-cause-fix-table"},{"level":4,"text":"Debugging Workflow Example: Diagnosing Duplicate Messages","id":"debugging-workflow-example-diagnosing-duplicate-messages"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: Structured Logger Wrapper","id":"c-infrastructure-starter-code-structured-logger-wrapper"},{"level":4,"text":"D. Core Logic Skeleton: Debug State Collection","id":"d-core-logic-skeleton-debug-state-collection"},{"level":4,"text":"E. Language-Specific Hints (Go)","id":"e-language-specific-hints-go"},{"level":4,"text":"F. Debugging Tips Table","id":"f-debugging-tips-table"},{"level":4,"text":"G. Milestone Checkpoint: Debugging Verification","id":"g-milestone-checkpoint-debugging-verification"},{"level":2,"text":"13. Future Extensions","id":"13-future-extensions"},{"level":3,"text":"13.1 Potential Feature Additions","id":"131-potential-feature-additions"},{"level":4,"text":"Compression: The Space-Saving Courier","id":"compression-the-space-saving-courier"},{"level":4,"text":"Exactly-Once Semantics: The Bank Teller&#39;s Ledger","id":"exactly-once-semantics-the-bank-teller39s-ledger"},{"level":4,"text":"Quotas &amp; Throttling: The Highway Traffic Control","id":"quotas-amp-throttling-the-highway-traffic-control"},{"level":4,"text":"Log Compaction: The Librarian&#39;s Archive Purge","id":"log-compaction-the-librarian39s-archive-purge"},{"level":4,"text":"Custom RPC Layer: Building Your Own Postal Service","id":"custom-rpc-layer-building-your-own-postal-service"},{"level":3,"text":"13.2 Design Considerations for Extensions","id":"132-design-considerations-for-extensions"},{"level":4,"text":"13.2.1 Compression: Batch-Level vs Record-Level","id":"1321-compression-batch-level-vs-record-level"},{"level":4,"text":"13.2.2 Exactly-Once Semantics: Transaction Coordinator Placement","id":"1322-exactly-once-semantics-transaction-coordinator-placement"},{"level":4,"text":"13.2.3 Quotas: Measurement and Enforcement Points","id":"1323-quotas-measurement-and-enforcement-points"},{"level":4,"text":"13.2.4 Log Compaction: Background vs Online Compaction","id":"1324-log-compaction-background-vs-online-compaction"},{"level":4,"text":"13.2.5 Custom RPC Layer: Protocol Design Trade-offs","id":"1325-custom-rpc-layer-protocol-design-trade-offs"},{"level":4,"text":"13.2.6 Tiered Storage: Hot/Warm/Cold Architecture","id":"1326-tiered-storage-hotwarmcold-architecture"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure for Extensions","id":"recommended-module-structure-for-extensions"},{"level":4,"text":"Compression Starter Code","id":"compression-starter-code"},{"level":4,"text":"Transaction Coordinator Skeleton","id":"transaction-coordinator-skeleton"},{"level":4,"text":"Language-Specific Hints for Go","id":"language-specific-hints-for-go"},{"level":4,"text":"Milestone Checkpoint for Compression Extension","id":"milestone-checkpoint-for-compression-extension"},{"level":2,"text":"14. Glossary","id":"14-glossary"},{"level":3,"text":"14.1 Terms and Definitions","id":"141-terms-and-definitions"}],"title":"Build Your Own Kafka: Design Document","markdown":"# Build Your Own Kafka: Design Document\n\n\n## Overview\n\nThis document describes the design and implementation of a distributed message queue supporting partitioned topics, consumer groups, and replication. The key architectural challenge is maintaining ordered, durable message delivery across a scalable and fault-tolerant cluster of brokers, while navigating the inherent trade-offs between latency, throughput, and consistency.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n> **Milestone(s):** All (foundational context)\n\n## 1. Context and Problem Statement\n\nAt the heart of modern software architecture lies a fundamental tension: how do we reliably and efficiently move streams of events—user actions, sensor readings, database updates, payment transactions—between dozens or hundreds of independent services? This is the **scalable event streaming problem**. Traditional request/response communication creates a tightly coupled web of dependencies, where the failure or slowness of one service can cascade through the entire system. Message queues emerged to decouple services by introducing an asynchronous buffer, but early designs often traded off one critical property for another: throughput for ordering, durability for latency, or simplicity for scale.\n\nApache Kafka, introduced by LinkedIn in 2011, revolutionized this space by adopting a surprisingly simple core abstraction: the **immutable, partitioned, append-only log**. Instead of treating messages as transient envelopes to be delivered and deleted, Kafka treats them as permanent records in a ledger. This design shifts the focus from complex routing and delivery semantics to durable storage and high-speed sequential I/O. This document outlines the design and implementation of a simplified, educational version of such a system—\"Build Your Own Kafka.\" We will explore how to construct a distributed message queue that provides strong ordering guarantees, horizontal scalability, and fault tolerance through replication, while navigating the inherent trade-offs between latency, throughput, and consistency.\n\n### 1.1 The Centralized Ledger Analogy\n\nTo build intuition, imagine a traditional bank's **centralized ledger**. This ledger records every transaction—deposits, withdrawals, transfers—in strict chronological order. Each account's balance is not a stored number but a derived state computed by replaying all transactions for that account from the beginning of time. This model provides an **authoritative, immutable history** of all activity.\n\nNow, scale this bank to serve millions of customers. A single ledger book becomes a bottleneck; clerks cannot write fast enough, and customers wait in long queues to record their transactions. The bank's solution is **partitioning**: they create multiple identical ledger books, each responsible for a specific subset of accounts (e.g., accounts A-L in Ledger 1, M-Z in Ledger 2). This is the core mental model for a Kafka **topic**:\n\n*   **Topic**: The overall stream of related events (e.g., `bank-transactions`). Analogous to the *concept* of the transaction log.\n*   **Partition**: A single, ordered, immutable sequence of records within a topic. Analogous to one physical ledger book. Ordering is guaranteed *only within a single partition*.\n*   **Record**: A single entry in the ledger (e.g., \"Account #123: Deposit $100 at 14:30\"). It consists of a key, a value, and a timestamp.\n*   **Offset**: The sequential, monotonically increasing index number assigned to each record as it is appended to its partition (e.g., record #0, #1, #2...). This is the page and line number in the ledger book.\n\nThe critical insight of the key is its role in **deterministic partitioning**. When a transaction (record) arrives, its account number (key) is hashed to consistently route it to the same ledger (partition). All transactions for account #123 always go to the same partition, preserving their chronological order. If the key is null, the record is assigned to partitions in a round-robin fashion for load distribution.\n\n> **Design Insight:** The shift from a \"message queue\" to a \"log\" is profound. A queue is about delivery and removal; a log is about durable recording and subscription. Consumers read from the log at their own pace, and the log persists independently of them. This enables replayability, auditing, and the ability to spawn new consumers for historical data analysis—patterns that are cumbersome or impossible with traditional queues.\n\nThis ledger analogy extends to the cluster:\n*   **Broker**: A bank branch office that safely stores a set of ledger books (partitions). A cluster consists of multiple brokers for redundancy and capacity.\n*   **Producer**: A teller or ATM that accepts new transactions and writes them into the correct ledger book.\n*   **Consumer**: An auditor or accountant who reads sequentially from one or more ledger books to compute balances or detect fraud.\n*   **Consumer Group**: A team of auditors, where each member is assigned a non-overlapping subset of the ledger books to parallelize the audit work.\n\n### 1.2 The Scalable Event Streaming Problem\n\nBuilding a system that realizes this ledger analogy at internet scale requires satisfying a interconnected set of core requirements. These requirements often exist in tension, and the design choices we make will prioritize certain qualities over others.\n\n**Core Requirements:**\n\n| Requirement | Description | Implication for Design |\n| :--- | :--- | :--- |\n| **High-Throughput & Low Latency** | The system must ingest and deliver hundreds of thousands—or millions—of events per second with minimal delay. | Demands efficient serialization, batching, and heavy reliance on sequential disk I/O over random access. Network and disk overhead must be minimized. |\n| **Ordered Delivery** | Events with a causal relationship (e.g., updates to the same entity) must be processed in the order they occurred. | Requires a strong ordering guarantee *within a partition*. The system must ensure all records for a given key go to the same partition and are appended in arrival order. |\n| **Durability & Fault Tolerance** | Once acknowledged, messages must not be lost, even in the face of individual server failures, power outages, or network partitions. | Necessitates replication of data across multiple brokers (servers) and durable writes to disk (fsync). The system must define what \"acknowledged\" means (e.g., written to leader? written to all replicas?). |\n| **Scalability & Elasticity** | The system's capacity should increase linearly by adding more brokers. It should also allow consumers and producers to scale independently. | Requires stateless producers/consumers, data partitioning, and dynamic reassignment of partition ownership. No single component (like a central router) should become a bottleneck. |\n| **Decoupling of Producers & Consumers** | Producers should not need to know about consumers, and vice versa. Consumers should be able to process data at their own pace. | Implies a publish-subscribe model with persistent storage. The log acts as a buffer, allowing producers to write as fast as they can and consumers to read when they are ready. |\n| **Consumer Group Semantics** | Multiple consumers can work together to process a topic, with each partition being consumed by exactly one group member at a time. This provides parallel processing and fault tolerance for the consumption side. | Requires a coordination service to manage group membership, assign partitions, and track progress (offsets). |\n\nThe primary challenge is that these requirements conflict. For example:\n*   **Durability vs. Latency:** Ensuring a message is written to disk on multiple replicas (high durability) increases latency compared to acknowledging once the leader receives it.\n*   **Ordering vs. Throughput:** Maintaining strict global order requires a single sequencing point (a bottleneck). Partitioning sacrifices global order for parallelism and throughput.\n*   **Scalability vs. Simplicity:** A simple, single-broker design is easy to reason about but doesn't scale. A distributed design introduces complexity around consistency, failure detection, and recovery.\n\n**The Log-Based Solution:**\nThe partitioned log model directly addresses these tensions:\n1.  **Throughput & Scalability:** Partitioning allows parallel ingestion and consumption across many brokers and disks. Sequential appends maximize disk I/O efficiency.\n2.  **Ordered Delivery:** Order is guaranteed per-partition, which is sufficient for most use cases when keys are chosen wisely (e.g., `user_id`).\n3.  **Durability:** The log is an immutable, append-only file. Replication provides durability against node failure, while periodic fsync provides durability against process crashes.\n4.  **Decoupling:** The log is the persistent interface. Producers append; consumers subscribe to positions (offsets) in the log.\n\n> **Architecture Decision Record (ADR): Log as the Primary Abstraction**\n> *   **Context:** We need a data structure that serves as the durable, ordered backbone for event streaming, balancing throughput, ordering, and simplicity.\n> *   **Options Considered:**\n>     1.  **Traditional Message Queue (e.g., AMQP model):** Messages are stored in in-memory or disk-backed queues, deleted upon acknowledgment. Supports complex routing (exchanges, bindings).\n>     2.  **Distributed Commit Log:** Messages are immutable records appended to partitioned, replicated log segments. Simple append/read semantics.\n>     3.  **Event Sourcing Database:** Treats the log as a system of record, with built-in query capabilities and state derivation.\n> *   **Decision:** Use a **Distributed Commit Log** as the primary abstraction.\n> *   **Rationale:** The log model provides the optimal balance for our core requirements. Its simplicity (append/read) enables extremely high throughput. Immutability simplifies replication and failure recovery. Partitioning provides horizontal scalability. The retention-based cleanup model is simpler than per-message deletion and enables valuable replay capabilities.\n> *   **Consequences:**\n>     *   **Enables:** High throughput, simple replication, replayability, and natural support for multiple consumers.\n>     *   **Trade-offs:** Requires consumers to manage their own position (offset). Storage management shifts from message TTL to log segment retention/compaction. No built-in complex routing; filtering must happen on the consumer side.\n\n| Option | Pros | Cons | Why Chosen? |\n| :--- | :--- | :--- | :--- |\n| **Traditional Message Queue** | Rich routing semantics, immediate deletion saves space, mature protocols (AMQP). | Deletion complicates replication and replay, often relies on slower random I/O, complex to scale horizontally. | Better for complex routing needs, but our primary goals are throughput and scalability. |\n| **Distributed Commit Log** | Maximizes sequential I/O, simple replication, natural replay, excellent horizontal scalability. | Consumers must manage offset, no built-in message routing, storage grows until retention/compaction. | Directly aligns with high-throughput, ordered, durable event streaming. Simplicity is a feature. |\n| **Event Sourcing Database** | Powerful query abilities, strong consistency models, integrated state derivation. | Higher overhead per message, more complex operational model, often lower write throughput. | Overkill for a messaging backbone; we want a transport layer, not a system of record. |\n\n### 1.3 Existing Approaches & Trade-offs\n\nKafka's log-centric design did not emerge in a vacuum. It sits within a landscape of messaging and streaming systems, each making different architectural trade-offs. Understanding these alternatives clarifies *why* Kafka's choices are particularly well-suited for high-volume event streaming.\n\n**1. Traditional Enterprise Message Brokers (e.g., RabbitMQ, ActiveMQ):**\nThese systems are built around the **smart broker/dumb consumer** model. The broker is responsible for complex message routing, delivery guarantees, and transaction management.\n*   **Typical Architecture:** Centralized or clustered brokers hold messages in queues. They support advanced patterns like publish/subscribe, request/reply, and point-to-point via exchanges, bindings, and queues.\n*   **Trade-offs:**\n    *   **Pros:** Rich feature set (priority, TTL, dead-letter queues), strong delivery guarantees with transactions, flexible routing.\n    *   **Cons:** The broker is a complex, stateful monolith that can become a performance bottleneck. Scaling often involves partitioning (federation/shovel) which is complex. Persistence models (often B-trees) can be slower than sequential logs. Consumers are generally stateful connections to the broker.\n*   **Comparison to Our Design:** Our system adopts a **dumb broker/smart consumer** model. The broker's job is simple: durably store logs and serve reads/writes. All intelligence (partition assignment, offset management, consumer state) is pushed to the client libraries. This shifts complexity but allows the broker to be simpler, more robust, and easier to scale horizontally.\n\n**2. Distributed Log Systems (e.g., Apache BookKeeper, NATS Streaming):**\nThese share Kafka's log-oriented philosophy but differ in implementation details.\n*   **Apache BookKeeper:** Provides a reliable storage service for log segments. Kafka historically used BookKeeper for tiered storage. It offers strong durability guarantees via a quorum-write protocol.\n*   **NATS Streaming:** Provides a similar log abstraction but often with simpler clustering models and a focus on the Go ecosystem.\n*   **Trade-offs:** BookKeeper separates storage and serving, which can offer flexibility. NATS Streaming offers simplicity and integration with the NATS core. Kafka's integrated design (managing its own storage) provides end-to-end control over performance and semantics.\n\n**3. Streaming Platforms & Databases (e.g., Apache Pulsar, AWS Kinesis, Apache Flink):**\nThese systems extend the core log model with additional capabilities.\n*   **Apache Pulsar:** Uses a similar log abstraction but with a clearer separation of compute (brokers) and storage (Apache BookKeeper). Supports both queuing and streaming semantics natively.\n*   **AWS Kinesis:** A managed service offering a Kafka-like shard (partition) model, with tight integration into the AWS ecosystem. Often has stricter limits on retention and throughput per shard.\n*   **Apache Flink:** A true stream processing engine that can also act as a persistent event store with its Stateful Functions API, blurring the line between transport and processing.\n*   **Trade-offs:** Pulsar's separation can aid in independent scaling and operational flexibility. Kinesis offers simplicity as a service. Flink integrates processing and storage. Our educational design follows Kafka's more monolithic broker model for simplicity of understanding, acknowledging that separation of concerns is a valid advanced architectural pattern.\n\n**4. General-Purpose Distributed Databases (e.g., etcd, CockroachDB):**\nThese can be (ab)used as message queues by treating a table as a queue.\n*   **Trade-offs:**\n    *   **Pros:** Strong consistency, transactions, and rich querying.\n    *   **Cons:** Write amplification, lower throughput for streaming workloads, and inefficient tail reads. They are optimized for random access and point queries, not sequential appends and reads.\n*   **Key Distinction:** Databases are optimized for **point-in-time queries** over a dataset. Logs are optimized for **continuous ingestion and sequential reads** of an unbounded stream. The access patterns are fundamentally different.\n\nThe following table summarizes the key architectural trade-offs:\n\n| System Category | Primary Data Model | Scaling Model | Durability Mechanism | Typical Use Case |\n| :--- | :--- | :--- | :--- | :--- |\n| **Traditional Broker (RabbitMQ)** | Queue/Exchange | Vertical scaling or complex federation | Persistent messages in broker (often B-tree) | Enterprise application integration, complex routing. |\n| **Our Design / Kafka** | Partitioned, Append-Only Log | Horizontal partitioning across brokers | Replicated log segments, sequential disk I/O | High-throughput event streaming, log aggregation, activity tracking. |\n| **Pulsar** | Segmented Log (with separate storage) | Independent scaling of brokers & storage | BookKeeper ledger replication | Streaming and queuing with cloud-native operations. |\n| **Database-as-Queue** | Table/Key-Value | Horizontal sharding (for distributed DBs) | Replicated write-ahead log (WAL) | When strong consistency and queueing are both required (generally an anti-pattern for high-volume streams). |\n\n> **The Central Trade-Off: Simplicity vs. Control.** Our educational design, following Kafka's early architecture, chooses to keep storage management within the broker. This creates a more monolithic but simpler-to-understand component. In production systems at scale, the trend is toward disaggregating storage (as seen in Pulsar/Kafka Tiered Storage) to improve elasticity and cost efficiency. However, for learning the fundamentals of replication, partitioning, and consensus, the integrated model provides a more coherent mental model.\n\nBy grounding our design in the immutable log analogy and understanding the landscape of alternatives, we establish a clear foundation. The subsequent sections will delve into how to concretely build a system that embodies these principles, starting with the fundamental unit of storage: the topic and its partitions.\n\n### Implementation Guidance\n\nThis section is purely conceptual and foundational; there is no code to implement. However, it is critical to internalize the mental models and trade-offs discussed here before writing any code. A strong conceptual understanding will prevent fundamental architectural missteps.\n\n**Next Steps for the Learner:**\n1.  **Internalize the Ledger Analogy:** Sketch out how a topic named `page-views` with 3 partitions would store records for `user_id` keys 1, 2, and 3. Determine which partition each record would land on using a simple hash modulo partition count strategy.\n2.  **Contrast with a Queue:** Write down two key differences in behavior between a traditional queue (message deleted after acknowledgment) and a log (message retained until retention expires) from the perspective of a consumer that crashes and restarts.\n3.  **Anticipate Trade-offs:** Before starting Milestone 1, consider: If you prioritize very low latency for producers, what durability setting (`acks`) might you choose, and what is the potential risk?\n\nProceed to **Section 5: Component Design: Broker and Topic Partitions** to begin the implementation of the ledger (log) itself.\n\n\n## 2. Goals and Non-Goals\n\n> **Milestone(s):** All (foundational requirements)\n\nThis section establishes the bounded scope of our educational system. In any engineering endeavor, particularly when learning complex distributed systems, explicitly defining what we **will** and **will not** build is crucial. It prevents scope creep, focuses effort on core learning objectives, and sets realistic expectations about the system's capabilities and limitations.\n\nThe design philosophy for \"Build Your Own Kafka\" prioritizes **educational clarity over feature completeness** and **conceptual implementation over production robustness**. We aim to build a *functional prototype* that illustrates the architectural patterns and trade-offs of a distributed log, not a production-ready message broker.\n\n### 2.1 Functional Goals\n\nOur system must implement the core abstractions and protocols that make a Kafka-like message queue work. Think of these as the **minimum viable pillars** that, when combined, deliver the fundamental value proposition: scalable, ordered, and durable event streaming.\n\n| Functional Area | Required Capability | Description & Learning Objective |\n| :--- | :--- | :--- |\n| **Topic & Partition Management** | Create topics with configurable partitions | Learn how logical data streams (topics) are physically sharded across partitions for horizontal scaling. Each partition is an independent, ordered sequence. |\n| | Consistent key-based routing | Understand how message keys determine partition assignment, enabling per-key ordering guarantees while distributing load. |\n| | Sequential offset assignment | Implement the immutable, append-only log abstraction where each message gets a unique, monotonic offset within its partition. |\n| **Producer** | Configurable acknowledgment levels (0, 1, all) | Experience the durability vs. latency trade-off firsthand by implementing fire-and-forget, leader-acknowledged, and fully-replicated write semantics. |\n| | Client-side batching | Learn how accumulating messages into batches drastically improves network and disk I/O efficiency, a key technique for high-throughput systems. |\n| | Retry with backoff | Handle transient failures (network timeouts, leader elections) gracefully without requiring application intervention. |\n| **Consumer Groups** | Dynamic group membership | Implement the \"team reading a book\" metaphor where consumers in a group coordinate to share the work of consuming a topic's partitions. |\n| | Partition assignment strategies (Range, RoundRobin) | Explore different algorithms for distributing partitions among group members, balancing fairness and locality. |\n| | Offset commit & fetch | Provide at-least-once delivery semantics by allowing consumers to periodically save their read position, enabling resume-after-restart. |\n| | Group rebalancing | Handle the complex distributed coordination problem of redistributing partitions when group membership changes (joins, leaves, failures). |\n| **Replication** | Leader-follower log replication | Build fault tolerance by having followers asynchronously copy data from the partition leader, ensuring data survives single-broker failures. |\n| | In-Sync Replica (ISR) management | Maintain a dynamic set of \"caught-up\" followers and understand how this set defines durability guarantees (`acks=all`). |\n| | High Watermark advancement | Implement the safety mechanism that prevents consumers from reading data that could be lost on leader failure. |\n| | Leader election (from ISR) | Recover from leader failure by promoting a fully caught-up follower, ensuring no data loss during failover (when configured). |\n\nThese goals map directly to the four project milestones. Successfully implementing them yields a system where:\n1. A **producer** can send a message with a key to a topic.\n2. The system **hashes the key** to select a specific partition for that topic.\n3. The **partition leader** appends the message to its local log, assigns an offset, and (based on `acks`) waits for replication to followers.\n4. A **consumer group** with multiple members subscribes to the topic.\n5. The **group coordinator** assigns each partition to exactly one group member.\n6. Each **consumer** fetches messages from its assigned partitions, processes them, and commits its offsets.\n7. If a **broker fails**, the system elects a new leader from the in-sync replicas for its hosted partitions, and consumer groups rebalance to reassign the affected partitions.\n\n### 2.2 Non-Functional Goals\n\nBeyond the feature checklist, our system should exhibit certain qualitative properties. These are not explicit user-facing features but essential characteristics of a well-designed educational prototype.\n\n| Property | Goal & Rationale |\n| :--- | :--- |\n| **Durability (within reason)** | Messages acknowledged with `acks=all` should survive a single broker failure without loss. This is achieved via on-disk storage (fsync configurable) and replication. We trade off absolute durability (e.g., synchronous disk writes for every message) for implementation simplicity and performance that illustrates the concept. |\n| **Scalability (demonstrable)** | The architecture should allow horizontal scaling: adding more partitions to a topic should increase write throughput; adding more consumers to a group should increase read throughput. The implementation should not have central bottlenecks (like a global lock) that would prevent this in a multi-broker setup, even if our test cluster is small. |\n| **Fault Tolerance (basic)** | The system must handle predictable failures gracefully: broker crashes trigger leader election and consumer rebalancing; network timeouts trigger producer retries. We aim for *clean failure modes* (clear error messages, no silent data corruption) over complex recovery from arbitrary faults (e.g., Byzantine failures). |\n| **Understandability (paramount)** | Code and design must prioritize clarity for learners. This means: <br>• Choosing simpler, more explicit algorithms over optimized but obscure ones.<br>• Using clear naming and modular separation of concerns.<br>• Providing extensive logging to visualize internal state (e.g., \"Leader for partition `orders-0` changed from broker 1 to broker 2\").<br>• Avoiding premature optimization that obfuscates the core logic. |\n| **Operability (for learning)** | The system should be easy to run and observe locally. This includes simple startup/shutdown procedures, exposed metrics/logs for debugging, and the ability to simulate failures (e.g., killing a broker process) to see recovery in action. |\n\n> **Design Insight:** The primary non-functional goal is **pedagogical value**. Every design decision should be evaluated against the question: \"Does this make the core concepts easier or harder to understand?\" This often means choosing the more explicit, verbose, or modular approach over the most performant or idiomatic one.\n\n### 2.3 Explicit Non-Goals\n\nTo maintain focus, we deliberately exclude several features that are important in production Kafka but are considered advanced or tangential to our core learning objectives. Stating these upfront prevents \"feature creep\" and clarifies the system's boundaries.\n\n| Area | What We Are **NOT** Building | Rationale & Learning Impact |\n| :--- | :--- | :--- |\n| **Protocol Compatibility** | Full Apache Kafka wire protocol (binary) compatibility. | Implementing the official protocol is complex and adds significant serialization/deserialization overhead. We will define a **simplified, explicit protocol** (e.g., JSON over HTTP or a trivial binary format) to keep the focus on system logic, not wire-format intricacies. |\n| **Transactional Messaging** | Exactly-once semantics (EOS), idempotent producers, or cross-partition transactions. | Transactions introduce significant complexity (transaction coordinator, write-ahead logs for transaction state, two-phase commit). They are a valuable *advanced topic* but would double the project scope. We focus on at-least-once delivery as the foundational model. |\n| **Storage Efficiency** | Tiered storage (offloading to S3), log compaction, or sophisticated compression (Zstandard, LZ4). | While critical for production cost and performance, these are optimizations built *on top of* the core log abstraction. Implementing them would distract from understanding the log itself. Our log segments will be simple on-disk files. |\n| **Production-Grade Security** | Authentication (SASL), Authorization (ACLs), or Encryption (TLS). | Security is vital but largely orthogonal to the distributed systems algorithms we're studying. Adding it would complicate every network call and configuration without deepening understanding of replication, consensus, or partitioning. |\n| **Advanced Cluster Management** | Dynamic configuration, partition reassignment tools, rack-aware replica placement, or multi-region replication. | These are operations and reliability features for managing large clusters. Our educational cluster will be static-configured and likely run on a single machine. |\n| **Monitoring & Metrics** | A comprehensive metrics subsystem (JMX, Prometheus endpoints) or detailed health checks. | While we value observability for learning, we'll achieve it through structured logging rather than a full metrics pipeline to keep the codebase lean. |\n| **High-Performance Networking** | Custom RPC layer with zero-copy, kernel-bypass, or sophisticated connection pooling. | We'll use the standard library's networking (`net/http` or `net`) to keep I/O models simple and understandable. Performance tuning is a separate, deep discipline. |\n| **Client Libraries** | Idiomatic, feature-complete client libraries for multiple languages. | We'll build a minimal client (producer/consumer) in Go for testing and demonstration. The focus is on the server-side broker logic. |\n\n> **ADR Summary: Scope Limitation for Educational Focus**\n> **Context:** The project could easily expand to encompass many production features of Apache Kafka.\n> **Options Considered:**\n> 1. **Minimal Core:** Build only the four milestone features with simplified protocols and storage.\n> 2. **Extended Feature Set:** Include 1-2 advanced features like idempotent producers or log compaction.\n> 3. **Protocol-First:** Focus on implementing the full Kafka binary protocol with a minimal feature set.\n> **Decision:** Adopt Option 1 (Minimal Core).\n> **Rationale:** The primary objective is to learn distributed systems concepts—replication, partitioning, consensus, and fault tolerance—not to build a production system. Every additional feature dilutes focus, increases complexity, and extends the timeline, risking that learners never complete the core educational journey. A working minimal system that demonstrates the key architectural patterns is more valuable than a half-finished system with many partially implemented features.\n> **Consequences:** The built system will be unsuitable for production use. Learners will need to understand its limitations. However, they will gain a crystal-clear understanding of the foundational mechanics, which is the best preparation for subsequently studying advanced features or contributing to real systems.\n\nUnderstanding these non-goals is as important as the goals themselves. It defines the \"finished line\" for the project and allows learners to celebrate a complete, coherent system that, while limited, fundamentally works and teaches the intended lessons.\n\n---\n---\n\n\n> **Milestone(s):** All (foundational architecture)\n\n## 3. High-Level Architecture\n\nThis section presents the architectural blueprint of our distributed message queue system. Before diving into individual components, we establish a **mental model** of the entire system as a **global library system**. Imagine a vast library (the cluster) with multiple branches (brokers). Authors (producers) submit new books (messages) to the library catalog (topics), where each book is placed on a specific shelf (partition). Reading clubs (consumer groups) visit the library, with each club member (consumer) assigned specific shelves to read from. A head librarian (group coordinator) tracks which club members are present and which shelves they're reading. This analogy helps ground the abstract distributed components in tangible, real-world concepts before we formalize their technical specifications.\n\n![System Component Overview](./diagrams/diagram-system-component.svg)\n\n### 3.1 Component Overview & Responsibilities\n\nThe system comprises four primary component types that work in concert to deliver durable, ordered message streaming. Each component has a distinct responsibility domain and maintains specific state.\n\n#### 3.1.1 Core Components\n\n| Component | Primary Responsibility | Key Data Held | Key Operations Performed |\n|-----------|------------------------|---------------|--------------------------|\n| **Broker** | Stores partition replicas, handles client read/write requests, and replicates data to other brokers. | - Local partition logs (`Partition.Log`)<br>- Topic metadata<br>- Replica state (leader/follower)<br>- Consumer offsets (cached) | - Append records to partition logs<br>- Serve fetch requests to consumers<br>- Replicate data to follower brokers<br>- Elect partition leaders<br>- Track In-Sync Replicas (ISR) |\n| **Producer** | Publishes records to topics, handling batching, reliability guarantees, and error recovery. | - Unsent message batches per partition<br>- Metadata about cluster (topic/partition leaders)<br>- Sequence numbers (for idempotency) | - Route messages to correct partitions<br>- Accumulate messages into batches<br>- Send batches to broker leaders<br>- Handle acknowledgments and retries |\n| **Consumer** | Reads records from topics, maintains consumption position, and participates in consumer groups. | - Assigned partitions and their current offsets<br>- Fetched but unprocessed records<br>- Group membership metadata | - Subscribe to topics<br>- Fetch records from broker leaders<br>- Process records and advance offset<br>- Commit offsets to broker<br>- Participate in group rebalancing |\n| **Group Coordinator** | Manages consumer group membership, partition assignment, and offset persistence. Typically co-located with a broker. | - Group membership lists<br>- Partition assignments per group<br>- Committed offsets (durable storage)<br>- Generation IDs (for rebalancing) | - Process join/leave requests<br>- Trigger and manage rebalancing<br>- Distribute partition assignments<br>- Store and serve committed offsets |\n\n#### 3.1.2 Component Interactions\n\nThe components interact through well-defined request/response patterns over network connections:\n\n1. **Producer → Broker (Data Plane)**: Producers send `Produce` requests to broker leaders for specific partitions. These requests contain batched messages that the broker appends to its local log.\n\n2. **Consumer → Broker (Data Plane)**: Consumers send `Fetch` requests to broker leaders to retrieve records from partitions, starting from a specific offset.\n\n3. **Consumer ↔ Coordinator (Control Plane)**: Consumers communicate with the group coordinator for membership management:\n   - `JoinGroup`: Register with a consumer group\n   - `SyncGroup`: Receive partition assignments\n   - `Heartbeat`: Maintain active membership\n   - `OffsetCommit`: Persist consumption progress\n   - `OffsetFetch`: Retrieve last committed offset\n\n4. **Broker ↔ Broker (Replication Plane)**: Brokers replicate data among themselves:\n   - Followers fetch new records from partition leaders\n   - Leaders track follower replication progress (ISR)\n   - Brokers elect new leaders when current leaders fail\n\n> **Key Insight**: The separation between data plane (message flow) and control plane (coordination) is fundamental. The data plane must be optimized for high throughput and low latency, while the control plane prioritizes consistency and correctness, even at the cost of some latency.\n\n#### 3.1.3 Deployment Topology\n\nIn a typical deployment:\n- Multiple **brokers** form a cluster, with each broker running on a separate physical or virtual machine.\n- **Producers** and **consumers** run as client libraries within application processes.\n- The **group coordinator** is typically implemented as a special role assumed by one of the brokers (often the first broker or one elected via consensus).\n- All components communicate over TCP/IP networks, with the system designed to tolerate network partitions and broker failures.\n\n### 3.2 Recommended File/Module Structure\n\nFor the Go implementation, we recommend a modular package structure that separates concerns, facilitates testing, and mirrors the architectural components. This structure balances simplicity for learners with good software engineering practices.\n\n```\nbuild-your-own-kafka/\n├── cmd/                          # Application entry points\n│   ├── broker/                   # Broker server executable\n│   │   └── main.go               # Broker startup and configuration\n│   ├── producer-cli/             # CLI tool for testing producers\n│   │   └── main.go\n│   └── consumer-cli/             # CLI tool for testing consumers\n│       └── main.go\n├── internal/                     # Private application code (not for external use)\n│   ├── broker/                   # Broker core logic\n│   │   ├── server.go             # Main broker server struct and lifecycle\n│   │   ├── api_handler.go        # Request/response handling (Produce, Fetch, etc.)\n│   │   ├── log_manager.go        # Partition log management\n│   │   ├── replica_manager.go    # Leader/follower replication\n│   │   └── coordinator.go        # Group coordination (if embedded)\n│   ├── client/                   # Client libraries\n│   │   ├── producer/             # Producer client implementation\n│   │   │   ├── producer.go       # Public Producer interface\n│   │   │   ├── accumulator.go    # Batch accumulation logic\n│   │   │   ├── partitioner.go    # Partition selection strategies\n│   │   │   └── sender.go         # Network sender with retries\n│   │   └── consumer/             # Consumer client implementation\n│   │       ├── consumer.go       # Public Consumer interface\n│   │       ├── fetcher.go        # Record fetching logic\n│   │       ├── group_member.go   # Consumer group membership\n│   │       └── offset_tracker.go # Offset commit/fetch management\n│   ├── protocol/                 # Wire protocol definitions\n│   │   ├── messages.go           # Request/response structs (ProduceRequest, etc.)\n│   │   ├── serialization.go      # Binary encoding/decoding\n│   │   └── api_keys.go           # API key constants\n│   ├── storage/                  # Persistent storage layer\n│   │   ├── log_segment.go        # Log segment with index file\n│   │   ├── wal.go                # Write-ahead log abstraction\n│   │   └── offset_store.go       # Consumer offset persistence\n│   ├── types/                    # Core domain types\n│   │   ├── topic.go              # Topic and Partition types\n│   │   ├── record.go             # Record and RecordBatch types\n│   │   └── metadata.go           # Cluster metadata types\n│   └── network/                  # Network abstraction\n│       ├── connection.go         # TCP connection management\n│       ├── request_handler.go    # Request routing\n│       └── server.go             # Generic TCP server\n├── pkg/                          # Public libraries (if any)\n│   └── publicapi/                # Stable public APIs for external use\n└── test/                         # Integration tests and utilities\n    ├── integration/              # End-to-end integration tests\n    └── helpers.go                # Test utilities\n```\n\n**Package Rationale:**\n\n- **`cmd/`**: Follows Go conventions for executable applications. Separating broker, producer-cli, and consumer-cli allows independent development and testing.\n\n- **`internal/`**: Contains implementation details that should not be imported by external code. This prevents dependency entanglement and maintains clean API boundaries.\n\n- **`internal/broker/`**: Houses the server-side logic. Separating `log_manager`, `replica_manager`, and `coordinator` allows each to evolve independently and be tested in isolation.\n\n- **`internal/client/`**: Contains client libraries. Producers and consumers are separate packages as they have distinct concerns, though they share some protocol code.\n\n- **`internal/protocol/`**: Centralizes wire format definitions. This is critical because brokers and clients must agree exactly on binary formats.\n\n- **`internal/storage/`**: Abstracts persistence concerns. This separation allows swapping storage backends (e.g., file system vs. embedded database) without affecting higher layers.\n\n- **`internal/types/`**: Defines core domain objects shared across packages. Keeping these in a central location prevents circular dependencies.\n\n> **Design Principle**: Each package should have a single, clear responsibility. Dependencies should flow downward: higher-level packages (`broker`, `client`) depend on lower-level ones (`protocol`, `storage`, `types`), but not vice versa.\n\n### 3.3 End-to-End Message Flow\n\nTo understand how components interact, let's trace the journey of a single message through the system. We'll follow a concrete example: a weather service (producer) publishing a temperature reading to a `weather-updates` topic, which is then consumed by a dashboard application (consumer).\n\n![Leader-Follower Replication Flow](./diagrams/diagram-replication-flow.svg)\n\n#### Scenario Setup:\n- **Topic**: `weather-updates` with 3 partitions\n- **Message Key**: `station-42` (identifies the weather station)\n- **Message Value**: `{\"temp\": 22.5, \"humidity\": 65}`\n- **Producer**: Configured with `acks=all` (wait for all ISR replicas)\n- **Consumer Group**: `dashboard-group` with 2 consumer instances\n\n#### Step-by-Step Flow:\n\n1. **Producer Initialization and Metadata Fetch**\n   - The producer starts with a bootstrap broker address (e.g., `broker1:9092`).\n   - It connects to the bootstrap broker and sends a `MetadataRequest` for the `weather-updates` topic.\n   - The broker responds with metadata including:\n     - Partition count (3)\n     - Leader broker for each partition (e.g., partition 0 → broker2, partition 1 → broker1, partition 2 → broker3)\n     - Replica brokers for each partition\n   - The producer caches this metadata for future requests (refreshing periodically or on errors).\n\n2. **Message Routing to Partition**\n   - The producer receives the message with key `station-42`.\n   - It applies the partitioner logic: hash the key, modulo partition count.\n   - `hash(\"station-42\") % 3 = 1` → message routes to partition 1.\n   - The producer knows broker1 leads partition 1 (from cached metadata).\n\n3. **Batch Accumulation**\n   - The producer maintains a per-partition batch accumulator.\n   - The message is added to the batch for partition 1.\n   - If the batch is not yet full and the linger time hasn't expired, the producer waits for more messages to batch.\n   - Once batch is full or timer fires, the batch is marked ready for sending.\n\n4. **Produce Request to Leader**\n   - The producer sends a `ProduceRequest` to broker1 (leader for partition 1).\n   - The request contains:\n     - Topic: `weather-updates`\n     - Partition: 1\n     - Record batch with our message (and potentially other batched messages)\n     - Required acks: `all` (wait for all ISR replicas)\n   - The producer starts a retry timer and stores the batch in an \"in-flight\" buffer.\n\n5. **Leader Log Append**\n   - Broker1 receives the `ProduceRequest` and validates permissions, topic existence, etc.\n   - It appends the record batch to its local log for partition 1.\n   - The log manager assigns the next sequential offset (e.g., offset 142) to the first record in the batch.\n   - The leader writes the batch to its write-ahead log and calls `fsync()` based on durability configuration.\n   - The leader updates its **log end offset** (LEO) for partition 1 to 143 (142 + 1 record).\n\n6. **Replication to Followers**\n   - Partition 1 has replicas on broker2 and broker3 (followers).\n   - Followers periodically send `FetchRequest` to the leader (or the leader pushes, depending on design).\n   - The leader sends the new record batch to followers in response to their fetch requests.\n   - Each follower appends the batch to its own log and acknowledges to the leader.\n   - The leader tracks which replicas have caught up. Once all ISR replicas have the record, the leader advances the **high watermark** to offset 143.\n\n7. **Acknowledgment to Producer**\n   - With `acks=all`, the leader waits until all ISR replicas have replicated the record.\n   - Once the high watermark advances past our record's offset, the leader sends a successful `ProduceResponse` to the producer.\n   - The response includes the base offset assigned (142) for the batch.\n   - The producer removes the batch from its in-flight buffer and can consider the message durably stored.\n\n8. **Consumer Subscription and Assignment**\n   - Meanwhile, two consumers (C1 and C2) in group `dashboard-group` subscribe to topic `weather-updates`.\n   - Each consumer sends `JoinGroup` request to the group coordinator (which may be broker1).\n   - The coordinator designates one consumer as the group leader and collects subscription preferences.\n   - The group leader runs the partition assignment strategy (e.g., range assignment):\n     - Partitions 0 and 1 → Consumer C1\n     - Partition 2 → Consumer C2\n   - The coordinator sends assignments via `SyncGroup` responses.\n\n9. **Consumer Fetch and Delivery**\n   - Consumer C1, now assigned partition 1, fetches records starting from its last committed offset (initially 0 or from previous session).\n   - It sends a `FetchRequest` to broker1 (leader for partition 1), requesting records from offset 142 onward.\n   - Broker1 returns records starting at offset 142, which includes our temperature reading.\n   - Consumer C1 delivers the record to the application callback for processing.\n   - The consumer periodically commits its offset (e.g., offset 143) via `OffsetCommit` requests.\n\n10. **Offset Commitment**\n    - Consumer C1 sends an `OffsetCommit` request to the group coordinator.\n    - The coordinator durably stores `{group: dashboard-group, topic: weather-updates, partition: 1, offset: 143}`.\n    - On restart, consumer C1 can fetch this committed offset and resume from where it left off.\n\n#### Failure Scenario Handling\n\nIf broker1 fails **after** appending the record but **before** replicating to followers:\n\n1. The producer's retry timer expires (no response received).\n2. The producer refreshes metadata and discovers broker1 is down.\n3. A new leader election occurs for partition 1 (from ISR set).\n4. The producer resends the batch to the new leader.\n5. Because the original write may or may not have persisted, we rely on **idempotent sequence numbers** (optional) or accept **at-least-once** delivery semantics.\n\nThis end-to-end flow illustrates the coordinated dance between components to deliver durable, ordered messaging while handling failures transparently.\n\n---\n\n## Implementation Guidance\n\n### A. Technology Recommendations Table\n\n| Component | Simple Option (Recommended for Learning) | Advanced Option (For Extension) |\n|-----------|------------------------------------------|----------------------------------|\n| **Network Transport** | Plain TCP with custom binary protocol using Go's `net` package | gRPC with Protocol Buffers for type-safe RPCs |\n| **Serialization** | Manual binary encoding using `encoding/binary` | Apache Avro with schema registry |\n| **Storage Backend** | Direct filesystem I/O with `os.File` | Embedded key-value store (BadgerDB, BoltDB) |\n| **Coordination** | In-memory coordinator on designated broker with periodic persistence | External coordination service (etcd, ZooKeeper) |\n| **Concurrency** | Goroutines + channels + `sync.Mutex`/`sync.RWMutex` | Actor model with third-party library (protoactor-go) |\n\n### B. Recommended Starter Code Structure\n\nHere's a complete, working foundation for the network and storage layers that learners can build upon:\n\n**File: `internal/network/server.go`**\n```go\npackage network\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net\"\n    \"sync\"\n)\n\n// TCPServer is a generic TCP server that accepts connections and delegates\n// handling to a RequestHandler.\ntype TCPServer struct {\n    addr     string\n    listener net.Listener\n    handler  RequestHandler\n    wg       sync.WaitGroup\n    ctx      context.Context\n    cancel   context.CancelFunc\n}\n\n// RequestHandler processes incoming requests from a connection.\ntype RequestHandler interface {\n    Handle(conn net.Conn) error\n}\n\n// NewTCPServer creates a new TCP server.\nfunc NewTCPServer(addr string, handler RequestHandler) *TCPServer {\n    return &TCPServer{\n        addr:    addr,\n        handler: handler,\n    }\n}\n\n// Start begins accepting connections.\nfunc (s *TCPServer) Start() error {\n    listener, err := net.Listen(\"tcp\", s.addr)\n    if err != nil {\n        return fmt.Errorf(\"failed to listen on %s: %w\", s.addr, err)\n    }\n    \n    s.listener = listener\n    s.ctx, s.cancel = context.WithCancel(context.Background())\n    \n    s.wg.Add(1)\n    go s.acceptLoop()\n    \n    return nil\n}\n\n// acceptLoop accepts incoming connections and spawns goroutines to handle them.\nfunc (s *TCPServer) acceptLoop() {\n    defer s.wg.Done()\n    \n    for {\n        select {\n        case <-s.ctx.Done():\n            return\n        default:\n            conn, err := s.listener.Accept()\n            if err != nil {\n                // Log error but continue\n                continue\n            }\n            \n            s.wg.Add(1)\n            go func(c net.Conn) {\n                defer s.wg.Done()\n                defer c.Close()\n                s.handler.Handle(c)\n            }(conn)\n        }\n    }\n}\n\n// Stop gracefully shuts down the server.\nfunc (s *TCPServer) Stop() error {\n    if s.cancel != nil {\n        s.cancel()\n    }\n    \n    if s.listener != nil {\n        s.listener.Close()\n    }\n    \n    s.wg.Wait()\n    return nil\n}\n```\n\n**File: `internal/storage/wal.go`**\n```go\npackage storage\n\nimport (\n    \"encoding/binary\"\n    \"fmt\"\n    \"io\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n)\n\n// WAL implements a simple write-ahead log for durable writes.\ntype WAL struct {\n    file     *os.File\n    filePath string\n    mu       sync.Mutex\n    offset   int64 // Current write offset\n}\n\n// OpenWAL opens or creates a WAL file at the given path.\nfunc OpenWAL(filePath string) (*WAL, error) {\n    // Ensure directory exists\n    dir := filepath.Dir(filePath)\n    if err := os.MkdirAll(dir, 0755); err != nil {\n        return nil, fmt.Errorf(\"failed to create directory %s: %w\", dir, err)\n    }\n    \n    // Open file in append mode, create if doesn't exist\n    file, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0644)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to open WAL file %s: %w\", filePath, err)\n    }\n    \n    // Get current file size\n    stat, err := file.Stat()\n    if err != nil {\n        file.Close()\n        return nil, fmt.Errorf(\"failed to stat WAL file: %w\", err)\n    }\n    \n    return &WAL{\n        file:     file,\n        filePath: filePath,\n        offset:   stat.Size(),\n    }, nil\n}\n\n// Append writes data to the WAL and optionally fsyncs.\nfunc (w *WAL) Append(data []byte, sync bool) (int64, error) {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    // Write length prefix (8 bytes)\n    length := uint64(len(data))\n    header := make([]byte, 8)\n    binary.BigEndian.PutUint64(header, length)\n    \n    // Write header\n    if _, err := w.file.Write(header); err != nil {\n        return 0, fmt.Errorf(\"failed to write header: %w\", err)\n    }\n    \n    // Write data\n    if _, err := w.file.Write(data); err != nil {\n        return 0, fmt.Errorf(\"failed to write data: %w\", err)\n    }\n    \n    // Sync if requested\n    if sync {\n        if err := w.file.Sync(); err != nil {\n            return 0, fmt.Errorf(\"failed to sync: %w\", err)\n        }\n    }\n    \n    startOffset := w.offset\n    w.offset += int64(8 + len(data)) // Update offset for next write\n    \n    return startOffset, nil\n}\n\n// ReadAll reads all entries from the WAL.\nfunc (w *WAL) ReadAll() ([][]byte, error) {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    // Seek to beginning\n    if _, err := w.file.Seek(0, io.SeekStart); err != nil {\n        return nil, fmt.Errorf(\"failed to seek: %w\", err)\n    }\n    \n    var entries [][]byte\n    \n    for {\n        // Read length prefix\n        var length uint64\n        if err := binary.Read(w.file, binary.BigEndian, &length); err != nil {\n            if err == io.EOF {\n                break\n            }\n            return nil, fmt.Errorf(\"failed to read length: %w\", err)\n        }\n        \n        // Read data\n        data := make([]byte, length)\n        if _, err := io.ReadFull(w.file, data); err != nil {\n            return nil, fmt.Errorf(\"failed to read data: %w\", err)\n        }\n        \n        entries = append(entries, data)\n    }\n    \n    return entries, nil\n}\n\n// Close closes the WAL file.\nfunc (w *WAL) Close() error {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    return w.file.Close()\n}\n\n// CurrentOffset returns the current write offset.\nfunc (w *WAL) CurrentOffset() int64 {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    return w.offset\n}\n```\n\n**File: `internal/types/topic.go`**\n```go\npackage types\n\n// Topic represents a named stream of records divided into partitions.\ntype Topic struct {\n    Name       string\n    Partitions []*Partition\n    Config     map[string]string\n}\n\n// Partition represents a single ordered, immutable sequence of records.\ntype Partition struct {\n    Topic           string\n    ID              int\n    LeaderBrokerID  int\n    ReplicaBrokerIDs []int\n    Log             Log\n    // Additional fields for replication\n    HighWatermark   int64\n    ISR             []int // Broker IDs in the in-sync replica set\n}\n\n// Log represents the storage for a partition.\ntype Log struct {\n    Segments      []*LogSegment\n    CurrentOffset int64 // Next offset to assign\n}\n\n// LogSegment represents a physical file segment of the log.\ntype LogSegment struct {\n    BaseOffset int64  // First offset in this segment\n    DataFile   string // Path to data file\n    IndexFile  string // Path to index file (offset → position)\n}\n```\n\n### C. Core Logic Skeleton\n\n**File: `internal/broker/server.go`**\n```go\npackage broker\n\nimport (\n    \"context\"\n    \"sync\"\n    \n    \"github.com/your-org/build-your-own-kafka/internal/network\"\n    \"github.com/your-org/build-your-own-kafka/internal/storage\"\n    \"github.com/your-org/build-your-own-kafka/internal/types\"\n)\n\n// Server is the main broker server.\ntype Server struct {\n    config    Config\n    server    *network.TCPServer\n    logDir    string\n    \n    // State protected by mutex\n    mu          sync.RWMutex\n    topics      map[string]*types.Topic\n    // TODO: Add other state fields: consumer groups, replica state, etc.\n    \n    // Dependencies\n    logManager  *LogManager\n    coordinator *Coordinator\n}\n\n// Config holds broker configuration.\ntype Config struct {\n    Host string\n    Port int\n    DataDir string\n}\n\n// NewServer creates a new broker server.\nfunc NewServer(config Config) *Server {\n    return &Server{\n        config: config,\n        topics: make(map[string]*types.Topic),\n        logDir: filepath.Join(config.DataDir, \"logs\"),\n    }\n}\n\n// Start starts the broker server.\nfunc (s *Server) Start() error {\n    // TODO 1: Create log directory if it doesn't exist\n    // TODO 2: Initialize log manager to handle partition logs\n    // TODO 3: Load existing topics and partitions from disk\n    // TODO 4: Initialize coordinator for consumer groups\n    // TODO 5: Create and start TCP server with request handler\n    // TODO 6: Start background goroutines: replica fetcher, leader election, etc.\n    return nil\n}\n\n// CreateTopic creates a new topic with the given partition count.\nfunc (s *Server) CreateTopic(name string, partitionCount int) error {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    // TODO 1: Validate topic name doesn't already exist\n    // TODO 2: Validate partition count is positive\n    // TODO 3: Create partition objects with empty logs\n    // TODO 4: Assign partition leaders (initially this broker)\n    // TODO 5: Initialize log segments for each partition\n    // TODO 6: Store topic in topics map\n    // TODO 7: Persist topic metadata to disk\n    \n    return nil\n}\n\n// HandleProduce handles a produce request from a client.\nfunc (s *Server) HandleProduce(req *protocol.ProduceRequest) (*protocol.ProduceResponse, error) {\n    // TODO 1: Validate request: topic exists, partition exists, acks valid\n    // TODO 2: For each partition in request:\n    //   - Check if this broker is leader for the partition\n    //   - Append records to partition log\n    //   - Update log end offset\n    // TODO 3: Based on acks level:\n    //   - acks=0: Return success immediately\n    //   - acks=1: Wait for leader write, then return\n    //   - acks=all: Wait for all ISR replicas, update high watermark, then return\n    // TODO 4: Build response with offsets for each partition\n    return nil, nil\n}\n\n// HandleFetch handles a fetch request from a consumer.\nfunc (s *Server) HandleFetch(req *protocol.FetchRequest) (*protocol.FetchResponse, error) {\n    // TODO 1: Validate request: topic exists, partition exists\n    // TODO 2: For each requested partition:\n    //   - Check if this broker is leader for the partition\n    //   - Read records starting from requested offset up to high watermark\n    //   - If offset is beyond high watermark, wait up to max wait time\n    // TODO 3: Build response with records for each partition\n    return nil, nil\n}\n```\n\n### D. Language-Specific Hints\n\n1. **Concurrency**: Use `sync.RWMutex` for protecting shared broker state (topics, partitions). Readers can acquire read locks concurrently, while writers need exclusive locks.\n\n2. **File I/O**: Always use `file.Sync()` after critical writes to ensure durability. For performance, batch sync operations when possible.\n\n3. **Network Connections**: Use `SetDeadline()` on connections to prevent hung connections from consuming resources.\n\n4. **Error Handling**: In Go, return errors rather than panicking. Use `fmt.Errorf(\"%w\", err)` to wrap errors with context.\n\n5. **Memory Management**: For high-throughput scenarios, reuse byte buffers with `sync.Pool` to reduce garbage collection pressure.\n\n6. **Testing**: Use `net.Listen(\"tcp\", \"localhost:0\")` to get random ports for integration tests, avoiding port conflicts.\n\n### E. Milestone Checkpoint\n\nAfter implementing the high-level architecture, you should be able to:\n\n1. **Start a broker**: Run `go run cmd/broker/main.go` and see it listen on the configured port (e.g., `:9092`).\n\n2. **Create a topic via API**: Use a simple curl command or test program to call `CreateTopic` and verify the topic appears in the broker's topic list.\n\n3. **Verify directory structure**: Check that the data directory contains subdirectories for each topic partition.\n\n4. **Basic connectivity**: Write a simple test that connects to the broker, sends a ping, and receives a response.\n\n**Expected directory structure after creating a topic:**\n```\ndata/\n└── logs/\n    └── weather-updates-0/\n        ├── 00000000000000000000.log\n        └── 00000000000000000000.index\n```\n\n**Signs something is wrong:**\n- Broker fails to start: Check port availability and directory permissions.\n- Topic creation fails: Verify partition count is positive and topic name follows naming rules.\n- No log files created: Ensure the broker has write permissions to the data directory.\n\n\n## 4. Data Model\n> **Milestone(s):** 1, 2, 3, 4 (foundational structures)\nThis section defines the foundational data structures that give our message queue its shape, both in memory and on persistent storage. Before diving into component interactions and algorithms, we must establish a precise vocabulary of types and their relationships. These structures embody the core concepts of topics, partitions, logs, and offsets, forming the skeleton upon which all system behavior is built.\n\n### 4.1 Core Types and Relationships\nThink of the data model as a **library system for events**. A `Topic` is like a book series (e.g., \"Financial Transactions\"). Each book in the series is a `Partition` (e.g., \"Volume 1: NYSE\", \"Volume 2: NASDAQ\"). Inside each partition, `Record`s are individual pages, numbered sequentially by `Offset` (page number). `LogSegment`s are the physical bindings that group a contiguous set of pages for storage efficiency. Finally, `ConsumerGroup`s are reading clubs; each member (`Consumer`) checks out specific volumes and keeps a bookmark (`OffsetMetadata`) to remember where they left off.\n\nThis mental model clarifies the hierarchy and ownership: a topic *has* partitions, a partition *is* a log, a log *contains* segments, and segments *hold* records. Consumer groups *track* progress per partition. The following tables define each type's fields and purpose.\n\n#### In-Memory Core Types\nThese structures are primarily held in memory on brokers and clients to manage runtime state.\n\n| Type | Field Name | Type | Description |\n|------|------------|------|-------------|\n| **`Topic`** | `Name` | `string` | Unique identifier for the topic (e.g., `\"user-events\"`). |\n| | `Partitions` | `[]Partition` | The list of partitions that constitute this topic. The length defines the partition count. |\n| | `Config` | `map[string]string` | Key-value settings for the topic (e.g., `\"retention.ms\": \"604800000\"`). |\n| **`Partition`** | `Topic` | `string` | The name of the parent topic. |\n| | `ID` | `int` | The zero-based identifier of this partition within the topic. |\n| | `LeaderBrokerID` | `int` | The broker ID currently acting as the leader for this partition, handling all reads and writes. |\n| | `ReplicaBrokerIDs` | `[]int` | The ordered list of broker IDs hosting replicas of this partition. The first element is typically the leader. |\n| | `Log` | `Log` | The append-only log instance managing the physical storage of records for this partition. |\n| | `HighWatermark` | `int64` | The offset of the last record that is known to be replicated to all In-Sync Replicas (ISR). Consumers cannot read beyond this point. |\n| | `LogEndOffset` | `int64` | The offset that will be assigned to the next record appended to the log (also called LEO). |\n| | `ISR` | `[]int` | The list of broker IDs currently considered \"in-sync\" (replicas caught up within a configurable lag threshold). |\n| **`Record`** | `Key` | `[]byte` | Optional key used for partitioning and log compaction. `nil` keys are allowed and handled by a default partitioner. |\n| | `Value` | `[]byte` | The message payload. Can be `nil` for tombstones in log compaction. |\n| | `Timestamp` | `int64` | The time the record was created, in milliseconds since the Unix epoch. Used for retention and client-side ordering. |\n| | `Headers` | `map[string][]byte` | Optional application-specific key-value pairs attached to the record (e.g., tracing IDs, content-type). |\n| **`Log`** | `Segments` | `[]LogSegment` | The ordered list of segment files comprising the log. The last segment is the active one for appends. |\n| | `CurrentOffset` | `int64` | The next offset to be assigned (identical to `Partition.LogEndOffset`, maintained for convenience). |\n| | `BaseDir` | `string` | The directory path where segment files for this partition are stored (e.g., `\"/data/topic-0\"`). |\n| **`LogSegment`** | `BaseOffset` | `int64` | The offset of the first record contained in this segment. Used for filename generation and binary search. |\n| | `DataFile` | `string` | Path to the primary data file (e.g., `\"00000000000000000000.log\"`) holding the record batches. |\n| | `IndexFile` | `string` | Path to the sparse index file (e.g., `\"00000000000000000000.index\"`) mapping record offsets to byte positions in the data file. |\n| | `SizeBytes` | `int64` | Current size of the data file in bytes. Used to trigger rolling to a new segment. |\n| **`ConsumerGroup`** | `GroupID` | `string` | Unique identifier for the consumer group (e.g., `\"email-processors\"`). |\n| | `Members` | `map[string]ConsumerMetadata` | Map of consumer member IDs to their metadata (subscriptions, assigned partitions). |\n| | `State` | `GroupState` | Current group state: `Stable`, `PreparingRebalance`, `Dead`. |\n| | `GenerationID` | `int32` | Monotonically increasing integer incremented on each successful rebalancing. Used to identify obsolete requests. |\n| **`ConsumerMetadata`** | `MemberID` | `string` | Unique ID assigned by the coordinator for this group member session. |\n| | `ClientID` | `string` | Client-provided identifier for debugging. |\n| | `SubscribedTopics` | `[]string` | List of topics this consumer has subscribed to. |\n| | `AssignedPartitions` | `map[string][]int32` | Map from topic to list of partition IDs assigned to this member after rebalancing. |\n| | `LastHeartbeat` | `int64` | Unix timestamp (ms) of the last received heartbeat. Used for failure detection. |\n| **`OffsetMetadata`** | `Partition` | `PartitionID` | Composite identifier (Topic, PartitionID) for the partition. |\n| | `GroupID` | `string` | The consumer group this offset belongs to. |\n| | `Offset` | `int64` | The last successfully processed record offset. The consumer will resume from `Offset + 1`. |\n| | `Metadata` | `string` | Optional client-provided string (e.g., a commit message). |\n| | `CommitTimestamp` | `int64` | When this offset was committed (ms since epoch). |\n\n#### Type Relationships and Lifetime\nThe relationships between these types form a directed graph. A `Topic` aggregates `Partition` objects. Each `Partition` owns a single `Log` instance. The `Log` manages a list of `LogSegment` objects, which are created, rolled, and deleted over time. `Record` objects are ephemeral; they are created by producers, serialized into batches, and written to segments. Once written, they are primarily accessed via byte offsets, not as in-memory objects.\n\n`ConsumerGroup` and `OffsetMetadata` have a separate lifecycle. Groups are created when the first consumer joins and persist until all members leave or a session timeout expires. Offsets are committed by consumers and stored durably, separate from the log data, to survive broker restarts.\n\n> **Design Insight:** The separation of *message data* (in partition logs) from *consumption progress* (in offset storage) is critical. It allows multiple independent consumer groups to read the same topic at their own pace, and enables replay by resetting offsets without modifying the underlying log.\n\nThe class diagram below visualizes these relationships:\n![Data Model Relationships](./diagrams/diagram-data-model.svg)\n\n### 4.2 On-Disk Storage Format\nOur system's durability guarantee hinges on reliably persisting two types of data: **log records** (the messages themselves) and **consumer offsets** (progress bookmarks). The design of the on-disk format must balance read efficiency, write performance, crash consistency, and operational simplicity.\n\n> **Mental Model: The Ship's Logbook and Its Index.** Imagine a captain's logbook (`*.log` file) where entries are written sequentially, day by day. A separate index (`*.index` file) records that \"Day 50 starts on page 120.\" The logbook is append-only; we never modify past entries. The index is sparse—we don't index every entry, only every few pages—to keep it small and fast to search.\n\n#### Log Segment Files\nEach partition's log is split into multiple **segment files** for manageability. Segments are rolled when the active data file reaches a configured size (e.g., 1 GB) or time threshold. The naming convention uses the `BaseOffset` (the offset of the first record in the segment) padded to 20 digits: `{base_offset}.log` and `{base_offset}.index`.\n\n**Record Batch Format (`.log` file)**\nMessages are grouped into **batches** for efficiency. The batch is the unit of writing and reading from disk. The binary format is as follows:\n\n| Field | Type | Value/Description |\n|-------|------|-------------------|\n| `base_offset` | int64 | The offset of the first record in this batch. Used for recovery and validation. |\n| `batch_length` | int32 | Length in bytes of the batch data (from `partition_leader_epoch` to the end of `records`). |\n| `partition_leader_epoch` | int32 | For leader fencing; can be set to 0 in initial implementation. |\n| `magic` | int8 | Protocol version (set to `2` for our system, indicating v2 record format). |\n| `crc` | uint32 | CRC32C checksum of the batch data (from `magic` to end of `records`). Validated on read. |\n| `attributes` | int16 | Bit flags for compression (`0x00` = none, `0x01` = gzip, `0x02` = snappy), timestamp type, and transactional control. |\n| `last_offset_delta` | int32 | The difference between the last and first offset in the batch. Allows quick calculation of the last offset without parsing all records. |\n| `first_timestamp` | int64 | The timestamp of the first record in the batch. |\n| `max_timestamp` | int64 | The maximum timestamp among records in the batch. |\n| `producer_id` | int64 | For idempotent producers; set to `-1` if not used. |\n| `producer_epoch` | int16 | For idempotent producers; set to `0` if not used. |\n| `base_sequence` | int32 | For idempotent producers; set to `0` if not used. |\n| `record_count` | int32 | Number of records in the batch. |\n| `records` | []Record | Array of individual records, each encoded as below. |\n\n**Individual Record Format (inside batch)**\nRecords are encoded within a batch to avoid repeating common metadata.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `length` | varint | Length of the record data in bytes (following this field). |\n| `attributes` | int8 | Currently unused (set to `0`). |\n| `timestamp_delta` | varint | Difference from `first_timestamp` in milliseconds. |\n| `offset_delta` | varint | Difference from `base_offset`. |\n| `key_length` | varint | Length of the key bytes, or `-1` for null key. |\n| `key` | bytes | The key bytes (if `key_length >= 0`). |\n| `value_length` | varint | Length of the value bytes, or `-1` for null value. |\n| `value` | bytes | The value bytes (if `value_length >= 0`). |\n| `headers_count` | varint | Number of headers. |\n| `headers` | []Header | Array of headers, each with a `header_key` (string varint) and `header_value` (bytes varint). |\n\n> **Why Batches?** Writing individual records would incur massive overhead per I/O operation. Batching amortizes the cost of the disk seek and filesystem sync across many messages, dramatically increasing throughput. The batch header also allows readers to skip batches (e.g., by offset or timestamp) without parsing individual records.\n\n**Sparse Index Format (`.index` file)**\nThe index file provides a mapping from **offset** to **byte position** in the corresponding `.log` file. It is sparse to remain small; we index only every `index.interval.bytes` (e.g., 4KB) of log data. Each entry is two 8-byte values:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `relative_offset` | int32 | The offset relative to the segment's `base_offset` (`actual_offset - base_offset`). This allows using a 4-byte integer, saving space. |\n| `position` | int32 | The physical byte position of the start of the log batch containing this offset, within the `.log` file. |\n\nThe index is memory-mapped for fast binary search. To find the position for a target offset `O`: 1) binary search the index for the largest `relative_offset <= (O - base_offset)`, 2) read the `.log` file from the corresponding `position`, and 3) scan forward until reaching offset `O`.\n\n#### Consumer Offset Storage Format\nConsumer offsets must be persisted durably and support fast updates and queries. We have two primary design options:\n\n> **Decision: Store Offsets in a Special Internal Topic**\n> - **Context**: We need a durable, replicated, and scalable storage for consumer group offsets that aligns with the system's existing replication and fault-tolerance mechanisms.\n> - **Options Considered**:\n>   1. **Special internal topic** (`__consumer_offsets`): A compacted topic where each message key is `(GroupID, Topic, Partition)` and the value is the offset metadata.\n>   2. **Dedicated file per broker**: A local file (e.g., `offsets.json`) storing all offsets for groups managed by that broker's coordinator.\n>   3. **External key-value store**: Using an embedded DB (like RocksDB/LevelDB) for offset storage.\n> - **Decision**: Use a special internal, compacted topic (`__consumer_offsets`).\n> - **Rationale**:\n>   - **Consistency**: Leverages the same replication, durability, and recovery mechanisms as regular topics, ensuring offsets survive broker failures.\n>   - **Scalability**: Partitions of the internal topic distribute the load across the cluster.\n>   - **Operational Simplicity**: No separate storage subsystem to manage; log compaction automatically removes obsolete offset commits.\n>   - **Educational Value**: Reinforces the power of the log as a universal storage abstraction.\n> - **Consequences**:\n>   - Offsets are replicated and fault-tolerant.\n>   - Requires implementing log compaction (or a simplified version) for cleanup.\n>   - Introduces a bootstrap dependency: the broker must be able to create and write to this topic before handling consumer groups.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Internal Topic | Unified storage, replication, scalability | Requires log compaction; circular dependency risk | **Yes** |\n| Dedicated File | Simple, no extra features needed | Not replicated, single point of failure, scales poorly | No |\n| External KV Store | Fast random updates, mature | Adds a new dependency, operational complexity | No |\n\n**Offset Topic Record Format**\nThe `__consumer_offsets` topic uses the same log segment format, but with a specific key and value schema for its records.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| **Key** | | |\n| `group_id` | string | The consumer group identifier. |\n| `topic` | string | The topic name. |\n| `partition` | int32 | The partition ID. |\n| **Value** | | |\n| `offset` | int64 | The committed offset. |\n| `metadata` | string | Optional client metadata string. |\n| `commit_timestamp` | int64 | Timestamp of the commit (ms). |\n| `expire_timestamp` | int64 | For session timeouts; can be omitted initially. |\n\nThis topic should be **compacted** so that only the latest offset for each `(group, topic, partition)` key is retained, saving space.\n\n### 4.3 Cluster Metadata\nCluster metadata is the **directory of the library**—it tells clients and brokers where to find things. It includes the list of live brokers, which broker leads each partition, and topic configurations. This metadata must be consistently available to all participants and updated dynamically as the cluster changes (brokers join/leave, leadership changes).\n\n#### Metadata Structures\nThe following structures are maintained by a **metadata coordinator** (which could be a dedicated broker or a consensus service like Raft) and cached by all brokers and clients.\n\n| Type | Field | Type | Description |\n|------|-------|------|-------------|\n| **`Broker`** | `ID` | `int` | Unique broker identifier (must be stable across restarts). |\n| | `Host` | `string` | Network hostname or IP address for client connections. |\n| | `Port` | `int` | TCP port for client connections. |\n| | `Rack` | `string` | Optional rack identifier for rack-aware replica placement. |\n| **`TopicMetadata`** | `Name` | `string` | Topic name. |\n| | `Partitions` | `[]PartitionMetadata` | Metadata for each partition in the topic. |\n| | `Config` | `map[string]string` | Topic-level configuration overrides. |\n| **`PartitionMetadata`** | `Topic` | `string` | Topic name. |\n| | `PartitionID` | `int` | Partition identifier. |\n| | `LeaderID` | `int` | Broker ID of the current partition leader. `-1` if leader unknown. |\n| | `ReplicaIDs` | `[]int` | Ordered list of broker IDs hosting replicas. |\n| | `ISR` | `[]int` | Subset of `ReplicaIDs` currently in-sync. |\n| **`ClusterMetadata`** | `Brokers` | `map[int]Broker` | Map of broker ID to broker details. |\n| | `Topics` | `map[string]TopicMetadata` | Map of topic name to topic metadata. |\n| | `ControllerID` | `int` | Broker ID of the current controller (manages leadership elections and partition assignments). |\n| | `ClusterID` | `string` | Unique string identifying the cluster (for multi-cluster routing). |\n\n#### Metadata Propagation and Consistency\nMetadata is disseminated through a **gossip-like protocol** or via direct responses to client requests. When a client (producer/consumer) requests metadata (e.g., because of an unknown topic or a `NOT_LEADER` error), it contacts any broker, which returns the current `ClusterMetadata`. Brokers update their metadata upon leadership changes, broker failures, or topic creation.\n\n> **Design Insight:** Eventually consistent metadata is acceptable for our system. Clients may temporarily have stale metadata, leading them to send produce/fetch requests to the wrong broker. The receiving broker must reject such requests with a `NOT_LEADER` error, prompting the client to refresh its metadata. This pattern ensures correctness while avoiding the complexity of a strongly consistent metadata store in the initial implementation.\n\n#### Metadata Storage\nCluster metadata itself must be persisted to survive controller failures. The controller (a designated broker) stores critical metadata in a durable **metadata log**, which could be a special internal topic (`__cluster_metadata`) or a replicated state machine (like Raft). For simplicity in early milestones, we can store metadata in a file on the controller broker, but this becomes a single point of failure.\n\n> **ADR: Metadata Storage for Milestone 1-3**\n> - **Context**: We need a simple way to persist topic and partition assignments that is sufficient for the educational project before implementing full replication.\n> - **Options**:\n>   1. **In-memory only**: No persistence; topics and assignments are lost on broker restart.\n>   2. **File on controller**: Write metadata to a JSON or binary file on the controller broker's disk.\n>   3. **Replicated log**: Use the same WAL/replication mechanism as data topics.\n> - **Decision**: Use a file on the controller broker (`metadata.json`).\n> - **Rationale**: Simplicity and focus. The primary learning goal in early milestones is the data path (produce/consume), not metadata high availability. A file is easy to implement and provides basic durability for development.\n> - **Consequences**: The controller is a single point of failure. If the controller crashes and its disk is lost, metadata is lost. This is acceptable for learning and can be upgraded to a replicated log in later milestones.\n\n### Implementation Guidance\nThis section provides concrete starter code and structure for implementing the data model in Go.\n\n#### A. Technology Recommendations Table\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| On-Disk Record Format | Custom binary format with `encoding/binary` | Use Protobuf/FlatBuffers for schema evolution |\n| Offset Storage | File-per-broker (`offsets.json`) | Internal compacted topic (`__consumer_offsets`) |\n| Metadata Storage | JSON file on controller (`metadata.json`) | Embedded Raft (hashicorp/raft) for consensus |\n| Index Access | Read entire index into `[]byte`, binary search | Memory-map index file (`mmap`) for zero-copy |\n\n#### B. Recommended File/Module Structure\n```\nbuild-your-own-kafka/\n├── internal/\n│   ├── types/                    # Core data structures\n│   │   ├── topic.go\n│   │   ├── partition.go\n│   │   ├── record.go\n│   │   ├── log.go\n│   │   └── metadata.go\n│   ├── storage/                  # On-disk formats and access\n│   │   ├── segment.go\n│   │   ├── index.go\n│   │   ├── wal.go               # Provided WAL wrapper\n│   │   └── offset_store.go\n│   └── protocol/                # Wire format encoding/decoding\n│       ├── record_batch.go\n│       ├── request_response.go\n│       └── encoding.go\n└── pkg/\n    └── server/                  # Broker server (uses types and storage)\n```\n\n#### C. Infrastructure Starter Code\nHere is a complete, ready-to-use Write-Ahead Log (WAL) wrapper for segment operations. This handles the low-level file appends and syncs, allowing the learner to focus on higher-level log management.\n\n```go\n// internal/storage/wal.go\npackage storage\n\nimport (\n    \"encoding/binary\"\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n)\n\n// WAL represents an append-only log file with optional fsync.\ntype WAL struct {\n    file     *os.File\n    filePath string\n    mu       sync.Mutex\n    offset   int64 // current write offset in bytes\n}\n\n// OpenWAL opens or creates a WAL file at the given path.\nfunc OpenWAL(path string) (*WAL, error) {\n    dir := filepath.Dir(path)\n    if err := os.MkdirAll(dir, 0755); err != nil {\n        return nil, fmt.Errorf(\"create wal dir: %w\", err)\n    }\n    file, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)\n    if err != nil {\n        return nil, fmt.Errorf(\"open wal file: %w\", err)\n    }\n    stat, err := file.Stat()\n    if err != nil {\n        file.Close()\n        return nil, fmt.Errorf(\"stat wal file: %w\", err)\n    }\n    return &WAL{\n        file:     file,\n        filePath: path,\n        offset:   stat.Size(),\n    }, nil\n}\n\n// Append writes data to the WAL and optionally syncs to disk.\n// Returns the file offset at which data was written.\nfunc (w *WAL) Append(data []byte, sync bool) (int64, error) {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n\n    // Write length prefix (8 bytes) followed by data\n    var buf [8]byte\n    binary.BigEndian.PutUint64(buf[:], uint64(len(data)))\n    if _, err := w.file.Write(buf[:]); err != nil {\n        return 0, fmt.Errorf(\"write length: %w\", err)\n    }\n    if _, err := w.file.Write(data); err != nil {\n        return 0, fmt.Errorf(\"write data: %w\", err)\n    }\n    if sync {\n        if err := w.file.Sync(); err != nil {\n            return 0, fmt.Errorf(\"sync: %w\", err)\n        }\n    }\n    writtenAt := w.offset\n    w.offset += int64(8 + len(data))\n    return writtenAt, nil\n}\n\n// ReadAll reads all entries from the WAL.\nfunc (w *WAL) ReadAll() ([][]byte, error) {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n\n    if _, err := w.file.Seek(0, 0); err != nil {\n        return nil, fmt.Errorf(\"seek to start: %w\", err)\n    }\n    var entries [][]byte\n    for {\n        var lengthBuf [8]byte\n        if _, err := w.file.Read(lengthBuf[:]); err != nil {\n            if err.Error() == \"EOF\" {\n                break\n            }\n            return nil, fmt.Errorf(\"read length: %w\", err)\n        }\n        length := binary.BigEndian.Uint64(lengthBuf[:])\n        data := make([]byte, length)\n        if _, err := w.file.Read(data); err != nil {\n            return nil, fmt.Errorf(\"read data: %w\", err)\n        }\n        entries = append(entries, data)\n    }\n    return entries, nil\n}\n\n// CurrentOffset returns the current write offset in bytes.\nfunc (w *WAL) CurrentOffset() int64 {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    return w.offset\n}\n\n// Close closes the WAL file.\nfunc (w *WAL) Close() error {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    return w.file.Close()\n}\n```\n\n#### D. Core Logic Skeleton Code\nBelow are skeleton implementations for the key data model types with TODOs that map to the design described above.\n\n```go\n// internal/types/record.go\npackage types\n\nimport \"time\"\n\ntype Record struct {\n    Key       []byte\n    Value     []byte\n    Timestamp int64\n    Headers   map[string][]byte\n}\n\n// NewRecord creates a new Record with the current timestamp if not provided.\nfunc NewRecord(key, value []byte, headers map[string][]byte) *Record {\n    return &Record{\n        Key:       key,\n        Value:     value,\n        Timestamp: time.Now().UnixMilli(),\n        Headers:   headers,\n    }\n}\n```\n\n```go\n// internal/types/log.go\npackage types\n\ntype Log struct {\n    Segments      []LogSegment\n    CurrentOffset int64\n    BaseDir       string\n    config        LogConfig\n}\n\ntype LogConfig struct {\n    SegmentMaxBytes int64\n    IndexInterval   int\n}\n\n// Append writes a batch of records to the log, returning the starting offset.\nfunc (l *Log) Append(records []*Record) (int64, error) {\n    // TODO 1: Check if active segment exists and has space (l.Segments[len(l.Segments)-1].SizeBytes < config.SegmentMaxBytes)\n    // TODO 2: If no space, roll a new segment (create new data and index files with BaseOffset = l.CurrentOffset)\n    // TODO 3: Serialize records into a RecordBatch using the binary format (see protocol/record_batch.go)\n    // TODO 4: Write the batch to the active segment's data file (using WAL.Append with sync=true for durability)\n    // TODO 5: If written bytes exceed IndexInterval, add an index entry (relative_offset, position) to the index file\n    // TODO 6: Update active segment's SizeBytes\n    // TODO 7: Update l.CurrentOffset by len(records)\n    // TODO 8: Return the starting offset (previous CurrentOffset)\n    return 0, nil\n}\n\n// Read fetches records starting from the given offset, up to maxBytes.\nfunc (l *Log) Read(startOffset int64, maxBytes int32) ([]*Record, error) {\n    // TODO 1: Locate the segment containing startOffset (binary search on Segments[i].BaseOffset)\n    // TODO 2: Load the segment's index into memory (if not already cached)\n    // TODO 3: Use index to find the approximate byte position in the data file\n    // TODO 4: Open data file, seek to position, and scan forward to find the batch with startOffset\n    // TODO 5: Deserialize batches, collect records whose offset >= startOffset\n    // TODO 6: Stop when accumulated size exceeds maxBytes or end of log\n    return nil, nil\n}\n```\n\n```go\n// internal/storage/segment.go\npackage storage\n\ntype LogSegment struct {\n    BaseOffset int64\n    DataFile   string\n    IndexFile  string\n    SizeBytes  int64\n    wal        *WAL\n    index      *Index\n}\n\n// Index manages the sparse offset-to-position mapping.\ntype Index struct {\n    entries []IndexEntry\n    mu      sync.RWMutex\n}\n\ntype IndexEntry struct {\n    RelativeOffset int32\n    Position       int32\n}\n\n// AddEntry adds a new index entry when a batch is written at the given position.\nfunc (idx *Index) AddEntry(offsetDelta int32, position int32) {\n    // TODO: Append entry to in-memory slice, periodically flush to index file\n}\n\n// FindEntry returns the index entry with the largest relativeOffset <= targetDelta.\nfunc (idx *Index) FindEntry(targetDelta int32) (IndexEntry, bool) {\n    // TODO: Binary search on idx.entries\n    return IndexEntry{}, false\n}\n```\n\n#### E. Language-Specific Hints\n- **Serialization**: Use `encoding/binary` with `binary.BigEndian` for portability across networks. For variable-length integers (varint), implement a simple version or use `binary.PutVarint` (though note it's little-endian; you may need to adapt).\n- **File I/O**: Use `os.File` with `O_APPEND` for log segments. For fsync, call `file.Sync()` after writes when `acks=all`. Use `bufio.Writer` for batching file writes, but be careful to flush before sync.\n- **Concurrency**: Protect in-memory structures like `Log.Segments` and `Index.entries` with `sync.RWMutex`. Multiple goroutines can read concurrently, but writes must be exclusive.\n- **Memory Management**: Avoid loading entire segment files into memory. Use `io.ReaderAt` and `io.WriterAt` for random access. Consider memory-mapping index files (`mmap`) for efficient binary search.\n\n#### F. Milestone Checkpoint (Data Model)\nTo verify your data model implementation works:\n1. Run the unit tests for the `types` and `storage` packages:\n   ```bash\n   go test ./internal/types/... ./internal/storage/... -v\n   ```\n2. Create a simple test that creates a `Log`, appends a few records, reads them back, and verifies offsets are sequential and content matches.\n3. Inspect the on-disk files: after appending, you should see `00000000000000000000.log` and `00000000000000000000.index` in your data directory. The `.log` file should contain the binary batch format; you can inspect it with a hex dump (`hexdump -C file.log`).\n\n#### G. Common Pitfalls\n⚠️ **Pitfall: Incorrect offset arithmetic leading to gaps or duplicates**\n- **Description**: When appending a batch of 5 records starting at offset 100, the next offset must be 105. A common mistake is incrementing by 1 instead of the batch size.\n- **Why it's wrong**: Consumers expect contiguous offsets. A gap causes them to stall waiting for the missing offset; duplicates cause reprocessing.\n- **Fix**: Maintain `CurrentOffset` as the *next* offset to assign. When a batch of N records is appended, the starting offset is `CurrentOffset`, and after append, `CurrentOffset += N`.\n\n⚠️ **Pitfall: Not fsync'ing when required by acknowledgment level**\n- **Description**: Writing to the file buffer without calling `Sync()` when `acks=all` can lose data if the OS crashes.\n- **Why it's wrong**: The producer receives an acknowledgment, but the data may not be on durable storage, violating durability guarantees.\n- **Fix**: In `WAL.Append`, conditionally call `file.Sync()` based on the `sync` parameter, which should be set according to the produce request's `acks`.\n\n⚠️ **Pitfall: Storing absolute offsets in index files**\n- **Description**: Using the full 8-byte offset in index entries instead of a 4-byte relative offset wastes space and limits segment size.\n- **Why it's wrong**: Index files become unnecessarily large, reducing the effectiveness of caching. Also, segment base offsets can be very large (e.g., after many writes), making 4-byte absolute offsets insufficient.\n- **Fix**: Store `relative_offset = actual_offset - segment_base_offset` as int32. Ensure segment rolling happens before `relative_offset` exceeds `math.MaxInt32` (i.e., segment size limit).\n\n\n## 5. Component Design: Broker and Topic Partitions\n\n> **Milestone(s):** 1 (Topic and Partitions)\n\n### 5.1 Responsibility and Scope\n\nThe **Broker** is the foundational server node that stores data and handles client requests. Think of it as a **library branch** in a distributed library system: it houses specific shelves (partitions) of books (messages), accepts new donations (producer writes), and allows patrons (consumers) to check out books for reading. Each broker operates independently but coordinates with others to form a complete, fault-tolerant cluster.\n\nThe broker's primary responsibilities are:\n\n1. **Log Storage**: Maintain an immutable, append-only log for each partition it hosts, physically persisted to disk with configurable durability guarantees.\n2. **Request Handling**: Serve **Produce** and **Fetch** requests from producers and consumers over a binary TCP protocol, ensuring correct ordering and offset tracking.\n3. **Topic Management**: Create and manage topics with their partitions, including partition assignment to brokers and metadata distribution.\n4. **Local Offset Tracking**: Store and retrieve consumer group offsets for partitions hosted locally (before replication, this is simply local file storage).\n5. **Coordination Participation**: Work with the cluster coordinator (which may be a designated broker) for leader election and metadata synchronization.\n\nThe scope of this component in Milestone 1 is intentionally bounded: we implement a **single broker** that can manage multiple topics and partitions locally, without yet handling replication or distributed coordination. This allows learners to solidify the core log storage and request handling patterns before adding complexity.\n\n### 5.2 Mental Model: The Immutable Ledger\n\nImagine a traditional **bank ledger book** used to record transactions in chronological order. Each page represents a **segment** of the log, and each line entry is a **record** with a sequential number (offset). The ledger has these key properties:\n\n- **Append-Only**: You never erase or modify past entries; you only add new entries at the end. If a mistake occurs, you add a correction entry rather than changing the original.\n- **Immutable History**: Once written, entries become permanent history. This simplifies concurrency—readers can access any part of the ledger without blocking writers.\n- **Sequential Offsets**: Each entry gets a monotonically increasing number (0, 1, 2...), which serves as a unique identifier and position indicator within that specific ledger.\n- **Partitioned Ledgers**: Instead of one giant ledger for all transactions, the bank maintains separate ledger books for different account types (checking, savings). This is **partitioning**—each ledger (partition) maintains its own independent sequence.\n\nIn our system, each **partition** is exactly such a ledger. The broker's primary job is to **guard these ledgers**: ensure writes are appended correctly in order, assign proper offsets, and allow readers to efficiently locate entries by their offset number.\n\n> **Design Insight**: The immutable log abstraction is the core innovation of Kafka-like systems. By treating messages as an ordered, append-only sequence rather than transient queues, we gain durability, replayability, and strong ordering guarantees that enable event sourcing and stream processing.\n\n### 5.3 Public Interface (APIs)\n\nThe broker exposes its functionality through a binary RPC protocol over TCP. While the full wire protocol will be detailed in Section 9, we define the logical API methods here. Clients (producers, consumers, administrative tools) interact with the broker through these request/response pairs.\n\n#### Core Broker RPC Methods\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `CreateTopic` | `name` (string), `partitionCount` (int), `replicationFactor` (int), `config` (map[string]string) | `error` | Creates a new topic with the specified number of partitions. In Milestone 1, `replicationFactor` is ignored (set to 1). Initializes empty logs for each partition and updates cluster metadata. |\n| `Produce` | `topic` (string), `partition` (int), `records` ([]`Record`), `requiredAcks` (int), `timeout` (int32) | `baseOffset` (int64), `error` | Appends a batch of records to the specified partition's log. Returns the offset assigned to the first record in the batch. `requiredAcks` controls durability (0=no ack, 1=leader ack, -1=all ISR ack). |\n| `Fetch` | `topic` (string), `partition` (int), `offset` (int64), `maxBytes` (int32) | `records` ([]`Record`), `error` | Retrieves records from the partition log starting at `offset`. Returns up to `maxBytes` worth of records. If the requested offset is beyond the log end, waits for new data up to a timeout. |\n| `GetOffsets` | `topic` (string), `partition` (int), `time` (int64) | `offsets` ([]int64), `error` | Returns offset metadata for a partition. `time` can be -1 (earliest), -2 (latest), or a timestamp. Used by consumers to find starting positions. |\n| `CommitOffsets` | `group` (string), `topic` (string), `partition` (int), `offset` (int64), `metadata` (string) | `error` | Commits a consumer group's offset for a specific partition. Stores the offset locally (later, in an internal `__consumer_offsets` topic). |\n| `FetchOffsets` | `group` (string), `topic` (string), `partition` (int) | `offset` (int64), `metadata` (string), `error` | Retrieves the last committed offset for a consumer group and partition. |\n\n#### Internal Management APIs (Used by Other Brokers/Coordinators)\n\nThese methods are not directly called by external clients but are used during replication and coordination (Milestone 4).\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `LeaderForPartition` | `topic` (string), `partition` (int) | `brokerID` (int), `error` | Returns the broker ID currently acting as leader for the partition. Used by clients for request routing. |\n| `UpdateISR` | `topic` (string), `partition` (int), `isr` ([]int) | `error` | Updates the In-Sync Replica set for a partition. Called by the controller after replication changes. |\n| `FetchReplica` | `topic` (string), `partition` (int), `offset` (int64), `maxBytes` (int32) | `records` ([]`Record`), `error` | Similar to `Fetch` but used by follower brokers to replicate data from the leader. May include additional replication metadata. |\n\n#### State Transition Table for Partition Leadership\n\nAs the broker manages partitions, each partition can be in different states regarding leadership and replication. This state machine becomes relevant in Milestone 4 but is introduced here for completeness.\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|-------|------------|---------------|\n| `Offline` | `CreateTopic` command received | `Online` (Follower) | Initialize empty log, set `LeaderBrokerID` to self, set `ISR` = [self], start log appender |\n| `Online` (Leader) | Broker receives `LeaderAndIsr` request with leader = self | `Online` (Leader) | Begin accepting produce requests, start tracking ISR, initialize high watermark to LEO |\n| `Online` (Leader) | Broker receives `LeaderAndIsr` request with leader != self | `Online` (Follower) | Stop accepting produce requests, start fetching from new leader, clear ISR list |\n| `Online` (Follower) | Fetched offset catches up to leader's LEO | `Online` (Follower in ISR) | Broker adds itself to leader's ISR via `UpdateISR` request |\n| Any `Online` state | Broker shutdown initiated | `Offline` | Close log files, persist final offsets, notify coordinator |\n\n### 5.4 Internal Behavior: Log Management\n\nThe broker's most critical internal subsystem is the **log manager**, which orchestrates the physical storage and retrieval of records for all partitions. Its algorithm ensures atomic appends, efficient reads, and proper log rolling.\n\n#### Data Structures for Log Management\n\nThe broker maintains the following core in-memory structures (as defined in Section 4):\n\n| Structure | Field | Type | Description |\n|-----------|-------|------|-------------|\n| `Server` | `topics` | `map[string]*Topic` | Registry of all topics known to this broker, keyed by topic name. |\n| `Server` | `logManager` | `*LogManager` | Central coordinator for all partition logs, handling segment rolling and cleanup. |\n| `Topic` | `Name` | `string` | Unique topic identifier. |\n| `Topic` | `Partitions` | `[]Partition` | List of partitions belonging to this topic. |\n| `Partition` | `Topic` | `string` | Topic name this partition belongs to. |\n| `Partition` | `ID` | `int` | Partition number within the topic (0-indexed). |\n| `Partition` | `LeaderBrokerID` | `int` | ID of broker currently leading this partition (self for local partitions). |\n| `Partition` | `Log` | `Log` | The actual log instance containing segments and current offset. |\n| `Partition` | `HighWatermark` | `int64` | Offset up to which all ISR replicas have replicated (for consumers). |\n| `Partition` | `LogEndOffset` | `int64` | The offset of the next message to be appended (LEO). |\n| `Log` | `Segments` | `[]LogSegment` | Ordered list of log segment files, from oldest to newest. |\n| `Log` | `CurrentOffset` | `int64` | The next offset to be assigned (equal to LEO). |\n| `Log` | `BaseDir` | `string` | Directory path where segment files are stored. |\n| `LogSegment` | `BaseOffset` | `int64` | Offset of the first record in this segment. |\n| `LogSegment` | `DataFile` | `string` | Path to the `.log` file containing record batches. |\n| `LogSegment` | `IndexFile` | `string` | Path to the `.index` file mapping offsets to byte positions. |\n| `LogSegment` | `SizeBytes` | `int64` | Current size of the data file in bytes. |\n\n#### Algorithm: Appending Records to a Partition Log\n\nWhen a `Produce` request arrives for a valid partition, the broker executes this append algorithm:\n\n1. **Validate Request**: Check that the partition exists, the broker is the leader (for now, assume true), and the request size is within configured limits.\n2. **Assign Offsets**: For each record in the incoming batch, assign a sequential offset starting from the partition's `LogEndOffset`. The first record gets offset = `LogEndOffset`, the second gets `LogEndOffset + 1`, etc.\n3. **Serialize Batch**: Encode the records along with their assigned offsets, timestamps, and headers into the binary **RecordBatch** format (see Section 9.2).\n4. **Append to Active Segment**: Write the serialized bytes to the current active segment's data file. The write is performed using the `WAL.Append` method which ensures atomic appends.\n5. **Update Index**: For every N records (configurable `IndexInterval`), add an entry to the segment's sparse index mapping the offset delta to the byte position in the data file.\n6. **Update In-Memory State**: Increment the partition's `LogEndOffset` by the number of records appended. If `requiredAcks` is 1 or -1, also update the `HighWatermark` (for now, set equal to `LogEndOffset`).\n7. **Conditional Sync**: If `requiredAcks` is -1 (all ISR), call `fsync` on the data file to flush OS buffers to disk. For acks=1, we may defer sync for performance.\n8. **Roll Segment if Needed**: If the active segment's `SizeBytes` exceeds `SegmentMaxBytes` (e.g., 1 GB), close it and create a new segment with `BaseOffset` = the new `LogEndOffset`.\n9. **Return Response**: Send the `baseOffset` (the offset assigned to the first record) back to the producer.\n\n#### Algorithm: Reading Records from a Partition Log\n\nWhen a `Fetch` request arrives, the broker must efficiently locate and return records:\n\n1. **Validate Offset**: Check that the requested offset is between the partition's earliest offset (base offset of oldest segment) and `LogEndOffset`. If offset = `LogEndOffset`, optionally wait for new data.\n2. **Locate Segment**: Perform binary search on the partition's `Segments` slice to find the segment where `segment.BaseOffset <= targetOffset < nextSegment.BaseOffset`.\n3. **Consult Index**: Load the segment's index (if not already in memory). Use `Index.FindEntry` to find the largest index entry with offset <= targetOffset, giving a byte position in the data file.\n4. **Seek and Scan**: Open the data file, seek to the byte position, and read forward until reaching `targetOffset`. Since records are stored in batches, we may need to scan through a batch to find the exact starting record.\n5. **Collect Records**: Read sequential records until either hitting `maxBytes` limit, reaching end of segment, or exceeding a maximum record count.\n6. **Return Results**: Package the records into a `FetchResponse`. Include the `HighWatermark` so consumers know the safe read limit.\n\n#### Log Segment Rolling and Cleanup\n\nThe log manager periodically checks segments and performs maintenance:\n\n1. **Size-Based Rolling**: After each append, if the active segment exceeds `SegmentMaxBytes`, it is rolled.\n2. **Time-Based Rolling**: A background thread checks segments older than `SegmentRollTime` (e.g., 7 days) and rolls the active segment if it's too old.\n3. **Deletion Policy**: For simplicity, we implement a **retention-based cleanup**. A background task runs every 5 minutes:\n   - For each partition, list segments sorted by `BaseOffset`.\n   - If total log size exceeds `RetentionBytes` or the oldest segment's timestamp is older than `RetentionTime`, delete the oldest segment file and its index.\n   - Update the `Segments` slice accordingly.\n\n> **Design Insight**: Segment-based storage (rather than one giant file per partition) enables efficient old data deletion and parallel I/O. The sparse index trades minor CPU overhead during reads for significant disk space savings, as we only index every N records.\n\n### 5.5 ADR: Log Storage Backend\n\n> **Decision: File-Based Append-Only Segments with Sparse Indexing**\n>\n> - **Context**: We need a persistent storage engine for partition logs that provides high-throughput sequential writes, efficient random reads by offset, and easy deletion of old data. The solution must be simple to implement and understand for educational purposes while being performant enough for the core use case.\n> - **Options Considered**:\n>   1. **Simple file-based append-only logs with custom sparse indexing** (Kafka's approach)\n>   2. **Embedded key-value store** (e.g., LevelDB/RocksDB, SQLite)\n>   3. **Memory-mapped files with byte-addressable offset indexing**\n> - **Decision**: Option 1 — file-based append-only logs with sparse indexing.\n> - **Rationale**:\n>   - **Conceptual Simplicity**: The mental model of \"segments as files\" maps directly to the immutable ledger analogy, making it easier for learners to debug and reason about.\n>   - **Predictable Performance**: Sequential disk writes are the fastest possible I/O pattern, and reads are mostly sequential scans after index lookup. This matches Kafka's proven performance profile.\n>   - **Explicit Control**: We avoid the complexity and black-box nature of an embedded database. Learners understand exactly when data is persisted, indexed, and deleted.\n>   - **Ease of Inspection**: Log segments are plain files viewable with hex editors or custom tools, aiding debugging and education.\n> - **Consequences**:\n>   - **Positive**: Excellent write throughput, straightforward implementation, easy to add compression or encryption per segment.\n>   - **Negative**: Manual management of file handles and indexes required. Need to implement our own recovery logic after crashes. No built-in transactions—we must ensure atomic batch writes ourselves.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| **File-based segments** | Maximum write throughput, simple mental model, easy inspection and debugging, matches Kafka's design for direct learning | Manual file management, need to implement own indexing and recovery, concurrent access requires careful locking | **Yes** - best for educational goals and performance |\n| **Embedded KV store** | Built-in recovery, concurrent access handled, potentially simpler API, good for prototyping | Black-box behavior, harder to debug, often optimized for random writes not sequential appends, adds dependency | No - obscures learning objectives |\n| **Memory-mapped files** | Excellent read performance, OS handles paging, simple offset-to-address arithmetic | Complex to handle file growth, portability concerns, risk of SIGBUS on truncated files, still need custom index | No - increased complexity without sufficient benefit |\n\n### 5.6 Common Pitfalls\n\n⚠️ **Pitfall: Forgetting to fsync or syncing too often**\n\n**Description**: After writing records to a file, developers may omit calling `file.Sync()` (or equivalent) to flush OS buffers to disk. Conversely, calling sync after every single write destroys throughput.\n\n**Why it's wrong**: Without fsync, a broker crash can lose recently \"acknowledged\" messages (if acks=1 or all), violating durability guarantees. With excessive fsync, throughput drops to disk seek speed (often < 1000 writes/sec).\n\n**How to fix**: Implement a **sync policy** based on `requiredAcks` and batch size. For acks=1, sync periodically (e.g., every 100ms or 1MB of data). For acks=-1 (all ISR), sync immediately after each write. Use the `WAL.Append` method's `sync` parameter to control this.\n\n---\n\n⚠️ **Pitfall: Incorrect offset gaps or reuse**\n\n**Description**: Assigning non-sequential offsets (e.g., skipping numbers) or reusing offsets after a crash recovery.\n\n**Why it's wrong**: Consumers rely on strictly monotonic offsets for progress tracking. Gaps confuse offset commit logic and may cause infinite loops. Reused offsets cause duplicate processing and break idempotency.\n\n**How to fix**: Always derive new offsets from `LogEndOffset`, which must be persisted (in the segment file names or a checkpoint file). On startup, scan segments to reconstruct the exact `LogEndOffset`. Never decrement it.\n\n---\n\n⚠️ **Pitfall: Poor key hashing leading to partition skew**\n\n**Description**: Using a naive hash function (like Java's `Object.hashCode()` equivalent) or not handling null keys properly, causing uneven distribution across partitions.\n\n**Why it's wrong**: Skewed partition distribution defeats the purpose of partitioning—one partition becomes a hotspot while others are underutilized, limiting scalability.\n\n**How to fix**: Use a proven hash function (e.g., MurmurHash2/3) and always apply a modulo operation with the number of partitions. For null keys, use a round-robin or random partition assignment to spread load.\n\n---\n\n⚠️ **Pitfall: Not handling concurrent reads and writes**\n\n**Description**: Allowing simultaneous read and write operations on the same log segment without proper synchronization, leading to corrupted reads or panics.\n\n**Why it's wrong**: Concurrent reads may see partially written records or incorrect offsets. File descriptors may be closed while in use.\n\n**How to fix**: Use a **reader-writer lock** per partition. Multiple fetches can read concurrently, but an append acquires an exclusive lock. For higher throughput, consider copy-on-write semantics for the active segment.\n\n---\n\n⚠️ **Pitfall: Infinite memory growth from unclosed segments**\n\n**Description**: Keeping all segment file handles and index data in memory forever, causing memory exhaustion as the log grows.\n\n**Why it's wrong**: A long-running broker will eventually run out of memory and crash, especially with many partitions.\n\n**How to fix**: Implement an **LRU cache** for index data. Close file handles for inactive segments (not the active one). Reopen files on demand when reads target old segments.\n\n---\n\n⚠️ **Pitfall: Ignoring disk space exhaustion**\n\n**Description**: Appending records without checking available disk space, causing write failures that may corrupt the log.\n\n**Why it's wrong**: When disk is full, writes may partially succeed, leaving torn records. The broker might crash inconsistently.\n\n**How to fix**: Before each append, check free space against a configurable threshold (e.g., 1GB). If below threshold, reject writes with an appropriate error. Implement alerting for operators.\n\n### 5.7 Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Log Storage | Plain files with `os.File`, manual indexing | Memory-mapped files (`mmap`) with atomic appends |\n| Concurrency Control | `sync.RWMutex` per partition | Lock-free ring buffer for appends, RCU for reads |\n| Serialization | Custom binary format with `encoding/binary` | Protocol Buffers or FlatBuffers for schema evolution |\n| File Operations | Direct `os.Open`/`Close` per operation | Pooled file descriptors with reference counting |\n\n#### B. Recommended File/Module Structure\n\n```\nbuild-your-own-kafka/\n├── cmd/\n│   ├── broker/\n│   │   └── main.go                 # Broker entry point\n│   └── client/                     # Producer/consumer CLI tools\n├── internal/\n│   ├── broker/\n│   │   ├── server.go              # Main Server struct and lifecycle\n│   │   ├── handler.go             # Request handling (Produce, Fetch, etc.)\n│   │   └── log_manager.go         # LogManager orchestrating partitions\n│   ├── storage/\n│   │   ├── log.go                 # Partition Log implementation\n│   │   ├── segment.go             # LogSegment with index\n│   │   ├── wal.go                 # Write-ahead log wrapper (provided below)\n│   │   └── index.go               # Sparse index implementation\n│   ├── types/\n│   │   ├── topic.go               # Topic and Partition structs\n│   │   ├── record.go              # Record and RecordBatch\n│   │   └── metadata.go            # Broker, ClusterMetadata\n│   └── protocol/\n│       ├── request.go             # Request structs and parsing\n│       └── response.go            # Response building\n└── pkg/\n    └── wal/                       # Reusable WAL package (could be external)\n```\n\n#### C. Infrastructure Starter Code: WAL Wrapper\n\nBelow is a complete, ready-to-use Write-Ahead Log wrapper. This handles atomic appends with optional fsync, which is crucial for durability. Learners can import this directly.\n\n```go\n// internal/storage/wal.go\npackage storage\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"sync\"\n)\n\n// WAL implements a simple write-ahead log with atomic appends.\ntype WAL struct {\n    file     *os.File\n    filePath string\n    mu       sync.Mutex\n    offset   int64 // current write offset in bytes\n}\n\n// OpenWAL opens or creates a WAL file at the given path.\nfunc OpenWAL(path string) (*WAL, error) {\n    file, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0644)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to open WAL file: %w\", err)\n    }\n    \n    stat, err := file.Stat()\n    if err != nil {\n        file.Close()\n        return nil, fmt.Errorf(\"failed to stat WAL file: %w\", err)\n    }\n    \n    return &WAL{\n        file:     file,\n        filePath: path,\n        offset:   stat.Size(),\n    }, nil\n}\n\n// Append writes data to the WAL with optional fsync.\n// Returns the file offset at which data was written.\nfunc (w *WAL) Append(data []byte, sync bool) (int64, error) {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    offset := w.offset\n    n, err := w.file.Write(data)\n    if err != nil {\n        return 0, fmt.Errorf(\"write failed: %w\", err)\n    }\n    if n != len(data) {\n        return 0, fmt.Errorf(\"short write: %d != %d\", n, len(data))\n    }\n    \n    w.offset += int64(n)\n    \n    if sync {\n        if err := w.file.Sync(); err != nil {\n            // We still wrote the data, but sync failed.\n            // The file system may have buffered it; we can't guarantee durability.\n            return offset, fmt.Errorf(\"fsync failed: %w\", err)\n        }\n    }\n    \n    return offset, nil\n}\n\n// ReadAll reads all entries from the WAL (for recovery).\nfunc (w *WAL) ReadAll() ([][]byte, error) {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    if _, err := w.file.Seek(0, 0); err != nil {\n        return nil, fmt.Errorf(\"seek failed: %w\", err)\n    }\n    \n    var entries [][]byte\n    buffer := make([]byte, 4096)\n    for {\n        n, err := w.file.Read(buffer)\n        if n > 0 {\n            // In a real implementation, you'd need to parse record boundaries.\n            // For simplicity, we assume each write is a complete entry.\n            entry := make([]byte, n)\n            copy(entry, buffer[:n])\n            entries = append(entries, entry)\n        }\n        if err != nil {\n            if err.Error() == \"EOF\" {\n                break\n            }\n            return entries, fmt.Errorf(\"read error: %w\", err)\n        }\n    }\n    \n    return entries, nil\n}\n\n// CurrentOffset returns the current write offset in bytes.\nfunc (w *WAL) CurrentOffset() int64 {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    return w.offset\n}\n\n// Close closes the WAL file.\nfunc (w *WAL) Close() error {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    return w.file.Close()\n}\n```\n\n#### D. Core Logic Skeleton Code\n\nBelow are skeleton implementations for the key components that learners should fill in. Each TODO corresponds to a step in the algorithms described in Section 5.4.\n\n**Log Manager** (orchestrates all partition logs):\n\n```go\n// internal/storage/log.go\npackage storage\n\nimport (\n    \"fmt\"\n    \"path/filepath\"\n    \"sync\"\n)\n\n// Log represents a partition's log composed of multiple segments.\ntype Log struct {\n    Segments      []LogSegment\n    CurrentOffset int64\n    BaseDir       string\n    config        LogConfig\n    mu            sync.RWMutex\n}\n\n// LogConfig holds configuration for log behavior.\ntype LogConfig struct {\n    SegmentMaxBytes int64\n    IndexInterval   int\n    RetentionBytes  int64\n}\n\n// Append writes a batch of records to the log and returns the starting offset.\nfunc (l *Log) Append(records []*Record) (int64, error) {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    // TODO 1: Validate input - ensure records slice is not empty\n    \n    // TODO 2: Assign sequential offsets starting from CurrentOffset\n    //         For each record, set Record.Offset = CurrentOffset + i\n    \n    // TODO 3: Serialize records into RecordBatch binary format\n    //         Use encodeRecordBatch() helper (to be implemented)\n    \n    // TODO 4: Get active segment (create first segment if none exists)\n    //         activeSegment := l.getOrCreateActiveSegment()\n    \n    // TODO 5: Write batch bytes to segment data file using WAL.Append\n    //         Call with sync=false for now (we'll add sync policy later)\n    \n    // TODO 6: Update segment size and add index entry if needed\n    //         if recordsWritten % IndexInterval == 0, add index entry\n    \n    // TODO 7: Update CurrentOffset by adding len(records)\n    \n    // TODO 8: Check if segment should be rolled (size > SegmentMaxBytes)\n    //         if yes, call l.rollSegment()\n    \n    // TODO 9: Return the starting offset (CurrentOffset before increment)\n    \n    return 0, fmt.Errorf(\"not implemented\")\n}\n\n// Read fetches records starting from the given offset.\nfunc (l *Log) Read(startOffset int64, maxBytes int32) ([]*Record, error) {\n    l.mu.RLock()\n    defer l.mu.RUnlock()\n    \n    // TODO 1: Validate startOffset is within range [oldestSegment.BaseOffset, CurrentOffset]\n    \n    // TODO 2: Locate segment containing startOffset using binary search\n    //         segment := l.findSegment(startOffset)\n    \n    // TODO 3: Load segment index (lazy load if not in memory)\n    \n    // TODO 4: Use index to find approximate position in data file\n    //         position := segment.Index.FindEntry(offsetDelta)\n    \n    // TODO 5: Open data file, seek to position, and scan forward to startOffset\n    \n    // TODO 6: Read records sequentially until hitting maxBytes or segment end\n    \n    // TODO 7: Decode RecordBatch(es) into []*Record\n    \n    // TODO 8: Return records (may be empty if startOffset == CurrentOffset)\n    \n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// getOrCreateActiveSegment returns the last segment if it has space, else creates new.\nfunc (l *Log) getOrCreateActiveSegment() (*LogSegment, error) {\n    // TODO 1: If Segments is empty, create first segment with BaseOffset = CurrentOffset\n    \n    // TODO 2: Get last segment, if its SizeBytes < config.SegmentMaxBytes, return it\n    \n    // TODO 3: Otherwise, roll a new segment\n    return l.rollSegment()\n}\n\n// rollSegment closes the current active segment and creates a new one.\nfunc (l *Log) rollSegment() (*LogSegment, error) {\n    // TODO 1: If there's an active segment, close its files\n    \n    // TODO 2: Create new segment with BaseOffset = CurrentOffset\n    \n    // TODO 3: Generate filenames: {BaseOffset}.log and {BaseOffset}.index\n    \n    // TODO 4: Open data file and index file\n    \n    // TODO 5: Append new segment to l.Segments\n    \n    // TODO 6: Return the new segment\n    return nil, fmt.Errorf(\"not implemented\")\n}\n```\n\n**Broker Request Handler**:\n\n```go\n// internal/broker/handler.go\npackage broker\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/yourusername/byok/internal/protocol\"\n    \"github.com/yourusername/byok/internal/storage\"\n    \"github.com/yourusername/byok/internal/types\"\n)\n\n// HandleProduce processes a Produce request from a client.\nfunc (s *Server) HandleProduce(req *protocol.ProduceRequest) (*protocol.ProduceResponse, error) {\n    // TODO 1: Validate topic exists (look up in s.topics map)\n    \n    // TODO 2: Validate partition exists and this broker is leader\n    //         partition := topic.Partitions[req.Partition]\n    \n    // TODO 3: Convert protocol.RecordBatch to []*types.Record\n    \n    // TODO 4: Call partition.Log.Append(records)\n    \n    // TODO 5: Handle requiredAcks:\n    //         - If 0: return immediately with success\n    //         - If 1: wait for append to complete (already done), return\n    //         - If -1: (future) wait for ISR replication\n    \n    // TODO 6: Build response with base offset and any errors\n    \n    // TODO 7: Update partition.HighWatermark (for now = LogEndOffset)\n    \n    return &protocol.ProduceResponse{\n        BaseOffset: -1, // placeholder\n    }, fmt.Errorf(\"not implemented\")\n}\n\n// HandleFetch processes a Fetch request from a consumer.\nfunc (s *Server) HandleFetch(req *protocol.FetchRequest) (*protocol.FetchResponse, error) {\n    // TODO 1: Validate topic and partition\n    \n    // TODO 2: Check requested offset against partition.HighWatermark\n    //         Consumers should not read beyond HighWatermark\n    \n    // TODO 3: Call partition.Log.Read(startOffset, maxBytes)\n    \n    // TODO 4: Convert []*types.Record to protocol.RecordBatch for response\n    \n    // TODO 5: Include HighWatermark in response so consumer knows safe read limit\n    \n    return &protocol.FetchResponse{}, fmt.Errorf(\"not implemented\")\n}\n```\n\n#### E. Language-Specific Hints\n\n- **File Sync**: Use `file.Sync()` for durability. On Linux, this maps to the `fsync()` system call.\n- **Binary Encoding**: Use `encoding/binary` with `binary.BigEndian` for network-portable format (Kafka uses big-endian).\n- **Concurrent Maps**: Use `sync.RWMutex` to protect the `topics` map. Consider `sync.Map` for read-heavy workloads once initialized.\n- **Path Operations**: Always use `filepath.Join()` instead of string concatenation for cross-platform compatibility.\n- **Error Handling**: Use `fmt.Errorf(\"... %w\", err)` to wrap errors with context for debugging.\n- **Clean Resource Management**: Implement `Close()` methods for all resources (files, network connections) and use `defer` appropriately.\n\n#### F. Milestone Checkpoint\n\nAfter implementing the broker core and log storage:\n\n1. **Run the test suite**:\n   ```\n   go test ./internal/storage/... -v\n   go test ./internal/broker/... -v\n   ```\n\n2. **Expected output**: All tests should pass, demonstrating:\n   - Log appends return sequential offsets\n   - Reading from an offset returns correct records\n   - Segment rolling occurs when size limit reached\n   - Topic creation creates appropriate directory structure\n\n3. **Manual verification**:\n   - Start the broker: `go run cmd/broker/main.go --data-dir /tmp/byok`\n   - Use a simple CLI producer (to be built in Milestone 2) or `netcat` to send a Produce request:\n     ```\n     echo -n \"test message\" | nc localhost 9092\n     ```\n   - Check that a segment file appears in `/tmp/byok/topics/test/0/`\n   - Verify offset tracking by reading the log via a simple dump utility.\n\n4. **Signs of trouble**:\n   - **\"No such file or directory\"**: Check that `BaseDir` is created with `os.MkdirAll`.\n   - **Offsets not sequential**: Ensure `CurrentOffset` is updated atomically with the write.\n   - **High memory usage**: You might be keeping all segment files open; implement LRU closing.\n   - **Test hangs**: Check for deadlocks—use `go test -race` to detect data races.\n\n\n## 6. Component Design: Producer\n\n> **Milestone(s):** 2 (Producer)\n\n### 6.1 Responsibility and Scope\n\nThe **Producer** is a client library that applications use to publish (write) records to topics. Its primary responsibility is to take messages from the application, efficiently route them to the correct partition leader, and ensure they are durably written according to the configured acknowledgment semantics. It shields the application from the complexities of the distributed system, such as broker discovery, leader changes, network retries, and batching for performance.\n\nThe Producer's scope encompasses:\n1.  **Message Serialization:** Converting application-provided keys, values, and headers into the binary `Record` and `RecordBatch` format understood by brokers.\n2.  **Topic Metadata Management:** Discovering which broker leads each partition of a topic, and refreshing this information when leadership changes.\n3.  **Partition Selection:** Determining the target partition for a message, using key-based hashing for ordering guarantees or round-robin for load distribution.\n4.  **Batching and Accumulation:** Grouping multiple messages destined for the same partition leader into single network requests to amortize overhead and achieve high throughput.\n5.  **Reliability Semantics:** Implementing configurable acknowledgment levels (0, 1, `all`) and corresponding retry logic with exponential backoff to provide at-least-once delivery guarantees.\n6.  **Connection Management:** Maintaining efficient, persistent network connections to broker leaders and handling connection failures.\n\nCrucially, the Producer is stateless regarding message content; it does not store messages durably itself. Its state is limited to in-flight message batches, metadata caches, and retry counters.\n\n### 6.2 Mental Model: The Postal Batch Sorter\n\nImagine you run a large postal service. Every day, individuals (applications) bring you millions of letters (messages) to send. Each letter has a destination city and street (topic and key). Your goal is to deliver them reliably and quickly, but sending each letter individually by courier is prohibitively expensive.\n\nYou set up a system of sorting bins:\n1.  **Sorting by Destination:** You first look up the correct regional sorting facility for each city (broker leader for a partition). Letters for the same facility go into the same large bin (`RecordBatch`).\n2.  **Batch Dispatch:** Once a bin is full or a timer expires, you seal it and send the entire bin via a single truck (network request) to the regional facility. This is far more efficient than sending individual letters.\n3.  **Delivery Receipts:** For important letters, you request a signed receipt (`ack`). If the receipt doesn't arrive, you put a copy of the letter in a new bin and try again (retry). For less critical mail, you might just trust the postal system (`acks=0`).\n4.  **Route Changes:** If a regional facility tells you they're no longer handling a certain street, you update your destination map (metadata) and re-route future letters accordingly.\n\nThis model captures the essence of the producer: **accumulation by destination, batch dispatch, and managed reliability.** The \"postal service\" (producer) handles the complexity so the \"individual\" (application) has a simple interface: just hand over the letter.\n\n### 6.3 Public Interface\n\nThe Producer exposes a simple, synchronous or asynchronous API for sending messages, along with configuration structs to control its behavior.\n\n**Configuration (`ProducerConfig`):**\nThe behavior of the producer is finely tuned via a configuration struct passed at creation time.\n\n| Field | Type | Description | Default |\n| :--- | :--- | :--- | :--- |\n| `BootstrapServers` | `[]string` | Initial list of broker addresses (host:port) for metadata discovery. | (required) |\n| `ClientID` | `string` | Logical name for this producer client, used in logs and server-side quotas. | `\"byok-producer\"` |\n| `Acks` | `AcksLevel` | Durability guarantee: `AcksNone` (0), `AcksLeader` (1), or `AcksAll` (-1). | `AcksLeader` |\n| `Retries` | `int` | Maximum number of retries for a failed batch before giving up. | `3` |\n| `RetryBackoffMs` | `int` | Base milliseconds to wait before the first retry. Exponential backoff applies. | `100` |\n| `BatchSize` | `int` | Maximum number of bytes to include in a batch before sending. | `16384` (16KB) |\n| `LingerMs` | `int` | Milliseconds to wait for additional messages to fill a batch before sending. | `5` |\n| `BufferMemory` | `int64` | Total bytes of memory the producer can use for unsent batches. | `33554432` (32MB) |\n| `Partitioner` | `Partitioner` | Class implementing partition selection logic (e.g., `HashPartitioner`, `RoundRobinPartitioner`). | `HashPartitioner` |\n| `CompressionType` | `CompressionType` | Compression algorithm for record batches (`none`, `gzip`, `snappy`). | `CompressionNone` |\n\n**Core API Methods:**\nThe primary interaction is through a `Producer` struct with the following methods.\n\n| Method | Parameters | Returns | Description |\n| :--- | :--- | :--- | :--- |\n| `Send` | `ctx context.Context`, `record *Record` | `(int64, error)` | Synchronously sends a single record. Blocks until the record is acknowledged according to the `Acks` setting, or until `ctx` times out/cancels. Returns the partition and offset the record was assigned, or an error. |\n| `SendAsync` | `record *Record`, `callback func(int64, error)` | `error` | Asynchronously sends a record. The callback is invoked when the send succeeds or fails. Returns immediately unless the internal buffers are full (`ErrBufferFull`). |\n| `Flush` | `ctx context.Context` | `error` | Blocks until all currently buffered (unsent) records are transmitted and acknowledged. Used for graceful shutdown or to ensure delivery at specific points. |\n| `Close` | `ctx context.Context` | `error` | Gracefully shuts down the producer. Flushes any buffered records, waits for pending callbacks, and closes all network connections. |\n\n**Example Usage:**\n```go\n// Creating a producer\nconfig := &ProducerConfig{\n    BootstrapServers: []string{\"broker1:9092\", \"broker2:9092\"},\n    ClientID:         \"my-app\",\n    Acks:             AcksAll,\n    Retries:          5,\n}\nproducer, err := NewProducer(config)\nif err != nil { ... }\ndefer producer.Close(context.Background())\n\n// Synchronous send\noffset, err := producer.Send(ctx, &Record{\n    Topic: \"orders\",\n    Key:   []byte(\"order-123\"),\n    Value: []byte(`{\"status\": \"shipped\"}`),\n})\nif err != nil { ... }\nfmt.Printf(\"Record written to partition %d at offset %d\\n\", partition, offset)\n```\n\n### 6.4 Internal Behavior: Batching and Sending\n\nInternally, the producer is a complex state machine orchestrating metadata, accumulation, and network I/O across multiple goroutines. Its core algorithm can be broken down into the following steps for a synchronous `Send`.\n\n**1. Metadata Fetch and Partition Selection:**\n   When a record arrives for a topic, the producer first checks its local metadata cache.\n   1. If the topic metadata is unknown or stale (e.g., last refresh > `metadata.max.age.ms`), it sends a `MetadataRequest` to one of the bootstrap brokers. The response populates the cache with the list of partitions and their current leader brokers.\n   2. Using the configured `Partitioner` (e.g., `HashPartitioner`), the producer computes the target partition ID. For a non-nil key, it hashes the key and applies modulus to the partition count. For a nil key, it may use a sticky partition strategy or round-robin.\n   3. The metadata cache provides the `BrokerID` (and network address) of the leader for that partition.\n\n**2. Record Accumulation (Batching):**\n   The producer maintains a map of in-memory buffers keyed by `TopicPartition` (Topic + Partition ID). Each buffer holds a `RecordBatch` under construction.\n   1. The serialized record is appended to the `RecordBatch` for its target `TopicPartition`.\n   2. The batch is not immediately sent. It waits until either:\n      - The total byte size of the batch exceeds `BatchSize`.\n      - The `LingerMs` timer for that batch expires.\n      - The `Flush` or `Close` method is called.\n   This batching is critical for throughput, as it amortizes the fixed cost of a network round-trip and disk I/O over many records.\n\n**3. Batch Dispatch and In-Flight Management:**\n   When a batch is ready to send, it is handed off to a **Sender** goroutine.\n   1. The Sender manages a pool of network connections, one per broker leader. It retrieves or creates a connection to the leader broker for the batch's partition.\n   2. It wraps the `RecordBatch` in a `ProduceRequest` and sends it over the network.\n   3. The batch is moved to an \"in-flight\" map, keyed by `TopicPartition`. This tracks batches awaiting acknowledgment.\n   4. The Sender can multiplex batches for the same broker into a single network request for further efficiency.\n\n**4. Acknowledgment Handling and Retry:**\n   The producer awaits the broker's `ProduceResponse`.\n   1. **Success:** If the response indicates success (and meets the `Acks` level), the in-flight batch is removed. For a synchronous `Send`, the calling goroutine is unblocked with the assigned offset. For `SendAsync`, the user's callback is invoked with the offset.\n   2. **Retriable Error:** Errors like `NotLeaderForPartition`, `NetworkException`, or `Timeout` trigger a retry.\n        a. The batch is re-queued to the accumulator for the same `TopicPartition`.\n        b. The producer's metadata for that topic is marked as stale, forcing a refresh before the next send attempt (to discover the new leader).\n        c. A retry delay is calculated using exponential backoff: `delay = RetryBackoffMs * 2^(attempt)`. The batch is scheduled for re-dispatch after this delay.\n   3. **Fatal Error:** Errors like `InvalidTopic`, `RecordTooLarge`, or exceeding `Retries` cause the batch to fail permanently. The error is reported to the application.\n\n**5. Ordering and Idempotence (Optional):**\n   To guarantee exactly-once semantics and preserve ordering across retries, an idempotent producer assigns a monotonic sequence number per `TopicPartition`. The broker rejects duplicates based on this number. For our educational project, this is an advanced extension, but the design should consider leaving room for it (e.g., a `ProducerID` and `SequenceNumber` field in the `RecordBatch`).\n\nThe following sequence diagram illustrates this flow for a successful send:\n![Producer Send Sequence](./diagrams/diagram-producer-flow.svg)\n\n### 6.5 ADR: Acknowledgment Semantics\n\n> **Decision: Support Three Acknowledgement Levels (0, 1, all)**\n>\n> - **Context:** The producer must offer trade-offs between durability and latency. Different applications have different needs: a logging system may tolerate some data loss for maximum speed, while a financial transaction processor cannot.\n> - **Options Considered:**\n>     1.  **Fire-and-forget (`acks=0`):** The producer sends the message and does not wait for any acknowledgment from the broker.\n>     2.  **Leader acknowledgment (`acks=1`):** The producer waits for the partition leader to have written the record to its local log before considering the send successful.\n>     3.  **Full ISR acknowledgment (`acks=all`):** The producer waits for the record to be written to the local log of *all* in-sync replicas (ISR) before success.\n> - **Decision:** Implement all three levels (`AcksNone`, `AcksLeader`, `AcksAll`) as configurable options.\n> - **Rationale:** This mirrors Apache Kafka's approach and provides a clear, practical spectrum of durability guarantees for learners to experiment with. Implementing all three demonstrates the incremental complexity: `acks=0` is trivial, `acks=1` introduces waiting for a network response, and `acks=all` requires understanding replication and the High Watermark. This graduated complexity is ideal for learning.\n> - **Consequences:** The broker's `HandleProduce` method must implement logic for each `acks` level. `AcksAll` requires the broker to track the High Watermark and may introduce higher latency. The producer must handle potential timeouts for `acks=all` if an ISR replica is slow.\n\n| Option | Pros | Cons | Chosen? |\n| :--- | :--- | :--- | :--- |\n| **`acks=0` (Fire-and-forget)** | Lowest latency, maximum throughput. Simple to implement. | Possible data loss if broker fails before writing. No backpressure signal. | **Yes** – for throughput-critical, loss-tolerant use cases. |\n| **`acks=1` (Leader ack)** | Good balance. Protects against leader process crash (record is on disk). Moderate latency. | Data loss possible if leader crashes *after* ack but before replicas copy the data (failover to a non-replica). | **Yes** – the default balance for many applications. |\n| **`acks=all` (ISR ack)** | Highest durability. Survives `f` broker failures if replication factor > `f+1`. | Highest latency (waits for slowest ISR). Throughput limited by slow replica. Can block if ISR shrinks. | **Yes** – for critical data where loss is unacceptable. |\n\n### 6.6 Common Pitfalls\n\n⚠️ **Pitfall: Blocking on a Full Buffer**\n*   **Description:** The `Send` method blocks indefinitely if the total size of unsent batches (`BufferMemory`) is exceeded and the producer cannot drain batches fast enough (e.g., due to slow network or broker).\n*   **Why it's wrong:** This can cause application threads to hang, leading to a cascading failure. It violates the principle of graceful degradation.\n*   **Fix:** Implement a bounded wait with a timeout in `Send`, or design `SendAsync` to return an `ErrBufferFull` immediately if the buffer is full, allowing the application to apply backpressure or shed load.\n\n⚠️ **Pitfall: Ignoring Leader Changes During Send**\n*   **Description:** A producer caches partition leader metadata. If a leader fails and a new election occurs after the producer has chosen a partition but before it sends the batch, the batch will be sent to the wrong (old leader) broker.\n*   **Why it's wrong:** The send fails with `NotLeaderForPartition`, incurring a retry delay and unnecessary load on the old leader. In a volatile cluster, this can cause a storm of misdirected requests.\n*   **Fix:** On receiving `NotLeaderForPartition` or `UnknownTopicPartition` errors, immediately invalidate the cached metadata for that topic and refresh it before the next retry. The metadata should also be refreshed periodically.\n\n⚠️ **Pitfall: Duplicate Messages on Retry**\n*   **Description:** When a `ProduceRequest` times out at the producer, it's ambiguous whether the broker processed it. A retry may cause the same record to be appended twice to the log.\n*   **Why it's wrong:** Consumers will process duplicate records, breaking at-least-once semantics and potentially causing incorrect application state (e.g., double-charging).\n*   **Fix (Basic):** For `acks=all`, duplicate writes are less likely but still possible on timeout. Educate learners that with retries, the system provides **at-least-once** semantics. Application logic must be idempotent.\n*   **Fix (Advanced):** Implement the idempotent producer with sequence numbers to allow the broker to deduplicate, enabling **exactly-once** semantics in the log.\n\n⚠️ **Pitfall: Poor Key Hashing Leading to Skewed Partitions**\n*   **Description:** Using a naive hash function (like Java's `Object.hashCode()` or Go's default for strings) or applying modulus to a non-power-of-two partition count can lead to uneven distribution of records across partitions.\n*   **Why it's wrong:** Some partitions become hotspots, limiting the overall throughput of the topic and causing uneven load on brokers.\n*   **Fix:** Use a robust, deterministic hash function (like MurmurHash2/3) on the key bytes. When performing modulus, ensure the hash value is non-negative. For nil keys, use a sticky random partitioner that batches records to the same partition for a short time to improve batching efficiency.\n\n### 6.7 Implementation Guidance\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n| :--- | :--- | :--- |\n| **Network I/O** | Synchronous I/O with `net.Dial` and `io.Read/Write`. Simple to understand. | Asynchronous I/O using `goroutines` per connection and channels for batching. Better performance. |\n| **Serialization** | Manual byte slice construction using `binary.Write` and `append`. Clear and direct. | Protocol Buffer definitions (`protobuf`) for request/response formats. More maintainable and extensible. |\n| **Partitioner** | Hash-based using `crc32` or `fnv`. Round-robin with a counter. | Implement the \"sticky partitioner\" from Kafka: fills one batch per partition before moving to the next for better batching. |\n| **Compression** | None initially. | Integrate `compress/gzip` or a third-party `snappy` library for record batches. |\n\n**B. Recommended File/Module Structure**\n\nAdd the following files for the producer client library. It should be a separate package that applications can import.\n```\nproject-root/\n  cmd/producer-cli/           # Optional: example CLI tool for testing\n    main.go\n  internal/producer/          # Producer client library\n    producer.go               # Main Producer struct and public API\n    config.go                 # ProducerConfig and constants\n    accumulator.go            # Batch accumulation logic\n    sender.go                 # Network sender and in-flight management\n    partitioner.go            # HashPartitioner, RoundRobinPartitioner\n    record_batch.go           # RecordBatch serialization format\n    errors.go                 # Producer-specific errors (e.g., ErrBufferFull)\n  internal/protocol/          # Shared request/response structs and serialization\n    produce.go                # ProduceRequest, ProduceResponse\n    metadata.go               # MetadataRequest, MetadataResponse\n    types.go                  # Common types (Record, RecordBatch)\n```\n\n**C. Infrastructure Starter Code**\n\nHere is a complete, ready-to-use `RecordBatch` serialization helper. This handles the binary format for a batch of records, which is the unit of writing for both the producer and the broker's log.\n\n```go\n// internal/protocol/record_batch.go\npackage protocol\n\nimport (\n    \"bytes\"\n    \"encoding/binary\"\n    \"time\"\n)\n\n// RecordBatch represents a batch of records as written to the log.\n// This is a simplified version. A full implementation includes crc, attributes, etc.\ntype RecordBatch struct {\n    BaseOffset      int64\n    PartitionLeaderEpoch int32 // Used for leader epoch tracking (advanced)\n    MagicByte       int8   // Version of the batch format (set to 2)\n    CRC             uint32 // CRC of the batch data (after this field)\n    Attributes      int16  // Bitmask for compression, timestamp type, etc.\n    LastOffsetDelta int32  // Delta from BaseOffset for the last record\n    FirstTimestamp  int64  // Timestamp of the first record\n    MaxTimestamp    int64  // Max timestamp in the batch\n    ProducerID      int64  // For idempotence (-1 for none)\n    ProducerEpoch   int16  // For idempotence\n    BaseSequence    int32  // For idempotence\n    Records         []*Record\n}\n\n// Record is an individual message.\ntype Record struct {\n    Length     int32  // Delta from the start of the batch\n    Attributes int8   // Currently unused\n    TimestampDelta int64 // Delta from FirstTimestamp\n    OffsetDelta int32 // Delta from BaseOffset\n    Key        []byte\n    Value      []byte\n    Headers    []Header\n}\n\ntype Header struct {\n    Key   string\n    Value []byte\n}\n\n// Encode converts the RecordBatch to its on-wire bytes.\nfunc (rb *RecordBatch) Encode() ([]byte, error) {\n    buf := new(bytes.Buffer)\n    // 1. Write the fixed-size header fields.\n    //    Note: We write a placeholder for CRC, compute it later, then overwrite.\n    binary.Write(buf, binary.BigEndian, rb.BaseOffset)\n    binary.Write(buf, binary.BigEndian, rb.PartitionLeaderEpoch)\n    binary.Write(buf, binary.BigEndian, rb.MagicByte)\n    crcPos := buf.Len()\n    binary.Write(buf, binary.BigEndian, uint32(0)) // Placeholder CRC\n    binary.Write(buf, binary.BigEndian, rb.Attributes)\n    binary.Write(buf, binary.BigEndian, rb.LastOffsetDelta)\n    binary.Write(buf, binary.BigEndian, rb.FirstTimestamp)\n    binary.Write(buf, binary.BigEndian, rb.MaxTimestamp)\n    binary.Write(buf, binary.BigEndian, rb.ProducerID)\n    binary.Write(buf, binary.BigEndian, rb.ProducerEpoch)\n    binary.Write(buf, binary.BigEndian, rb.BaseSequence)\n    binary.Write(buf, binary.BigEndian, int32(len(rb.Records)))\n\n    // 2. Encode each record.\n    for _, rec := range rb.Records {\n        // ... (Encode record length, attributes, deltas, key, value, headers)\n        // This is detailed serialization logic.\n    }\n\n    // 3. Compute CRC over the bytes starting from Attributes to the end.\n    data := buf.Bytes()\n    crc := computeCRC(data[crcPos+4:]) // Skip placeholder CRC\n    // Overwrite the placeholder CRC.\n    binary.BigEndian.PutUint32(data[crcPos:], crc)\n    return data, nil\n}\n\n// DecodeRecordBatch parses bytes into a RecordBatch.\nfunc DecodeRecordBatch(data []byte) (*RecordBatch, error) {\n    // Implementation omitted for brevity.\n    // Reads fields in order, validates CRC, decodes records.\n    return &RecordBatch{}, nil\n}\n\nfunc computeCRC(data []byte) uint32 {\n    // Use a CRC32 implementation (e.g., github.com/klauspost/crc32)\n    return 0 // Placeholder\n}\n```\n\n**D. Core Logic Skeleton Code**\n\n**1. Accumulator (`accumulator.go`):** Manages batching per TopicPartition.\n\n```go\n// internal/producer/accumulator.go\npackage producer\n\nimport (\n    \"sync\"\n    \"time\"\n    \"github.com/yourusername/byok/internal/protocol\"\n)\n\ntype TopicPartition struct {\n    Topic     string\n    Partition int32\n}\n\ntype Accumulator struct {\n    config *ProducerConfig\n    batches map[TopicPartition]*RecordBatch\n    mu      sync.RWMutex\n    cond    *sync.Cond // Used to signal sender when a batch is ready\n    closed  bool\n}\n\nfunc NewAccumulator(config *ProducerConfig) *Accumulator {\n    acc := &Accumulator{\n        config: config,\n        batches: make(map[TopicPartition]*RecordBatch),\n    }\n    acc.cond = sync.NewCond(&acc.mu)\n    return acc\n}\n\n// Append adds a record to the batch for the given TopicPartition.\n// It returns the batch if it's ready to send (size or linger), otherwise nil.\nfunc (a *Accumulator) Append(tp TopicPartition, record *protocol.Record) *protocol.RecordBatch {\n    a.mu.Lock()\n    defer a.mu.Unlock()\n\n    batch, exists := a.batches[tp]\n    if !exists {\n        batch = &protocol.RecordBatch{\n            FirstTimestamp: time.Now().UnixMilli(),\n            Records:        []*protocol.Record{},\n        }\n        a.batches[tp] = batch\n        // TODO 1: Start a linger timer for this batch in a goroutine.\n        //         When the timer fires, call a.readyBatch(tp).\n    }\n\n    // TODO 2: Serialize the record to estimate its size in bytes.\n    // TODO 3: Add the record to batch.Records.\n    // TODO 4: Update batch.LastOffsetDelta and batch.MaxTimestamp.\n    // TODO 5: If the batch's estimated size >= config.BatchSize, mark it ready.\n    //         Call a.readyBatch(tp) and return the batch.\n\n    return nil // Return nil if batch not ready yet.\n}\n\n// readyBatch marks a batch as ready to send and notifies the sender.\nfunc (a *Accumulator) readyBatch(tp TopicPartition) {\n    a.mu.Lock()\n    defer a.mu.Unlock()\n    batch, ok := a.batches[tp]\n    if !ok {\n        return\n    }\n    // TODO 6: Remove the batch from the `batches` map.\n    // TODO 7: Signal the waiting sender goroutine via a.cond.Signal().\n}\n\n// GetReadyBatch blocks until a batch is ready or the accumulator is closed.\n// Called by the sender goroutine.\nfunc (a *Accumulator) GetReadyBatch() (TopicPartition, *protocol.RecordBatch) {\n    a.mu.Lock()\n    defer a.mu.Unlock()\n    for !a.closed {\n        // TODO 8: Iterate through `batches` and find one marked as ready.\n        //         If found, remove it and return it.\n        // TODO 9: If none ready, wait on a.cond.\n    }\n    return TopicPartition{}, nil\n}\n\nfunc (a *Accumulator) Close() {\n    a.mu.Lock()\n    defer a.mu.Unlock()\n    a.closed = true\n    a.cond.Broadcast()\n}\n```\n\n**2. Partitioner (`partitioner.go`):** Selects a partition for a record.\n\n```go\n// internal/producer/partitioner.go\npackage producer\n\nimport (\n    \"hash\"\n    \"hash/fnv\"\n)\n\ntype Partitioner interface {\n    Partition(topic string, key []byte, numPartitions int32) (int32, error)\n}\n\ntype HashPartitioner struct {\n    hasher hash.Hash32\n}\n\nfunc NewHashPartitioner() *HashPartitioner {\n    return &HashPartitioner{hasher: fnv.New32a()}\n}\n\nfunc (p *HashPartitioner) Partition(topic string, key []byte, numPartitions int32) (int32, error) {\n    if numPartitions <= 0 {\n        return 0, ErrInvalidPartitionCount\n    }\n    if key == nil {\n        // TODO 1: Handle nil key. Common strategy: round-robin across partitions.\n        //         You may need state (a counter) per topic. Use sync/atomic.\n        return 0, nil\n    }\n    p.hasher.Reset()\n    p.hasher.Write(key)\n    hash := int32(p.hasher.Sum32())\n    // Ensure non-negative partition.\n    partition := (hash & 0x7FFFFFFF) % numPartitions\n    return partition, nil\n}\n```\n\n**3. Sender (`sender.go`):** Manages network communication and retries.\n\n```go\n// internal/producer/sender.go\npackage producer\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \"github.com/yourusername/byok/internal/protocol\"\n)\n\ntype Sender struct {\n    config        *ProducerConfig\n    accumulator   *Accumulator\n    metadataCache *MetadataCache\n    inFlight      map[TopicPartition]*InFlightBatch\n    mu            sync.RWMutex\n    connPool      *ConnectionPool\n    retryQueue    chan *RetryItem\n    done          chan struct{}\n}\n\nfunc NewSender(config *ProducerConfig, acc *Accumulator, meta *MetadataCache) *Sender {\n    s := &Sender{\n        config:        config,\n        accumulator:   acc,\n        metadataCache: meta,\n        inFlight:      make(map[TopicPartition]*InFlightBatch),\n        connPool:      NewConnectionPool(),\n        retryQueue:    make(chan *RetryItem, 1000),\n        done:          make(chan struct{}),\n    }\n    go s.run()\n    go s.retryLoop()\n    return s\n}\n\nfunc (s *Sender) run() {\n    for {\n        select {\n        case <-s.done:\n            return\n        default:\n            tp, batch := s.accumulator.GetReadyBatch()\n            if batch == nil {\n                continue // Accumulator closed\n            }\n            go s.sendBatch(tp, batch)\n        }\n    }\n}\n\nfunc (s *Sender) sendBatch(tp TopicPartition, batch *protocol.RecordBatch) {\n    var attempt int\n    for attempt = 0; attempt <= s.config.Retries; attempt++ {\n        // TODO 1: Look up the leader broker for tp using metadataCache.\n        //         If metadata is stale, refresh it.\n        leaderID, err := s.metadataCache.Leader(tp.Topic, tp.Partition)\n        if err != nil {\n            // TODO: Handle error, maybe schedule metadata refresh.\n            continue\n        }\n        // TODO 2: Get a connection to the leader from connPool.\n        conn, err := s.connPool.Get(leaderID)\n        if err != nil {\n            // TODO: Handle connection error, maybe mark broker as dead.\n            continue\n        }\n\n        // TODO 3: Encode the batch into a ProduceRequest.\n        req := &protocol.ProduceRequest{\n            Topic:      tp.Topic,\n            Partition:  tp.Partition,\n            RecordBatch: batch,\n            Acks:       s.config.Acks,\n        }\n        // TODO 4: Send the request and receive a response.\n        resp, err := conn.SendProduceRequest(req)\n        if err != nil {\n            // Network error. Schedule retry.\n            s.scheduleRetry(tp, batch, attempt)\n            return\n        }\n\n        // TODO 5: Check the response error code.\n        if resp.ErrorCode == protocol.ErrNone {\n            // Success! Notify the original caller (via callback or channel).\n            s.completeBatch(tp, batch, resp.Offset)\n            return\n        } else if isRetriableError(resp.ErrorCode) {\n            // Retriable error (e.g., NotLeaderForPartition).\n            s.metadataCache.MarkStale(tp.Topic)\n            s.scheduleRetry(tp, batch, attempt)\n            return\n        } else {\n            // Fatal error. Fail the batch permanently.\n            s.failBatch(tp, batch, resp.ErrorCode)\n            return\n        }\n    }\n    // Exhausted retries.\n    s.failBatch(tp, batch, protocol.ErrRetriesExhausted)\n}\n\nfunc (s *Sender) scheduleRetry(tp TopicPartition, batch *protocol.RecordBatch, attempt int) {\n    delay := time.Duration(s.config.RetryBackoffMs) * time.Millisecond * (1 << attempt)\n    item := &RetryItem{\n        tp:    tp,\n        batch: batch,\n        time:  time.Now().Add(delay),\n    }\n    // TODO: Implement a priority queue based on `time` instead of a simple channel.\n    go func() {\n        time.Sleep(delay)\n        select {\n        case s.retryQueue <- item:\n        case <-s.done:\n        }\n    }()\n}\n\nfunc (s *Sender) retryLoop() {\n    for {\n        select {\n        case <-s.done:\n            return\n        case item := <-s.retryQueue:\n            // TODO: Re-insert the batch into the accumulator for the same tp.\n            // This will cause it to be picked up by the sender again.\n        }\n    }\n}\n\nfunc (s *Sender) completeBatch(tp TopicPartition, batch *protocol.RecordBatch, offset int64) {\n    // TODO: Invoke the user's callback or unblock the synchronous Send.\n    s.mu.Lock()\n    delete(s.inFlight, tp)\n    s.mu.Unlock()\n}\n\nfunc (s *Sender) failBatch(tp TopicPartition, batch *protocol.RecordBatch, err protocol.ErrorCode) {\n    // TODO: Invoke the user's callback with error or return error for synchronous Send.\n    s.mu.Lock()\n    delete(s.inFlight, tp)\n    s.mu.Unlock()\n}\n\nfunc (s *Sender) Close() {\n    close(s.done)\n    s.connPool.Close()\n}\n```\n\n**E. Language-Specific Hints (Go)**\n\n1.  **Concurrency:** Use a separate goroutine for the main sender loop (`run`) and the retry loop. Use `sync.Cond` for efficient waiting between the accumulator and sender. Protect shared maps (`batches`, `inFlight`) with `sync.RWMutex`.\n2.  **Network Connections:** Implement a `ConnectionPool` that reuses `net.Conn` for each broker. Set `TCPKeepAlive` and reasonable read/write deadlines. Handle connection errors gracefully by evicting the broken connection from the pool.\n3.  **Context Propagation:** Use `context.Context` in the public `Send` and `Flush` methods to allow cancellation and timeouts from the application. Propagate this context through the call chain to network requests.\n4.  **Error Types:** Define specific error types (e.g., `ErrBufferFull`, `ErrTopicNotFound`) in `errors.go` for clear error handling.\n\n**F. Milestone Checkpoint**\n\nTo verify your producer implementation works with the broker from Milestone 1:\n1.  **Test Setup:** Start a single broker with a topic `test-topic` (2 partitions).\n2.  **Write a Test Producer:** Create a simple program that uses your producer library to send 100 messages with sequential keys.\n3.  **Expected Behavior:**\n    - Messages should be written to the broker's log segments. Check the data directory for new `.log` files.\n    - Use the broker's `HandleFetch` API (or a simple test consumer) to read back the messages. They should be present in order within each partition.\n    - For `acks=all`, test by killing the broker leader after a send but before acknowledgment; the producer should retry and eventually succeed once a new leader is elected (Milestone 4).\n4.  **Signs of Trouble:**\n    - **No messages appear:** Check network connectivity, broker logs for errors, and that the producer is looking up the correct leader.\n    - **Messages are duplicated:** Your retry logic is likely not handling timeouts correctly. Ensure you are not retrying on ambiguous errors without idempotence.\n    - **Producer hangs:** The `BufferMemory` might be too small, or the sender goroutine may be deadlocked. Add debug logs to trace the flow.\n\n\n## 7. Component Design: Consumer and Consumer Groups\n\n> **Milestone(s):** 3 (Consumer Groups)\n\n### 7.1 Responsibility and Scope\n\nThe **Consumer** is a client application that subscribes to topics and reads messages from partitions in a controlled, scalable manner. Its primary responsibility is to fetch batches of records from broker partitions, track its consumption progress via offsets, and participate in a **Consumer Group** for coordinated parallel consumption. The **Group Coordinator** (typically a designated broker) manages the metadata and lifecycle of consumer groups, including membership, partition assignment, and offset persistence.\n\n**Key Scope Boundaries:**\n- **Consumer Responsibilities:**\n  - Discover topic partition leaders via metadata requests\n  - Join/leave consumer groups and maintain heartbeat with the coordinator\n  - Execute partition assignment strategies (when acting as group leader)\n  - Fetch records from assigned partitions, respecting the high watermark\n  - Periodically commit consumed offsets for durability and resume capability\n  - Handle partition reassignment during group rebalancing\n\n- **Coordinator Responsibilities:**\n  - Maintain group membership state (active members, generation ID)\n  - Trigger and orchestrate rebalancing when membership changes\n  - Store and serve committed offsets for each group-partition pair\n  - Detect and evict dead consumers via heartbeat timeout\n  - Act as the single source of truth for group assignment\n\n- **Out of Scope for This Implementation:**\n  - Dynamic partition addition to existing topics (would trigger rebalance)\n  - Transactional offset commits (exactly-once semantics)\n  - Custom assignment strategies beyond range and round-robin\n  - Automatic offset reset policies (earliest/latest) — defaults to committed offset\n  - Standalone consumers (non-group) reading from explicit partitions\n\n### 7.2 Mental Model: The Team Reading a Shared Book\n\nImagine a team assigned to read and summarize a multi-volume encyclopedia (the **topic**). Each volume is a **partition** — an independent, ordered sequence of pages. The team's goal is to divide the work evenly and ensure every page is read exactly once. \n\n**The Initial Division:** When the team first gathers, they elect a **leader** who inspects all volumes and assigns each volume to exactly one team member. This is the **partition assignment**. Each member notes their starting page number (**offset**) in their assigned volumes.\n\n**Parallel Reading with Coordination:** Each member reads pages sequentially from their assigned volumes, periodically checking in with the team coordinator to confirm they're still active (**heartbeat**). If a member leaves (goes for coffee) or joins the team, the coordinator calls everyone back to re-divide the volumes — a **rebalance**. During rebalance, everyone stops reading, waits for new assignments, then resumes from their last noted page.\n\n**Progress Tracking:** Each member writes down the page number they've finished reading in a shared notebook (**offset commit**). If a member restarts (takes a nap), they can consult the notebook to resume from where they left off, avoiding re-reading or skipping pages.\n\nThis model illustrates the core consumer group concepts: **shared workload** (partitions assigned to one consumer), **coordination overhead** (rebalances pause consumption), **fault tolerance** (heartbeats detect failures), and **progress persistence** (offset commits enable resume).\n\n### 7.3 Public Interface\n\nThe consumer and coordinator expose APIs for applications and internal communication. These interfaces are expressed as RPC methods the broker implements and the consumer client calls.\n\n#### Consumer Client API (Application-Facing)\n\nThe consumer provides a pull-based interface where the application explicitly requests records.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `Subscribe(topics []string)` | `topics`: List of topic names to subscribe to | `error` | Joins the consumer group and begins partition assignment for the given topics. Triggers initial rebalance. |\n| `Poll(timeout time.Duration)` | `timeout`: Maximum time to wait for records | `[]*Record` (and internal offset update) | Fetches records from assigned partitions. Blocks until records arrive or timeout expires. Automatically handles heartbeats in background. |\n| `CommitSync(offsets map[TopicPartition]int64)` | `offsets`: Map from partition to offset to commit | `error` | Synchronously commits specific offsets for the consumer's group. Waits for coordinator acknowledgment. |\n| `CommitAsync(offsets map[TopicPartition]int64, callback func(error))` | `offsets`: Map to commit, `callback`: Invoked on completion | - | Asynchronously commits offsets. Callback receives error if commit fails. |\n| `Close()` | - | `error` | Leaves consumer group, commits final offsets, and releases network resources. |\n\n**Configuration Knobs:**\n- `group.id`: Unique string identifying the consumer group\n- `session.timeout.ms`: Timeout after which coordinator considers consumer dead (triggers rebalance)\n- `heartbeat.interval.ms`: Frequency of heartbeat signals to coordinator\n- `max.poll.interval.ms`: Maximum time between `Poll()` calls before consumer considered stalled\n- `auto.offset.reset`: What to do when no committed offset exists (`earliest`, `latest`, `none` – error)\n- `enable.auto.commit`: Whether to automatically commit offsets periodically (simplified: we implement manual commit)\n\n#### Coordinator Internal API (Broker-to-Consumer)\n\nThe coordinator implements these RPC endpoints that consumers call for group management.\n\n| Method | Request Parameters | Response Fields | Description |\n|--------|-------------------|-----------------|-------------|\n| `JoinGroup(groupID, memberID, protocolType, protocols)` | `groupID`: Group identifier<br>`memberID`: Current member ID (empty on first join)<br>`protocolType`: Always \"consumer\"<br>`protocols`: List of supported assignment strategies | `memberID`: Assigned member ID<br>`generationID`: Current group generation<br>`leaderID`: ID of elected leader<br>`protocol`: Chosen assignment strategy<br>`members`: List of group members (only returned to leader) | Registers consumer with group. If first member or rebalance needed, triggers new generation. Returns member list to leader for assignment. |\n| `SyncGroup(groupID, generationID, memberID, assignments)` | `groupID`, `generationID`, `memberID`: Identity<br>`assignments`: Partition assignments (from leader only) | `assignments`: Partition assignments for this member | After JoinGroup, members call this to receive their assigned partitions. Leader provides assignments for all members. |\n| `Heartbeat(groupID, generationID, memberID)` | `groupID`, `generationID`, `memberID`: Identity | `error` | Periodic keep-alive. Coordinator resets session timeout. If generation mismatch or member unknown, signals rejoin. |\n| `OffsetCommit(groupID, offsets)` | `groupID`: Group identifier<br>`offsets`: Map from partition to offset + metadata | `error` | Persists offsets for partitions. Used for manual commit and auto-commit. |\n| `OffsetFetch(groupID, partitions)` | `groupID`: Group identifier<br>`partitions`: List of partitions to fetch offsets for | `offsets`: Map from partition to committed offset + metadata | Retrieves previously committed offsets. Called on consumer startup to resume. |\n\n### 7.4 Internal Behavior: Group Membership & Rebalancing\n\nConsumer group operation is a distributed state machine coordinated by the group coordinator. Each consumer transitions through states, and the group as a whole undergoes phases.\n\n#### Consumer Member State Machine\n\nEach group member follows the state transitions below. Reference diagram: ![Consumer Group State Machine](./diagrams/diagram-consumer-group-state.svg)\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|-------|------------|---------------|\n| **UNJOINED** | Application calls `Subscribe()` | **JOINING** | Generate temporary `memberID` (empty), send `JoinGroup` request to coordinator. |\n| **JOINING** | Coordinator responds with `memberID` and `generationID` | **AWAITING_ASSIGNMENT** | Store assigned `memberID`. If elected leader, run partition assignment algorithm. Send `SyncGroup` with assignments (if leader) or empty assignments. |\n| **AWAITING_ASSIGNMENT** | Coordinator responds to `SyncGroup` with partition assignments | **STABLE** | Update local assignment map. Start fetch loop for assigned partitions. Begin periodic heartbeats. |\n| **STABLE** | `Poll()` called | **STABLE** | Fetch records from assigned partitions. Reset `max.poll.interval` timer. |\n| **STABLE** | Heartbeat response indicates `REBALANCE_IN_PROGRESS` | **JOINING** | Stop fetching. Re-join group with current `memberID`. |\n| **STABLE** | Session timeout (no heartbeat response) | **UNJOINED** | Assume coordinator dead. Re-discover coordinator and rejoin. |\n| **STABLE** | Coordinator fails heartbeat (generation mismatch) | **JOINING** | Stop fetching. Re-join group with current `memberID`. |\n| **ANY** | Application calls `Close()` | **UNJOINED** | Send `LeaveGroup` (optional), stop heartbeats, close network connections. |\n\n#### Group Rebalancing Protocol\n\nRebalancing ensures partitions are redistributed when members join or leave. The coordinator orchestrates this process:\n\n1. **Trigger Detection:** Coordinator detects rebalance trigger:\n   - New `JoinGroup` request for a group in `Stable` state\n   - Existing member's heartbeat times out (`session.timeout.ms`)\n   - Explicit member leave (TCP disconnect, `LeaveGroup` request)\n\n2. **Generation Bump:** Coordinator increments `generationID` for the group, transitions group state to `PreparingRebalance`. It delays responding to new `JoinGroup` requests for up to `rebalance.timeout.ms` to allow existing members to rejoin.\n\n3. **Member Joining:** Coordinator collects `JoinGroup` requests from all (existing and new) members. It chooses a leader (first member to join) and a partition assignment protocol (intersection of all members' supported protocols).\n\n4. **Leader Assignment:** Coordinator responds to `JoinGroup` with member list **only to the leader**. Other members receive empty member list.\n\n5. **Assignment Calculation:** Leader consumer runs partition assignment algorithm (range or round-robin) and sends assignments for all members in its `SyncGroup` request.\n\n6. **Distribution:** Coordinator stores leader's assignments. When each member calls `SyncGroup`, it receives its specific assignment.\n\n7. **Completion:** Once all members have called `SyncGroup`, coordinator transitions group to `Stable` state. Members receive assignments and begin fetching.\n\n**Critical Behavior:** During rebalance (between steps 2-7), the coordinator responds to `Heartbeat` requests with `REBALANCE_IN_PROGRESS` error, prompting consumers to re-join. This ensures all members synchronize to the new generation.\n\n#### Partition Assignment Algorithm (Leader Side)\n\nWhen elected leader, the consumer runs one of the following algorithms to assign partitions to group members.\n\n**Range Assignment (Default):**\n1. Sort topics lexicographically, sort partitions within each topic numerically.\n2. Sort consumer members lexicographically by `memberID`.\n3. For each topic:\n   - Calculate `partitionsPerConsumer = floor(totalPartitions / totalConsumers)`\n   - Calculate `consumersWithExtra = totalPartitions % totalConsumers`\n   - Iterate through sorted consumers: assign `partitionsPerConsumer + 1` partitions to the first `consumersWithExtra` consumers, then `partitionsPerConsumer` to the rest.\n4. Output mapping: `memberID -> list of TopicPartition`.\n\n*Example:* Topic `orders` with partitions 0-5, consumers C1, C2, C3.\n- `partitionsPerConsumer = floor(6/3)=2`\n- `consumersWithExtra = 6%3=0`\n- Assignment: C1 gets [0,1], C2 gets [2,3], C3 gets [4,5].\n\n**RoundRobin Assignment:**\n1. Flatten all partitions across subscribed topics into a single list, sorted by topic then partition.\n2. Sort consumer members lexicographically by `memberID`.\n3. Distribute partitions in circular fashion: partition i goes to consumer at position `(i mod totalConsumers)`.\n4. Output mapping.\n\n*Example:* Same topic and consumers. Sorted partitions: [orders-0, orders-1, orders-2, orders-3, orders-4, orders-5].\n- Assignment: C1 gets [orders-0, orders-3], C2 gets [orders-1, orders-4], C3 gets [orders-2, orders-5].\n\n> **Design Insight:** Range assignment tends to preserve per-topic partition ordering and is simpler, but can lead to imbalance when subscriptions differ across members. RoundRobin distributes more evenly but scatters partitions of the same topic across consumers. For our educational implementation, we implement both but default to Range for its conceptual clarity.\n\n#### Offset Commit and Fetch\n\nOffset management is crucial for resume-on-restart semantics. The coordinator stores offsets in a durable store (simplified: in-memory map persisted to a file).\n\n**Commit Flow:**\n1. Consumer periodically (or manually) calls `OffsetCommit` RPC with a map of `TopicPartition -> offset`.\n2. Coordinator validates the consumer is still a member of the group and generation matches.\n3. Coordinator persists offset to storage (fsync for durability).\n4. Coordinator responds success; consumer may discard locally cached commits.\n\n**Fetch Flow (on startup):**\n1. Consumer calls `OffsetFetch` for all partitions it is assigned (or potentially subscribes to).\n2. Coordinator returns stored offset for each partition, or `-1` if none exists.\n3. Consumer uses returned offset as its starting position. If offset is `-1`, it uses `auto.offset.reset` policy (default to earliest).\n\n**Storage Format:** Offsets are stored per `(groupID, topic, partition)` tuple. Each entry includes offset, metadata (optional string), and timestamp.\n\n### 7.5 ADR: Partition Assignment Strategy\n\n> **Decision: Implement Both Range and RoundRobin, Default to Range**\n\n- **Context:** Consumer groups need a deterministic algorithm to map partitions to members. The algorithm must produce balanced assignments while being understandable for learners. Real Kafka supports pluggable strategies; we must choose which to implement first.\n- **Options Considered:**\n  1. **Range Assignment:** Assign contiguous ranges of partitions per topic to each consumer.\n  2. **RoundRobin Assignment:** Distribute partitions in circular order across all consumers.\n  3. **Sticky Assignment:** Advanced algorithm that minimizes partition movement during rebalance (out of scope).\n- **Decision:** Implement both Range and RoundRobin, with Range as the default. The coordinator will choose the protocol based on what all members support (intersection).\n- **Rationale:**\n  - **Range** is conceptually simpler to understand and implement — it mirrors how humans naturally divide ordered lists. This aligns with our educational goal.\n  - **RoundRobin** introduces the concept of cross-topic balancing and is slightly more complex, but provides a clear contrast in behavior.\n  - Implementing both allows learners to experiment with trade-offs and understand protocol negotiation.\n  - Sticky assignment is omitted because its optimization (minimal reassignment) adds complexity disproportionate to learning value for Milestone 3.\n- **Consequences:**\n  - Learners must implement protocol negotiation in `JoinGroup`.\n  - Range may cause workload imbalance when members subscribe to different topic subsets (but our simplified implementation assumes uniform subscription).\n  - The system can be extended later with custom assignment strategies.\n\n**Comparison Table:**\n\n| Option | Pros | Cons | Suitable For |\n|--------|------|------|--------------|\n| **Range** | Simple to implement and reason about; preserves partition ordering per consumer | Can create imbalance when partitions per topic not divisible by consumers; uneven if subscriptions differ | Educational default; good for understanding basic assignment |\n| **RoundRobin** | More even distribution across all partitions; balances load better | Scatters partitions of same topic across consumers; may hurt locality | Demonstrating alternative strategy; better balance |\n| **Sticky** | Minimizes partition movement during rebalance; reduces temporary unavailability | Complex algorithm; stateful coordination required | Production systems where rebalance cost is high (out of scope) |\n\n### 7.6 Common Pitfalls\n\n⚠️ **Pitfall: Rebalance Storms (Frequent Unnecessary Rebalances)**  \n**Description:** Consumers repeatedly trigger rebalances due to aggressive timeout settings or misconfigured heartbeats, causing constant consumption pauses.  \n**Why Wrong:** System spends more time rebalancing than processing messages; throughput plummets.  \n**Fix:** Set `session.timeout.ms` appropriately (e.g., 10-30 seconds). Ensure heartbeat thread runs independently of poll loop. Implement `max.poll.interval.ms` separately for slow processing detection.\n\n⚠️ **Pitfall: Zombie Consumers (Double Assignment)**  \n**Description:** A consumer thought dead (heartbeat timeout) but actually just slow, continues fetching from its assigned partitions while new consumer gets same assignment.  \n**Why Wrong:** Two consumers read same partitions → duplicate processing and offset commit conflicts.  \n**Fix:** Coordinator must increment `generationID` on rebalance; include generation in fetch requests; broker should reject fetches from stale generations.\n\n⚠️ **Pitfall: Offset Commit Races During Rebalance**  \n**Description:** Consumer commits offsets after being assigned partitions in rebalance, but commit may be applied after new generation is active, corrupting offset for new member.  \n**Why Wrong:** New consumer starts from wrong offset, causing missed or repeated messages.  \n**Fix:** Coordinator must reject offset commits with stale `generationID`. Consumer should commit offsets *before* joining new rebalance (implement `onPartitionsRevoked` callback in real Kafka).\n\n⚠️ **Pitfall: Missing Heartbeats Due to Blocking Poll**  \n**Description:** Consumer's `Poll()` blocks longer than `session.timeout.ms` (e.g., processing large batch), preventing heartbeat thread from sending keep-alives.  \n**Why Wrong:** Coordinator evicts consumer, triggering rebalance even though consumer is active.  \n**Fix:** Run heartbeat thread independently with its own timer. Ensure `Poll()` returns quickly; process messages in application thread after fetch.\n\n⚠️ **Pitfall: Incorrect Assignment on Heterogeneous Subscriptions**  \n**Description:** If group members subscribe to different topic lists, naive assignment algorithms may assign partitions for topics a consumer didn't subscribe to.  \n**Why Wrong:** Consumer receives partitions it cannot consume; some partitions may be unassigned.  \n**Fix:** In `JoinGroup`, each member must list its subscribed topics. Leader should assign only partitions of topics a member subscribes to. Filter assignments before distribution.\n\n### 7.7 Implementation Guidance\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Group State Storage | In-memory map with periodic file snapshot (JSON) | Embedded KV store (BadgerDB) with WAL |\n| Network Protocol | Plain TCP with custom binary format (same as produce/fetch) | gRPC with Protocol Buffers for type safety |\n| Heartbeat Scheduler | Goroutine with `time.Ticker` | Dedcheduled timer wheel for efficiency |\n| Offset Storage | Separate file per group (`offsets/groupID.json`) | Internal `__consumer_offsets` topic (like Kafka) |\n\n**B. Recommended File/Module Structure**\n\n```\nproject-root/\n  cmd/\n    server/                 # Broker main\n      main.go\n    consumer/               # Consumer client main\n      main.go\n  internal/\n    coordinator/            # Group coordinator logic\n      coordinator.go        # Group membership, rebalance orchestration\n      assignment.go         # Range and RoundRobin assignment strategies\n      offsets.go            # Offset commit/fetch storage\n      coordinator_test.go\n    consumer/               # Consumer client library\n      client.go             # Public Consumer type (Subscribe, Poll, Commit)\n      fetcher.go            # Background fetch loop for assigned partitions\n      heartbeater.go        # Heartbeat management\n      member.go             # Group membership state machine\n    protocol/               # Shared wire format\n      join_group.go         # JoinGroup request/response structs\n      sync_group.go         # SyncGroup request/response\n      heartbeat.go          # Heartbeat request/response\n      offset_commit.go      # Offset commit/fetch structs\n    storage/\n      offset_store.go       # Offset persistence abstraction\n```\n\n**C. Infrastructure Starter Code: Offset Storage**\n\n```go\n// internal/coordinator/offset_store.go\npackage coordinator\n\nimport (\n    \"encoding/json\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n)\n\n// OffsetStore persists consumer group offsets to disk.\n// Simplified: one JSON file per group. Not optimized for performance.\ntype OffsetStore struct {\n    baseDir string\n    mu      sync.RWMutex\n    // in-memory cache: group -> topic -> partition -> offset\n    cache   map[string]map[string]map[int32]int64\n}\n\n// NewOffsetStore creates or loads offset store from directory.\nfunc NewOffsetStore(baseDir string) (*OffsetStore, error) {\n    if err := os.MkdirAll(baseDir, 0755); err != nil {\n        return nil, err\n    }\n    store := &OffsetStore{\n        baseDir: baseDir,\n        cache:   make(map[string]map[string]map[int32]int64),\n    }\n    // Load existing offsets from disk (optional)\n    // store.loadAll()\n    return store, nil\n}\n\n// Commit writes offsets for a group.\nfunc (s *OffsetStore) Commit(groupID string, topic string, partition int32, offset int64) error {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    if _, ok := s.cache[groupID]; !ok {\n        s.cache[groupID] = make(map[string]map[int32]int64)\n    }\n    if _, ok := s.cache[groupID][topic]; !ok {\n        s.cache[groupID][topic] = make(map[int32]int64)\n    }\n    s.cache[groupID][topic][partition] = offset\n    \n    // Persist to disk (simplified: write entire group's offsets)\n    return s.persistGroup(groupID)\n}\n\n// Fetch retrieves offset for a group-topic-partition.\nfunc (s *OffsetStore) Fetch(groupID, topic string, partition int32) (int64, bool) {\n    s.mu.RLock()\n    defer s.mu.RUnlock()\n    \n    if groupMap, ok := s.cache[groupID]; ok {\n        if topicMap, ok := groupMap[topic]; ok {\n            offset, ok := topicMap[partition]\n            return offset, ok\n        }\n    }\n    return -1, false\n}\n\n// persistGroup writes group's offsets to JSON file.\nfunc (s *OffsetStore) persistGroup(groupID string) error {\n    groupFile := filepath.Join(s.baseDir, groupID+\".json\")\n    s.mu.RLock()\n    data, err := json.MarshalIndent(s.cache[groupID], \"\", \"  \")\n    s.mu.RUnlock()\n    if err != nil {\n        return err\n    }\n    // Atomic write: write to temp then rename\n    tmpFile := groupFile + \".tmp\"\n    if err := os.WriteFile(tmpFile, data, 0644); err != nil {\n        return err\n    }\n    return os.Rename(tmpFile, groupFile)\n}\n\n// LoadGroup reads offsets from disk (call on startup).\nfunc (s *OffsetStore) LoadGroup(groupID string) error {\n    groupFile := filepath.Join(s.baseDir, groupID+\".json\")\n    data, err := os.ReadFile(groupFile)\n    if os.IsNotExist(err) {\n        return nil // No existing offsets\n    }\n    if err != nil {\n        return err\n    }\n    var groupMap map[string]map[int32]int64\n    if err := json.Unmarshal(data, &groupMap); err != nil {\n        return err\n    }\n    s.mu.Lock()\n    s.cache[groupID] = groupMap\n    s.mu.Unlock()\n    return nil\n}\n```\n\n**D. Core Logic Skeleton Code**\n\n**Group Coordinator Main Logic:**\n\n```go\n// internal/coordinator/coordinator.go\npackage coordinator\n\nimport (\n    \"sync\"\n    \"time\"\n)\n\ntype GroupState int32\n\nconst (\n    GroupStateStable GroupState = iota\n    GroupStatePreparingRebalance\n    GroupStateDead\n)\n\ntype ConsumerGroup struct {\n    GroupID      string\n    State        GroupState\n    GenerationID int32\n    LeaderID     string\n    Protocol     string // e.g., \"range\", \"roundrobin\"\n    Members      map[string]*ConsumerMetadata // keyed by memberID\n    TimeoutMs    int32\n    mu           sync.RWMutex\n    // Tracks when rebalance started for timeout\n    rebalanceStartedAt time.Time\n}\n\ntype ConsumerMetadata struct {\n    MemberID           string\n    ClientID           string\n    SubscribedTopics   []string\n    AssignedPartitions map[string][]int32 // topic -> partition list\n    LastHeartbeat      time.Time\n    JoinWait           chan struct{} // signals member that join is complete\n}\n\ntype Coordinator struct {\n    groups   map[string]*ConsumerGroup\n    offsetStore *OffsetStore\n    mu       sync.RWMutex\n}\n\n// HandleJoinGroup processes a consumer's request to join a group.\n// Returns memberID, generationID, leaderID, protocol, and member list (if leader).\nfunc (c *Coordinator) HandleJoinGroup(groupID, memberID, protocolType string, \n    protocols []string, timeoutMs int32) (string, int32, string, string, []*ConsumerMetadata, error) {\n    // TODO 1: Lookup or create group for groupID\n    // TODO 2: If memberID is empty, generate a unique ID (e.g., \"consumer-<uuid>\")\n    // TODO 3: Validate protocolType == \"consumer\"\n    // TODO 4: If group is Stable and this is first member joining (new consumer), trigger rebalance:\n    //   - Set group state to PreparingRebalance\n    //   - Increment GenerationID\n    //   - Record rebalance start time\n    // TODO 5: If group is already PreparingRebalance, add member to Members map\n    // TODO 6: If this member is first to join in this generation, set as LeaderID\n    // TODO 7: Choose protocol from intersection of all members' protocols (prefer \"range\")\n    // TODO 8: If all expected members have joined or timeout reached, move to next step:\n    //   - For leader: return full member list so it can compute assignments\n    //   - For followers: return empty member list\n    // TODO 9: Return assigned memberID, current GenerationID, LeaderID, chosen Protocol\n    return \"\", 0, \"\", \"\", nil, nil\n}\n\n// HandleSyncGroup distributes partition assignments to members.\nfunc (c *Coordinator) HandleSyncGroup(groupID string, generationID int32, \n    memberID string, assignments map[string]*Assignment) (map[string][]int32, error) {\n    // TODO 1: Validate group exists, generation matches, member is in group\n    // TODO 2: If member is leader, store assignments for all members\n    // TODO 3: If assignments already stored (by leader), return this member's assignment\n    // TODO 4: If all members have retrieved assignments, transition group to Stable state\n    // TODO 5: Return assigned partitions for this member: topic -> []partition\n    return nil, nil\n}\n\n// HandleHeartbeat validates member liveness.\nfunc (c *Coordinator) HandleHeartbeat(groupID string, generationID int32, \n    memberID string) error {\n    // TODO 1: Lookup group, validate generation matches\n    // TODO 2: Update member's LastHeartbeat timestamp\n    // TODO 3: If group is PreparingRebalance, return error to signal consumer to rejoin\n    // TODO 4: Return nil if successful\n    return nil\n}\n\n// checkRebalanceTimeout runs periodically to complete rebalance if members don't all join.\nfunc (c *Coordinator) checkRebalanceTimeout() {\n    // TODO 1: Iterate groups in PreparingRebalance state\n    // TODO 2: If rebalance started more than TimeoutMs ago, proceed with current members\n    // TODO 3: Mark missing members as dead, remove from group\n    // TODO 4: Trigger assignment with remaining members\n}\n```\n\n**Consumer Client Membership State Machine:**\n\n```go\n// internal/consumer/member.go\npackage consumer\n\nimport (\n    \"context\"\n    \"time\"\n)\n\ntype MemberState int\n\nconst (\n    StateUnjoined MemberState = iota\n    StateJoining\n    StateAwaitingAssignment\n    StateStable\n)\n\ntype GroupMember struct {\n    groupID        string\n    memberID       string\n    generationID   int32\n    state          MemberState\n    assigned       map[TopicPartition]bool\n    coordinator    *BrokerConnection // connection to group coordinator\n    heartbeatInterval time.Duration\n    lastHeartbeat   time.Time\n    cancelHeartbeat context.CancelFunc\n    mu             sync.RWMutex\n}\n\n// joinGroup performs the JoinGroup/SyncGroup protocol.\nfunc (m *GroupMember) joinGroup(topics []string) error {\n    // TODO 1: Set state to Joining\n    // TODO 2: Send JoinGroup request to coordinator with empty memberID (if first time)\n    // TODO 3: On response, store assigned memberID, generationID\n    // TODO 4: If elected leader, compute assignments for all members using chosen protocol\n    // TODO 5: Send SyncGroup request with assignments (if leader) or empty map\n    // TODO 6: On SyncGroup response, update assigned partitions map\n    // TODO 7: Start heartbeat goroutine\n    // TODO 8: Transition state to Stable\n    // TODO 9: Start fetcher for assigned partitions\n    return nil\n}\n\n// startHeartbeat begins periodic heartbeat to coordinator.\nfunc (m *GroupMember) startHeartbeat(ctx context.Context) {\n    ticker := time.NewTicker(m.heartbeatInterval)\n    go func() {\n        defer ticker.Stop()\n        for {\n            select {\n            case <-ticker.C:\n                // TODO 1: Send Heartbeat request\n                // TODO 2: If error indicates rebalance needed, call joinGroup again\n                // TODO 3: Update lastHeartbeat timestamp\n            case <-ctx.Done():\n                return\n            }\n        }\n    }()\n}\n```\n\n**Partition Assignment Strategies:**\n\n```go\n// internal/coordinator/assignment.go\npackage coordinator\n\n// Assignment represents partitions assigned to a consumer member.\ntype Assignment struct {\n    TopicPartitions map[string][]int32 // topic -> list of partition IDs\n}\n\n// RangeAssigner implements range assignment strategy.\ntype RangeAssigner struct{}\n\nfunc (r *RangeAssigner) Assign(members []*ConsumerMetadata, \n    topics map[string][]int32) map[string]*Assignment {\n    // TODO 1: Sort members by MemberID\n    // TODO 2: For each topic, sort partitions numerically\n    // TODO 3: Calculate partitionsPerConsumer and consumersWithExtra\n    // TODO 4: Assign contiguous ranges to each consumer\n    // TODO 5: Build Assignment map for each member\n    return nil\n}\n\n// RoundRobinAssigner implements round-robin assignment.\ntype RoundRobinAssigner struct{}\n\nfunc (rr *RoundRobinAssigner) Assign(members []*ConsumerMetadata,\n    topics map[string][]int32) map[string]*Assignment {\n    // TODO 1: Sort members by MemberID\n    // TODO 2: Flatten all partitions into single list: [topic-partition, ...]\n    // TODO 3: Distribute in round-robin fashion: partition i -> member i % len(members)\n    // TODO 4: Build Assignment map for each member\n    return nil\n}\n```\n\n**E. Language-Specific Hints (Go)**\n\n1. **Concurrency:** Use `sync.RWMutex` for `ConsumerGroup` and `Coordinator` maps. Heartbeat and fetch loops should run in separate goroutines; coordinate shutdown with `context.Context`.\n2. **ID Generation:** Use `crypto/rand` for unique `memberID` (e.g., `\"consumer-\" + hex.EncodeToString(uuid[:])`). Avoid using client IP/port as it may change.\n3. **Timers:** For session timeout, store `LastHeartbeat` as `time.Time` and periodically check `time.Since(last) > sessionTimeout`. Use `time.Ticker` for heartbeat loop.\n4. **Network Errors:** When TCP connection to coordinator fails, consumer should rediscover coordinator via metadata request and rejoin group.\n5. **State Persistence:** Write offset commits with `os.WriteFile` to temporary file then `os.Rename` for atomicity. Use JSON for simplicity; in production you'd use binary format.\n\n**F. Milestone Checkpoint**\n\nAfter implementing consumer groups:\n\n1. **Start a broker:** `go run cmd/server/main.go --port 9092`\n2. **Create a topic with 3 partitions:** Use your admin tool or implement a simple `CreateTopic` RPC.\n3. **Start two consumers in same group:**\n   ```bash\n   # Terminal 1\n   go run cmd/consumer/main.go --group my-group --topic test-topic\n   \n   # Terminal 2  \n   go run cmd/consumer/main.go --group my-group --topic test-topic\n   ```\n4. **Start a producer and send 100 messages:** Messages should be divided between the two consumers.\n5. **Kill one consumer (Ctrl+C):** Observe logs: surviving consumer should get reassigned all partitions after rebalance timeout.\n6. **Restart killed consumer:** It should rejoin and partitions should be redistributed.\n7. **Verify no duplicates:** Count messages received across all consumers; should equal 100 exactly.\n8. **Test offset persistence:** Stop all consumers, restart, produce more messages. New consumers should resume from committed offsets.\n\n**Expected Observations:**\n- Consumers log \"Assigned partitions: [...]\" on each rebalance.\n- During rebalance, message consumption pauses briefly.\n- If you implement auto-commit, offsets are saved periodically.\n- Heartbeat logs (if enabled) show periodic \"Heartbeat sent\" messages.\n\n**Signs of Trouble:**\n- **Messages duplicated:** Likely generation ID not validated in fetch requests, or offset commits race.\n- **Consumer gets no messages:** Check assignment algorithm; verify partitions actually have data.\n- **Rebalance loops continuously:** Session timeout too short or heartbeat not sent.\n- **Offsets not persisted:** Offset store file not being written (check permissions).\n\n---\n\n\n## 8. Component Design: Replication\n\n> **Milestone(s):** 4 (Replication)\n\n### 8.1 Responsibility and Scope\n\nThe **Replication module** is the core subsystem responsible for providing **fault tolerance** and **data durability** in our distributed message queue. Its primary responsibility is to maintain **identical copies** (replicas) of each partition's log across multiple broker nodes, ensuring that if one broker fails, another can seamlessly take over without data loss or unavailability. This module operates within the **replication plane** of the cluster—the internal network path over which brokers communicate to synchronize data.\n\nThe scope of the replication module encompasses four critical functions:\n1. **Leader-Follower Replication Protocol:** Continuously copying newly appended records from the **leader replica** (the broker responsible for handling all read/write requests for a partition) to one or more **follower replicas**.\n2. **In-Sync Replica (ISR) Management:** Dynamically tracking which follower replicas are sufficiently caught up with the leader to be considered \"in-sync,\" and managing the membership of this set based on configurable lag thresholds.\n3. **High Watermark Advancement:** Determining the offset up to which all in-sync replicas have replicated data, thereby defining the **durable frontier** that consumers can safely read without risking exposure to data that could be lost during a failover.\n4. **Leader Election:** Selecting a new leader replica from the current ISR when the existing leader becomes unavailable, ensuring continuous availability of the partition.\n\nThis module directly interacts with the `Log` and `Partition` components within each broker, and communicates with other brokers via internal RPCs. It is a foundational requirement for achieving the **exactly-once semantics** and **guaranteed delivery** that characterize production-grade message systems.\n\n### 8.2 Mental Model: The Ship's Log Replica\n\nTo build intuition, imagine a **captain's logbook** on a sailing ship (the partition log). The captain (the **leader**) records every important event (messages) in chronological order. To protect against the logbook being lost overboard, the captain employs several **first mates** (followers) who each maintain their own copy.\n\n- **Copying Process:** After each entry, the captain calls out the new lines, and the first mates write them into their own logbooks. Some mates write quickly and stay current; others might lag behind if distracted.\n- **In-Sync Crew (ISR):** The captain periodically checks which mates have fully copied the latest entries. Only those who are completely up-to-date are considered part of the \"in-sync crew.\" If a mate falls too far behind (beyond a tolerance threshold), they are temporarily removed from this trusted set until they catch up.\n- **Safe Reading Point (High Watermark):** When a sailor (consumer) wants to read the log, the captain only allows them to read entries that **every in-sync mate** has successfully copied. This ensures that even if the captain is suddenly lost at sea, the new captain (elected from the in-sync crew) will have a complete copy of everything the sailor has read, guaranteeing no data loss.\n- **Captain Election:** If the captain falls ill, the crew holds a quick vote among the in-sync mates to choose a new captain. They select the mate whose log is most complete (has the highest offset). This new captain immediately assumes responsibility for recording new events.\n\nThis model illustrates the core trade-off: **durability versus latency**. Waiting for all mates to acknowledge each entry (synchronous replication) maximizes safety but slows down the recording process. The ISR model allows the captain to proceed once a **quorum** of mates (often just the leader itself, or a majority) has acknowledged, striking a practical balance.\n\n### 8.3 Internal Broker-to-Broker API\n\nReplication is driven by a set of internal Remote Procedure Calls (RPCs) between brokers. These are separate from the client-facing `Produce` and `Fetch` APIs, though they may share the same transport layer. The following table details the key RPCs.\n\n| Method Name | Initiator | Target | Parameters | Returns | Description |\n|-------------|-----------|--------|------------|---------|-------------|\n| `LeaderForPartition` | Follower/Coordinator | Any Broker | `Topic string`, `PartitionID int` | `LeaderBrokerID int`, `LeaderEpoch int32`, `Error error` | Discovers the current leader broker for a given partition. Used by followers during startup or after a leader change. |\n| `FetchReplica` | Follower | Leader | `Topic string`, `PartitionID int`, `FollowerBrokerID int`, `FetchOffset int64`, `MaxBytes int32` | `Records []*Record`, `HighWatermark int64`, `LeaderEpoch int32`, `Error error` | The core replication fetch request. A follower uses this to pull records from the leader's log starting at `FetchOffset`. The leader includes the current **high watermark** so the follower knows what is safely committed. |\n| `UpdateISR` | Leader | Metadata Coordinator / Controller | `Topic string`, `PartitionID int`, `NewISR []int`, `LeaderEpoch int32` | `Error error` | Notifies the cluster metadata service of changes to the In-Sync Replica set (e.g., a follower is added or removed). This update must be persisted and propagated to all brokers. |\n| `BeginLeaderElection` | Controller / Coordinator | Candidate Broker | `Topic string`, `PartitionID int`, `CandidateISR []int` | `Success bool`, `Error error` | Initiates a leader election for a partition. The controller proposes a candidate broker (from the ISR) to become the new leader. The candidate validates it is in the ISR and takes leadership. |\n| `FollowerHeartbeat` | Follower | Leader | `Topic string`, `PartitionID int`, `FollowerBrokerID int`, `FollowerLEO int64` | `CurrentLeaderEpoch int32`, `Error error` | Optional periodic heartbeat from follower to leader, carrying the follower's latest Log End Offset (LEO). This allows the leader to track follower lag without waiting for a fetch request. |\n\n> **Design Insight:** The `FetchReplica` RPC is intentionally similar to the client `Fetch` API. This symmetry simplifies the code—a follower is essentially a special consumer that reads from the leader's log. However, replication fetches are continuous, long-polling requests that wait for new data, unlike typical consumer fetches.\n\nIn our simplified educational system, we may combine the roles of **Metadata Coordinator** and **Controller** into a single component (the `Coordinator` from previous sections). Therefore, `UpdateISR` and `BeginLeaderElection` would be directed to that coordinator broker.\n\n### 8.4 Internal Behavior: Follower Sync and ISR Management\n\nThe replication process is a continuous loop run by each follower replica for every partition it is assigned to. The leader, in parallel, monitors the progress of all followers to maintain the ISR.\n\n#### Follower Synchronization Algorithm\n\nFor each partition where the broker is a follower:\n\n1. **Determine Leader:** If not known, call `LeaderForPartition` to discover the current leader broker.\n2. **Establish Connection:** Open a persistent network connection to the leader's replication endpoint.\n3. **Initialize Fetch Offset:** Start fetching from the last offset successfully written to the follower's local log (its **Log End Offset** or **LEO**). If the log is empty, start at 0.\n4. **Loop:**\n   1. Send a `FetchReplica` request to the leader with the current `FetchOffset`.\n   2. If the leader responds with records:\n      1. Append the records to the local `Log` in order, ensuring no gaps in offsets.\n      2. Advance the local LEO to `FetchOffset + len(records)`.\n      3. Optionally, periodically send a `FollowerHeartbeat` with the new LEO to keep the leader informed.\n   3. If the leader responds with an error (e.g., `ErrNotLeaderForPartition`):\n      1. Break the connection.\n      2. Wait a short, randomized backoff period (to avoid thundering herds).\n      3. Go to step 1 to rediscover the new leader.\n   4. If the leader responds with no new records (i.e., the follower is caught up):\n      1. Wait for a configurable `replica.fetch.wait.ms` interval, then repeat the fetch (long polling). This reduces busy-waiting.\n\n#### Leader ISR Management Algorithm\n\nFor each partition where the broker is the leader:\n\n1. **Initialize ISR:** The ISR starts as the set of all assigned replicas (including the leader). This is stored in the `Partition.ISR` field.\n2. **Track Follower State:** Maintain, for each follower in the ISR, the last known **last fetched offset** (from `FetchReplica` requests) or **last heartbeat LEO**.\n3. **Periodic Check (every `replica.lag.time.max.ms`):**\n   1. Calculate the **replica lag** for each follower: `Leader's LEO - Follower's Last Fetched Offset`.\n   2. If a follower's lag exceeds `replica.lag.max.messages` **or** its last fetch time is older than `replica.lag.time.max.ms`, remove it from the ISR.\n   3. If a previously out-of-sync follower's lag falls to zero (it has caught up), add it back to the ISR.\n4. **On ISR Change:**\n   1. Persist the new ISR set (e.g., to ZooKeeper or an internal topic).\n   2. Broadcast the update via `UpdateISR` to the metadata coordinator so other brokers can update their caches.\n5. **Advance High Watermark:**\n   1. The **high watermark** (`Partition.HighWatermark`) is the maximum offset for which **all replicas in the current ISR** have acknowledged replication.\n   2. After each successful `FetchReplica` response acknowledgment from a follower, the leader recalculates the high watermark as the minimum LEO across all ISR members.\n   3. The high watermark is included in every `FetchReplica` response, allowing followers (and consumers) to know what is safely durable.\n\n> **Critical Detail:** The high watermark advancement is **monotonic**. It never moves backward, even if the ISR shrinks. If a follower is removed from the ISR because it is slow, the high watermark can still advance based on the remaining ISR members. This ensures availability at the potential cost of durability if the remaining ISR size falls below a desired minimum (a pitfall we address later).\n\n### 8.5 ADR: Leader Election Trigger\n\n> **Decision: Coordinator-Initiated Leader Election**\n\n- **Context:** When a partition leader fails (due to broker crash, network partition, or graceful shutdown), a new leader must be elected promptly to maintain partition availability. The election must choose a replica that is **guaranteed to have all committed messages** (i.e., a member of the ISR) to prevent data loss. We need a simple, deterministic mechanism that learners can implement without complex consensus protocols.\n\n- **Options Considered:**\n  1. **Explicit Coordinator-Managed Election:** A dedicated controller/coordinator broker monitors leader liveness (via heartbeats). Upon detecting leader failure, it selects a new leader from the ISR and directs it to assume leadership via RPC.\n  2. **Follower-Triggered Election:** Followers detect the leader is unresponsive (via fetch timeouts) and initiate a leader election among themselves using a consensus protocol like Raft or a simple voting mechanism.\n\n- **Decision:** We choose **Option 1: Explicit Coordinator-Managed Election**. The `Coordinator` component (introduced for consumer groups) will be extended to act as a **controller** for partition leadership.\n\n- **Rationale:**\n  - **Simplicity for Learners:** Implementing a full distributed consensus protocol (like Raft) is a significant undertaking beyond the core learning goals of message queue replication. Coordinator-managed election centralizes the decision logic, making it easier to implement, debug, and understand.\n  - **Alignment with Kafka's Design:** Apache Kafka uses a similar controller-based leader election, proving the pattern's effectiveness at scale. This allows learners to map concepts directly to the real system.\n  - **Deterministic Outcome:** The coordinator has a global view of the ISR and can make an unambiguous choice (e.g., pick the replica with the highest LEO). This avoids split-brain scenarios that can occur in leaderless voting if network partitions are not properly handled.\n  - **Integration with Metadata:** The coordinator already manages cluster metadata (brokers, topics). Extending it to manage partition leadership state is a natural cohesion of responsibilities.\n\n- **Consequences:**\n  - **Single Point of Failure:** The coordinator becomes a critical component. If it crashes, leader elections cannot occur until a new coordinator is elected. We mitigate this by making the coordinator role **electable** among brokers (similar to Kafka's controller election), but this adds complexity.\n  - **Increased Latency on Failover:** Detection and election involve two hops: follower detects leader failure, reports to coordinator, coordinator elects new leader. This may take slightly longer than a follower immediately taking over, but within acceptable bounds for our educational system.\n  - **Simplified Follower Logic:** Followers only need to report leader failure and accept leadership assignments; they don't need election logic.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| **Coordinator-Managed** | Simple to implement; Deterministic; Centralized metadata consistency. | Coordinator is a SPOF; Slightly slower failover. | **Yes** |\n| **Follower-Triggered** | Faster failover; No single point of failure. | Requires complex consensus protocol; Risk of split-brain; Harder to implement correctly. | No |\n\nThe election algorithm, managed by the coordinator, proceeds as follows:\n\n1. **Failure Detection:** The coordinator receives heartbeat or metadata from brokers. If the leader broker for a partition fails to heartbeat within `session.timeout.ms`, it is considered dead.\n2. **ISR Validation:** The coordinator fetches the latest ISR for the partition from its persisted metadata.\n3. **Leader Selection:** From the ISR, select the replica with the highest **Log End Offset** (the most up-to-date). If multiple have the same LEO, choose the first by broker ID for determinism.\n4. **Leadership Transition:** Send a `BeginLeaderElection` RPC to the chosen broker. The broker validates it is in the ISR, updates its `Partition.LeaderBrokerID` to itself, and begins accepting produce/fetch requests.\n5. **Metadata Propagation:** The coordinator updates the `PartitionMetadata` for all brokers and notifies them of the new leader.\n\n### 8.6 Common Pitfalls\n\nImplementing replication is fraught with subtle bugs that can lead to data loss or inconsistency. Here are the most common pitfalls learners encounter.\n\n⚠️ **Pitfall 1: Allowing Unclean Leader Election**\n- **Description:** Electing a leader that is **not** in the ISR (i.e., a lagging follower) because the ISR has become empty. This new leader may be missing messages that were acknowledged to producers, causing **data loss**.\n- **Why It's Wrong:** The fundamental guarantee of durability is violated. Producers that received an acknowledgment (`acks=all`) believe their message is durable, but it is lost.\n- **How to Fix:** Implement a strict policy: **never elect a non-ISR replica**. If the ISR becomes empty, the partition must become unavailable (return an error to clients) until a replica recovers and rejoins the ISR. This is the \"unclean.leader.election.enable=false\" policy in Kafka.\n\n⚠️ **Pitfall 2: ISR Shrinking to Zero**\n- **Description:** If followers are consistently slower than the leader (due to network or disk issues), they may be removed from the ISR one by one until no followers remain. This leaves the leader as the only ISR member, which is risky—if it fails, no eligible successor exists.\n- **Why It's Wrong:** The system loses fault tolerance. While the partition remains available, it is one failure away from permanent unavailability.\n- **How to Fix:** Monitor ISR size and alert if it falls below a minimum threshold (e.g., `min.insync.replicas`). Producers can be configured to require a minimum ISR size (`acks=all` will fail if ISR size is below this minimum), trading availability for safety. Also, tune replica fetch parameters to reduce lag.\n\n⚠️ **Pitfall 3: Incorrect High Watermark Update**\n- **Description:** Updating the high watermark based on **all replicas** (including out-of-sync ones) or updating it before a follower's write is durable on disk. This can cause the watermark to advance too slowly (hindering consumers) or, worse, to advance too quickly (exposing uncommitted data).\n- **Why It's Wrong:** If the high watermark advances before data is durable on followers, and the leader crashes, a new leader may not have that data, yet consumers have already read it (data loss for consumers). Conversely, a stagnant watermark reduces consumer throughput.\n- **How to Fix:** The high watermark must be the minimum LEO **only among current ISR members**. Additionally, a follower should only update its LEO (and thus be counted for the leader's watermark calculation) after it has durably appended the records to its local log (fsync). The leader should wait for this acknowledgment.\n\n⚠️ **Pitfall 4: Not Handling Leader Epoch**\n- **Description:** Failing to track a **leader epoch**—a monotonically increasing number for each leader change—can cause **message duplication** or **reordering** after a leader failover.\n- **Why It's Wrong:** Without epoch tracking, a former leader that comes back online (e.g., after a network partition) may still think it's the leader and accept writes, creating a split-brain scenario with divergent logs.\n- **How to Fix:** Introduce a `LeaderEpoch` field in `PartitionMetadata`. Each new leader increments the epoch. Include the epoch in every replication RPC and client request. Followers and clients reject requests from stale leaders (with older epochs). The leader also truncates its log to the last known offset for each epoch to maintain consistency.\n\n⚠️ **Pitfall 5: Busy-Waiting in Follower Fetch Loop**\n- **Description:** Followers that immediately re-send fetch requests when caught up, consuming excessive CPU and network resources.\n- **Why It's Wrong:** Inefficient resource usage; can overwhelm the leader with requests.\n- **How to Fix:** Implement **long polling**: the leader holds the fetch request for a short period (`replica.fetch.wait.ms`) if there is no new data, returning immediately when data arrives or the wait time elapses. This reduces the request rate while maintaining low latency.\n\n### 8.7 Implementation Guidance\n\nThis section provides starter code and structure for the replication module in Go.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Replication Transport** | Plain TCP with custom binary protocol (reuse `TCPServer`) | gRPC with streaming for fetch requests |\n| **Metadata Storage** | In-memory map in the coordinator, persisted to a local WAL file | Embedded key-value store (BadgerDB) for fault-tolerant metadata |\n| **Leader Election** | Centralized coordinator with manual failover | Raft consensus for coordinator fault tolerance |\n| **Follower Sync** | Periodic fetch with sleep interval | Long-polling fetch with `net.Conn` read timeout |\n\n#### B. Recommended File/Module Structure\n\nAdd the following directories and files to your project. This separates replication logic from the core broker and coordinator.\n\n```\nproject-root/\n  internal/\n    broker/\n      server.go           # Main broker logic (existing)\n      log_manager.go      # Log management (existing)\n    replication/\n      replica_manager.go  # Main replication logic on each broker\n      follower_syncer.go  # Follower sync loop\n      isr_manager.go      # ISR tracking and high watermark updates\n      leader_election.go  # Leader election logic (in coordinator)\n    coordinator/\n      coordinator.go      # Extended with controller functions\n      metadata_store.go   # Persists topic/partition metadata\n    protocol/\n      replication.go      # Replication RPC request/response structs\n```\n\n#### C. Infrastructure Starter Code: Replication RPC Protocol\n\nFirst, define the replication-specific protocol messages. These are separate from client protocol messages.\n\n```go\n// internal/protocol/replication.go\npackage protocol\n\n// FetchReplicaRequest is sent by follower to leader to fetch records.\ntype FetchReplicaRequest struct {\n    Topic           string\n    PartitionID     int32\n    FollowerBrokerID int32\n    FetchOffset     int64\n    MaxBytes        int32\n    // LeaderEpoch for fencing\n    CurrentLeaderEpoch int32\n}\n\n// FetchReplicaResponse is returned by leader to follower.\ntype FetchReplicaResponse struct {\n    ErrorCode       int16 // e.g., ErrNone, ErrNotLeaderForPartition\n    HighWatermark   int64\n    LeaderEpoch     int32\n    Records         []*Record\n}\n\n// UpdateISRRequest is sent by leader to coordinator.\ntype UpdateISRRequest struct {\n    Topic       string\n    PartitionID int32\n    NewISR      []int32\n    LeaderEpoch int32\n    // ZK version or similar for optimistic concurrency\n    ZKVersion   int32\n}\n\n// UpdateISRResponse is returned by coordinator.\ntype UpdateISRResponse struct {\n    ErrorCode int16\n}\n\n// LeaderForPartitionRequest asks for the current leader of a partition.\ntype LeaderForPartitionRequest struct {\n    Topic       string\n    PartitionID int32\n}\n\n// LeaderForPartitionResponse contains leader info.\ntype LeaderForPartitionResponse struct {\n    ErrorCode       int16\n    LeaderBrokerID  int32\n    LeaderEpoch     int32\n}\n\n// BeginLeaderElectionRequest instructs a broker to become leader.\ntype BeginLeaderElectionRequest struct {\n    Topic       string\n    PartitionID int32\n    // The ISR at time of election, for validation\n    ISR         []int32\n    LeaderEpoch int32 // The new epoch\n}\n\n// BeginLeaderElectionResponse confirms leadership taken.\ntype BeginLeaderElectionResponse struct {\n    ErrorCode int16\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**1. Replica Manager** (runs on each broker)\n\n```go\n// internal/replication/replica_manager.go\npackage replication\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \"yourproject/internal/types\"\n)\n\n// ReplicaManager manages all replica sync tasks for partitions hosted on this broker.\ntype ReplicaManager struct {\n    brokerID      int32\n    config        ReplicaConfig\n    // Maps TopicPartition -> FollowerSyncer (for partitions where we are follower)\n    followers     map[types.TopicPartition]*FollowerSyncer\n    // Maps TopicPartition -> ISRManager (for partitions where we are leader)\n    leaders       map[types.TopicPartition]*ISRManager\n    mu            sync.RWMutex\n    ctx           context.Context\n    cancel        context.CancelFunc\n}\n\ntype ReplicaConfig struct {\n    FetchWaitMs          int\n    ReplicaFetchMinBytes int32\n    ReplicaFetchMaxBytes int32\n    ReplicaLagTimeMaxMs  int\n}\n\n// NewReplicaManager creates a new manager.\nfunc NewReplicaManager(brokerID int32, config ReplicaConfig) *ReplicaManager {\n    ctx, cancel := context.WithCancel(context.Background())\n    return &ReplicaManager{\n        brokerID: brokerID,\n        config:   config,\n        followers: make(map[types.TopicPartition]*FollowerSyncer),\n        leaders:   make(map[types.TopicPartition]*ISRManager),\n        ctx:       ctx,\n        cancel:    cancel,\n    }\n}\n\n// Start begins all sync loops for followers and leaders.\nfunc (rm *ReplicaManager) Start() error {\n    // TODO 1: Load partition assignments from local metadata (which partitions this broker hosts)\n    // TODO 2: For each partition where this broker is a follower, create a FollowerSyncer and start its run loop\n    // TODO 3: For each partition where this broker is the leader, create an ISRManager and start its monitoring loop\n    // TODO 4: Start a background goroutine to handle partition assignment changes (e.g., after rebalance)\n    return nil\n}\n\n// OnNewPartitionAssignment is called when the broker's assigned partitions change.\nfunc (rm *ReplicaManager) OnNewPartitionAssignment(assignments []types.PartitionMetadata) {\n    // TODO 1: Compare new assignments with current followers/leaders maps\n    // TODO 2: Stop and remove syncers for partitions we are no longer assigned\n    // TODO 3: Add new syncers for newly assigned partitions (determine role: leader or follower)\n    // TODO 4: For partitions where role changed (follower->leader or vice versa), recreate the syncer\n}\n```\n\n**2. Follower Syncer** (per partition where broker is follower)\n\n```go\n// internal/replication/follower_syncer.go\npackage replication\n\nimport (\n    \"context\"\n    \"time\"\n    \"yourproject/internal/types\"\n)\n\n// FollowerSyncer continuously fetches from the leader for a single partition.\ntype FollowerSyncer struct {\n    brokerID      int32\n    topic         string\n    partitionID   int32\n    config        ReplicaConfig\n    log           types.Log // local log to append to\n    // Connection pool to talk to other brokers\n    connPool      *ConnectionPool\n    leaderBrokerID int32\n    leaderEpoch   int32\n    fetchOffset   int64\n    mu            sync.RWMutex\n    ctx           context.Context\n    cancel        context.CancelFunc\n}\n\n// Run is the main sync loop.\nfunc (fs *FollowerSyncer) Run() {\n    // TODO 1: Discover leader using LeaderForPartition RPC (retry until successful)\n    // TODO 2: Loop until context is done:\n    //   a. Send FetchReplicaRequest to leader with current fetchOffset\n    //   b. On response:\n    //        i. If error is ErrNotLeaderForPartition, break to rediscover leader\n    //       ii. If no error, append records to local log (ensure offset continuity)\n    //      iii. Advance fetchOffset by number of records appended\n    //       iv. Update high watermark from response (store in partition metadata)\n    //   c. If no records returned, sleep for config.FetchWaitMs before next fetch (long polling)\n    //   d. If fetch failed (network error), backoff exponentially and retry\n}\n\n// AppendToLocalLog writes records to the local log filesystem.\nfunc (fs *FollowerSyncer) AppendToLocalLog(records []*types.Record, leaderHW int64) error {\n    // TODO 1: Acquire lock on log to ensure sequential writes\n    // TODO 2: Validate that the first record's offset equals the current local LEO (no gaps)\n    // TODO 3: Call log.Append(records)\n    // TODO 4: Update in-memory high watermark for this partition (min(leaderHW, localLEO))\n    // TODO 5: Return any error from append\n    return nil\n}\n```\n\n**3. ISR Manager** (per partition where broker is leader)\n\n```go\n// internal/replication/isr_manager.go\npackage replication\n\nimport (\n    \"context\"\n    \"time\"\n    \"yourproject/internal/types\"\n)\n\n// ISRManager tracks followers and manages ISR for a partition where this broker is leader.\ntype ISRManager struct {\n    topic         string\n    partitionID   int32\n    config        ReplicaConfig\n    partition     *types.Partition // reference to the partition object\n    // Map followerBrokerID -> lastCaughtUpTime and lastFetchOffset\n    followerState map[int32]*followerStatus\n    mu            sync.RWMutex\n    ctx           context.Context\n    cancel        context.CancelFunc\n}\n\ntype followerStatus struct {\n    lastFetchOffset int64\n    lastFetchTime   time.Time\n}\n\n// Start begins the periodic ISR check.\nfunc (im *ISRManager) Start() {\n    ticker := time.NewTicker(time.Duration(im.config.ReplicaLagTimeMaxMs) * time.Millisecond)\n    defer ticker.Stop()\n    for {\n        select {\n        case <-im.ctx.Done():\n            return\n        case <-ticker.C:\n            im.evaluateISR()\n        }\n    }\n}\n\n// evaluateISR checks each follower's lag and updates ISR.\nfunc (im *ISRManager) evaluateISR() {\n    // TODO 1: Get current leader LEO from partition.Log\n    // TODO 2: For each follower in partition.ReplicaBrokerIDs:\n    //   a. Compute lag = leaderLEO - followerState[brokerID].lastFetchOffset\n    //   b. Compute time since last fetch\n    //   c. If lag > replica.lag.max.messages OR time since last fetch > replica.lag.time.max.ms:\n    //        Remove follower from ISR (if present)\n    //   d. Else if follower is not in ISR and lag == 0:\n    //        Add follower back to ISR\n    // TODO 3: If ISR changed:\n    //   a. Update partition.ISR\n    //   b. Persist new ISR via UpdateISR RPC to coordinator\n    //   c. Recalculate high watermark (min LEO across new ISR)\n}\n\n// UpdateFollowerState is called when a FetchReplica request is processed.\nfunc (im *ISRManager) UpdateFollowerState(followerID int32, fetchOffset int64, fetchedBytes int) {\n    // TODO 1: Update followerState map with new offset and current time\n    // TODO 2: If fetchOffset == leaderLEO (follower is caught up), ensure follower is in ISR\n}\n```\n\n**4. Leader Election in Coordinator**\n\n```go\n// internal/coordinator/leader_election.go\npackage coordinator\n\nimport (\n    \"context\"\n    \"sort\"\n    \"time\"\n    \"yourproject/internal/types\"\n)\n\n// LeaderElector runs in the coordinator and manages partition leader elections.\ntype LeaderElector struct {\n    metadataStore *MetadataStore\n    brokerPool    *BrokerPool // to send RPCs\n    ctx           context.Context\n}\n\n// OnBrokerFailure triggers leader election for partitions whose leader was the failed broker.\nfunc (le *LeaderElector) OnBrokerFailure(failedBrokerID int32) {\n    // TODO 1: Query metadataStore for all partitions where leader == failedBrokerID\n    // TODO 2: For each such partition:\n    //   a. Get current ISR from metadata\n    //   b. Remove failedBrokerID from ISR (if present)\n    //   c. If ISR is empty, log error and mark partition offline (no leader)\n    //   d. Else:\n    //        i. Select new leader: replica in ISR with highest LEO (from metadata)\n    //       ii. Send BeginLeaderElection RPC to selected broker\n    //      iii. On success, update metadataStore with new leader and ISR\n    //       iv. Propagate metadata update to all brokers\n}\n\n// selectNewLeader chooses a replica from ISR to become leader.\nfunc (le *LeaderElector) selectNewLeader(isr []int32, partitionInfo types.PartitionMetadata) int32 {\n    // TODO 1: If ISR is empty, return -1 (no leader possible)\n    // TODO 2: Fetch LEO for each replica in ISR from metadata (may be stale)\n    // TODO 3: Sort replicas by LEO descending, then by brokerID ascending\n    // TODO 4: Return the first replica ID\n    return -1\n}\n```\n\n#### E. Language-Specific Hints\n\n- **Concurrency:** Use `sync.RWMutex` to protect `Partition.ISR` and `Partition.HighWatermark`. Followers update high watermark on read; leaders update it and ISR on write.\n- **Disk Persistence:** When a follower appends records, it must call `log.Sync()` if the leader's request had `acks=all` to ensure durability before acknowledging to leader.\n- **Context for Cancellation:** Pass `context.Context` to all long-running loops (follower sync, ISR monitor) to allow graceful shutdown on broker stop.\n- **Time:** Use `time.Time` for last fetch timestamps. `time.Since()` is convenient for calculating lag duration.\n- **Network Errors:** Use `net.Error` type checks to differentiate temporary vs permanent network failures. Implement exponential backoff with `time.Sleep` for temporary errors.\n\n#### F. Milestone Checkpoint\n\nAfter implementing replication, you should be able to verify the following scenario:\n\n1. **Start a 3-broker cluster** (e.g., on ports 9092, 9093, 9094). Designate broker 1 as the coordinator/controller.\n2. **Create a topic** with 1 partition and replication factor 3.\n3. **Produce messages** with `acks=all`. Observe logs: the leader should append, and followers should fetch and append.\n4. **Kill the leader broker** (e.g., kill -9). Wait for the session timeout (e.g., 10 seconds).\n5. **Produce more messages** to the same topic. The new leader should be elected and accept writes.\n6. **Read all messages** from the beginning. You should see **no data loss**—all messages from step 3 and step 5 should be present, in correct offset order.\n\n**Expected Logs:**\n- Follower logs: \"Fetching from leader broker X at offset Y\"\n- Leader logs: \"Updated ISR to [1,2,3]\"\n- Coordinator logs: \"Broker 2 failed, electing new leader for topic T partition 0\"\n- New leader logs: \"Assuming leadership for topic T partition 0, epoch Z\"\n\n**Debugging Tips:**\n- If followers are not fetching, check that they know the correct leader (metadata cache).\n- If ISR shrinks unexpectedly, check follower fetch frequency and network latency.\n- If high watermark does not advance, ensure leader is calculating min LEO across ISR correctly.\n\n---\n\n\n## 9. Interactions and Data Flow\n\n> **Milestone(s):** 2 (Producer), 3 (Consumer Groups), 4 (Replication)\n\nUnderstanding how system components interact is crucial for building a correct distributed message queue. This section details the choreography between producers, brokers, consumers, and coordinators through three key perspectives: sequence diagrams showing the temporal flow of operations, the binary wire protocol that defines their communication format, and the coordination service that maintains cluster metadata. These three layers—behavioral, syntactic, and infrastructural—form the foundation of reliable distributed communication.\n\n### 9.1 Key Sequence Diagrams\n\nSequence diagrams provide a visual timeline of interactions between system components, helping developers understand the causality and concurrency in distributed operations. Think of these diagrams as **air traffic control logs**—they show which aircraft (components) communicated with the tower (brokers) at what time, what instructions were exchanged, and how the overall flow of traffic is coordinated to avoid collisions and ensure safe delivery.\n\n#### 9.1.1 Message Production Flow\n\nThe producer-to-broker interaction follows a \"prepare-route-send-acknowledge\" pattern that balances throughput with durability guarantees. This flow resembles a **courier service delivery process**: a sender packages multiple items into a single shipment (batching), addresses them to specific destinations (partition selection), dispatches them via the most efficient route (network send), and waits for delivery confirmation (acknowledgment).\n\n![Producer Send Sequence](./diagrams/diagram-producer-flow.svg)\n\nThe complete production sequence involves these steps:\n\n1. **Metadata Resolution**: The producer consults its local `MetadataCache` to determine which broker leads the target partition. If cache is stale or missing, it sends a metadata request to any bootstrap broker.\n\n2. **Record Preparation**: For each message, the producer:\n   - Serializes key, value, and headers into a `Record`\n   - Applies optional compression to the entire batch\n   - Calculates CRC32 checksum for data integrity verification\n\n3. **Partition Assignment**: Using the configured `Partitioner` (typically `HashPartitioner`), the producer determines the target partition:\n   - For records with keys: `hash(key) % partition_count` ensures consistent mapping\n   - For null-key records: round-robin distribution across partitions\n   - The partition assignment determines which broker (leader) receives the batch\n\n4. **Batch Accumulation**: Records are grouped by `TopicPartition` in the `Accumulator`:\n   - Batches grow until reaching `BatchSize` bytes or `LingerMs` timeout\n   - Each batch becomes a `RecordBatch` with common metadata (base offset, timestamps)\n   - The accumulator maintains an in-memory buffer limited by `BufferMemory`\n\n5. **Network Dispatch**: The `Sender` thread retrieves ready batches and:\n   - Establishes or reuses a TCP connection to the partition leader\n   - Encodes the batch using the binary protocol (Section 9.2)\n   - Sends the request with the configured `Acks` level\n\n6. **Acknowledgement Handling**: Based on the `Acks` configuration:\n   - `AcksNone` (0): Fire-and-forget—no response expected, maximum throughput\n   - `AcksLeader` (1): Wait for leader to append to its local log (fsync optional)\n   - `AcksAll` (-1): Wait for all ISR replicas to acknowledge (highest durability)\n   - On timeout or error, `RetryItem` is scheduled with exponential backoff\n\n7. **Delivery Completion**: Successful acknowledgment includes:\n   - Partition ID and starting offset for the batch\n   - Leader epoch for fencing protection\n   - Error code (0 for success)\n   - The producer invokes any registered callback with this information\n\nThe critical insight is that batching happens at **two levels**: within the producer (multiple records) and within the broker (multiple batches to disk). This double batching amortizes network and disk I/O overhead, making high-throughput streaming possible.\n\n| Step | Component | Action | Key Consideration |\n|------|-----------|--------|-------------------|\n| 1 | Producer | Fetch metadata | Cache TTL vs. leader change frequency |\n| 2 | Producer | Serialize record | Compression trade-off: CPU vs. bandwidth |\n| 3 | Partitioner | Select partition | Hash consistency during partition count changes |\n| 4 | Accumulator | Group records | Memory pressure vs. latency trade-off |\n| 5 | Sender | Send batch | Connection pooling and TCP congestion control |\n| 6 | Broker | Process request | Durability vs. latency based on acks |\n| 7 | Broker | Send response | Include leader epoch for client fencing |\n\n#### 9.1.2 Consumer Group Rebalancing Flow\n\nConsumer group rebalancing is a distributed consensus protocol that redistributes partitions when membership changes. Imagine a **sports team drafting players**: when a new player joins or leaves, the team captain (coordinator) reassesses everyone's positions, reallocates responsibilities, and ensures each player knows exactly what territory they cover.\n\n![Consumer Group Rebalance Sequence](./diagrams/diagram-rebalance-sequence.svg)\n\nThe rebalancing protocol follows a three-phase \"join-sync-stabilize\" pattern:\n\n**Phase 1: Membership Change Detection**\n1. A new consumer calls `Subscribe()` or an existing consumer fails to send `Heartbeat` within `SessionTimeoutMs`\n2. The `GroupCoordinator` detects membership change and transitions group state to `GroupStatePreparingRebalance`\n3. All existing members receive `ErrRebalanceInProgress` on their next heartbeat, signaling them to rejoin\n\n**Phase 2: Join Group Synchronization**\n4. Each consumer (including the new one) sends `JoinGroupRequest` containing:\n   - `GroupID`: Identifier for the consumer group\n   - `MemberID`: Current member ID (empty for new members)\n   - `ProtocolType`: Always \"consumer\" for this system\n   - `Protocols`: List of supported partition assignment strategies (range, round-robin)\n   - `SessionTimeoutMs`: Maximum time without heartbeat before considered dead\n\n5. The coordinator waits up to `RebalanceTimeoutMs` for all expected members to join:\n   - First joiner becomes the \"leader\" consumer (selected by coordinator)\n   - Coordinator records each member's subscribed topics and supported protocols\n   - If timeout expires, proceeds with currently joined members (stragglers excluded)\n\n**Phase 3: Assignment Distribution**\n6. The coordinator sends `JoinGroupResponse` to all members containing:\n   - `GenerationID`: Incremented epoch number for this assignment\n   - `LeaderID`: Member ID of the designated leader consumer\n   - Assigned `MemberID` for each consumer (new members receive generated IDs)\n\n7. The leader consumer computes partition assignments using the selected `PartitionAssigner`:\n   - Gathers all subscribed topics from member metadata\n   - Applies assignment strategy (range or round-robin) to distribute partitions\n   - Creates `Assignment` mapping from member ID to topic-partition list\n\n8. Each member sends `SyncGroupRequest`:\n   - Leader includes the computed assignments in its request\n   - Followers send empty assignment payload\n\n9. Coordinator distributes assignments via `SyncGroupResponse`:\n   - Validates leader's assignment covers all partitions\n   - Sends each member their specific assigned partitions\n   - Updates stored group metadata with new generation\n\n**Phase 4: Stable Operation**\n10. Upon receiving assignments, each consumer:\n    - Updates its `GroupMember` state to `StateStable`\n    - Begins fetching from assigned partitions starting from committed offsets\n    - Resumes periodic heartbeats to maintain membership\n\n11. The coordinator transitions group to `GroupStateStable` and monitors heartbeats\n\nThe protocol ensures **exactly-once partition ownership** within a generation: while a consumer holds a generation ID, it uniquely owns its assigned partitions. If a consumer fails to heartbeat, the coordinator increments the generation, invalidating the previous owner's claims.\n\n| Rebalance Trigger | Detection Mechanism | Coordinator Action | Consumer Action |\n|-------------------|---------------------|-------------------|-----------------|\n| New consumer joins | `JoinGroupRequest` from unknown member | Transition to `GroupStatePreparingRebalance` | Re-send join after receiving error |\n| Consumer leaves | Heartbeat timeout after `SessionTimeoutMs` | Mark member dead, trigger rebalance | N/A (consumer is dead) |\n| Consumer crashes | TCP connection drop + heartbeat timeout | Same as leave | N/A (consumer crashed) |\n| Topic metadata changes | Admin API or broker notification | Trigger rebalance if partition count changed | Rejoin when notified |\n| Manual rebalance | External command via API | Force transition to rebalancing state | All members receive error code |\n\n#### 9.1.3 Message Consumption Flow\n\nConsumer message fetching follows a \"poll-fetch-advance-commit\" cycle that balances throughput with memory constraints. Picture a **library checkout system**: patrons (consumers) periodically visit the library (broker), check out several books (records) at once, read them at their own pace, and update their checkout record (commit offset) before returning for more.\n\nThe consumption sequence proceeds as follows:\n\n1. **Subscription Initialization**: Consumer calls `Subscribe()` which triggers the join-sync protocol (Section 9.1.2), resulting in assigned partitions.\n\n2. **Offset Initialization**: For each assigned partition, consumer determines starting position:\n   - If `auto.offset.reset` = \"earliest\": starts at partition's lowest available offset\n   - If \"latest\": starts at current `LogEndOffset` (only new messages)\n   - If valid committed offset exists: resumes from `OffsetStore` persisted value\n   - Consumer maintains `FetchOffset` per partition as its read position\n\n3. **Fetch Request Preparation**: Consumer builds `FetchRequest` containing:\n   - Maximum wait time (`MaxWaitMs`) for broker to accumulate data\n   - Minimum bytes (`MinBytes`) to return before satisfying request\n   - Maximum bytes (`MaxBytes`) per partition to prevent memory overflow\n   - Per-partition fetch state: `TopicPartition`, `FetchOffset`, `MaxBytes`\n\n4. **Broker Processing**: For each requested partition, broker leader:\n   - Validates consumer has read permission and offset is within range\n   - Reads from `Log` starting at `FetchOffset` up to `MaxBytes` or log end\n   - Returns records with `HighWatermark` (last safely replicable offset)\n   - If `FetchOffset` ≥ `HighWatermark`, waits up to `MaxWaitMs` for new data\n   - Returns empty batch if no new data after wait period\n\n5. **Record Delivery**: Consumer receives `FetchResponse` and:\n   - Processes records in offset order within each partition\n   - Delivers to application via `Poll()` return or callback\n   - Advances `FetchOffset` past delivered records\n   - Updates metrics (bytes consumed, lag, throughput)\n\n6. **Offset Commitment**: Periodically or explicitly, consumer commits progress:\n   - `CommitSync()` blocks until offset stored durably\n   - `CommitAsync()` returns immediately, calls callback on completion\n   - Offset committed to `__consumer_offsets` internal topic via broker\n   - Coordinator validates generation ID to prevent stale commits\n\n7. **Rebalance Handling**: If rebalance occurs during consumption:\n   - Consumer completes current batch processing\n   - Commits offsets for revoked partitions\n   - Releases partition ownership (stops fetching)\n   - Rejoins group for new assignment\n\nThe **fetch session** optimization is crucial: consumers maintain long-lived connections to brokers, sending incremental fetch requests that only list changed partitions. This reduces request size and parsing overhead for steady-state consumption.\n\n### 9.2 Wire Protocol Specification\n\nThe wire protocol defines the binary format for all network communication between components. Think of it as the **international shipping container standard**: just as standardized containers enable efficient global logistics regardless of cargo type, a well-defined binary protocol enables interoperability between different client implementations and server versions while minimizing parsing overhead.\n\n#### 9.2.1 Protocol Design Principles\n\nThe protocol follows several key design decisions documented in this ADR:\n\n> **Decision: Binary Protocol Over Text Protocol**\n> - **Context**: Need for high-throughput, low-latency communication between distributed components with efficient parsing and minimal serialization overhead.\n> - **Options Considered**:\n>   1. **Text-based (JSON/XML)**: Human-readable, easy to debug, but high parsing cost and large payload size\n>   2. **Binary with TLV (Type-Length-Value)**: Self-describing, flexible evolution, but header overhead per field\n>   3. **Fixed-position binary**: Compact, fast parsing, but requires versioning for schema changes\n> - **Decision**: Fixed-position binary protocol with versioned requests and explicit schema evolution rules.\n> - **Rationale**: Throughput and latency are primary concerns for a messaging system. Binary encoding reduces serialization costs by 5-10x compared to JSON, and fixed-position parsing avoids heap allocations during deserialization. Versioning allows backward-compatible evolution.\n> - **Consequences**: Protocol debugging requires specialized tools, but the performance benefits justify the complexity. Schema changes must follow compatibility rules (add-only fields, bump version).\n\n| Protocol Aspect | Design Choice | Rationale | Trade-off |\n|-----------------|---------------|-----------|-----------|\n| **Encoding** | Binary, big-endian | Network byte order standard | Not human-readable |\n| **Framing** | Length-prefixed | Simple parsing, no delimiters | Requires length prefix overhead |\n| **Versioning** | Per-request API key + version | Backward compatibility | Multiple code paths |\n| **Error handling** | Error code per response partition | Granular failure reporting | Additional complexity |\n| **Compression** | Per-record-batch | Reduces network bandwidth | CPU overhead, delayed batching |\n\n#### 9.2.2 Common Request/Response Structure\n\nAll protocol messages follow the same envelope structure:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Size` | int32 | Total message size in bytes (excluding this field) |\n| `APIKey` | int16 | Numeric identifier for the request type (Produce=0, Fetch=1, etc.) |\n| `APIVersion` | int16 | Version of the API for forward/backward compatibility |\n| `CorrelationID` | int32 | Client-generated ID to match responses to requests |\n| `ClientID` | string | Client identifier for debugging (nullable) |\n| `Request/Response Body` | variable | API-specific structured data |\n\nThe **version negotiation** process: clients send requests with their supported version; brokers respond with the same version if supported, or downgrade/error if not. This enables gradual feature rollout without breaking existing clients.\n\n#### 9.2.3 Produce Request/Response Format\n\nThe Produce API writes records to partition logs with configurable durability guarantees.\n\n**ProduceRequest (API Key: 0)**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `TransactionID` | int64 | For idempotent producers, 0 otherwise |\n| `Acks` | int16 | Required acknowledgments: -1=all, 0=none, 1=leader |\n| `TimeoutMs` | int32 | Maximum time to wait for `Acks` fulfillment |\n| `TopicData` | array | Topics to produce to (see below) |\n\n**TopicData Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Topic` | string | Topic name |\n| `Data` | array | Partition data (see below) |\n\n**PartitionData Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Partition` | int32 | Partition index |\n| `RecordSet` | bytes | Serialized `RecordBatch` (Section 4.2) |\n\n**ProduceResponse**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Responses` | array | Per-topic response (see below) |\n| `ThrottleTimeMs` | int32 | Time client should wait before next request |\n\n**TopicResponse Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Topic` | string | Topic name |\n| `PartitionResponses` | array | Per-partition response (see below) |\n\n**PartitionResponse Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Partition` | int32 | Partition index |\n| `ErrorCode` | int16 | 0=success, non-zero error codes |\n| `BaseOffset` | int64 | Offset of first record in batch |\n| `LogAppendTime` | int64 | Broker timestamp when appended |\n| `LogStartOffset` | int64 | New log start offset after compaction |\n\nThe `RecordSet` field contains a complete serialized `RecordBatch` as defined in Section 4.2, including compression. Brokers validate CRC before appending to log.\n\n#### 9.2.4 Fetch Request/Response Format\n\nThe Fetch API reads records from partition logs with configurable batching and wait semantics.\n\n**FetchRequest (API Key: 1)**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ReplicaID` | int32 | -1 for consumers, broker ID for replication |\n| `MaxWaitMs` | int32 | Maximum time to block waiting for `MinBytes` |\n| `MinBytes` | int32 | Minimum bytes to accumulate before responding |\n| `MaxBytes` | int32 | Maximum bytes to return in response |\n| `IsolationLevel` | int8 | 0=read uncommitted, 1=read committed (≤ high watermark) |\n| `SessionID` | int32 | Fetch session ID for incremental fetches |\n| `SessionEpoch` | int32 | Fetch session epoch for incremental fetches |\n| `Topics` | array | Topics to fetch from (see below) |\n| `ForgottenTopics` | array | Topics to remove from session (incremental only) |\n\n**TopicFetch Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Topic` | string | Topic name |\n| `Partitions` | array | Partitions to fetch (see below) |\n\n**PartitionFetch Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Partition` | int32 | Partition index |\n| `FetchOffset` | int64 | Starting offset to fetch from |\n| `LogStartOffset` | int64 | Earliest available offset (for truncation detection) |\n| `MaxBytes` | int32 | Maximum bytes to fetch for this partition |\n\n**FetchResponse**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ThrottleTimeMs` | int32 | Time client should wait before next request |\n| `ErrorCode` | int16 | Top-level error (0=success) |\n| `SessionID` | int32 | Fetch session ID (if incremental) |\n| `Responses` | array | Per-topic response (see below) |\n\n**TopicFetchResponse Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Topic` | string | Topic name |\n| `PartitionResponses` | array | Per-partition response (see below) |\n\n**PartitionFetchResponse Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Partition` | int32 | Partition index |\n| `ErrorCode` | int16 | Partition-level error (0=success) |\n| `HighWatermark` | int64 | Last committed offset in partition |\n| `LastStableOffset` | int64 | Last stable offset (for transactions) |\n| `LogStartOffset` | int64 | Earliest available offset |\n| `AbortedTransactions` | array | List of aborted transactions (for read committed) |\n| `RecordSet` | bytes | Serialized `RecordBatch` (empty if no data) |\n\nThe **incremental fetch session** optimization: when `SessionID` ≠ 0, broker remembers requested partitions across requests. Subsequent requests only need to list changed partitions (`ForgottenTopics` to remove), reducing request size significantly.\n\n#### 9.2.5 JoinGroup Request/Response Format\n\nThe JoinGroup API coordinates consumer group membership and leader election.\n\n**JoinGroupRequest (API Key: 11)**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `GroupID` | string | Consumer group identifier |\n| `SessionTimeoutMs` | int32 | Time after which inactive members are removed |\n| `RebalanceTimeoutMs` | int32 | Maximum time coordinator waits for members |\n| `MemberID` | string | Current member ID (empty for new members) |\n| `ProtocolType` | string | \"consumer\" for consumer groups |\n| `Protocols` | array | Supported partition assignment protocols (see below) |\n\n**GroupProtocol Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Name` | string | Protocol name (e.g., \"range\", \"roundrobin\") |\n| `Metadata` | bytes | Serialized protocol metadata (see below) |\n\n**ProtocolMetadata Structure**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Version` | int16 | Metadata format version |\n| `Topics` | array | Subscribed topics list |\n| `UserData` | bytes | Optional custom data |\n\n**JoinGroupResponse**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ErrorCode` | int16 | Top-level error (0=success) |\n| `GenerationID` | int32 | Group generation identifier (increments each rebalance) |\n| `ProtocolName` | string | Selected protocol by coordinator |\n| `LeaderID` | string | Member ID of group leader |\n| `MemberID` | string | Assigned member ID for this consumer |\n| `Members` | array | Group member metadata (only returned to leader) |\n\n**MemberMetadata Array Element**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `MemberID` | string | Member identifier |\n| `Metadata` | bytes | Serialized protocol metadata (same as request) |\n\nThe coordinator selects the **protocol** based on member intersection: if all members support \"range\", it's selected; otherwise, coordinator picks highest common protocol. This enables rolling upgrades of assignment strategies.\n\n### 9.3 Coordination Service Abstraction\n\nThe coordination service maintains cluster metadata and enables consensus for critical operations like leader election. Think of it as the **air traffic control database**: it doesn't control planes directly but maintains the authoritative registry of which runways are active, which controllers are on duty, and which flight plans are approved—information all pilots and towers reference to coordinate safely.\n\n#### 9.3.1 Minimal Coordination Interface\n\nFor our educational system, we define a simplified coordination interface that can be implemented with various backends (in-memory, etcd, ZooKeeper). This abstraction follows the **registry pattern**: components register themselves and subscribe to changes, receiving notifications when relevant metadata updates.\n\n**Core Coordination Interface Methods:**\n\n| Method Signature | Parameters | Returns | Description |\n|------------------|-------------|---------|-------------|\n| `RegisterBroker(broker *Broker) error` | `broker`: Broker metadata | error | Registers a broker with the cluster; generates unique ID if not set |\n| `DeregisterBroker(brokerID int) error` | `brokerID`: Broker identifier | error | Removes broker from cluster; triggers partition reassignment |\n| `GetBrokers() ([]*Broker, error)` | None | Broker list, error | Returns all currently registered brokers |\n| `CreateTopic(topic *TopicMetadata) error` | `topic`: Topic configuration | error | Creates topic with specified partitions and replication factor |\n| `DeleteTopic(topicName string) error` | `topicName`: Topic to delete | error | Marks topic for deletion; brokers clean up replicas |\n| `GetTopicMetadata(topicName string) (*TopicMetadata, error)` | `topicName`: Topic name | Metadata, error | Returns current partition leadership and replica assignments |\n| `UpdatePartitionLeadership(partition *PartitionMetadata) error` | `partition`: Updated metadata | error | Updates leader and ISR for a partition (atomic compare-and-swap) |\n| `ElectPartitionLeader(topic string, partition int, isr []int) (int, error)` | `topic`, `partition`, `isr` | leaderID, error | Selects new leader from ISR using deterministic algorithm |\n| `WatchBrokers(callback func([]*Broker)) (cancel func(), error)` | `callback`: Change handler | cancel function, error | Registers for broker list changes (for client metadata updates) |\n| `WatchTopic(topicName string, callback func(*TopicMetadata)) (cancel func(), error)` | `topicName`, `callback` | cancel function, error | Registers for topic metadata changes (for producer routing) |\n\n**State Management Guarantees:**\n\n> **Design Principle: Eventual Consistency with Strong Consistency for Critical Paths**\n> - **Metadata reads** may be slightly stale (cached for performance)\n> - **Leadership elections** are linearizable (all observers see same leader)\n> - **Partition assignment** uses compare-and-swap to prevent lost updates\n> - **Watch notifications** are at-least-once (may receive duplicates)\n\n#### 9.3.2 Metadata Propagation Patterns\n\nCluster metadata flows through the system using a **publish-subscribe with cache invalidation** pattern. This minimizes coordination traffic while ensuring timely updates:\n\n1. **Broker Registration Flow**:\n   ```\n   Broker startup → RegisterBroker() → Coordination service stores → \n   WatchBrokers() callbacks fire → All brokers update ClusterMetadata →\n   Clients fetch metadata on next request → Updated routing tables\n   ```\n\n2. **Leader Election Flow**:\n   ```\n   Leader failure detected → ElectPartitionLeader() → Coordination service selects →\n   UpdatePartitionLeadership() → WatchTopic() callbacks fire → \n   Followers update their replica state → Clients get ErrNotLeaderForPartition →\n   Client metadata refresh → New routing established\n   ```\n\n3. **Topic Creation Flow**:\n   ```\n   Admin creates topic → CreateTopic() → Coordination service validates →\n   Assign partitions to brokers → Store TopicMetadata → \n   WatchTopic() callbacks fire → Brokers create local log directories →\n   Clients can produce/consume\n   ```\n\nThe **metadata cache lifecycle** in clients:\n- Cache initialized with bootstrap broker list\n- On each request, check cache freshness (timestamp)\n- On `ErrNotLeaderForPartition` or `ErrUnknownTopicOrPartition`, force refresh\n- Refresh interval: min(metadata.max.age.ms, calculated based on error rate)\n- Cache entries include generation ID to detect stale updates\n\n#### 9.3.3 Fault Tolerance Considerations\n\nThe coordination service itself must be highly available. Our abstraction supports different implementations with varying consistency guarantees:\n\n| Implementation | Consistency Model | Failure Handling | Recommended Use |\n|----------------|-------------------|------------------|-----------------|\n| **In-memory with single leader** | Strong consistency (leader) | Leader election via Raft/ZooKeeper | Small clusters, educational use |\n| **Embedded etcd** | Linearizable via Raft | Automatic failover, data replication | Production-like deployment |\n| **External ZooKeeper** | Sequential consistency | Ensemble quorum maintenance | Integration with existing infra |\n| **Gossip-based** | Eventual consistency | No single point of failure | Large-scale, AP-focused systems |\n\n**Critical coordination paths requiring strong consistency:**\n1. **Partition leadership**: To prevent split-brain (two brokers believing they're leader)\n2. **Consumer group generation**: To prevent duplicate processing during rebalance\n3. **Transaction state**: For exactly-once semantics (future extension)\n4. **Controller election**: To ensure single active controller broker\n\n**Eventual consistency acceptable for:**\n1. **Broker liveness**: Slight delay in detecting failures acceptable\n2. **Topic configuration**: Temporary inconsistency doesn't cause data loss\n3. **Client metadata**: Stale routing corrected via error retry\n\nThe coordination service represents a **single point of truth** but not a **single point of failure** when implemented with replication and automatic failover.\n\n### 9.4 Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Protocol Serialization** | Manual byte packing/unpacking | Protocol Buffers with code generation |\n| **Connection Management** | `net.Conn` with manual pooling | Custom connection pool with keep-alive |\n| **Compression** | None (simplicity) | `compress/gzip` or third-party Snappy |\n| **Coordination Service** | In-memory with file persistence | Embedded etcd (`go.etcd.io/etcd/client/v3`) |\n\n#### Recommended File Structure\n\n```\nproject-root/\n  cmd/\n    broker/                # Broker entry point\n      main.go\n    producer/              # Producer CLI example\n      main.go\n    consumer/              # Consumer CLI example\n      main.go\n  internal/\n    protocol/              # Wire protocol definitions\n      api_keys.go          # API key constants (Produce=0, Fetch=1, etc.)\n      requests.go          # Request structs and serialization\n      responses.go         # Response structs and serialization\n      errors.go            # Error code definitions\n      framing.go           # Length-prefixed message framing\n    network/\n      connection.go        # TCP connection wrapper with timeout\n      pool.go              # Connection pool management\n      server.go            # TCPServer implementation\n    coordination/          # Coordination service abstraction\n      interface.go         # CoordinationService interface\n      memory/              # In-memory implementation\n        store.go           # In-memory metadata store\n        watcher.go         # Watch notification system\n      etcd/                # etcd implementation (optional)\n        client.go\n    metadata/              # Client metadata cache\n      cache.go             # Metadata caching with TTL\n      refresher.go         # Background metadata refresh\n```\n\n#### Protocol Framing Infrastructure (Complete)\n\n```go\n// internal/protocol/framing.go\npackage protocol\n\nimport (\n    \"encoding/binary\"\n    \"errors\"\n    \"io\"\n)\n\nvar (\n    ErrMessageTooLarge = errors.New(\"message size exceeds maximum allowed\")\n    ErrInvalidSize     = errors.New(\"invalid message size field\")\n)\n\nconst (\n    MaxMessageSize = 100 * 1024 * 1024 // 100 MB maximum message size\n    SizeFieldLen   = 4                 // int32 size prefix\n)\n\n// ReadMessage reads a length-prefixed message from the reader\nfunc ReadMessage(r io.Reader) ([]byte, error) {\n    // Read size prefix (4 bytes, big-endian)\n    var sizeBuf [SizeFieldLen]byte\n    if _, err := io.ReadFull(r, sizeBuf[:]); err != nil {\n        return nil, err\n    }\n    \n    size := int32(binary.BigEndian.Uint32(sizeBuf[:]))\n    if size < 0 || size > MaxMessageSize {\n        return nil, ErrInvalidSize\n    }\n    \n    // Allocate buffer and read message body\n    buf := make([]byte, size)\n    if _, err := io.ReadFull(r, buf); err != nil {\n        return nil, err\n    }\n    \n    return buf, nil\n}\n\n// WriteMessage writes a length-prefixed message to the writer\nfunc WriteMessage(w io.Writer, data []byte) error {\n    size := int32(len(data))\n    if size > MaxMessageSize {\n        return ErrMessageTooLarge\n    }\n    \n    // Write size prefix\n    var sizeBuf [SizeFieldLen]byte\n    binary.BigEndian.PutUint32(sizeBuf[:], uint32(size))\n    if _, err := w.Write(sizeBuf[:]); err != nil {\n        return err\n    }\n    \n    // Write message body\n    _, err := w.Write(data)\n    return err\n}\n\n// MessageHeader represents the common header for all requests/responses\ntype MessageHeader struct {\n    Size          int32  // Already read by framing layer\n    APIKey        int16\n    APIVersion    int16\n    CorrelationID int32\n    ClientID      string // Nullable string\n}\n\n// DecodeHeader parses the common header from a message buffer\nfunc DecodeHeader(buf []byte) (*MessageHeader, error) {\n    if len(buf) < 10 { // Minimum header size (without ClientID)\n        return nil, errors.New(\"buffer too small for header\")\n    }\n    \n    h := &MessageHeader{}\n    pos := 0\n    \n    // APIKey (int16)\n    h.APIKey = int16(binary.BigEndian.Uint16(buf[pos:]))\n    pos += 2\n    \n    // APIVersion (int16)\n    h.APIVersion = int16(binary.BigEndian.Uint16(buf[pos:]))\n    pos += 2\n    \n    // CorrelationID (int32)\n    h.CorrelationID = int32(binary.BigEndian.Uint32(buf[pos:]))\n    pos += 4\n    \n    // ClientID (nullable string: int16 length + bytes)\n    clientIDLen := int16(binary.BigEndian.Uint16(buf[pos:]))\n    pos += 2\n    \n    if clientIDLen == -1 {\n        h.ClientID = \"\"\n    } else if clientIDLen >= 0 {\n        if pos+int(clientIDLen) > len(buf) {\n            return nil, errors.New(\"buffer underflow for ClientID\")\n        }\n        h.ClientID = string(buf[pos : pos+int(clientIDLen)])\n        pos += int(clientIDLen)\n    } else {\n        return nil, errors.New(\"invalid ClientID length\")\n    }\n    \n    return h, nil\n}\n\n// EncodeHeader serializes the header to bytes\nfunc EncodeHeader(h *MessageHeader) ([]byte, error) {\n    // Calculate total size (APIKey + APIVersion + CorrelationID + ClientID length + ClientID)\n    size := 2 + 2 + 4 + 2 + len(h.ClientID)\n    buf := make([]byte, size)\n    pos := 0\n    \n    // APIKey\n    binary.BigEndian.PutUint16(buf[pos:], uint16(h.APIKey))\n    pos += 2\n    \n    // APIVersion\n    binary.BigEndian.PutUint16(buf[pos:], uint16(h.APIVersion))\n    pos += 2\n    \n    // CorrelationID\n    binary.BigEndian.PutUint32(buf[pos:], uint32(h.CorrelationID))\n    pos += 4\n    \n    // ClientID (nullable string)\n    if h.ClientID == \"\" {\n        binary.BigEndian.PutUint16(buf[pos:], uint16(-1))\n        pos += 2\n    } else {\n        binary.BigEndian.PutUint16(buf[pos:], uint16(len(h.ClientID)))\n        pos += 2\n        copy(buf[pos:], h.ClientID)\n        pos += len(h.ClientID)\n    }\n    \n    return buf, nil\n}\n```\n\n#### Core Protocol Serialization Skeleton\n\n```go\n// internal/protocol/requests.go\npackage protocol\n\nimport (\n    \"encoding/binary\"\n    \"errors\"\n    \"fmt\"\n)\n\n// ProduceRequest represents a produce API request\ntype ProduceRequest struct {\n    TransactionalID string\n    Acks           int16\n    TimeoutMs      int32\n    TopicData      []TopicProduceData\n}\n\n// TopicProduceData represents topic-level produce data\ntype TopicProduceData struct {\n    Topic      string\n    Partitions []PartitionProduceData\n}\n\n// PartitionProduceData represents partition-level produce data\ntype PartitionProduceData struct {\n    Partition  int32\n    RecordSet  []byte // Serialized RecordBatch\n}\n\n// DecodeProduceRequest parses a ProduceRequest from bytes\nfunc DecodeProduceRequest(apiVersion int16, buf []byte) (*ProduceRequest, error) {\n    req := &ProduceRequest{}\n    pos := 0\n    \n    // TODO 1: For apiVersion >= 3, read TransactionalID (nullable string)\n    // Hint: Read int16 length, if -1 set to \"\", else read that many bytes\n    \n    // TODO 2: Read Acks (int16)\n    \n    // TODO 3: Read TimeoutMs (int32)\n    \n    // TODO 4: Read array size (int32) for TopicData\n    // For each topic:\n    //   a) Read topic name (nullable string)\n    //   b) Read array size (int32) for Partitions\n    //   c) For each partition:\n    //        i) Read Partition (int32)\n    //        ii) Read RecordSet size (int32) and bytes\n    //        iii) Append to Partitions slice\n    \n    // TODO 5: Validate buffer boundaries (pos should equal len(buf))\n    \n    return req, nil\n}\n\n// EncodeProduceRequest serializes a ProduceRequest to bytes\nfunc EncodeProduceRequest(req *ProduceRequest, apiVersion int16) ([]byte, error) {\n    // TODO 1: Calculate total buffer size needed\n    // Hint: Iterate through all nested structures counting bytes\n    \n    // TODO 2: Allocate buffer with correct size\n    \n    // TODO 3: Write TransactionalID if apiVersion >= 3\n    // Hint: Nullable string: length (-1 for null) then bytes\n    \n    // TODO 4: Write Acks (int16, big-endian)\n    \n    // TODO 5: Write TimeoutMs (int32)\n    \n    // TODO 6: Write TopicData array size (int32)\n    // For each topic:\n    //   a) Write topic name (nullable string)\n    //   b) Write Partitions array size (int32)\n    //   c) For each partition:\n    //        i) Write Partition (int32)\n    //        ii) Write RecordSet length (int32) followed by bytes\n    \n    // TODO 7: Return buffer\n    \n    return nil, nil\n}\n\n// HandleProduceRequest processes a produce request on the broker\nfunc HandleProduceRequest(req *ProduceRequest, broker *Server) (*ProduceResponse, error) {\n    resp := &ProduceResponse{\n        Responses: make([]TopicProduceResponse, 0, len(req.TopicData)),\n    }\n    \n    for _, topicData := range req.TopicData {\n        topicResp := TopicProduceResponse{\n            Topic:             topicData.Topic,\n            PartitionResponses: make([]PartitionProduceResponse, 0, len(topicData.Partitions)),\n        }\n        \n        for _, partitionData := range topicData.Partitions {\n            partitionResp := PartitionProduceResponse{\n                Partition: partitionData.Partition,\n            }\n            \n            // TODO 1: Validate topic exists and partition is valid\n            \n            // TODO 2: Check if this broker is leader for the partition\n            // If not, set ErrorCode = ErrNotLeaderForPartition\n            \n            // TODO 3: Validate RecordSet CRC and decompress if needed\n            \n            // TODO 4: Append records to partition log\n            // baseOffset, err := broker.logManager.Append(...)\n            \n            // TODO 5: Based on Acks, wait for replication\n            // - AcksNone: immediate success\n            // - AcksLeader: wait for local fsync\n            // - AcksAll: wait for all ISR acknowledgment\n            \n            // TODO 6: Set response fields: BaseOffset, LogAppendTime, LogStartOffset\n            \n            topicResp.PartitionResponses = append(topicResp.PartitionResponses, partitionResp)\n        }\n        \n        resp.Responses = append(resp.Responses, topicResp)\n    }\n    \n    return resp, nil\n}\n```\n\n#### Coordination Service Skeleton\n\n```go\n// internal/coordination/interface.go\npackage coordination\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// CoordinationService defines the minimal interface for cluster coordination\ntype CoordinationService interface {\n    // Broker management\n    RegisterBroker(ctx context.Context, broker *Broker) error\n    DeregisterBroker(ctx context.Context, brokerID int) error\n    GetBrokers(ctx context.Context) ([]*Broker, error)\n    \n    // Topic management\n    CreateTopic(ctx context.Context, topic *TopicMetadata) error\n    DeleteTopic(ctx context.Context, topicName string) error\n    GetTopicMetadata(ctx context.Context, topicName string) (*TopicMetadata, error)\n    ListTopics(ctx context.Context) ([]string, error)\n    \n    // Partition leadership\n    UpdatePartitionLeadership(ctx context.Context, partition *PartitionMetadata) error\n    ElectPartitionLeader(ctx context.Context, topic string, partition int, isr []int) (int, error)\n    GetPartitionMetadata(ctx context.Context, topic string, partition int) (*PartitionMetadata, error)\n    \n    // Watch/notification system\n    WatchBrokers(ctx context.Context, callback func([]*Broker)) (func(), error)\n    WatchTopic(ctx context.Context, topicName string, callback func(*TopicMetadata)) (func(), error)\n    \n    // Close releases resources\n    Close() error\n}\n\n// In-memory implementation skeleton\n// internal/coordination/memory/store.go\npackage memory\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/yourproject/internal/coordination\"\n    \"github.com/yourproject/internal/types\"\n)\n\ntype MemoryCoordinator struct {\n    mu sync.RWMutex\n    \n    brokers map[int]*types.Broker\n    topics  map[string]*types.TopicMetadata\n    \n    // Watch subscriptions\n    brokerWatchers []func([]*types.Broker)\n    topicWatchers  map[string][]func(*types.TopicMetadata)\n    \n    closeChan chan struct{}\n}\n\nfunc NewMemoryCoordinator() *MemoryCoordinator {\n    return &MemoryCoordinator{\n        brokers:       make(map[int]*types.Broker),\n        topics:        make(map[string]*types.TopicMetadata),\n        topicWatchers: make(map[string][]func(*types.TopicMetadata)),\n        closeChan:     make(chan struct{}),\n    }\n}\n\nfunc (c *MemoryCoordinator) RegisterBroker(ctx context.Context, broker *types.Broker) error {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    // TODO 1: If broker.ID is 0, assign next available ID\n    \n    // TODO 2: Store broker in map\n    \n    // TODO 3: Notify all broker watchers\n    // Hint: Call each callback in a goroutine to avoid blocking\n    \n    return nil\n}\n\nfunc (c *MemoryCoordinator) WatchBrokers(ctx context.Context, callback func([]*types.Broker)) (func(), error) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    // TODO 1: Add callback to brokerWatchers slice\n    \n    // TODO 2: Return cleanup function that removes the callback\n    \n    // TODO 3: Immediately call callback with current broker list\n    \n    return func() {}, nil\n}\n\nfunc (c *MemoryCoordinator) ElectPartitionLeader(ctx context.Context, topic string, partition int, isr []int) (int, error) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    // TODO 1: Look up current partition metadata\n    \n    // TODO 2: Validate ISR is not empty\n    \n    // TODO 3: Select leader using deterministic algorithm:\n    //   - Sort ISR replicas by broker ID\n    //   - Pick the replica that was most recently NOT leader (if any)\n    //   - Otherwise, pick the first replica in sorted list\n    \n    // TODO 4: Update partition metadata with new leader and generation\n    \n    // TODO 5: Notify topic watchers of the change\n    \n    return 0, nil\n}\n```\n\n#### Language-Specific Hints for Go\n\n1. **Binary Serialization**: Use `encoding/binary.BigEndian.PutUint32()` and `binary.BigEndian.Uint32()` for protocol serialization. Create helper functions for nullable strings (int16 length prefix with -1 for null).\n\n2. **Connection Management**: Wrap `net.Conn` with read/write deadlines using `SetReadDeadline()` and `SetWriteDeadline()`. Implement connection pooling with `sync.Pool` for reuse.\n\n3. **Watch Pattern**: Implement watch callbacks using channels or function slices. Use `context.Context` for cancellation. Always call callbacks in goroutines to avoid blocking the coordinator.\n\n4. **Memory Optimization**: For protocol buffers, use `bytes.Buffer` with pre-allocated capacity to reduce allocations. Consider object pooling for frequently allocated structs.\n\n5. **Error Propagation**: Define custom error types for protocol errors with error codes. Use `errors.Is()` and `errors.As()` for error handling in client retry logic.\n\n#### Milestone Checkpoint: Protocol Implementation\n\nAfter implementing the wire protocol, test with this verification:\n\n```bash\n# Start a broker\ngo run cmd/broker/main.go --port 9092\n\n# In another terminal, test produce protocol\necho '{\"key\": \"test\", \"value\": \"hello\"}' | \\\ngo run cmd/producer/main.go --topic test --brokers localhost:9092\n\n# Expected output:\n# Message sent to partition 0 at offset 0\n\n# Test consume protocol\ngo run cmd/consumer/main.go --topic test --group test-group --brokers localhost:9092\n\n# Expected output (after producing):\n# Received message at offset 0: key=\"test\", value=\"hello\"\n\n# Verify with network capture\nsudo tcpdump -i lo -A port 9092 | grep -A5 -B5 \"test\"\n# Should show binary data (not human-readable)\n```\n\n**Debugging Tips:**\n- **Symptom**: \"Connection reset by peer\" or unexpected EOF\n  - **Cause**: Incorrect message framing (size field wrong)\n  - **Diagnosis**: Log raw bytes sent/received with hex dump\n  - **Fix**: Verify `ReadMessage()`/`WriteMessage()` handle size prefix correctly\n\n- **Symptom**: \"Invalid API key\" error\n  - **Cause**: Wrong byte order or buffer offset\n  - **Diagnosis**: Print decoded header fields vs. expected\n  - **Fix**: Ensure big-endian encoding throughout\n\n- **Symptom**: Consumer doesn't receive messages after produce\n  - **Cause**: Fetch offset not advancing or incorrect high watermark\n  - **Diagnosis**: Check FetchResponse contains correct HighWatermark\n  - **Fix**: Ensure broker updates high watermark after replication\n\n\n## 10. Error Handling and Edge Cases\n\n> **Milestone(s):** 1, 2, 3, 4 (critical for all operational aspects)\n\nDistributed systems operate in a hostile environment where components fail independently and unpredictably. A message queue's reliability hinges not on preventing failures (impossible) but on gracefully recovering from them. This section catalogs the inevitable failure modes and edge cases your system will encounter, providing a comprehensive playbook for designing robust recovery mechanisms. Think of this as the system's immune system: it must detect pathogens (failures), contain damage (prevent cascading failures), and initiate healing (recovery) while maintaining core functions.\n\n### 10.1 Failure Mode Categories\n\nFailures in a distributed message queue can be categorized by their origin, duration, and observability. Each category presents distinct challenges for detection and recovery.\n\n#### 10.1.1 Broker Crashes (Process Termination)\nBroker crashes occur when a broker process terminates abruptly due to fatal errors, `SIGKILL`, out-of-memory conditions, or administrative shutdown. The broker disappears from the network entirely, leaving its hosted partitions leaderless if they were leading replicas.\n\n**Mental Model: The Vanishing Bank Teller**\nImagine a bank with multiple tellers (brokers) serving customers (clients). One teller suddenly collapses mid-transaction. Customers waiting at that teller's line are stuck, but the bank manager (controller/coordinator) notices the absence and redirects customers to other tellers who have duplicate ledger copies (replicas).\n\n| Failure Characteristic | Description | Detection Mechanism |\n|------------------------|-------------|---------------------|\n| **Sudden Termination** | Process exits without cleanup (OOM, segmentation fault) | Missing heartbeats in coordination service, TCP connection failures |\n| **Graceful Shutdown**  | Controlled stop with cleanup attempts | Explicit deregistration request before exit |\n| **Partial Functionality** | Process runs but critical subsystems fail (log full, disk error) | Health checks, failed produce/fetch requests, metrics monitoring |\n\n#### 10.1.2 Network Partitions (Split-Brain Scenarios)\nNetwork partitions occur when subsets of brokers can communicate within their group but cannot reach brokers in other groups. This creates \"split-brain\" scenarios where each partition believes the others have failed, potentially leading to multiple leaders for the same partition.\n\n**Mental Model: The Islanded Archipelago**\nPicture an archipelago (broker cluster) where a storm severs communication links between islands. Islands can't coordinate, so each might independently declare itself the new capital (leader) for shared territories (partitions), creating conflicting governance.\n\n| Partition Type | Typical Cause | Duration | Impact Severity |\n|----------------|---------------|----------|-----------------|\n| **Client-Broker** | Client network issues, firewall rules | Seconds to minutes | Temporary unavailability for specific clients |\n| **Broker-Broker** | Switch failures, misconfigured routes | Variable | Replication stalls, ISR shrinkage, potential data inconsistency |\n| **Controller Isolation** | Controller broker loses connection to majority | Until partition heals | No leadership elections, metadata stagnation |\n\n#### 10.1.3 Disk Failures (Storage Corruption)\nDisk failures range from complete device failure to silent corruption where reads return incorrect data. The log's durability guarantees depend entirely on stable storage, making disk failures particularly insidious.\n\n**Mental Model: The Fading Ledger**\nEnvision a ledger (log) whose ink gradually fades (bit rot) or pages tear (sector errors). Transactions recorded yesterday become unreadable today, threatening the entire financial record.\n\n| Failure Mode | Manifestation | Detection Challenge |\n|--------------|---------------|---------------------|\n| **Full Disk** | No space for new writes | `ENOSPC` errors on append, monitoring alerts |\n| **Corrupted Files** | CRC mismatches, invalid offsets | Read validation failures, checksum verification |\n| **Slow I/O** | Latency spikes beyond tolerance | Timeout monitoring, percentile latency tracking |\n| **Controller Failure** | RAID/HBA issues affecting all disks | Health checks, SMART monitoring |\n\n#### 10.1.4 Client Timeouts and Transient Failures\nClients may experience temporary connectivity issues, processing delays, or resource exhaustion. While not broker failures, these affect system semantics (message delivery guarantees, offset commits).\n\n**Mental Model: The Distracted Customer**\nImagine a customer (consumer) who steps away from the counter (network timeout) while their transaction is being processed. The teller (broker) must decide whether to hold the transaction open or serve the next customer.\n\n| Client Issue | Typical Cause | System Impact |\n|--------------|---------------|---------------|\n| **Network Timeout** | Packet loss, congestion, DNS issues | In-flight requests fail, retries triggered |\n| **Processing Stall** | GC pause, deadlock, resource exhaustion | Heartbeat failures, session timeout |\n| **Clock Skew** | NTP misconfiguration, virtualization artifacts | Incorrect timestamps, session expiration confusion |\n\n### 10.2 Recovery Strategies\n\nEach failure category demands specific recovery mechanisms. The system's resilience emerges from the careful orchestration of these strategies across components.\n\n#### 10.2.1 Broker Crash Recovery\nWhen a broker crashes, the system must restore availability without compromising consistency for partitions where the crashed broker was leader.\n\n> **Key Insight:** Recovery from broker crashes follows a two-phase approach: first **detect and isolate** the failed broker, then **reassign and restore** its responsibilities to healthy replicas.\n\n**Step-by-Step Recovery Algorithm:**\n\n1. **Failure Detection:**\n   - Coordination service (`MemoryCoordinator`) monitors broker heartbeats\n   - Missing heartbeats for `session.timeout.ms` triggers failure suspicion\n   - Controller confirms via TCP connection attempts to all broker ports\n   - After confirmation delay (configurable), broker marked as `DEAD`\n\n2. **Leadership Reassignment:**\n   - Controller (`LeaderElector`) enumerates all partitions where failed broker was leader\n   - For each partition:\n     - Remove failed broker from ISR list in `PartitionMetadata`\n     - Select new leader from remaining ISR members using deterministic algorithm (e.g., oldest replica in ISR)\n     - If ISR is empty, trigger **unclean leader election** decision (see ADR below)\n   - Update `PartitionMetadata` across cluster via coordination service\n   - Notify all brokers of leadership changes via metadata propagation\n\n3. **Replica Rebuilding (Post-Recovery):**\n   - When failed broker restarts, it rejoins cluster as follower for all its assigned partitions\n   - For each partition:\n     - New leader truncates follower's log to **high watermark** to ensure consistency\n     - Follower fetches from leader starting at high watermark + 1\n     - Once caught up within `replica.lag.time.max.ms`, rejoins ISR\n\n**ADR: Unclean Leader Election Policy**\n\n> **Decision: Disallow Unclean Leader Election by Default**\n> - **Context:** When all ISR replicas for a partition are unavailable (e.g., multiple broker failures), the system faces a choice: wait for an ISR member to recover (availability impact) or elect a non-ISR follower as leader (consistency risk).\n> - **Options Considered:**\n>   1. **Always wait for ISR:** Never elect non-ISR replicas, guaranteeing no data loss but risking extended unavailability.\n>   2. **Allow unclean election:** Elect any available replica when ISR is empty, maximizing availability but risking data loss from un-replicated messages.\n>   3. **Configurable policy:** Let operators choose based on topic importance.\n> - **Decision:** Implement option 1 (wait for ISR) as default for this educational project, with clear logging when availability is impacted.\n> - **Rationale:** Simplicity and safety for learners. Understanding data loss implications requires sophisticated monitoring; better to err on the side of preserving data. Real systems (like Kafka) offer configurable `unclean.leader.election.enable`.\n> - **Consequences:** Topics may become unavailable during multi-broker failures, but learners won't silently lose data. Forces consideration of replication factor and broker distribution.\n\n| Recovery Strategy | Trigger Condition | Action | Consistency Guarantee |\n|-------------------|-------------------|--------|-----------------------|\n| **Clean Failover** | Leader fails, ISR non-empty | Elect new leader from ISR | No data loss, monotonic offsets |\n| **ISR Shrinkage** | Follower lags beyond threshold | Remove from ISR, continue with reduced replication | Potentially reduced durability |\n| **Unavailable Partition** | All ISR members fail | Wait for recovery, reject produce/fetch requests | Strong consistency, availability loss |\n\n#### 10.2.2 Network Partition Mitigation\nNetwork partitions create the most complex failure scenarios due to conflicting views of cluster state.\n\n**Detection and Containment Strategy:**\n\n1. **Dual Monitoring:**\n   - **Heartbeat-based:** Coordination service tracks broker liveness via periodic heartbeats\n   - **Request-based:** Brokers track peer liveness via replication fetch requests and metadata exchanges\n   - Discrepancies between these signals suggest network issues rather than true failures\n\n2. **Fencing via Epochs:**\n   - Each partition leader has a monotonically increasing `leader_epoch` stored in `Partition`\n   - All client requests include the last known leader epoch\n   - Requests with stale epoch are rejected with `ErrNotLeaderForPartition`\n   - This prevents clients from accidentally writing to old leaders during partitions\n\n3. **Controller Resilience:**\n   - Controller itself can be partitioned from the majority\n   - Use **controller epoch** in `ClusterMetadata` to prevent multiple active controllers\n   - Brokers reject metadata updates from controllers with stale epoch\n\n**Recovery Sequence During Partition Healing:**\n\n```\nWhen network partition heals:\n1. Previously isolated brokers re-establish TCP connections\n2. They discover higher controller/leader epochs from the majority side\n3. Isolated leaders step down upon receiving higher epoch notifications\n4. Their logs are inspected for divergent writes:\n   - Compare high watermarks between old and new leaders\n   - Any messages beyond common high watermark are truncated (lost)\n5. Brokers rejoin as followers and resynchronize from current leaders\n```\n\n#### 10.2.3 Disk Failure Handling\nDisk failures threaten the fundamental durability guarantees of the message log.\n\n**Tiered Response Strategy:**\n\n| Failure Severity | Detection | Immediate Response | Long-term Remediation |\n|------------------|-----------|-------------------|------------------------|\n| **Full Disk** | `ENOSPC` on append | Reject produce requests for affected partitions, continue serving fetches | Add disk space, delete old segments, or migrate partitions |\n| **Corrupted Segment** | CRC mismatch on read | Mark segment as corrupted, skip to next segment for reads; writes continue to new segment | Attempt repair from replicas, or accept data loss for that segment |\n| **Slow I/O** | Write latency > `log.flush.timeout.ms` | Log warnings, metrics alerts; continue with degraded performance | Investigate disk health, redistribute load, upgrade hardware |\n| **Complete Disk Loss** | All I/O operations fail | Mark broker as unhealthy in coordination service, trigger partition reassignment | Replace hardware, restore from replicas, recommission broker |\n\n**Segmented Isolation Approach:**\nThe `Log` design with multiple `LogSegment` files provides natural isolation boundaries. A corrupted segment affects only messages within its offset range, allowing the rest of the partition to remain operational. The system should:\n1. Detect corruption via CRC checks in `Log.Read()`\n2. Move corrupted segment to quarantine directory\n3. Update `Log.Segments` list to skip corrupted segment\n4. Log detailed error with segment metadata for admin intervention\n5. For replicated partitions, attempt to fetch missing range from other replicas\n\n#### 10.2.4 Client Failure Recovery\nClient failures primarily affect consumer progress tracking and producer message delivery semantics.\n\n**Consumer Session Recovery:**\n```\nWhen consumer fails (misses heartbeats):\n1. Group coordinator (`ConsumerGroup.State`) marks consumer as dead after `session.timeout.ms`\n2. Coordinator triggers rebalance by moving group to `PreparingRebalance` state\n3. Remaining consumers rejoin via `JoinGroup`/`SyncGroup`\n4. Partitions assigned to failed consumer are redistributed\n5. New consumers start fetching from last committed offsets\n```\n\n**Producer Retry with Idempotence:**\nFor `Producer` with `AcksAll`, failed requests require careful retry logic:\n\n```go\n// Pseudo-algorithm (not code block - prose description)\n1. On send failure (timeout, network error, NotLeader error):\n2. Refresh metadata cache to discover new partition leader\n3. Reassign batch to new leader if partition leadership changed\n4. Apply exponential backoff: wait = baseBackoff * 2^attempt\n5. Retry up to `Retries` configuration\n6. If idempotent producer enabled (optional), include sequence numbers\n   to allow broker to deduplicate retried batches\n```\n\n### 10.3 Edge Cases and Race Conditions\n\nEdge cases represent improbable but possible scenarios arising from specific timing of events. Race conditions occur when multiple processes access shared data concurrently, and the outcome depends on the sequence of operations.\n\n#### 10.3.1 Leader Change During Produce Request\n\n**Scenario:** A producer sends a `ProduceRequest` to partition leader. Mid-processing, leadership changes (old leader crashes or steps down). The request might be partially processed, duplicated, or lost.\n\n**Mental Model: The Relaying Runner**\nImagine a relay race where the baton (message) is passed between runners (brokers). If the current runner (leader) stumbles while holding the baton, and a new runner takes over, what happens to the baton? It might be dropped (lost), held by both (duplicated), or successfully passed (handled correctly).\n\n| Timing | Producer Action | Old Leader State | New Leader State | Potential Outcome |\n|--------|----------------|------------------|------------------|-------------------|\n| **Before send** | Request in flight | Healthy | Not yet elected | Connection refused, producer retries |\n| **During append** | Request being written | Crashes mid-write | Elected | Partial write: data may be corrupted or lost |\n| **After append, before ack** | Waiting for response | Crashes after write | Elected | Duplicate possible if producer retries |\n| **After ack, before receipt** | Response in flight | Crashes after sending ack | Elected | Producer sees success, consumer may not see data until recovery |\n\n**Mitigation Strategy:**\n1. **Leader Epoch in Requests:** Include `leader_epoch` in `ProduceRequest`; new leader rejects requests with stale epoch\n2. **Sequence Numbers:** Idempotent producers include `producer_id`, `epoch`, and `sequence_number` for deduplication\n3. **High Watermark Validation:** New leader compares its log with old leader's before accepting writes to detect gaps\n4. **Client Metadata Refresh:** Producer refreshes metadata on `ErrNotLeaderForPartition` and retries\n\n#### 10.3.2 Duplicate Consumer ID During Rebalance\n\n**Scenario:** A consumer crashes and restarts quickly, rejoining the group with the same `member_id` before the coordinator has marked it dead. Two instances with the same identity exist temporarily.\n\n**Race Condition Timeline:**\n```\nT0: Consumer C1 (member_id=\"A\") sends heartbeat\nT1: C1 crashes\nT2: C1 restarts, reuses member_id=\"A\", sends JoinGroup\nT3: Coordinator still has C1 as active (heartbeat within session timeout)\nT4: Coordinator now has two \"logical\" consumers with same ID\n```\n\n**Consequences:**\n- Coordinator might assign partitions to both instances\n- Both instances might commit offsets, causing overwrites\n- Group state becomes inconsistent\n\n**Solution: Generation ID Fencing:**\n- Each rebalance increments `GenerationID` in `ConsumerGroup`\n- Coordinator includes `generation_id` in `JoinGroupResponse`\n- Members include `generation_id` in all subsequent requests\n- Coordinator rejects requests with stale `generation_id`\n- Restarted consumer gets new `member_id` if rejoining after generation change\n\n#### 10.3.3 Partial Write Before Crash (Torn Writes)\n\n**Scenario:** Broker crashes while appending a `RecordBatch` to disk, writing only part of the batch. On restart, the log contains corrupted data.\n\n**Detailed Failure Modes:**\n\n| Corruption Type | Cause | Detection |\n|-----------------|-------|-----------|\n| **Incomplete batch** | Crash during `WAL.Append()` | Batch length prefix doesn't match actual bytes |\n| **Misaligned offset** | Crash between writing record and updating index | Index points to invalid file position |\n| **CRC mismatch** | Crash during write leaves partial data | CRC validation fails on read |\n\n**Recovery via Write-Ahead Log with Batch Atomicity:**\n1. **Write Strategy:** Entire `RecordBatch` written as single `WAL.Append()` operation\n2. **CRC Protection:** Compute CRC after serialization, include in header\n3. **Fsync Control:** `sync` parameter controls durability trade-off\n4. **Restart Recovery:** `Log` scans segments on startup:\n   - Read forward until incomplete batch detected (length mismatch)\n   - Truncate file to last valid batch boundary\n   - Rebuild index from remaining valid data\n   - Log truncation offset for admin review\n\n#### 10.3.4 Zombie Consumers in Rebalance\n\n**Scenario:** A consumer is considered dead by coordinator (missed heartbeats) but is actually alive and still fetching messages. It becomes a \"zombie\" – processing messages without the coordinator's knowledge.\n\n**Root Causes:**\n1. **Network Partition:** Consumer isolated from coordinator but connected to partition leaders\n2. **Heartbeat Thread Stall:** Consumer process alive but heartbeat goroutine blocked\n3. **Clock Skew:** Consumer's clock ahead, causing early heartbeat timeout calculation\n\n**Dangers:**\n- Zombie continues committing offsets, interfering with new consumer's progress\n- Messages processed twice (by zombie and replacement consumer)\n- Offset commits race condition\n\n**Fencing via Generation ID:**\n```\nSolution implemented in HandleFetch:\n1. Each FetchRequest includes (group_id, member_id, generation_id)\n2. Broker validates against local cache of valid consumers\n3. If generation_id stale or member not in current generation:\n   - Return error to zombie\n   - Zombie must rejoin group to get current generation\n4. Partition leaders get valid member list from coordinator periodically\n```\n\n#### 10.3.5 ISR Shrinkage to Empty Set\n\n**Scenario:** All followers for a partition fall behind the leader due to network issues, broker failures, or sustained high load. The ISR shrinks to just the leader, then the leader fails, leaving no in-sync replicas.\n\n**Progression:**\n```\nInitial: ISR = [Leader L, Followers F1, F2]\nStep 1: F1 network partition → removed from ISR after replica.lag.time.max.ms\nStep 2: F2 disk slowdown → falls behind → removed from ISR\nStep 3: ISR = [L] (single replica)\nStep 4: L crashes → No in-sync replicas remain\n```\n\n**Recovery Options:**\n1. **Wait for Recovery:** Don't elect new leader until at least one replica recovers and catches up\n2. **Best-Effort Election:** Elect the replica with most recent data (highest LEO)\n3. **Admin Intervention:** Manual override via tooling\n\n**Implementation Guidance:**\n- Track `lastCaughtUpTime` for each follower in `ISRManager`\n- Configurable `unclean.leader.election.enable` per topic\n- Log warnings when ISR size drops below `min.insync.replicas`\n- Expose metrics for admin monitoring\n\n#### 10.3.6 Offset Commit Race During Rebalance\n\n**Scenario:** Consumer commits offsets while rebalance is in progress. The commit might apply to old partition assignment, causing offsets to be associated with wrong partitions after reassignment.\n\n**Timeline:**\n```\nT0: Consumer C1 has partitions [P1, P2]\nT1: C1 commits offset for P2\nT2: Rebalance starts (new consumer joins)\nT3: Coordinator reassigns P2 to C2\nT4: C1's offset commit arrives (delayed network)\nT5: Offset for P2 stored under C1's member_id, but C2 now owns P2\n```\n\n**Solution: Generation-aware Offset Commits:**\n- `CommitOffset` request includes `generation_id`\n- Coordinator rejects commits with stale `generation_id`\n- Consumers should commit offsets **before** rejoining group during rebalance\n- Alternatively, commit offsets **after** receiving new assignment (Kafka's approach)\n\n#### 10.3.7 Producer Retry Causing Duplicate Sequence\n\n**Scenario:** Idempotent producer sends batch with sequence number N, gets timeout, retries with same sequence number N, but first batch actually succeeded and was durably stored.\n\n**Without Proper Deduplication:**\n```\nP1: Send batch (producer_id=100, epoch=1, seq=5) → Timeout\nP2: Retry same batch (pid=100, epoch=1, seq=5) → Success\nP3: First batch eventually succeeds (network delay) → Duplicate in log\n```\n\n**Broker-side Deduplication:**\n- Maintain `lastSequence` per `(producer_id, producer_epoch, partition)`\n- Reject batches with sequence ≤ `lastSequence`\n- Accept batches with sequence = `lastSequence + 1`\n- Gap in sequence numbers (sequence > `lastSequence + 1`) indicates lost messages → return error\n\n#### 10.3.8 Log Truncation During Follower Sync\n\n**Scenario:** Leader truncates its log (e.g., after unclean leader election or admin operation) while follower is fetching. Follower may have already replicated data that no longer exists on leader.\n\n**Detection and Recovery:**\n1. Leader includes `log_start_offset` in `FetchResponse`\n2. Follower compares its fetch offset with leader's `log_start_offset`\n3. If `fetch_offset < log_start_offset`, follower must truncate its log\n4. Follower rewinds to `log_start_offset` and re-fetches from there\n5. Any local messages beyond truncation point are discarded\n\n**Implementation in `FollowerSyncer.AppendToLocalLog()`:**\n- Validate `leader_hw` and `log_start_offset` against local log\n- If divergence detected, truncate local log to common point\n- Log warning with offset details for admin review\n\n### 10.4 Failure Recovery Summary Table\n\n| Failure Mode | Detection Method | Recovery Action | Consistency Impact | Availability Impact |\n|--------------|------------------|-----------------|-------------------|---------------------|\n| **Broker crash (leader)** | Missed heartbeats, TCP failure | Elect new leader from ISR | None if ISR > 1 | Brief unavailability during election |\n| **Broker crash (follower)** | Missed heartbeats | Remove from ISR, replicate to other followers | Reduced durability margin | None |\n| **Network partition** | Divergent metadata views, heartbeat failures | Fencing via epochs, wait for healing | Potential divergent writes if unclean election allowed | Partial availability loss |\n| **Disk full** | `ENOSPC` on write | Reject writes, alert admin | Potential data loss if buffers overflow | Write unavailability for affected partitions |\n| **Consumer timeout** | Missed heartbeats | Rebalance, redistribute partitions | At-least-once reprocessing possible | Brief consumption pause during rebalance |\n| **Producer retry storm** | High error rate, timeout spikes | Exponential backoff, circuit breaker | Potential duplicates without idempotence | Increased latency, reduced throughput |\n| **Controller failure** | Missed controller heartbeats | Elect new controller from brokers | Metadata update stall | No new leader elections until new controller |\n| **Zombie consumer** | Generation ID mismatch in fetch | Reject fetches, force rejoin | Prevents duplicate processing | Brief disruption for zombie consumer |\n\n### Implementation Guidance\n\n> **Technology Note:** While production systems use sophisticated monitoring (Prometheus, health checks), our educational implementation can rely on simpler timeouts and periodic checks. The key is implementing the recovery logic correctly, not building enterprise monitoring.\n\n#### A. Recommended Failure Detection Infrastructure\n\n**Simple Health Check Endpoint:**\nAdd to your `Server` a basic HTTP health endpoint (separate port) that checks:\n- Disk space above threshold\n- WAL write/read test\n- Coordination service connectivity\n- Memory usage\n\n**Timeout Configuration Table:**\n| Configuration | Default Value | Purpose | Override Guidance |\n|---------------|---------------|---------|-------------------|\n| `session.timeout.ms` | 10000 (10s) | Consumer heartbeat timeout | Increase for GC-heavy environments |\n| `replica.lag.time.max.ms` | 30000 (30s) | Max time follower can lag before ISR removal | Adjust based on network reliability |\n| `request.timeout.ms` | 30000 (30s) | Client request timeout | Match expected network latency |\n| `log.flush.timeout.ms` | 1000 (1s) | Max time for log fsync | Tune based on disk performance |\n\n#### B. Core Recovery Logic Skeletons\n\n**Leader Election on Broker Failure:**\n\n```go\n// In internal/coordinator/leader_elector.go\n\n// OnBrokerFailure triggers leader election for all partitions where the failed\n// broker was the leader. Called by coordination service when broker marked dead.\nfunc (e *LeaderElector) OnBrokerFailure(failedBrokerID int32) error {\n    // TODO 1: Query coordination service for all partitions where\n    //         failedBrokerID == PartitionMetadata.LeaderID\n    \n    // TODO 2: For each affected partition:\n    //   a. Get current ISR from PartitionMetadata\n    //   b. Remove failedBrokerID from ISR slice\n    //   c. If ISR is empty:\n    //        - Log critical warning \"No in-sync replicas for partition\"\n    //        - Skip election (partition unavailable) OR\n    //        - If unclean election enabled, use all replicas as candidates\n    \n    // TODO 3: Select new leader using deterministic algorithm:\n    //   - Prefer replicas in original ISR order (oldest first)\n    //   - Ensure new leader is alive (check broker heartbeats)\n    \n    // TODO 4: Update PartitionMetadata with new LeaderID and ISR\n    \n    // TODO 5: Notify all brokers of metadata change via coordination service\n    \n    // TODO 6: Return any errors encountered during the process\n    return nil\n}\n\n// selectNewLeader chooses a replica from ISR to become the new leader.\n// Returns -1 if no suitable leader found (ISR empty).\nfunc (e *LeaderElector) selectNewLeader(isr []int32, partitionInfo PartitionMetadata) int32 {\n    // TODO 1: Filter ISR to only alive brokers (check broker heartbeats)\n    \n    // TODO 2: If filtered ISR empty, return -1 (partition unavailable)\n    \n    // TODO 3: Apply deterministic selection:\n    //   - Sort alive ISR members by broker ID\n    //   - Choose first in sorted list (simplest strategy)\n    //   - Alternative: choose replica with highest LEO (requires extra metadata)\n    \n    // TODO 4: Return selected broker ID\n    return -1\n}\n```\n\n**Consumer Zombie Detection in Fetch Handler:**\n\n```go\n// In internal/broker/fetch_handler.go\n\n// validateConsumerGeneration checks if consumer is part of current generation.\n// Returns ErrUnknownMember if consumer is zombie (stale generation).\nfunc (h *FetchHandler) validateConsumerGeneration(\n    groupID string,\n    memberID string,\n    generationID int32,\n) error {\n    // TODO 1: Get group from coordinator using groupID\n    \n    // TODO 2: If group not found, return ErrUnknownMemberId\n    \n    // TODO 3: Check if memberID exists in group.Members map\n    \n    // TODO 4: Compare generationID with group.GenerationID\n    \n    // TODO 5: If generationID < group.GenerationID, consumer is zombie\n    //         Return ErrUnknownMemberId to force rejoin\n    \n    // TODO 6: If member not in current members list, return ErrUnknownMemberId\n    \n    return nil\n}\n\n// HandleFetch now includes validation:\nfunc (h *FetchHandler) HandleFetch(req *FetchRequest) (*FetchResponse, error) {\n    // TODO: For consumer fetch requests (ReplicaID = -1):\n    //   Call validateConsumerGeneration for each unique (group, member)\n    //   Reject entire request if any consumer is zombie\n    \n    // ... rest of fetch logic\n}\n```\n\n**Log Recovery on Broker Startup:**\n\n```go\n// In internal/log/log_manager.go\n\n// RecoverLogs scans all log directories and recovers from partial writes.\n// Called during broker startup before accepting client requests.\nfunc (lm *LogManager) RecoverLogs() error {\n    // TODO 1: Walk data directory to find all topic-partition directories\n    \n    // TODO 2: For each partition directory:\n    //   a. List segment files sorted by base offset\n    //   b. For each segment file:\n    //        i. Open data file and index file\n    //        ii. Read from start, validating each RecordBatch CRC\n    //        iii. When invalid batch found (CRC mismatch or length wrong):\n    //              - Truncate file at last valid batch boundary\n    //              - Rebuild index from remaining valid data\n    //              - Log warning with truncation offset\n    //        iv. Update Log.Segments list and Log.CurrentOffset\n    \n    // TODO 3: For replicated partitions, compare with leader's high watermark\n    //         Truncate beyond high watermark if follower is ahead\n    \n    // TODO 4: Return any unrecoverable errors (e.g., disk errors)\n    return nil\n}\n```\n\n**Network Partition Detection via Dual Heartbeats:**\n\n```go\n// In internal/coordinator/memory_coordinator.go\n\n// checkBrokerHealth performs dual health checking to detect network partitions.\nfunc (mc *MemoryCoordinator) checkBrokerHealth() {\n    // TODO 1: For each registered broker:\n    //   a. Check last heartbeat time (coordination plane)\n    //   b. Check last metadata request time (data plane proxy)\n    \n    // TODO 2: If heartbeat stale but metadata recent:\n    //   - Log warning \"Possible network partition for broker X\"\n    //   - Don't mark as dead immediately\n    //   - Increase suspicion level\n    \n    // TODO 3: If both stale beyond thresholds:\n    //   - Mark broker as dead\n    //   - Trigger leader election for affected partitions\n    \n    // TODO 4: If heartbeat recent but metadata stale:\n    //   - Broker may be overloaded or have disk issues\n    //   - Consider removing from ISR for hosted partitions\n}\n```\n\n#### C. Debugging Tips for Common Failure Scenarios\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| **Messages not consumed after consumer restart** | Offset commit failed during shutdown | 1. Check offset store for group<br>2. Verify last commit timestamp<br>3. Check consumer logs for commit errors | Ensure `CommitSync()` called before `Close()`, increase `request.timeout.ms` |\n| **Producer hangs indefinitely** | All replicas down for target partition | 1. Check partition ISR size<br>2. Verify broker health<br>3. Check producer logs for `ErrNotLeaderForPartition` | Configure `retries` and `retry.backoff.ms`, monitor broker health |\n| **Consumer group stuck in rebalance** | One consumer not responding to JoinGroup | 1. Check coordinator logs for timeout members<br>2. Verify all consumers can reach coordinator<br>3. Check for clock skew between machines | Increase `session.timeout.ms`, synchronize NTP, ensure network connectivity |\n| **High watermark not advancing** | Followers not catching up to leader | 1. Check follower fetch logs<br>2. Verify network between brokers<br>3. Check disk I/O on followers | Increase `replica.fetch.wait.ms`, check network links, monitor disk performance |\n| **Duplicate messages consumed** | Consumer processed messages, crashed before commit | 1. Check offset lag before/after crash<br>2. Verify commit frequency settings<br>3. Check for zombie consumers | Decrease `auto.commit.interval.ms`, implement synchronous commits, add generation fencing |\n| **Log segment corruption** | Disk error or improper shutdown | 1. Check CRC validation logs<br>2. Verify disk SMART status<br>3. Check for OOM kill during write | Implement segment CRC validation, add disk monitoring, ensure proper shutdown hooks |\n\n#### D. Testing Failure Recovery\n\n**Integration Test Skeleton for Broker Failure:**\n\n```go\n// In tests/integration/broker_failure_test.go\nfunc TestLeaderElectionOnBrokerFailure(t *testing.T) {\n    // TODO 1: Start 3 brokers in test cluster\n    \n    // TODO 2: Create topic with replication factor 3, partitions 1\n    \n    // TODO 3: Produce messages to establish leadership\n    \n    // TODO 4: Kill leader broker (simulate crash)\n    \n    // TODO 5: Wait for election timeout\n    \n    // TODO 6: Verify new leader elected from ISR\n    \n    // TODO 7: Produce more messages to new leader\n    \n    // TODO 8: Verify all messages preserved (no data loss)\n    \n    // TODO 9: Restart dead broker\n    \n    // TODO 10: Verify it rejoins as follower and catches up\n}\n```\n\n**Property-based Test for Idempotent Producer:**\n\n```go\n// Use gopter for property testing\nfunc TestProducerIdempotence(t *testing.T) {\n    parameters := gopter.DefaultTestParameters()\n    properties := gopter.NewProperties(parameters)\n    \n    properties.Property(\"retried batches produce no duplicates\", prop.ForAll(\n        func(initialSeq int32) bool {\n            // TODO: Implement test that:\n            // 1. Sends batch with sequence N\n            // 2. Simulates network timeout\n            // 3. Retries same batch\n            // 4. Verifies only one copy in log\n            return true\n        },\n        gen.Int32Range(1, 1000),\n    ))\n    \n    properties.TestingRun(t)\n}\n```\n\n> **Key Learning Point:** The most robust systems aren't those that never fail, but those that fail in predictable, recoverable ways. Your implementation should prioritize clear failure modes and recovery paths over trying to prevent all failures.\n\n\n## 11. Testing Strategy\n> **Milestone(s):** 1, 2, 3, 4 (essential for validating all system behaviors)\n\nBuilding a distributed message queue is an exercise in managing complexity. The system's correctness depends on subtle interactions between concurrent components, network communication, and persistent storage. A comprehensive testing strategy is therefore not merely a validation step—it is a fundamental design tool that helps you reason about the system's behavior, catch bugs early, and build confidence in the implementation. This section provides a structured approach to testing, from unit tests of individual algorithms to integration tests that simulate real-world cluster operations.\n\n### 11.1 Testing Approach\n\nTesting a distributed system is akin to stress-testing a suspension bridge. You must first validate each cable and beam in isolation (unit tests), then assemble them into spans and verify their connections (integration tests), and finally subject the entire structure to extreme loads and unexpected forces (property-based and fault injection tests) to ensure it won't collapse under real-world conditions.\n\nOur testing strategy employs three complementary layers, each targeting different levels of abstraction and failure modes:\n\n**1. Unit Tests: The Foundation of Logic Verification**\nUnit tests focus on isolated components—individual functions, methods, and data structures—in a controlled environment. Their primary goal is to validate algorithmic correctness and internal state transitions without the complexity of network or disk I/O. For example, testing that the `HashPartitioner.Partition` function consistently routes the same key to the same partition, or that the `Index.FindEntry` binary search returns the correct file position. These tests should be fast, deterministic, and run without external dependencies.\n\n> **Key Insight:** Mock all I/O and external dependencies in unit tests. Use dependency injection to replace network clients, file systems, and random number generators with deterministic test doubles. This ensures tests remain reliable and fast, even as the system grows.\n\n**2. Integration Tests: Validating Component Interactions**\nIntegration tests assemble multiple components and verify they work together correctly. This layer is crucial for our system because the core value emerges from interactions—between producers and brokers, consumers and coordinators, and leaders and followers. We recommend two types of integration tests:\n- **In-process cluster tests:** Launch multiple `Server` instances within the same process, connected via loopback network interfaces. This allows testing full cluster behaviors (rebalancing, leader election) without the overhead of process spawning.\n- **Component integration tests:** Test specific interfaces between components, such as a `Producer` sending to a real `Server` over TCP, or a `ConsumerGroup` interacting with a `Coordinator`.\n\nThese tests should use real network sockets and file I/O, but run within a controlled test environment (e.g., temporary directories, deterministic port assignment).\n\n**3. Property-Based Tests: Exploring Edge Cases Systematically**\nProperty-based testing (PBT) is a powerful technique for uncovering edge cases that example-based tests might miss. Instead of writing specific test cases (e.g., \"send 3 messages\"), you define properties that should *always* hold true for any valid input, and a test framework generates thousands of random inputs to verify them. For our system, important properties include:\n- **Idempotence:** Sending the same batch of records twice (with the same producer ID and sequence) should result in exactly one set of records in the log.\n- **Ordering within partitions:** For any sequence of messages sent to the same partition, the offsets returned must be strictly increasing and contiguous when no failures occur.\n- **No data loss:** The concatenation of all messages read by a consumer (after accounting for commits) must equal the concatenation of all messages sent by producers.\n\nPBT frameworks like [gopter](https://github.com/leanovate/gopter) for Go can generate complex scenarios: random network partitions, broker crashes, and concurrent client operations.\n\n**ADR: Testing Strategy Layering**\n\n> **Decision: Three-Layer Testing Pyramid with Heavy Integration Focus**\n> - **Context:** Our educational system must validate both algorithmic correctness (unit tests) and complex distributed behaviors (integration). Learners need immediate feedback on logic errors, but also confidence that components work together.\n> - **Options Considered:**\n>     1. **Unit-heavy pyramid:** Traditional approach with 70% unit, 20% integration, 10% end-to-end tests. Fast but misses interaction bugs.\n>     2. **Integration-heavy \"hourglass\":** Equal emphasis on unit and end-to-end, with integration as the thick middle layer. Slower but catches more distributed bugs.\n>     3. **Property-based emphasis:** Rely primarily on generated tests to explore state space. Powerful but requires significant upfront investment and expertise.\n> - **Decision:** Adopt an integration-heavy hourglass model, supplemented by property-based tests for core invariants.\n> - **Rationale:** The greatest learning value and most subtle bugs in distributed systems arise from component interactions. Integration tests provide concrete, observable behaviors that learners can debug. Property-based tests complement by systematically exploring edge cases that manual test design might miss.\n> - **Consequences:** Test suite will run slower than unit-only approaches, requiring careful use of test parallelism and short timeouts. However, it will provide higher confidence in system correctness and better prepare learners for real-world debugging.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Unit-heavy pyramid | Fast execution, clear isolation, easy to debug | Misses interaction bugs, gives false confidence | No |\n| Integration-heavy hourglass | Catches distributed bugs, validates real workflows | Slower, more complex test setup | **Yes** |\n| Property-based emphasis | Explores vast state space, finds obscure edge cases | Steep learning curve, harder to debug failures | Supplementary |\n\n**Common Pitfalls in Testing Distributed Systems**\n\n⚠️ **Pitfall: Non-deterministic tests due to timing and concurrency**\nTests that rely on real-time delays (`time.Sleep`) or uncoordinated goroutines will fail intermittently, eroding trust in the test suite.\n- **Why it's wrong:** Flaky tests mask real bugs and waste debugging time. They often pass in development but fail in CI.\n- **How to fix:** Use synchronized channels or condition variables to signal when events occur. For time-based operations, inject a mock clock that can be advanced manually in tests.\n\n⚠️ **Pitfall: Not cleaning up resources between tests**\nLeaving open files, network sockets, or goroutines from one test can affect subsequent tests.\n- **Why it's wrong:** Tests become order-dependent and may fail when run in isolation vs. as a suite.\n- **How to fix:** Use `t.Cleanup()` or `defer` to close resources. For goroutines, pass a `context.Context` that gets cancelled when the test ends.\n\n⚠️ **Pitfall: Over-mocking eliminates integration value**\nMocking every dependency, including the local log storage or in-memory maps, reduces integration tests to unit tests of the mock setup.\n- **Why it's wrong:** You're not testing the actual interaction between components, just your assumptions about how they *should* interact.\n- **How to fix:** Use real implementations for internal components (like the `WAL` or `Log`), but mock only external boundaries (network between separate processes, disk I/O if too slow).\n\n### 11.2 Milestone Verification Checkpoints\n\nEach milestone in the project builds upon the previous one, creating a cumulative verification challenge. The following checkpoints provide concrete, executable scenarios you can use to validate that your implementation meets the core requirements. These are not exhaustive tests, but they represent critical paths that, if working correctly, indicate fundamental correctness.\n\n**Checkpoint Structure:**\nFor each milestone, we define:\n1. **Test Scenario:** A narrative description of the operation to test.\n2. **Preconditions:** State of the system before the test.\n3. **Steps:** Concrete actions to perform (manually or via test code).\n4. **Expected Output:** What should happen after each step.\n5. **Verification Method:** How to observe and confirm the expected behavior.\n\n**Milestone 1: Topic and Partitions**\n\n| Aspect | Details |\n|--------|---------|\n| **Test Scenario** | Create a topic with multiple partitions, produce messages with and without keys, and verify consistent partitioning and offset ordering. |\n| **Preconditions** | Single broker running with clean data directory. |\n| **Steps** | 1. Create topic \"orders\" with 3 partitions.<br>2. Produce 5 messages: two with key=\"user1\", two with key=\"user2\", one with null key.<br>3. Fetch metadata to see partition assignments.<br>4. Read messages from each partition individually. |\n| **Expected Output** | - Topic creation succeeds.<br>- Messages with same key land in same partition.<br>- Offsets within each partition are sequential (0,1,...).<br>- Null-key message distributes via round-robin.<br>- `LogEndOffset` for each partition matches message count. |\n| **Verification Method** | Inspect broker logs or use a debug API to dump partition contents. Programmatically verify offsets and key-to-partition mapping. |\n\n**Milestone 2: Producer**\n\n| Aspect | Details |\n|--------|---------|\n| **Test Scenario** | Producer with `acks=all` sends batched messages, survives a leader failover, and retries without duplicates (idempotence optional). |\n| **Preconditions** | 3-node cluster with topic \"logs\" (replication factor 3). Producer configured with batch size=2, linger=100ms. |\n| **Steps** | 1. Send 10 messages asynchronously with callbacks.<br>2. Kill the partition leader while messages are in-flight.<br>3. Wait for new leader election.<br>4. Call `Flush()` and verify all callbacks fired with success. |\n| **Expected Output** | - All 10 messages are eventually acknowledged.<br>- No duplicate messages appear in the log (check offsets).<br>- Producer logs show retry attempts after leader failure.<br>- High watermark advances only after all ISRs acknowledge. |\n| **Verification Method** | Compare sent message count with total records in log segments. Check producer callback invocation count. |\n\n**Milestone 3: Consumer Groups**\n\n| Aspect | Details |\n|--------|---------|\n| **Test Scenario** | Two consumers join a group, receive balanced partition assignments, commit offsets, and trigger rebalance when one leaves. |\n| **Preconditions** | Topic \"events\" with 4 partitions, containing 100 pre-existing messages. |\n| **Steps** | 1. Start Consumer A in group \"grp1\", subscribe to \"events\".<br>2. Start Consumer B in same group, subscribe to \"events\".<br>3. Let each consumer poll 20 messages and commit offsets.<br>4. Stop Consumer B gracefully.<br>5. Observe Consumer A's partition reassignment. |\n| **Expected Output** | - Initial assignment: each consumer gets 2 partitions (balanced).<br>- Each consumer reads only from its assigned partitions.<br>- Offset commits are persisted and survive consumer restart.<br>- After B leaves, A gets all 4 partitions within session timeout.<br>- No messages are processed twice (thanks to committed offsets). |\n| **Verification Method** | Check coordinator's assignment state. Verify committed offsets in `__consumer_offsets` internal topic. Count total unique messages processed. |\n\n**Milestone 4: Replication**\n\n| Aspect | Details |\n|--------|---------|\n| **Test Scenario** | Leader-follower replication maintains consistency during network partition and recovers via ISR management. |\n| **Preconditions** | 3-node cluster, topic \"audit\" with RF=3, all replicas in sync. |\n| **Steps** | 1. Produce 50 messages with `acks=all`. Verify all replicas have same LEO.<br>2. Isolate follower 2 from leader (simulate network partition).<br>3. Produce 20 more messages.<br>4. Wait for replica.lag.time.max.ms to expire.<br>5. Check ISR size (should shrink to 2).<br>6. Heal network partition, verify follower catches up and rejoins ISR. |\n| **Expected Output** | - First 50 messages replicated to all followers.<br>- ISR shrinks after follower 2 lags beyond threshold.<br>- Messages 51-70 are only replicated to remaining ISR members.<br>- After healing, follower 2 fetches missing messages and rejoins ISR.<br>- High watermark never exceeds LEO of slowest ISR member. |\n| **Verification Method** | Query each replica's log end offset. Monitor ISR changes via metadata. Verify consumer reads only up to high watermark. |\n\n> **Design Insight:** These checkpoints intentionally stress the \"happy path\" *and* failure scenarios. A system that works only when nothing goes wrong is useless in production. Each milestone's checkpoint includes at least one injected failure (leader kill, consumer stop, network partition) to validate recovery logic.\n\n### 11.3 Tools and Libraries\n\nThe Go ecosystem provides excellent testing libraries that align with our layered approach. The following recommendations balance simplicity for learners with power for complex scenarios.\n\n**Core Testing Framework: `testing` + `testify`**\nGo's built-in `testing` package is sufficient for most needs, but `testify` adds valuable assertions and mocking capabilities.\n\n| Library | Purpose | Key Features | Example Use |\n|---------|---------|--------------|-------------|\n| `testing` (standard) | Basic test framework | `t.Run()` for subtests, `t.Helper()`, `t.Cleanup()` | All unit and integration tests |\n| `testify/assert` | Readable assertions | `assert.Equal()`, `assert.NoError()`, helpful failure messages | Replacing `if err != nil { t.Fatal() }` patterns |\n| `testify/require` | Assertions that stop test | `require.True()` stops test immediately on failure | Precondition validation in tests |\n| `testify/suite` | Test suite organization | `SetupSuite()`, `TearDownTest()` methods | Structuring integration test lifecycle |\n| `testify/mock` | Mock object generation | Generate mocks for interfaces, verify calls | Mocking `ConnectionPool` in producer tests |\n\n**Mocking and Dependency Injection**\nFor unit testing components in isolation, generate mocks for their dependencies. The `testify/mock` package works well, but consider interface design that facilitates testing:\n\n```go\n// Example: Interface for network connection to allow mocking\ntype Connection interface {\n    Send(request []byte) (response []byte, err error)\n    Close() error\n}\n\n// Real implementation\ntype TCPConnection struct { /* ... */ }\n\n// Mock implementation\ntype MockConnection struct {\n    mock.Mock\n}\nfunc (m *MockConnection) Send(request []byte) ([]byte, error) {\n    args := m.Called(request)\n    return args.Get(0).([]byte), args.Error(1)\n}\n```\n\n**Property-Based Testing: `gopter`**\n`gopter` (GO Property Tester) generates random inputs and shrinks failing cases to minimal examples.\n\n| Feature | Benefit for Our System |\n|---------|------------------------|\n| Arbitrary value generation | Create random `Record` objects with varied keys, values, headers |\n| Stateful command testing | Model producer/consumer interactions as state transitions |\n| Shrinking | Reduce failing 1000-message test to 3-message minimal case |\n\n**Concurrency and Race Detection**\nGo's race detector (`go test -race`) is essential for catching data races in concurrent code. Additionally:\n\n| Pattern | Implementation | Purpose |\n|---------|----------------|---------|\n| Synchronized test verification | Use channels to collect results from goroutines | Verify concurrent producer sends |\n| Context cancellation | `context.WithTimeout` in tests | Prevent hanging tests on deadlock |\n| WaitGroups | `sync.WaitGroup` to await goroutine completion | Ensure all test goroutines finish |\n\n**Network Testing Utilities**\nFor integration tests that require real network communication:\n\n| Utility | Purpose | Example |\n|---------|---------|---------|\n| `net.Listen(\"tcp\", \"localhost:0\")` | Get free port for test servers | Start broker on random port |\n| `httptest.Server` (if using HTTP) | In-memory HTTP server for client tests | Not used in our binary protocol |\n| `io.Pipe()` | Connect reader/writer without sockets | Test protocol encoding/decoding |\n\n**Temporary Resources Management**\nAlways use temporary directories for test data to avoid collisions and ensure cleanup:\n\n```go\nfunc TestBroker_RecoversLogs(t *testing.T) {\n    tempDir := t.TempDir() // Automatically cleaned up after test\n    broker := NewBroker(tempDir)\n    // ... test logic\n}\n```\n\n**Integration Test Helper Pattern**\nCreate a test helper that spins up an in-process cluster:\n\n```go\ntype TestCluster struct {\n    Brokers []*Server\n    TempDirs []string\n}\n\nfunc NewTestCluster(t *testing.T, size int) *TestCluster {\n    // Initialize 'size' brokers with temporary directories\n    // Set up inter-broker communication via loopback\n    // Return cluster handle with cleanup via t.Cleanup()\n}\n```\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Test Framework | `testing` + `testify` | `ginkgo` + `gomega` (BDD style) |\n| Mocking | `testify/mock` (manual) | `gomock` (code generation) |\n| Property Testing | Manual random generation | `gopter` with custom generators |\n| Concurrency Testing | `go test -race`, channels | `stress` test with goroutine profiling |\n| Network Testing | Loopback TCP with real sockets | `netem` for network fault injection |\n| Temporary Storage | `t.TempDir()` | RAM disk for faster I/O |\n\n**B. Recommended File/Module Structure for Tests**\n\n```\nproject-root/\n  internal/\n    broker/\n      broker.go\n      broker_test.go           # Unit tests for broker logic\n    log/\n      log.go\n      log_test.go              # Unit tests for log operations\n    protocol/\n      encode_decode.go\n      encode_decode_test.go    # Property tests for wire format\n    coordinator/\n      coordinator.go\n      coordinator_integration_test.go  # Integration with real network\n  test/\n    integration/\n      cluster_test.go          # Multi-broker integration tests\n      producer_consumer_test.go # End-to-end workflows\n    property/\n      ordering_property_test.go # PBT for ordering guarantees\n    helpers/\n      test_cluster.go          # TestCluster helper\n      test_client.go           # Test producer/consumer clients\n  cmd/\n    server/\n      main.go\n    server_integration_test.go # Integration test for main binary\n```\n\n**C. Infrastructure Starter Code: Test Helper Utilities**\n\n```go\n// test/helpers/test_cluster.go\npackage helpers\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net\"\n    \"path/filepath\"\n    \"sync\"\n    \"testing\"\n    \"time\"\n\n    \"github.com/stretchr/testify/require\"\n)\n\n// TestCluster manages an in-process broker cluster for integration tests.\ntype TestCluster struct {\n    t       *testing.T\n    size    int\n    brokers []*Server\n    tempDirs []string\n    mu      sync.RWMutex\n}\n\n// NewTestCluster creates a cluster of 'size' brokers.\nfunc NewTestCluster(t *testing.T, size int) *TestCluster {\n    require.Greater(t, size, 0, \"cluster size must be positive\")\n    \n    tc := &TestCluster{\n        t:      t,\n        size:   size,\n        brokers: make([]*Server, size),\n        tempDirs: make([]string, size),\n    }\n    \n    // Create coordination service (in-memory for tests)\n    coord := NewMemoryCoordinator()\n    \n    // Initialize each broker\n    for i := 0; i < size; i++ {\n        tempDir := t.TempDir()\n        tc.tempDirs[i] = tempDir\n        \n        // Find free port\n        listener, err := net.Listen(\"tcp\", \"localhost:0\")\n        require.NoError(t, err)\n        addr := listener.Addr().(*net.TCPAddr)\n        listener.Close()\n        \n        config := Config{\n            Host:   \"localhost\",\n            Port:   addr.Port,\n            DataDir: tempDir,\n            BrokerID: int32(i + 1),\n        }\n        \n        broker, err := NewServer(config, coord)\n        require.NoError(t, err)\n        \n        tc.brokers[i] = broker\n        \n        // Start broker in background\n        go func(b *Server) {\n            if err := b.Start(); err != nil {\n                t.Logf(\"broker %d stopped: %v\", i, err)\n            }\n        }(broker)\n        \n        // Register broker with coordination service\n        err = coord.RegisterBroker(context.Background(), &Broker{\n            ID:   int32(i + 1),\n            Host: \"localhost\",\n            Port: int32(addr.Port),\n        })\n        require.NoError(t, err)\n    }\n    \n    // Wait for brokers to be ready\n    tc.waitForBrokersReady()\n    \n    // Cleanup on test completion\n    t.Cleanup(func() {\n        tc.Shutdown()\n    })\n    \n    return tc\n}\n\n// waitForBrokersReady polls each broker until it responds to metadata requests.\nfunc (tc *TestCluster) waitForBrokersReady() {\n    deadline := time.Now().Add(10 * time.Second)\n    for i, broker := range tc.brokers {\n        for time.Now().Before(deadline) {\n            // Try to connect to broker's admin port (simplified)\n            conn, err := net.DialTimeout(\"tcp\", \n                fmt.Sprintf(\"%s:%d\", broker.config.Host, broker.config.Port), \n                100*time.Millisecond)\n            if err == nil {\n                conn.Close()\n                break\n            }\n            time.Sleep(100 * time.Millisecond)\n        }\n        require.True(tc.t, time.Now().Before(deadline), \n            \"broker %d failed to start within timeout\", i)\n    }\n}\n\n// Shutdown gracefully stops all brokers.\nfunc (tc *TestCluster) Shutdown() {\n    tc.mu.Lock()\n    defer tc.mu.Unlock()\n    \n    for _, broker := range tc.brokers {\n        if broker != nil {\n            broker.Stop() // Assumes Stop() method exists\n        }\n    }\n}\n\n// GetBroker returns the broker at index i (0-based).\nfunc (tc *TestCluster) GetBroker(i int) *Server {\n    tc.mu.RLock()\n    defer tc.mu.RUnlock()\n    require.Less(tc.t, i, tc.size, \"broker index out of range\")\n    return tc.brokers[i]\n}\n\n// CreateTopic creates a topic on the cluster with default settings.\nfunc (tc *TestCluster) CreateTopic(topic string, partitions, replicationFactor int) error {\n    // Implementation creates topic metadata via coordinator\n    // TODO: Implement using coordinator API\n    return nil\n}\n```\n\n**D. Core Logic Skeleton Code: Property-Based Test Example**\n\n```go\n// test/property/ordering_property_test.go\npackage property\n\nimport (\n    \"testing\"\n    \n    \"github.com/leanovate/gopter\"\n    \"github.com/leanovate/gopter/gen\"\n    \"github.com/leanovate/gopter/prop\"\n)\n\n// TestPartitionOrderingProperty verifies that offsets within a partition\n// are always strictly increasing and contiguous when no failures occur.\nfunc TestPartitionOrderingProperty(t *testing.T) {\n    parameters := gopter.DefaultTestParameters()\n    parameters.MinSuccessfulTests = 1000\n    parameters.MaxSize = 100 // Max messages per test run\n    \n    properties := gopter.NewProperties(parameters)\n    \n    // Generator for a slice of records with random keys/values\n    recordsGen := gen.SliceOf(genRecord())\n    \n    properties.Property(\"offsets are strictly increasing within partition\", \n        prop.ForAll(func(records []*Record) bool {\n            // TODO 1: Create a fresh in-memory Log for testing\n            // TODO 2: Append all records using Log.Append\n            // TODO 3: Read back records from offset 0\n            // TODO 4: Verify each record's offset increases by exactly 1\n            // TODO 5: Verify the sequence of records matches input sequence\n            return true // placeholder\n        }, recordsGen))\n    \n    properties.TestingRun(t)\n}\n\n// genRecord generates a random Record for property testing.\nfunc genRecord() gopter.Gen {\n    return gen.Struct(reflect.TypeOf(&Record{}), map[string]gopter.Gen{\n        \"Key\":   gen.SliceOf(gen.UInt8()).SuchThat(func(v interface{}) bool { return len(v.([]byte)) <= 1024 }),\n        \"Value\": gen.SliceOf(gen.UInt8()).SuchThat(func(v interface{}) bool { return len(v.([]byte)) <= 1024 }),\n        \"Headers\": gen.SliceOf(genHeader()),\n    })\n}\n\n// genHeader generates a random Header.\nfunc genHeader() gopter.Gen {\n    return gen.Struct(reflect.TypeOf(Header{}), map[string]gopter.Gen{\n        \"Key\":   gen.AlphaString(),\n        \"Value\": gen.SliceOf(gen.UInt8()),\n    })\n}\n```\n\n**E. Language-Specific Hints for Go Testing**\n\n1. **Table-Driven Tests:** Use this pattern extensively for unit tests with multiple input cases:\n   ```go\n   func TestHashPartitioner(t *testing.T) {\n       tests := []struct{\n           name string\n           key []byte\n           numPartitions int32\n           want int32\n       }{\n           {\"nil key\", nil, 5, 0},\n           {\"key 'abc'\", []byte(\"abc\"), 3, 1},\n       }\n       for _, tt := range tests {\n           t.Run(tt.name, func(t *testing.T) {\n               p := &HashPartitioner{}\n               got, err := p.Partition(\"test\", tt.key, tt.numPartitions)\n               require.NoError(t, err)\n               require.Equal(t, tt.want, got)\n           })\n       }\n   }\n   ```\n\n2. **Test Parallelization:** Use `t.Parallel()` in independent tests to speed up execution, but avoid it for tests that share resources (like a global test cluster).\n\n3. **Golden Files:** For protocol serialization tests, consider using golden files to compare expected binary output:\n   ```go\n   golden := filepath.Join(\"testdata\", \"produce_request_v2.golden\")\n   if *update {\n       os.WriteFile(golden, encoded, 0644)\n   }\n   expected, _ := os.ReadFile(golden)\n   require.Equal(t, expected, encoded)\n   ```\n\n**F. Milestone Checkpoint Verification**\n\nAfter implementing each milestone, run the corresponding verification test:\n\n```bash\n# Milestone 1: Topic and Partition basics\ngo test ./internal/broker -run TestBroker_CreateTopicAndProduce -v\n\n# Expected output:\n# === RUN   TestBroker_CreateTopicAndProduce\n# --- PASS: TestBroker_CreateTopicAndProduce (0.2s)\n# PASS\n\n# Milestone 2: Producer with batching and retries  \ngo test ./internal/producer -run TestProducer_SendWithLeaderFailover -v\n\n# Milestone 3: Consumer group rebalancing\ngo test ./internal/coordinator -run TestConsumerGroup_Rebalance -v\n\n# Milestone 4: Replication and ISR management\ngo test ./internal/replication -run TestReplication_ISRShrinkAndRecover -v\n```\n\n**G. Debugging Tips for Test Failures**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Test hangs indefinitely | Deadlock in goroutines, missing channel receive | Use `pprof` goroutine dump, add test timeouts with `context.WithTimeout` | Ensure all goroutines have cancellation paths, use `select` with `ctx.Done()` |\n| Intermittent test failures | Race condition, timing dependency | Run with `go test -race`, add `t.Log` statements to trace execution order | Add proper synchronization (mutexes, channels), make tests deterministic |\n| \"Address already in use\" | Previous test didn't clean up sockets | Check for proper `t.Cleanup()` or `defer server.Stop()` | Ensure all servers are stopped, use random ports with `:0` |\n| Disk space errors in CI | Temporary files not cleaned up | Check `t.TempDir()` usage, look for leftover files | Always use `t.TempDir()`, not manual `/tmp` creation |\n| Consumer group rebalance storms | Heartbeat intervals too short, session timeout too low | Log rebalance triggers, increase test timeouts | Adjust `session.timeout.ms` and `heartbeat.interval.ms` in test config |\n\n> **Final Testing Principle:** Write tests that fail in the way you expect. If a bug is fixed, add a test that would have caught it. Tests are not just verification—they are executable documentation of your system's intended behavior.\n\n\n## 12. Debugging Guide\n\n> **Milestone(s):** 1, 2, 3, 4 (Debugging is relevant to all milestones)\n\nBuilding a distributed message queue is a complex endeavor with many moving parts. Even with careful design and implementation, bugs are inevitable. This debugging guide provides a systematic approach to diagnosing and fixing common issues that arise during development. Think of debugging a distributed system like being a **mechanical engineer troubleshooting a multi-engine aircraft**: you need to check each subsystem (engines, fuel lines, electrical) individually while also understanding how they interact, using both instrument readings (logs) and test procedures (diagnosis techniques) to isolate the root cause.\n\n### 12.1 Common Bug Symptoms\n\nDistributed system bugs manifest in observable ways that affect either correctness (wrong results) or performance (slow results). Learning to recognize these symptoms is the first step toward diagnosis.\n\n1. **Messages Lost (Missing Data)**\n   - *Description*: Messages sent by producers never appear when consumers fetch, or only some messages are delivered. This violates the at-least-once delivery guarantee and indicates data loss.\n   - *Example Scenario*: A producer sends 100 messages with `acks=all`, but a consumer reading from the beginning only receives 85 messages, with no apparent pattern to which ones are missing.\n\n2. **Duplicate Messages (Extra Data)**\n   - *Description*: The same message appears multiple times in consumer output, either immediately or after restart. This violates at-most-once semantics and can cause incorrect application behavior.\n   - *Example Scenario*: After a consumer crashes and restarts, it reprocesses messages it had already processed before the crash, even though it had committed offsets.\n\n3. **Consumers Stuck (No Progress)**\n   - *Description*: One or more consumers in a group stop fetching new messages despite producers continuing to send data. The consumer appears \"frozen\" at a particular offset.\n   - *Example Scenario*: In a three-consumer group, two consumers continue processing messages while the third remains stuck at offset 42, never advancing despite new messages being available in its assigned partitions.\n\n4. **High Latency (Slow Performance)**\n   - *Description*: Message delivery takes significantly longer than expected, with producers experiencing slow acknowledgment times or consumers experiencing long delays between message batches.\n   - *Example Scenario*: A producer configured with 10ms `linger.ms` actually waits 500ms before sending batches, or a consumer calling `Poll()` takes seconds to return even when messages are available.\n\n5. **Rebalance Storms (Frequent Churn)**\n   - *Description*: Consumer groups undergo constant rebalancing, with partitions being reassigned frequently even without consumers joining or leaving. This wastes resources and disrupts processing.\n   - *Example Scenario*: A stable consumer group suddenly starts rebalancing every 2-3 seconds, causing repeated pauses in message consumption without any apparent reason.\n\n6. **Inconsistent Ordering (Sequence Violation)**\n   - *Description*: Messages with the same key appear in different order when consumed, violating the per-key ordering guarantee within a partition.\n   - *Example Scenario*: A producer sends messages with keys A, B, A (in that order), but the consumer receives them as A, A, B, even though both A messages went to the same partition.\n\n7. **ISR Shrinkage to Empty (No Replicas Available)**\n   - *Description*: The In-Sync Replica set for a partition becomes empty, making the partition unavailable for writes or unsafe reads, often after a series of failures.\n   - *Example Scenario*: After a network partition isolates the leader, all followers are removed from ISR, and when the leader fails, no eligible replica exists to become the new leader.\n\n8. **Memory Leak (Growing Resource Usage)**\n   - *Description*: Broker or client memory usage grows continuously over time without stabilizing, eventually leading to out-of-memory crashes.\n   - *Example Scenario*: A broker's RSS memory increases by 10MB per hour even with constant message volume, eventually crashing after 24 hours.\n\n### 12.2 Diagnosis Techniques\n\nEffective debugging requires systematic observation and hypothesis testing. These techniques provide the \"instruments\" to understand what's happening inside your system.\n\n#### Structured Logging with Context\n\n> **Mental Model**: Think of structured logs as a **flight data recorder** (black box) for your distributed system. Each component continuously records its state, decisions, and interactions with timestamps, allowing you to reconstruct events leading to a failure.\n\n- **Implementation Approach**: Instead of `fmt.Printf`, use a structured logging library that supports:\n  - **Levels**: DEBUG (internal state), INFO (normal operations), WARN (potential issues), ERROR (failures)\n  - **Fields**: Key-value pairs attached to each log message (e.g., `partition=3`, `offset=142`, `consumer_id=\"c1\"`)\n  - **Correlation IDs**: Unique identifiers passed through request chains to trace messages across components\n\n- **Critical Log Points to Add**:\n  - *Message Flow*: Log when messages are appended to log (with offset), when batches are sent/received, when consumers fetch messages\n  - *State Changes*: Log partition leadership changes, ISR membership updates, consumer group state transitions\n  - *Timing*: Log request durations, batch accumulation times, network round-trip times\n  - *Errors*: Log all errors with full context (what operation failed, with what parameters, what error code)\n\n- **Example Log Configuration**:\n  ```go\n  // In initialization\n  log.SetLevel(log.DebugLevel)\n  log.SetFormatter(&log.JSONFormatter{})\n  \n  // In broker append logic\n  log.WithFields(log.Fields{\n      \"topic\": topic,\n      \"partition\": partitionID,\n      \"offset\": newOffset,\n      \"batch_size\": len(batch.Records),\n      \"leader_epoch\": leaderEpoch,\n  }).Info(\"Appended records to partition\")\n  ```\n\n#### Internal State Inspection via Admin APIs\n\n> **Mental Model**: Think of state inspection as a **submarine's control panel** with gauges for each subsystem. By exposing internal metrics and state through diagnostic endpoints, you can check \"vital signs\" without stopping the system.\n\n- **What to Expose**:\n  - *Broker Metrics*: Memory usage, open file descriptors, Goroutine count, request queue depths\n  - *Partition State*: Log end offset, high watermark, ISR members, leader epoch, follower lag\n  - *Consumer Group State*: Member list, assigned partitions, last heartbeat time, generation ID\n  - *Producer State*: Batch accumulator sizes, in-flight request counts, retry queue depth\n\n- **Implementation Approach**: Create a simple HTTP endpoint (`/debug/state`) that returns JSON representations of key data structures. Consider thread-safe snapshot approaches:\n\n  ```go\n  // Example: Broker state snapshot\n  type BrokerDebugState struct {\n      Topics     map[string]TopicDebugState `json:\"topics\"`\n      Goroutines int                        `json:\"goroutines\"`\n      MemoryMB   float64                    `json:\"memory_mb\"`\n      Uptime     string                     `json:\"uptime\"`\n  }\n  \n  // Called from debug endpoint handler\n  func (b *Broker) GetDebugState() BrokerDebugState {\n      b.mu.RLock()\n      defer b.mu.RUnlock()\n      \n      state := BrokerDebugState{\n          Topics: make(map[string]TopicDebugState),\n          Goroutines: runtime.NumGoroutine(),\n          MemoryMB: getMemoryUsageMB(),\n      }\n      \n      for name, topic := range b.topics {\n          state.Topics[name] = topic.GetDebugState()\n      }\n      return state\n  }\n  ```\n\n#### Timeout-Based Bottleneck Identification\n\n> **Mental Model**: Think of timeouts as **circuit breakers** in an electrical system. When a component takes too long to respond, the timeout \"trips\" and allows the system to continue with degraded functionality, while also signaling where bottlenecks exist.\n\n- **Strategic Timeout Placement**:\n  1. **Network Timeouts**: Set deadlines on all socket reads/writes (e.g., 30 seconds)\n  2. **Request Timeouts**: Time out entire RPC operations (produce, fetch, join group)\n  3. **Internal Operation Timeouts**: Time out log flushes, lock acquisitions, channel operations\n\n- **Diagnosis Technique**: When timeouts occur, examine:\n  - *Which operation* timed out (produce, fetch, heartbeat)\n  - *Which component* was involved (which broker, which partition)\n  - *What else* was happening at that time (other requests, garbage collection, disk I/O)\n  - *Correlation* with other symptoms (high latency, stuck consumers)\n\n- **Timeout Configuration Table**:\n  \n  | Timeout Type | Default Value | Purpose | What to Check When Hit |\n  |--------------|---------------|---------|------------------------|\n  | `socket.timeout.ms` | 30000 | Network read/write deadline | Network connectivity, broker load |\n  | `request.timeout.ms` | 30000 | Complete RPC operation | Broker processing time, lock contention |\n  | `rebalance.timeout.ms` | 60000 | Maximum rebalance duration | Consumer join/leave coordination |\n  | `fetch.timeout.ms` | 500 | Consumer poll wait | Message availability, consumer lag |\n  | `heartbeat.timeout.ms` | 10000 | Consumer liveness detection | Network partitions, consumer GC pauses |\n\n#### Controlled Experimentation (Scientific Method)\n\n> **Mental Model**: Think of debugging as conducting **laboratory experiments**. You formulate hypotheses about root causes, design tests to isolate variables, run experiments, and analyze results.\n\n- **Process**:\n  1. **Observe**: Document the symptom precisely (what, when, where, how often)\n  2. **Hypothesize**: Propose a potential root cause based on understanding of the system\n  3. **Experiment**: Design a minimal test to verify/reject the hypothesis\n  4. **Analyze**: Compare actual results with expected results\n  5. **Iterate**: Refine hypothesis based on results\n\n- **Example Experiment for Message Loss**:\n  - *Observation*: 15% of messages lost with `acks=1`\n  - *Hypothesis*: Leader is crashing after acknowledging but before replicating to disk\n  - *Experiment*: Add detailed logging at leader: log immediately before and after disk sync\n  - *Analysis*: Check if crash logs show unsynced writes; verify with `WAL.Append(..., sync=true)`\n\n#### Concurrency Race Detection\n\n> **Mental Model**: Think of race conditions as **traffic intersections without signals**. Multiple threads (cars) approach simultaneously, and depending on timing, may collide or proceed safely. Detection tools are like traffic cameras capturing these interactions.\n\n- **Go-Specific Tools**:\n  - `go test -race`: Run tests with race detector enabled (adds overhead but finds data races)\n  - `sync/atomic` operations: For simple counters where lock overhead is unacceptable\n  - `context.Context` for cancellation: Properly propagate cancellation through goroutines\n\n- **Common Race Patterns to Watch For**:\n  - *Read-Modify-Write*: Incrementing a counter without synchronization\n  - *Check-Then-Act*: Checking a condition then acting on it without holding lock\n  - *Publication*: Publishing a reference before initialization completes\n\n### 12.3 Symptom → Cause → Fix Table\n\nThis table maps observable symptoms to their most likely root causes, diagnosis steps, and specific fixes. Use it as a starting point for your debugging journey.\n\n| Symptom | Likely Cause | Diagnosis Steps | Fix |\n|---------|--------------|-----------------|-----|\n| **Messages Lost** | 1. **Producer using `acks=0` (fire-and-forget)** with network failures<br>2. **Leader crashes** after acknowledging but before persisting to disk (`acks=1`)<br>3. **ISR shrinks to empty**, then unclean leader election discards unreplicated data<br>4. **Consumer commits offset** before processing, then crashes | 1. Check producer configuration for `AcksNone`<br>2. Examine broker logs for crashes right after produce acknowledgments<br>3. Monitor ISR size metrics; check for follower lag exceeding threshold<br>4. Check consumer commit logic (offset committed before business logic) | 1. Use `AcksLeader` or `AcksAll` for durability<br>2. Ensure `WAL.Append` calls `sync=true` when `acks>=1`<br>3. Configure `unclean.leader.election.enable=false`<br>4. Commit offsets **after** successful message processing |\n| **Duplicate Messages** | 1. **Producer retries** without idempotency after transient errors<br>2. **Consumer rebalance** causes reprocessing of uncommitted offsets<br>3. **Consumer commits offset** after processing but crashes before commit persists<br>4. **Leader epoch mismatch** during leadership change causes old leader to accept writes | 1. Check producer logs for retry attempts with same batch<br>2. Examine consumer group generation changes; check offset commits during rebalance<br>3. Check disk sync on offset commit store (`OffsetStore.persistGroup`)<br>4. Check leader epoch validation in produce request handling | 1. Implement idempotent producer with sequence numbers<br>2. Implement cooperative rebalancing with offset commit before partition revocation<br>3. Use synchronous offset commits with disk sync<br>4. Validate leader epoch in all write paths; reject stale writes |\n| **Consumers Stuck** | 1. **Heartbeat failures** causing coordinator to mark consumer dead<br>2. **Fetch loop blocked** on slow I/O or deadlock<br>3. **No messages** at current offset (log compacted or truncated)<br>4. **Assignment mismatch** - consumer thinks it has partitions it doesn't | 1. Check coordinator logs for heartbeat timeouts<br>2. Add debug logs to fetch loop; check for lock acquisition<br>3. Compare consumer offset with log start offset<br>4. Verify assignment in `ConsumerMetadata` vs actual fetch requests | 1. Increase `session.timeout.ms` or fix long GC pauses<br>2. Add timeouts to all blocking operations; fix deadlocks<br>3. Implement auto-offset reset to `earliest` or `latest`<br>4. Validate assignments in `Poll()` before fetching |\n| **High Latency** | 1. **Small batch sizes** causing excessive network round trips<br>2. **Disk I/O contention** from multiple partitions on same disk<br>3. **Memory pressure** causing excessive garbage collection<br>4. **Network bottlenecks** between producers and broker cluster | 1. Measure batch sizes sent; check `BatchSize` and `LingerMs` settings<br>2. Monitor disk I/O wait times; check if partitions share data directory<br>3. Monitor Go GC pause times and heap size<br>4. Measure network latency between client and broker nodes | 1. Tune `batch.size` and `linger.ms` for throughput/latency tradeoff<br>2. Separate partition data to different physical disks<br>3. Optimize memory allocations; reuse buffers; set GOGC appropriately<br>4. Ensure brokers are geographically close to producers or use compression |\n| **Rebalance Storms** | 1. **Session timeout too short** for consumers with GC pauses<br>2. **Heartbeat thread blocked** by long-running operations<br>3. **Coordinator overload** causing delayed heartbeat processing<br>4. **Network flakiness** causing intermittent connectivity loss | 1. Check consumer GC logs for long pauses (> session timeout)<br>2. Verify heartbeat runs in dedicated goroutine without blocking<br>3. Monitor coordinator CPU/memory during rebalances<br>4. Check network packet loss statistics between consumers and coordinator | 1. Increase `session.timeout.ms` to accommodate GC pauses<br>2. Run heartbeat in separate goroutine with its own context<br>3. Add load shedding to coordinator; scale horizontally<br>4. Improve network reliability or implement exponential backoff on rejoin |\n| **Inconsistent Ordering** | 1. **Key hash collision** sending same key to different partitions<br>2. **Producer retries** reordering messages due to partial failures<br>3. **Multiple producers** writing to same partition without coordination<br>4. **Consumer reading from wrong offset** due to incorrect index lookup | 1. Test partitioner with known keys; verify consistent mapping<br>2. Examine producer logs for out-of-order retry attempts<br>3. Check if multiple producer instances use same partition<br>4. Verify `Index.FindEntry` logic returns correct position | 1. Ensure `HashPartitioner.Partition` uses stable hash algorithm<br>2. Implement producer idempotency with sequence numbers<br>3. Use single producer per partition or leader epoch fencing<br>4. Test index lookup with edge cases (exact match, between entries) |\n| **ISR Shrinkage to Empty** | 1. **Follower lag threshold too small** for normal operation<br>2. **Network partition** isolating followers from leader<br>3. **Follower I/O issues** causing slow replication<br>4. **Leader not updating follower state** in `ISRManager` | 1. Check `replica.lag.time.max.ms` vs actual replication latency<br>2. Examine network connectivity between broker nodes<br>3. Monitor follower disk I/O metrics and replication fetch times<br>4. Verify `ISRManager.UpdateFollowerState` is called on successful fetch | 1. Increase `replica.lag.time.max.ms` to accommodate temporary lag<br>2. Implement network health checks and partition detection<br>3. Separate replication traffic to dedicated network interfaces<br>4. Ensure follower state updates are not silently dropped |\n| **Memory Leak** | 1. **Goroutine leak** from unbounded channel operations or missing `context` cancellation<br>2. **Cache growth without eviction** in metadata or connection pools<br>3. **File descriptor leak** from unclosed log segment files<br>4. **Reference cycles** preventing garbage collection | 1. Monitor goroutine count over time; check for `go` statements without cleanup<br>2. Track cache sizes (`MetadataCache`, `ConnectionPool`)<br>3. Check `lsof` output for growing open files in broker process<br>4. Use heap profiling to identify reference chains | 1. Add `defer cancel()` to all `context.WithCancel`; close all channels<br>2. Implement LRU eviction or size limits on caches<br>3. Ensure `LogSegment` files are closed when no longer needed<br>4. Break cycles using weak references or explicit cleanup methods |\n\n#### Debugging Workflow Example: Diagnosing Duplicate Messages\n\nLet's walk through a concrete debugging scenario using the techniques above:\n\n1. **Observe Symptom**: Consumer application reports processing the same message ID twice.\n\n2. **Initial Investigation**:\n   - Check consumer logs: \"Committing offset 142 for partition topic-0\"\n   - Check producer logs: \"Retrying batch for topic-0 partition 0 (attempt 2/3)\"\n   - Correlation: Retry occurs around same time as offset commit\n\n3. **Form Hypothesis**: Producer retry after successful send but before acknowledgment received.\n\n4. **Design Experiment**:\n   - Add logging to producer: \"Batch sent with sequence X\" and \"Received ack for sequence X\"\n   - Add logging to consumer: \"Processing offset Y\" and \"Committed offset Y\"\n   - Reproduce with network simulation: Add artificial latency between send and ack\n\n5. **Analyze Results**:\n   - Logs show: Send seq=5 → Network delay → Timeout → Retry seq=5 → Original ack arrives → Retry succeeds → Two copies\n   - Consumer processes offset 142 twice because it crashes between processing and commit\n\n6. **Root Cause**: Two issues combined:\n   - Producer: No idempotency, retries create duplicates\n   - Consumer: Offset committed asynchronously, crash causes reprocessing\n\n7. **Implement Fix**:\n   - Producer: Add `ProducerID` and sequence numbers to `RecordBatch`, broker deduplicates\n   - Consumer: Change `CommitAsync` to `CommitSync` after processing completes\n\n8. **Verify Fix**:\n   - Run integration test with injected network failures\n   - Confirm no duplicates in 10,000 message test\n   - Monitor for performance regression from synchronous commits\n\n### Implementation Guidance\n\n> **Mental Model**: Think of debugging infrastructure as **scaffolding around a building under construction**. You build temporary platforms (logging), install safety nets (metrics), and create access points (debug APIs) that help you work on the main structure, then remove or minimize them when the building is complete.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Logging** | `logrus` or `zap` for structured JSON logging | OpenTelemetry with distributed tracing to correlate requests across services |\n| **Metrics** | Prometheus client library exposing `/metrics` endpoint | StatsD/DataDog integration with dashboards and alerting |\n| **Profiling** | Go's built-in `pprof` HTTP endpoints (`/debug/pprof/`) | Continuous profiling with Parca or Pyroscope for production |\n| **Debug API** | Simple HTTP server with JSON endpoints for internal state | gRPC health checks and reflection for deeper inspection |\n| **Chaos Testing** | Manual network partition simulation (`iptables`) | ChaosMesh or Litmus for automated failure injection |\n\n#### B. Recommended File/Module Structure\n\n```\nproject-root/\n├── cmd/\n│   ├── broker/                 # Broker executable\n│   │   └── main.go\n│   ├── producer-cli/           # Producer CLI tool\n│   │   └── main.go\n│   └── consumer-cli/           # Consumer CLI tool\n│       └── main.go\n├── internal/\n│   ├── debug/                  # Debugging infrastructure\n│   │   ├── debug_server.go     # HTTP server for debug endpoints\n│   │   ├── metrics.go          # Prometheus metrics collection\n│   │   ├── pprof.go            # pprof endpoint setup\n│   │   └── state_dump.go       # State snapshot utilities\n│   ├── logging/                # Structured logging setup\n│   │   ├── logger.go           # Logger initialization\n│   │   ├── context.go          # Context with correlation IDs\n│   │   └── fields.go           # Common log field definitions\n│   └── ... (other components)\n└── scripts/\n    ├── simulate-failure.sh     # Script to inject network failures\n    ├── analyze-logs.py         # Log analysis utility\n    └── memory-profile.sh       # Memory profiling script\n```\n\n#### C. Infrastructure Starter Code: Structured Logger Wrapper\n\n```go\n// internal/logging/logger.go\npackage logging\n\nimport (\n    \"context\"\n    \"os\"\n    \"runtime\"\n    \"strings\"\n    \n    log \"github.com/sirupsen/logrus\"\n)\n\ntype ctxKey string\n\nconst (\n    correlationIDKey ctxKey = \"correlation_id\"\n    componentKey     ctxKey = \"component\"\n)\n\n// InitLogger configures the global logger with structured JSON output\nfunc InitLogger(level string, enableCaller bool) {\n    logLevel, err := log.ParseLevel(level)\n    if err != nil {\n        logLevel = log.InfoLevel\n    }\n    log.SetLevel(logLevel)\n    \n    log.SetFormatter(&log.JSONFormatter{\n        TimestampFormat: \"2006-01-02T15:04:05.999Z07:00\",\n        FieldMap: log.FieldMap{\n            log.FieldKeyTime:  \"timestamp\",\n            log.FieldKeyLevel: \"severity\",\n            log.FieldKeyMsg:   \"message\",\n            log.FieldKeyFunc:  \"caller\",\n        },\n    })\n    \n    log.SetOutput(os.Stdout)\n    \n    if enableCaller {\n        log.SetReportCaller(true)\n    }\n}\n\n// WithContext returns a log entry with fields from context\nfunc WithContext(ctx context.Context) *log.Entry {\n    entry := log.NewEntry(log.StandardLogger())\n    \n    if ctx != nil {\n        if corrID, ok := ctx.Value(correlationIDKey).(string); ok && corrID != \"\" {\n            entry = entry.WithField(\"correlation_id\", corrID)\n        }\n        if component, ok := ctx.Value(componentKey).(string); ok && component != \"\" {\n            entry = entry.WithField(\"component\", component)\n        }\n    }\n    \n    // Add caller info if enabled\n    if log.StandardLogger().ReportCaller {\n        if pc, file, line, ok := runtime.Caller(1); ok {\n            funcName := runtime.FuncForPC(pc).Name()\n            // Simplify file path\n            if idx := strings.LastIndex(file, \"/\"); idx != -1 {\n                file = file[idx+1:]\n            }\n            entry = entry.WithField(\"caller\", \n                log.Fields{\"file\": file, \"line\": line, \"function\": funcName})\n        }\n    }\n    \n    return entry\n}\n\n// NewContextWithCorrelation creates a new context with correlation ID\nfunc NewContextWithCorrelation(parent context.Context, corrID string) context.Context {\n    return context.WithValue(parent, correlationIDKey, corrID)\n}\n\n// NewContextWithComponent creates a new context with component name\nfunc NewContextWithComponent(parent context.Context, component string) context.Context {\n    return context.WithValue(parent, componentKey, component)\n}\n\n// LogLevelFromString safely parses log level\nfunc LogLevelFromString(level string) log.Level {\n    l, err := log.ParseLevel(level)\n    if err != nil {\n        return log.InfoLevel\n    }\n    return l\n}\n```\n\n#### D. Core Logic Skeleton: Debug State Collection\n\n```go\n// internal/debug/state_dump.go\npackage debug\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"runtime\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/yourproject/internal/types\"\n)\n\n// DebugStateCollector gathers state from various components\ntype DebugStateCollector struct {\n    broker    *types.Server\n    startTime time.Time\n    mu        sync.RWMutex\n    metrics   map[string]interface{}\n}\n\n// NewDebugStateCollector creates a new collector\nfunc NewDebugStateCollector(broker *types.Server) *DebugStateCollector {\n    return &DebugStateCollector{\n        broker:    broker,\n        startTime: time.Now(),\n        metrics:   make(map[string]interface{}),\n    }\n}\n\n// CollectBrokerState gathers comprehensive broker state\nfunc (d *DebugStateCollector) CollectBrokerState() (map[string]interface{}, error) {\n    d.mu.Lock()\n    defer d.mu.Unlock()\n    \n    state := make(map[string]interface{})\n    \n    // TODO 1: Collect basic broker info (ID, host, port, uptime)\n    // TODO 2: Collect memory statistics using runtime.MemStats\n    // TODO 3: Collect goroutine count and thread information\n    // TODO 4: Collect topic and partition state (call broker.GetDebugState())\n    // TODO 5: Collect consumer group state from coordinator\n    // TODO 6: Collect replication state from replica manager\n    // TODO 7: Collect connection pool statistics\n    // TODO 8: Collect request queue depths and processing times\n    // TODO 9: Collect disk usage information for log directories\n    // TODO 10: Collect any custom metrics stored in d.metrics\n    \n    return state, nil\n}\n\n// RecordMetric stores a custom metric for debugging\nfunc (d *DebugStateCollector) RecordMetric(name string, value interface{}) {\n    d.mu.Lock()\n    defer d.mu.Unlock()\n    d.metrics[name] = value\n}\n\n// GetMemoryStats returns Go memory statistics\nfunc (d *DebugStateCollector) GetMemoryStats() map[string]interface{} {\n    var memStats runtime.MemStats\n    runtime.ReadMemStats(&memStats)\n    \n    return map[string]interface{}{\n        \"alloc_mb\":        float64(memStats.Alloc) / 1024 / 1024,\n        \"total_alloc_mb\":  float64(memStats.TotalAlloc) / 1024 / 1024,\n        \"sys_mb\":          float64(memStats.Sys) / 1024 / 1024,\n        \"num_gc\":          memStats.NumGC,\n        \"last_gc_pause_ns\": memStats.PauseNs[(memStats.NumGC+255)%256],\n        \"goroutines\":      runtime.NumGoroutine(),\n        \"cgo_calls\":       runtime.NumCgoCall(),\n    }\n}\n\n// GetUptime returns formatted uptime\nfunc (d *DebugStateCollector) GetUptime() string {\n    uptime := time.Since(d.startTime)\n    days := int(uptime.Hours() / 24)\n    hours := int(uptime.Hours()) % 24\n    minutes := int(uptime.Minutes()) % 60\n    seconds := int(uptime.Seconds()) % 60\n    \n    if days > 0 {\n        return fmt.Sprintf(\"%dd %dh %dm %ds\", days, hours, minutes, seconds)\n    }\n    return fmt.Sprintf(\"%dh %dm %ds\", hours, minutes, seconds)\n}\n\n// JSONString returns state as pretty-printed JSON\nfunc (d *DebugStateCollector) JSONString() (string, error) {\n    state, err := d.CollectBrokerState()\n    if err != nil {\n        return \"\", err\n    }\n    \n    data, err := json.MarshalIndent(state, \"\", \"  \")\n    if err != nil {\n        return \"\", err\n    }\n    \n    return string(data), nil\n}\n```\n\n#### E. Language-Specific Hints (Go)\n\n1. **Memory Profiling**: Use `go tool pprof -alloc_space http://localhost:6060/debug/pprof/heap` to find memory leaks.\n2. **Block Profiling**: Use `go tool pprof http://localhost:6060/debug/pprof/block` to find goroutine blocking.\n3. **Race Detection**: Always run tests with `go test -race ./...`; add `-race` flag to production for critical services.\n4. **Context Usage**: Pass `context.Context` through all async operations; use `context.WithTimeout` for operations that should complete within a deadline.\n5. **Structured Logging**: Use `logrus.Fields` for key-value pairs rather than formatted strings.\n6. **Metrics Collection**: Use Prometheus `Gauge`, `Counter`, and `Histogram` for different types of measurements.\n7. **Debug Endpoints**: Use `net/http/pprof` package and register handlers: `import _ \"net/http/pprof\"`.\n8. **Testing Network Failures**: Use `nettest` package for simulated network conditions in tests.\n\n#### F. Debugging Tips Table\n\n| Symptom | How to Diagnose | Useful Command/Tool | What to Look For |\n|---------|-----------------|---------------------|------------------|\n| **High CPU** | CPU profiling | `go tool pprof -seconds 30 http://localhost:6060/debug/pprof/profile` | Functions with highest `cum` (cumulative) time |\n| **Memory Leak** | Heap profiling | `go tool pprof -alloc_space http://localhost:6060/debug/pprof/heap` | Objects with highest `inuse_space` retained over time |\n| **Goroutine Leak** | Goroutine dump | `curl http://localhost:6060/debug/pprof/goroutine?debug=2` | Goroutines stuck in `chan send` or `chan receive` |\n| **Slow Disk I/O** | Disk monitoring | `iostat -x 1` (Linux) | High `await` or `%util` on data disk |\n| **Network Issues** | Packet capture | `tcpdump -i any port 9092 -w capture.pcap` | Retransmissions, zero windows, RST packets |\n| **Deadlock** | Mutex profiling | `go tool pprof http://localhost:6060/debug/pprof/mutex` | Mutexes with high contention times |\n| **GC Pressure** | GC trace | `GODEBUG=gctrace=1 ./broker` | High `STW` (stop-the-world) pause times |\n\n#### G. Milestone Checkpoint: Debugging Verification\n\nAfter implementing logging and debugging infrastructure, verify it works:\n\n1. **Start your broker** with debug logging enabled:\n   ```\n   LOG_LEVEL=debug ./broker --port 9092 --data-dir ./data\n   ```\n\n2. **Check debug endpoints**:\n   ```\n   curl http://localhost:6060/debug/state | jq .\n   curl http://localhost:6060/debug/pprof/goroutine?debug=1 | head -50\n   ```\n\n3. **Produce test messages** and verify logs show the flow:\n   ```\n   ./producer-cli --topic test --message \"debug test\"\n   # Check broker logs for: \"Appended records\", \"Handled produce request\"\n   ```\n\n4. **Simulate a failure** and verify error logging:\n   ```\n   # In another terminal, kill the broker process\n   pkill -9 broker\n   # Check logs for: \"Received signal\", \"Shutting down\", \"Closing WAL\"\n   ```\n\n5. **Expected Output**: \n   - Structured JSON logs for all major operations\n   - Debug endpoints return valid JSON with broker state\n   - Errors include sufficient context for diagnosis\n   - No panics or data races (run with `-race` flag)\n\n6. **Signs of Problems**:\n   - Logs are missing key fields (partition, offset, correlation ID)\n   - Debug endpoints timeout or return incomplete data\n   - Memory usage grows continuously during idle period\n   - Goroutine count increases without bound\n\n---\n\n\n## 13. Future Extensions\n\n> **Milestone(s):** All (post-core extension opportunities)\n\nHaving successfully implemented the core distributed message queue with partitioned topics, producer batching, consumer groups, and leader-follower replication, you now possess a solid foundation in distributed systems principles. This section explores potential advanced features you could implement to deepen your understanding and extend the system's capabilities. Each extension represents real-world challenges faced by production message brokers like Apache Kafka, and implementing them will sharpen your skills in performance optimization, consistency guarantees, and resource management.\n\n### 13.1 Potential Feature Additions\n\nThe core system you've built provides reliable, ordered message delivery with basic fault tolerance. However, production systems require additional features to handle diverse workloads, optimize resource usage, and guarantee stronger semantics. Below are several advanced extensions, each with a mental model to build intuition before technical details.\n\n| Feature | Mental Model | Core Benefit | Complexity |\n|---------|--------------|--------------|------------|\n| **Compression** | A vacuum-sealed package: multiple items compressed into a smaller space for efficient shipping, then decompressed at destination. | Reduces network bandwidth and disk storage by 60-90% for text-based messages. | Medium |\n| **Exactly-Once Semantics** | A bank's transaction ledger with debit/credit pairs: each operation is atomic and idempotent, ensuring final balance reflects exactly one execution of each transfer. | Eliminates duplicate processing in stream processing pipelines, critical for financial use cases. | High |\n| **Quotas & Throttling** | A highway toll system with speed limits and vehicle quotas: controls throughput per client to prevent any single user from overwhelming shared infrastructure. | Protects system stability from misbehaving clients and enables multi-tenancy. | Medium |\n| **Log Compaction** | A key-value store's \"last value wins\" semantics: older updates for the same key are periodically discarded, retaining only the latest state. | Enables using the log as a durable, replayable database of current state rather than infinite history. | High |\n| **Custom RPC Layer** | Replacing postal mail with a dedicated courier service: designing your own protocol optimized for your specific communication patterns. | Reduces serialization overhead, enables zero-copy transfers, and provides finer control over connection management. | High |\n| **Tiered Storage** | A library archive: frequently accessed recent books stay on main shelves, while older volumes move to deep storage, retrievable when needed. | Dramatically reduces storage costs for long-retention topics while maintaining access to historical data. | Very High |\n| **Transaction Support** | A database's ACID transactions across multiple partitions: all writes within a transaction are committed or aborted together. | Enconsistent updates across multiple partitions, crucial for event-driven microservices. | Very High |\n| **Schema Registry** | A central dictionary for data formats: producers and consumers agree on message structure via shared schema definitions, enabling evolution. | Prevents data corruption from format mismatches and enables efficient binary serialization. | Medium |\n\n#### Compression: The Space-Saving Courier\n\nImagine you run a courier service shipping books between libraries. Instead of sending each book individually, you pack multiple books into a single box, vacuum-seal it to reduce volume, then label the box with its contents. At the destination, the box is opened and books returned to normal size. **Compression** works similarly: multiple records in a `RecordBatch` are compressed together using algorithms like GZIP, Snappy, or LZ4 before transmission and storage, then decompressed when consumers fetch them.\n\nThe key insight is that compression operates at the **batch level**, not individual records, because compression algorithms achieve better ratios with more data. Your existing `RecordBatch` structure already has an `Attributes` field with bits for compression type—this extension involves implementing the actual compression/decompression logic in the `RecordBatch.Encode()` and `DecodeRecordBatch()` methods, plus configuring the `Producer` to compress batches meeting a minimum size threshold.\n\n#### Exactly-Once Semantics: The Bank Teller's Ledger\n\nConsider a bank teller processing deposits. Each transaction gets a unique sequence number in the ledger. If the network fails after the deposit but before the customer receives confirmation, the customer might retry—but the teller checks the sequence number and ignores duplicates. **Exactly-once semantics** extends your idempotent producer (which prevents duplicates within a single partition) to span multiple partitions and consumer side effects via **transactional producers** and **read-committed isolation**.\n\nThis requires three coordinated mechanisms: 1) **Transactional IDs** for producer fencing, 2) **Transaction coordinator** (a specialized broker role) managing two-phase commit across partitions, and 3) **Transaction markers** written to logs to indicate commit/abort boundaries. Consumers in `read_committed` mode only deliver messages after the commit marker, avoiding exposure to uncommitted data.\n\n#### Quotas & Throttling: The Highway Traffic Control\n\nA highway system uses toll booths, speed limits, and vehicle quotas to ensure no single route becomes congested. **Quotas** in messaging systems similarly limit the byte rate or request rate per client, user, or topic to prevent a single misconfigured producer from overwhelming broker network I/O or a greedy consumer from monopolizing CPU. Throttling involves measuring traffic, comparing against quotas, and delaying excess requests or adding backpressure.\n\nYour system already has natural measurement points: `Server.HandleProduce()` and `Server.HandleFetch()` can track bytes per client ID. The challenge is implementing fair, responsive throttling without introducing substantial overhead. A token bucket algorithm per client, checked before processing each request, provides smooth rate limiting.\n\n#### Log Compaction: The Librarian's Archive Purge\n\nImagine a librarian periodically scanning shelves, removing all but the latest edition of each book title. **Log compaction** does this for keyed messages: it periodically rewrites log segments, discarding older records for keys that have more recent updates, while retaining all records for keys without updates (including null-keyed records). This transforms the log from an infinite append-only history into a finite, replayable key-value store.\n\nCompaction requires background threads scanning `LogSegment` files, creating new compacted segments, and atomically swapping them. The `Log` needs to track the **clean offset**—the point before which all keys are guaranteed to be compacted. Consumers can then optionally read from this offset to get the latest state of all keys.\n\n#### Custom RPC Layer: Building Your Own Postal Service\n\nWhile you've implemented a basic length-prefixed TCP protocol, production systems often optimize further with custom binary protocols supporting multiplexing, header compression, and zero-copy transfers. Designing a **custom RPC layer** involves creating a wire format tailored to your specific request/response patterns, potentially using frameworks like gRPC or building directly on epoll/kqueue for high throughput.\n\nThis extension would replace your current `TCPServer` and request handling with a more sophisticated transport that better leverages system calls, reduces allocations, and provides better connection pooling. It's an excellent deep dive into network programming and performance optimization.\n\n### 13.2 Design Considerations for Extensions\n\nEach extension presents unique design challenges and requires modifications to your existing architecture. Below we analyze each feature's impact through the lens of Architecture Decision Records (ADRs), comparing implementation approaches and highlighting integration points with your current components.\n\n#### 13.2.1 Compression: Batch-Level vs Record-Level\n\n> **Decision: Implement Compression at RecordBatch Level**\n> - **Context**: Messages often have high redundancy (JSON fields, repeated text), especially within related records produced together. We need to reduce network and storage overhead without excessive CPU cost.\n> - **Options Considered**:\n>     1. **Record-level compression**: Each `Record` compressed individually.\n>     2. **Batch-level compression**: Entire `RecordBatch` compressed as a unit.\n>     3. **Streaming compression**: Continuous compression across batch boundaries.\n> - **Decision**: Implement batch-level compression using the existing `RecordBatch.Attributes` field to indicate compression type.\n> - **Rationale**: Batch-level achieves better compression ratios (more data for dictionary-based algorithms) and aligns with the natural unit of network transfer and disk I/O. It also simplifies implementation: compression/decompression happens at the same boundaries as existing serialization.\n> - **Consequences**: Producers must accumulate enough records to make compression worthwhile; smaller batches gain less benefit. Consumers must handle decompression transparently. CPU overhead concentrated at producer and consumer rather than brokers.\n\n| Option | Pros | Cons | Suitable For |\n|--------|------|------|--------------|\n| **Record-level** | Fine-grained, no batching delay | Poor compression ratio, high overhead | Large individual messages (>1MB) |\n| **Batch-level** | Good ratio, aligns with existing unit | Requires minimum batch size | Typical throughput-optimized workloads |\n| **Streaming** | Best ratio across many batches | Complex state management, harder random access | Very high throughput archival |\n\n**Integration with Current Design**:\n- **Producer**: The `Sender.sendBatch()` method would compress the serialized batch bytes if `batch.Attributes` indicates compression and batch size exceeds `compression.threshold`.\n- **Broker**: The `Log.Append()` stores compressed bytes directly; brokers don't decompress except for validation. The `Log.Read()` returns compressed batches to consumers.\n- **Consumer**: `DecodeRecordBatch()` detects compression via `Attributes` and decompresses before parsing individual records.\n- **Configuration**: Add `CompressionType` and `CompressionThreshold` to `ProducerConfig`.\n\nThe `RecordBatch.Encode()` algorithm extends to:\n1. Serialize records to temporary buffer\n2. If compression enabled and buffer size > threshold:\n   - Compress buffer using selected algorithm\n   - Set compression bits in `Attributes`\n3. Calculate CRC on compressed data (if using)\n4. Write batch header followed by compressed data\n\n#### 13.2.2 Exactly-Once Semantics: Transaction Coordinator Placement\n\n> **Decision: Embed Transaction Coordinator in Existing Brokers**\n> - **Context**: Transactions require a coordinator to manage two-phase commit across multiple partitions, which may be hosted on different brokers. We need to decide where this coordinator runs.\n> - **Options Considered**:\n>     1. **Dedicated coordinator broker**: Single broker elected as transaction coordinator.\n>     2. **Embedded in all brokers**: Each broker can coordinate transactions for producers that choose it.\n>     3. **External service**: Separate process/container managing transactions.\n> - **Decision**: Embed transaction coordination capability in all brokers, with producers hashing transactional ID to select a coordinator.\n> - **Rationale**: This follows Kafka's design, ensuring scalability (coordination load distributes across cluster) and fault tolerance (coordinator failure only affects its transactions). It leverages existing broker infrastructure for persistence and networking.\n> - **Consequences**: Each broker needs additional transaction state; producer must discover coordinator via hash; transaction recovery requires scanning logs.\n\n**Implementation Outline**:\n1. **Transaction Coordinator Component**: New `TransactionCoordinator` type in each broker, managing `TransactionalId → ProducerIdMapping` and `TransactionLog`.\n2. **Transaction Protocol**: \n   - `InitProducerId(TransactionalId)` → returns `ProducerId, Epoch` (for fencing)\n   - `AddPartitionsToTransaction(TxnId, [TopicPartition])`\n   - `EndTransaction(TxnId, CommitOrAbort)`\n3. **Transaction Log**: Special internal topic `__transaction_state` storing transaction metadata.\n4. **Consumer Read Committed**: Modify `Log.Read()` to skip messages between `BEGIN` and `COMMIT/ABORT` markers when `isolation.level=read_committed`.\n\nYour existing `Partition` log would need to support **transaction markers**—special control records written during two-phase commit. The `HighWatermark` advancement logic must consider that messages before an uncommitted transaction marker shouldn't be exposed to `read_committed` consumers.\n\n#### 13.2.3 Quotas: Measurement and Enforcement Points\n\n> **Decision: Enforce Quotas at Request Handler with Token Bucket**\n> - **Context**: Need to limit client throughput to protect cluster stability. Must decide where to measure and how to enforce limits.\n> - **Options Considered**:\n>     1. **Network layer enforcement**: Throttle at TCP accept/read level.\n>     2. **Request handler enforcement**: Delay processing after parsing request but before business logic.\n>     3. **Response throttling**: Allow processing but delay response.\n> - **Decision**: Implement token bucket algorithm in `Server.HandleProduce()` and `Server.HandleFetch()` methods, tracking bytes per client ID.\n> - **Rationale**: Request handler has access to structured request information (client ID, topic, byte count) and can make precise decisions. Token bucket provides smooth throttling (not all-or-nothing) and accumulates credits during idle periods.\n> - **Consequences**: Adds per-request overhead for quota checking; delayed responses may cause client timeouts.\n\n**Integration Points**:\n- **Quota Config**: Add to `Server` config: `quota.producer.byte.rate`, `quota.consumer.byte.rate` (bytes/sec per client).\n- **Quota Manager**: New `QuotaManager` with `CheckQuota(clientID string, bytes int) time.Duration` returning delay needed.\n- **Request Handling**:\n```go\n// In Server.HandleProduce\ndelay := s.quotaManager.CheckQuota(clientID, len(request.Data))\nif delay > 0 {\n    time.Sleep(delay) // Or implement asynchronous delaying\n}\n```\n- **Metrics**: Track throttling time per client for observability.\n\nA key challenge is **quota distribution across broker cluster**—if a client connects to multiple brokers, each sees only partial traffic. A simplified approach enforces quotas per-broker, which works for clients with stable broker connections.\n\n#### 13.2.4 Log Compaction: Background vs Online Compaction\n\n> **Decision: Implement Background Compaction Thread per Log**\n> - **Context**: Need to periodically clean logs while maintaining availability for reads and writes.\n> - **Options Considered**:\n>     1. **Background thread**: Separate goroutine periodically scans and rewrites segments.\n>     2. **Online compaction**: Compact during normal writes (append-only with periodic merging).\n>     3. **Offline compaction**: Stop serving partition, compact entire log, resume.\n> - **Decision**: Background compaction thread per `Log` that runs when segments meet compaction criteria.\n> - **Rationale**: Background compaction minimizes impact on read/write latency. It can be scheduled during low-load periods. The algorithm mirrors Kafka's log cleaner.\n> - **Consequences**: Increases disk I/O during compaction; requires careful coordination to avoid compacting active segments.\n\n**Compaction Algorithm**:\n1. **Select Segments**: Choose log segments where `dirtyRatio = (segment.size - clean.size) / segment.size > minDirtyRatio`.\n2. **Build Key Map**: Read selected segments, track latest offset per key.\n3. **Write Clean Segment**: Create new segment containing only latest records for each key (plus all records without keys).\n4. **Atomically Replace**: Swap old segments with new clean segment in `Log.Segments`.\n5. **Update Clean Offset**: Record offset before which all keys are compacted.\n\nYour `Log` would need new fields:\n```go\ntype Log struct {\n    // ... existing fields\n    CleanOffset      int64        // Offset before which compaction is guaranteed\n    CompactConfig    CompactConfig\n    compacting       bool         // Guard against concurrent compaction\n}\n```\n\nConsumers reading with `isolation.level=read_committed` and from `clean.offset` would see a consistent snapshot of the latest value for each key—effectively turning your message queue into a persistent key-value store.\n\n#### 13.2.5 Custom RPC Layer: Protocol Design Trade-offs\n\n> **Decision: Implement Connection Multiplexing with Request Batching**\n> - **Context**: Current simple TCP server handles one request per connection serially, limiting throughput.\n> - **Options Considered**:\n>     1. **HTTP/2 with gRPC**: Use standard multiplexed protocol.\n>     2. **Custom binary with framing**: Design own length-prefixed multiplexed protocol.\n>     3. **UDP-based protocol**: For lower latency but lossy transport.\n> - **Decision**: Design custom binary protocol supporting request pipelining and connection multiplexing.\n> - **Rationale**: Custom protocol allows optimization for specific patterns (small metadata requests mixed with large data transfers). Avoids HTTP overhead while maintaining reliability via TCP.\n> - **Consequences**: Significant implementation effort; must handle backpressure, timeouts, and connection lifecycle.\n\n**Protocol Enhancements**:\n1. **Multiplexing**: Single connection carries multiple concurrent request/response streams with correlation IDs.\n2. **Zero-copy for large fetches**: Use `sendfile()` or similar to transfer log segments without copying to userspace.\n3. **Header compression**: Repeated headers (topic names, client IDs) compressed across requests.\n4. **Binary encoding**: Replace current ad-hoc encoding with Protocol Buffers or similar for schema evolution.\n\nThis extension would essentially replace your entire `network` package with a more sophisticated implementation, touching almost every component. It's the most invasive but educational extension for understanding high-performance network services.\n\n#### 13.2.6 Tiered Storage: Hot/Warm/Cold Architecture\n\n> **Decision: Implement Transparent Tiering with Async Offload**\n> - **Context**: Logs grow indefinitely, consuming expensive local SSD storage. Need to move older data to cheaper object storage (S3-like).\n> - **Options Considered**:\n>     1. **Manual tiering**: Users manually move segments between tiers.\n>     2. **Transparent offload**: System automatically moves segments based on age.\n>     3. **Hierarchical storage**: OS-level tiering (e.g., LVM cache).\n> - **Decision**: Implement background offload of closed log segments to object storage, with local LRU cache for recently accessed segments.\n> - **Rationale**: Transparent operation requires no application changes. Async offload minimizes performance impact. Object storage provides durability and virtually unlimited capacity.\n> - **Consequences**: Adds complexity for fetch path (check local, then remote); requires object storage integration; network latency for cold reads.\n\n**Architecture Impact**:\n- **LogSegment**: Add `StorageTier` field (`LOCAL`, `REMOTE`, `ARCHIVED`) and `RemoteURI`.\n- **Log.Read()**: If segment is remote, fetch to local cache (possibly with prefetch).\n- **TierManager**: Background goroutine scanning for segments older than `retention.ms` to offload.\n- **Configuration**: Add `tiered.storage.enabled`, `remote.storage.endpoint`, `local.cache.size`.\n\nThis extension transforms your broker from a storage node to a **caching layer** over durable object storage—a common pattern in modern data systems like Kafka with Tiered Storage.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Compression | Standard library `compress/gzip` | C bindings to Snappy or LZ4 via cgo |\n| Transaction Coordinator | In-memory state with WAL for recovery | Full Paxos/Raft for coordinator fault tolerance |\n| Quota Enforcement | Simple token bucket per client | Distributed quota tracking via gossip protocol |\n| Log Compaction | Single-pass scanner building map in memory | Multi-pass with disk-based hash tables for large logs |\n| Custom RPC | Connection-per-request with pipelining | Reactor pattern with epoll/kqueue event loop |\n| Tiered Storage | Mock object storage (local filesystem) | Integration with MinIO or AWS S3 SDK |\n\n#### Recommended Module Structure for Extensions\n\n```\nproject-root/\n  internal/\n    compression/           # Compression algorithms\n      codec.go            # Interface: Compress(data []byte) []byte\n      gzip.go             # GZIP implementation\n      snappy.go           # Snappy implementation (optional)\n    transaction/\n      coordinator.go      # TransactionCoordinator\n      log.go              # Transaction log (internal topic)\n      producer_id.go      # Producer ID generation and fencing\n    quotas/\n      manager.go          # QuotaManager with token buckets\n      metrics.go          # Throttling metrics collection\n    compaction/\n      cleaner.go          # LogCleaner background thread\n      strategy.go         # Compaction strategy interface\n    storage/\n      tier/               # Tiered storage\n        manager.go        # TierManager for offloading\n        cache.go          # LRU cache for remote segments\n        s3.go             # S3 client (optional)\n    protocol/             # Enhanced RPC layer\n      framing.go          # Multiplexed frame encoding\n      connection.go       # Managed connection with pipelining\n      reactor.go          # Event loop (advanced)\n```\n\n#### Compression Starter Code\n\n```go\n// internal/compression/codec.go\npackage compression\n\ntype Codec interface {\n    Compress(src []byte) ([]byte, error)\n    Decompress(src []byte) ([]byte, error)\n    Name() string\n}\n\n// internal/compression/gzip.go\npackage compression\n\nimport \"compress/gzip\"\nimport \"bytes\"\n\ntype GzipCodec struct{}\n\nfunc (g *GzipCodec) Compress(src []byte) ([]byte, error) {\n    var buf bytes.Buffer\n    w := gzip.NewWriter(&buf)\n    if _, err := w.Write(src); err != nil {\n        return nil, err\n    }\n    if err := w.Close(); err != nil {\n        return nil, err\n    }\n    return buf.Bytes(), nil\n}\n\nfunc (g *GzipCodec) Decompress(src []byte) ([]byte, error) {\n    r, err := gzip.NewReader(bytes.NewReader(src))\n    if err != nil {\n        return nil, err\n    }\n    defer r.Close()\n    var buf bytes.Buffer\n    if _, err := buf.ReadFrom(r); err != nil {\n        return nil, err\n    }\n    return buf.Bytes(), nil\n}\n\nfunc (g *GzipCodec) Name() string { return \"gzip\" }\n\n// In RecordBatch.Encode() modification\nfunc (b *RecordBatch) Encode() ([]byte, error) {\n    // 1. Serialize records to buffer (as before)\n    // 2. Check if compression configured\n    if b.Attributes & CompressionMask != CompressionNone {\n        // 3. Get appropriate codec\n        codec := compression.GetCodec(b.Attributes)\n        compressed, err := codec.Compress(buffer.Bytes())\n        if err != nil {\n            return nil, err\n        }\n        // 4. Update buffer with compressed data\n        buffer.Reset()\n        buffer.Write(compressed)\n    }\n    // 5. Continue with CRC and final encoding\n}\n```\n\n#### Transaction Coordinator Skeleton\n\n```go\n// internal/transaction/coordinator.go\npackage transaction\n\ntype TransactionCoordinator struct {\n    brokerID      int32\n    logManager    *LogManager\n    pendingTxns   map[string]*TransactionMetadata // key: transactionalID\n    producerState map[string]*ProducerState       // key: transactionalID\n    mu            sync.RWMutex\n}\n\ntype TransactionMetadata struct {\n    TransactionalID    string\n    ProducerID         int64\n    ProducerEpoch      int16\n    State              TransactionState\n    Partitions         []TopicPartition\n    TimeoutMs          int32\n    LastUpdateTime     time.Time\n}\n\n// HandleInitProducerId processes request for new producer ID\nfunc (tc *TransactionCoordinator) HandleInitProducerId(\n    transactionalID string,\n    timeoutMs int32,\n) (producerID int64, producerEpoch int16, err error) {\n    // TODO 1: Check if transactionalID already exists in producerState\n    // TODO 2: If exists, validate epoch for fencing (reject if old epoch)\n    // TODO 3: Generate new producerID (monotonically increasing)\n    // TODO 4: Initialize new transaction metadata with state EMPTY\n    // TODO 5: Write to transaction log for durability\n    // TODO 6: Return producerID and epoch\n}\n\n// HandleAddPartitions processes request to add partitions to transaction\nfunc (tc *TransactionCoordinator) HandleAddPartitions(\n    transactionalID string,\n    partitions []TopicPartition,\n) error {\n    // TODO 1: Look up transaction metadata\n    // TODO 2: Validate producer epoch matches\n    // TODO 3: Ensure transaction is in ONGOING state\n    // TODO 4: Add partitions to metadata (deduplicate)\n    // TODO 5: Write updated metadata to transaction log\n}\n\n// HandleEndTransaction processes commit/abort request\nfunc (tc *TransactionCoordinator) HandleEndTransaction(\n    transactionalID string,\n    commit bool,\n) error {\n    // TODO 1: Look up transaction metadata\n    // TODO 2: Validate producer epoch\n    // TODO 3: Prepare PREPARE_COMMIT or PREPARE_ABORT marker\n    // TODO 4: Write PREPARE marker to transaction log (fsync)\n    // TODO 5: For each partition in transaction:\n    //   - Write transaction marker (COMMIT/ABORT) to partition log\n    // TODO 6: After all markers written, write COMPLETE to transaction log\n    // TODO 7: Clean up transaction state\n}\n```\n\n#### Language-Specific Hints for Go\n\n- **Compression**: Use `compress/gzip` for simplicity, but consider `github.com/golang/snappy` for better speed if needed.\n- **Concurrent Compaction**: Use `sync.RWMutex` in `Log` with `TryLock()` to skip compaction if log is busy.\n- **Token Bucket Quotas**: Implement with `time.Ticker` refilling tokens and `sync/atomic` for token count.\n- **Transaction Recovery**: On coordinator startup, scan transaction log to rebuild in-memory state of incomplete transactions.\n- **Tiered Storage**: Use `io.ReaderAt` interface for random access to remote objects; implement LRU cache with `container/list` and `map`.\n\n#### Milestone Checkpoint for Compression Extension\n\n**Verification Test**:\n```bash\n# Run compression integration test\ncd project-root\ngo test ./internal/compression/... -v -run TestCompressionRatio\n\n# Expected output should show:\n# === RUN   TestCompressionRatio\n#     compression_test.go:45: GZIP ratio: 85% (1500 bytes → 225 bytes)\n# --- PASS: TestCompressionRatio (0.02s)\n\n# Manual verification:\n# 1. Start broker with compression enabled\n# 2. Use producer with compression.type=gzip\n# 3. Produce 1000 messages with repetitive JSON content\n# 4. Check disk usage in log directory - should be ~15% of uncompressed size\n# 5. Consume messages - should decompress transparently\n```\n\n**Debugging Tips for Compression**:\n- **Symptom**: Consumer receives garbled data or fails to decode.\n  - **Cause**: Compression attributes mismatched between producer and consumer.\n  - **Diagnosis**: Check `RecordBatch.Attributes` bits in stored log segment (hex dump).\n  - **Fix**: Ensure `DecodeRecordBatch()` reads compression bits and uses correct decompressor.\n\n- **Symptom**: High CPU usage on producer.\n  - **Cause**: Compressing very small batches (worse than no compression).\n  - **Diagnosis**: Log batch sizes before compression.\n  - **Fix**: Increase `batch.size` or set `compression.threshold` higher.\n\nEach extension represents a significant engineering challenge that will deepen your understanding of distributed systems trade-offs. Start with compression (easiest) to gain confidence, then progress to more complex features like transactions. Remember that production systems evolve incrementally—each feature builds upon a solid foundation, which you now possess.\n\n\n## 14. Glossary\n\n> **Milestone(s):** 1, 2, 3, 4 (all foundational concepts)\n\nThis glossary defines the key terms, acronyms, and domain-specific vocabulary used throughout this design document. Building intuition for these concepts is essential for understanding the architecture and implementation of a distributed message queue.\n\n### 14.1 Terms and Definitions\n\nThe following table provides an alphabetical reference of technical terms used in this project.\n\n| Term | Definition |\n|------|------------|\n| **Accumulator** | A producer component that temporarily batches multiple records by their destination `TopicPartition` before sending them as a single network request. It improves throughput by amortizing the overhead of network round-trips and serialization. |\n| **Acks (Acknowledgments)** | The producer's configured durability guarantee, represented by `AcksLevel`. `AcksNone` (0) means fire-and-forget; `AcksLeader` (1) waits for the partition leader to persist the record; `AcksAll` (-1) waits for all in-sync replicas (ISR) to acknowledge. |\n| **API Key** | A numeric identifier in the wire protocol's `MessageHeader` that specifies the type of request (e.g., `ProduceRequest`, `FetchRequest`, `JoinGroupRequest`). |\n| **BaseOffset** | The offset of the first record in a `LogSegment`. All offsets within the segment are relative to this base. |\n| **Broker** | A server node in the cluster that stores partition replicas and handles client requests. A broker hosts a subset of partitions for various topics, acting as both a storage layer and a network endpoint. |\n| **Compaction** | See **Log Compaction**. |\n| **Consumer** | A client application that subscribes to and reads records from topics. Consumers belong to a `ConsumerGroup` to coordinate parallel consumption. |\n| **Consumer Group** | A set of consumers that cooperate to consume a topic. The group coordinator assigns each partition of a subscribed topic to exactly one member of the group, enabling parallel consumption while preserving ordering guarantees within each partition. |\n| **Control Plane** | The network path dedicated to coordination operations, such as consumer group management (`JoinGroup`, `SyncGroup`, `Heartbeat`), metadata propagation, and leadership elections. This is distinct from the **Data Plane** which carries the actual message flow. |\n| **Controller** | A designated broker (with the lowest or elected ID) that manages partition leadership elections and replica assignments for the entire cluster. It acts as the centralized decision-maker for partition state changes. |\n| **Coordination Service** | A service (like the in-memory `MemoryCoordinator` or an external system like ZooKeeper) responsible for maintaining and distributing cluster metadata (brokers, topics, partition leadership) and enabling consensus for operations like leader election. |\n| **Correlation ID** | A client-generated integer included in every request's `MessageHeader`. The server echoes it back in the corresponding response, allowing the client to match asynchronous requests with their responses. |\n| **Data Plane** | The network path dedicated to the actual flow of messages, including produce requests (writes) and fetch requests (reads). This is distinct from the **Control Plane** used for coordination. |\n| **Exactly-once Semantics** | A delivery guarantee where each message is processed exactly once, even in the face of producer retries, consumer restarts, or broker failures. This typically requires idempotent producers and transactional commits. |\n| **Follower Syncer** | A component running on a broker that continuously fetches new records from a partition leader (via `FetchReplica` requests) to keep its local replica synchronized. Each follower maintains its own `fetchOffset`. |\n| **Generation ID** | A monotonically increasing number (`GenerationID` in `ConsumerGroup`) that identifies the current epoch of a consumer group. It is incremented on each successful rebalance and used to fence out stale members from previous generations. |\n| **Group Coordinator** | A specific broker (elected per consumer group) that manages consumer group membership, heartbeats, partition assignment, and offset commits for that group. It runs the group membership protocol. |\n| **High Watermark** | The offset of the last record that has been successfully replicated to all In-Sync Replicas (ISR). Consumers can only read up to the high watermark; records beyond it are considered \"uncommitted\" and could be lost if the leader fails. |\n| **Idempotent Producer** | A producer configured to use sequence numbers (`ProducerID`, `ProducerEpoch`, `BaseSequence`) per partition to detect and discard duplicate batches caused by retries, enabling at-least-once semantics without duplicates. |\n| **Incremental Fetch** | An optimization in the `FetchRequest` protocol where consumers only list partitions that have changed since their last request, reducing the size of requests when most partitions have no new data. |\n| **Index (Sparse Index)** | A file (`IndexFile`) associated with a log segment that maps some record offset deltas to their byte positions in the data file. It is \"sparse\" because it does not index every record, trading some precision for smaller size. |\n| **Internal Topic** | A special topic used by the system for its own metadata storage. In this project, `__consumer_offsets` is an example used to persistently store consumer group offset commits. |\n| **ISR (In-Sync Replica)** | The set of replicas (leader and followers) for a partition that are fully caught up with the leader, defined as replicas whose `lastFetchOffset` is within a configured lag threshold (`replica.lag.time.max.ms`) of the leader's log end offset. |\n| **ISR Manager** | A component running on the partition leader that tracks the progress (`followerStatus`) of each follower replica and periodically evaluates which replicas belong in the ISR, removing those that have fallen too far behind. |\n| **ISR Shrinkage** | The process of removing followers from the in-sync replica set when they fail to keep up with the leader's write rate (exceeding the `replica.lag.time.max.ms` threshold). Excessive shrinkage can reduce replication factor and durability. |\n| **Leader Epoch** | A monotonically increasing number (`LeaderEpoch`) associated with each leadership term for a partition. It is used by followers to detect and recover from stale or duplicate data after a leader change, serving as a fencing mechanism. |\n| **Leader-Follower Replication** | The primary replication model where each partition has one designated leader that handles all produce/fetch requests, and one or more followers that asynchronously replicate data from the leader. |\n| **Length-prefixed** | A message format where the first field is an integer (typically 4 bytes) specifying the total size of the remaining message bytes. This allows the receiver to read the complete message without parsing its internal structure first. |\n| **Log Compaction** | A background process (`LogCleaner`) that removes older records for the same key from a log, retaining only the latest value. This reduces storage usage for keyed topics where only the current state matters (e.g., database change logs). |\n| **Log End Offset (LEO)** | The offset of the next message that will be appended to the log (i.e., one greater than the offset of the last written record). For followers, this is their local `fetchOffset`. |\n| **Metadata Coordinator** | The component (often the same as the **Controller**) responsible for maintaining and distributing cluster metadata—broker registrations, topic configurations, partition leader assignments, and ISR sets. |\n| **Multiplexing** | Carrying multiple logical request/response streams over a single physical TCP connection. In this system, a producer or consumer may have one connection to a broker but send many independent requests with different `CorrelationID`s. |\n| **Nullable String** | A string encoding in the wire protocol where a length prefix of `-1` indicates a `null` value, while non-negative lengths indicate a valid string of that length. This is distinct from zero-length strings. |\n| **Offset** | A monotonically increasing integer assigned to each record within a partition, representing its immutable position in the ordered sequence. Offsets start at 0 and are contiguous. |\n| **Offset Commit** | The action by a consumer to persistently store its current read position (offset) for each assigned partition, typically to an internal topic (`__consumer_offsets`). This allows the consumer to resume from that point after a restart. |\n| **Partition** | An ordered, immutable sequence of records that is a subset of a topic. Partitions enable horizontal scaling—different partitions of the same topic can be hosted on different brokers and consumed in parallel. |\n| **Partition Assignment** | The mapping of partitions to consumers within a consumer group, determined by the group coordinator using a strategy like **Range** or **RoundRobin**. Each consumer receives an `Assignment` describing the partitions it should consume. |\n| **Partitioner** | A component (`HashPartitioner`, `RoundRobinAssigner`) that selects the target partition for a record based on its key (if present) or a round-robin scheme (if key is `nil`). Ensures records with the same key go to the same partition. |\n| **Producer** | A client that publishes (writes) records to topics. It batches records, selects target partitions, handles retries, and respects acknowledgment configurations. |\n| **Quota Enforcement** | Limiting resource usage (e.g., bytes per second) per client using algorithms like the **Token Bucket** to prevent a single misbehaving client from overwhelming the broker. |\n| **Rebalance** | The process of redistributing partitions among the members of a consumer group when membership changes (a consumer joins, leaves, or fails). During a rebalance, the group coordinator reassigns partitions and informs all members. |\n| **Rebalance Storm** | A pathological condition where consumer group members frequently join and leave, triggering continuous rebalances and preventing the group from settling into a stable state. Often caused by misconfigured `session.timeout.ms` or network issues. |\n| **Record** | The fundamental unit of data, consisting of a key, value, headers, and metadata (timestamp, attributes). Records are grouped into **RecordBatch**es for efficient storage and transmission. |\n| **Record Batch** | A group of records written together as a single unit on disk and over the network. Batches include metadata like `BaseOffset`, `FirstTimestamp`, and `CRC` for integrity. They are the atomic unit of writing and replication. |\n| **Replication Plane** | The network path dedicated to data replication between brokers (leader to follower via `FetchReplica` requests). This traffic is separate from client produce/fetch traffic to avoid interference. |\n| **Segment (Log Segment)** | A physical file (`DataFile`) containing a contiguous range of log records, along with its accompanying index file (`IndexFile`). Segments are rolled when they reach a configured maximum size (`SegmentMaxBytes`). |\n| **Sender** | A producer component that manages network connections to brokers, sends accumulated batches, handles retries with exponential backoff, and processes acknowledgment responses. |\n| **Session Timeout** | The time (`session.timeout.ms`) after which the group coordinator considers a consumer dead if it hasn't received a heartbeat. This triggers a rebalance to reassign the dead consumer's partitions. |\n| **Sparse Index** | See **Index**. |\n| **Tiered Storage** | A hierarchical storage architecture where older log segments are moved from fast, expensive local storage (`StorageTierLocal`) to slower, cheaper remote storage (`StorageTierRemote`) like object storage, managed by a `TierManager`. |\n| **Token Bucket** | An algorithm (`TokenBucket`) used for rate limiting (**Quota Enforcement**). Tokens are added at a fixed rate (`refillRate`) up to a `capacity`. Each request consumes tokens; if insufficient tokens are available, the request is delayed. |\n| **Topic** | A named stream of records, divided into one or more partitions. Topics are the primary abstraction for categorization—producers write to a topic, and consumers subscribe to one or more topics. |\n| **Two-phase Commit** | A distributed transaction protocol used by `TransactionCoordinator` to ensure atomic commit across multiple partitions. It involves a prepare phase (where participants vote) and a commit/abort phase. |\n| **Unclean Leader Election** | Electing a leader from outside the current ISR (when `allow.unclean.leader.election` is enabled). This risks data loss because the new leader may not have all committed records, but it improves availability when the ISR is empty. |\n| **Watch Pattern** | A callback-based notification mechanism where components can register interest (`WatchBrokers`, `WatchTopic`) in changes to cluster metadata. The coordination service invokes callbacks when the watched data changes. |\n| **Wire Protocol** | The binary format for network communication between components (clients and brokers, brokers and brokers). It defines how requests and responses are serialized into bytes for transmission over TCP. |\n| **Zero-copy** | A technique for data transfer where data is moved between buffers (e.g., from disk to network) without CPU copying, often using OS-level features like `sendfile`. This reduces CPU overhead for high-throughput systems. |\n| **Zombie Consumer** | A consumer that is considered dead by the group coordinator (due to heartbeat timeout) but is still alive and processing messages. This can lead to duplicate processing if partitions are reassigned while the zombie is still active. |\n\n---\n"}