{"html":"<h1 id=\"neural-network-framework-design-document\">Neural Network Framework: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This system implements a PyTorch-like deep learning framework with automatic differentiation, enabling users to build and train neural networks. The key architectural challenge is efficiently computing gradients through dynamic computation graphs using reverse-mode automatic differentiation while maintaining a clean, extensible API.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - establishes the core problem that automatic differentiation solves</p>\n</blockquote>\n<p>Building neural networks from scratch presents a fundamental challenge that every deep learning practitioner must understand: <strong>gradient computation</strong>. While forward propagation through a neural network is conceptually straightforward—data flows through layers, each applying mathematical transformations—the backward pass that enables learning requires computing gradients of complex, nested functions with respect to hundreds or thousands of parameters. This process, known as <strong>backpropagation</strong>, becomes intractable to implement manually as networks grow in complexity.</p>\n<p>Modern deep learning frameworks like PyTorch, TensorFlow, and JAX solve this challenge through <strong>automatic differentiation</strong> (autodiff), a technique that mechanically computes exact derivatives of computer programs. Rather than requiring developers to manually derive and implement gradient formulas for every possible combination of operations, autodiff systems automatically track computations during the forward pass and then systematically apply the chain rule during the backward pass to compute all necessary gradients.</p>\n<p>The framework we will build implements <strong>reverse-mode automatic differentiation</strong>, the same approach used by PyTorch and TensorFlow&#39;s eager mode. This technique constructs a computational graph during forward execution, then traverses this graph in reverse topological order to compute gradients efficiently. Understanding how to build such a system from the ground up provides deep insights into how modern neural network training actually works under the hood.</p>\n<h3 id=\"the-recipe-book-analogy\">The Recipe Book Analogy</h3>\n<p>To understand why automatic differentiation is necessary and how it works, consider neural network training as following a complex recipe book where we need to track how changing any ingredient affects the final dish&#39;s quality.</p>\n<p>Imagine you&#39;re a chef developing a new recipe by combining multiple sub-recipes. Your main dish requires making a sauce (which itself requires combining spices, oils, and acids), preparing vegetables (involving chopping, seasoning, and cooking), and combining everything with a protein. Each sub-recipe transforms its ingredients through specific operations—heating, mixing, seasoning—and passes the result to the next step.</p>\n<p>In this analogy, <strong>tensors are ingredients</strong>, <strong>operations are cooking techniques</strong>, and the <strong>computation graph is your recipe dependency chart</strong> showing which ingredients flow into which preparation steps. During cooking (forward pass), you execute each step and note exactly which ingredients went into which operations and in what order. This creates a detailed record of your cooking process—the computational graph.</p>\n<p>Now suppose your final dish tastes slightly too salty, and you want to know exactly how much less salt to use in each sub-recipe to achieve the perfect flavor balance. This is analogous to having a loss function that measures how far your neural network&#39;s predictions are from the target, and wanting to know how to adjust each parameter (weight and bias) to reduce that loss.</p>\n<p>To solve this, you need to trace backwards through your recipe: the saltiness came from the final combination step, which got contributions from the sauce, the vegetables, and the protein seasoning. The sauce&#39;s saltiness came from the spice blend and the acid reduction. Each step in reverse tells you how much changing that ingredient would affect the final taste, and you can use the chain rule to combine these effects: if reducing the sauce salt by X affects the final dish by Y, and the sauce salt comes from the spice blend with ratio Z, then reducing the spice blend affects the final dish by Y×Z.</p>\n<p>This backwards tracing through the recipe dependency chart, applying the chain rule at each step, is exactly how <strong>reverse-mode automatic differentiation</strong> works. The computation graph records the &quot;recipe&quot; of your neural network&#39;s computation, and backpropagation systematically traces backwards through this graph, computing how much each parameter (ingredient) contributes to the final loss (dish quality).</p>\n<p>The key insight is that you don&#39;t need to manually figure out all possible ways ingredients could affect the final dish—you just need to record what actually happened during this particular cooking session (forward pass), then systematically trace backwards through those recorded steps. The automatic differentiation system handles this bookkeeping automatically, ensuring that no matter how complex your &quot;recipe&quot; (neural network architecture) becomes, gradients are computed correctly and efficiently.</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fcomputation-graph.svg\" alt=\"Computation Graph Structure\"></p>\n<h3 id=\"the-gradient-computation-challenge\">The Gradient Computation Challenge</h3>\n<p>Manual gradient computation becomes intractable for neural networks due to the <strong>exponential explosion of partial derivatives</strong> required as network complexity grows. To understand why automatic differentiation is essential, consider the mathematical challenges faced when implementing backpropagation manually.</p>\n<p>For a simple two-layer neural network with input <code>x</code>, hidden layer weights <code>W1</code>, hidden layer biases <code>b1</code>, activation function <code>σ</code>, output weights <code>W2</code>, output biases <code>b2</code>, and loss function <code>L</code>, the forward pass computes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>h = σ(W1 @ x + b1)\ny = W2 @ h + b2  \nloss = L(y, target)</code></pre></div>\n\n<p>Computing gradients manually requires applying the chain rule to find how <code>loss</code> changes with respect to each parameter. For <code>W2</code>, this involves ∂L/∂W2 = ∂L/∂y × ∂y/∂W2. For <code>W1</code>, the chain becomes longer: ∂L/∂W1 = ∂L/∂y × ∂y/∂h × ∂h/∂W1, requiring intermediate gradient computations and careful tracking of tensor shapes for matrix derivatives.</p>\n<p>As networks grow deeper and more complex, several challenges compound this difficulty:</p>\n<p><strong>Computational Complexity Explosion</strong>: A network with L layers requires computing L sets of gradients, each depending on gradients from subsequent layers. For ResNet-style skip connections, gradients flow through multiple paths, requiring careful accumulation. Modern architectures like Transformers with attention mechanisms involve complex tensor operations (scaled dot-product attention, layer normalization) where manual derivative computation for each operation becomes extremely error-prone.</p>\n<p><strong>Shape Management Complexity</strong>: Tensor operations involve broadcasting, reshaping, and dimension manipulation that affect gradient computation. Computing gradients for broadcasted operations requires &quot;unbroadcasting&quot; gradients back to original shapes. Matrix operations require transposition and careful axis management. Batch dimensions add another layer of complexity where gradients must be summed across batch elements.</p>\n<p><strong>Dynamic Computation Graphs</strong>: Modern neural networks often involve conditional execution, loops, and dynamic shapes that change based on input data. Recurrent networks process variable-length sequences, requiring gradient computation through dynamic unrolling. Attention mechanisms involve dynamic masking and variable sequence lengths. Manual gradient computation cannot handle these dynamic patterns systematically.</p>\n<p><strong>Numerical Stability Issues</strong>: Manual implementations often suffer from numerical precision problems. Gradient clipping, proper initialization, and handling of edge cases (like log(0) in cross-entropy loss) require careful implementation. Automatic differentiation systems can implement these safeguards systematically across all operations.</p>\n<p><strong>Error-Prone Implementation</strong>: Manual gradient computation requires implementing derivatives for every operation, ensuring consistency between forward and backward passes, and maintaining this consistency as code evolves. A single error in any gradient formula can cause subtle training failures that are difficult to debug.</p>\n<p><strong>Composition Complexity</strong>: Neural networks compose operations in arbitrary ways—convolutions followed by batch normalization, then activation functions, then attention mechanisms. Manual implementation requires deriving gradient formulas for every possible composition, an exponentially growing challenge.</p>\n<p>Consider the gradient computation for a single attention head in a Transformer:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Q, K, V = input @ WQ, input @ WK, input @ WV\nscores = Q @ K.T / sqrt(d_k)  \nattn_weights = softmax(scores, mask=mask)\noutput = attn_weights @ V</code></pre></div>\n\n<p>Computing ∂loss/∂WQ manually requires tracing through matrix multiplications, softmax with masking, another matrix multiplication, and handling variable sequence lengths—a derivation that spans multiple pages of mathematical notation and is extremely error-prone to implement.</p>\n<p><strong>The Manual Approach Breakdown</strong>: To illustrate why manual approaches fail, consider implementing a simple three-layer network manually. You must:</p>\n<ol>\n<li>Derive gradient formulas for each layer&#39;s weights and biases</li>\n<li>Implement forward pass storing all intermediate activations  </li>\n<li>Implement backward pass applying chain rule in reverse order</li>\n<li>Handle broadcasting and shape changes correctly at each step</li>\n<li>Ensure numerical stability and proper gradient accumulation</li>\n<li>Debug gradient computation by comparing with numerical differentiation</li>\n<li>Maintain consistency as you modify the network architecture</li>\n</ol>\n<p>This process must be repeated for every new layer type, activation function, or architectural change. The implementation becomes a maintenance nightmare that prevents experimentation and innovation.</p>\n<p><strong>Automatic Differentiation as Solution</strong>: Automatic differentiation transforms this intractable manual process into a systematic, mechanical procedure. Instead of deriving gradients by hand, you implement each operation&#39;s forward pass and its corresponding gradient computation rule once. The autodiff system automatically composes these rules using the chain rule, handles shape management, and ensures correctness regardless of how operations are combined.</p>\n<p>This transformation enables the rapid experimentation and complex architectures that drive modern deep learning research. Without automatic differentiation, the field would be limited to simple, manually tractable network architectures, preventing the development of transformative models like ResNets, Transformers, and modern generative models.</p>\n<h3 id=\"existing-framework-comparison\">Existing Framework Comparison</h3>\n<p>Understanding how established frameworks approach automatic differentiation reveals different design philosophies and trade-offs that inform our implementation choices. Each major framework—PyTorch, TensorFlow, and JAX—represents a distinct approach to solving the gradient computation challenge.</p>\n<p><strong>PyTorch: Dynamic Computation Graphs with Eager Execution</strong></p>\n<p>PyTorch pioneered the <strong>define-by-run</strong> paradigm where computation graphs are built dynamically during forward execution. Each tensor operation creates nodes in the computation graph immediately, enabling flexible control flow and debugging.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Implementation</th>\n<th>Benefits</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Graph Construction</td>\n<td>Built during forward pass execution</td>\n<td>Natural Python control flow, easy debugging</td>\n<td>Runtime overhead, memory growth</td>\n</tr>\n<tr>\n<td>Execution Model</td>\n<td>Eager evaluation of operations</td>\n<td>Immediate results, interactive development</td>\n<td>Cannot optimize across operations</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>Reverse-mode autodiff with <code>autograd</code></td>\n<td>Efficient for ML workloads, handles dynamic shapes</td>\n<td>Requires keeping computation history</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Reference counting with cycle detection</td>\n<td>Automatic cleanup, predictable behavior</td>\n<td>Memory peaks during backward pass</td>\n</tr>\n<tr>\n<td>API Design</td>\n<td>Object-oriented with operator overloading</td>\n<td>Intuitive syntax, familiar to NumPy users</td>\n<td>Harder to extend with new operations</td>\n</tr>\n</tbody></table>\n<p>PyTorch&#39;s design prioritizes <strong>developer experience and flexibility</strong>. Researchers can implement complex, dynamic architectures with conditional execution and variable-length sequences naturally. The immediate feedback from eager execution makes debugging straightforward—you can inspect tensor values at any point during computation.</p>\n<p>However, this flexibility comes with performance costs. Each operation incurs Python overhead, and the dynamic graph prevents compile-time optimizations. Memory usage can be high since the entire computation history must be retained for gradient computation.</p>\n<p><strong>TensorFlow: Static Graphs with Compilation (TF 1.x) + Eager Mode (TF 2.x)</strong></p>\n<p>TensorFlow originally used <strong>define-then-run</strong> static computation graphs, later adding eager execution to compete with PyTorch&#39;s usability.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Static Graphs (TF 1.x)</th>\n<th>Eager Mode (TF 2.x)</th>\n<th>Design Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Graph Construction</td>\n<td>Explicit graph definition phase</td>\n<td>Dynamic like PyTorch</td>\n<td>Hybrid approach complexity</td>\n</tr>\n<tr>\n<td>Execution Model</td>\n<td>Compiled graph execution</td>\n<td>Eager evaluation option</td>\n<td>Performance vs usability trade-off</td>\n</tr>\n<tr>\n<td>Optimization</td>\n<td>Aggressive graph-level optimizations</td>\n<td>Limited optimization scope</td>\n<td>Best performance requires graph mode</td>\n</tr>\n<tr>\n<td>Debugging</td>\n<td>Difficult, requires sessions</td>\n<td>Natural Python debugging</td>\n<td>Two different mental models</td>\n</tr>\n<tr>\n<td>Control Flow</td>\n<td>Special control flow operations</td>\n<td>Native Python control flow</td>\n<td>API inconsistency between modes</td>\n</tr>\n</tbody></table>\n<p>TensorFlow&#39;s evolution reflects the tension between <strong>performance and usability</strong>. Static graphs enable powerful optimizations—operation fusion, memory layout optimization, and cross-device scheduling—but at the cost of programming complexity. TensorFlow 2.x attempts to provide both through <code>@tf.function</code> decorators that trace eager code into static graphs, but this hybrid approach introduces subtle complexities around when tracing occurs and how Python semantics translate to graph operations.</p>\n<p>The XLA (Accelerated Linear Algebra) compiler in TensorFlow demonstrates static graphs&#39; optimization potential, achieving significant speedups through operation fusion and memory optimization that dynamic approaches cannot match.</p>\n<p><strong>JAX: Functional Programming with Composable Transformations</strong></p>\n<p>JAX takes a radically different approach, treating automatic differentiation as one of several <strong>composable program transformations</strong> applied to pure functions.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>JAX Approach</th>\n<th>Unique Benefits</th>\n<th>Limitations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Programming Model</td>\n<td>Pure functional, no mutable state</td>\n<td>Mathematical clarity, composability</td>\n<td>Requires functional thinking</td>\n</tr>\n<tr>\n<td>AD Implementation</td>\n<td>Multiple transformation modes</td>\n<td>Forward-mode, reverse-mode, mixed</td>\n<td>More complex implementation</td>\n</tr>\n<tr>\n<td>Transformations</td>\n<td>Composable: <code>grad</code>, <code>jit</code>, <code>vmap</code>, <code>pmap</code></td>\n<td>Powerful abstractions, research flexibility</td>\n<td>Steep learning curve</td>\n</tr>\n<tr>\n<td>Graph Representation</td>\n<td>Traced JaxPR intermediate representation</td>\n<td>Clean separation of concerns</td>\n<td>Less transparent than PyTorch</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>XLA compilation for all operations</td>\n<td>Consistent high performance</td>\n<td>Compilation overhead</td>\n</tr>\n</tbody></table>\n<p>JAX&#39;s <code>grad</code> transformation converts a function <code>f: a -&gt; b</code> into its gradient function <code>grad(f): a -&gt; a</code>, which can be composed with other transformations like <code>jit</code> (compilation), <code>vmap</code> (vectorization), and <code>pmap</code> (parallelization). This composability enables expressing complex training patterns concisely:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># JAX: compose transformations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">batched_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vmap(grad(loss_fn), </span><span style=\"color:#FFAB70\">in_axes</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">compiled_update </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> jit(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> params, batch: sgd_update(params, batched_grad(params, batch.x, batch.y)))</span></span></code></pre></div>\n\n<p>The functional approach eliminates many sources of bugs common in stateful frameworks—no accidental mutation, no hidden state, clear data dependencies. However, it requires rethinking neural network implementation patterns and has a steeper learning curve for developers accustomed to object-oriented frameworks.</p>\n<p><strong>Framework Comparison Analysis</strong></p>\n<blockquote>\n<p><strong>Decision: Framework Design Philosophy for Our Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to choose which framework&#39;s approach best serves the educational goals of understanding automatic differentiation fundamentals</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>PyTorch-style dynamic graphs with eager execution</li>\n<li>TensorFlow-style static graph compilation  </li>\n<li>JAX-style functional transformations</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: PyTorch-style dynamic computation graphs</li>\n<li><strong>Rationale</strong>: Dynamic graphs make the computation graph construction and traversal explicit and observable, which is essential for learning how autodiff works. Students can inspect the graph structure, understand the backward pass step-by-step, and debug gradient computation interactively. The immediate feedback from eager execution helps build intuition.</li>\n<li><strong>Consequences</strong>: We sacrifice some performance optimizations possible with static graphs, but gain clarity and debuggability that serves the educational mission. The implementation will be more straightforward and the resulting framework more approachable for experimentation.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Design Dimension</th>\n<th>PyTorch</th>\n<th>TensorFlow</th>\n<th>JAX</th>\n<th>Our Choice</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Learning Curve</td>\n<td>Gentle, familiar to NumPy users</td>\n<td>Moderate, two modes to learn</td>\n<td>Steep, requires functional thinking</td>\n<td>Gentle ✓</td>\n</tr>\n<tr>\n<td>Graph Transparency</td>\n<td>High, easy to inspect</td>\n<td>Low in static mode, medium in eager</td>\n<td>Low, abstract transformations</td>\n<td>High ✓</td>\n</tr>\n<tr>\n<td>Implementation Complexity</td>\n<td>Medium, object-oriented</td>\n<td>High, hybrid system</td>\n<td>High, transformation system</td>\n<td>Medium ✓</td>\n</tr>\n<tr>\n<td>Debugging Experience</td>\n<td>Excellent, natural Python</td>\n<td>Poor in static, good in eager</td>\n<td>Good but requires functional debugging</td>\n<td>Excellent ✓</td>\n</tr>\n<tr>\n<td>Educational Value</td>\n<td>High, concepts map directly</td>\n<td>Medium, abstractions hide details</td>\n<td>High but advanced</td>\n<td>High ✓</td>\n</tr>\n</tbody></table>\n<p><strong>Key Insights from Framework Analysis</strong></p>\n<p>The framework comparison reveals several critical insights that will guide our implementation:</p>\n<p><strong>Eager Execution Aids Learning</strong>: PyTorch&#39;s success in research and education demonstrates that immediate feedback and transparent execution help developers understand what&#39;s happening. Our framework should prioritize clarity over optimization.</p>\n<p><strong>Graph Construction Strategy Matters</strong>: How and when the computation graph is built fundamentally affects the programming model. Dynamic construction aligns with natural Python control flow and makes the autodiff process observable.</p>\n<p><strong>API Design Shapes User Experience</strong>: PyTorch&#39;s operator overloading creates an intuitive interface where <code>a + b</code> automatically creates computation graph nodes. This removes boilerplate and lets users focus on model logic rather than framework mechanics.</p>\n<p><strong>Memory Management Complexity</strong>: All frameworks struggle with memory management during gradient computation. Our implementation must carefully consider when to release intermediate values and how to handle gradient accumulation.</p>\n<p><strong>The Abstraction Level Trade-off</strong>: More abstraction (like JAX transformations) can enable powerful patterns but may hide the underlying mechanics we want students to understand. Our framework should expose the computation graph and backward pass explicitly.</p>\n<p>These insights inform our design principles: prioritize transparency and learnability, use dynamic computation graphs with eager execution, provide an intuitive tensor API with operator overloading, and make the autodiff process observable and debuggable. While this may sacrifice some performance optimizations, it serves the educational goal of deeply understanding how automatic differentiation enables modern neural network training.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Understanding how existing frameworks solve the gradient computation problem provides the foundation for implementing our own neural network framework. This section bridges the conceptual understanding developed above with practical implementation choices and starter code.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tensor Storage</td>\n<td>Pure NumPy arrays with Python lists for metadata</td>\n<td>NumPy arrays with custom memory pools</td>\n</tr>\n<tr>\n<td>Graph Representation</td>\n<td>Python objects with references</td>\n<td>Optimized node structures with arena allocation</td>\n</tr>\n<tr>\n<td>Operation Implementation</td>\n<td>Individual Python classes per operation</td>\n<td>Single dispatch or visitor pattern</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>Recursive traversal with Python call stack</td>\n<td>Iterative traversal with explicit stack</td>\n</tr>\n<tr>\n<td>Broadcasting</td>\n<td>NumPy&#39;s broadcasting with wrapper functions</td>\n<td>Custom broadcasting implementation</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Built-in unittest with numerical gradient checking</td>\n<td>pytest with property-based testing</td>\n</tr>\n</tbody></table>\n<p>For learning purposes, we recommend starting with the simple options to understand the core concepts, then optionally exploring advanced optimizations once the basic system works.</p>\n<p><strong>B. Recommended Project Structure</strong></p>\n<p>Organize your neural network framework with clear separation of concerns that mirrors the conceptual architecture:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n├── core/\n│   ├── __init__.py\n│   ├── tensor.py              ← Tensor class with operations\n│   ├── autograd.py            ← Automatic differentiation engine  \n│   └── operations.py          ← Operation classes (Add, Multiply, etc.)\n├── nn/\n│   ├── __init__.py\n│   ├── module.py              ← Module base class and parameter management\n│   ├── layers.py              ← Linear, activation layers\n│   └── loss.py                ← Loss functions\n├── optim/\n│   ├── __init__.py\n│   ├── optimizer.py           ← Optimizer base class\n│   └── sgd.py, adam.py        ← Specific optimizers\n├── utils/\n│   ├── __init__.py\n│   └── testing.py             ← Gradient checking utilities\n├── examples/\n│   ├── simple_regression.py   ← End-to-end training example\n│   └── mnist_classifier.py    ← More complex example\n└── tests/\n    ├── test_tensor.py\n    ├── test_autograd.py\n    └── test_training.py</code></pre></div>\n\n<p>This structure separates the core autodiff functionality from the higher-level neural network abstractions, making it easier to understand and test each component independently.</p>\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here&#39;s complete, working infrastructure code that handles the non-core-learning components, allowing you to focus on the automatic differentiation logic:</p>\n<p><strong>Gradient Checking Utility (<code>utils/testing.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> numerical_gradient</span><span style=\"color:#E1E4E8\">(f: Callable[[Tensor], Tensor], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      inputs: List[Tensor], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> List[np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute numerical gradients using finite differences.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Used to verify automatic differentiation correctness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numerical_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> input_tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros_like(input_tensor.data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Flatten for easier iteration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flat_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> input_tensor.data.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flat_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(flat_input)):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute f(x + h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_input[i] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_plus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(input_tensor).data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute f(x - h)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_input[i] </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_minus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(input_tensor).data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Restore original value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_input[i] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute numerical gradient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_grad[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sum((f_plus </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> f_minus) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> h))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        numerical_grads.append(flat_grad.reshape(input_tensor.shape))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> numerical_grads</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(f: Callable[[Tensor], Tensor],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   inputs: List[Tensor],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compare automatic gradients with numerical gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns True if they match within tolerance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute automatic gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inp.requires_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inp.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auto_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [inp.grad.data </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute numerical gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numerical_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> numerical_gradient(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">args: f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args), inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compare</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> auto_grad, num_grad </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> zip</span><span style=\"color:#E1E4E8\">(auto_grads, numerical_grads):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.allclose(auto_grad, num_grad, </span><span style=\"color:#FFAB70\">atol</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tolerance):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient mismatch!\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Automatic: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">auto_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Numerical: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">num_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Difference: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">np.abs(auto_grad </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> num_grad)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>Broadcasting Utilities (<code>core/broadcasting.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> broadcast_shapes</span><span style=\"color:#E1E4E8\">(shape1: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], shape2: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute the broadcasted shape following NumPy broadcasting rules.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> np.broadcast_shapes(shape1, shape2)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> unbroadcast_gradient</span><span style=\"color:#E1E4E8\">(grad: np.ndarray, original_shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Reduce gradient from broadcasted shape back to original tensor shape.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is crucial for gradient computation in broadcasted operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum out added dimensions (from left)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ndims_added </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(grad.shape) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(original_shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(ndims_added):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.sum(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum over broadcasted dimensions  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (grad_dim, orig_dim) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(grad.shape, original_shape)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> orig_dim </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> grad_dim </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.sum(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">i, </span><span style=\"color:#FFAB70\">keepdims</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> grad.reshape(original_shape)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> can_broadcast</span><span style=\"color:#E1E4E8\">(shape1: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], shape2: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Check if two shapes can be broadcasted together.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        broadcast_shapes(shape1, shape2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>Here are the essential class signatures and method skeletons that you need to implement, with detailed TODOs mapping to the concepts discussed above:</p>\n<p><strong>Tensor Class Skeleton (<code>core/tensor.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, Union, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tensor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    N-dimensional array with automatic differentiation support.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Similar to PyTorch tensors but simplified for learning.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 data: Union[np.ndarray, List, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 grad_fn: Optional[</span><span style=\"color:#9ECBFF\">'Operation'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize tensor with data and gradient tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: The actual numerical data (converted to numpy array)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            requires_grad: Whether to compute gradients for this tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            grad_fn: The operation that created this tensor (for autodiff)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert data to numpy array and store in self.data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store shape, dtype from the numpy array  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set requires_grad flag and initialize grad to None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store grad_fn for backward pass linkage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.asarray() to handle different input types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, gradient: Optional[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initiate backpropagation from this tensor.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This is the entry point that triggers gradient computation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If gradient not provided, create tensor of ones with same shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call _backward() to start recursive gradient computation  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle the case where this tensor doesn't require gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: gradient=None means this is the loss tensor (scalar)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _backward</span><span style=\"color:#E1E4E8\">(self, gradient: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Internal method for recursive gradient computation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This implements the core autodiff logic.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If this tensor requires gradients, accumulate gradient</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If this tensor has grad_fn, call its backward method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle gradient accumulation (+=, not =) for shared tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check if self.grad is None before accumulating</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __add__</span><span style=\"color:#E1E4E8\">(self, other: Union[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Tensor addition with gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert other to Tensor if it's a scalar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Add operation and apply it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return result tensor with proper grad_fn</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Import Add operation from operations.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __mul__</span><span style=\"color:#E1E4E8\">(self, other: Union[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Element-wise multiplication with gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Similar to __add__ but use Multiply operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> matmul</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Matrix multiplication with gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create MatMul operation and apply it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This will be more complex due to shape handling</span></span></code></pre></div>\n\n<p><strong>Operation Base Class (<code>core/operations.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Base class for all operations that can compute gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Each operation knows how to compute its forward pass and backward pass.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.inputs: Tuple[Tensor, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __call__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Apply this operation to input tensors.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store inputs for backward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call forward() to compute result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create result tensor with this operation as grad_fn</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set requires_grad=True if any input requires gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: Tensor) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute the forward pass. Return raw numpy array.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: Tensor) -> Tuple[Tensor, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute gradients with respect to inputs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            grad_output: Gradient flowing back from subsequent operations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of gradients for each input tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Element-wise addition operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, a: Tensor, b: Tensor) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add the data arrays using numpy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle broadcasting automatically via numpy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Just return a.data + b.data, numpy handles broadcasting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: Tensor) -> Tuple[Tensor, Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Gradient of addition: ∂(a+b)/∂a = 1, ∂(a+b)/∂b = 1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        But we must handle broadcasting by unbroadcasting gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Gradient w.r.t. first input is grad_output  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Gradient w.r.t. second input is also grad_output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Unbroadcast gradients to match original input shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return tuple of gradient tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use unbroadcast_gradient() from broadcasting.py</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<p><strong>NumPy Broadcasting</strong>: Use <code>np.broadcast_arrays(a, b)</code> to see how arrays will be broadcasted, but let operations happen naturally - NumPy handles broadcasting automatically in arithmetic operations.</p>\n<p><strong>Memory Management</strong>: Python&#39;s garbage collector handles most cleanup, but be aware that computation graphs can create reference cycles. Consider implementing <code>zero_grad()</code> methods that explicitly clear gradients to free memory.</p>\n<p><strong>Shape Debugging</strong>: Use <code>tensor.shape</code> frequently and add shape assertions. Many bugs come from shape mismatches that numpy silently broadcasts in unexpected ways.</p>\n<p><strong>Gradient Accumulation</strong>: Always use <code>+=</code> when accumulating gradients, never <code>=</code>. Shared tensors (used multiple times) need gradients summed from all uses.</p>\n<p><strong>Testing Strategy</strong>: Start with simple operations (addition, multiplication) and verify gradients using numerical differentiation before moving to complex operations like matrix multiplication.</p>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the basic tensor and autodiff system:</p>\n<p><strong>Test Command</strong>: <code>python -c &quot;from utils.testing import check_gradients; from core.tensor import Tensor; import numpy as np; a = Tensor([1.0, 2.0], requires_grad=True); b = Tensor([3.0, 4.0], requires_grad=True); print(check_gradients(lambda x, y: x + y, [a, b]))&quot;</code></p>\n<p><strong>Expected Output</strong>: <code>True</code> (indicating gradients match numerical computation)</p>\n<p><strong>Manual Verification</strong>:</p>\n<ol>\n<li>Create two tensors with <code>requires_grad=True</code></li>\n<li>Perform addition: <code>c = a + b</code>  </li>\n<li>Call <code>c.backward()</code></li>\n<li>Check that <code>a.grad</code> and <code>b.grad</code> both contain arrays of ones</li>\n<li>Verify the computation graph exists: <code>c.grad_fn</code> should be an <code>Add</code> operation</li>\n</ol>\n<p><strong>Signs of Problems</strong>:</p>\n<ul>\n<li><code>AttributeError</code> on <code>backward()</code>: Tensor class not properly initialized</li>\n<li>Gradient is <code>None</code>: Either <code>requires_grad=False</code> or backward pass not implemented</li>\n<li>Shape mismatch errors: Broadcasting not handled correctly in gradient computation</li>\n<li>Gradients don&#39;t match numerical: Error in backward pass implementation</li>\n</ul>\n<p>This checkpoint ensures your foundational tensor and autodiff system works before building neural network layers on top.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - establishes clear scope and learning objectives for the entire project</p>\n</blockquote>\n<p>Building a neural network framework from scratch is an ambitious undertaking that could easily spiral into a years-long project if we attempt to match the full feature set of production frameworks like PyTorch or TensorFlow. To ensure this remains an effective learning experience, we must carefully define what we will and won&#39;t implement. Think of this like planning a cross-country road trip - without clear destinations and route boundaries, you&#39;ll find yourself lost in fascinating detours that prevent you from reaching your core learning objectives.</p>\n<p>This section establishes our framework&#39;s scope by defining three critical boundaries: the functional requirements that form our core learning goals, the performance and quality standards that prioritize educational value over production optimization, and the explicit non-goals that we&#39;ll consciously exclude to maintain focus. These boundaries aren&#39;t limitations - they&#39;re strategic choices that ensure we build deep understanding of automatic differentiation, tensor operations, and neural network fundamentals rather than getting lost in the peripheral complexity of production systems.</p>\n<h3 id=\"functional-requirements\">Functional Requirements</h3>\n<p>Our neural network framework must implement a carefully curated set of core features that demonstrate the fundamental principles of automatic differentiation and neural network training. These requirements represent the minimum viable functionality needed to train real neural networks while keeping the implementation scope manageable for a learning project.</p>\n<p>The functional requirements are organized around four key capabilities that mirror the milestone structure: tensor operations that provide the computational foundation, automatic differentiation that enables gradient-based learning, neural network modules that offer composable building blocks, and optimization algorithms that drive the training process.</p>\n<table>\n<thead>\n<tr>\n<th>Requirement Category</th>\n<th>Must Support</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tensor Operations</strong></td>\n<td>N-dimensional arrays with shape tracking</td>\n<td>Create tensors with arbitrary dimensions, track shape and dtype correctly</td>\n</tr>\n<tr>\n<td></td>\n<td>Element-wise arithmetic</td>\n<td>Add, subtract, multiply, divide tensors with broadcasting</td>\n</tr>\n<tr>\n<td></td>\n<td>Matrix multiplication</td>\n<td>2D matrix multiplication and batched operations</td>\n</tr>\n<tr>\n<td></td>\n<td>Broadcasting</td>\n<td>NumPy-compatible shape expansion for mismatched tensors</td>\n</tr>\n<tr>\n<td></td>\n<td>Gradient tracking</td>\n<td><code>requires_grad</code> flag enables/disables gradient computation</td>\n</tr>\n<tr>\n<td><strong>Automatic Differentiation</strong></td>\n<td>Computation graph construction</td>\n<td>Operations automatically build DAG during forward pass</td>\n</tr>\n<tr>\n<td></td>\n<td>Reverse-mode backpropagation</td>\n<td>Chain rule application through topological sort</td>\n</tr>\n<tr>\n<td></td>\n<td>Gradient accumulation</td>\n<td>Multiple uses of same tensor accumulate gradients correctly</td>\n</tr>\n<tr>\n<td></td>\n<td>Gradient computation</td>\n<td>Matches numerical differentiation within tolerance</td>\n</tr>\n<tr>\n<td><strong>Neural Network Modules</strong></td>\n<td>Linear layers</td>\n<td>Fully connected layers with weight and bias parameters</td>\n</tr>\n<tr>\n<td></td>\n<td>Activation functions</td>\n<td>ReLU, sigmoid, tanh with correct gradients</td>\n</tr>\n<tr>\n<td></td>\n<td>Parameter management</td>\n<td>Automatic registration and collection of trainable parameters</td>\n</tr>\n<tr>\n<td></td>\n<td>Module composition</td>\n<td>Sequential and nested module support</td>\n</tr>\n<tr>\n<td><strong>Optimization</strong></td>\n<td>SGD optimizer</td>\n<td>Parameter updates with learning rate and momentum</td>\n</tr>\n<tr>\n<td></td>\n<td>Adam optimizer</td>\n<td>Adaptive learning rates with bias correction</td>\n</tr>\n<tr>\n<td></td>\n<td>Loss functions</td>\n<td>Cross-entropy and mean squared error</td>\n</tr>\n<tr>\n<td></td>\n<td>Training loop</td>\n<td>Mini-batch processing with forward/backward/update cycle</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Functional Scope</strong></p>\n<ul>\n<li><strong>Context</strong>: Neural networks require dozens of operations and layers for production use, but implementing all would obscure core learning objectives</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Full PyTorch API compatibility (200+ operations)</li>\n<li>Minimal subset for basic networks (10-15 operations)</li>\n<li>Core operations plus extensible architecture (20-30 operations)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Core operations plus extensible architecture</li>\n<li><strong>Rationale</strong>: Provides sufficient functionality to train meaningful models while demonstrating all key automatic differentiation principles. Extensible design allows learners to add operations after mastering fundamentals</li>\n<li><strong>Consequences</strong>: Can train multi-layer perceptrons and simple convolutional networks, but lacks advanced operations like attention or specialized layers</li>\n</ul>\n</blockquote>\n<p>The tensor operations form the computational foundation of our framework. Every tensor must track its shape, data type, and whether it requires gradient computation. Element-wise operations like addition and multiplication must support NumPy-compatible broadcasting, automatically expanding tensor dimensions to enable operations between tensors of different shapes. Matrix multiplication must handle both 2D matrices and batched operations where multiple matrix multiplications are performed in parallel across batch dimensions.</p>\n<p>The automatic differentiation engine represents the core intellectual challenge of the project. During the forward pass, each operation must construct nodes in a computation graph that records the relationships between input and output tensors. The backward pass must traverse this graph in reverse topological order, applying the chain rule to compute gradients. When tensors are used multiple times in a computation, their gradients must be accumulated rather than overwritten.</p>\n<p>Neural network modules provide the building blocks for constructing complex models. The <code>Module</code> base class must automatically track parameters in nested hierarchies, allowing optimizers to find and update all trainable weights. Linear layers implement the fundamental <code>y = Wx + b</code> transformation with proper weight initialization. Activation functions apply element-wise nonlinearities while preserving gradient flow through the network.</p>\n<p>The optimization system coordinates the training process by implementing gradient-based parameter updates. SGD must support momentum for accelerated convergence, while Adam provides adaptive learning rates with bias correction for first and second moment estimates. Loss functions must compute both the loss value and provide gradients that initiate the backward pass.</p>\n<h3 id=\"performance-and-quality-goals\">Performance and Quality Goals</h3>\n<p>Our framework prioritizes educational clarity and correctness over production-level performance optimizations. This represents a fundamental trade-off that shapes every design decision - we choose implementations that are easy to understand, debug, and extend rather than those that maximize computational efficiency.</p>\n<p>Think of this like the difference between a driving instructor&#39;s car and a Formula 1 race car. The instructor&#39;s car has clear visibility, simple controls, and forgiving handling characteristics that help students learn fundamental driving skills. The race car optimizes for maximum speed with complex controls that would overwhelm a learning driver. Our framework is the instructor&#39;s car - designed to teach automatic differentiation principles clearly rather than achieve maximum throughput.</p>\n<table>\n<thead>\n<tr>\n<th>Quality Dimension</th>\n<th>Goal</th>\n<th>Rationale</th>\n<th>Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Educational Clarity</strong></td>\n<td>Code readability over performance</td>\n<td>Learning objective is understanding autodiff</td>\n<td>Can explain algorithm by reading code</td>\n</tr>\n<tr>\n<td><strong>Correctness</strong></td>\n<td>Gradients match numerical differentiation</td>\n<td>Foundation for all neural network learning</td>\n<td>Gradient checker passes with 1e-6 tolerance</td>\n</tr>\n<tr>\n<td><strong>Debuggability</strong></td>\n<td>Clear error messages and inspection tools</td>\n<td>Students need to diagnose their mistakes</td>\n<td>Shape mismatches include tensor dimensions</td>\n</tr>\n<tr>\n<td><strong>Extensibility</strong></td>\n<td>Easy to add new operations</td>\n<td>Encourages experimentation beyond core features</td>\n<td>New operation requires &lt;50 lines of code</td>\n</tr>\n<tr>\n<td><strong>Simplicity</strong></td>\n<td>Minimize dependencies and abstraction layers</td>\n<td>Reduces cognitive load during learning</td>\n<td>Core autodiff engine fits in single file</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Performance vs. Clarity Trade-off</strong></p>\n<ul>\n<li><strong>Context</strong>: Production frameworks use complex optimizations (operation fusion, memory pooling, CUDA kernels) that obscure core algorithms</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Performance-first with extensive optimizations</li>\n<li>Clarity-first with readable but slower implementations  </li>\n<li>Hybrid approach with optional optimization layers</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Clarity-first with readable implementations</li>\n<li><strong>Rationale</strong>: Learning automatic differentiation requires understanding the mathematical operations and graph traversal algorithms. Performance optimizations create abstraction layers that hide these fundamentals</li>\n<li><strong>Consequences</strong>: Framework will be 10-100x slower than PyTorch but students can trace every operation step-by-step</li>\n</ul>\n</blockquote>\n<p>Educational clarity means that someone reading the code should be able to understand the mathematical operations being performed without deciphering complex optimization layers. When implementing matrix multiplication, we use clear NumPy operations rather than optimized BLAS routines. When traversing the computation graph, we use explicit loops and condition checks rather than vectorized graph algorithms.</p>\n<p>Correctness takes priority over speed in all situations. Every operation must produce mathematically correct results, and gradients must match numerical differentiation within reasonable tolerance. We implement comprehensive gradient checking utilities that compare automatic differentiation results against finite difference approximations. When there&#39;s a choice between a fast algorithm with edge cases and a slower algorithm that handles all cases correctly, we choose correctness.</p>\n<p>Debuggability means providing rich error messages and inspection tools that help learners diagnose problems. When tensor shapes don&#39;t match for an operation, the error message should include the actual shapes and the operation being attempted. The computation graph should be inspectable so students can visualize the operations being performed and verify that gradients flow correctly.</p>\n<p>Extensibility ensures that students can experiment with new operations and layer types after mastering the core framework. Adding a new operation should require implementing only the forward pass computation and gradient calculation, with the framework handling graph construction and backpropagation automatically. The module system should make it trivial to compose new layer types from existing building blocks.</p>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>To maintain focus on core learning objectives, we explicitly exclude several categories of functionality that would be essential for production use but would distract from understanding automatic differentiation principles. These non-goals aren&#39;t shortcomings - they&#39;re strategic exclusions that keep the project scope manageable and the learning objectives clear.</p>\n<p>Think of these non-goals as the advanced driving techniques we skip in basic driving instruction. Students learning to drive don&#39;t start with parallel parking in tight spaces, highway merging in heavy traffic, or performance driving techniques. They master basic vehicle control, traffic rules, and safety principles first. Similarly, our framework focuses on automatic differentiation fundamentals rather than production deployment challenges.</p>\n<table>\n<thead>\n<tr>\n<th>Non-Goal Category</th>\n<th>Excluded Features</th>\n<th>Rationale for Exclusion</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Production Performance</strong></td>\n<td>CUDA kernels, operation fusion, memory pooling</td>\n<td>Implementation complexity would obscure core algorithms</td>\n</tr>\n<tr>\n<td><strong>Advanced Operations</strong></td>\n<td>Convolutions, attention, batch normalization</td>\n<td>Requires understanding of specialized mathematical algorithms</td>\n</tr>\n<tr>\n<td><strong>Distributed Training</strong></td>\n<td>Multi-GPU, parameter servers, gradient synchronization</td>\n<td>Networking and coordination complexity unrelated to autodiff</td>\n</tr>\n<tr>\n<td><strong>Deployment Optimization</strong></td>\n<td>Quantization, pruning, ONNX export</td>\n<td>Model optimization techniques beyond basic training</td>\n</tr>\n<tr>\n<td><strong>Advanced Optimizers</strong></td>\n<td>AdaGrad, RMSprop, learning rate scheduling</td>\n<td>Additional complexity beyond demonstrating optimization principles</td>\n</tr>\n<tr>\n<td><strong>Data Pipeline</strong></td>\n<td>Data loaders, augmentation, distributed datasets</td>\n<td>Infrastructure concerns separate from neural network computation</td>\n</tr>\n<tr>\n<td><strong>Model Architecture</strong></td>\n<td>Pre-trained models, standard architectures (ResNet, Transformer)</td>\n<td>Focus is on building blocks, not complete model zoo</td>\n</tr>\n<tr>\n<td><strong>Debugging Tools</strong></td>\n<td>Profilers, visualization, TensorBoard integration</td>\n<td>Tooling complexity beyond educational scope</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Advanced Operations Exclusion</strong></p>\n<ul>\n<li><strong>Context</strong>: Modern deep learning relies heavily on convolutions, attention mechanisms, and normalization layers</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Include convolutions as core operation</li>\n<li>Focus on dense layers and element-wise operations only</li>\n<li>Provide convolution as extension example</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Focus on dense layers, provide extension pathway</li>\n<li><strong>Rationale</strong>: Convolutions require understanding im2col transformations, padding modes, and stride calculations that are orthogonal to automatic differentiation principles. Students can add convolutions after mastering basic tensor operations and gradient computation</li>\n<li><strong>Consequences</strong>: Framework can&#39;t train modern computer vision models out-of-box, but students understand autodiff principles that apply to any operation</li>\n</ul>\n</blockquote>\n<p>Production performance optimizations like GPU acceleration and operation fusion would require implementing CUDA kernels or complex CPU vectorization that obscures the underlying mathematical operations. While these optimizations are critical for practical deep learning, they add layers of systems complexity that distract from understanding how gradients propagate through computation graphs.</p>\n<p>Advanced neural network operations like convolutions and attention mechanisms involve sophisticated mathematical algorithms that are interesting topics in their own right. However, implementing these operations requires understanding specialized techniques (like im2col transformations for convolutions or scaled dot-product attention) that are orthogonal to automatic differentiation principles. Students can add these operations after mastering the core framework.</p>\n<p>Distributed training introduces networking protocols, fault tolerance, and gradient synchronization challenges that belong more in a distributed systems course than an automatic differentiation tutorial. While distributed training is essential for large-scale machine learning, the coordination complexity would overwhelm the core learning objectives around computation graphs and gradient computation.</p>\n<p>Deployment optimizations like quantization and model pruning represent post-training techniques for reducing model size and inference cost. These optimizations are valuable for production systems but don&#39;t contribute to understanding how neural networks learn through gradient-based optimization.</p>\n<p>The data pipeline infrastructure required for production machine learning - data loaders, augmentation pipelines, distributed datasets - represents a substantial engineering effort that&#39;s largely separate from neural network computation. Students can use simple NumPy arrays or basic data loading utilities while focusing on the automatic differentiation algorithms.</p>\n<blockquote>\n<p>⚠️ <strong>Pitfall: Scope Creep During Implementation</strong>\nStudents often encounter interesting optimization opportunities or missing features during implementation and want to add them immediately. This leads to projects that become too complex to complete or understand. Resist the temptation to add &quot;just one more feature&quot; - focus on the core learning objectives first. Advanced features can always be added after mastering the fundamentals, but attempting everything at once typically results in completing nothing well.</p>\n</blockquote>\n<p>These explicit non-goals don&#39;t represent permanent limitations - they&#39;re training wheels that help students focus on core concepts. After mastering automatic differentiation with our simplified framework, students will have the foundational knowledge needed to understand how production frameworks implement these advanced features. The goal is deep understanding of fundamental principles rather than broad coverage of peripheral features.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The goals and non-goals defined above translate into specific technical choices and architectural constraints that guide the implementation process. This guidance helps maintain focus on educational objectives while providing concrete direction for technical decisions.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommended for Learning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Numerical Computing</strong></td>\n<td>Pure Python + NumPy</td>\n<td>NumPy + SciPy + BLAS optimization</td>\n<td>Pure Python + NumPy</td>\n</tr>\n<tr>\n<td><strong>Testing Framework</strong></td>\n<td>Built-in unittest</td>\n<td>pytest with fixtures</td>\n<td>pytest (better error messages)</td>\n</tr>\n<tr>\n<td><strong>Gradient Checking</strong></td>\n<td>Simple finite differences</td>\n<td>scipy.optimize.approx_fprime</td>\n<td>Custom implementation</td>\n</tr>\n<tr>\n<td><strong>Graph Visualization</strong></td>\n<td>Print statements + manual inspection</td>\n<td>graphviz + matplotlib</td>\n<td>Manual inspection first</td>\n</tr>\n<tr>\n<td><strong>Documentation</strong></td>\n<td>Docstrings + comments</td>\n<td>Sphinx + readthedocs</td>\n<td>Rich docstrings</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended Project Structure:</strong></p>\n<p>The project organization should reflect the four-layer architecture while keeping related functionality grouped together:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural-framework/\n├── neuralnet/\n│   ├── __init__.py              ← Public API exports\n│   ├── tensor.py                ← Tensor class and basic operations\n│   ├── operations.py            ← Operation subclasses (Add, Multiply, MatMul)\n│   ├── autodiff.py              ← Backward pass and gradient computation\n│   ├── module.py                ← Module base class and parameter management\n│   ├── layers.py                ← Linear layer and activation functions\n│   ├── optimizers.py            ← SGD and Adam implementations\n│   ├── losses.py                ← Loss functions\n│   └── utils.py                 ← Gradient checking and debugging utilities\n├── tests/\n│   ├── test_tensor.py           ← Tensor operation tests\n│   ├── test_autodiff.py         ← Gradient computation tests  \n│   ├── test_modules.py          ← Layer and module tests\n│   ├── test_optimizers.py       ← Optimizer tests\n│   └── test_integration.py      ← End-to-end training tests\n├── examples/\n│   ├── linear_regression.py     ← Simple regression example\n│   ├── mnist_mlp.py            ← Multi-layer perceptron\n│   └── gradient_checking.py     ← Debugging utilities demo\n└── README.md                    ← Project overview and usage</code></pre></div>\n\n<p><strong>C. Core Design Constraints:</strong></p>\n<p>Based on our goals and non-goals, implement with these constraints:</p>\n<table>\n<thead>\n<tr>\n<th>Constraint Category</th>\n<th>Specific Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Dependencies</strong></td>\n<td>NumPy only for numerical computation (no torch, tensorflow, jax)</td>\n</tr>\n<tr>\n<td><strong>Performance</strong></td>\n<td>Prefer readable code over optimization (no Cython, no custom C extensions)</td>\n</tr>\n<tr>\n<td><strong>API Design</strong></td>\n<td>Mirror PyTorch conventions where possible for familiar interface</td>\n</tr>\n<tr>\n<td><strong>Error Handling</strong></td>\n<td>Detailed error messages with tensor shapes and operation context</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td>Every operation must pass gradient check with tolerance=1e-6</td>\n</tr>\n<tr>\n<td><strong>Documentation</strong></td>\n<td>Every public method documented with examples and mathematical notation</td>\n</tr>\n</tbody></table>\n<p><strong>D. Architecture Validation Checklist:</strong></p>\n<p>Use this checklist to ensure implementations stay aligned with educational goals:</p>\n<table>\n<thead>\n<tr>\n<th>Design Decision</th>\n<th>Educational Priority Questions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Adding new operation</strong></td>\n<td>Can student trace through forward and backward pass by hand?</td>\n</tr>\n<tr>\n<td><strong>Choosing algorithm</strong></td>\n<td>Is the mathematical relationship clear from the code?</td>\n</tr>\n<tr>\n<td><strong>Error handling</strong></td>\n<td>Do error messages help student understand what went wrong?</td>\n</tr>\n<tr>\n<td><strong>API design</strong></td>\n<td>Can student predict behavior without reading documentation?</td>\n</tr>\n<tr>\n<td><strong>Performance optimization</strong></td>\n<td>Does optimization obscure the underlying algorithm?</td>\n</tr>\n</tbody></table>\n<p><strong>E. Milestone Validation Strategy:</strong></p>\n<p>After completing each milestone, validate that goals are met:</p>\n<table>\n<thead>\n<tr>\n<th>Milestone</th>\n<th>Validation Approach</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tensor Operations</strong></td>\n<td>Manual calculation verification</td>\n<td>Hand-computed results match tensor operations</td>\n</tr>\n<tr>\n<td><strong>Automatic Differentiation</strong></td>\n<td>Numerical gradient comparison</td>\n<td>gradient_check passes for all operations</td>\n</tr>\n<tr>\n<td><strong>Neural Modules</strong></td>\n<td>Parameter counting and initialization</td>\n<td>Module.parameters() returns expected tensors</td>\n</tr>\n<tr>\n<td><strong>Optimizers</strong></td>\n<td>Simple function minimization</td>\n<td>SGD and Adam converge on quadratic function</td>\n</tr>\n</tbody></table>\n<p><strong>F. Scope Management Guidelines:</strong></p>\n<p>To prevent scope creep during implementation:</p>\n<table>\n<thead>\n<tr>\n<th>Tempting Addition</th>\n<th>Decision Framework</th>\n<th>Recommended Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>&quot;Just one more operation&quot;</strong></td>\n<td>Does it demonstrate new autodiff principle?</td>\n<td>Usually defer to post-completion extension</td>\n</tr>\n<tr>\n<td><strong>&quot;Better error messages&quot;</strong></td>\n<td>Does it help debug common student mistakes?</td>\n<td>Generally worth the investment</td>\n</tr>\n<tr>\n<td><strong>&quot;Performance improvement&quot;</strong></td>\n<td>Does it maintain algorithm clarity?</td>\n<td>Usually violates educational goals</td>\n</tr>\n<tr>\n<td><strong>&quot;Production feature&quot;</strong></td>\n<td>Is it needed for basic neural network training?</td>\n<td>Add to explicit non-goals list</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The key insight for scope management is that every feature addition should be justified by its contribution to understanding automatic differentiation principles, not by its practical utility for production machine learning. This framework succeeds when students understand how gradients flow through computation graphs, not when it achieves competitive benchmark performance.</p>\n</blockquote>\n<p>This implementation guidance provides concrete guardrails for maintaining focus on educational objectives while building a framework that demonstrates all core automatic differentiation concepts. The structure and constraints ensure that complexity stays manageable while covering sufficient functionality to train real neural networks on meaningful problems.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - establishes the overall system design and component relationships</p>\n</blockquote>\n<p>Building a neural network framework is like constructing a sophisticated manufacturing plant where raw materials (data) flow through an assembly line of operations, with each station (layer) performing specific transformations while keeping detailed records of every step for quality control (gradient computation). Just as a modern factory has distinct departments—receiving (tensor operations), production line (computation graph), quality assurance (modules), and management (optimizers)—our framework is organized into four distinct architectural layers that work together seamlessly.</p>\n<p>The genius of this layered architecture lies in its separation of concerns: lower layers handle the mechanical details of computation while higher layers focus on the intelligent orchestration of learning. Each layer builds upon the foundation provided by the layer below, creating a clean abstraction hierarchy that makes the framework both powerful and maintainable.</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"Neural Framework System Architecture\"></p>\n<h3 id=\"four-layer-architecture\">Four-Layer Architecture</h3>\n<p>Our neural network framework follows a <strong>four-layer architectural pattern</strong> that mirrors the natural hierarchy of concepts in deep learning. Think of it as a pyramid where each level provides services to the level above while depending only on services from levels below. This creates clear boundaries and enables each layer to evolve independently without breaking the entire system.</p>\n<p>The <strong>bottom layer</strong> handles <strong>tensor operations</strong>—the fundamental mathematical building blocks. Like the foundation of a building, this layer must be rock-solid because everything else depends on it. Tensors at this level are &quot;smart arrays&quot; that know their shape, track whether they need gradients, and can perform basic arithmetic operations with broadcasting support.</p>\n<p>The <strong>second layer</strong> implements the <strong>automatic differentiation engine</strong>—the computation graph that records operations and computes gradients. This is the nervous system of our framework, automatically tracking every computation during the forward pass and enabling efficient gradient computation during the backward pass. The key insight is that this layer transforms the imperative code users write into a functional computation graph suitable for differentiation.</p>\n<p>The <strong>third layer</strong> provides the <strong>neural network module system</strong>—composable building blocks like linear layers and activation functions. This layer is where the framework becomes user-friendly, providing the LEGO-like components that users combine to build complex architectures. Modules automatically register their parameters and handle the forward pass, abstracting away the low-level tensor operations.</p>\n<p>The <strong>top layer</strong> contains <strong>optimizers and training infrastructure</strong>—the algorithms that actually learn by updating parameters based on computed gradients. This layer orchestrates the entire training process, coordinating forward passes, loss computation, backward passes, and parameter updates in the correct sequence.</p>\n<table>\n<thead>\n<tr>\n<th>Layer</th>\n<th>Primary Responsibility</th>\n<th>Key Components</th>\n<th>Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tensor Operations</td>\n<td>Mathematical primitives and shape management</td>\n<td><code>Tensor</code>, arithmetic operators, broadcasting</td>\n<td>NumPy arrays, memory management</td>\n</tr>\n<tr>\n<td>Autodiff Engine</td>\n<td>Computation graph construction and gradient computation</td>\n<td><code>Operation</code> nodes, topological sort, chain rule</td>\n<td>Tensor layer</td>\n</tr>\n<tr>\n<td>Neural Modules</td>\n<td>Composable network components and parameter management</td>\n<td><code>Module</code>, <code>Linear</code>, activation functions</td>\n<td>Tensor and Autodiff layers</td>\n</tr>\n<tr>\n<td>Optimizers</td>\n<td>Parameter updates and training coordination</td>\n<td><code>SGD</code>, <code>Adam</code>, loss functions, training loops</td>\n<td>All lower layers</td>\n</tr>\n</tbody></table>\n<p>The beauty of this architecture is its <strong>define-by-run</strong> nature: users write imperative Python code using tensors and modules, but behind the scenes, the autodiff engine automatically constructs a computation graph that enables efficient gradient computation. This combines the ease of imperative programming with the mathematical rigor required for automatic differentiation.</p>\n<blockquote>\n<p><strong>Key Architectural Insight:</strong> Each layer provides a different level of abstraction—from mechanical tensor operations to high-level training orchestration—but they all work together transparently. Users interact primarily with the top two layers while the bottom two layers handle the complex mathematics automatically.</p>\n</blockquote>\n<p>The layers communicate through <strong>well-defined interfaces</strong> that maintain the abstraction boundaries. Modules create and manipulate tensors, tensors automatically build the computation graph during operations, the autodiff engine computes gradients when requested, and optimizers use those gradients to update parameters. This creates a clean data flow that&#39;s easy to understand and debug.</p>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>Understanding what each architectural layer owns and how they depend on each other is crucial for implementing a maintainable framework. Think of each layer as a specialized department in our neural network factory, with clear job descriptions and communication protocols.</p>\n<h4 id=\"tensor-operations-layer-responsibilities\">Tensor Operations Layer Responsibilities</h4>\n<p>The <strong>Tensor Operations Layer</strong> serves as the mathematical foundation, responsible for all numerical computation and shape management. Like the engine room of a ship, this layer handles the mechanical work that powers everything else.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility Area</th>\n<th>Specific Duties</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Storage</td>\n<td>N-dimensional array storage, dtype tracking, shape validation</td>\n<td><code>data</code>, <code>shape</code>, <code>dtype</code> attributes</td>\n</tr>\n<tr>\n<td>Arithmetic Operations</td>\n<td>Element-wise operations, matrix multiplication, broadcasting</td>\n<td><code>__add__</code>, <code>__mul__</code>, <code>matmul</code> methods</td>\n</tr>\n<tr>\n<td>Shape Management</td>\n<td>Broadcasting rule implementation, shape compatibility checking</td>\n<td><code>broadcast_shapes</code>, dimension expansion</td>\n</tr>\n<tr>\n<td>Gradient Metadata</td>\n<td>Tracking gradient requirements, storing gradient values</td>\n<td><code>requires_grad</code>, <code>grad</code>, <code>grad_fn</code> attributes</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Efficient array allocation, in-place operation prevention</td>\n<td>Memory layout optimization, copy semantics</td>\n</tr>\n</tbody></table>\n<p>This layer <strong>depends only on NumPy</strong> for the actual numerical computations, making it the foundation that all other layers build upon. It must handle edge cases like shape mismatches and provide clear error messages when operations are incompatible.</p>\n<p>The tensor layer <strong>exposes its capabilities</strong> through operator overloading, making tensor arithmetic feel natural to Python users while automatically tracking the operations needed for gradient computation. Every operation returns a new tensor (avoiding in-place modifications that break gradients) with proper gradient metadata attached.</p>\n<h4 id=\"automatic-differentiation-engine-responsibilities\">Automatic Differentiation Engine Responsibilities</h4>\n<p>The <strong>Autodiff Engine</strong> acts as the framework&#39;s memory system, automatically recording every operation and enabling gradient computation through the chain rule. Think of it as a detailed accountant who tracks every mathematical operation so we can later compute how changes propagate backward through the computation.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility Area</th>\n<th>Specific Duties</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Graph Construction</td>\n<td>Recording operations as nodes, linking tensors as edges</td>\n<td><code>Operation</code> base class, graph node creation</td>\n</tr>\n<tr>\n<td>Topological Sorting</td>\n<td>Ordering nodes for correct backward pass traversal</td>\n<td>Dependency resolution, cycle detection</td>\n</tr>\n<tr>\n<td>Chain Rule Application</td>\n<td>Computing gradients using calculus chain rule</td>\n<td><code>backward()</code> method, gradient propagation</td>\n</tr>\n<tr>\n<td>Gradient Accumulation</td>\n<td>Summing gradients when tensors used multiple times</td>\n<td>Gradient aggregation, memory management</td>\n</tr>\n<tr>\n<td>Graph Memory Management</td>\n<td>Preventing circular references, cleaning up graphs</td>\n<td>Reference counting, graph lifecycle</td>\n</tr>\n</tbody></table>\n<p>The autodiff engine <strong>depends on the tensor layer</strong> for the actual tensor operations but adds the crucial capability of reversibility. It transforms the forward computation into a data structure that can be traversed backward to compute gradients efficiently.</p>\n<p>This layer <strong>provides gradient computation services</strong> to the upper layers through the <code>backward()</code> method, which triggers the reverse-mode differentiation algorithm. The key insight is that this layer makes gradient computation completely automatic—users never need to manually compute derivatives.</p>\n<blockquote>\n<p><strong>Critical Design Decision:</strong> The autodiff engine uses <strong>reverse-mode differentiation</strong> (backpropagation) rather than forward-mode because reverse-mode is more efficient for the typical case where we have many parameters but few outputs (like a scalar loss function).</p>\n</blockquote>\n<h4 id=\"neural-network-modules-layer-responsibilities\">Neural Network Modules Layer Responsibilities</h4>\n<p>The <strong>Modules Layer</strong> provides the user-facing building blocks for constructing neural networks. Like a hardware store that sells pre-made components, this layer offers tested, composable pieces that users can combine to build complex architectures without worrying about the underlying mathematical details.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility Area</th>\n<th>Specific Duties</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter Management</td>\n<td>Registering, initializing, and tracking trainable parameters</td>\n<td><code>parameters()</code> method, parameter registration</td>\n</tr>\n<tr>\n<td>Forward Pass Logic</td>\n<td>Implementing layer-specific computations</td>\n<td><code>forward()</code> method, modular composition</td>\n</tr>\n<tr>\n<td>Initialization Strategies</td>\n<td>Setting initial parameter values for stable training</td>\n<td>Weight initialization schemes, bias handling</td>\n</tr>\n<tr>\n<td>Composability</td>\n<td>Enabling layers to be combined and nested</td>\n<td><code>Module</code> base class, container patterns</td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>Handling training vs. inference modes, layer state</td>\n<td>Mode switching, stateful layer support</td>\n</tr>\n</tbody></table>\n<p>The modules layer <strong>depends on both tensor and autodiff layers</strong> because modules create tensors (which automatically build computation graphs) and rely on gradient computation for training. However, modules abstract away these details, providing a clean interface for network construction.</p>\n<p>This layer <strong>serves the optimizer layer</strong> by exposing all trainable parameters through the <code>parameters()</code> method, enabling optimizers to update all network weights without needing to understand the network structure. The recursive parameter collection is a key design feature that makes nested modules work seamlessly.</p>\n<h4 id=\"optimizers-and-training-layer-responsibilities\">Optimizers and Training Layer Responsibilities</h4>\n<p>The <strong>Optimizers Layer</strong> coordinates the entire learning process, orchestrating forward passes, loss computation, gradient computation, and parameter updates. Think of optimizers as conductors who coordinate all the musicians (modules) in the orchestra to create a harmonious performance (successful training).</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility Area</th>\n<th>Specific Duties</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter Updates</td>\n<td>Applying gradient-based updates to model parameters</td>\n<td><code>step()</code> method, parameter modification</td>\n</tr>\n<tr>\n<td>Optimizer State</td>\n<td>Maintaining momentum, adaptive learning rates, etc.</td>\n<td>State dictionaries, optimizer-specific variables</td>\n</tr>\n<tr>\n<td>Training Coordination</td>\n<td>Managing forward/backward/update sequence</td>\n<td>Training loop orchestration, batch processing</td>\n</tr>\n<tr>\n<td>Loss Computation</td>\n<td>Computing scalar loss values for gradient computation</td>\n<td>Loss function implementations, reduction strategies</td>\n</tr>\n<tr>\n<td>Learning Rate Management</td>\n<td>Adjusting learning rates according to schedules</td>\n<td>Learning rate scheduling, adaptive algorithms</td>\n</tr>\n</tbody></table>\n<p>The optimizers layer <strong>depends on all lower layers</strong>: it uses modules to get parameters, relies on autodiff for gradients, and manipulates tensors directly during parameter updates. This layer sits at the top of the dependency hierarchy because it orchestrates all the other components.</p>\n<p>This layer <strong>provides the primary user interface</strong> for training neural networks, exposing simple methods like <code>optimizer.step()</code> that hide the complexity of gradient computation and parameter updates. The training loop logic coordinates all the lower layers to implement the complete learning algorithm.</p>\n<h4 id=\"inter-layer-communication-patterns\">Inter-Layer Communication Patterns</h4>\n<p>The layers communicate through <strong>well-defined data flows</strong> that maintain clean abstractions while enabling the complex coordination required for neural network training.</p>\n<table>\n<thead>\n<tr>\n<th>Communication Path</th>\n<th>Data Flow</th>\n<th>Interface</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Modules → Tensors</td>\n<td>Parameter creation, forward computation</td>\n<td>Tensor constructors, arithmetic operators</td>\n<td>Enable neural computation</td>\n</tr>\n<tr>\n<td>Tensors → Autodiff</td>\n<td>Operation recording, graph construction</td>\n<td><code>Operation</code> registration, gradient functions</td>\n<td>Enable automatic differentiation</td>\n</tr>\n<tr>\n<td>Autodiff → Tensors</td>\n<td>Gradient computation, backward propagation</td>\n<td><code>grad</code> attribute population</td>\n<td>Provide computed gradients</td>\n</tr>\n<tr>\n<td>Optimizers → Modules</td>\n<td>Parameter collection, state inspection</td>\n<td><code>parameters()</code> method</td>\n<td>Access trainable parameters</td>\n</tr>\n<tr>\n<td>Optimizers → Autodiff</td>\n<td>Gradient computation triggering</td>\n<td><code>backward()</code> method calls</td>\n<td>Compute parameter gradients</td>\n</tr>\n<tr>\n<td>Optimizers → Tensors</td>\n<td>Parameter updates, gradient clearing</td>\n<td>Direct tensor manipulation</td>\n<td>Update model weights</td>\n</tr>\n</tbody></table>\n<p>The key insight is that <strong>data flows both up and down</strong> the architectural stack: forward computation flows upward (tensors → autodiff → modules → optimizers) while gradient computation flows downward (optimizers trigger backward pass → autodiff computes gradients → tensors store results → optimizers apply updates).</p>\n<h3 id=\"recommended-project-structure\">Recommended Project Structure</h3>\n<p>A well-organized project structure is like a well-designed library—everything has its place, related components are grouped together, and users can quickly find what they need. Our framework&#39;s four-layer architecture naturally suggests a directory structure that mirrors the conceptual organization.</p>\n<p>The project structure should <strong>separate core framework code from user examples</strong>, <strong>group related functionality together</strong>, and <strong>make the dependency hierarchy obvious</strong> from the directory layout. This helps both implementers understand where to put new code and users understand how the framework is organized.</p>\n<h4 id=\"directory-layout\">Directory Layout</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural-framework/\n├── neural_framework/              # Core framework package\n│   ├── __init__.py               # Public API exports\n│   ├── tensor/                   # Tensor Operations Layer\n│   │   ├── __init__.py          # Tensor class exports\n│   │   ├── tensor.py            # Core Tensor implementation\n│   │   ├── operations.py        # Arithmetic operations (+, *, etc.)\n│   │   └── broadcasting.py      # Broadcasting utilities\n│   ├── autodiff/                # Automatic Differentiation Engine\n│   │   ├── __init__.py         # Autodiff exports\n│   │   ├── operation.py        # Operation base class\n│   │   ├── graph.py            # Computation graph management\n│   │   └── backward.py         # Backward pass algorithms\n│   ├── modules/                # Neural Network Modules Layer\n│   │   ├── __init__.py        # Module class exports\n│   │   ├── module.py          # Base Module class\n│   │   ├── linear.py          # Linear/Dense layers\n│   │   ├── activation.py      # Activation functions\n│   │   └── container.py       # Sequential, ModuleList, etc.\n│   ├── optim/                 # Optimizers and Training Layer\n│   │   ├── __init__.py       # Optimizer exports\n│   │   ├── optimizer.py      # Base Optimizer class\n│   │   ├── sgd.py           # SGD implementation\n│   │   ├── adam.py          # Adam implementation\n│   │   └── loss.py          # Loss functions\n│   └── utils/               # Cross-cutting utilities\n│       ├── __init__.py     # Utility exports\n│       ├── gradient_check.py # Numerical gradient checking\n│       └── testing.py      # Testing utilities\n├── examples/               # Example usage and tutorials\n│   ├── basic_tensor.py    # Tensor operation examples\n│   ├── simple_network.py  # Basic neural network training\n│   ├── mnist_classifier.py # Complete classification example\n│   └── gradient_check_demo.py # Gradient checking examples\n├── tests/                 # Test suite\n│   ├── test_tensor.py    # Tensor operation tests\n│   ├── test_autodiff.py  # Gradient computation tests\n│   ├── test_modules.py   # Neural module tests\n│   ├── test_optimizers.py # Optimizer tests\n│   └── test_integration.py # End-to-end tests\n├── docs/                 # Documentation\n│   ├── tutorial.md      # Getting started guide\n│   ├── api_reference.md # Complete API documentation\n│   └── design_notes.md  # Architecture explanations\n├── setup.py             # Package installation\n├── requirements.txt     # Dependencies\n└── README.md           # Project overview</code></pre></div>\n\n<h4 id=\"package-organization-rationale\">Package Organization Rationale</h4>\n<p>The directory structure reflects several key design principles that make the framework easy to understand and maintain.</p>\n<blockquote>\n<p><strong>Decision: Layer-Based Package Organization</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to organize code in a way that reflects the architectural layers and makes dependencies clear</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Flat structure with all classes in one package</li>\n<li>Feature-based packages (training, inference, etc.)</li>\n<li>Layer-based packages matching architectural design</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use layer-based packages (<code>tensor/</code>, <code>autodiff/</code>, <code>modules/</code>, <code>optim/</code>)</li>\n<li><strong>Rationale</strong>: Makes the dependency hierarchy obvious, prevents circular imports, and helps developers understand where to implement new features</li>\n<li><strong>Consequences</strong>: Clear separation of concerns but requires understanding the layer architecture to navigate the codebase</li>\n</ul>\n</blockquote>\n<p>Each package corresponds to one architectural layer, making the <strong>dependency flow obvious</strong>: <code>tensor/</code> has no internal dependencies, <code>autodiff/</code> depends only on <code>tensor/</code>, <code>modules/</code> depends on <code>tensor/</code> and <code>autodiff/</code>, and <code>optim/</code> depends on all lower layers.</p>\n<table>\n<thead>\n<tr>\n<th>Package</th>\n<th>Layer</th>\n<th>Key Files</th>\n<th>Dependencies</th>\n<th>Exported Classes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>tensor/</code></td>\n<td>Tensor Operations</td>\n<td><code>tensor.py</code>, <code>operations.py</code>, <code>broadcasting.py</code></td>\n<td>NumPy only</td>\n<td><code>Tensor</code>, arithmetic functions</td>\n</tr>\n<tr>\n<td><code>autodiff/</code></td>\n<td>Autodiff Engine</td>\n<td><code>operation.py</code>, <code>graph.py</code>, <code>backward.py</code></td>\n<td><code>tensor/</code></td>\n<td><code>Operation</code>, graph utilities</td>\n</tr>\n<tr>\n<td><code>modules/</code></td>\n<td>Neural Modules</td>\n<td><code>module.py</code>, <code>linear.py</code>, <code>activation.py</code></td>\n<td><code>tensor/</code>, <code>autodiff/</code></td>\n<td><code>Module</code>, <code>Linear</code>, <code>ReLU</code>, etc.</td>\n</tr>\n<tr>\n<td><code>optim/</code></td>\n<td>Optimizers</td>\n<td><code>optimizer.py</code>, <code>sgd.py</code>, <code>adam.py</code>, <code>loss.py</code></td>\n<td>All layers</td>\n<td><code>SGD</code>, <code>Adam</code>, loss functions</td>\n</tr>\n</tbody></table>\n<p>The <strong><code>__init__.py</code> files</strong> in each package serve as <strong>API gateways</strong>, exposing only the classes and functions that users of that layer should access. This creates clean public interfaces while hiding implementation details.</p>\n<h4 id=\"import-structure-and-api-design\">Import Structure and API Design</h4>\n<p>The framework should provide a <strong>PyTorch-like import experience</strong> where users can access everything they need through intuitive import paths. The main <code>neural_framework/__init__.py</code> file serves as the primary API entry point.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># neural_framework/__init__.py - Main API exports</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .modules </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Module, Linear, ReLU, Sequential  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .optim </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> SGD</span><span style=\"color:#E1E4E8\">, Adam</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .optim.loss </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CrossEntropyLoss, MSELoss</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Version and metadata</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__version__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"0.1.0\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__all__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"Tensor\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Module\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Linear\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"ReLU\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Sequential\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"SGD\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Adam\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"CrossEntropyLoss\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"MSELoss\"</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<p>This enables <strong>clean user imports</strong> like <code>from neural_framework import Tensor, Linear, SGD</code>, mirroring the experience that PyTorch users expect. Advanced users who need lower-level access can import directly from subpackages like <code>from neural_framework.autodiff import Operation</code>.</p>\n<h4 id=\"development-workflow-organization\">Development Workflow Organization</h4>\n<p>The project structure supports a <strong>milestone-driven development approach</strong> where implementers can build and test each layer independently before integrating with higher layers.</p>\n<table>\n<thead>\n<tr>\n<th>Milestone</th>\n<th>Primary Packages</th>\n<th>Test Files</th>\n<th>Example Files</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Milestone 1: Tensors</td>\n<td><code>tensor/</code></td>\n<td><code>test_tensor.py</code></td>\n<td><code>basic_tensor.py</code></td>\n</tr>\n<tr>\n<td>Milestone 2: Autodiff</td>\n<td><code>autodiff/</code></td>\n<td><code>test_autodiff.py</code></td>\n<td><code>gradient_check_demo.py</code></td>\n</tr>\n<tr>\n<td>Milestone 3: Modules</td>\n<td><code>modules/</code></td>\n<td><code>test_modules.py</code></td>\n<td><code>simple_network.py</code></td>\n</tr>\n<tr>\n<td>Milestone 4: Training</td>\n<td><code>optim/</code></td>\n<td><code>test_optimizers.py</code>, <code>test_integration.py</code></td>\n<td><code>mnist_classifier.py</code></td>\n</tr>\n</tbody></table>\n<p>Each milestone can be <strong>developed and tested independently</strong> because the package structure enforces the dependency hierarchy. Developers can implement the tensor layer, verify it works with tests and examples, then move to the autodiff layer knowing they have a solid foundation.</p>\n<p>The <strong><code>examples/</code> directory</strong> provides <strong>learning checkpoints</strong> where users can verify their implementation works correctly after each milestone. These examples start simple (<code>basic_tensor.py</code>) and gradually build complexity (<code>mnist_classifier.py</code>), providing a clear learning progression.</p>\n<blockquote>\n<p><strong>Educational Architecture Insight:</strong> The directory structure itself teaches the framework architecture—by organizing code, developers internalize the layer dependencies and understand where different types of functionality belong.</p>\n</blockquote>\n<h4 id=\"testing-strategy-integration\">Testing Strategy Integration</h4>\n<p>The test structure supports both <strong>unit testing</strong> (individual layer testing) and <strong>integration testing</strong> (cross-layer functionality). Each package has corresponding test files that can be run independently or as part of a complete test suite.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Files</th>\n<th>Purpose</th>\n<th>Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Tests</td>\n<td><code>test_tensor.py</code>, <code>test_autodiff.py</code>, etc.</td>\n<td>Test individual layer functionality</td>\n<td>Only the layer being tested</td>\n</tr>\n<tr>\n<td>Integration Tests</td>\n<td><code>test_integration.py</code></td>\n<td>Test complete training workflows</td>\n<td>All layers</td>\n</tr>\n<tr>\n<td>Gradient Tests</td>\n<td><code>gradient_check_demo.py</code></td>\n<td>Verify autodiff correctness</td>\n<td><code>tensor/</code>, <code>autodiff/</code>, <code>utils/</code></td>\n</tr>\n<tr>\n<td>Performance Tests</td>\n<td><code>test_performance.py</code></td>\n<td>Benchmark critical operations</td>\n<td>All layers</td>\n</tr>\n</tbody></table>\n<p>The <strong><code>utils/</code> package</strong> provides testing infrastructure like <code>gradient_check.py</code> that compares automatic differentiation results with numerical differentiation, helping verify that the autodiff implementation is mathematically correct.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building a neural network framework requires careful technology choices and well-structured starter code that provides necessary infrastructure without solving the core learning challenges. The implementation should prioritize <strong>educational clarity</strong> over performance optimization while still demonstrating professional software development practices.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommended for Learning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tensor Storage</td>\n<td>Pure NumPy arrays</td>\n<td>Custom memory management with stride handling</td>\n<td>Pure NumPy arrays</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>Python lists for graph nodes</td>\n<td>C++ extensions for graph traversal</td>\n<td>Python lists for graph nodes</td>\n</tr>\n<tr>\n<td>Broadcasting</td>\n<td>Manual shape expansion</td>\n<td>Vectorized NumPy broadcasting</td>\n<td>Manual implementation then NumPy</td>\n</tr>\n<tr>\n<td>Parameter Initialization</td>\n<td>Simple random normal</td>\n<td>Xavier/He initialization schemes</td>\n<td>Start simple, add advanced schemes</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Built-in <code>assert</code> statements</td>\n<td><code>pytest</code> with fixtures</td>\n<td><code>pytest</code> with numerical gradient checking</td>\n</tr>\n<tr>\n<td>GPU Support</td>\n<td>CPU-only with NumPy</td>\n<td>CUDA integration with CuPy</td>\n<td>CPU-only (optional CuPy extension)</td>\n</tr>\n</tbody></table>\n<p>The <strong>Simple Option</strong> choices prioritize understanding the underlying algorithms over performance. Students should implement core concepts manually first, then optionally optimize with advanced techniques after mastering the fundamentals.</p>\n<h4 id=\"recommended-project-structure-setup\">Recommended Project Structure Setup</h4>\n<p>Start with this directory structure and gradually populate it as you complete each milestone:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural-framework/\n├── neural_framework/\n│   ├── __init__.py              # Start empty, add exports as you build\n│   ├── tensor/\n│   │   ├── __init__.py         # from .tensor import Tensor\n│   │   ├── tensor.py           # Core Tensor class (YOUR IMPLEMENTATION)\n│   │   ├── operations.py       # Arithmetic operations (YOUR IMPLEMENTATION) \n│   │   └── broadcasting.py     # Broadcasting utilities (STARTER CODE)\n│   ├── autodiff/\n│   │   ├── __init__.py        # from .operation import Operation\n│   │   ├── operation.py       # Operation base class (YOUR IMPLEMENTATION)\n│   │   ├── graph.py          # Graph utilities (STARTER CODE)\n│   │   └── backward.py       # Backward pass (YOUR IMPLEMENTATION)\n│   ├── modules/\n│   │   ├── __init__.py       # Module exports (add as you implement)\n│   │   ├── module.py         # Base Module class (YOUR IMPLEMENTATION)\n│   │   ├── linear.py         # Linear layer (YOUR IMPLEMENTATION)\n│   │   └── activation.py     # Activation functions (YOUR IMPLEMENTATION)\n│   ├── optim/\n│   │   ├── __init__.py      # Optimizer exports\n│   │   ├── optimizer.py     # Base Optimizer (STARTER CODE)\n│   │   ├── sgd.py          # SGD implementation (YOUR IMPLEMENTATION)\n│   │   ├── adam.py         # Adam implementation (YOUR IMPLEMENTATION)  \n│   │   └── loss.py         # Loss functions (YOUR IMPLEMENTATION)\n│   └── utils/\n│       ├── __init__.py     # Utility exports\n│       ├── gradient_check.py  # Numerical gradient checking (STARTER CODE)\n│       └── testing.py         # Test utilities (STARTER CODE)\n├── examples/                   # Add examples after each milestone\n├── tests/                     # Add tests as you implement\n└── setup.py                   # Basic package setup (STARTER CODE)</code></pre></div>\n\n<p>Create this structure first, then implement files marked &quot;YOUR IMPLEMENTATION&quot; during the corresponding milestones. Files marked &quot;STARTER CODE&quot; are provided below as complete, working implementations.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong><code>neural_framework/utils/gradient_check.py</code></strong> - Complete numerical gradient checking utilities:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Numerical gradient checking utilities for verifying autodiff correctness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> numerical_gradient</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[Tensor], h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> List[np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute numerical gradients using finite differences.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the reference implementation for checking autodiff correctness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradients </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> input_tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros_like(input_tensor.data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        it </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.nditer(input_tensor.data, </span><span style=\"color:#FFAB70\">flags</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'multi_index'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> it.finished:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            idx </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> it.multi_index</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute f(x + h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            old_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> input_tensor.data[idx]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data[idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_value </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            fxh_pos </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute f(x - h)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data[idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_value </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            fxh_neg </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Restore original value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data[idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Finite difference approximation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad[idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (fxh_pos </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> fxh_neg) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            it.iternext()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradients.append(grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradients</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[Tensor], tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compare automatic differentiation gradients with numerical gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns True if gradients match within tolerance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute numerical gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numerical_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> numerical_gradient(f, inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute automatic gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auto_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [inp.grad.data </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compare gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (num_grad, auto_grad) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(numerical_grads, auto_grads)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.abs(num_grad </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> auto_grad)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> np.max(diff) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> tolerance:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient check failed for input </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Max difference: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">np.max(diff)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Numerical gradient: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">num_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Automatic gradient: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">auto_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Gradient check passed!\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong><code>neural_framework/utils/testing.py</code></strong> - Testing utilities for framework validation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Testing utilities for neural framework validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_test_tensors</span><span style=\"color:#E1E4E8\">(shapes: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">], requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> List[Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create test tensors with random data for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> shape </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> shapes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">shape).astype(np.float32) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tensor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(data, </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">requires_grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tensors.append(tensor)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tensors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_tensor_equal</span><span style=\"color:#E1E4E8\">(a: Tensor, b: Tensor, tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Assert two tensors have equal data within tolerance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> a.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> b.shape, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Shapes don't match: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">a.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">b.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.abs(a.data </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> b.data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> np.max(diff) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> tolerance, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Tensors differ by </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">np.max(diff)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_gradient_exists</span><span style=\"color:#E1E4E8\">(tensor: Tensor):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Assert that tensor has computed gradients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tensor.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Gradient not computed\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isnan(tensor.grad.data).any(), </span><span style=\"color:#9ECBFF\">\"Gradient contains NaN\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> np.isfinite(tensor.grad.data).all(), </span><span style=\"color:#9ECBFF\">\"Gradient contains infinite values\"</span></span></code></pre></div>\n\n<p><strong><code>neural_framework/tensor/broadcasting.py</code></strong> - Broadcasting utilities:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Broadcasting utilities for tensor operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> broadcast_shapes</span><span style=\"color:#E1E4E8\">(shape1: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], shape2: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute the broadcasted shape for two input shapes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Follows NumPy broadcasting rules.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Pad with 1s to make shapes same length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_dims </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(shape1), </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(shape2))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shape1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (max_dims </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(shape1)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> shape1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shape2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (max_dims </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(shape2)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> shape2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute broadcasted shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> dim1, dim2 </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> zip</span><span style=\"color:#E1E4E8\">(shape1, shape2):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> dim1 </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result_shape.append(dim2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> dim2 </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result_shape.append(dim1)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> dim1 </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> dim2:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result_shape.append(dim1)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot broadcast shapes </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> and </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> tuple</span><span style=\"color:#E1E4E8\">(result_shape)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> unbroadcast_gradient</span><span style=\"color:#E1E4E8\">(grad: np.ndarray, original_shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Reduce gradient from broadcasted shape back to original shape.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is crucial for correct gradient computation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Handle scalar case</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> original_shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> ():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> np.sum(grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum out added dimensions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ndims_added </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.ndim </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(original_shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(ndims_added):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.sum(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum over broadcasted dimensions  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (grad_dim, orig_dim) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(grad.shape, original_shape)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> orig_dim </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> grad_dim </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.sum(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">i, </span><span style=\"color:#FFAB70\">keepdims</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> grad</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong><code>neural_framework/tensor/tensor.py</code></strong> - Tensor class skeleton for your implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Core Tensor class implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, Union, </span><span style=\"color:#79B8FF\">TYPE_CHECKING</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> TYPE_CHECKING</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    from</span><span style=\"color:#E1E4E8\"> ..autodiff.operation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Operation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tensor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    N-dimensional array with automatic differentiation support.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the foundational class that everything else builds upon.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 data: Union[np.ndarray, </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 grad_fn: Optional[</span><span style=\"color:#9ECBFF\">'Operation'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize tensor with data and gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert data to numpy array if needed, ensure float32 dtype</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store data, requires_grad, and grad_fn attributes  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize grad to None, shape and dtype from data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.asarray() and .astype(np.float32) for data conversion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, gradient: Optional[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initiate backpropagation from this tensor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If gradient is None, create gradient of ones with same shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call _backward() to start recursive gradient computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle case where tensor doesn't require gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This is the public interface that users call</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _backward</span><span style=\"color:#E1E4E8\">(self, gradient: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Internal recursive gradient computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Accumulate gradient into self.grad (handle None case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If grad_fn exists, call its backward method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure gradients are properly accumulated for shared tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This implements the recursive chain rule application</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __add__</span><span style=\"color:#E1E4E8\">(self, other: Union[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Tensor addition with automatic differentiation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert other to Tensor if it's a scalar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Add operation and call its forward method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return new Tensor with proper grad_fn for autodiff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Import Add operation from ..autodiff.operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __mul__</span><span style=\"color:#E1E4E8\">(self, other: Union[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Element-wise multiplication with automatic differentiation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert other to Tensor if it's a scalar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Multiply operation and call its forward method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return new Tensor with proper grad_fn for autodiff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> matmul</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Matrix multiplication with automatic differentiation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate shapes are compatible for matrix multiplication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create MatMul operation and call its forward method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle batched matrix multiplication (3+ dimensions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return new Tensor with proper grad_fn for autodiff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> shape</span><span style=\"color:#E1E4E8\">(self) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return shape of tensor data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return self.data.shape</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> dtype</span><span style=\"color:#E1E4E8\">(self) -> np.dtype:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return data type of tensor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return self.data.dtype</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong><code>neural_framework/autodiff/operation.py</code></strong> - Operation base class skeleton:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Base class for automatic differentiation operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, </span><span style=\"color:#79B8FF\">TYPE_CHECKING</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> TYPE_CHECKING</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Base class for differentiable operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Each operation knows how to compute forward pass and backward gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, inputs: Tuple[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store input tensors for gradient computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store inputs tuple for backward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Determine if this operation requires gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: requires_grad if any input requires gradients</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.inputs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inputs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.requires_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(inp.requires_grad </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute forward pass, return numpy array.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement in subclasses (Add, Multiply, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients w.r.t inputs and propagate backwards.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement in subclasses using chain rule</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing each milestone, verify your implementation with these checkpoints:</p>\n<p><strong>Milestone 1 Checkpoint - Tensor Operations:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create this as examples/test_milestone1.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test basic tensor creation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">4.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">6.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test arithmetic operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> b  </span><span style=\"color:#6A737D\"># Should give [5.0, 7.0, 9.0]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">d </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> b  </span><span style=\"color:#6A737D\"># Should give [4.0, 10.0, 18.0]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test matrix multiplication</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([[</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">z </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.matmul(y)  </span><span style=\"color:#6A737D\"># Should give [[19, 22], [43, 50]]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Milestone 1 checkpoint passed!\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong> All operations complete without errors, tensors have correct values and shapes.</p>\n<p><strong>Milestone 2 Checkpoint - Automatic Differentiation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test gradient computation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> check_gradients</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_function</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([[</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> x  </span><span style=\"color:#6A737D\"># y = x² + x</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> y.sum()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># This should pass if autodiff is implemented correctly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">check_gradients(test_function, [x])</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong> &quot;Gradient check passed!&quot; confirming autodiff matches numerical gradients.</p>\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>NumPy Integration:</strong></p>\n<ul>\n<li>Use <code>np.asarray()</code> to convert inputs to arrays</li>\n<li>Use <code>.astype(np.float32)</code> for consistent float precision</li>\n<li>Use <code>np.broadcast_arrays()</code> for automatic shape expansion</li>\n<li>Use <code>np.sum(axis=...)</code> with <code>keepdims=True</code> for gradient unbroadcasting</li>\n</ul>\n<p><strong>Memory Management:</strong></p>\n<ul>\n<li>Always create new tensors instead of in-place operations</li>\n<li>Use weak references or manual cleanup for computation graphs</li>\n<li>Consider implementing <code>__del__</code> methods for large tensor cleanup</li>\n</ul>\n<p><strong>Error Handling:</strong></p>\n<ul>\n<li>Provide clear error messages for shape mismatches</li>\n<li>Check for NaN/infinity in gradients during debugging</li>\n<li>Validate tensor shapes before operations</li>\n</ul>\n<p><strong>Testing Integration:</strong></p>\n<ul>\n<li>Use <code>pytest</code> for organized test discovery: <code>pytest tests/</code></li>\n<li>Implement property-based tests with random tensor shapes</li>\n<li>Use <code>np.allclose()</code> for floating-point comparisons with tolerance</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Tensor &amp; Operations), Milestone 2 (Automatic Differentiation), Milestone 3 (Layers &amp; Modules) - establishes core data structures that support tensor operations, gradient computation, and neural network organization</p>\n</blockquote>\n<p>The data model forms the foundation of our neural network framework, defining the core data structures that enable tensor computations, automatic differentiation, and neural network construction. Think of the data model as the <strong>blueprint for a smart calculator</strong> - not just any calculator, but one that remembers every calculation it performs and can work backwards to figure out how each input contributed to the final result. Just as a calculator needs number storage, operation buttons, and a display, our framework needs tensors to store data, operations to transform it, and a way to organize everything into trainable models.</p>\n<p>This smart calculator analogy reveals why our data model is more complex than simple arrays. When you press &quot;+&quot; on a regular calculator, the numbers get added and that&#39;s it - the calculation history is lost. But our neural network framework must remember that &quot;tensor A was added to tensor B to produce tensor C&quot; because later, when we discover tensor C was slightly wrong, we need to trace back and figure out how to adjust tensors A and B to fix the error. This backward tracing is the essence of automatic differentiation and gradient-based learning.</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"Neural Framework System Architecture\"></p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Ftensor-relationships.svg\" alt=\"Tensor and Operation Type Hierarchy\"></p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fmodule-hierarchy.svg\" alt=\"Module System Organization\"></p>\n<p>The data model consists of three interconnected hierarchies that work together to enable neural network computation. The <strong>tensor hierarchy</strong> provides the fundamental data containers with gradient tracking capabilities. The <strong>operation hierarchy</strong> defines the computational nodes that transform tensors while building the computation graph. The <strong>module hierarchy</strong> organizes trainable parameters and neural network layers into composable building blocks. These three hierarchies interact during every forward and backward pass, with tensors flowing through operations that are organized within modules.</p>\n<p>Understanding the relationships between these data structures is crucial for implementing the framework correctly. A tensor knows which operation created it (through <code>grad_fn</code>), an operation knows which tensors it operates on (through <code>inputs</code>), and a module knows which parameters it owns (through parameter registration). This forms a web of references that enables automatic differentiation to trace computation backwards and modules to expose their trainable parameters to optimizers.</p>\n<h3 id=\"tensor-data-structure\">Tensor Data Structure</h3>\n<p>The <code>Tensor</code> class serves as the fundamental building block of our framework, much like how <strong>LEGO blocks</strong> are the basic unit of construction in LEGO sets. But these aren&#39;t ordinary LEGO blocks - they&#39;re smart blocks that remember how they were assembled and can tell you exactly which other blocks contributed to building any structure. Each tensor carries both the actual numerical data (like the plastic of the LEGO block) and the metadata needed for automatic differentiation (like a memory chip inside each block that tracks its construction history).</p>\n<p>The intelligence of our tensor lies in its ability to participate in <strong>define-by-run</strong> computation graphs. Unlike traditional arrays that simply hold numbers, our tensors are active participants in computation. When you add two tensors together, the result isn&#39;t just a new array with the sum - it&#39;s a new tensor that remembers it was created by adding two specific parent tensors. This memory enables the automatic differentiation engine to later traverse backwards through the computation, applying the chain rule to compute gradients.</p>\n<p>The tensor&#39;s dual nature as both data container and computation node creates design challenges that don&#39;t exist with regular arrays. Every tensor operation must simultaneously produce correct numerical results and maintain the computation graph structure needed for gradient computation. This means tensors must carefully track their creation history, handle broadcasting for mismatched shapes, and manage memory efficiently while supporting both eager execution and gradient computation.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>data</code></td>\n<td><code>np.ndarray</code></td>\n<td>The actual numerical data stored as a NumPy array, supporting N-dimensional arrays with efficient memory layout and vectorized operations</td>\n</tr>\n<tr>\n<td><code>requires_grad</code></td>\n<td><code>bool</code></td>\n<td>Flag indicating whether this tensor should track gradients during automatic differentiation - leaf tensors with this flag become optimization targets</td>\n</tr>\n<tr>\n<td><code>grad</code></td>\n<td><code>Optional[Tensor]</code></td>\n<td>Accumulated gradient tensor with same shape as data, initially None until backward pass computes and stores gradients</td>\n</tr>\n<tr>\n<td><code>grad_fn</code></td>\n<td><code>Optional[Operation]</code></td>\n<td>Reference to the operation that created this tensor, forming parent-child links in the computation graph for backpropagation</td>\n</tr>\n<tr>\n<td><code>shape</code></td>\n<td><code>Tuple[int, ...]</code></td>\n<td>Tuple describing tensor dimensions, cached from underlying NumPy array for quick access during shape validation and broadcasting</td>\n</tr>\n<tr>\n<td><code>dtype</code></td>\n<td><code>np.dtype</code></td>\n<td>Data type of tensor elements (float32, float64, int32, etc.), inherited from NumPy array for memory efficiency and type safety</td>\n</tr>\n</tbody></table>\n<p>The <code>data</code> field contains the actual numerical values as a NumPy array, leveraging NumPy&#39;s mature implementation of N-dimensional arrays with efficient memory layout and vectorized operations. By building on NumPy rather than implementing our own array library, we inherit decades of optimization work and maintain compatibility with the broader scientific Python ecosystem. The tensor wraps this NumPy array with additional metadata needed for automatic differentiation.</p>\n<p>The <code>requires_grad</code> flag determines whether this tensor participates in gradient computation. Leaf tensors (those not created by operations) with <code>requires_grad=True</code> become the variables that optimizers will update during training. Intermediate tensors inherit their gradient requirement from their inputs - if any input requires gradients, the output will also require gradients. This automatic propagation ensures the computation graph includes all tensors needed for backpropagation.</p>\n<p>The <code>grad</code> field accumulates gradients during the backward pass, starting as None and being populated when <code>backward()</code> is called on some downstream tensor. Gradient accumulation is crucial because tensors can be used multiple times in a computation (like when a variable appears multiple times in an equation), and the total gradient is the sum of all partial contributions. The gradient tensor always has the same shape as the original tensor&#39;s data.</p>\n<p>The <code>grad_fn</code> field creates the parent-child links that form the computation graph. When an operation produces a new tensor, it sets the tensor&#39;s <code>grad_fn</code> to point back to itself. This creates a directed acyclic graph where each operation node knows its input tensors, and each tensor knows the operation that created it. During backpropagation, this graph structure enables traversal from output tensors back to input tensors.</p>\n<blockquote>\n<p><strong>Decision: NumPy Array Backend</strong></p>\n<ul>\n<li><strong>Context</strong>: Need efficient N-dimensional array operations with broadcasting and vectorization</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Custom array implementation with full control over memory layout</li>\n<li>NumPy arrays with wrapper for gradient tracking</li>\n<li>Pure Python lists with manual broadcasting</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: NumPy arrays with gradient tracking wrapper</li>\n<li><strong>Rationale</strong>: NumPy provides battle-tested implementations of broadcasting, vectorized operations, and memory management. Building custom arrays would require months of optimization work with marginal educational benefit.</li>\n<li><strong>Consequences</strong>: Inherits NumPy&#39;s performance characteristics and broadcasting semantics. Limits us to CPU computation without additional GPU backends, but enables focus on automatic differentiation concepts rather than array implementation details.</li>\n</ul>\n</blockquote>\n<p>The tensor creation process involves careful initialization of all fields to ensure proper gradient tracking. When creating a leaf tensor (from raw data), only <code>data</code>, <code>requires_grad</code>, and <code>dtype</code> are specified, with <code>grad</code> starting as None and <code>grad_fn</code> remaining None to indicate this tensor wasn&#39;t created by an operation. When operations create new tensors, they set the <code>grad_fn</code> field appropriately and inherit <code>requires_grad</code> from their inputs.</p>\n<p>Memory management becomes critical when tensors form large computation graphs. The <code>grad_fn</code> references create a chain of objects that must be carefully managed to prevent memory leaks. In production frameworks, computation graphs are often released after each backward pass, but for educational purposes we maintain them to enable inspection and debugging. The tensor&#39;s lifecycle involves creation during forward pass, gradient accumulation during backward pass, and eventual cleanup when the computation graph is released.</p>\n<h3 id=\"computation-graph-representation\">Computation Graph Representation</h3>\n<p>The computation graph transforms our neural network framework from a simple calculator into a <strong>time machine for mathematics</strong>. Just as a time machine must record every moment to enable traveling backwards, our computation graph records every operation to enable gradient computation backwards through the network. The graph isn&#39;t built ahead of time like a blueprint - instead, it emerges dynamically during the forward pass as operations execute, creating a perfect record of the computational path taken.</p>\n<p>This <strong>define-by-run</strong> approach means the computation graph reflects exactly what happened during the forward pass, capturing conditional logic, loops, and dynamic tensor shapes. Unlike static graph frameworks that require pre-declaring the computation structure, our dynamic graph adapts to the actual execution path. This flexibility comes at the cost of some optimization opportunities, but provides the intuitive programming model that has made PyTorch popular for research and education.</p>\n<p>The graph structure follows a specific pattern: <strong>operations are nodes, tensors are edges</strong>. Each operation node knows its input tensors and can compute gradients with respect to those inputs. Each tensor edge knows which operation created it and whether it requires gradient computation. This dual representation enables efficient forward computation (following tensor edges) and backward gradient computation (following operation nodes in reverse).</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fcomputation-graph.svg\" alt=\"Computation Graph Structure\"></p>\n<p>The <code>Operation</code> base class defines the interface that all computational nodes must implement. Operations serve as the <strong>factories</strong> in our computation assembly line, taking input tensors, producing output tensors, and recording their transformation in the computation graph. Each operation must implement both forward computation (producing numerical results) and backward computation (producing gradients with respect to inputs).</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>forward</code></td>\n<td><code>*inputs: Tensor</code></td>\n<td><code>np.ndarray</code></td>\n<td>Computes forward pass result as NumPy array, implementing the mathematical operation on input data</td>\n</tr>\n<tr>\n<td><code>backward</code></td>\n<td><code>grad_output: np.ndarray</code></td>\n<td><code>Tuple[np.ndarray, ...]</code></td>\n<td>Computes gradients with respect to each input using chain rule, returning tuple matching input count</td>\n</tr>\n<tr>\n<td><code>__init__</code></td>\n<td><code>*inputs: Tensor</code></td>\n<td><code>None</code></td>\n<td>Stores input tensors and validates shapes/types, setting up operation node in computation graph</td>\n</tr>\n</tbody></table>\n<p>The <code>inputs</code> field in every operation stores references to the input tensors, creating the parent-child relationships needed for backpropagation. When an operation executes, it stores these references permanently, ensuring the computation graph remains intact until explicitly released. This creates a memory chain where leaf tensors are kept alive by intermediate operations, which are kept alive by their output tensors.</p>\n<p>Operations must handle <strong>broadcasting</strong> carefully during both forward and backward passes. When tensors of different shapes are combined, NumPy&#39;s broadcasting rules automatically expand dimensions to make the operation valid. However, during backpropagation, gradients must be reduced back to the original tensor shapes to ensure dimensional consistency. This requires operations to remember the original shapes of their inputs and apply appropriate reduction operations during backward pass.</p>\n<blockquote>\n<p><strong>Decision: Operation Node Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to represent mathematical operations in computation graph for automatic differentiation</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Function-based operations that capture closures</li>\n<li>Class-based operations with inheritance hierarchy</li>\n<li>Single operation class with type discriminator</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Class-based operations with inheritance hierarchy</li>\n<li><strong>Rationale</strong>: Classes provide clear structure for forward/backward method pairs and enable specialized gradient computation for each operation type. Inheritance allows sharing common functionality while specializing gradient computation logic.</li>\n<li><strong>Consequences</strong>: Requires creating operation subclasses for each mathematical operation but provides type safety and clear separation of concerns. Makes gradient computation debugging easier by isolating each operation&#39;s backward logic.</li>\n</ul>\n</blockquote>\n<p>The operation hierarchy includes concrete implementations for common mathematical operations:</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Forward Computation</th>\n<th>Backward Gradient</th>\n<th>Broadcasting Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Add</code></td>\n<td>Element-wise sum <code>a + b</code></td>\n<td>Gradient passed unchanged to both inputs</td>\n<td>Supports full NumPy broadcasting rules</td>\n</tr>\n<tr>\n<td><code>Multiply</code></td>\n<td>Element-wise product <code>a * b</code></td>\n<td>Gradient multiplied by other input <code>grad * b</code>, <code>grad * a</code></td>\n<td>Supports full NumPy broadcasting rules</td>\n</tr>\n<tr>\n<td><code>MatMul</code></td>\n<td>Matrix multiplication <code>a @ b</code></td>\n<td><code>grad @ b.T</code>, <code>a.T @ grad</code> for 2D case</td>\n<td>No broadcasting, requires compatible dimensions</td>\n</tr>\n<tr>\n<td><code>ReLU</code></td>\n<td><code>max(0, x)</code> element-wise</td>\n<td>Gradient where input positive, zero elsewhere</td>\n<td>No broadcasting, operates element-wise</td>\n</tr>\n</tbody></table>\n<p>Each operation implementation must carefully handle the mathematical correctness of both forward and backward computations. The forward pass simply implements the mathematical operation using NumPy functions. The backward pass requires applying the chain rule by computing the partial derivative of the operation with respect to each input and multiplying by the incoming gradient.</p>\n<p>The <strong>topological ordering</strong> of operations becomes crucial during backpropagation. Gradients must flow backwards through the graph in an order that ensures all downstream gradients are computed before upstream gradients. This requires maintaining the computation graph structure during forward pass and performing a topological sort during backward pass. The sort ensures that when an operation&#39;s <code>backward</code> method is called, gradients from all downstream operations have already been computed and accumulated.</p>\n<p>Gradient accumulation handling is essential for correctness when tensors are used multiple times in a computation. Consider a simple case like <code>y = x + x</code> where the same tensor <code>x</code> appears twice. The gradient of <code>y</code> with respect to <code>x</code> should be 2, not 1, because <code>x</code> contributes to <code>y</code> through two different paths. The computation graph must track all these paths and sum the gradients appropriately.</p>\n<table>\n<thead>\n<tr>\n<th>Graph Property</th>\n<th>Implementation</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Acyclic Structure</td>\n<td>Operations only reference earlier tensors</td>\n<td>Prevents circular dependencies in gradient computation</td>\n</tr>\n<tr>\n<td>Dynamic Construction</td>\n<td>Graph built during forward pass execution</td>\n<td>Supports conditional logic and dynamic shapes</td>\n</tr>\n<tr>\n<td>Reference Integrity</td>\n<td>Operations maintain strong references to inputs</td>\n<td>Keeps computation graph alive for gradient computation</td>\n</tr>\n<tr>\n<td>Topological Ordering</td>\n<td>Depth-first traversal from output to inputs</td>\n<td>Ensures correct gradient flow direction during backpropagation</td>\n</tr>\n</tbody></table>\n<h3 id=\"parameter-and-module-hierarchy\">Parameter and Module Hierarchy</h3>\n<p>The module system transforms our collection of tensors and operations into a <strong>LEGO construction system</strong> for neural networks. Just as LEGO sets provide specialized pieces (wheels, bricks, windows) that snap together to build complex structures, our module system provides specialized neural network components (linear layers, activations, normalization) that compose to build sophisticated models. Each module knows what pieces it contains and can recursively report all its trainable parts to parent modules.</p>\n<p>The module hierarchy solves the <strong>parameter organization problem</strong> that emerges when building complex neural networks. A large model might contain millions of parameters spread across hundreds of layers, each with different initialization requirements, learning rates, and regularization settings. Without systematic organization, these parameters become impossible to manage. The module system provides a structured way to group related parameters, nest modules within modules, and recursively collect all trainable tensors for optimization.</p>\n<p>The <code>Module</code> base class serves as the <strong>universal connector</strong> that enables arbitrary composition of neural network components. Every neural network layer, activation function, and complete model inherits from this base class, ensuring consistent interfaces for parameter management, forward propagation, and recursive traversal. This design enables building networks by simply declaring modules and connecting them, without manually tracking parameters or implementing forward pass logic.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>forward</code></td>\n<td><code>*inputs: Tensor</code></td>\n<td><code>Tensor</code></td>\n<td>Abstract method that subclasses implement to define layer&#39;s computation, automatically called by <code>__call__</code></td>\n</tr>\n<tr>\n<td><code>parameters</code></td>\n<td><code>recursive: bool = True</code></td>\n<td><code>List[Tensor]</code></td>\n<td>Recursively collects all trainable tensors from this module and submodules, used by optimizers for updates</td>\n</tr>\n<tr>\n<td><code>register_parameter</code></td>\n<td><code>name: str, param: Tensor</code></td>\n<td><code>None</code></td>\n<td>Registers a tensor as a trainable parameter, adding it to the module&#39;s parameter dictionary for collection</td>\n</tr>\n<tr>\n<td><code>register_module</code></td>\n<td><code>name: str, module: Module</code></td>\n<td><code>None</code></td>\n<td>Registers a submodule, enabling recursive parameter collection and nested module organization</td>\n</tr>\n<tr>\n<td><code>__call__</code></td>\n<td><code>*inputs: Tensor</code></td>\n<td><code>Tensor</code></td>\n<td>Invokes forward method with additional hooks and validation, providing consistent interface for module execution</td>\n</tr>\n</tbody></table>\n<p>The parameter registration system enables modules to declare their trainable tensors in a structured way. When a module creates parameters (like weight matrices), it calls <code>register_parameter</code> to add them to an internal dictionary. This dictionary enables the <code>parameters()</code> method to collect all trainable tensors recursively. Parameter registration happens during module initialization, ensuring parameters are available for optimization before any forward pass.</p>\n<p>Submodule registration follows the same pattern but for child modules rather than individual tensors. When a module contains other modules (like a Sequential containing Linear layers), it registers them using <code>register_module</code>. This creates a tree structure where each module knows its children, enabling recursive operations like parameter collection, device movement, and serialization.</p>\n<blockquote>\n<p><strong>Decision: Recursive Parameter Collection</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to gather all trainable parameters from complex nested module hierarchies for optimizer updates</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Manual parameter list maintenance by users</li>\n<li>Global parameter registry with automatic discovery</li>\n<li>Recursive traversal of module hierarchy</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Recursive traversal of module hierarchy</li>\n<li><strong>Rationale</strong>: Recursive traversal provides automatic parameter discovery without global state or manual bookkeeping. Each module only needs to track its direct parameters and submodules, with recursion handling arbitrary nesting depth.</li>\n<li><strong>Consequences</strong>: Enables compositional module design where complex models are built by nesting simple modules. Requires careful implementation to avoid infinite recursion or duplicate parameter collection.</li>\n</ul>\n</blockquote>\n<p>The module hierarchy includes several essential concrete implementations:</p>\n<table>\n<thead>\n<tr>\n<th>Module Type</th>\n<th>Parameters</th>\n<th>Computation</th>\n<th>Typical Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Linear</code></td>\n<td><code>weight: Tensor</code>, <code>bias: Optional[Tensor]</code></td>\n<td><code>y = x @ weight.T + bias</code></td>\n<td>Fully connected layers, output projections</td>\n</tr>\n<tr>\n<td><code>Sequential</code></td>\n<td><code>modules: List[Module]</code></td>\n<td>Chains forward passes through contained modules</td>\n<td>Building feedforward networks, feature extractors</td>\n</tr>\n<tr>\n<td><code>ReLU</code></td>\n<td>None (no parameters)</td>\n<td><code>max(0, x)</code> element-wise</td>\n<td>Nonlinear activations between linear layers</td>\n</tr>\n<tr>\n<td><code>Sigmoid</code></td>\n<td>None (no parameters)</td>\n<td><code>1 / (1 + exp(-x))</code> element-wise</td>\n<td>Output activations for binary classification</td>\n</tr>\n</tbody></table>\n<p>The <code>Linear</code> module demonstrates parameter management principles. During initialization, it creates weight and bias tensors with appropriate shapes and initialization values, then registers them as parameters. The forward method implements matrix multiplication followed by bias addition, handling batch dimensions correctly. The gradient computation happens automatically through the underlying tensor operations.</p>\n<p>The <code>Sequential</code> module demonstrates composition patterns. It accepts a list of modules during initialization and registers each one as a submodule. Its forward method simply chains the modules together, passing the output of each module as input to the next. This enables building complex networks with a simple declarative syntax.</p>\n<p>Parameter initialization presents important design considerations for training stability. Different layer types require different initialization strategies to prevent vanishing or exploding gradients. The module system must provide sensible defaults while allowing customization for advanced users.</p>\n<table>\n<thead>\n<tr>\n<th>Initialization Strategy</th>\n<th>Distribution</th>\n<th>Typical Usage</th>\n<th>Mathematical Justification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Xavier Uniform</td>\n<td><code>Uniform(-sqrt(6/(fan_in+fan_out)), sqrt(6/(fan_in+fan_out)))</code></td>\n<td>Tanh/Sigmoid activations</td>\n<td>Maintains activation variance across layers</td>\n</tr>\n<tr>\n<td>He Normal</td>\n<td><code>Normal(0, sqrt(2/fan_in))</code></td>\n<td>ReLU activations</td>\n<td>Accounts for ReLU&#39;s variance reduction</td>\n</tr>\n<tr>\n<td>Zero Initialization</td>\n<td><code>Constant(0)</code></td>\n<td>Bias terms</td>\n<td>Prevents systematic shifts in initial activations</td>\n</tr>\n<tr>\n<td>Identity</td>\n<td><code>Eye()</code> with appropriate scaling</td>\n<td>Residual connections</td>\n<td>Enables identity mapping for skip connections</td>\n</tr>\n</tbody></table>\n<p>The module lifecycle involves initialization, repeated forward passes, and parameter updates. During initialization, modules create and register their parameters with appropriate initialization. During training, the optimizer calls <code>parameters()</code> to collect all trainable tensors, then updates them based on computed gradients. The module system must ensure parameters remain correctly registered throughout this lifecycle.</p>\n<p>Module composition enables powerful design patterns for building complex architectures. Modules can be nested arbitrarily deeply, with each level adding its own parameters and functionality. This compositional approach scales from simple feedforward networks to complex architectures like transformers or residual networks, all using the same underlying module interface.</p>\n<p>⚠️ <strong>Pitfall: Parameter Not Registered</strong></p>\n<p>A common mistake is creating parameter tensors without registering them with the module system. For example:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BrokenLinear</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, in_features, out_features):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # WRONG: Creates parameter but doesn't register it</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.weight </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.random.randn(out_features, in_features), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p>This breaks parameter collection because <code>parameters()</code> only returns registered parameters. The optimizer won&#39;t see these parameters and won&#39;t update them during training. The fix is always calling <code>register_parameter</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CorrectLinear</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, in_features, out_features):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        weight </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.random.randn(out_features, in_features), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.register_parameter(</span><span style=\"color:#9ECBFF\">'weight'</span><span style=\"color:#E1E4E8\">, weight)  </span><span style=\"color:#6A737D\"># CORRECT: Registers parameter</span></span></code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Circular Module References</strong></p>\n<p>Another common mistake is creating circular references between modules, which can cause infinite recursion during parameter collection. This typically happens when implementing attention mechanisms or recurrent networks incorrectly:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># WRONG: Creates circular reference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">module_a.child </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> module_b</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">module_b.parent </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> module_a  </span><span style=\"color:#6A737D\"># Circular reference</span></span></code></pre></div>\n\n<p>The recursive parameter collection will infinitely traverse this cycle. The fix is carefully designing module hierarchies to be truly hierarchical (tree-like) rather than containing cycles.</p>\n<p>⚠️ <strong>Pitfall: In-Place Parameter Modification</strong></p>\n<p>Modifying parameters in-place during forward pass breaks gradient computation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.weight </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#6A737D\">  # WRONG: In-place modification breaks gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">@</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.weight.T</span></span></code></pre></div>\n\n<p>In-place modifications destroy the computation graph needed for backpropagation. Parameter updates should only happen through optimizers during the update step, never during forward pass.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building the data model requires careful attention to object relationships and memory management. The tensor, operation, and module hierarchies must work together seamlessly while maintaining clear separation of responsibilities.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Array Backend</td>\n<td>NumPy arrays with manual gradient tracking</td>\n<td>Custom array class with integrated autodiff</td>\n</tr>\n<tr>\n<td>Graph Storage</td>\n<td>Python lists and dictionaries</td>\n<td>Specialized graph data structures</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Python garbage collection with manual graph cleanup</td>\n<td>Weak references and custom memory pools</td>\n</tr>\n<tr>\n<td>Parameter Storage</td>\n<td>Python dictionaries with string keys</td>\n<td>Custom parameter containers with type safety</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>framework/\n  tensor.py              ← Core Tensor class with gradient tracking\n  operations.py          ← Operation base class and implementations (Add, Multiply, MatMul, etc.)\n  module.py             ← Module base class and parameter management\n  layers.py             ← Concrete layer implementations (Linear, Sequential, etc.)\n  functional.py         ← Functional versions of operations for advanced users\n  __init__.py           ← Public API exports\ntests/\n  test_tensor.py        ← Tensor creation, arithmetic, and gradient tests\n  test_operations.py    ← Forward/backward correctness for all operations\n  test_modules.py       ← Parameter collection and module composition tests</code></pre></div>\n\n<p><strong>Core Tensor Implementation (starter template):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, List, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tensor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"N-dimensional array with automatic differentiation support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 data: Union[np.ndarray, </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 grad_fn: Optional[</span><span style=\"color:#9ECBFF\">'Operation'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize tensor with data and gradient tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: Numerical data as NumPy array or convertible type</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            requires_grad: Whether to track gradients for this tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            grad_fn: Operation that created this tensor (None for leaf tensors)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert data to NumPy array if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set requires_grad flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize grad to None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store grad_fn reference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Cache shape and dtype from NumPy array</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.asarray() for robust data conversion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, gradient: Optional[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients by backpropagating through computation graph.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            gradient: Incoming gradient tensor (defaults to ones for scalar output)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Handle gradient=None case by creating ones tensor with same shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Accumulate gradient in self.grad (handle None case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If grad_fn exists, call its backward method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Propagate gradients to input tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use topological sort to ensure correct ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __add__</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Element-wise addition with gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Import Add operation class</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Add operation with self and other as inputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute forward result using operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create result tensor with appropriate grad_fn</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set requires_grad if either input requires gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement __mul__, __sub__, __truediv__, matmul methods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Follow same pattern as __add__ with appropriate operation classes</span></span></code></pre></div>\n\n<p><strong>Operation Base Class (complete implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all operations in computation graph.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store input tensors for gradient computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.inputs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inputs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">input_arrays: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute forward pass result.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input_arrays: NumPy arrays from input tensors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Result as NumPy array</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: np.ndarray) -> Tuple[np.ndarray, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients with respect to inputs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            grad_output: Gradient of loss with respect to operation output</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of gradients with respect to each input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Utility functions for gradient handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> unbroadcast_gradient</span><span style=\"color:#E1E4E8\">(grad: np.ndarray, target_shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Reduce broadcasted gradient back to target shape.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum over dimensions that were broadcasted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This is provided as complete utility since broadcasting is complex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # but not the main learning objective</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Handle scalar case</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> target_shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> ():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> np.sum(grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum over extra dimensions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ndim_extra </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.ndim </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(target_shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(ndim_extra):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sum(grad, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum over broadcasted dimensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (grad_dim, target_dim) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(grad.shape, target_shape)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> target_dim </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> grad_dim </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sum(grad, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">i, </span><span style=\"color:#FFAB70\">keepdims</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> grad</span></span></code></pre></div>\n\n<p><strong>Module Base Class (skeleton for implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Module</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all neural network modules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize module with empty parameter and submodule dictionaries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create empty dictionary for parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create empty dictionary for submodules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use descriptive names like _parameters and _modules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_parameter</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, param: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a tensor as a trainable parameter.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            name: Parameter name for identification</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            param: Tensor with requires_grad=True</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate that param is a Tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that param.requires_grad is True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store in parameters dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set as attribute on self for easy access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_module</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, module: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a submodule for recursive operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate that module is a Module instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store in modules dictionary  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set as attribute on self</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parameters</span><span style=\"color:#E1E4E8\">(self, recursive: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Collect all trainable parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            recursive: Whether to include submodule parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of all parameter tensors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start with parameters from this module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If recursive, iterate through submodules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Recursively call parameters() on each submodule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Combine all parameters into single list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use list comprehension or extend() for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Define module computation - must be implemented by subclasses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __call__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute forward pass with validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This is provided complete since it's infrastructure, not core learning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.forward(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs)</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the data model, verify correct behavior with these tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test 1: Tensor creation and basic properties</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> x.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> x.requires_grad </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> x.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 2: Operation creates computation graph</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> x</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> y.grad_fn </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(y.grad_fn, Add)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> y.grad_fn.inputs </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (x, x)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 3: Module parameter collection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">linear </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Linear(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">params </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> linear.parameters()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(params) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">  # weight and bias</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(p.requires_grad </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> params)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 4: Nested module hierarchy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Sequential([Linear(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">), ReLU(), Linear(</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">params </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.parameters()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(params) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">  # 2 linear layers × 2 parameters each</span></span></code></pre></div>\n\n<p>Expected output should show tensors with correct shapes, operations properly linked in computation graph, and parameters correctly collected from module hierarchy. Any assertion failures indicate issues with data structure implementation that must be fixed before proceeding to automatic differentiation.</p>\n<p><strong>Common Implementation Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Debug</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>grad_fn</code> is None after operations</td>\n<td>Operation not setting grad_fn on result tensor</td>\n<td>Print grad_fn after each operation</td>\n<td>Set <code>result.grad_fn = operation</code> in tensor arithmetic methods</td>\n</tr>\n<tr>\n<td>Parameters not found by optimizer</td>\n<td>Parameters not registered with module</td>\n<td>Print <code>module._parameters.keys()</code></td>\n<td>Call <code>self.register_parameter()</code> in module <code>__init__</code></td>\n</tr>\n<tr>\n<td>Shape errors during backward pass</td>\n<td>Broadcasting not handled in gradient computation</td>\n<td>Print tensor shapes before/after operations</td>\n<td>Implement <code>unbroadcast_gradient()</code> for all operations</td>\n</tr>\n<tr>\n<td>Circular import errors</td>\n<td>Tensor and Operation importing each other</td>\n<td>Check import structure</td>\n<td>Use string type hints and import at bottom of files</td>\n</tr>\n</tbody></table>\n<p>The data model serves as the foundation for all subsequent functionality. Solid implementation of these core data structures is essential for correct automatic differentiation and neural network training. Take time to thoroughly test and debug the tensor, operation, and module classes before proceeding to implement the automatic differentiation engine.</p>\n<h2 id=\"tensor-operations-layer-milestone-1\">Tensor Operations Layer (Milestone 1)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Tensor &amp; Operations) - implements the foundation tensor class with N-dimensional arrays, broadcasting, and basic arithmetic operations</p>\n</blockquote>\n<p>The tensor operations layer forms the bedrock of our neural network framework, providing the fundamental data structure and computational primitives that enable machine learning. This layer implements tensors as intelligent multi-dimensional arrays that not only store numerical data but also track their computational lineage for automatic differentiation. The challenge lies in creating tensors that seamlessly integrate NumPy-style operations with gradient tracking while maintaining clean, intuitive APIs that feel familiar to users of existing frameworks.</p>\n<h3 id=\"tensor-as-smart-arrays\">Tensor as Smart Arrays</h3>\n<p>Think of tensors as <strong>smart notebooks</strong> that not only contain your mathematical calculations but also remember exactly how each number was computed. Imagine you&#39;re working through a complex physics problem in a notebook - a regular notebook just shows your final answers, but a &quot;smart notebook&quot; would remember that &quot;this velocity came from dividing distance by time&quot; and &quot;this acceleration came from the derivative of velocity.&quot; When you later need to understand how changing the initial distance affects the final result, the smart notebook can trace backwards through all the computational steps automatically.</p>\n<p>This analogy captures the essence of what makes tensors &quot;smart&quot; compared to plain NumPy arrays. A <code>Tensor</code> wraps numerical data with additional metadata that enables automatic differentiation. The tensor remembers not just its current values, but also whether those values need gradients computed (<code>requires_grad</code>), what operation created it (<code>grad_fn</code>), and where to store the computed gradients (<code>grad</code>). This metadata transforms simple array operations into building blocks for neural network training.</p>\n<p>The mental model for tensor lifecycle follows three phases: <strong>creation, computation, and gradient flow</strong>. During creation, we initialize tensors with data and specify whether they participate in gradient computation. During the computation phase, operations between tensors automatically construct a computation graph by linking output tensors back to their input tensors through operation nodes. Finally, during gradient flow (backpropagation), this graph enables automatic computation of how changes to any tensor would affect the final loss.</p>\n<p>Consider a simple example to illustrate this smart behavior: when we multiply two tensors <code>a * b</code>, a regular NumPy operation just produces numerical results. However, a smart tensor operation produces results that &quot;remember&quot; they came from multiplication of <code>a</code> and <code>b</code>. Later, when gradients flow backwards, the multiplication operation can automatically compute that the gradient with respect to <code>a</code> should be multiplied by the values in <code>b</code>, and vice versa. This automatic bookkeeping eliminates the error-prone manual gradient calculations that plague traditional numerical optimization.</p>\n<p>The &quot;smartness&quot; extends to shape and broadcasting intelligence as well. Smart tensors automatically handle dimension mismatches through NumPy-compatible broadcasting while ensuring that gradients flow back to their original shapes correctly. This means users can write intuitive mathematical expressions like <code>tensor_2x3 + scalar</code> without worrying about the underlying shape manipulations, while the framework handles both forward computation and backward gradient unbroadcasting automatically.</p>\n<h3 id=\"tensor-api-design\">Tensor API Design</h3>\n<p>The <code>Tensor</code> class provides a comprehensive interface that balances mathematical expressiveness with gradient tracking capabilities. The API design follows the principle of <strong>least surprise</strong> - operations that work with NumPy arrays should work identically with tensors, while gradient-specific functionality remains opt-in through explicit flags and methods.</p>\n<p>The core tensor creation interface provides multiple pathways for instantiating tensors from different data sources:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Tensor(data, requires_grad=False)</code></td>\n<td><code>data: array_like, requires_grad: bool</code></td>\n<td><code>Tensor</code></td>\n<td>Primary constructor accepting NumPy arrays, lists, or scalars</td>\n</tr>\n<tr>\n<td><code>zeros(shape, requires_grad=False)</code></td>\n<td><code>shape: Tuple[int, ...], requires_grad: bool</code></td>\n<td><code>Tensor</code></td>\n<td>Create tensor filled with zeros in specified shape</td>\n</tr>\n<tr>\n<td><code>ones(shape, requires_grad=False)</code></td>\n<td><code>shape: Tuple[int, ...], requires_grad: bool</code></td>\n<td><code>Tensor</code></td>\n<td>Create tensor filled with ones in specified shape</td>\n</tr>\n<tr>\n<td><code>randn(shape, requires_grad=False)</code></td>\n<td><code>shape: Tuple[int, ...], requires_grad: bool</code></td>\n<td><code>Tensor</code></td>\n<td>Create tensor with random normal distribution values</td>\n</tr>\n<tr>\n<td><code>from_numpy(array, requires_grad=False)</code></td>\n<td><code>array: np.ndarray, requires_grad: bool</code></td>\n<td><code>Tensor</code></td>\n<td>Wrap existing NumPy array as tensor</td>\n</tr>\n</tbody></table>\n<p>The arithmetic operations interface leverages Python&#39;s operator overloading to provide natural mathematical syntax while maintaining gradient tracking:</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Addition</td>\n<td><code>__add__(other)</code></td>\n<td><code>other: Union[Tensor, Number]</code></td>\n<td><code>Tensor</code></td>\n<td>Element-wise addition with broadcasting</td>\n</tr>\n<tr>\n<td>Subtraction</td>\n<td><code>__sub__(other)</code></td>\n<td><code>other: Union[Tensor, Number]</code></td>\n<td><code>Tensor</code></td>\n<td>Element-wise subtraction with broadcasting</td>\n</tr>\n<tr>\n<td>Multiplication</td>\n<td><code>__mul__(other)</code></td>\n<td><code>other: Union[Tensor, Number]</code></td>\n<td><code>Tensor</code></td>\n<td>Element-wise multiplication with broadcasting</td>\n</tr>\n<tr>\n<td>Division</td>\n<td><code>__truediv__(other)</code></td>\n<td><code>other: Union[Tensor, Number]</code></td>\n<td><code>Tensor</code></td>\n<td>Element-wise division with broadcasting</td>\n</tr>\n<tr>\n<td>Matrix Multiply</td>\n<td><code>matmul(other)</code></td>\n<td><code>other: Tensor</code></td>\n<td><code>Tensor</code></td>\n<td>Matrix multiplication following Einstein notation</td>\n</tr>\n<tr>\n<td>Power</td>\n<td><code>__pow__(other)</code></td>\n<td><code>other: Union[Tensor, Number]</code></td>\n<td><code>Tensor</code></td>\n<td>Element-wise exponentiation</td>\n</tr>\n</tbody></table>\n<p>The shape manipulation and inspection interface provides essential tensor metadata access:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>shape</code></td>\n<td>Property</td>\n<td><code>Tuple[int, ...]</code></td>\n<td>Current tensor dimensions</td>\n</tr>\n<tr>\n<td><code>dtype</code></td>\n<td>Property</td>\n<td><code>np.dtype</code></td>\n<td>Data type of tensor elements</td>\n</tr>\n<tr>\n<td><code>ndim</code></td>\n<td>Property</td>\n<td><code>int</code></td>\n<td>Number of tensor dimensions</td>\n</tr>\n<tr>\n<td><code>size</code></td>\n<td>Property</td>\n<td><code>int</code></td>\n<td>Total number of elements</td>\n</tr>\n<tr>\n<td><code>reshape(shape)</code></td>\n<td><code>shape: Tuple[int, ...]</code></td>\n<td><code>Tensor</code></td>\n<td>Return tensor with new shape, same data</td>\n</tr>\n<tr>\n<td><code>transpose()</code></td>\n<td>None</td>\n<td><code>Tensor</code></td>\n<td>Return tensor with reversed dimension order</td>\n</tr>\n<tr>\n<td><code>squeeze(dim=None)</code></td>\n<td><code>dim: Optional[int]</code></td>\n<td><code>Tensor</code></td>\n<td>Remove dimensions of size 1</td>\n</tr>\n<tr>\n<td><code>unsqueeze(dim)</code></td>\n<td><code>dim: int</code></td>\n<td><code>Tensor</code></td>\n<td>Add dimension of size 1 at specified position</td>\n</tr>\n</tbody></table>\n<p>The gradient computation interface enables automatic differentiation integration:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>backward(gradient=None)</code></td>\n<td><code>gradient: Optional[Tensor]</code></td>\n<td><code>None</code></td>\n<td>Initiate backpropagation from this tensor</td>\n</tr>\n<tr>\n<td><code>detach()</code></td>\n<td>None</td>\n<td><code>Tensor</code></td>\n<td>Create new tensor sharing data but no gradient tracking</td>\n</tr>\n<tr>\n<td><code>requires_grad_(requires_grad)</code></td>\n<td><code>requires_grad: bool</code></td>\n<td><code>Tensor</code></td>\n<td>In-place modification of gradient tracking flag</td>\n</tr>\n<tr>\n<td><code>zero_grad()</code></td>\n<td>None</td>\n<td><code>None</code></td>\n<td>Reset accumulated gradients to zero</td>\n</tr>\n</tbody></table>\n<p>The tensor data access interface provides NumPy compatibility for inspection and conversion:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>numpy()</code></td>\n<td>None</td>\n<td><code>np.ndarray</code></td>\n<td>Return underlying NumPy array (detached from gradients)</td>\n</tr>\n<tr>\n<td><code>item()</code></td>\n<td>None</td>\n<td><code>Number</code></td>\n<td>Return single element as Python scalar (for 0-d tensors)</td>\n</tr>\n<tr>\n<td><code>tolist()</code></td>\n<td>None</td>\n<td><code>List</code></td>\n<td>Convert tensor to nested Python lists</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight: Operator Overloading Strategy</strong></p>\n<p>The decision to overload Python&#39;s arithmetic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>) rather than requiring explicit method calls (<code>add()</code>, <code>sub()</code>, etc.) prioritizes mathematical readability. Neural network code involves extensive mathematical expressions, and operator overloading makes these expressions more natural to read and write. The trade-off is slightly more complex implementation (handling both tensor-tensor and tensor-scalar operations), but the usability benefit justifies this complexity.</p>\n</blockquote>\n<h3 id=\"broadcasting-implementation\">Broadcasting Implementation</h3>\n<p>Broadcasting represents one of the most sophisticated aspects of tensor operations, automatically handling shape mismatches in arithmetic operations while preserving mathematical semantics and gradient flow. The implementation follows NumPy&#39;s broadcasting rules exactly, ensuring compatibility with existing numerical code while extending the mechanism to support gradient backpropagation.</p>\n<p>The <strong>broadcasting algorithm</strong> operates through a systematic shape alignment and expansion process:</p>\n<ol>\n<li><p><strong>Shape Alignment Phase</strong>: Align tensor shapes by padding shorter shapes with dimensions of size 1 on the left. For example, shapes <code>(3, 4)</code> and <code>(4,)</code> become <code>(3, 4)</code> and <code>(1, 4)</code> respectively.</p>\n</li>\n<li><p><strong>Compatibility Checking Phase</strong>: Verify that aligned dimensions are either equal or one of them is size 1. Dimensions <code>(3, 4)</code> and <code>(1, 4)</code> are compatible because the first dimension has sizes 3 and 1 (one is size 1), and the second dimension has matching sizes 4 and 4.</p>\n</li>\n<li><p><strong>Output Shape Determination Phase</strong>: The broadcasted shape takes the maximum size along each dimension. From <code>(3, 4)</code> and <code>(1, 4)</code>, the output shape becomes <code>(3, 4)</code> (max of 3 and 1, max of 4 and 4).</p>\n</li>\n<li><p><strong>Data Expansion Phase</strong>: Conceptually expand tensors to the broadcasted shape by repeating elements along dimensions of size 1. The tensor with shape <code>(1, 4)</code> gets repeated 3 times along the first dimension.</p>\n</li>\n<li><p><strong>Element-wise Operation Phase</strong>: Perform the requested operation (addition, multiplication, etc.) on the expanded tensors element by element.</p>\n</li>\n</ol>\n<p>The broadcasting shape computation can be implemented through this systematic comparison:</p>\n<table>\n<thead>\n<tr>\n<th>Input Shape A</th>\n<th>Input Shape B</th>\n<th>Aligned A</th>\n<th>Aligned B</th>\n<th>Compatible?</th>\n<th>Output Shape</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>(3, 4)</code></td>\n<td><code>(4,)</code></td>\n<td><code>(3, 4)</code></td>\n<td><code>(1, 4)</code></td>\n<td>✓</td>\n<td><code>(3, 4)</code></td>\n</tr>\n<tr>\n<td><code>(2, 1, 3)</code></td>\n<td><code>(1, 5, 1)</code></td>\n<td><code>(2, 1, 3)</code></td>\n<td><code>(1, 5, 1)</code></td>\n<td>✓</td>\n<td><code>(2, 5, 3)</code></td>\n</tr>\n<tr>\n<td><code>(3, 4)</code></td>\n<td><code>(2, 4)</code></td>\n<td><code>(3, 4)</code></td>\n<td><code>(2, 4)</code></td>\n<td>✗</td>\n<td>Error</td>\n</tr>\n<tr>\n<td><code>(5,)</code></td>\n<td><code>(3, 1)</code></td>\n<td><code>(1, 5)</code></td>\n<td><code>(3, 1)</code></td>\n<td>✓</td>\n<td><code>(3, 5)</code></td>\n</tr>\n</tbody></table>\n<p>The gradient unbroadcasting process reverses this expansion to ensure gradients flow back to their original tensor shapes. This involves identifying which dimensions were expanded during broadcasting and summing the gradients along those dimensions:</p>\n<ol>\n<li><p><strong>Expansion Detection Phase</strong>: Compare the original tensor shape with the broadcasted result shape to identify expanded dimensions.</p>\n</li>\n<li><p><strong>Summation Phase</strong>: Sum gradients along dimensions that were expanded from size 1 to larger sizes.</p>\n</li>\n<li><p><strong>Reshape Phase</strong>: Remove dimensions that were added during left-padding by reshaping to the original number of dimensions.</p>\n</li>\n</ol>\n<p>Consider gradient flow through a broadcast operation <code>a + b</code> where <code>a</code> has shape <code>(3, 4)</code> and <code>b</code> has shape <code>(4,)</code>. The forward pass broadcasts <code>b</code> to shape <code>(3, 4)</code> and performs element-wise addition. During backpropagation, the gradient with shape <code>(3, 4)</code> must be unbroadcast to match the original shapes: the gradient for <code>a</code> keeps shape <code>(3, 4)</code>, while the gradient for <code>b</code> gets summed along the first dimension to produce shape <code>(4,)</code>.</p>\n<blockquote>\n<p><strong>Critical Implementation Detail: Memory Efficiency</strong></p>\n<p>Actual broadcasting implementations avoid physically expanding arrays in memory. Instead, they use NumPy&#39;s stride manipulation to create views that behave as if the data were expanded. This saves enormous amounts of memory when broadcasting small tensors to large shapes. However, the conceptual model of expansion remains useful for understanding gradient flow.</p>\n</blockquote>\n<h3 id=\"tensor-design-decisions\">Tensor Design Decisions</h3>\n<p>The tensor implementation requires careful architectural decisions that balance performance, usability, and educational clarity. Each decision involves trade-offs between competing priorities, and understanding these trade-offs illuminates the complexity underlying seemingly simple tensor operations.</p>\n<blockquote>\n<p><strong>Decision: NumPy Backend for Data Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Need underlying numerical array implementation for tensor data storage and computation</li>\n<li><strong>Options Considered</strong>: Pure Python lists, custom C extension, NumPy arrays, PyTorch tensors as backend</li>\n<li><strong>Decision</strong>: Use NumPy arrays as the underlying data storage mechanism</li>\n<li><strong>Rationale</strong>: NumPy provides mature, optimized implementations of broadcasting, mathematical operations, and memory management. Building equivalent functionality from scratch would require substantial C/C++ development and optimization work. NumPy&#39;s broadcasting rules are the de facto standard that users expect. The educational goal focuses on automatic differentiation concepts rather than low-level array implementation.</li>\n<li><strong>Consequences</strong>: Inherits NumPy&#39;s performance characteristics and memory layout. Simplifies implementation by delegating numerical computation to NumPy. Limits control over memory allocation patterns and GPU integration options. Creates dependency on NumPy version compatibility.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Backend Option</th>\n<th>Performance</th>\n<th>Implementation Complexity</th>\n<th>Educational Value</th>\n<th>GPU Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NumPy arrays</td>\n<td>High</td>\n<td>Low</td>\n<td>High (focus on autodiff)</td>\n<td>External only</td>\n</tr>\n<tr>\n<td>Pure Python</td>\n<td>Very Low</td>\n<td>High</td>\n<td>Medium</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Custom C extension</td>\n<td>Very High</td>\n<td>Very High</td>\n<td>Low (distracted by C)</td>\n<td>Custom</td>\n</tr>\n<tr>\n<td>PyTorch backend</td>\n<td>Very High</td>\n<td>Medium</td>\n<td>Low (defeats purpose)</td>\n<td>Native</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Eager Execution Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Must choose between eager execution (operations execute immediately) vs lazy evaluation (operations build symbolic graphs for later execution)</li>\n<li><strong>Options Considered</strong>: Eager execution like PyTorch, lazy evaluation like TensorFlow 1.x, hybrid approach</li>\n<li><strong>Decision</strong>: Implement eager execution where operations execute immediately and build computation graphs dynamically</li>\n<li><strong>Rationale</strong>: Eager execution provides intuitive debugging experience since tensors contain actual values that can be inspected immediately. The define-by-run approach matches how users naturally think about mathematical operations. Educational clarity benefits from seeing immediate results rather than symbolic placeholders. Implementation complexity is lower without needing separate compilation and execution phases.</li>\n<li><strong>Consequences</strong>: Enables immediate value inspection and debugging. Requires computation graph construction during forward pass. May have slightly higher memory usage due to storing intermediate results. Limits some optimization opportunities available in static graph systems.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Reference-Based Gradient Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to decide how to store and manage gradient information for each tensor</li>\n<li><strong>Options Considered</strong>: Store gradients directly in tensors, separate gradient dictionary, gradient tensors as separate objects</li>\n<li><strong>Decision</strong>: Store gradients as optional <code>Tensor</code> references within each tensor object</li>\n<li><strong>Rationale</strong>: Direct storage provides intuitive access pattern (<code>tensor.grad</code>) matching PyTorch conventions. Simplifies gradient accumulation since each tensor owns its gradient storage. Enables automatic gradient initialization and management. Memory overhead only affects tensors that actually require gradients.</li>\n<li><strong>Consequences</strong>: Each tensor carries gradient storage overhead even when not needed. Simplifies API design with direct <code>.grad</code> attribute access. Creates potential for circular references if not managed carefully. Enables straightforward gradient accumulation semantics.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Immutable Tensor Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Must decide whether arithmetic operations modify existing tensors (in-place) or create new tensors</li>\n<li><strong>Options Considered</strong>: All operations in-place for memory efficiency, all operations create new tensors, mixed approach with explicit in-place variants</li>\n<li><strong>Decision</strong>: Implement all basic arithmetic operations as immutable (creating new tensors) with optional in-place variants</li>\n<li><strong>Rationale</strong>: Immutable operations prevent accidental modification of tensors that might be referenced elsewhere in the computation graph. Simplifies reasoning about gradient flow since intermediate results remain stable. Matches functional programming principles and mathematical intuition where <code>a + b</code> doesn&#39;t modify <code>a</code> or <code>b</code>. In-place operations can break gradient computation if not handled carefully.</li>\n<li><strong>Consequences</strong>: Higher memory usage due to creating intermediate tensors. Clearer semantics for gradient computation. May require explicit memory management in memory-constrained scenarios. Provides predictable behavior that matches mathematical expectations.</li>\n</ul>\n</blockquote>\n<p>The data type handling strategy follows NumPy&#39;s conventions while adding gradient-specific considerations:</p>\n<table>\n<thead>\n<tr>\n<th>Data Type</th>\n<th>Use Case</th>\n<th>Gradient Support</th>\n<th>Memory per Element</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>float32</code></td>\n<td>Default for neural networks</td>\n<td>Yes</td>\n<td>4 bytes</td>\n</tr>\n<tr>\n<td><code>float64</code></td>\n<td>High-precision computations</td>\n<td>Yes</td>\n<td>8 bytes</td>\n</tr>\n<tr>\n<td><code>int32</code></td>\n<td>Indices and discrete values</td>\n<td>No</td>\n<td>4 bytes</td>\n</tr>\n<tr>\n<td><code>int64</code></td>\n<td>Large integer indices</td>\n<td>No</td>\n<td>8 bytes</td>\n</tr>\n<tr>\n<td><code>bool</code></td>\n<td>Masks and conditions</td>\n<td>No</td>\n<td>1 byte</td>\n</tr>\n<tr>\n<td><code>complex64</code></td>\n<td>Complex number computations</td>\n<td>Limited</td>\n<td>8 bytes</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-tensor-implementation-pitfalls\">Common Tensor Implementation Pitfalls</h3>\n<p>Implementing tensors correctly requires navigating several subtle pitfalls that frequently trap developers building neural network frameworks. These pitfalls often manifest as hard-to-debug issues during training, making awareness and prevention crucial for successful implementation.</p>\n<p>⚠️ <strong>Pitfall: In-Place Operations Breaking Gradient Flow</strong></p>\n<p>The most dangerous pitfall involves in-place modifications of tensors that participate in gradient computation. When a tensor&#39;s values change after being used in the computation graph, the stored references become invalid, leading to incorrect gradient calculations.</p>\n<p>Consider this problematic sequence: create tensor <code>a</code>, compute <code>b = a * 2</code>, then modify <code>a</code> in-place with <code>a += 1</code>. When gradients flow back through the multiplication operation, the gradient computation tries to access the original values of <code>a</code>, but finds the modified values instead. This produces incorrect gradients that can cause training divergence or mysterious convergence failures.</p>\n<p>The fix involves strict separation between tensors used in gradient computation and those being modified. Implement safeguards that detect when in-place operations would affect tensors with <code>requires_grad=True</code> or tensors referenced in active computation graphs. Provide clear error messages that identify the problematic operation and suggest alternatives like <code>a = a + 1</code> instead of <code>a += 1</code>.</p>\n<p>⚠️ <strong>Pitfall: Broadcasting Gradient Shape Mismatches</strong></p>\n<p>Broadcasting operations create subtle gradient flow issues when the backward pass attempts to return gradients to their original shapes. The forward pass successfully broadcasts tensors to compatible shapes, but the backward pass must carefully unbroadcast gradients to match the input tensor shapes.</p>\n<p>This manifests when broadcasting a scalar to a matrix shape during forward pass, then trying to assign a matrix-shaped gradient back to the scalar tensor during backward pass. The shapes are fundamentally incompatible, causing either runtime errors or silent gradient corruption.</p>\n<p>The solution requires implementing robust <code>unbroadcast_gradient</code> functionality that identifies which dimensions were expanded during broadcasting and appropriately sums gradients along those dimensions. Every arithmetic operation must store sufficient metadata about the original input shapes to enable correct gradient unbroadcasting.</p>\n<p>⚠️ <strong>Pitfall: Memory Aliasing in Tensor Construction</strong></p>\n<p>Memory aliasing occurs when multiple tensors share the same underlying NumPy array storage without proper isolation. Modifications to one tensor unexpectedly affect other tensors, creating confusing bugs where seemingly independent operations interfere with each other.</p>\n<p>This typically happens when tensors are created by slicing existing tensors or when NumPy arrays are reused across multiple tensor objects. The issue becomes particularly problematic during gradient computation, where gradient updates to one tensor may corrupt gradients for other tensors sharing the same memory.</p>\n<p>Prevention requires careful copying of data during tensor construction and explicit checks for memory sharing. Implement tensor creation methods that default to copying data unless sharing is explicitly requested. Provide debugging utilities that can detect memory aliasing and warn users about potentially problematic sharing patterns.</p>\n<p>⚠️ <strong>Pitfall: Gradient Accumulation Race Conditions</strong></p>\n<p>In complex computation graphs where the same tensor is used multiple times, gradients must be accumulated (summed) rather than overwritten. Failing to implement proper gradient accumulation leads to lost gradient information and incorrect parameter updates.</p>\n<p>This occurs when a tensor appears in multiple branches of a computation graph, such as <code>c = a + a</code> where tensor <code>a</code> is used twice. During backpropagation, both uses of <code>a</code> contribute gradients, and these contributions must be summed together. If the second gradient overwrites the first instead of adding to it, half the gradient information disappears.</p>\n<p>The solution involves implementing gradient accumulation logic that detects when gradients already exist and adds new contributions rather than replacing them. Initialize gradients to zero rather than leaving them uninitialized, and ensure all gradient assignment operations use addition semantics.</p>\n<p>⚠️ <strong>Pitfall: Dtype Promotion Confusion</strong></p>\n<p>Arithmetic operations between tensors with different data types require careful type promotion to maintain numerical precision and prevent unexpected behavior. NumPy&#39;s automatic type promotion rules can be complex and counterintuitive, leading to precision loss or memory usage increases.</p>\n<p>For example, operations between <code>float32</code> and <code>float64</code> tensors typically promote results to <code>float64</code>, doubling memory usage unexpectedly. Operations between integer and floating-point tensors may produce different results than expected due to precision differences in the promotion process.</p>\n<p>Address this by implementing explicit type promotion policies that match user expectations and provide clear documentation about type conversion behavior. Consider defaulting to <code>float32</code> for neural network applications where double precision is rarely needed, but provide mechanisms for users to control type promotion explicitly when required.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building robust tensors requires careful technology choices and systematic implementation of the core data structures and operations. This guidance provides complete starter infrastructure and skeleton code for implementing the tensor layer while maintaining focus on the educational objectives.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Array Backend</td>\n<td>NumPy arrays (recommended)</td>\n<td>Custom C extension with Python bindings</td>\n</tr>\n<tr>\n<td>Data Types</td>\n<td>NumPy dtypes (float32, int32, bool)</td>\n<td>Extended precision types and custom numeric types</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Python garbage collection</td>\n<td>Manual memory pools and allocation tracking</td>\n</tr>\n<tr>\n<td>Broadcasting</td>\n<td>NumPy broadcast_arrays function</td>\n<td>Custom broadcasting with optimized stride patterns</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Python exceptions with descriptive messages</td>\n<td>Structured error codes with recovery suggestions</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>NumPy testing utilities (assert_allclose)</td>\n<td>Property-based testing with Hypothesis</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The tensor operations layer should be organized to separate core tensor functionality from operation implementations and testing utilities:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n├── tensor/\n│   ├── __init__.py              ← Public tensor API exports\n│   ├── tensor.py                ← Core Tensor class implementation\n│   ├── operations.py            ← Operation base class and arithmetic ops\n│   ├── creation.py              ← Tensor creation utilities (zeros, ones, randn)\n│   ├── broadcasting.py          ← Broadcasting utilities and shape manipulation\n│   └── testing_utils.py         ← Gradient checking and numerical testing\n├── tests/\n│   ├── test_tensor.py           ← Core tensor functionality tests\n│   ├── test_operations.py       ← Arithmetic operation tests\n│   ├── test_broadcasting.py     ← Broadcasting behavior tests\n│   └── test_gradients.py        ← Gradient correctness tests\n└── examples/\n    ├── tensor_basics.py         ← Simple tensor usage examples\n    └── gradient_flow.py         ← Autodiff demonstration</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>File: tensor/broadcasting.py</strong> (Complete implementation for learner use)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Broadcasting utilities for tensor operations.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">This module provides complete broadcasting functionality that learners can use</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">without implementation. Focus remains on tensor and autodiff concepts.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> broadcast_shapes</span><span style=\"color:#E1E4E8\">(shape1: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], shape2: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compute the broadcasted shape for two input shapes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        shape1: Shape of first tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        shape2: Shape of second tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Broadcasted output shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: If shapes are not broadcastable</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Align shapes by prepending 1s to shorter shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(shape1), </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(shape2))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    aligned_shape1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (max_len </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(shape1)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> shape1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    aligned_shape2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (max_len </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(shape2)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> shape2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check compatibility and compute output shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> dim1, dim2 </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> zip</span><span style=\"color:#E1E4E8\">(aligned_shape1, aligned_shape2):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> dim1 </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            output_shape.append(dim2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> dim2 </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            output_shape.append(dim1)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> dim1 </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> dim2:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            output_shape.append(dim1)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot broadcast shapes </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> and </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> tuple</span><span style=\"color:#E1E4E8\">(output_shape)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> unbroadcast_gradient</span><span style=\"color:#E1E4E8\">(grad: np.ndarray, original_shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Reduce gradient to original tensor shape by summing over broadcasted dimensions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        grad: Gradient array with broadcasted shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        original_shape: Target shape to reduce gradient to</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Gradient array reduced to original shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Handle scalar case</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(original_shape) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> np.sum(grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Determine how many dimensions were prepended</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ndim_added </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.ndim </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(original_shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum over prepended dimensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(ndim_added):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.sum(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sum over dimensions that were size 1 in original</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (grad_dim, orig_dim) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(grad.shape, original_shape)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> orig_dim </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> grad_dim </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.sum(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">i, </span><span style=\"color:#FFAB70\">keepdims</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> grad.reshape(original_shape)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_broadcastable</span><span style=\"color:#E1E4E8\">(shape1: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], shape2: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if two shapes are broadcastable without computing result.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        shape1: Shape of first tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        shape2: Shape of second tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        True if shapes are broadcastable, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        broadcast_shapes(shape1, shape2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>File: tensor/testing_utils.py</strong> (Complete implementation for learner use)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Testing utilities for gradient checking and numerical verification.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> numerical_gradient</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">], h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> List[np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compute numerical gradients using finite differences.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        f: Function that takes tensors and returns scalar tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        inputs: List of input tensors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        h: Step size for finite differences</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of numerical gradient arrays, one per input tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradients </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(inputs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros_like(tensor.data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flat_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor.data.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flat_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> j </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(flat_data)):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Forward step</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            old_val </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_data[j]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_data[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_val </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_plus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs).data.item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Backward step</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_data[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_val </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_minus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs).data.item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute gradient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_grad[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (f_plus </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> f_minus) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Restore original value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_data[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_val</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradients.append(grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradients</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">], tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compare automatic differentiation gradients with numerical gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        f: Function that takes tensors and returns scalar tensor  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        inputs: List of input tensors (must have requires_grad=True)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tolerance: Maximum allowed difference between gradients</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        True if gradients match within tolerance, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute automatic gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> inp.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            inp.grad.data.fill(</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Zero existing gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auto_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [inp.grad.data </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute numerical gradients  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numerical_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> numerical_gradient(f, inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compare gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> auto_grad, num_grad </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> zip</span><span style=\"color:#E1E4E8\">(auto_grads, numerical_grads):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.allclose(auto_grad, num_grad, </span><span style=\"color:#FFAB70\">atol</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tolerance):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            max_diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.max(np.abs(auto_grad </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> num_grad))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient check failed! Max difference: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">max_diff</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Automatic gradient:</span><span style=\"color:#79B8FF\">\\n{</span><span style=\"color:#E1E4E8\">auto_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Numerical gradient:</span><span style=\"color:#79B8FF\">\\n{</span><span style=\"color:#E1E4E8\">num_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>File: tensor/tensor.py</strong> (Skeleton for learner implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Core Tensor implementation with gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, Union, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .broadcasting </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> broadcast_shapes, unbroadcast_gradient</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tensor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"N-dimensional array with automatic differentiation support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data: Union[np.ndarray, </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 grad_fn: Optional[</span><span style=\"color:#9ECBFF\">'Operation'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize tensor with data and gradient tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: Numerical data (array, list, or scalar)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            requires_grad: Whether to track gradients for this tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            grad_fn: Operation that created this tensor (for autodiff)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert data to NumPy array if not already</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store data, requires_grad flag, and grad_fn</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize grad to None (will be created when needed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store shape and dtype properties from data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.asarray() to convert various inputs to arrays</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> shape</span><span style=\"color:#E1E4E8\">(self) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return shape of tensor data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return shape tuple from self.data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> dtype</span><span style=\"color:#E1E4E8\">(self) -> np.dtype:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return data type of tensor elements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return dtype from self.data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> ndim</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return number of dimensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return ndim from self.data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> size</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return total number of elements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return size from self.data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __add__</span><span style=\"color:#E1E4E8\">(self, other: Union[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Element-wise addition with broadcasting and gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert other to Tensor if it's a scalar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use broadcast_shapes to compute output shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Perform NumPy addition: result_data = self.data + other.data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Determine if result requires gradients (either input requires grad)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create Add operation if gradients needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return new Tensor with result data and gradient tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: from .operations import Add</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __mul__</span><span style=\"color:#E1E4E8\">(self, other: Union[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Element-wise multiplication with broadcasting and gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert other to Tensor if it's a scalar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use broadcast_shapes to compute output shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Perform NumPy multiplication: result_data = self.data * other.data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Determine if result requires gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create Multiply operation if gradients needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return new Tensor with result and gradient tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> matmul</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Matrix multiplication with gradient tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that shapes are compatible for matrix multiplication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Perform NumPy matmul: result_data = np.matmul(self.data, other.data)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Determine if result requires gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create MatMul operation if gradients needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return new Tensor with result and gradient tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.matmul for the actual computation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, gradient: Optional[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initiate backpropagation from this tensor.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            gradient: Gradient to backpropagate (defaults to ones_like)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If gradient not provided, create tensor of ones with same shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call _backward method to perform recursive backpropagation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle case where tensor doesn't require gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This is the public entry point that sets up the initial gradient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _backward</span><span style=\"color:#E1E4E8\">(self, gradient: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Internal recursive gradient computation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            gradient: Gradient flowing back to this tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Accumulate gradient into self.grad (create if doesn't exist)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If this tensor has grad_fn, call its backward method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle gradient accumulation (sum if grad already exists)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use += for gradient accumulation, create new Tensor if grad is None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detach</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new tensor sharing data but not requiring gradients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create new Tensor with same data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set requires_grad=False and grad_fn=None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return detached tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This breaks gradient tracking while sharing memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> numpy</span><span style=\"color:#E1E4E8\">(self) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return data as NumPy array (detached from gradients).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return self.data (already a NumPy array)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"String representation for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\", requires_grad=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.requires_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.requires_grad </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Tensor(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.data</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">grad_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Tensor creation utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> zeros</span><span style=\"color:#E1E4E8\">(shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create tensor filled with zeros.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use np.zeros to create data, return Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> ones</span><span style=\"color:#E1E4E8\">(shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create tensor filled with ones.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use np.ones to create data, return Tensor  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> randn</span><span style=\"color:#E1E4E8\">(shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], requires_grad: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create tensor with random normal distribution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use np.random.randn to create data, return Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>File: tensor/operations.py</strong> (Skeleton for learner implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Operation base class and arithmetic operations for automatic differentiation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .broadcasting </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> unbroadcast_gradient</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for differentiable operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, inputs: Tuple[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store input tensors for gradient computation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            inputs: Tuple of input tensors that created this operation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store inputs tuple for backward pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute forward pass result.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            *inputs: Input arrays</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output array</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients and propagate to input tensors.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            grad_output: Gradient flowing back from output</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Element-wise addition operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute a + b with broadcasting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return a + b (NumPy handles broadcasting automatically)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients for addition inputs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get original shapes of both input tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Gradient w.r.t. first input is grad_output (identity function)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Gradient w.r.t. second input is also grad_output  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Use unbroadcast_gradient to reduce gradients to original shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Call _backward on both input tensors with their gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Addition gradient is just pass-through, but must handle broadcasting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Multiply</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Element-wise multiplication operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute a * b with broadcasting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return a * b</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients for multiplication inputs.\"\"\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get data from both input tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Gradient w.r.t. first input is grad_output * second_input_data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Gradient w.r.t. second input is grad_output * first_input_data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Use unbroadcast_gradient to handle broadcasting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Call _backward on both input tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Multiplication rule: d(a*b)/da = b, d(a*b)/db = a</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MatMul</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Matrix multiplication operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute matrix multiplication a @ b.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return np.matmul(a, b)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients for matrix multiplication.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get data from both input tensors  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Gradient w.r.t. first input: grad_output @ second_input.T</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Gradient w.r.t. second input: first_input.T @ grad_output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle transpose for different tensor dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Call _backward on both input tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Matrix multiplication chain rule involves transposes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the tensor operations layer, verify correct behavior with these checkpoints:</p>\n<p><strong>Basic Tensor Operations Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run this test to verify tensor creation and arithmetic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor, zeros, ones</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test tensor creation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">4.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">6.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> zeros((</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test arithmetic operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> b  </span><span style=\"color:#6A737D\"># Should be [5.0, 7.0, 9.0]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">product </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> b  </span><span style=\"color:#6A737D\"># Should be [4.0, 10.0, 18.0]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Addition result: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Multiplication result: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">product</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Shapes preserved: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> a.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Broadcasting Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test broadcasting behavior</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scalar </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">vector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">matrix </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([[</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4.0</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># These should work with broadcasting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scalar_vector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scalar </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> vector  </span><span style=\"color:#6A737D\"># Shape: (3,)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scalar_matrix </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scalar </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> matrix  </span><span style=\"color:#6A737D\"># Shape: (2, 2)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Scalar + vector: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">scalar_vector</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Scalar * matrix: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">scalar_matrix</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Expected behavior indicators:</strong></p>\n<ul>\n<li>Tensor creation produces objects with correct shape and dtype attributes</li>\n<li>Arithmetic operations return new Tensor objects (not NumPy arrays)  </li>\n<li>Broadcasting operations produce correctly shaped results</li>\n<li><code>requires_grad</code> flag propagates correctly to operation results</li>\n<li>Operations between tensors and scalars work without explicit conversion</li>\n</ul>\n<p><strong>Signs something is wrong:</strong></p>\n<ul>\n<li>Operations return NumPy arrays instead of Tensor objects</li>\n<li>Shape mismatches cause crashes instead of broadcasting</li>\n<li>Gradient tracking flags are not preserved through operations</li>\n<li>Scalar operations require manual tensor conversion</li>\n</ul>\n<h2 id=\"automatic-differentiation-engine-milestone-2\">Automatic Differentiation Engine (Milestone 2)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (Automatic Differentiation) - implements reverse-mode automatic differentiation with computation graph construction and gradient backpropagation</p>\n</blockquote>\n<p>The automatic differentiation engine is the beating heart of any modern neural network framework. While tensors provide the data structures and operations provide the computations, the autodiff engine transforms these simple building blocks into a powerful gradient computation system that can automatically compute derivatives of arbitrarily complex functions. This capability is what makes training deep neural networks practical—without it, we would need to manually derive and implement gradients for every possible combination of operations, which becomes intractable for networks with millions of parameters.</p>\n<p>The autodiff engine implements <strong>reverse-mode automatic differentiation</strong>, also known as backpropagation, which efficiently computes gradients by traversing the computation graph in reverse topological order. Unlike numerical differentiation (which approximates derivatives using finite differences) or symbolic differentiation (which manipulates algebraic expressions), automatic differentiation computes exact derivatives to machine precision by mechanically applying the chain rule during program execution.</p>\n<h3 id=\"the-assembly-line-metaphor\">The Assembly Line Metaphor</h3>\n<p>Think of the automatic differentiation engine as a sophisticated factory assembly line that can run in reverse. During the <strong>forward pass</strong>, raw materials (input tensors) move through a series of workstations (operations like addition, multiplication, matrix multiplication), with each station transforming the materials and passing them to the next station. The final product emerges at the end of the line (the output tensor).</p>\n<p>What makes this factory special is that it meticulously records every transformation that happens at each workstation. It tracks which raw materials entered each station, what operations were performed, and where the processed materials went next. This creates a detailed <strong>computation graph</strong>—essentially a blueprint of the entire assembly process.</p>\n<p>During the <strong>backward pass</strong>, the factory runs in reverse. Starting from the final product, we trace back through each workstation to determine how much each raw material contributed to the final result. If we want to know how changing the input by a small amount would affect the output (the gradient), we can propagate this information backwards through the assembly line, accumulating the contributions from each workstation using the chain rule.</p>\n<p>The key insight is that each workstation (operation) only needs to know how to compute its own local gradients—how its inputs affect its outputs. The autodiff engine handles the global coordination, ensuring that gradients flow backwards through the entire network in the correct order and that contributions from multiple paths are properly accumulated.</p>\n<p>This metaphor captures several critical aspects of automatic differentiation: the forward pass builds the computation graph (records the assembly process), the backward pass traverses it in reverse (traces contributions backwards), and each operation contributes its local gradient information (each workstation knows its own transformation rules).</p>\n<h3 id=\"forward-pass-graph-building\">Forward Pass Graph Building</h3>\n<p>The forward pass is where the magic of dynamic computation graph construction happens. Unlike static graph frameworks that require you to define the entire network structure upfront, our framework uses a <strong>define-by-run</strong> approach where the computation graph is built dynamically as operations execute. This provides tremendous flexibility—the graph can change based on control flow, input data, or runtime conditions.</p>\n<p>When a tensor operation executes (like <code>c = a + b</code>), three things happen simultaneously: the numerical computation occurs, a new tensor is created to hold the result, and an edge is added to the computation graph. The graph construction is entirely automatic and transparent to the user—they simply write natural-looking mathematical expressions, and the framework handles all the bookkeeping required for later gradient computation.</p>\n<p>The computation graph is a <strong>directed acyclic graph (DAG)</strong> where nodes represent either tensors (data) or operations (computations). Tensor nodes store the actual numerical data and metadata like shape and gradient requirements. Operation nodes store references to their input tensors, the function used to compute the output, and most importantly, the gradient function needed for backpropagation.</p>\n<p>Every tensor that participates in gradient computation carries a <code>grad_fn</code> field that points to the operation that created it. This creates a chain of references that allows the autodiff engine to traverse backwards from any tensor to discover all the operations that contributed to computing it. Leaf tensors (like parameters and inputs) have <code>grad_fn = None</code> since they weren&#39;t created by any operation—they&#39;re the starting points of the computation.</p>\n<p>The graph building process is eager and immediate. As soon as you execute <code>c = a.matmul(b)</code>, the matrix multiplication happens, the result tensor <code>c</code> is created with the computed values, and <code>c.grad_fn</code> is set to point to a <code>MatMul</code> operation node that remembers <code>a</code> and <code>b</code> as its inputs. This means the computation graph always reflects the exact sequence of operations that were actually executed.</p>\n<p>Here&#39;s how the core tensor operations participate in graph construction:</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Graph Node Created</th>\n<th>Inputs Stored</th>\n<th>Gradient Function</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>a + b</code></td>\n<td><code>Add</code> operation</td>\n<td>References to tensors <code>a</code> and <code>b</code></td>\n<td>Computes gradients w.r.t both inputs</td>\n</tr>\n<tr>\n<td><code>a * b</code></td>\n<td><code>Multiply</code> operation</td>\n<td>References to tensors <code>a</code> and <code>b</code></td>\n<td>Implements product rule derivatives</td>\n</tr>\n<tr>\n<td><code>a.matmul(b)</code></td>\n<td><code>MatMul</code> operation</td>\n<td>References to tensors <code>a</code> and <code>b</code></td>\n<td>Handles matrix multiplication gradients</td>\n</tr>\n<tr>\n<td><code>a.sum()</code></td>\n<td><code>Sum</code> operation</td>\n<td>Reference to tensor <code>a</code> and reduction axes</td>\n<td>Broadcasts gradient back to original shape</td>\n</tr>\n</tbody></table>\n<p>The graph construction must handle several important details to support correct gradient computation. First, it preserves the exact tensor shapes at each operation, since gradients must flow back through the same shape transformations in reverse. Second, it maintains references to the original input tensors (not copies), ensuring that gradients accumulate in the correct locations. Third, it tracks whether each tensor requires gradients, allowing optimization by skipping gradient computation for tensors that don&#39;t need it.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The forward pass serves dual purposes—it computes the actual function values needed by the application AND builds the data structure needed for gradient computation. This dual purpose is what makes automatic differentiation so powerful—gradient computation comes &quot;for free&quot; with any forward computation.</p>\n</blockquote>\n<h3 id=\"reverse-mode-differentiation-algorithm\">Reverse-Mode Differentiation Algorithm</h3>\n<p>The reverse-mode differentiation algorithm is the core of the autodiff engine, implementing the systematic application of the chain rule to compute gradients efficiently. The algorithm operates in two phases: topological ordering of the computation graph and gradient propagation in reverse topological order.</p>\n<p><strong>Phase 1: Topological Sort</strong></p>\n<p>Before gradients can be computed, the autodiff engine must determine the correct order for processing nodes during the backward pass. This requires a topological sort of the computation graph, which orders nodes such that every node appears before any nodes that depend on it. In the context of gradient computation, this means processing nodes in reverse order of their creation during the forward pass.</p>\n<p>The topological sort is essential because gradients must be computed in dependency order. If tensor <code>c</code> depends on tensor <code>b</code>, and tensor <code>b</code> depends on tensor <code>a</code>, then we must compute gradients with respect to <code>c</code> before we can compute gradients with respect to <code>b</code>, and gradients with respect to <code>b</code> before gradients with respect to <code>a</code>. Processing nodes in the wrong order would mean trying to use gradient information that hasn&#39;t been computed yet.</p>\n<p>The algorithm for topological sort in our context works as follows:</p>\n<ol>\n<li>Start from the output tensor (where <code>backward()</code> was called) and perform a depth-first traversal of the computation graph</li>\n<li>Visit each operation node by following <code>grad_fn</code> pointers from tensors to operations and from operations to their input tensors</li>\n<li>Mark nodes as visited to handle cases where the same tensor is used multiple times in the computation</li>\n<li>Build a list of operation nodes ordered by their &quot;finish time&quot; in the depth-first traversal</li>\n<li>Reverse this list to get the correct processing order for the backward pass</li>\n</ol>\n<p>This produces a topologically sorted list where operations are ordered from the output backwards toward the inputs, ensuring that when we process each operation, all operations that depend on its outputs have already been processed.</p>\n<p><strong>Phase 2: Gradient Propagation</strong></p>\n<p>Once the topological order is established, gradient propagation proceeds by visiting each operation node in order and computing the gradients of its inputs based on the gradient of its output. This systematically applies the chain rule: if we know how the final loss depends on an operation&#39;s output, we can compute how it depends on the operation&#39;s inputs using the operation&#39;s local gradient function.</p>\n<p>The gradient propagation algorithm follows these steps:</p>\n<ol>\n<li>Initialize the gradient of the starting tensor (usually the loss) to a tensor of ones with the same shape</li>\n<li>For each operation in topologically sorted order:<ul>\n<li>Retrieve the accumulated gradient with respect to the operation&#39;s output</li>\n<li>Call the operation&#39;s <code>backward()</code> method to compute gradients with respect to its inputs</li>\n<li>For each input tensor that requires gradients, accumulate the computed gradient</li>\n</ul>\n</li>\n<li>Continue until all operations have been processed</li>\n</ol>\n<p>Each operation&#39;s <code>backward()</code> method implements the local gradient computation for that specific operation type. For example:</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Local Gradient Computation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Add</code></td>\n<td>Gradient w.r.t both inputs is the same as output gradient (derivative of <code>a + b</code> is 1 w.r.t both <code>a</code> and <code>b</code>)</td>\n</tr>\n<tr>\n<td><code>Multiply</code></td>\n<td>Gradient w.r.t first input is output gradient times second input; gradient w.r.t second input is output gradient times first input</td>\n</tr>\n<tr>\n<td><code>MatMul</code></td>\n<td>Gradient w.r.t first input is output gradient matrix-multiplied by transpose of second input</td>\n</tr>\n<tr>\n<td><code>Sum</code></td>\n<td>Gradient is broadcasted from reduced shape back to original input shape</td>\n</tr>\n</tbody></table>\n<p>The beauty of this approach is that each operation only needs to know its own local derivatives. The autodiff engine handles the global coordination, ensuring that gradients flow backwards through the entire network correctly.</p>\n<blockquote>\n<p><strong>Chain Rule in Action</strong>: The reverse-mode algorithm is essentially a mechanical application of the multivariate chain rule. If we have a composition of functions <code>f(g(h(x)))</code>, the derivative is <code>f&#39;(g(h(x))) * g&#39;(h(x)) * h&#39;(x)</code>. The backward pass computes these derivatives from right to left, accumulating the product as it goes.</p>\n</blockquote>\n<h3 id=\"gradient-accumulation-strategy\">Gradient Accumulation Strategy</h3>\n<p>One of the most subtle aspects of automatic differentiation is handling cases where the same tensor appears multiple times in a computation. When a tensor contributes to the output through multiple paths in the computation graph, its total gradient is the sum of contributions from all paths. This is a direct consequence of the multivariate chain rule—if <code>y = f(x) + g(x)</code>, then <code>dy/dx = df/dx + dg/dx</code>.</p>\n<p>Consider a simple example where we compute <code>y = x * x + x * 2</code>. The tensor <code>x</code> appears three times in this expression: twice in the first term and once in the second term. During the forward pass, this creates a computation graph where <code>x</code> has multiple outgoing edges. During the backward pass, gradient contributions will flow back through each of these edges, and they must be summed to get the total gradient with respect to <code>x</code>.</p>\n<p>The gradient accumulation strategy must handle several scenarios:</p>\n<p><strong>Scenario 1: Direct Multiple Usage</strong>\nWhen a tensor is used directly in multiple operations, like <code>y = a + a</code>, the gradient with respect to <code>a</code> is the sum of contributions from both addition operands. The <code>Add</code> operation computes gradients with respect to both its inputs, and both gradients (which happen to be identical in this case) must be accumulated into <code>a.grad</code>.</p>\n<p><strong>Scenario 2: Indirect Multiple Usage</strong>\nMore complex cases arise when a tensor influences the output through multiple intermediate computations. For example, in <code>b = a * 2; c = a + 1; y = b + c</code>, the tensor <code>a</code> influences <code>y</code> through both the <code>b</code> path and the <code>c</code> path. The gradients must be accumulated from both paths.</p>\n<p><strong>Scenario 3: Loop and Control Flow Usage</strong>\nIn dynamic graphs, the same tensor might be used in loops or conditional branches, creating multiple gradient contributions that must be accumulated. This is particularly important for recurrent neural networks where the same parameters are used at multiple time steps.</p>\n<p>The gradient accumulation implementation follows these principles:</p>\n<table>\n<thead>\n<tr>\n<th>Accumulation Rule</th>\n<th>Implementation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>First gradient contribution</td>\n<td>Initialize <code>tensor.grad = computed_gradient</code></td>\n</tr>\n<tr>\n<td>Subsequent contributions</td>\n<td>Update <code>tensor.grad += computed_gradient</code></td>\n</tr>\n<tr>\n<td>Shape compatibility</td>\n<td>Ensure accumulated gradients maintain correct tensor shapes</td>\n</tr>\n<tr>\n<td>Memory management</td>\n<td>Avoid creating unnecessary intermediate gradient tensors</td>\n</tr>\n</tbody></table>\n<p>The accumulation happens automatically during the backward pass. When an operation computes gradients with respect to its inputs, it checks whether each input tensor already has accumulated gradients. If the <code>grad</code> field is <code>None</code>, the computed gradient is stored directly. If gradients already exist, the new gradient is added to the existing accumulated gradient.</p>\n<p>This requires careful attention to tensor shapes and broadcasting. When gradients are accumulated, they must be compatible for element-wise addition. If broadcasting was used during the forward pass, the gradients must be &quot;unbroadcast&quot; back to the original tensor shapes before accumulation.</p>\n<blockquote>\n<p><strong>Memory Efficiency Insight</strong>: Gradient accumulation reuses the same <code>grad</code> tensors throughout the backward pass, avoiding the creation of many temporary gradient tensors. This is crucial for memory efficiency in large neural networks where creating copies of all gradients would quickly exhaust available memory.</p>\n</blockquote>\n<h3 id=\"autodiff-architecture-decisions\">Autodiff Architecture Decisions</h3>\n<p>The design of the automatic differentiation engine involves several critical architecture decisions that affect performance, memory usage, and ease of implementation. Each decision represents a trade-off between different priorities, and understanding these trade-offs is essential for building a robust autodiff system.</p>\n<blockquote>\n<p><strong>Decision: Define-by-Run vs. Static Graph Construction</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to choose between building the computation graph dynamically during forward pass execution (define-by-run) or requiring users to define the graph structure upfront before execution (static graphs)</li>\n<li><strong>Options Considered</strong>: PyTorch-style dynamic graphs, TensorFlow 1.x-style static graphs, hybrid approaches with tracing</li>\n<li><strong>Decision</strong>: Implement define-by-run dynamic graph construction</li>\n<li><strong>Rationale</strong>: Dynamic graphs provide superior debugging experience, support natural control flow like loops and conditionals, and offer more intuitive APIs for educational purposes. The slight performance overhead is acceptable for our learning-focused framework</li>\n<li><strong>Consequences</strong>: Enables flexible model architectures but requires careful memory management to avoid retaining computation graphs longer than necessary</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Graph Construction Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Define-by-run (PyTorch)</td>\n<td>Natural control flow, easy debugging, flexible architectures</td>\n<td>Slight runtime overhead, harder to optimize</td>\n<td>✅ Yes</td>\n</tr>\n<tr>\n<td>Static graphs (TF 1.x)</td>\n<td>Better optimization opportunities, clear separation of definition/execution</td>\n<td>Complex control flow, difficult debugging</td>\n<td>❌ No</td>\n</tr>\n<tr>\n<td>Hybrid tracing (JAX)</td>\n<td>Best of both worlds</td>\n<td>Added complexity, requires sophisticated tracing</td>\n<td>❌ Too complex</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Operation Storage and Memory Management</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to decide how long to retain operation nodes and intermediate tensors in the computation graph, balancing memory usage against gradient computation requirements</li>\n<li><strong>Options Considered</strong>: Retain entire graph until manual release, automatic graph cleanup after backward pass, reference counting with weak references</li>\n<li><strong>Decision</strong>: Retain graph until backward pass completes, then automatic cleanup</li>\n<li><strong>Rationale</strong>: Provides predictable memory behavior while ensuring all gradient information remains available during backpropagation. Automatic cleanup prevents common memory leak bugs that beginners encounter</li>\n<li><strong>Consequences</strong>: Higher peak memory usage during training but prevents accidental memory leaks and provides clear memory lifecycle</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Gradient Storage Location</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to choose where to store computed gradients—in the operation nodes, in the tensors themselves, or in a separate gradient tape structure</li>\n<li><strong>Options Considered</strong>: Store gradients in tensors (<code>.grad</code> attribute), store in operation nodes, maintain separate gradient dictionary</li>\n<li><strong>Decision</strong>: Store gradients directly in tensor objects as <code>.grad</code> attribute</li>\n<li><strong>Rationale</strong>: Matches PyTorch&#39;s intuitive API, makes gradients easily accessible for debugging and optimization, and simplifies the parameter update process for optimizers</li>\n<li><strong>Consequences</strong>: Tensors become slightly heavier objects but provide much more convenient API for users</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Gradient Storage Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tensor <code>.grad</code> attribute</td>\n<td>Intuitive API, easy optimizer access, simple debugging</td>\n<td>Heavier tensor objects</td>\n<td>✅ Yes</td>\n</tr>\n<tr>\n<td>Operation node storage</td>\n<td>Lighter tensors, clear separation</td>\n<td>Complex gradient retrieval, poor debugging</td>\n<td>❌ No</td>\n</tr>\n<tr>\n<td>Separate gradient dictionary</td>\n<td>Memory efficient, flexible</td>\n<td>Complex API, hard to debug</td>\n<td>❌ No</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Eager vs. Lazy Gradient Computation</strong></p>\n<ul>\n<li><strong>Context</strong>: We can either compute gradients immediately during the backward pass (eager) or defer computation until gradients are actually needed (lazy)</li>\n<li><strong>Options Considered</strong>: Eager computation during backward pass, lazy computation on gradient access, hybrid with caching</li>\n<li><strong>Decision</strong>: Implement eager gradient computation during backward pass</li>\n<li><strong>Rationale</strong>: Simpler implementation, predictable performance characteristics, and easier debugging since gradients are available immediately after <code>backward()</code> call</li>\n<li><strong>Consequences</strong>: May compute gradients that are never used, but provides more predictable behavior for educational purposes</li>\n</ul>\n</blockquote>\n<p>The autodiff engine also must handle several implementation details that affect correctness and performance:</p>\n<p><strong>Graph Node Lifecycle Management</strong>: Operation nodes must remain alive until the backward pass completes, but should be cleaned up afterwards to prevent memory leaks. This requires careful reference management between tensors and operations.</p>\n<p><strong>Thread Safety Considerations</strong>: While our educational framework doesn&#39;t target multi-threaded execution, the autodiff engine should avoid obvious thread safety issues that could cause confusion if users accidentally use it in multi-threaded contexts.</p>\n<p><strong>Gradient Dtype Consistency</strong>: Gradients must maintain the same data type as the tensors they correspond to, requiring careful dtype handling during gradient computation and accumulation.</p>\n<p><strong>Shape Preservation</strong>: The autodiff engine must carefully preserve tensor shapes throughout the backward pass, ensuring that gradients have exactly the same shape as the tensors they correspond to.</p>\n<h3 id=\"common-autodiff-pitfalls\">Common Autodiff Pitfalls</h3>\n<p>Building an automatic differentiation system presents numerous opportunities for subtle bugs that can be difficult to debug. Understanding these common pitfalls and how to avoid them is crucial for successful implementation.</p>\n<p>⚠️ <strong>Pitfall: Gradient Not Accumulated Across Multiple Uses</strong></p>\n<p>One of the most common mistakes is failing to properly accumulate gradients when a tensor is used multiple times in a computation. Beginners often implement gradient assignment (<code>tensor.grad = new_gradient</code>) instead of gradient accumulation (<code>tensor.grad += new_gradient</code>), causing later gradient contributions to overwrite earlier ones.</p>\n<p>This manifests as mysteriously incorrect gradients that seem to have the right magnitude but wrong values. For example, if computing <code>y = x + x</code>, the gradient with respect to <code>x</code> should be 2, but without proper accumulation it would be 1 (only the contribution from one of the addition operands).</p>\n<p>The fix is to always check if a tensor already has accumulated gradients before storing new ones:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Wrong: overwrites existing gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tensor.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> computed_gradient</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Correct: accumulates gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> tensor.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> computed_gradient</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor.grad </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> computed_gradient</span></span></code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Incorrect Topological Ordering</strong></p>\n<p>The backward pass must process operations in reverse topological order, but implementing topological sort correctly is tricky. Common mistakes include not handling cycles properly (though computation graphs should be acyclic), processing nodes in creation order instead of dependency order, or not handling the case where the same operation is reachable through multiple paths.</p>\n<p>Incorrect ordering leads to gradients being computed before their dependencies are ready, resulting in missing or incorrect gradient values. The symptoms include gradients that are zero when they should be non-zero, or exceptions about undefined gradient values.</p>\n<p>The fix requires implementing a proper depth-first search with post-order traversal:</p>\n<ol>\n<li>Start from the output tensor and follow <code>grad_fn</code> pointers</li>\n<li>Mark nodes as visited to avoid infinite loops</li>\n<li>Only add a node to the topological order after visiting all its dependencies</li>\n<li>Reverse the final order to get correct backward pass sequence</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Memory Leaks from Circular References</strong></p>\n<p>The computation graph creates a web of references between tensors and operations that can lead to circular references and memory leaks. Tensors hold references to operations through <code>grad_fn</code>, and operations hold references to input tensors, creating cycles that Python&#39;s garbage collector may not handle efficiently.</p>\n<p>This manifests as steadily increasing memory usage during training, even when the computation graphs should be freed after each backward pass. The memory usage grows until the system runs out of memory, even for small models.</p>\n<p>The fix involves carefully breaking reference cycles after gradient computation completes:</p>\n<ol>\n<li>Clear <code>grad_fn</code> references from tensors after backward pass</li>\n<li>Clear input tensor references from operation nodes</li>\n<li>Consider using weak references for some connections to break cycles automatically</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Broadcasting Gradient Shape Mismatches</strong></p>\n<p>When operations involve broadcasting, the backward pass must &quot;unbroadcast&quot; gradients back to their original shapes. Failing to handle this correctly leads to gradient tensors with the wrong shape, causing shape mismatch errors during gradient accumulation or parameter updates.</p>\n<p>For example, if computing <code>c = a + b</code> where <code>a</code> has shape <code>(3, 1)</code> and <code>b</code> has shape <code>(3, 4)</code>, broadcasting expands <code>a</code> to <code>(3, 4)</code> during the forward pass. During the backward pass, the gradient with respect to <code>a</code> starts with shape <code>(3, 4)</code> but must be reduced back to <code>(3, 1)</code> by summing along the appropriate axes.</p>\n<p>The fix requires implementing proper gradient unbroadcasting:</p>\n<ol>\n<li>Track the original shapes of all input tensors during the forward pass</li>\n<li>During backward pass, reduce gradient tensors back to their original shapes</li>\n<li>Use sum operations to collapse broadcasted dimensions</li>\n<li>Ensure the final gradient shape exactly matches the original tensor shape</li>\n</ol>\n<p>⚠️ <strong>Pitfall: In-Place Operations Breaking Gradient Flow</strong></p>\n<p>In-place operations that modify tensors directly can break gradient computation by changing values that are needed for computing gradients of earlier operations. This is particularly problematic when the same tensor is used in multiple operations and then modified in-place.</p>\n<p>For example, if computing <code>b = a * 2</code> followed by <code>a += 1</code>, the in-place addition changes the value of <code>a</code> that is needed to compute gradients for the multiplication operation. This leads to incorrect gradients and unpredictable behavior.</p>\n<p>The fix is to either prohibit in-place operations on tensors that require gradients, or implement sophisticated version tracking to detect when gradients might be affected by in-place modifications.</p>\n<p>⚠️ <strong>Pitfall: Forgetting to Set requires_grad</strong></p>\n<p>The <code>requires_grad</code> flag controls whether a tensor participates in gradient computation. Forgetting to set this flag on parameters or input tensors means no gradients will be computed for them, leading to parameters that don&#39;t update during training.</p>\n<p>This manifests as models that don&#39;t learn—loss doesn&#39;t decrease, parameters remain at their initial values, and gradient inspection shows all gradients are None or zero.</p>\n<p>The fix is to ensure that all tensors that should have gradients computed are properly marked:</p>\n<ul>\n<li>Set <code>requires_grad=True</code> on all model parameters during initialization</li>\n<li>Set <code>requires_grad=True</code> on input tensors if computing gradients with respect to inputs</li>\n<li>Verify that intermediate tensors inherit gradient requirements from their inputs</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Common Pitfall</th>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradient not accumulated</td>\n<td>Incorrect gradient magnitudes</td>\n<td>Assignment instead of accumulation</td>\n<td>Use <code>+=</code> for gradient updates</td>\n</tr>\n<tr>\n<td>Wrong topological order</td>\n<td>Missing or zero gradients</td>\n<td>Incorrect dependency ordering</td>\n<td>Implement proper DFS post-order traversal</td>\n</tr>\n<tr>\n<td>Memory leaks</td>\n<td>Ever-increasing memory usage</td>\n<td>Circular tensor-operation references</td>\n<td>Break cycles after backward pass</td>\n</tr>\n<tr>\n<td>Broadcasting shape mismatch</td>\n<td>Shape errors during accumulation</td>\n<td>Missing gradient unbroadcasting</td>\n<td>Reduce gradients to original shapes</td>\n</tr>\n<tr>\n<td>In-place operations</td>\n<td>Incorrect gradients</td>\n<td>Modified values needed for gradients</td>\n<td>Prohibit in-place ops with gradients</td>\n</tr>\n<tr>\n<td>Missing requires_grad</td>\n<td>No gradients computed</td>\n<td>Flag not set on parameters</td>\n<td>Mark all learnable tensors</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The automatic differentiation engine bridges the gap between the mathematical theory of differentiation and the practical implementation of gradient computation in neural networks. This section provides concrete guidance for implementing the core components while avoiding common pitfalls.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Graph Storage</td>\n<td>Python lists and dictionaries</td>\n<td>Custom graph data structures with optimized traversal</td>\n</tr>\n<tr>\n<td>Topological Sort</td>\n<td>Recursive DFS with visited set</td>\n<td>Iterative DFS with explicit stack to avoid recursion limits</td>\n</tr>\n<tr>\n<td>Gradient Storage</td>\n<td>Direct tensor attributes</td>\n<td>Separate gradient tape with lazy computation</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Manual reference clearing</td>\n<td>Weak references and context managers</td>\n</tr>\n<tr>\n<td>Numerical Verification</td>\n<td>Simple finite differences</td>\n<td>Sophisticated gradient checking with multiple step sizes</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<p>The autodiff engine should be organized to separate concerns clearly while maintaining easy integration with the tensor operations:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n  autodiff/\n    __init__.py              ← public API exports\n    engine.py                ← core backward pass algorithm\n    operations.py            ← operation base class and implementations\n    graph.py                 ← computation graph utilities\n    gradient_check.py        ← numerical verification utilities\n  tensor/\n    tensor.py                ← tensor class with grad_fn integration\n  test/\n    test_autodiff.py         ← comprehensive gradient tests\n    test_gradient_check.py   ← numerical verification tests</code></pre></div>\n\n<p><strong>Core Infrastructure (Complete Implementation)</strong></p>\n<p>The following infrastructure components are essential but not the primary learning focus. These can be used as-is to support the core autodiff implementation:</p>\n<p><strong>Gradient Checking Utilities</strong> - Complete implementation for verifying gradient correctness:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Gradient checking utilities for verifying automatic differentiation correctness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">These utilities compare automatic gradients against numerical approximations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Tuple, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> numerical_gradient</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">], h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> List[np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute numerical gradients using finite differences.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        f: Function that takes list of tensors and returns single tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        inputs: List of input tensors to compute gradients for</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        h: Step size for finite differences</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of numerical gradient arrays, one per input tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradients </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, input_tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(inputs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros_like(input_tensor.data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flat_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> input_tensor.data.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flat_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> j </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(flat_input)):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute f(x + h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            original_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input[j]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_input[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> original_value </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_plus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(inputs).data.item() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> f(inputs).data.size </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> f(inputs).data.sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Compute f(x - h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_input[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> original_value </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_minus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(inputs).data.item() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> f(inputs).data.size </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> f(inputs).data.sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Numerical gradient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_grad[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (f_plus </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> f_minus) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Restore original value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_input[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> original_value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradients.append(flat_grad.reshape(input_tensor.shape))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradients</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compare automatic gradients against numerical gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        f: Function to test (should return scalar tensor)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        inputs: Input tensors with requires_grad=True</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tolerance: Maximum allowed difference between gradients</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        True if gradients match within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute automatic gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inp.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Clear any existing gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auto_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [inp.grad.data </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> inp.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> np.zeros_like(inp.data) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                  for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute numerical gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numerical_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> numerical_gradient(f, inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compare gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    all_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (auto_grad, num_grad) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(auto_grads, numerical_grads)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.abs(auto_grad </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> num_grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.max(diff)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> max_diff </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> tolerance:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient mismatch for input </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: max difference = </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">max_diff</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Automatic: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">auto_grad.flatten()[:</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Numerical: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">num_grad.flatten()[:</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> all_match</span></span></code></pre></div>\n\n<p><strong>Graph Traversal Utilities</strong> - Complete implementation for topological sorting:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Computation graph traversal utilities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles topological sorting and graph cleanup for automatic differentiation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Set, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> deque</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> topological_sort</span><span style=\"color:#E1E4E8\">(start_tensor: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#9ECBFF\">'Operation'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Perform topological sort starting from output tensor.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns operations in reverse topological order (correct for backward pass).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        start_tensor: Output tensor to start traversal from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of operations in backward pass order</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    visited </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    topo_order </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> dfs_visit</span><span style=\"color:#E1E4E8\">(tensor: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> id</span><span style=\"color:#E1E4E8\">(tensor) </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> visited:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        visited.add(</span><span style=\"color:#79B8FF\">id</span><span style=\"color:#E1E4E8\">(tensor))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tensor.grad_fn </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            operation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor.grad_fn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Visit all input tensors first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> input_tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> operation.inputs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                dfs_visit(input_tensor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Add operation to topological order</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> operation </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> topo_order:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                topo_order.append(operation)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dfs_visit(start_tensor)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">reversed</span><span style=\"color:#E1E4E8\">(topo_order))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> clear_computation_graph</span><span style=\"color:#E1E4E8\">(tensor: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Clear computation graph to prevent memory leaks.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Breaks reference cycles between tensors and operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tensor: Root tensor to start cleanup from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    visited </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> clear_recursive</span><span style=\"color:#E1E4E8\">(t: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> id</span><span style=\"color:#E1E4E8\">(t) </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> visited:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        visited.add(</span><span style=\"color:#79B8FF\">id</span><span style=\"color:#E1E4E8\">(t))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> t.grad_fn </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            operation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> t.grad_fn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Recursively clear inputs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> input_tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> operation.inputs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                clear_recursive(input_tensor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Break references</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            operation.inputs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            t.grad_fn </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clear_recursive(tensor)</span></span></code></pre></div>\n\n<p><strong>Core Autodiff Implementation (Skeleton with TODOs)</strong></p>\n<p>The following skeleton provides the structure for the core automatic differentiation components. Each TODO corresponds to a specific algorithm step from the design discussion above:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core automatic differentiation engine implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Students should implement the TODOs to complete the backward pass algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Base class for all operations that participate in automatic differentiation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Each operation must implement forward and backward methods.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store references to input tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: self.inputs = inputs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">input_arrays: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute forward pass operation on numpy arrays.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Should be implemented by subclasses.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Subclasses must implement forward\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute gradients with respect to inputs given gradient of output.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Should be implemented by subclasses.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            grad_output: Gradient with respect to operation output</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of gradients with respect to each input (None if input doesn't need gradients)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Subclasses must implement backward\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Addition operation: output = input1 + input2\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Implement forward pass for addition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: return a + b (numpy handles broadcasting)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Gradient of addition: d(a+b)/da = 1, d(a+b)/db = 1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Must handle broadcasting by unbroadcasting gradients to input shapes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute gradients for both inputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle broadcasting - use unbroadcast_gradient helper</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return None for inputs that don't require gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hints:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Both gradients are initially the same as grad_output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Use self.inputs[0].shape and self.inputs[1].shape for target shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Check self.inputs[i].requires_grad before computing gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Multiply</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Element-wise multiplication: output = input1 * input2\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Implement forward pass for multiplication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Gradient of multiplication: d(a*b)/da = b, d(a*b)/db = a</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Compute gradient w.r.t first input (grad_output * second_input)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Compute gradient w.r.t second input (grad_output * first_input)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Handle broadcasting and requires_grad checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MatMul</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Matrix multiplication: output = input1 @ input2\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Implement matrix multiplication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.matmul or @ operator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward</span><span style=\"color:#E1E4E8\">(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Gradient of matrix multiplication:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d(A@B)/dA = grad_output @ B^T</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d(A@B)/dB = A^T @ grad_output</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Compute gradient w.r.t first input (grad_output @ B.T)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Compute gradient w.r.t second input (A.T @ grad_output)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Handle batch dimensions and requires_grad checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hints:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Use np.swapaxes(-1, -2) for transpose of last two dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Access input data via self.inputs[0].data and self.inputs[1].data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> unbroadcast_gradient</span><span style=\"color:#E1E4E8\">(grad: np.ndarray, target_shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Reduce gradient tensor to target shape by summing over broadcasted dimensions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This reverses the effect of broadcasting during forward pass.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Handle case where gradient needs to be summed over extra dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Handle case where gradient needs to be summed and squeezed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Algorithm:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. Sum over dimensions that were added by broadcasting (leading dimensions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. Sum over dimensions that were expanded from size 1 (keepdims=True)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. Ensure final shape exactly matches target_shape</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Integration with Tensor class</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> backward_pass</span><span style=\"color:#E1E4E8\">(tensor: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, gradient: Optional[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Execute backward pass starting from the given tensor.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is called by tensor.backward() method.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 16: Initialize gradient if not provided (ones with same shape as tensor)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 17: Get topologically sorted operations using imported utility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 18: For each operation in sorted order:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Get accumulated gradient for operation output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Call operation.backward() to compute input gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Accumulate gradients in input tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 19: Clear computation graph to prevent memory leaks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Algorithm outline:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. if gradient is None: gradient = ones_like(tensor)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. operations = topological_sort(tensor)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. for op in operations:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #      output_grad = get_tensor_gradient(op.output)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #      input_grads = op.backward(output_grad)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #      for input_tensor, grad in zip(op.inputs, input_grads):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #          accumulate_gradient(input_tensor, grad)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 4. clear_computation_graph(tensor)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the automatic differentiation engine, verify correctness with these tests:</p>\n<p><strong>Basic Gradient Computation Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create test tensors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Compute function: f = a * b + a</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> b</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">d </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> c </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> a</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">f </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d  </span><span style=\"color:#6A737D\"># f = a*b + a = a*(b+1)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected gradients: df/da = b+1 = 4, df/db = a = 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">f.backward()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> np.allclose(a.grad.data, [</span><span style=\"color:#79B8FF\">4.0</span><span style=\"color:#E1E4E8\">]), </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected a.grad=[4.0], got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">a.grad.data</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> np.allclose(b.grad.data, [</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">]), </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected b.grad=[2.0], got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">b.grad.data</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Basic gradient computation test passed\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Gradient Accumulation Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test multiple usage of same tensor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> x  </span><span style=\"color:#6A737D\"># y = 3*x, dy/dx should be 3</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y.backward()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> np.allclose(x.grad.data, [</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">]), </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected x.grad=[3.0], got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">x.grad.data</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Gradient accumulation test passed\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Numerical Gradient Verification</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Verify against numerical gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_function</span><span style=\"color:#E1E4E8\">(inputs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a, b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inputs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> b </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> a</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">is_correct </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_gradients(test_function, [a, b], </span><span style=\"color:#FFAB70\">tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> is_correct, </span><span style=\"color:#9ECBFF\">\"Gradients don't match numerical approximation\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Numerical gradient verification passed\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>All gradients are None</td>\n<td><code>requires_grad</code> not set</td>\n<td>Check <code>tensor.requires_grad</code></td>\n<td>Set <code>requires_grad=True</code> on inputs</td>\n</tr>\n<tr>\n<td>Gradients have wrong shape</td>\n<td>Broadcasting not handled</td>\n<td>Print shapes during backward pass</td>\n<td>Implement <code>unbroadcast_gradient</code></td>\n</tr>\n<tr>\n<td>Gradients are half expected value</td>\n<td>Missing accumulation</td>\n<td>Check if tensor used multiple times</td>\n<td>Use <code>+=</code> not <code>=</code> for gradient updates</td>\n</tr>\n<tr>\n<td>Memory keeps growing</td>\n<td>Computation graph not cleared</td>\n<td>Monitor reference counts</td>\n<td>Call <code>clear_computation_graph</code></td>\n</tr>\n<tr>\n<td>Random gradient errors</td>\n<td>Wrong topological order</td>\n<td>Print operation execution order</td>\n<td>Fix topological sort implementation</td>\n</tr>\n</tbody></table>\n<p>The automatic differentiation engine is complete when all gradient tests pass, numerical verification succeeds, and memory usage remains stable across multiple forward/backward passes.</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fcomputation-graph.svg\" alt=\"Computation Graph Structure\"></p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fgradient-flow.svg\" alt=\"Gradient Backpropagation Flow\"></p>\n<h2 id=\"neural-network-modules-milestone-3\">Neural Network Modules (Milestone 3)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (Layers &amp; Modules) - implements the module system with layers like Linear and activations, plus parameter management and initialization.</p>\n</blockquote>\n<p>Neural network modules are the building blocks that transform our powerful tensor operations and automatic differentiation engine into practical tools for constructing machine learning models. While the previous milestones gave us the mathematical foundation—tensors that can store multidimensional data and automatically compute gradients—this milestone focuses on the abstraction layer that makes neural network construction intuitive and composable. Think of modules as intelligent LEGO blocks that not only snap together in meaningful ways but also automatically handle the complex bookkeeping required for gradient-based learning.</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fmodule-hierarchy.svg\" alt=\"Module System Organization\"></p>\n<p>The module system addresses several critical challenges that emerge when building neural networks. First, it provides a clean abstraction for common neural network operations like linear transformations and activation functions, encapsulating both the forward computation and the gradient computation logic. Second, it implements a parameter management system that automatically tracks all trainable parameters across potentially complex nested network architectures. Third, it establishes conventions for weight initialization that can significantly impact training success. Finally, it creates a composable architecture where complex networks can be built by combining simpler, well-tested components.</p>\n<h3 id=\"modules-as-lego-blocks\">Modules as LEGO Blocks</h3>\n<blockquote>\n<p>The power of the module system lies in its composability—simple components can be combined to create arbitrarily complex architectures while maintaining clean interfaces and automatic parameter tracking.</p>\n</blockquote>\n<p>Understanding neural network modules requires thinking beyond individual mathematical operations to consider how we can create reusable, composable components. Imagine building with LEGO blocks where each block not only has physical connection points but also carries information about how to process data and how to learn from mistakes. A linear layer is like a specialized LEGO block that knows how to perform matrix multiplication and adjust its internal weights based on feedback. An activation function is like another specialized block that applies a nonlinear transformation. A sequential container is like a baseplate that connects multiple blocks in a specific order, allowing data to flow through them systematically.</p>\n<p>The key insight is that each module encapsulates both <strong>state</strong> (the parameters that define its behavior) and <strong>behavior</strong> (the forward computation and gradient handling). This encapsulation allows us to reason about network components at a higher level of abstraction. When we compose modules together, we don&#39;t need to manually track which tensors belong to which layer or manually chain gradient computations—the module system handles this automatically.</p>\n<p>Consider how this composability manifests in practice. A simple neural network might consist of a linear layer, followed by a ReLU activation, followed by another linear layer. Each component has a well-defined responsibility: the linear layers perform learnable transformations, and the ReLU introduces nonlinearity. When we chain them together in a sequential module, the result is a complete neural network that automatically handles forward computation, parameter tracking, and gradient flow.</p>\n<p>This modular approach provides several crucial advantages. <strong>Reusability</strong> means we can define a linear layer once and use it throughout our network without duplication. <strong>Testability</strong> allows us to verify each component in isolation before composing them into larger systems. <strong>Maintainability</strong> lets us modify or replace individual components without affecting the entire network. <strong>Extensibility</strong> enables us to add new layer types by following established patterns and interfaces.</p>\n<p>The module abstraction also handles complex scenarios that would be error-prone if managed manually. When modules are nested—such as a sequential module containing other sequential modules—the parameter tracking system recursively discovers and exposes all trainable parameters. When the same module instance is used multiple times in a network (weight sharing), the gradient accumulation system ensures that gradients from all usage sites are properly combined.</p>\n<h3 id=\"module-base-class-design\">Module Base Class Design</h3>\n<p>The <code>Module</code> base class serves as the foundation for all neural network components in our framework, establishing the essential interface and common functionality that every layer and network component must provide. The design centers around three core responsibilities: managing trainable parameters, defining the forward computation contract, and enabling recursive operations across potentially complex nested module hierarchies.</p>\n<p>The fundamental challenge in designing the module base class is balancing flexibility with structure. We need an interface that&#39;s general enough to accommodate diverse layer types—from simple element-wise activations to complex attention mechanisms—while providing enough structure to enable automatic parameter management and gradient flow. The solution is to define a minimal but powerful interface that handles the common concerns while allowing subclasses to implement their specific computational logic.</p>\n<p><strong>Module Interface Definition:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>forward</code></td>\n<td><code>*inputs: Tensor</code></td>\n<td><code>Tensor</code></td>\n<td>Abstract method defining forward computation logic</td>\n</tr>\n<tr>\n<td><code>parameters</code></td>\n<td>None</td>\n<td><code>List[Tensor]</code></td>\n<td>Recursively collect all trainable parameters</td>\n</tr>\n<tr>\n<td><code>named_parameters</code></td>\n<td><code>prefix: str = &quot;&quot;</code></td>\n<td><code>Dict[str, Tensor]</code></td>\n<td>Collect parameters with full hierarchical names</td>\n</tr>\n<tr>\n<td><code>register_parameter</code></td>\n<td><code>name: str, param: Tensor</code></td>\n<td><code>None</code></td>\n<td>Register a tensor as trainable parameter</td>\n</tr>\n<tr>\n<td><code>register_module</code></td>\n<td><code>name: str, module: Module</code></td>\n<td><code>None</code></td>\n<td>Register submodule for recursive operations</td>\n</tr>\n<tr>\n<td><code>train</code></td>\n<td><code>mode: bool = True</code></td>\n<td><code>Module</code></td>\n<td>Set training/evaluation mode recursively</td>\n</tr>\n<tr>\n<td><code>eval</code></td>\n<td>None</td>\n<td><code>Module</code></td>\n<td>Set to evaluation mode (calls train(False))</td>\n</tr>\n<tr>\n<td><code>zero_grad</code></td>\n<td>None</td>\n<td><code>None</code></td>\n<td>Reset gradients for all parameters to None</td>\n</tr>\n<tr>\n<td><code>__call__</code></td>\n<td><code>*inputs: Tensor</code></td>\n<td><code>Tensor</code></td>\n<td>Make module callable, delegates to forward</td>\n</tr>\n</tbody></table>\n<p><strong>Module State Management:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Attribute</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>_parameters</code></td>\n<td><code>Dict[str, Tensor]</code></td>\n<td>Dictionary storing registered trainable parameters</td>\n</tr>\n<tr>\n<td><code>_modules</code></td>\n<td><code>Dict[str, Module]</code></td>\n<td>Dictionary storing registered submodules</td>\n</tr>\n<tr>\n<td><code>training</code></td>\n<td><code>bool</code></td>\n<td>Flag indicating training vs evaluation mode</td>\n</tr>\n<tr>\n<td><code>_name</code></td>\n<td><code>Optional[str]</code></td>\n<td>Optional name for debugging and visualization</td>\n</tr>\n</tbody></table>\n<p>The <code>forward</code> method represents the core abstraction of the module system. Every module must implement this method to define how it transforms input tensors into output tensors. By making this an abstract method, we ensure that all modules provide their computational logic while leaving the specific implementation details to each module type. The method signature accepts variable arguments to accommodate modules that take multiple inputs, such as attention layers that might receive query, key, and value tensors.</p>\n<p>The parameter management system centers around the <code>register_parameter</code> and <code>register_module</code> methods. When a module registers a parameter, it&#39;s added to the internal <code>_parameters</code> dictionary with the given name. The parameter must be a <code>Tensor</code> with <code>requires_grad=True</code> to be considered trainable. When a module registers a submodule, the submodule is added to the <code>_modules</code> dictionary, enabling recursive operations like parameter collection and mode switching.</p>\n<p>The recursive nature of parameter collection is crucial for handling nested module hierarchies. The <code>parameters</code> method implements a depth-first traversal that visits all submodules and collects their parameters. This means that a complex network built from nested sequential modules and individual layers will automatically expose all its trainable parameters through a single method call. The <code>named_parameters</code> method extends this by providing fully qualified names that reflect the module hierarchy, which is essential for debugging and parameter analysis.</p>\n<blockquote>\n<p><strong>Decision: Abstract Base Class vs Interface</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to define common module functionality while allowing diverse implementations</li>\n<li><strong>Options Considered</strong>: Pure interface with no shared code, abstract base class with common implementations, concrete base class with overridable methods</li>\n<li><strong>Decision</strong>: Abstract base class with implemented parameter management and abstract forward method</li>\n<li><strong>Rationale</strong>: Provides essential shared functionality (parameter tracking, recursive operations) while enforcing interface contract. Reduces boilerplate in subclasses while maintaining type safety.</li>\n<li><strong>Consequences</strong>: Slightly more complex inheritance hierarchy, but significantly reduces implementation burden for new module types</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Decision Comparison:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pure Interface</td>\n<td>Maximum flexibility, no coupling</td>\n<td>Duplicated parameter management code</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Abstract Base Class</td>\n<td>Shared functionality, enforced interface</td>\n<td>Some implementation constraints</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Concrete Base Class</td>\n<td>Default implementations, very easy to extend</td>\n<td>Weak interface contract, harder to verify</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The mode management system handles the distinction between training and evaluation phases. Some modules behave differently during training (dropout, batch normalization), so the base class provides infrastructure for propagating mode changes throughout the entire module hierarchy. The <code>train</code> and <code>eval</code> methods recursively update all submodules, ensuring consistent behavior across the entire network.</p>\n<p>The <code>__call__</code> method enables modules to be used as callable objects, providing a clean interface where <code>output = module(input)</code> automatically invokes the forward method. This design pattern, borrowed from PyTorch, makes module usage intuitive while maintaining the separation between the public interface (<code>__call__</code>) and the implementation detail (<code>forward</code>).</p>\n<p>Error handling in the module base class focuses on parameter registration validation and recursive operation safety. When registering parameters, the base class verifies that the provided object is actually a tensor and optionally checks that it has gradient tracking enabled. When performing recursive operations, the base class guards against infinite recursion by maintaining visited module sets during traversal.</p>\n<h3 id=\"linear-layer-implementation\">Linear Layer Implementation</h3>\n<p>The linear layer represents the fundamental building block of feedforward neural networks, implementing the mathematical operation <code>y = Wx + b</code> where <code>W</code> is a learnable weight matrix, <code>b</code> is an optional learnable bias vector, and the operation represents matrix multiplication followed by bias addition. This seemingly simple transformation is remarkably powerful and serves as the foundation for fully connected networks, the final classification layers of convolutional networks, and components within more complex architectures like transformers.</p>\n<p>Understanding the linear layer requires appreciating both its mathematical simplicity and its implementation complexity. Mathematically, we&#39;re performing a linear transformation that maps input vectors from one dimension space to another. If our input has dimension <code>in_features</code> and we want output dimension <code>out_features</code>, we need a weight matrix of shape <code>(out_features, in_features)</code> and a bias vector of shape <code>(out_features,)</code>. However, the implementation must handle batched inputs, proper weight initialization, gradient flow, and broadcasting edge cases.</p>\n<p>The design challenges for the linear layer center around shape management and initialization strategy. Neural networks typically process batches of inputs simultaneously, so our linear layer must handle input tensors of shape <code>(batch_size, in_features)</code> and produce outputs of shape <code>(batch_size, out_features)</code>. The weight matrix multiplication must be configured correctly to work with batched inputs, and the bias addition must broadcast properly across the batch dimension.</p>\n<p><strong>Linear Layer Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>__init__</code></td>\n<td><code>in_features: int, out_features: int, bias: bool = True</code></td>\n<td>None</td>\n<td>Initialize layer with specified dimensions</td>\n</tr>\n<tr>\n<td><code>forward</code></td>\n<td><code>input: Tensor</code></td>\n<td><code>Tensor</code></td>\n<td>Compute y = xW^T + b for input batch</td>\n</tr>\n<tr>\n<td><code>reset_parameters</code></td>\n<td>None</td>\n<td>None</td>\n<td>Reinitialize weights and bias using default strategy</td>\n</tr>\n</tbody></table>\n<p><strong>Linear Layer Parameters:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Shape</th>\n<th>Description</th>\n<th>Initialization</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>weight</code></td>\n<td><code>(out_features, in_features)</code></td>\n<td>Learnable transformation matrix</td>\n<td>Xavier uniform</td>\n</tr>\n<tr>\n<td><code>bias</code></td>\n<td><code>(out_features,)</code></td>\n<td>Optional learnable bias vector</td>\n<td>Zeros or small uniform</td>\n</tr>\n</tbody></table>\n<p>The weight initialization strategy significantly impacts training dynamics and convergence behavior. Poor initialization can lead to vanishing or exploding gradients, particularly in deeper networks. The linear layer implements Xavier (also called Glorot) initialization by default, which sets initial weights by sampling from a uniform distribution with bounds calculated as <code>±sqrt(6 / (in_features + out_features))</code>. This initialization scheme aims to maintain similar variance in activations and gradients across layers, promoting stable training.</p>\n<p>The forward computation involves matrix multiplication between the batched input and the transposed weight matrix, followed by bias addition if bias is enabled. For input tensor <code>x</code> with shape <code>(batch_size, in_features)</code> and weight tensor <code>W</code> with shape <code>(out_features, in_features)</code>, we compute <code>x @ W.T</code> to get shape <code>(batch_size, out_features)</code>. The bias tensor broadcasts automatically across the batch dimension during addition.</p>\n<p><strong>Forward Pass Algorithm:</strong></p>\n<ol>\n<li><strong>Input validation</strong>: Verify input tensor has exactly 2 dimensions (batch_size, in_features)</li>\n<li><strong>Shape compatibility check</strong>: Confirm input&#39;s second dimension matches layer&#39;s in_features</li>\n<li><strong>Weight matrix multiplication</strong>: Compute <code>input @ self.weight.T</code> using tensor matmul operation</li>\n<li><strong>Bias addition</strong>: If bias enabled, add bias tensor (broadcasts automatically across batch dimension)</li>\n<li><strong>Return result</strong>: Output tensor has shape (batch_size, out_features)</li>\n</ol>\n<p>The gradient flow through linear layers demonstrates the elegance of automatic differentiation. During backpropagation, gradients flow from the output back to both the input and the parameters. The gradient with respect to the input is computed as <code>grad_output @ weight</code>, enabling gradient flow to previous layers. The gradient with respect to the weight is computed as <code>grad_output.T @ input</code>, accumulating the contribution from all batch samples. The bias gradient is simply the sum of grad_output across the batch dimension.</p>\n<blockquote>\n<p><strong>Decision: Weight Matrix Orientation</strong></p>\n<ul>\n<li><strong>Context</strong>: Matrix multiplication can be organized as input @ weight.T or input @ weight depending on weight shape</li>\n<li><strong>Options Considered</strong>: Weight shape (in_features, out_features) with input @ weight, weight shape (out_features, in_features) with input @ weight.T, weight shape matches PyTorch convention</li>\n<li><strong>Decision</strong>: Weight shape (out_features, in_features) with input @ weight.T computation</li>\n<li><strong>Rationale</strong>: Matches PyTorch convention for compatibility and intuitive parameter counting. Weight[i] represents the incoming connections to output neuron i.</li>\n<li><strong>Consequences</strong>: Requires transpose during forward pass but enables intuitive parameter interpretation and framework compatibility</li>\n</ul>\n</blockquote>\n<p>Bias handling requires careful consideration of when bias should be included versus omitted. Many modern architectures omit bias in certain contexts, particularly when batch normalization follows immediately after the linear transformation. The linear layer constructor accepts a <code>bias</code> parameter that controls whether bias is created and registered as a parameter. When bias is disabled, the bias parameter is set to None and no bias addition occurs during forward computation.</p>\n<p>The parameter registration system ensures that both weight and bias tensors are properly tracked by the module system. During initialization, the layer calls <code>self.register_parameter(&#39;weight&#39;, self.weight)</code> and conditionally <code>self.register_parameter(&#39;bias&#39;, self.bias)</code> to make these tensors discoverable by optimizers and parameter collection methods.</p>\n<h3 id=\"activation-function-modules\">Activation Function Modules</h3>\n<p>Activation functions introduce nonlinearity into neural networks, enabling them to learn complex patterns and approximate arbitrary functions. Without activation functions, neural networks would be limited to linear transformations, regardless of depth. The mathematical property that makes activation functions powerful is their element-wise application and nonlinear response, which allows networks to model curved decision boundaries and complex feature interactions.</p>\n<p>The implementation of activation function modules differs fundamentally from linear layers because they typically have no learnable parameters and operate element-wise on their inputs. However, they still benefit from the module abstraction for consistency, composability, and potential future extensions (like learnable activation functions). Each activation function must implement both the forward transformation and ensure proper gradient computation during backpropagation.</p>\n<p><strong>Common Activation Functions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Function</th>\n<th>Formula</th>\n<th>Derivative</th>\n<th>Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ReLU</td>\n<td><code>max(0, x)</code></td>\n<td><code>1 if x &gt; 0 else 0</code></td>\n<td>Simple, addresses vanishing gradients, sparse</td>\n</tr>\n<tr>\n<td>Sigmoid</td>\n<td><code>1 / (1 + exp(-x))</code></td>\n<td><code>sigmoid(x) * (1 - sigmoid(x))</code></td>\n<td>Smooth, bounded output [0,1], can saturate</td>\n</tr>\n<tr>\n<td>Tanh</td>\n<td><code>(exp(x) - exp(-x)) / (exp(x) + exp(-x))</code></td>\n<td><code>1 - tanh²(x)</code></td>\n<td>Smooth, bounded output [-1,1], zero-centered</td>\n</tr>\n<tr>\n<td>Leaky ReLU</td>\n<td><code>max(0.01*x, x)</code></td>\n<td><code>1 if x &gt; 0 else 0.01</code></td>\n<td>Addresses dying ReLU problem</td>\n</tr>\n</tbody></table>\n<p><strong>ReLU Implementation Details:</strong></p>\n<p>The Rectified Linear Unit (ReLU) has become the default activation function in many neural network architectures due to its computational simplicity and beneficial training properties. ReLU addresses the vanishing gradient problem that plagued earlier activation functions like sigmoid and tanh, particularly in deep networks. The function&#39;s derivative is either 0 or 1, which means gradients can flow through without attenuation when the unit is active.</p>\n<p>The forward pass for ReLU is mathematically simple: <code>output = maximum(0, input)</code>. However, the implementation must handle the gradient computation correctly. During backpropagation, gradients only flow through units where the input was positive. This creates a sparse gradient pattern that can be computationally efficient but also leads to the &quot;dying ReLU&quot; problem where neurons can become permanently inactive.</p>\n<p><strong>Sigmoid and Tanh Considerations:</strong></p>\n<p>Sigmoid and tanh activation functions were historically important but have largely been superseded by ReLU and its variants in hidden layers. However, they remain useful in specific contexts: sigmoid for binary classification output layers and tanh when zero-centered activations are beneficial. Both functions suffer from saturation issues where large positive or negative inputs produce gradients very close to zero, leading to vanishing gradients in deep networks.</p>\n<p>The implementation of sigmoid requires careful numerical stability considerations. The naive implementation <code>1 / (1 + exp(-x))</code> can overflow when x is large and negative. A more stable implementation uses the identity <code>sigmoid(x) = exp(x) / (1 + exp(x))</code> for positive x and the original formula for negative x, avoiding overflow in both directions.</p>\n<p><strong>Activation Module Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>__init__</code></td>\n<td><code>inplace: bool = False</code></td>\n<td>None</td>\n<td>Initialize activation with optional in-place operation</td>\n</tr>\n<tr>\n<td><code>forward</code></td>\n<td><code>input: Tensor</code></td>\n<td><code>Tensor</code></td>\n<td>Apply activation function element-wise</td>\n</tr>\n</tbody></table>\n<p><strong>Gradient Flow Architecture:</strong></p>\n<p>Activation functions participate in the computation graph like any other operation, creating nodes that link input tensors to output tensors with appropriate backward functions. The automatic differentiation system handles the derivative computation automatically, but the activation function implementations must ensure they create the correct computational graph structure.</p>\n<p>For ReLU, the backward function receives the gradient with respect to the output and must compute the gradient with respect to the input. This involves element-wise multiplication of the output gradient with a mask indicating where the original input was positive. The mask can be computed from the original input (stored during forward pass) or derived from the output (since output &gt; 0 if and only if input &gt; 0 for ReLU).</p>\n<blockquote>\n<p><strong>Decision: In-place vs Out-of-place Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Activation functions can potentially modify input tensors directly or create new output tensors</li>\n<li><strong>Options Considered</strong>: Always out-of-place for safety, optional in-place for memory efficiency, always in-place for performance</li>\n<li><strong>Decision</strong>: Optional in-place with default out-of-place behavior</li>\n<li><strong>Rationale</strong>: Out-of-place is safer for gradient computation and debugging, but in-place can be crucial for memory efficiency in large networks. Optional flag provides flexibility.</li>\n<li><strong>Consequences</strong>: Requires careful gradient handling for in-place operations, but enables memory optimization when needed</li>\n</ul>\n</blockquote>\n<p>The numerical stability of activation function implementations becomes critical when dealing with extreme input values. For exponential-based functions like sigmoid and tanh, input values with large magnitude can cause numerical overflow or underflow. Modern implementations use mathematically equivalent formulations that are more numerically stable, such as using the log-sum-exp trick or choosing different computational paths based on input sign.</p>\n<p><strong>Parameter-free Module Benefits:</strong></p>\n<p>Although activation functions typically have no learnable parameters, implementing them as modules provides several advantages. First, it maintains consistency in the module hierarchy, allowing activations to be treated uniformly with other layer types in sequential containers. Second, it provides a foundation for parameterized activation functions like Leaky ReLU or learnable activations. Third, it enables proper participation in module operations like mode switching and recursive traversal.</p>\n<p>The absence of parameters means activation modules have trivial parameter registration—they simply register no parameters with the base module class. This allows them to participate in parameter collection operations without contributing any trainable parameters, which is the expected behavior for standard activation functions.</p>\n<h3 id=\"parameter-registration-system\">Parameter Registration System</h3>\n<p>The parameter registration system forms the backbone of automatic parameter management in neural networks, enabling optimizers to discover and update all trainable parameters without manual bookkeeping. This system must solve several challenging problems: discovering parameters in arbitrarily nested module hierarchies, providing unique names for debugging and analysis, handling parameter sharing scenarios, and maintaining efficient access patterns for training loops.</p>\n<p>The fundamental challenge is that neural networks can have complex, nested structures where modules contain other modules, which in turn contain their own parameters and submodules. A transformer model, for example, might have multiple encoder layers, each containing attention modules and feedforward modules, each with their own linear layers and parameters. The parameter registration system must traverse this hierarchy automatically and expose all trainable parameters through a clean interface.</p>\n<p><strong>Parameter Registration Workflow:</strong></p>\n<ol>\n<li><strong>Module initialization</strong>: During module creation, parameters are created as <code>Tensor</code> objects with <code>requires_grad=True</code></li>\n<li><strong>Registration call</strong>: Module calls <code>self.register_parameter(name, tensor)</code> for each trainable parameter</li>\n<li><strong>Storage and validation</strong>: Base class stores parameter in <code>_parameters</code> dict after validation</li>\n<li><strong>Submodule registration</strong>: Complex modules register submodules using <code>self.register_module(name, module)</code></li>\n<li><strong>Recursive collection</strong>: Parameter collection methods traverse module tree to gather all parameters</li>\n<li><strong>Name generation</strong>: Hierarchical names are constructed by concatenating module and parameter names</li>\n</ol>\n<p>The registration validation process ensures that only appropriate objects are registered as parameters. The base module class checks that registered parameters are <code>Tensor</code> instances and optionally verifies that they have gradient tracking enabled. This validation catches common errors like accidentally registering regular numpy arrays or tensors with gradient tracking disabled.</p>\n<p><strong>Parameter Collection Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>parameters()</code></td>\n<td><code>Iterator[Tensor]</code></td>\n<td>Yield all parameters in depth-first order</td>\n</tr>\n<tr>\n<td><code>named_parameters(prefix=&quot;&quot;)</code></td>\n<td><code>Iterator[Tuple[str, Tensor]]</code></td>\n<td>Yield (name, parameter) pairs with hierarchical names</td>\n</tr>\n<tr>\n<td><code>parameter_count()</code></td>\n<td><code>int</code></td>\n<td>Count total trainable parameters including all submodules</td>\n</tr>\n<tr>\n<td><code>parameter_summary()</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Detailed parameter statistics for analysis</td>\n</tr>\n</tbody></table>\n<p>The recursive parameter collection algorithm implements depth-first traversal to ensure consistent parameter ordering across multiple collection calls. For each module, the algorithm first yields the module&#39;s own parameters, then recursively traverses each submodule. This ordering is important for reproducible initialization and consistent optimizer state management.</p>\n<p><strong>Recursive Collection Algorithm:</strong></p>\n<ol>\n<li><strong>Initialize empty result list</strong>: Create container for collected parameters</li>\n<li><strong>Add local parameters</strong>: Iterate through <code>_parameters</code> dict and add each tensor</li>\n<li><strong>Traverse submodules</strong>: For each module in <code>_modules</code> dict, recursively call parameter collection</li>\n<li><strong>Generate hierarchical names</strong>: Construct names by joining module path with parameter name using dot notation</li>\n<li><strong>Handle duplicates</strong>: Check for parameter sharing and avoid duplicate collection</li>\n<li><strong>Return results</strong>: Yield parameters in consistent depth-first order</li>\n</ol>\n<p>The hierarchical naming system provides crucial debugging capabilities by showing exactly where each parameter originates in the module hierarchy. For example, a parameter might have the name <code>&quot;encoder.layer_2.attention.query.weight&quot;</code>, immediately indicating its location and purpose. This naming convention follows the same patterns as PyTorch and other major frameworks, ensuring familiar debugging experiences.</p>\n<p><strong>Parameter Sharing Scenarios:</strong></p>\n<p>Parameter sharing occurs when the same tensor object is used in multiple places within a network, either through module instance reuse or explicit tensor sharing. The parameter registration system must handle these scenarios correctly to avoid duplicate parameter collection and ensure proper gradient accumulation.</p>\n<p>When the same module instance appears multiple times in a network (such as using the same embedding layer for input and output), the parameter collection system recognizes that the parameter tensors are identical objects and collects them only once. The automatic differentiation system handles gradient accumulation automatically when the same parameter contributes to loss multiple times.</p>\n<blockquote>\n<p><strong>Decision: Parameter Collection Caching</strong></p>\n<ul>\n<li><strong>Context</strong>: Parameter collection involves recursive traversal which could be expensive for large networks</li>\n<li><strong>Options Considered</strong>: No caching for simplicity, cache with invalidation on parameter changes, cache with manual refresh</li>\n<li><strong>Decision</strong>: No caching with efficient iterator implementation</li>\n<li><strong>Rationale</strong>: Parameter collection typically happens once per training step, so caching complexity isn&#39;t justified. Iterator approach avoids creating large intermediate lists.</li>\n<li><strong>Consequences</strong>: Slightly more expensive repeated calls, but simpler implementation and no cache invalidation logic</li>\n</ul>\n</blockquote>\n<p><strong>Memory Management Considerations:</strong></p>\n<p>The parameter registration system must carefully manage references to avoid creating circular dependencies that prevent garbage collection. The base module class stores direct references to parameter tensors and submodules, creating a tree structure rooted at the top-level module. When modules are no longer referenced externally, the entire subtree should be eligible for garbage collection.</p>\n<p>However, the computation graph created during forward passes can create temporary circular references between tensors and operations. The parameter registration system doesn&#39;t interfere with this automatic differentiation mechanism but ensures that the module hierarchy itself doesn&#39;t prevent proper cleanup.</p>\n<p><strong>Integration with Optimizers:</strong></p>\n<p>The parameter registration system is designed to integrate seamlessly with optimizer implementations. Optimizers typically call <code>model.parameters()</code> to discover all trainable parameters, then maintain internal state (like momentum buffers) associated with each parameter. The consistent ordering provided by the registration system ensures that optimizer state remains properly aligned with parameters across training steps.</p>\n<p>For advanced scenarios like fine-tuning where some parameters should be frozen, the parameter registration system supports parameter filtering. Methods like <code>named_parameters()</code> can be combined with filtering logic to expose only specific subsets of parameters to optimizers, enabling selective training scenarios.</p>\n<h3 id=\"module-system-architecture-decisions\">Module System Architecture Decisions</h3>\n<p>The design of the module system involves numerous architectural decisions that impact usability, performance, extensibility, and maintainability. These decisions must balance competing concerns: the system should be simple enough for educational purposes while remaining powerful enough to support realistic neural network architectures. Each design choice creates trade-offs that ripple through the entire framework.</p>\n<blockquote>\n<p><strong>Decision: Eager vs Lazy Parameter Initialization</strong></p>\n<ul>\n<li><strong>Context</strong>: Parameters can be initialized during module creation or delayed until first forward pass</li>\n<li><strong>Options Considered</strong>: Eager initialization during <code>__init__</code>, lazy initialization on first forward pass, hybrid approach with shape inference</li>\n<li><strong>Decision</strong>: Eager initialization with explicit shape requirements</li>\n<li><strong>Rationale</strong>: Simpler implementation, immediate error detection, predictable memory usage. Educational framework benefits from explicit parameter management.</li>\n<li><strong>Consequences</strong>: Requires explicit input/output shapes during layer creation, but provides clearer error messages and deterministic initialization</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Decision Comparison:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Eager Init</td>\n<td>Simple, predictable, immediate errors</td>\n<td>Requires shape specification upfront</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Lazy Init</td>\n<td>Shape inference, more flexible API</td>\n<td>Complex state management, delayed errors</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Best of both worlds</td>\n<td>Increased complexity, harder to debug</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The module composition strategy determines how complex networks are constructed from simpler components. The framework supports both explicit composition (manually connecting layers) and container-based composition (using Sequential and other container modules). This dual approach provides flexibility while maintaining simplicity for common cases.</p>\n<p><strong>Sequential Container Design:</strong></p>\n<p>Sequential containers represent the most common neural network architecture pattern: a linear chain of modules where the output of one module becomes the input to the next. The implementation is remarkably simple yet powerful, automatically handling forward pass propagation and parameter collection across all contained modules.</p>\n<p>The Sequential module stores an ordered list of submodules and implements forward pass by iterating through them in sequence, passing the output of each module as input to the next. Parameter collection works through the standard recursive mechanism, automatically discovering all parameters in all contained modules.</p>\n<p><strong>Sequential Implementation Characteristics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Behavior</th>\n<th>Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Forward Pass</td>\n<td>Sequential application of submodules</td>\n<td>Automatic data flow chaining</td>\n</tr>\n<tr>\n<td>Parameter Collection</td>\n<td>Recursive traversal of submodules</td>\n<td>Automatic parameter discovery</td>\n</tr>\n<tr>\n<td>Gradient Flow</td>\n<td>Automatic through computation graph</td>\n<td>No manual gradient routing</td>\n</tr>\n<tr>\n<td>Error Propagation</td>\n<td>Shape mismatches caught at runtime</td>\n<td>Clear error location identification</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Module Mutability After Creation</strong></p>\n<ul>\n<li><strong>Context</strong>: Should modules allow parameter modification, layer addition/removal after creation?</li>\n<li><strong>Options Considered</strong>: Immutable modules for safety, mutable with careful state management, copy-on-write semantics</li>\n<li><strong>Decision</strong>: Mutable modules with explicit state management</li>\n<li><strong>Rationale</strong>: Educational framework benefits from experimentation and modification. Real frameworks are mutable. Complexity is manageable with clear documentation.</li>\n<li><strong>Consequences</strong>: Potential for state inconsistencies, but enables important use cases like transfer learning and architecture search</li>\n</ul>\n</blockquote>\n<p>The initialization strategy architecture decision impacts training success significantly. Different layer types benefit from different initialization schemes, and the framework must provide both sensible defaults and customization capabilities. The approach balances automatic initialization with explicit control when needed.</p>\n<p><strong>Initialization Strategy Options:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Use Case</th>\n<th>Mathematical Basis</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Xavier/Glorot</td>\n<td>General feedforward networks</td>\n<td>Maintains activation variance</td>\n<td>Uniform(-bound, bound) where bound = sqrt(6/(fan_in + fan_out))</td>\n</tr>\n<tr>\n<td>He/Kaiming</td>\n<td>ReLU networks</td>\n<td>Accounts for ReLU&#39;s zero region</td>\n<td>Normal(0, sqrt(2/fan_in))</td>\n</tr>\n<tr>\n<td>Zero</td>\n<td>Bias initialization</td>\n<td>No initial bias preference</td>\n<td>All zeros</td>\n</tr>\n<tr>\n<td>Identity</td>\n<td>Skip connections</td>\n<td>Preserve input initially</td>\n<td>Identity matrix where applicable</td>\n</tr>\n</tbody></table>\n<p><strong>State Management Architecture:</strong></p>\n<p>Module state management encompasses both trainable parameters and non-trainable state like running statistics in batch normalization or cached values for efficiency. The framework distinguishes between these state types while providing consistent interfaces for state access and modification.</p>\n<p>The training/evaluation mode system represents a critical state management concern. Some modules behave differently during training versus inference, and the framework must propagate mode changes throughout the entire module hierarchy automatically. This is implemented through recursive mode setting that traverses all submodules.</p>\n<p><strong>Error Handling Strategy:</strong></p>\n<p>The module system implements defensive error handling that catches common mistakes early with informative error messages. Shape mismatches, parameter registration errors, and invalid module compositions are detected and reported with context about the specific module and operation involved.</p>\n<p><strong>Common Error Detection:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Detection Point</th>\n<th>Error Message Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape Mismatch</td>\n<td>Forward pass entry</td>\n<td>Include expected vs actual shapes, layer name</td>\n</tr>\n<tr>\n<td>Parameter Registration</td>\n<td>Registration call</td>\n<td>Validate tensor type, gradient requirements</td>\n</tr>\n<tr>\n<td>Module Composition</td>\n<td>Container construction</td>\n<td>Check compatibility of adjacent layers</td>\n</tr>\n<tr>\n<td>Initialization</td>\n<td>Parameter creation</td>\n<td>Validate initialization parameters, warn about common issues</td>\n</tr>\n</tbody></table>\n<p>The extensibility architecture ensures that new module types can be added easily while following established patterns. The base class provides the essential infrastructure, and new modules need only implement the forward computation logic while following parameter registration conventions.</p>\n<p><strong>Extension Points:</strong></p>\n<ol>\n<li><strong>Custom Layer Types</strong>: Implement Module subclass with forward method and parameter registration</li>\n<li><strong>Custom Initialization</strong>: Override reset_parameters method or provide custom initialization functions</li>\n<li><strong>Custom Containers</strong>: Implement Module subclass with custom composition logic and parameter forwarding</li>\n<li><strong>Custom State Management</strong>: Extend base class with additional state tracking for specialized requirements</li>\n</ol>\n<h3 id=\"common-module-implementation-pitfalls\">Common Module Implementation Pitfalls</h3>\n<p>Building neural network modules involves numerous subtle details that can lead to hard-to-debug issues when implemented incorrectly. These pitfalls often manifest as training instabilities, incorrect gradients, or runtime errors that occur only in specific scenarios. Understanding and avoiding these common mistakes is crucial for successful framework implementation.</p>\n<p>⚠️ <strong>Pitfall: Parameters Not Properly Registered</strong></p>\n<p>One of the most common and frustrating errors occurs when module parameters are created but not registered with the parameter management system. This happens when developers create parameter tensors during module initialization but forget to call <code>self.register_parameter()</code>, or when they modify parameter references after registration.</p>\n<p>The symptom is that optimizers cannot find the parameters, leading to no learning during training. The model forward pass works correctly, but parameters never update despite calling optimizer.step(). This issue is particularly insidious because the forward pass computation is correct, making the problem seem like an optimizer or gradient computation bug.</p>\n<p><strong>Why it&#39;s wrong</strong>: Unregistered parameters are invisible to the parameter collection system, so optimizers never see them. Even if gradients are computed correctly, the optimizer has no reference to the parameter tensors for updates.</p>\n<p><strong>How to fix</strong>: Always call <code>self.register_parameter(name, tensor)</code> immediately after creating parameter tensors. Use descriptive names and verify registration by calling <code>list(module.parameters())</code> after module creation. Implement parameter registration checks in module unit tests.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Weight Initialization Destroying Training</strong></p>\n<p>Weight initialization has a dramatic impact on training dynamics, and poor initialization choices can make networks untrainable regardless of architecture or optimization algorithm quality. Common mistakes include using initialization schemes inappropriate for the activation function, failing to account for layer fan-in/fan-out, or using the same initialization for all layer types.</p>\n<p>The symptoms include vanishing gradients (loss doesn&#39;t decrease, gradients become very small), exploding gradients (loss becomes NaN, gradients grow exponentially), or extremely slow convergence (training makes progress but very slowly). These issues often manifest differently depending on network depth and architecture.</p>\n<p><strong>Why it&#39;s wrong</strong>: Inappropriate initialization breaks the careful balance of activation and gradient magnitudes that enable effective gradient-based learning. Xavier initialization assumes linear/sigmoid activations, while He initialization is designed for ReLU networks. Using the wrong scheme can cause activation saturation or gradient attenuation.</p>\n<p><strong>How to fix</strong>: Use Xavier/Glorot initialization for sigmoid/tanh networks and He/Kaiming initialization for ReLU networks. Initialize biases to zero unless specific architectural reasons suggest otherwise. Implement multiple initialization schemes and provide clear documentation about when to use each. Test initialization effects on toy problems to verify behavior.</p>\n<p>⚠️ <strong>Pitfall: In-Place Operations Breaking Gradient Computation</strong></p>\n<p>In-place operations modify tensor data directly rather than creating new tensors, which can break the computation graph required for backpropagation. This occurs when modules modify input tensors directly or when activation functions use in-place operations without proper gradient handling.</p>\n<p>The symptom is incorrect gradients during backpropagation, often manifesting as NaN gradients or gradients that don&#39;t match numerical differentiation. The error might not appear immediately but surface during gradient checking or when gradients are examined manually.</p>\n<p><strong>Why it&#39;s wrong</strong>: In-place operations can overwrite data needed for gradient computation or break the computation graph links between tensors and operations. The automatic differentiation system relies on preserving the computation history, which in-place operations can corrupt.</p>\n<p><strong>How to fix</strong>: Default to out-of-place operations for safety. When implementing in-place operations for memory efficiency, ensure they properly handle gradient computation by preserving necessary information for backpropagation. Implement gradient checking tests that compare automatic differentiation results with numerical differentiation.</p>\n<p>⚠️ <strong>Pitfall: Shape Broadcasting Edge Cases</strong></p>\n<p>Broadcasting rules can create subtle bugs when tensor shapes interact in unexpected ways, particularly when batch dimensions, singleton dimensions, or empty tensors are involved. Common issues include bias vectors that don&#39;t broadcast correctly, weight matrices with transposed dimensions, or operations that broadcast when they shouldn&#39;t.</p>\n<p>The symptoms include shape errors during forward pass, incorrect output shapes that cause downstream failures, or silent correctness bugs where operations produce wrong results with correct shapes. These issues often appear only with specific input shapes or batch sizes.</p>\n<p><strong>Why it&#39;s wrong</strong>: Incorrect shape handling leads to mathematical operations that don&#39;t correspond to the intended neural network computation. Even when operations succeed due to broadcasting, the result may not represent the desired linear transformation or element-wise operation.</p>\n<p><strong>How to fix</strong>: Implement explicit shape checking in forward methods, particularly for input validation and output verification. Test modules with various input shapes including edge cases like batch size 1, different input dimensions, and boundary conditions. Use descriptive variable names that indicate tensor shapes and include shape comments in complex operations.</p>\n<p>⚠️ <strong>Pitfall: Module State Inconsistencies During Mode Switching</strong></p>\n<p>Neural network modules can have different behavior during training versus evaluation (like dropout or batch normalization), and failing to handle mode switching correctly leads to inconsistent behavior across training and inference phases. This often occurs when custom modules don&#39;t properly implement mode switching or when mode changes aren&#39;t propagated to submodules.</p>\n<p>The symptoms include different outputs for the same input during training vs evaluation when no difference is expected, or identical outputs when differences are expected (like dropout being active in both modes). Performance discrepancies between training and inference can also indicate mode switching issues.</p>\n<p><strong>Why it&#39;s wrong</strong>: Inconsistent mode handling means the model behavior during training doesn&#39;t match inference behavior, leading to poor generalization or incorrect evaluation metrics. Some modules fundamentally require different behavior between modes to function correctly.</p>\n<p><strong>How to fix</strong>: Implement proper mode switching in the base Module class and ensure it propagates recursively to all submodules. Test modules in both training and evaluation modes to verify expected behavior differences. For stateless modules like most activations, ensure mode switching is properly ignored without causing errors.</p>\n<p>⚠️ <strong>Pitfall: Parameter Sharing Reference Management</strong></p>\n<p>When modules share parameters (like using the same embedding layer for input and output), improper reference management can lead to gradient accumulation issues, memory leaks, or incorrect parameter updates. This is particularly common in architectures with weight tying or when manually sharing parameters across modules.</p>\n<p>The symptoms include gradients that are too large (doubled when two modules share parameters), memory usage that grows over time, or parameters that update incorrectly during optimization. These issues can be subtle and may only appear in specific architectural configurations.</p>\n<p><strong>Why it&#39;s wrong</strong>: Parameter sharing requires careful coordination between the modules that share parameters, the automatic differentiation system that accumulates gradients, and the optimizer that applies updates. Incorrect reference management can break any of these interactions.</p>\n<p><strong>How to fix</strong>: When implementing parameter sharing, ensure that shared parameters are registered properly in all modules that use them. Test gradient accumulation behavior with shared parameters using numerical differentiation. Implement memory management tests to detect reference cycle issues that prevent garbage collection.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The module system represents the highest-level abstraction in our neural network framework, building directly on the tensor operations and automatic differentiation components developed in previous milestones. This implementation guidance provides concrete code structure and implementation patterns for creating the module hierarchy, parameter management system, and common layer implementations.</p>\n<p><strong>A. Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Base Module Class</td>\n<td>Plain Python class with dict storage</td>\n<td>Abstract base class with metaclass registration</td>\n</tr>\n<tr>\n<td>Parameter Storage</td>\n<td>Simple dict with string keys</td>\n<td>OrderedDict for consistent parameter ordering</td>\n</tr>\n<tr>\n<td>Initialization</td>\n<td>Basic random initialization</td>\n<td>Multiple initialization schemes with automatic selection</td>\n</tr>\n<tr>\n<td>Container Modules</td>\n<td>List-based sequential container</td>\n<td>Generic container with arbitrary connection patterns</td>\n</tr>\n<tr>\n<td>Type Checking</td>\n<td>Runtime assertions</td>\n<td>Full type hints with mypy validation</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>framework/\n  modules/\n    __init__.py              ← Module exports and public API\n    module.py                ← Base Module class and core functionality\n    linear.py                ← Linear layer implementation\n    activation.py            ← Activation function modules (ReLU, Sigmoid, Tanh)\n    container.py             ← Sequential and other container modules\n    parameter.py             ← Parameter registration and collection utilities\n    init.py                  ← Weight initialization functions\n  tests/\n    test_modules.py          ← Module system integration tests\n    test_linear.py           ← Linear layer specific tests\n    test_activations.py      ← Activation function tests\n    test_parameter_mgmt.py   ← Parameter management system tests</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p><strong>Parameter Registration Utilities (framework/modules/parameter.py):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Parameter management utilities for module system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Iterator, Tuple, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_parameter</span><span style=\"color:#E1E4E8\">(param: Any) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that an object is a proper parameter tensor.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        param: Object to validate as parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Validated tensor parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        TypeError: If param is not a Tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: If param doesn't have requires_grad=True</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(param, Tensor):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> TypeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Parameters must be Tensor instances, got </span><span style=\"color:#79B8FF\">{type</span><span style=\"color:#E1E4E8\">(param)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> param.requires_grad:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Parameter tensors must have requires_grad=True for training\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> param</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(parameters: Iterator[Tensor]) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Count total number of trainable parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        parameters: Iterator over parameter tensors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Total number of scalar parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> parameters:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> param.data.size</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> total</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parameter_summary</span><span style=\"color:#E1E4E8\">(named_params: Iterator[Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Tensor]]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate detailed parameter statistics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        named_params: Iterator over (name, parameter) pairs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Dictionary with parameter statistics and analysis</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'total_params'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'trainable_params'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'layers'</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'param_sizes'</span><span style=\"color:#E1E4E8\">: []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> named_params:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        param_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> param.data.size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        summary[</span><span style=\"color:#9ECBFF\">'total_params'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> param_count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> param.requires_grad:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            summary[</span><span style=\"color:#9ECBFF\">'trainable_params'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> param_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        layer_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> name.split(</span><span style=\"color:#9ECBFF\">'.'</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'root'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> layer_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> summary[</span><span style=\"color:#9ECBFF\">'layers'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            summary[</span><span style=\"color:#9ECBFF\">'layers'</span><span style=\"color:#E1E4E8\">][layer_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        summary[</span><span style=\"color:#9ECBFF\">'layers'</span><span style=\"color:#E1E4E8\">][layer_name] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> param_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        summary[</span><span style=\"color:#9ECBFF\">'param_sizes'</span><span style=\"color:#E1E4E8\">].append((name, param.shape, param_count))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> summary</span></span></code></pre></div>\n\n<p><strong>Weight Initialization Functions (framework/modules/init.py):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Weight initialization schemes for neural network parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> xavier_uniform_</span><span style=\"color:#E1E4E8\">(tensor: Tensor, gain: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize tensor with Xavier/Glorot uniform distribution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tensor: Parameter tensor to initialize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        gain: Scaling factor for initialization range</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialized tensor (modified in-place)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fan_in </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tensor.shape) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fan_out </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tensor.shape) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    std </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gain </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> math.sqrt(</span><span style=\"color:#79B8FF\">6.0</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> (fan_in </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> fan_out))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bound </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> std  </span><span style=\"color:#6A737D\"># For uniform distribution, bound = std</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.uniform(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">bound, bound, tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> kaiming_uniform_</span><span style=\"color:#E1E4E8\">(tensor: Tensor, a: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, mode: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'fan_in'</span><span style=\"color:#E1E4E8\">) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize tensor with Kaiming/He uniform distribution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tensor: Parameter tensor to initialize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        a: Negative slope for LeakyReLU (0 for ReLU)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        mode: Either 'fan_in' or 'fan_out'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialized tensor (modified in-place)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tensor.shape) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tensor.shape) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> tensor.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_input </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> mode </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'fan_in'</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> num_output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gain </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.sqrt(</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    std </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gain </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> math.sqrt(fan)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bound </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.sqrt(</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> std</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.uniform(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">bound, bound, tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> zeros_</span><span style=\"color:#E1E4E8\">(tensor: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize tensor with zeros.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tensor: Parameter tensor to initialize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialized tensor (modified in-place)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros(tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> ones_</span><span style=\"color:#E1E4E8\">(tensor: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize tensor with ones.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tensor: Parameter tensor to initialize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialized tensor (modified in-place)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.ones(tensor.shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tensor</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<p><strong>Base Module Class (framework/modules/module.py):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Base Module class providing core neural network module functionality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Iterator, Tuple, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .parameter </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> validate_parameter, count_parameters, parameter_summary</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Module</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all neural network modules.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides parameter registration, recursive operations, and training/eval modes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    All custom layers should inherit from this class and implement forward().</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize module with empty parameter and submodule storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._modules: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.training: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._name: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Define forward computation logic.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            *inputs: Input tensors for computation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor from forward computation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Note:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Subclasses must implement this method with their specific logic.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement forward pass computation specific to your module type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Ensure all operations create proper computation graph for gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle both single tensor and multi-tensor inputs as appropriate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return tensor with correct shape and gradient tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Subclasses must implement forward() method\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __call__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make module callable, delegating to forward method.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            *inputs: Input tensors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output from forward pass</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add any pre-forward hooks or validation here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call self.forward() with inputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add any post-forward hooks or processing here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return the forward pass result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_parameter</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, param: Optional[Tensor]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a parameter tensor for training.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            name: Parameter name for identification</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            param: Parameter tensor (None to remove parameter)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate parameter name is not empty and is string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If param is None, remove from _parameters dict if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If param is not None, validate it using validate_parameter()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store validated parameter in _parameters dict with given name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set the tensor's parameter name attribute for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_module</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, module: Optional[</span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a submodule for recursive operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            name: Submodule name for identification</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            module: Submodule instance (None to remove submodule)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate module name is not empty and is string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If module is None, remove from _modules dict if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If module is not None, validate it's a Module instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store module in _modules dict with given name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set the submodule's _name attribute for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parameters</span><span style=\"color:#E1E4E8\">(self, recurse: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> Iterator[Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return iterator over module parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            recurse: If True, include parameters from submodules</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Yields:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Parameter tensors in depth-first order</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through self._parameters.values() and yield each parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If recurse is True, iterate through self._modules.values()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each submodule, recursively call submodule.parameters(recurse=True)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Yield all parameters from submodules in depth-first order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Ensure consistent ordering across multiple calls</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> named_parameters</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, recurse: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> Iterator[Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Tensor]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return iterator over module parameters with names.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            prefix: Prefix to prepend to parameter names</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            recurse: If True, include parameters from submodules</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Yields:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            (name, parameter) pairs with hierarchical names</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through self._parameters.items() </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each (name, param), construct full_name using prefix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Yield (full_name, param) tuple</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If recurse is True, iterate through self._modules.items()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each submodule, construct submodule_prefix and recurse</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Ensure proper dot notation for hierarchical names</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> train</span><span style=\"color:#E1E4E8\">(self, mode: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set training mode recursively.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mode: Training mode (True) or evaluation mode (False)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set self.training = mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Iterate through all submodules in self._modules.values()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Call submodule.train(mode) for each submodule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return self to enable method chaining</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> eval</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set evaluation mode recursively.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Call self.train(False) to set evaluation mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return the result for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> zero_grad</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reset gradients for all parameters to None.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all parameters using self.parameters()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each parameter, set param.grad = None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: This clears accumulated gradients before backward pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Linear Layer Implementation (framework/modules/linear.py):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Linear (fully connected) layer implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .module </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Module</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .init </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> xavier_uniform_, zeros_</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Linear</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Linear transformation layer: y = xW^T + b</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Applies linear transformation to incoming data with learnable weights and bias.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports batched inputs and automatic gradient computation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, in_features: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, out_features: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, bias: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize linear layer.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            in_features: Size of input features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            out_features: Size of output features  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            bias: If True, add learnable bias vector</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.in_features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> in_features</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.out_features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> out_features</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create weight tensor with shape (out_features, in_features)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set requires_grad=True for weight tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Register weight as parameter using self.register_parameter()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If bias=True, create bias tensor with shape (out_features,)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set requires_grad=True for bias and register as parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If bias=False, set self.bias = None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Call self.reset_parameters() to initialize weights</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> reset_parameters</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize parameters using appropriate initialization scheme.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize self.weight using xavier_uniform_()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If self.bias is not None, initialize bias using zeros_()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Xavier initialization is good default for general networks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, input: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Forward pass: compute y = xW^T + b</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input: Input tensor of shape (batch_size, in_features)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor of shape (batch_size, out_features)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input tensor has correct number of dimensions (should be 2D)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input.shape[-1] == self.in_features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute matrix multiplication: input @ self.weight.T</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If self.bias is not None, add bias to result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return result tensor with proper gradient tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use tensor's matmul() method for matrix multiplication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Bias addition will broadcast automatically across batch dimension</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extra_repr</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return extra information for module representation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">'in_features=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.in_features</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, out_features=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.out_features</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, bias=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.bias </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None}</span><span style=\"color:#9ECBFF\">'</span></span></code></pre></div>\n\n<p><strong>ReLU Activation Implementation (framework/modules/activation.py):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Activation function modules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .module </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Module</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ReLU</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Rectified Linear Unit activation: f(x) = max(0, x)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Applies element-wise ReLU function with optional in-place operation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, inplace: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize ReLU activation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            inplace: If True, modify input tensor directly (saves memory)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.inplace </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inplace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, input: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply ReLU activation element-wise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input: Input tensor of any shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor with ReLU applied element-wise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply max(0, x) operation element-wise to input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If self.inplace is True, modify input tensor directly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If self.inplace is False, create new tensor for output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure proper gradient tracking in computation graph</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return result tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: You may need to implement a ReLU operation in your tensor class</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Can use tensor comparison and multiplication operations if no direct ReLU</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Sigmoid</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Sigmoid activation: f(x) = 1 / (1 + exp(-x))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Applies element-wise sigmoid function with numerical stability.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, input: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply sigmoid activation element-wise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input: Input tensor of any shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor with sigmoid applied element-wise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement numerically stable sigmoid computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For positive x, use: exp(x) / (1 + exp(x))  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For negative x, use: 1 / (1 + exp(-x))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: This avoids overflow for large positive/negative values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return result with proper gradient tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: May need to implement conditional operations or use numpy functions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tanh</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Hyperbolic tangent activation: f(x) = tanh(x)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Applies element-wise tanh function.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, input: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply tanh activation element-wise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input: Input tensor of any shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor with tanh applied element-wise  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply tanh function element-wise to input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Can use numpy.tanh or implement using exponentials</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure proper gradient computation in automatic differentiation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return result tensor with gradient tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Sequential Container Implementation (framework/modules/container.py):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Container modules for composing neural networks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .module </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Module</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Sequential</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Sequential container that chains modules in order.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Passes input through each module sequentially, where output of module i</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    becomes input to module i+1.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">modules: Module):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize sequential container.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            *modules: Variable number of modules to chain sequentially</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store modules in an ordered list or dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Register each module using self.register_module() with numeric names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate that all provided objects are Module instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle empty module list case appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use enumerate() to generate numeric names like \"0\", \"1\", \"2\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, input: Tensor) -> Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Forward pass through all modules sequentially.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input: Input tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor after passing through all modules</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start with input tensor as current value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Iterate through all registered modules in order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each module, call module(current_value) to get next value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update current_value with the result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return final current_value as output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle case where no modules are registered</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(self, module: Module) -> </span><span style=\"color:#9ECBFF\">'Sequential'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add a module to the end of the sequence.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            module: Module to append</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate next numeric name for the module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Register module using self.register_module()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>super().__init__()</code> in all module constructors to initialize base class properly</li>\n<li>Store module parameters in <code>_parameters</code> dict and submodules in <code>_modules</code> dict with descriptive names</li>\n<li>Use <code>isinstance(obj, Tensor)</code> to validate parameter types during registration</li>\n<li>Implement <code>__repr__()</code> methods in modules for better debugging and visualization</li>\n<li>Use <code>typing</code> module for clear type hints, especially <code>Optional[Tensor]</code> for optional parameters</li>\n<li>Consider using <code>@property</code> decorators for computed attributes like parameter counts</li>\n<li>Use descriptive variable names that indicate tensor shapes: <code>input_batch</code>, <code>weight_matrix</code>, <code>bias_vector</code></li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the module system, verify functionality with these tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test linear layer creation and forward pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">linear </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Linear(</span><span style=\"color:#FFAB70\">in_features</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">784</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">out_features</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">input_batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.random.randn(</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">784</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> linear(input_batch)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> output.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test parameter collection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">params </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(linear.parameters())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(params) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">  # weight and bias</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> params[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">784</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># weight</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> params[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">,)      </span><span style=\"color:#6A737D\"># bias</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test sequential container</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Sequential(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Linear(</span><span style=\"color:#79B8FF\">784</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReLU(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Linear(</span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReLU(), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Linear(</span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test end-to-end forward pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model(input_batch)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> output.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test parameter counting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">total_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.data.size </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters())</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Total parameters: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">total_params</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test gradient flow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> output.sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> param.grad.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> param.shape</span></span></code></pre></div>\n\n<p>Expected behaviors:</p>\n<ul>\n<li>Linear layer should initialize weights with reasonable values (not all zeros or very large)</li>\n<li>ReLU should zero out negative values while preserving positive values</li>\n<li>Sequential container should chain operations correctly with proper shape propagation</li>\n<li>Parameter collection should find all parameters recursively through the hierarchy</li>\n<li>Gradients should flow back through all parameters after loss.backward()</li>\n</ul>\n<p>Signs something is wrong:</p>\n<ul>\n<li>Parameters are all zeros after initialization → Check initialization implementation</li>\n<li>Shapes don&#39;t match expectations → Review broadcasting and matrix multiplication logic</li>\n<li>Parameters not found by optimizer → Check parameter registration calls</li>\n<li>Gradients are None after backward pass → Verify computation graph construction in forward pass</li>\n</ul>\n<h2 id=\"optimizers-and-training-loop-milestone-4\">Optimizers and Training Loop (Milestone 4)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 (Optimizers &amp; Training) - implements optimization algorithms like SGD and Adam, plus training loop infrastructure with loss functions and learning rate scheduling.</p>\n</blockquote>\n<p>Now that we have built tensors with automatic differentiation and composable neural network modules, we face the final challenge: how do we actually train these networks to learn from data? The training process requires coordinating multiple components—computing predictions, measuring errors, calculating gradients, and updating parameters—in a carefully orchestrated dance that transforms random weights into a functioning neural network.</p>\n<p>The optimization layer sits at the top of our four-layer architecture, orchestrating the interaction between all lower layers. It takes the gradients computed by our automatic differentiation engine and uses them to improve the parameters stored in our neural network modules. This process of iterative improvement, guided by mathematical optimization principles, is what enables neural networks to learn complex patterns from data.</p>\n<h3 id=\"optimizers-as-gps-navigation\">Optimizers as GPS Navigation</h3>\n<p>Think of training a neural network like navigating through an unfamiliar mountainous landscape in dense fog, where your goal is to reach the lowest valley (the minimum loss). You can&#39;t see the entire terrain, but you have a special GPS device that tells you the slope of the ground right where you&#39;re standing. This slope information is your <strong>gradient</strong>—it points in the direction of steepest uphill climb.</p>\n<p>An <strong>optimizer</strong> is like your navigation strategy for using this gradient information to reach the valley. Just as different GPS routing algorithms make different trade-offs between speed and accuracy, different optimizers use gradient information in different ways:</p>\n<ul>\n<li><p><strong>Stochastic Gradient Descent (SGD)</strong> is like always walking directly downhill from your current position. It&#39;s simple and reliable, but might get stuck in small dips or take inefficient zigzag paths down narrow valleys.</p>\n</li>\n<li><p><strong>SGD with Momentum</strong> is like rolling a heavy ball downhill instead of walking. The ball builds up speed in consistent directions and pushes through small obstacles, but takes longer to change direction when the terrain shifts.</p>\n</li>\n<li><p><strong>Adam</strong> is like having an advanced GPS that remembers the terrain you&#39;ve already explored and adapts its recommendations based on both recent slopes and long-term patterns. It automatically adjusts step sizes and has built-in shock absorbers for noisy terrain.</p>\n</li>\n</ul>\n<p>The key insight is that optimizers don&#39;t just use the current gradient—they maintain <strong>state</strong> about the optimization process. This state might include momentum from previous steps, running averages of gradient magnitudes, or adaptive learning rates. Just as your GPS routing improves by remembering traffic patterns and road conditions, optimizers improve parameter updates by remembering the history of gradients.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The optimizer is the only component that modifies parameter values. Modules compute gradients, but they never update their own parameters. This separation of concerns ensures that optimization strategies can be changed independently of network architecture.</p>\n</blockquote>\n<h3 id=\"stochastic-gradient-descent\">Stochastic Gradient Descent</h3>\n<p>Stochastic Gradient Descent represents the foundation of neural network optimization. Despite its simplicity, SGD with proper tuning can train state-of-the-art models and provides the conceptual baseline for understanding more sophisticated optimizers.</p>\n<p>The core SGD algorithm updates each parameter by subtracting a small fraction of its gradient. If we think of the loss function as a hilly landscape, the gradient at any point indicates the direction of steepest ascent. By moving in the opposite direction (negative gradient), we move toward lower loss values. The <strong>learning rate</strong> controls how large steps we take—too small and training progresses slowly, too large and we might overshoot the minimum.</p>\n<p><strong>Basic SGD Update Rule:</strong></p>\n<ol>\n<li>Compute gradients for all parameters using backpropagation</li>\n<li>For each parameter, subtract learning rate times its gradient</li>\n<li>Mathematically: <code>parameter = parameter - learning_rate * gradient</code></li>\n<li>Clear gradients to prepare for the next training step</li>\n</ol>\n<p>However, basic SGD suffers from several limitations. In narrow valleys, it oscillates back and forth across the valley walls instead of making steady progress along the valley floor. When the loss landscape has different curvatures in different directions, SGD struggles to find efficient paths to the minimum.</p>\n<p><strong>Momentum Enhancement</strong> addresses these issues by accumulating a velocity vector that represents the exponentially weighted moving average of gradients. This creates inertia that smooths out oscillations and accelerates movement in consistent directions.</p>\n<p><strong>SGD with Momentum Algorithm:</strong></p>\n<ol>\n<li>Compute current gradients using backpropagation</li>\n<li>Update velocity: <code>velocity = momentum_coefficient * previous_velocity + gradient</code></li>\n<li>Update parameters: <code>parameter = parameter - learning_rate * velocity</code></li>\n<li>Store velocity for the next iteration</li>\n<li>Clear gradients</li>\n</ol>\n<p>The momentum coefficient (typically 0.9) controls how much previous gradients influence the current update. Higher values create more inertia, which helps with optimization stability but can make the optimizer slower to respond to changes in gradient direction.</p>\n<p><strong>SGD Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>parameters</code></td>\n<td><code>List[Tensor]</code></td>\n<td>References to all trainable tensors in the model</td>\n</tr>\n<tr>\n<td><code>learning_rate</code></td>\n<td><code>float</code></td>\n<td>Step size multiplier for parameter updates</td>\n</tr>\n<tr>\n<td><code>momentum</code></td>\n<td><code>float</code></td>\n<td>Coefficient for velocity accumulation (0.0 to 1.0)</td>\n</tr>\n<tr>\n<td><code>velocity_buffers</code></td>\n<td><code>Dict[Tensor, Tensor]</code></td>\n<td>Stores momentum velocity for each parameter</td>\n</tr>\n<tr>\n<td><code>weight_decay</code></td>\n<td><code>float</code></td>\n<td>L2 regularization coefficient for parameter shrinkage</td>\n</tr>\n</tbody></table>\n<p><strong>SGD Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>__init__</code></td>\n<td><code>parameters, lr, momentum, weight_decay</code></td>\n<td><code>None</code></td>\n<td>Initialize optimizer with parameter references</td>\n</tr>\n<tr>\n<td><code>step</code></td>\n<td><code>None</code></td>\n<td><code>None</code></td>\n<td>Apply one optimization step to all parameters</td>\n</tr>\n<tr>\n<td><code>zero_grad</code></td>\n<td><code>None</code></td>\n<td><code>None</code></td>\n<td>Reset all parameter gradients to None</td>\n</tr>\n<tr>\n<td><code>state_dict</code></td>\n<td><code>None</code></td>\n<td><code>Dict</code></td>\n<td>Export optimizer state for checkpointing</td>\n</tr>\n<tr>\n<td><code>load_state_dict</code></td>\n<td><code>state_dict</code></td>\n<td><code>None</code></td>\n<td>Restore optimizer state from checkpoint</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Separate Gradient Clearing from Parameter Updates</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to decide whether <code>optimizer.step()</code> should automatically clear gradients or require explicit <code>zero_grad()</code> calls</li>\n<li><strong>Options Considered</strong>: Auto-clear after updates vs. manual clearing vs. clear before updates</li>\n<li><strong>Decision</strong>: Manual clearing with separate <code>zero_grad()</code> method</li>\n<li><strong>Rationale</strong>: Explicit gradient clearing provides more control for advanced techniques like gradient accumulation across multiple batches, and matches PyTorch conventions for familiarity</li>\n<li><strong>Consequences</strong>: Requires developers to remember to call <code>zero_grad()</code>, but enables gradient accumulation and debugging workflows where gradients need inspection</li>\n</ul>\n</blockquote>\n<h3 id=\"adam-optimizer\">Adam Optimizer</h3>\n<p>The Adam optimizer (Adaptive Moment Estimation) represents one of the most significant advances in neural network optimization. Adam combines the benefits of momentum with adaptive learning rates that automatically adjust based on the historical behavior of each parameter&#39;s gradients. This makes Adam particularly effective for training deep networks and handling sparse gradients.</p>\n<p>Adam maintains two types of moving averages for each parameter: <strong>first moments</strong> (exponentially weighted average of gradients) and <strong>second moments</strong> (exponentially weighted average of squared gradients). The first moment provides momentum-like behavior, while the second moment enables adaptive learning rates that are larger for parameters with consistently small gradients and smaller for parameters with large or variable gradients.</p>\n<p><strong>Adam&#39;s Key Innovations:</strong></p>\n<ol>\n<li><strong>Per-parameter adaptive learning rates</strong> based on gradient magnitude history</li>\n<li><strong>Bias correction</strong> that accounts for initialization bias in the moving averages</li>\n<li><strong>Numerical stability</strong> through epsilon term preventing division by zero</li>\n<li><strong>Default hyperparameters</strong> that work well across a wide range of problems</li>\n</ol>\n<p><strong>Adam Algorithm Steps:</strong></p>\n<ol>\n<li>Compute current gradients using backpropagation</li>\n<li>Update first moment estimate: <code>m = beta1 * m + (1 - beta1) * gradient</code></li>\n<li>Update second moment estimate: <code>v = beta2 * v + (1 - beta2) * gradient²</code></li>\n<li>Apply bias correction: <code>m_corrected = m / (1 - beta1^t)</code> and <code>v_corrected = v / (1 - beta2^t)</code></li>\n<li>Update parameters: <code>parameter = parameter - learning_rate * m_corrected / (sqrt(v_corrected) + epsilon)</code></li>\n<li>Increment time step counter <code>t</code></li>\n<li>Clear gradients</li>\n</ol>\n<p>The <strong>bias correction</strong> step is crucial and frequently misunderstood. Because the first and second moment estimates are initialized to zero, they are biased toward zero during the early steps of training. The correction terms <code>(1 - beta1^t)</code> and <code>(1 - beta2^t)</code> compensate for this bias, where <code>t</code> is the current time step. As training progresses and <code>t</code> increases, these correction terms approach 1.0 and have minimal effect.</p>\n<p><strong>Adam Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>parameters</code></td>\n<td><code>List[Tensor]</code></td>\n<td>References to all trainable tensors in the model</td>\n</tr>\n<tr>\n<td><code>learning_rate</code></td>\n<td><code>float</code></td>\n<td>Base learning rate before adaptive scaling</td>\n</tr>\n<tr>\n<td><code>beta1</code></td>\n<td><code>float</code></td>\n<td>Exponential decay rate for first moment estimates (default: 0.9)</td>\n</tr>\n<tr>\n<td><code>beta2</code></td>\n<td><code>float</code></td>\n<td>Exponential decay rate for second moment estimates (default: 0.999)</td>\n</tr>\n<tr>\n<td><code>epsilon</code></td>\n<td><code>float</code></td>\n<td>Small constant for numerical stability (default: 1e-8)</td>\n</tr>\n<tr>\n<td><code>weight_decay</code></td>\n<td><code>float</code></td>\n<td>L2 regularization coefficient</td>\n</tr>\n<tr>\n<td><code>first_moments</code></td>\n<td><code>Dict[Tensor, Tensor]</code></td>\n<td>First moment estimates for each parameter</td>\n</tr>\n<tr>\n<td><code>second_moments</code></td>\n<td><code>Dict[Tensor, Tensor]</code></td>\n<td>Second moment estimates for each parameter</td>\n</tr>\n<tr>\n<td><code>step_count</code></td>\n<td><code>int</code></td>\n<td>Current optimization step for bias correction</td>\n</tr>\n</tbody></table>\n<p><strong>Adam Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>__init__</code></td>\n<td><code>parameters, lr, betas, eps, weight_decay</code></td>\n<td><code>None</code></td>\n<td>Initialize Adam with hyperparameters</td>\n</tr>\n<tr>\n<td><code>step</code></td>\n<td><code>None</code></td>\n<td><code>None</code></td>\n<td>Apply one Adam optimization step with bias correction</td>\n</tr>\n<tr>\n<td><code>zero_grad</code></td>\n<td><code>None</code></td>\n<td><code>None</code></td>\n<td>Reset all parameter gradients to None</td>\n</tr>\n<tr>\n<td><code>state_dict</code></td>\n<td><code>None</code></td>\n<td><code>Dict</code></td>\n<td>Export optimizer state including step count and moments</td>\n</tr>\n<tr>\n<td><code>load_state_dict</code></td>\n<td><code>state_dict</code></td>\n<td><code>None</code></td>\n<td>Restore optimizer state from checkpoint</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Store Per-Parameter State vs. Global State</strong></p>\n<ul>\n<li><strong>Context</strong>: Adam needs to track first and second moments for each parameter separately</li>\n<li><strong>Options Considered</strong>: Dictionary mapping tensors to state vs. global state arrays vs. parameter-attached state</li>\n<li><strong>Decision</strong>: Dictionary mapping parameter tensors to their moment estimates</li>\n<li><strong>Rationale</strong>: Provides natural association between parameters and their optimization state, handles variable parameter counts gracefully, and matches established optimizer patterns</li>\n<li><strong>Consequences</strong>: Requires careful handling of tensor identity for dictionary keys, but provides clear state organization and supports dynamic parameter addition/removal</li>\n</ul>\n</blockquote>\n<h3 id=\"training-loop-architecture\">Training Loop Architecture</h3>\n<p>The training loop represents the high-level orchestration that coordinates all components of our neural network framework. It manages the flow of data through the network, coordinates forward and backward passes, applies optimization steps, and tracks training progress. A well-designed training loop abstracts away the mechanical details while providing flexibility for different training strategies.</p>\n<p>Think of the training loop as the conductor of an orchestra, ensuring that each section (data loading, forward pass, loss computation, backpropagation, optimization) plays their part at exactly the right time. The conductor doesn&#39;t play any instruments directly, but coordinates the entire performance to create a coherent result.</p>\n<p><strong>Core Training Loop Components:</strong></p>\n<ol>\n<li><strong>Data Loading</strong>: Batching samples from the dataset and shuffling for each epoch</li>\n<li><strong>Forward Pass</strong>: Computing model predictions for the current batch</li>\n<li><strong>Loss Computation</strong>: Measuring prediction quality against ground truth labels</li>\n<li><strong>Backward Pass</strong>: Computing gradients via automatic differentiation</li>\n<li><strong>Parameter Update</strong>: Applying optimizer step to improve parameters</li>\n<li><strong>Progress Tracking</strong>: Logging metrics and monitoring convergence</li>\n</ol>\n<p><strong>Training Loop Data Flow:</strong></p>\n<ol>\n<li>Load a mini-batch of samples and labels from the training dataset</li>\n<li>Forward propagate samples through the network to generate predictions</li>\n<li>Compute loss by comparing predictions against ground truth labels</li>\n<li>Clear previous gradients from all parameters using <code>optimizer.zero_grad()</code></li>\n<li>Backward propagate loss through the network to compute parameter gradients</li>\n<li>Apply optimizer step to update parameters based on computed gradients</li>\n<li>Record training metrics (loss, accuracy) for monitoring progress</li>\n<li>Repeat for next batch until epoch complete, then shuffle data for next epoch</li>\n</ol>\n<p><strong>Training State Management:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>model</code></td>\n<td><code>Module</code></td>\n<td>Neural network with trainable parameters</td>\n</tr>\n<tr>\n<td><code>optimizer</code></td>\n<td><code>Optimizer</code></td>\n<td>Parameter update strategy (SGD, Adam, etc.)</td>\n</tr>\n<tr>\n<td><code>loss_function</code></td>\n<td><code>Loss</code></td>\n<td>Differentiable loss computation</td>\n</tr>\n<tr>\n<td><code>data_loader</code></td>\n<td><code>DataLoader</code></td>\n<td>Batched, shuffled dataset iteration</td>\n</tr>\n<tr>\n<td><code>current_epoch</code></td>\n<td><code>int</code></td>\n<td>Current training epoch number</td>\n</tr>\n<tr>\n<td><code>global_step</code></td>\n<td><code>int</code></td>\n<td>Total number of optimization steps taken</td>\n</tr>\n<tr>\n<td><code>best_loss</code></td>\n<td><code>float</code></td>\n<td>Best validation loss seen during training</td>\n</tr>\n<tr>\n<td><code>scheduler</code></td>\n<td><code>LRScheduler</code></td>\n<td>Learning rate decay strategy</td>\n</tr>\n</tbody></table>\n<p><strong>Training Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>train_epoch</code></td>\n<td><code>model, data_loader, optimizer, loss_fn</code></td>\n<td><code>float</code></td>\n<td>Train for one epoch, return average loss</td>\n</tr>\n<tr>\n<td><code>validate</code></td>\n<td><code>model, data_loader, loss_fn</code></td>\n<td><code>float</code></td>\n<td>Evaluate model on validation set</td>\n</tr>\n<tr>\n<td><code>training_step</code></td>\n<td><code>batch, labels</code></td>\n<td><code>Tensor</code></td>\n<td>Single forward/backward/update cycle</td>\n</tr>\n<tr>\n<td><code>save_checkpoint</code></td>\n<td><code>path, epoch, model, optimizer</code></td>\n<td><code>None</code></td>\n<td>Save training state to disk</td>\n</tr>\n<tr>\n<td><code>load_checkpoint</code></td>\n<td><code>path</code></td>\n<td><code>Dict</code></td>\n<td>Restore training state from disk</td>\n</tr>\n</tbody></table>\n<p>The training loop also handles <strong>mode switching</strong> between training and evaluation. During training, modules like dropout and batch normalization behave differently than during evaluation. The <code>model.train()</code> and <code>model.eval()</code> methods recursively set the mode for all modules in the network.</p>\n<p><strong>Batch Processing Strategy:</strong>\nMini-batch training provides a crucial balance between gradient accuracy and computational efficiency. Processing individual samples would provide the most frequent parameter updates but would be computationally inefficient and provide noisy gradient estimates. Processing the entire dataset would provide the most accurate gradients but would be computationally prohibitive and could lead to poor generalization.</p>\n<p>Mini-batches typically contain 32-256 samples, chosen based on available memory and convergence characteristics. The batch size affects both training speed and final model quality—larger batches provide more stable gradients but may converge to worse local minima.</p>\n<blockquote>\n<p><strong>Decision: Separate Training Step from Training Loop</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to decide how to structure the relationship between individual training steps and the overall training loop</li>\n<li><strong>Options Considered</strong>: Monolithic training loop vs. separate training step function vs. trainer class with methods</li>\n<li><strong>Decision</strong>: Separate <code>training_step</code> function called by higher-level training loop</li>\n<li><strong>Rationale</strong>: Enables testing individual training steps in isolation, supports different training strategies (standard training, adversarial training, etc.), and provides clear separation between batch processing and epoch management</li>\n<li><strong>Consequences</strong>: Requires passing state between functions, but improves modularity and testing capabilities</li>\n</ul>\n</blockquote>\n<h3 id=\"loss-function-implementation\">Loss Function Implementation</h3>\n<p>Loss functions serve as the bridge between model predictions and the optimization process by providing a differentiable measure of prediction quality. They must compute both the scalar loss value used for monitoring progress and the gradients needed for backpropagation. The design of loss functions significantly impacts training dynamics, convergence speed, and final model performance.</p>\n<p>A loss function transforms the model&#39;s predictions and ground truth labels into a scalar value that quantifies how &quot;wrong&quot; the predictions are. This scalar serves as the starting point for backpropagation—we compute gradients of this loss with respect to all model parameters, then use those gradients to improve the parameters via optimization.</p>\n<p><strong>Loss Function Requirements:</strong></p>\n<ol>\n<li><strong>Differentiability</strong>: Must provide gradients for all inputs to enable backpropagation</li>\n<li><strong>Numerical Stability</strong>: Should avoid operations that cause overflow, underflow, or NaN values</li>\n<li><strong>Appropriate Scale</strong>: Loss magnitude should be reasonable for the optimizer&#39;s learning rate</li>\n<li><strong>Task Alignment</strong>: Loss should accurately reflect the quality metric we care about</li>\n<li><strong>Batch Processing</strong>: Must handle mini-batches efficiently with proper reduction</li>\n</ol>\n<p><strong>Mean Squared Error (MSE) Loss:</strong>\nMSE loss is primarily used for regression tasks where we predict continuous values. It computes the average squared difference between predictions and targets, which penalizes large errors more heavily than small errors due to the squaring operation.</p>\n<p><strong>MSE Computation Steps:</strong></p>\n<ol>\n<li>Compute elementwise squared differences: <code>squared_diff = (predictions - targets)²</code></li>\n<li>Sum across all dimensions to get per-sample losses</li>\n<li>Average across the batch dimension: <code>loss = mean(sum(squared_diff, dims=-1))</code></li>\n<li>For backpropagation, gradient w.r.t. predictions: <code>2 * (predictions - targets) / batch_size</code></li>\n</ol>\n<p><strong>Cross-Entropy Loss:</strong>\nCross-entropy loss is the standard choice for classification tasks. It measures the difference between the predicted probability distribution and the true distribution (typically one-hot encoded labels). Cross-entropy loss encourages the model to assign high probability to the correct class while keeping probabilities for incorrect classes low.</p>\n<p><strong>Cross-Entropy Computation Steps:</strong></p>\n<ol>\n<li>Apply softmax to convert logits to probabilities: <code>probabilities = softmax(logits)</code></li>\n<li>Compute negative log-likelihood: <code>loss = -sum(targets * log(probabilities + epsilon))</code></li>\n<li>Average across batch dimension</li>\n<li>For backpropagation, gradient combines softmax and cross-entropy derivatives</li>\n</ol>\n<p>The epsilon term in cross-entropy prevents taking the logarithm of zero, which would result in infinite loss. However, a more numerically stable approach combines softmax and cross-entropy computation to avoid intermediate probability values that might underflow.</p>\n<p><strong>Loss Function Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>reduction</code></td>\n<td><code>str</code></td>\n<td>How to reduce batch dimension: &#39;mean&#39;, &#39;sum&#39;, or &#39;none&#39;</td>\n</tr>\n<tr>\n<td><code>ignore_index</code></td>\n<td><code>int</code></td>\n<td>Label value to ignore in loss computation</td>\n</tr>\n<tr>\n<td><code>label_smoothing</code></td>\n<td><code>float</code></td>\n<td>Smoothing factor to prevent overconfident predictions</td>\n</tr>\n<tr>\n<td><code>class_weights</code></td>\n<td><code>Tensor</code></td>\n<td>Per-class weights for handling imbalanced datasets</td>\n</tr>\n</tbody></table>\n<p><strong>Loss Function Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>forward</code></td>\n<td><code>predictions, targets</code></td>\n<td><code>Tensor</code></td>\n<td>Compute loss value with gradient tracking</td>\n</tr>\n<tr>\n<td><code>__call__</code></td>\n<td><code>predictions, targets</code></td>\n<td><code>Tensor</code></td>\n<td>Convenience method that calls forward</td>\n</tr>\n<tr>\n<td><code>backward</code></td>\n<td><code>grad_output</code></td>\n<td><code>Tuple[Tensor, ...]</code></td>\n<td>Compute gradients w.r.t. inputs</td>\n</tr>\n</tbody></table>\n<p><strong>Numerical Stability Considerations:</strong>\nLoss functions must handle edge cases that commonly occur during training. For cross-entropy loss, predicted probabilities near zero cause log(0) which results in infinite loss. For MSE loss, very large prediction errors can cause gradient explosion. Proper implementation includes clipping, epsilon terms, and numerically stable formulations.</p>\n<p><strong>Loss Reduction Strategies:</strong>\nLoss functions typically compute per-sample losses, then reduce across the batch dimension. The reduction strategy affects gradient magnitudes and training dynamics:</p>\n<ul>\n<li><strong>Mean reduction</strong>: Gradients are independent of batch size, providing consistent training dynamics</li>\n<li><strong>Sum reduction</strong>: Gradients scale with batch size, requiring learning rate adjustment</li>\n<li><strong>No reduction</strong>: Returns per-sample losses, useful for custom weighting or analysis</li>\n</ul>\n<blockquote>\n<p><strong>Decision: Numerically Stable Cross-Entropy Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Standard cross-entropy computation can suffer from numerical instability when probabilities approach zero</li>\n<li><strong>Options Considered</strong>: Separate softmax + cross-entropy vs. fused log-softmax + NLL vs. epsilon clipping</li>\n<li><strong>Decision</strong>: Fused log-softmax computation that combines operations for numerical stability</li>\n<li><strong>Rationale</strong>: Avoids intermediate probability computation that can underflow, provides more accurate gradients, and matches behavior of production frameworks</li>\n<li><strong>Consequences</strong>: Requires more complex implementation but provides better numerical stability and gradient accuracy</li>\n</ul>\n</blockquote>\n<h3 id=\"training-architecture-decisions\">Training Architecture Decisions</h3>\n<p>The design of the training system involves several critical architectural decisions that affect flexibility, performance, and maintainability. These decisions establish patterns that will influence how users interact with the framework and how easily it can be extended with new features.</p>\n<blockquote>\n<p><strong>Decision: Optimizer Parameter Registration Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Optimizers need references to all trainable parameters, but modules can be nested arbitrarily deep and parameters can be added dynamically</li>\n<li><strong>Options Considered</strong>: Pass parameters at optimizer creation vs. automatic parameter discovery vs. manual registration per parameter</li>\n<li><strong>Decision</strong>: Pass parameter iterator from <code>model.parameters()</code> at optimizer creation</li>\n<li><strong>Rationale</strong>: Leverages the module system&#39;s recursive parameter collection, works with any module hierarchy, and follows established patterns from PyTorch</li>\n<li><strong>Consequences</strong>: Requires rebuilding optimizer if model structure changes, but provides clean separation between model definition and optimization strategy</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Learning Rate Scheduling Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Learning rates typically need to decay during training based on epochs, steps, or validation metrics</li>\n<li><strong>Options Considered</strong>: Built into optimizers vs. separate scheduler objects vs. callback system</li>\n<li><strong>Decision</strong>: Separate scheduler objects that modify optimizer learning rates</li>\n<li><strong>Rationale</strong>: Allows combining any scheduler with any optimizer, enables complex scheduling strategies, and separates concerns cleanly</li>\n<li><strong>Consequences</strong>: Requires coordinating scheduler.step() calls with training loop, but provides maximum flexibility</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Training Loop State Management</strong></p>\n<ul>\n<li><strong>Context</strong>: Training requires coordinating state across multiple components (model, optimizer, scheduler, metrics)</li>\n<li><strong>Options Considered</strong>: Monolithic trainer class vs. functional training loop vs. stateful training manager</li>\n<li><strong>Decision</strong>: Functional training utilities with explicit state passing</li>\n<li><strong>Rationale</strong>: Provides maximum flexibility for custom training loops, easier to test individual components, and avoids complex state management</li>\n<li><strong>Consequences</strong>: Requires more manual coordination but enables advanced training strategies and easier debugging</li>\n</ul>\n</blockquote>\n<p><strong>Optimizer State Persistence:</strong>\nOptimizers maintain internal state (momentum buffers, Adam moments) that should be saved and restored during checkpointing. This state is often larger than the model parameters themselves, especially for optimizers like Adam that maintain multiple state tensors per parameter.</p>\n<table>\n<thead>\n<tr>\n<th>State Component</th>\n<th>Description</th>\n<th>Persistence Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter References</td>\n<td>Pointers to trainable tensors</td>\n<td>Not saved (reconstructed from model)</td>\n</tr>\n<tr>\n<td>Momentum Buffers</td>\n<td>Velocity accumulation for SGD</td>\n<td>Saved as tensor dictionaries</td>\n</tr>\n<tr>\n<td>First/Second Moments</td>\n<td>Adam&#39;s gradient statistics</td>\n<td>Saved with parameter tensor keys</td>\n</tr>\n<tr>\n<td>Step Counters</td>\n<td>For bias correction and scheduling</td>\n<td>Saved as scalar values</td>\n</tr>\n<tr>\n<td>Hyperparameters</td>\n<td>Learning rate, beta values</td>\n<td>Saved for reconstruction</td>\n</tr>\n</tbody></table>\n<p><strong>Learning Rate Scheduling Strategies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Schedule Type</th>\n<th>Description</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Step Decay</td>\n<td>Multiply by factor every N epochs</td>\n<td>Simple baseline, well-understood</td>\n</tr>\n<tr>\n<td>Exponential Decay</td>\n<td>Continuous exponential reduction</td>\n<td>Smooth decay, theoretical justification</td>\n</tr>\n<tr>\n<td>Cosine Annealing</td>\n<td>Cosine curve with restarts</td>\n<td>Modern deep learning, cyclical training</td>\n</tr>\n<tr>\n<td>Reduce on Plateau</td>\n<td>Decay when validation loss stagnates</td>\n<td>Adaptive to training progress</td>\n</tr>\n<tr>\n<td>Warmup + Decay</td>\n<td>Linear increase then exponential decay</td>\n<td>Large batch training, transformer models</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-training-pitfalls\">Common Training Pitfalls</h3>\n<p>Training neural networks involves numerous subtle implementation details that can silently break the optimization process. These pitfalls often manifest as slow convergence, poor final performance, or training instability rather than obvious errors. Understanding and avoiding these common mistakes is crucial for successful framework implementation.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Adam Bias Correction</strong>\nMany implementations incorrectly apply bias correction by using the current step count for all parameters, rather than tracking separate step counts per parameter. This becomes critical when parameters are added or removed during training, or when using different optimizers for different parameter groups.</p>\n<p><strong>Why it&#39;s wrong</strong>: Adam&#39;s bias correction terms <code>(1 - beta1^t)</code> and <code>(1 - beta2^t)</code> compensate for the initialization bias in moment estimates. Using a global step counter means parameters added later in training receive incorrect bias correction, leading to poor optimization behavior.</p>\n<p><strong>How to fix</strong>: Track step counts per parameter or per parameter group, not globally. Initialize step count to 1 when parameters are first added to the optimizer.</p>\n<p>⚠️ <strong>Pitfall: Learning Rate Scale Mismatch</strong>\nDifferent optimizers require different learning rate scales due to their internal mechanics. A learning rate that works well for SGD will typically be too large for Adam, and vice versa. Additionally, batch size changes require learning rate adjustments for consistent training dynamics.</p>\n<p><strong>Why it&#39;s wrong</strong>: Adam&#39;s adaptive scaling effectively increases the effective learning rate compared to SGD, especially early in training. Using SGD learning rates with Adam can cause unstable training or poor convergence.</p>\n<p><strong>How to fix</strong>: Use established learning rate ranges for each optimizer (SGD: 0.1-0.01, Adam: 0.001-0.0001). When changing batch sizes, scale learning rate proportionally for SGD but consider keeping it constant for Adam.</p>\n<p>⚠️ <strong>Pitfall: Gradient Accumulation Errors</strong>\nWhen implementing gradient accumulation across multiple batches, developers often forget to scale gradients by the accumulation factor, leading to effectively larger learning rates than intended.</p>\n<p><strong>Why it&#39;s wrong</strong>: Accumulating gradients from multiple batches without scaling creates gradient magnitudes equivalent to using a much larger batch size, which changes the effective learning rate and can destabilize training.</p>\n<p><strong>How to fix</strong>: Scale accumulated gradients by the number of accumulation steps before applying the optimizer step, or scale the learning rate by the same factor.</p>\n<p>⚠️ <strong>Pitfall: Data Shuffling Inconsistencies</strong>\nForgetting to shuffle training data between epochs, or shuffling validation data, can lead to misleading training dynamics and evaluation metrics.</p>\n<p><strong>Why it&#39;s wrong</strong>: Without shuffling, the model sees samples in the same order each epoch, which can lead to overfitting to the sample sequence rather than learning generalizable patterns. Shuffling validation data makes it harder to track consistent progress.</p>\n<p><strong>How to fix</strong>: Always shuffle training data at the start of each epoch, but never shuffle validation or test data. Use fixed random seeds for validation splits to ensure reproducible evaluation.</p>\n<p>⚠️ <strong>Pitfall: Loss Scaling Issues</strong>\nLoss functions that aren&#39;t properly scaled for the batch size or output dimensions can lead to gradient magnitudes that are too large or too small for the optimizer&#39;s learning rate.</p>\n<p><strong>Why it&#39;s wrong</strong>: Very large losses (e.g., summing instead of averaging across batch dimensions) create large gradients that require smaller learning rates. Very small losses create gradients that vanish and prevent learning.</p>\n<p><strong>How to fix</strong>: Use mean reduction for loss functions to maintain consistent gradient scales across different batch sizes. Monitor gradient norms during training to ensure they&#39;re in reasonable ranges (typically 1e-3 to 1e1).</p>\n<p>⚠️ <strong>Pitfall: Optimizer State Corruption</strong>\nModifying parameters outside of the optimizer step (e.g., manual weight clipping or re-initialization) can corrupt optimizer state like momentum buffers or Adam moments, leading to unexpected optimization behavior.</p>\n<p><strong>Why it&#39;s wrong</strong>: Optimizers assume they have exclusive control over parameter updates. External modifications break assumptions about gradient history and can cause momentum or adaptive learning rates to become misaligned with actual parameter values.</p>\n<p><strong>How to fix</strong>: Perform parameter modifications through the optimizer when possible, or explicitly reset optimizer state after manual parameter changes using <code>optimizer.state_dict()</code> manipulation.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides the concrete code infrastructure needed to build optimization components. The optimizer implementations focus on educational clarity while maintaining the essential features needed for training neural networks effectively.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Optimizer Base</td>\n<td>Single class with virtual methods</td>\n<td>Abstract base with plugin architecture</td>\n</tr>\n<tr>\n<td>State Storage</td>\n<td>Python dictionaries with tensor keys</td>\n<td>Memory-mapped state for large models</td>\n</tr>\n<tr>\n<td>Learning Rate Scheduling</td>\n<td>Function-based schedules</td>\n<td>Class-based schedulers with state</td>\n</tr>\n<tr>\n<td>Loss Functions</td>\n<td>Separate forward/backward methods</td>\n<td>Autograd-integrated implementations</td>\n</tr>\n<tr>\n<td>Training Loop</td>\n<td>Simple function with explicit steps</td>\n<td>Iterator-based training framework</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n  optimizers/\n    __init__.py              ← optimizer exports\n    base.py                  ← abstract optimizer base class\n    sgd.py                   ← SGD with momentum implementation\n    adam.py                  ← Adam optimizer implementation\n    lr_scheduler.py          ← learning rate scheduling utilities\n  losses/\n    __init__.py              ← loss function exports\n    mse.py                   ← mean squared error loss\n    cross_entropy.py         ← cross-entropy loss with softmax\n  training/\n    __init__.py              ← training utilities exports\n    trainer.py               ← training loop implementation\n    metrics.py               ← training metric computation\n    checkpoints.py           ← model and optimizer state saving</code></pre></div>\n\n<p><strong>Infrastructure Starter Code - Base Optimizer:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional, Iterator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Optimizer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all optimizers providing common functionality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, parameters: Iterator[Tensor]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize optimizer with parameters to optimize.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            parameters: Iterator of tensors with requires_grad=True</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.param_groups </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert parameters to list and validate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        param_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(parameters)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(param_list) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Optimizer got empty parameter list\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validate all parameters require gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> param_list:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> param.requires_grad:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"All parameters must have requires_grad=True\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store as single parameter group (can extend later for multiple groups)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.param_groups.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'params'</span><span style=\"color:#E1E4E8\">: param_list,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'lr'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># default learning rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # State dict for optimizer-specific state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> step</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Perform single optimization step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> zero_grad</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clear gradients for all parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> group </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.param_groups:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> group[</span><span style=\"color:#9ECBFF\">'params'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    param.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> state_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return optimizer state for checkpointing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'state'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.state,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'param_groups'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.param_groups</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_state_dict</span><span style=\"color:#E1E4E8\">(self, state_dict: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load optimizer state from checkpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> state_dict[</span><span style=\"color:#9ECBFF\">'state'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.param_groups </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> state_dict[</span><span style=\"color:#9ECBFF\">'param_groups'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LRScheduler</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for learning rate schedulers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, optimizer: Optimizer):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_epoch </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_lr</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute learning rate for current epoch.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> step</span><span style=\"color:#E1E4E8\">(self, epoch: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update learning rates for all parameter groups.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> epoch </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            epoch </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_epoch </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_epoch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> epoch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        learning_rates </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_lr()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param_group, lr </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> zip</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.optimizer.param_groups, learning_rates):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            param_group[</span><span style=\"color:#9ECBFF\">'lr'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> lr</span></span></code></pre></div>\n\n<p><strong>Infrastructure Starter Code - Training Utilities:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Iterator, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.module </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Module</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.losses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Loss</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.optimizers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optimizer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataLoader</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple data loader with batching and shuffling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, dataset: Tuple[np.ndarray, np.ndarray], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, shuffle: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize data loader.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            dataset: Tuple of (samples, labels) as numpy arrays</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            batch_size: Number of samples per batch</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            shuffle: Whether to shuffle data each epoch</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.samples, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.labels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dataset</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.batch_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch_size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shuffle </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shuffle</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.samples) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.labels):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Samples and labels must have same length\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __iter__</span><span style=\"color:#E1E4E8\">(self) -> Iterator[Tuple[Tensor, Tensor]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Iterate over batches for one epoch.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        n_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.arange(n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.shuffle:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            np.random.shuffle(indices)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> start_idx </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, n_samples, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.batch_size):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_idx </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(start_idx </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.batch_size, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            batch_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> indices[start_idx:end_idx]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            batch_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.samples[batch_indices], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            batch_labels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.labels[batch_indices], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> batch_samples, batch_labels</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> save_checkpoint</span><span style=\"color:#E1E4E8\">(path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, epoch: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, model: Module, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   optimizer: Optimizer, loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Save training checkpoint to disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'epoch'</span><span style=\"color:#E1E4E8\">: epoch,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'model_state_dict'</span><span style=\"color:#E1E4E8\">: model.state_dict(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'optimizer_state_dict'</span><span style=\"color:#E1E4E8\">: optimizer.state_dict(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'loss'</span><span style=\"color:#E1E4E8\">: loss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # In real implementation, use pickle or torch.save</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # For now, just print what would be saved</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Would save checkpoint to </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> with keys: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">checkpoint.keys()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_checkpoint</span><span style=\"color:#E1E4E8\">(path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load training checkpoint from disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # In real implementation, use pickle or torch.load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # For now, return empty dict</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Would load checkpoint from </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {}</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton - SGD Optimizer:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.optimizers.base </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optimizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Iterator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SGD</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Optimizer</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Stochastic Gradient Descent optimizer with momentum support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, parameters: Iterator[Tensor], lr: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 momentum: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">, weight_decay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize SGD optimizer.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            parameters: Model parameters to optimize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            lr: Learning rate for parameter updates</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            momentum: Momentum coefficient for velocity accumulation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            weight_decay: L2 regularization coefficient</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(parameters)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store hyperparameters in param_groups</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update self.param_groups[0] with lr, momentum, weight_decay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize momentum buffers if momentum > 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # For each parameter, create velocity buffer in self.state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use parameter id as key: self.state[id(param)] = {'momentum_buffer': zeros_like_param}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> step</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply SGD update step to all parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        with_grad_parameters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect parameters that have gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Iterate through param_groups and collect params where grad is not None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Skip parameters without gradients (they don't need updates)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(with_grad_parameters) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#6A737D\">  # No gradients to process</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> with_grad_parameters:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Extract hyperparameters for this parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Get lr, momentum, weight_decay from param_groups</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply weight decay if specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Add weight_decay * param.data to gradient</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # This implements L2 regularization: grad = grad + weight_decay * param</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply momentum update if momentum > 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # If momentum == 0: param.data = param.data - lr * grad</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # If momentum > 0: </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   velocity = momentum * old_velocity + grad</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   param.data = param.data - lr * velocity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   Store updated velocity in self.state[id(param)]['momentum_buffer']</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Apply parameter update</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Subtract lr * effective_grad from param.data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # effective_grad is either raw gradient or momentum-modified gradient</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton - Adam Optimizer:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.optimizers.base </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optimizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Iterator, Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Adam</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Optimizer</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Adam optimizer with adaptive learning rates and bias correction.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, parameters: Iterator[Tensor], lr: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.001</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 betas: Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0.9</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.999</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 eps: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-8</span><span style=\"color:#E1E4E8\">, weight_decay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize Adam optimizer.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            parameters: Model parameters to optimize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            lr: Learning rate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            betas: Coefficients for moment estimates (beta1, beta2)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            eps: Small constant for numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            weight_decay: L2 regularization coefficient</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(parameters)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store hyperparameters in param_groups</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add lr, betas, eps, weight_decay to param_groups[0]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize state for each parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # For each param, create state dict with:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - 'step': 0 (for bias correction)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - 'exp_avg': zeros like param (first moment)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - 'exp_avg_sq': zeros like param (second moment)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> step</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply Adam optimization step with bias correction.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> group </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.param_groups:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract hyperparameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Get lr, beta1, beta2, eps, weight_decay from group</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> group[</span><span style=\"color:#9ECBFF\">'params'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Get parameter state and increment step count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Get state dict for this parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Increment step count: state['step'] += 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply weight decay to gradient if specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # grad = grad + weight_decay * param.data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update first moment estimate (exponential moving average of gradients)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # exp_avg = beta1 * exp_avg + (1 - beta1) * grad</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Store updated exp_avg back to state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update second moment estimate (exponential moving average of squared gradients)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * grad²</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Store updated exp_avg_sq back to state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Apply bias correction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # bias_correction1 = 1 - beta1 ** step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # bias_correction2 = 1 - beta2 ** step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # corrected_exp_avg = exp_avg / bias_correction1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # corrected_exp_avg_sq = exp_avg_sq / bias_correction2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Compute parameter update</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # denominator = sqrt(corrected_exp_avg_sq) + eps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # step_size = lr / denominator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # param.data = param.data - step_size * corrected_exp_avg</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton - Training Loop:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.module </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Module</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.optimizers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optimizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> neural_framework.losses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Loss</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> train_epoch</span><span style=\"color:#E1E4E8\">(model: Module, data_loader, optimizer: Optimizer, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                loss_fn: Loss) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Train model for one epoch.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Average loss for the epoch</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to training mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Call model.train() to enable training-specific behavior</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_batches </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> batch_samples, batch_labels </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data_loader:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Clear gradients from previous step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Call optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Forward pass - compute predictions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # predictions = model.forward(batch_samples)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure predictions is a Tensor with requires_grad=True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # loss = loss_fn(predictions, batch_labels)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Accumulate loss value: total_loss += loss.data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Backward pass - compute gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Call loss.backward() to compute gradients for all parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Call optimizer.step() to apply parameter updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        num_batches </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return average loss for epoch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # return total_loss / num_batches if num_batches > 0 else 0.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(model: Module, data_loader, loss_fn: Loss) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Evaluate model on validation set.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Set model to evaluation mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Call model.eval() to disable training-specific behavior (dropout, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_batches </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Disable gradient computation for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # In real implementation, use context manager to disable gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> batch_samples, batch_labels </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data_loader:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Forward pass only (no backward pass needed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # predictions = model.forward(batch_samples)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # loss = loss_fn(predictions, batch_labels)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # total_loss += loss.data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        num_batches </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Return average validation loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # return total_loss / num_batches if num_batches > 0 else 0.0</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong>\nAfter implementing the optimizer and training components, verify the complete framework integration:</p>\n<ol>\n<li><p><strong>Basic Training Test</strong>: Create a simple linear model, generate synthetic data, and train for a few epochs. Loss should decrease consistently.</p>\n</li>\n<li><p><strong>Optimizer State Test</strong>: Save and load optimizer state, verify that momentum/Adam buffers are preserved correctly.</p>\n</li>\n<li><p><strong>Learning Rate Scheduling</strong>: Implement step decay scheduler, verify learning rate changes at specified epochs.</p>\n</li>\n<li><p><strong>Gradient Checking</strong>: Compare optimizer updates with numerical gradients on a small model to verify correctness.</p>\n</li>\n<li><p><strong>End-to-End Training</strong>: Train a multi-layer network on a toy dataset (e.g., XOR problem), achieve convergence to demonstrate complete framework functionality.</p>\n</li>\n</ol>\n<p>Expected behavior: Training loss should decrease smoothly, validation loss should track reasonably with training loss, and the trained model should make sensible predictions on test data. Monitor gradient norms to ensure they remain in reasonable ranges (1e-4 to 1e0 typically).</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Ftraining-sequence.svg\" alt=\"Training Sequence Diagram\"></p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Foptimizer-states.svg\" alt=\"Optimizer State Transitions\"></p>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - describes how components communicate during training, from tensor operations through gradient computation to parameter updates</p>\n</blockquote>\n<p>Understanding how data flows through a neural network framework during training is like watching a well-choreographed dance where multiple performers coordinate their movements in perfect harmony. Each component—tensors, modules, optimizers, and the automatic differentiation engine—has its role to play, and the beauty of the system emerges from their seamless coordination. This section dissects the intricate choreography that transforms input data into learned parameters through the interplay of forward passes, gradient computation, and optimization steps.</p>\n<p>The training process orchestrates three distinct but interconnected data flows. During the <strong>forward pass</strong>, input data cascades through the neural network modules while the automatic differentiation engine silently constructs a computation graph that records every operation. The <strong>backward pass</strong> reverses this flow, sending gradients from the loss function back through the network using the recorded computation graph to compute exact derivatives. Finally, the <strong>optimization step</strong> uses these gradients to update model parameters, completing one iteration of the learning cycle.</p>\n<h3 id=\"forward-pass-data-flow\">Forward Pass Data Flow</h3>\n<p>Think of the forward pass as a factory assembly line where raw materials (input data) move through a series of specialized stations (neural network modules), with each station transforming the materials and passing them to the next station. However, unlike a typical assembly line, this one has a quality inspector (the automatic differentiation engine) who meticulously records every transformation in a logbook (computation graph) so that if quality issues are discovered later, the inspector can trace back through the entire process to identify exactly which stations need adjustment.</p>\n<p>The forward pass begins when input data enters the neural network as a <code>Tensor</code> object. This tensor carries not only the numerical data in its <code>data</code> field but also metadata that governs how it participates in gradient computation. The <code>requires_grad</code> flag indicates whether this tensor needs gradient tracking, while the <code>grad_fn</code> field will eventually point to the operation that created this tensor during computation.</p>\n<table>\n<thead>\n<tr>\n<th>Tensor Attribute</th>\n<th>Role in Forward Pass</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>data</code></td>\n<td>Contains the actual numerical values flowing through the network</td>\n<td><code>np.array([[1.0, 2.0], [3.0, 4.0]])</code></td>\n</tr>\n<tr>\n<td><code>requires_grad</code></td>\n<td>Determines if this tensor participates in gradient computation</td>\n<td><code>True</code> for parameters, <code>False</code> for constants</td>\n</tr>\n<tr>\n<td><code>grad_fn</code></td>\n<td>Points to the operation that created this tensor</td>\n<td><code>AddOperation</code> instance after addition</td>\n</tr>\n<tr>\n<td><code>shape</code></td>\n<td>Defines tensor dimensions for broadcasting and validation</td>\n<td><code>(2, 2)</code> for 2x2 matrix</td>\n</tr>\n<tr>\n<td><code>grad</code></td>\n<td>Accumulates gradients during backward pass (initially None)</td>\n<td><code>None</code> during forward pass</td>\n</tr>\n</tbody></table>\n<p>As the input tensor flows through each module in the network, a sophisticated graph construction process occurs behind the scenes. Every operation that the tensor undergoes—whether element-wise addition in an activation function or matrix multiplication in a linear layer—creates an <code>Operation</code> node that records its inputs and maintains a reference to the function needed to compute gradients during backpropagation.</p>\n<p>The module system orchestrates this data flow through a consistent interface. When a tensor enters a module&#39;s <code>forward</code> method, the module applies its transformation and returns a new tensor. Crucially, this new tensor has its <code>grad_fn</code> set to point to the operation that created it, establishing a link in the computation graph that the automatic differentiation engine will later traverse.</p>\n<p>Consider the data flow through a simple two-layer network with ReLU activation. The input tensor first enters the first <code>Linear</code> module, which performs matrix multiplication <code>y = Wx + b</code>. This operation creates a <code>MatMul</code> node in the computation graph with the input tensor and weight tensor as inputs. The resulting tensor then flows to the <code>ReLU</code> module, which applies the activation function element-wise, creating another operation node. This pattern continues through each module, building an increasingly complex computation graph.</p>\n<table>\n<thead>\n<tr>\n<th>Forward Pass Stage</th>\n<th>Input</th>\n<th>Operation Created</th>\n<th>Output</th>\n<th>Graph Node Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>First Linear Layer</td>\n<td>Input tensor <code>x</code></td>\n<td><code>MatMul(x, weight1) + bias1</code></td>\n<td>Hidden tensor <code>h1</code></td>\n<td><code>h1.grad_fn</code> → <code>AddOperation</code></td>\n</tr>\n<tr>\n<td>ReLU Activation</td>\n<td>Hidden tensor <code>h1</code></td>\n<td><code>ReLU(h1)</code></td>\n<td>Activated tensor <code>a1</code></td>\n<td><code>a1.grad_fn</code> → <code>ReLUOperation</code></td>\n</tr>\n<tr>\n<td>Second Linear Layer</td>\n<td>Activated tensor <code>a1</code></td>\n<td><code>MatMul(a1, weight2) + bias2</code></td>\n<td>Output tensor <code>y</code></td>\n<td><code>y.grad_fn</code> → <code>AddOperation</code></td>\n</tr>\n<tr>\n<td>Loss Computation</td>\n<td>Output <code>y</code>, Target <code>t</code></td>\n<td><code>CrossEntropy(y, t)</code></td>\n<td>Loss scalar <code>L</code></td>\n<td><code>L.grad_fn</code> → <code>CrossEntropyOperation</code></td>\n</tr>\n</tbody></table>\n<p>The computation graph construction follows a define-by-run approach where the graph structure emerges dynamically as operations execute. Each operation node maintains references to its input tensors, creating a directed acyclic graph that mirrors the forward computation. This graph serves as a blueprint for the backward pass, encoding not only what operations were performed but also the order in which gradients must be computed.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The forward pass serves double duty—it computes the network&#39;s predictions while simultaneously building the roadmap for gradient computation. This define-by-run approach provides flexibility for dynamic network architectures but requires careful memory management to prevent the computation graph from accumulating indefinitely.</p>\n</blockquote>\n<p><strong>Decision: Eager Graph Construction vs. Lazy Graph Construction</strong></p>\n<ul>\n<li><strong>Context</strong>: During forward pass, we need to decide when to build computation graph nodes</li>\n<li><strong>Options Considered</strong>: Build nodes immediately during operations (eager) vs. defer until backward pass needed (lazy) vs. static graph definition before execution</li>\n<li><strong>Decision</strong>: Eager graph construction during forward pass operations</li>\n<li><strong>Rationale</strong>: Eager construction provides immediate feedback for debugging, allows dynamic control flow, and aligns with define-by-run semantics. The overhead is manageable for educational frameworks.</li>\n<li><strong>Consequences</strong>: Enables dynamic networks and easy debugging but requires more memory and careful graph cleanup to prevent leaks.</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Graph Construction Approach</th>\n<th>Memory Usage</th>\n<th>Debugging Ease</th>\n<th>Dynamic Networks</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Eager (chosen)</td>\n<td>Higher during forward pass</td>\n<td>Excellent - immediate graph inspection</td>\n<td>Full support</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>Lazy</td>\n<td>Lower until backward needed</td>\n<td>Difficult - no graph until backprop</td>\n<td>Limited support</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Static</td>\n<td>Lowest - fixed graph</td>\n<td>Good - graph known upfront</td>\n<td>No support</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>The forward pass also handles broadcasting automatically when tensors of different shapes interact. The <code>broadcast_shapes</code> function computes the result shape according to NumPy broadcasting rules, and the operation nodes record both the original input shapes and the broadcasted shapes. This information becomes crucial during the backward pass when gradients must be reduced back to their original parameter shapes.</p>\n<p>Parameter tensors flow through the network differently than input data. When a <code>Linear</code> module&#39;s weight tensor participates in matrix multiplication, it becomes part of the computation graph just like any other tensor. However, parameters have <code>requires_grad=True</code> by default, ensuring they receive gradients during backpropagation. The module system&#39;s parameter registration mechanism ensures that optimizers can discover and update these parameter tensors after gradient computation.</p>\n<p><strong>Common Pitfalls in Forward Pass Data Flow</strong></p>\n<p>⚠️ <strong>Pitfall: In-place Operations Breaking Gradient Flow</strong>\nMany learners accidentally use in-place operations (like <code>+=</code> or <code>*=</code>) on tensors that require gradients. In-place operations modify the tensor&#39;s data directly without creating new operation nodes, which breaks the computation graph and prevents proper gradient computation. For example, writing <code>hidden_state += bias</code> instead of <code>hidden_state = hidden_state + bias</code> destroys the gradient flow because no <code>Add</code> operation node is created to record the computation.</p>\n<p>⚠️ <strong>Pitfall: Forgetting to Set requires_grad on Parameters</strong>\nParameter tensors must have <code>requires_grad=True</code> to participate in gradient computation. If a learner manually creates parameter tensors and forgets this flag, the parameters will be treated as constants during forward pass, no gradients will be computed for them, and the optimizer will have no gradients to use for updates. Always verify that <code>param.requires_grad</code> is True for all trainable parameters.</p>\n<p>⚠️ <strong>Pitfall: Graph Memory Leaks from Retained References</strong>\nThe computation graph holds references to all tensors involved in operations. If learners store references to intermediate tensors or operation nodes beyond their needed lifetime, the entire computation graph remains in memory. This is particularly problematic in training loops where each iteration creates a new graph. Always clear computation graphs after each backward pass by calling <code>loss.backward()</code> followed by <code>optimizer.zero_grad()</code>.</p>\n<h3 id=\"backward-pass-coordination\">Backward Pass Coordination</h3>\n<p>Imagine the backward pass as a detective investigating a crime scene in reverse chronological order. Starting from the crime (the loss function), the detective traces back through each piece of evidence (tensor operations) to determine exactly how each suspect (parameter) contributed to the final outcome. The detective follows a strict protocol: always examine evidence in reverse order of occurrence, and for each piece of evidence, calculate precisely how much each suspect influenced the situation using forensic analysis (chain rule).</p>\n<p>The backward pass transforms the computation graph built during forward pass into a gradient computation engine. This transformation requires careful coordination between multiple components: the topological sorting algorithm that determines the order of gradient computation, the chain rule application that computes local gradients at each operation node, and the gradient accumulation mechanism that handles cases where tensors are used multiple times in the computation.</p>\n<p>Backward pass initiation occurs when <code>loss.backward()</code> is called on the final tensor in the computation graph, typically the loss value. This call triggers a cascading process that computes gradients for every tensor in the graph that has <code>requires_grad=True</code>. The process begins by setting the loss tensor&#39;s gradient to a tensor of ones (representing the derivative of the loss with respect to itself), then traverses the computation graph in reverse topological order.</p>\n<table>\n<thead>\n<tr>\n<th>Backward Pass Component</th>\n<th>Responsibility</th>\n<th>Key Algorithm</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Topological Sort</td>\n<td>Determine gradient computation order</td>\n<td>Depth-first traversal from loss tensor</td>\n<td>Detect cycles, handle disconnected nodes</td>\n</tr>\n<tr>\n<td>Chain Rule Application</td>\n<td>Compute local gradients at each node</td>\n<td>Apply operation-specific backward functions</td>\n<td>Handle numerical instability, zero gradients</td>\n</tr>\n<tr>\n<td>Gradient Accumulation</td>\n<td>Sum gradients for multiply-used tensors</td>\n<td>Add incoming gradients to existing values</td>\n<td>Initialize gradients, handle None values</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Clean up computation graph after use</td>\n<td>Clear references between nodes</td>\n<td>Prevent memory leaks, handle circular refs</td>\n</tr>\n</tbody></table>\n<p>The topological sort algorithm ensures that gradients are computed in the correct dependency order. Starting from the loss tensor, the algorithm performs a depth-first traversal of the computation graph, visiting each operation node and its input tensors. The sort produces an ordering where each tensor&#39;s gradient is computed only after all tensors that depend on it have had their gradients computed.</p>\n<p><strong>Decision: Recursive vs. Iterative Topological Sort</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to traverse computation graph in correct order for gradient computation</li>\n<li><strong>Options Considered</strong>: Recursive depth-first search vs. iterative with explicit stack vs. breadth-first queue-based approach</li>\n<li><strong>Decision</strong>: Iterative depth-first search with explicit stack</li>\n<li><strong>Rationale</strong>: Recursive approaches can hit Python&#39;s recursion limit on deep networks. Iterative provides the same ordering guarantees while handling arbitrarily deep graphs. Stack-based maintains DFS semantics needed for proper dependency resolution.</li>\n<li><strong>Consequences</strong>: More complex implementation but handles deep networks reliably and provides better debugging capabilities through stack inspection.</li>\n</ul>\n<p>The chain rule application at each operation node follows a precise protocol. When an operation node receives a gradient from its output (representing how the loss changes with respect to the node&#39;s output), it must compute how the loss changes with respect to each of its inputs. This computation uses the operation&#39;s backward function, which implements the mathematical derivative of the forward operation.</p>\n<p>Consider gradient flow through a matrix multiplication operation <code>C = matmul(A, B)</code>. When this operation receives gradient <code>grad_C</code> from downstream computations, it must compute gradients for both input matrices A and B. The backward function applies the chain rule: <code>grad_A = matmul(grad_C, B.transpose())</code> and <code>grad_B = matmul(A.transpose(), grad_C)</code>. These computed gradients then propagate to the operations that produced tensors A and B.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Forward Computation</th>\n<th>Gradient Computation (Chain Rule)</th>\n<th>Special Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Add(a, b)</code></td>\n<td><code>result = a + b</code></td>\n<td><code>grad_a = grad_result</code>, <code>grad_b = grad_result</code></td>\n<td>Handle broadcasting shapes</td>\n</tr>\n<tr>\n<td><code>Multiply(a, b)</code></td>\n<td><code>result = a * b</code></td>\n<td><code>grad_a = grad_result * b</code>, <code>grad_b = grad_result * a</code></td>\n<td>Element-wise multiplication</td>\n</tr>\n<tr>\n<td><code>MatMul(a, b)</code></td>\n<td><code>result = a @ b</code></td>\n<td><code>grad_a = grad_result @ b.T</code>, <code>grad_b = a.T @ grad_result</code></td>\n<td>Matrix dimension compatibility</td>\n</tr>\n<tr>\n<td><code>ReLU(x)</code></td>\n<td><code>result = max(0, x)</code></td>\n<td><code>grad_x = grad_result * (x &gt; 0)</code></td>\n<td>Zero gradient for negative inputs</td>\n</tr>\n</tbody></table>\n<p>Gradient accumulation becomes critical when the same tensor appears multiple times in the computation graph. This situation arises commonly in neural networks—for example, when the same weight tensor is used in multiple operations or when control flow causes a tensor to be processed along multiple paths. The backward pass must sum all gradient contributions for such tensors rather than overwriting previous gradients.</p>\n<p>The accumulation process follows a careful protocol. When a tensor receives its first gradient during backward pass, the framework initializes the tensor&#39;s <code>grad</code> field with this gradient. For subsequent gradients from other operations, the framework adds the new gradient to the existing accumulated value. This summation implements the mathematical requirement that the total derivative equals the sum of partial derivatives from all usage paths.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Gradient accumulation is not just an optimization detail—it&#39;s a mathematical requirement. When a tensor contributes to the loss through multiple computational paths, the chain rule demands that we sum the gradients from all paths to get the correct total derivative.</p>\n</blockquote>\n<p>The backward pass also handles broadcasting-related gradient adjustments. When forward pass operations used broadcasting to make tensors compatible for element-wise operations, the backward pass must &quot;unbroadcast&quot; the gradients to match the original tensor shapes. This process uses the <code>unbroadcast_gradient</code> function to sum gradients across the dimensions that were expanded during broadcasting.</p>\n<table>\n<thead>\n<tr>\n<th>Broadcasting Scenario</th>\n<th>Forward Shape Change</th>\n<th>Backward Gradient Adjustment</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scalar + Vector</td>\n<td><code>(1,) + (5,)</code> → <code>(5,)</code></td>\n<td>Sum gradient across expanded dimension</td>\n<td><code>grad.sum(axis=0, keepdims=True)</code></td>\n</tr>\n<tr>\n<td>Vector + Matrix</td>\n<td><code>(3,) + (4,3)</code> → <code>(4,3)</code></td>\n<td>Sum gradient across batch dimension</td>\n<td><code>grad.sum(axis=0)</code></td>\n</tr>\n<tr>\n<td>Bias Addition</td>\n<td><code>(1,) + (64,10)</code> → <code>(64,10)</code></td>\n<td>Sum across both batch and feature dims</td>\n<td><code>grad.sum(axis=(0,1))</code></td>\n</tr>\n</tbody></table>\n<p>Memory management during backward pass requires careful attention to prevent leaks and circular references. As gradients flow through the computation graph, operation nodes can be deallocated once their gradients have been computed and propagated. The framework implements this cleanup by clearing references between nodes as the backward pass progresses, allowing Python&#39;s garbage collector to reclaim memory.</p>\n<p><strong>Common Pitfalls in Backward Pass Coordination</strong></p>\n<p>⚠️ <strong>Pitfall: Incorrect Topological Sort Leading to Wrong Gradient Order</strong>\nThe most critical error in backward pass implementation is computing gradients in the wrong order. This happens when the topological sort algorithm has bugs or when the computation graph contains cycles (which should never happen in valid neural networks). Wrong ordering leads to gradients being computed using stale or incorrect values from downstream operations. To debug this, print the order of operations during backward pass and verify that each operation receives gradients only after all operations that depend on it have completed.</p>\n<p>⚠️ <strong>Pitfall: Gradients Not Accumulated for Shared Tensors</strong>\nWhen the same parameter tensor is used multiple times in a network (such as shared embeddings or recurrent weight sharing), learners often overwrite gradients instead of accumulating them. This leads to incorrect gradient values and poor training performance. The fix is to always check if <code>tensor.grad</code> is None before assignment: if None, assign the new gradient directly; if not None, add the new gradient to the existing value.</p>\n<p>⚠️ <strong>Pitfall: Broadcasting Gradients Not Properly Unbroadcast</strong>\nDuring forward pass, operations may broadcast tensors to compatible shapes, but during backward pass, gradients must be reduced back to the original tensor shapes. Learners often forget this step, leading to shape mismatches when trying to assign gradients to parameters. Always call <code>unbroadcast_gradient(grad, original_shape)</code> to ensure gradient shapes match parameter shapes before assignment.</p>\n<h3 id=\"complete-training-step-sequence\">Complete Training Step Sequence</h3>\n<p>The complete training step orchestrates the forward pass, backward pass, and optimization update into a coordinated sequence that transforms input data into learned parameters. Think of this sequence as a complete learning cycle, similar to how a human learns from a mistake: first, make a prediction (forward pass), then evaluate how wrong it was (loss computation), next understand exactly what led to the error (backward pass), and finally adjust behavior to avoid similar mistakes in the future (parameter update).</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Ftraining-sequence.svg\" alt=\"Complete Training Step Sequence\"></p>\n<p>The training step sequence involves precise coordination between multiple components, each with specific responsibilities and timing requirements. The sequence must ensure data consistency, proper gradient computation, and correct parameter updates while managing memory efficiently and handling potential failures gracefully.</p>\n<table>\n<thead>\n<tr>\n<th>Training Step Phase</th>\n<th>Primary Component</th>\n<th>Key Operations</th>\n<th>Success Criteria</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading</td>\n<td><code>DataLoader</code></td>\n<td>Load batch, apply transforms</td>\n<td>Batch shape matches model input</td>\n<td>Handle malformed data, missing files</td>\n</tr>\n<tr>\n<td>Forward Pass</td>\n<td><code>Module</code> hierarchy</td>\n<td>Tensor operations, graph building</td>\n<td>Output shape correct, no NaN values</td>\n<td>Gradient clipping, numerical checks</td>\n</tr>\n<tr>\n<td>Loss Computation</td>\n<td>Loss function</td>\n<td>Compare predictions to targets</td>\n<td>Single scalar loss value</td>\n<td>Handle class imbalance, numerical stability</td>\n</tr>\n<tr>\n<td>Backward Pass</td>\n<td>Autodiff engine</td>\n<td>Gradient computation, accumulation</td>\n<td>All parameters have gradients</td>\n<td>Check gradient magnitudes, detect vanishing</td>\n</tr>\n<tr>\n<td>Parameter Update</td>\n<td><code>Optimizer</code></td>\n<td>Apply gradients to parameters</td>\n<td>Parameters change by expected amount</td>\n<td>Gradient clipping, learning rate adjustment</td>\n</tr>\n<tr>\n<td>Cleanup</td>\n<td>Framework</td>\n<td>Clear graphs, reset gradients</td>\n<td>Memory usage returns to baseline</td>\n<td>Force garbage collection if needed</td>\n</tr>\n</tbody></table>\n<p>The sequence begins with data preparation where the <code>DataLoader</code> provides a mini-batch of training examples. The data loader handles shuffling, batching, and any necessary preprocessing to ensure the input data is in the correct format for the model. This phase also converts raw data into <code>Tensor</code> objects with appropriate device placement and dtype settings.</p>\n<p><strong>Step-by-step Training Sequence:</strong></p>\n<ol>\n<li><p><strong>Batch Preparation</strong>: The <code>DataLoader</code> selects the next mini-batch from the shuffled training dataset, applies any configured data transformations, and converts the data into framework tensors with <code>requires_grad=False</code> (since input data doesn&#39;t need gradients).</p>\n</li>\n<li><p><strong>Model Mode Setting</strong>: The training loop calls <code>model.train()</code> to ensure all modules are in training mode, which affects the behavior of layers like dropout and batch normalization that behave differently during training versus inference.</p>\n</li>\n<li><p><strong>Gradient Zeroing</strong>: The optimizer&#39;s <code>zero_grad()</code> method clears any residual gradients from the previous training step by setting all parameter gradients to None, preventing accumulation across training steps.</p>\n</li>\n<li><p><strong>Forward Pass Execution</strong>: Input tensors flow through the model via <code>predictions = model(inputs)</code>, triggering the forward pass data flow described earlier while building the computation graph for automatic differentiation.</p>\n</li>\n<li><p><strong>Loss Computation</strong>: The loss function compares predictions to ground truth targets via <code>loss = loss_function(predictions, targets)</code>, producing a scalar tensor that represents the model&#39;s performance on this batch.</p>\n</li>\n<li><p><strong>Backward Pass Initiation</strong>: Calling <code>loss.backward()</code> initiates reverse-mode automatic differentiation, computing gradients for all parameters in the model by traversing the computation graph in reverse topological order.</p>\n</li>\n<li><p><strong>Parameter Updates</strong>: The optimizer&#39;s <code>step()</code> method applies the computed gradients to update model parameters according to the optimization algorithm (SGD, Adam, etc.), implementing one step of gradient-based learning.</p>\n</li>\n<li><p><strong>Graph Cleanup</strong>: The computation graph from this training step is deallocated, freeing memory for the next iteration. This happens automatically when the loss tensor goes out of scope, but can be forced by clearing tensor references.</p>\n</li>\n</ol>\n<p>The coordination between components during this sequence requires careful attention to state management and error propagation. Each phase depends on successful completion of previous phases, and failures must be handled gracefully to maintain training stability.</p>\n<p><strong>Decision: Synchronous vs. Asynchronous Training Step Execution</strong></p>\n<ul>\n<li><strong>Context</strong>: Training steps can potentially be pipelined or parallelized across multiple devices</li>\n<li><strong>Options Considered</strong>: Synchronous single-device execution vs. asynchronous multi-device vs. hybrid approach</li>\n<li><strong>Decision</strong>: Synchronous single-device execution for educational framework</li>\n<li><strong>Rationale</strong>: Synchronous execution is easier to debug, understand, and implement correctly. Educational frameworks prioritize clarity over performance optimization. Multi-device complexity would obscure the core learning objectives.</li>\n<li><strong>Consequences</strong>: Simpler implementation and debugging but limited scalability to large models or datasets. Future extensions could add asynchronous capabilities.</li>\n</ul>\n<p>Memory management throughout the training sequence follows a careful protocol to prevent memory leaks and ensure consistent memory usage across training steps. The computation graph built during forward pass must be completely deallocated after backward pass completion to prevent memory accumulation over many training iterations.</p>\n<p>Error detection and handling occurs at multiple points in the training sequence. Numerical issues like NaN or infinite values can arise during forward pass, backward pass, or parameter updates. The framework implements checks at critical points to detect these issues early and provide meaningful error messages to help learners debug their implementations.</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Detection Point</th>\n<th>Symptoms</th>\n<th>Debugging Approach</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape Mismatch</td>\n<td>Forward pass operations</td>\n<td>Runtime exception during tensor operations</td>\n<td>Print tensor shapes at each operation</td>\n<td>Fix model architecture or input preprocessing</td>\n</tr>\n<tr>\n<td>Vanishing Gradients</td>\n<td>After backward pass</td>\n<td>All gradients near zero</td>\n<td>Monitor gradient magnitudes</td>\n<td>Adjust initialization, learning rate, or architecture</td>\n</tr>\n<tr>\n<td>Exploding Gradients</td>\n<td>After backward pass</td>\n<td>Very large gradient values</td>\n<td>Check gradient norms</td>\n<td>Implement gradient clipping</td>\n</tr>\n<tr>\n<td>NaN Values</td>\n<td>Forward or backward pass</td>\n<td>NaN in loss, gradients, or parameters</td>\n<td>Print intermediate values</td>\n<td>Check for division by zero, log of negative values</td>\n</tr>\n<tr>\n<td>Memory Leaks</td>\n<td>Between training steps</td>\n<td>Increasing memory usage</td>\n<td>Monitor memory consumption over time</td>\n<td>Ensure proper graph cleanup and reference clearing</td>\n</tr>\n</tbody></table>\n<p>The training loop typically wraps the single training step in iteration logic that processes multiple batches per epoch and multiple epochs for complete training. This outer loop handles additional concerns like validation evaluation, checkpointing, and learning rate scheduling.</p>\n<p>Loss tracking and logging occur throughout the training sequence to provide visibility into training progress. The framework typically accumulates loss values across batches within an epoch and computes epoch-level statistics like average loss and accuracy metrics.</p>\n<p><strong>Common Pitfalls in Complete Training Step Sequence</strong></p>\n<p>⚠️ <strong>Pitfall: Forgetting to Call optimizer.zero_grad()</strong>\nOne of the most common mistakes is forgetting to clear gradients between training steps. PyTorch and similar frameworks accumulate gradients by default, so if <code>zero_grad()</code> is not called, gradients from previous steps will be added to gradients from the current step. This leads to incorrect gradient values that grow without bound and cause training instability. Always call <code>optimizer.zero_grad()</code> before the forward pass of each training step.</p>\n<p>⚠️ <strong>Pitfall: Calling backward() Multiple Times on Same Loss</strong>\nLearners sometimes accidentally call <code>loss.backward()</code> multiple times within the same training step, perhaps in different parts of the code or within conditional branches. This causes gradients to be computed and accumulated multiple times, leading to incorrect gradient magnitudes. Each loss tensor should have <code>backward()</code> called exactly once per training step.</p>\n<p>⚠️ <strong>Pitfall: Not Setting Model to Training Mode</strong>\nFailing to call <code>model.train()</code> before training and <code>model.eval()</code> before validation can cause subtle but serious issues. Layers like dropout and batch normalization behave differently in training versus evaluation mode. Without proper mode setting, the model may not learn effectively during training or may produce inconsistent results during evaluation.</p>\n<p>⚠️ <strong>Pitfall: Gradient Explosion from Incorrect Learning Rates</strong>\nUsing learning rates that are too large can cause gradients to explode, leading to NaN values in parameters after just a few training steps. This manifests as sudden spikes in loss values or NaN loss after a few iterations. Monitor gradient norms and implement gradient clipping as a safeguard, but also tune learning rates appropriately for the model and dataset size.</p>\n<p><img src=\"/api/project/build-nn-framework/architecture-doc/asset?path=diagrams%2Fgradient-flow.svg\" alt=\"Gradient Backpropagation Flow\"></p>\n<p>The complete training step sequence represents the fundamental learning cycle in neural network training. Understanding this sequence and the coordination between its components is crucial for building reliable training loops and debugging issues that arise during model development. The sequence transforms raw data into learned knowledge through the precise orchestration of forward computation, gradient calculation, and parameter optimization.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The coordination between components during training requires careful orchestration of state management, error handling, and resource cleanup. This implementation guidance provides concrete code structures and utilities to manage these complex interactions effectively.</p>\n<p><strong>A. Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Graph Traversal</td>\n<td>Python list-based topological sort</td>\n<td>NetworkX graph algorithms with cycle detection</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Manual reference clearing with weakref</td>\n<td>Automatic graph cleanup with context managers</td>\n</tr>\n<tr>\n<td>Error Detection</td>\n<td>Basic type checking and assertions</td>\n<td>Comprehensive numerical stability monitoring</td>\n</tr>\n<tr>\n<td>Progress Tracking</td>\n<td>Print statements with loss values</td>\n<td>Structured logging with tensorboard integration</td>\n</tr>\n<tr>\n<td>Debugging Tools</td>\n<td>Simple gradient printing utilities</td>\n<td>Full computation graph visualization</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n  core/\n    __init__.py\n    training.py              ← Training coordination logic\n    data_flow.py            ← Forward/backward pass orchestration\n  autograd/\n    __init__.py\n    graph.py                ← Computation graph utilities\n    backward.py             ← Backward pass implementation\n  optimizers/\n    __init__.py\n    base.py                 ← Base optimizer with step coordination\n    sgd.py                  ← SGD implementation\n    adam.py                 ← Adam implementation\n  utils/\n    __init__.py\n    debugging.py            ← Training debugging utilities\n    memory.py               ← Memory management helpers\n  examples/\n    simple_training_loop.py ← Complete training example</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># utils/debugging.py - Complete debugging utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> weakref</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> gc</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete monitoring system for training diagnostics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, check_frequency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_frequency</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_norms </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_step</span><span style=\"color:#E1E4E8\">(self, loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, gradients: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log training step metrics with numerical stability checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history.append(loss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for numerical issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> np.isnan(loss) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> np.isinf(loss):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid loss value at step </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.step_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">loss</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Monitor gradient magnitudes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> name, tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> gradients.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> tensor.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linalg.norm(tensor.grad.data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                grad_norm </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> norm </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> norm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sqrt(grad_norm)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_norms.append(grad_norm)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._print_diagnostics()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _print_diagnostics</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Print comprehensive training diagnostics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recent_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.mean(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.loss_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.check_frequency:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recent_grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.mean(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.gradient_norms[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.check_frequency:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Step </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.step_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: Loss=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">recent_loss</span><span style=\"color:#F97583\">:.6f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">              f</span><span style=\"color:#9ECBFF\">\"Grad Norm=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">recent_grad_norm</span><span style=\"color:#F97583\">:.6f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for gradient issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> recent_grad_norm </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"WARNING: Very small gradients detected (vanishing gradients)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> recent_grad_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"WARNING: Very large gradients detected (exploding gradients)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComputationGraphTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Track computation graph memory usage and provide cleanup utilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_graphs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> weakref.WeakSet()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_tensor</span><span style=\"color:#E1E4E8\">(self, tensor: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register tensor for graph tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tensor.requires_grad:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.active_graphs.add(tensor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_graphs</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Force cleanup of computation graphs and return memory freed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        initial_objects </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(gc.get_objects())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clear weak references</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_graphs.clear()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Force garbage collection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        collected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gc.collect()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        final_objects </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(gc.get_objects())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        objects_freed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_objects </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> final_objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> objects_freed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_memory_stats</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current memory usage statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'active_graphs'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.active_graphs),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'total_objects'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(gc.get_objects()),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'tensor_objects'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">([obj </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> obj </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> gc.get_objects() </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                                 if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(obj, </span><span style=\"color:#9ECBFF\">'data'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(obj, </span><span style=\"color:#9ECBFF\">'grad'</span><span style=\"color:#E1E4E8\">)])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># utils/memory.py - Memory management helpers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GraphCleanupContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for automatic computation graph cleanup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tensors: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tensors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __enter__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __exit__</span><span style=\"color:#E1E4E8\">(self, exc_type, exc_val, exc_tb):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clear computation graphs for all tracked tensors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tensors:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(tensor, </span><span style=\"color:#9ECBFF\">'grad_fn'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tensor.grad_fn </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(tensor, </span><span style=\"color:#9ECBFF\">'grad'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tensor.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Force garbage collection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gc.collect()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradient_flow</span><span style=\"color:#E1E4E8\">(model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">, loss: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check gradient flow through all model parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.named_parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linalg.norm(param.grad.data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gradient_info[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(grad_norm)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gradient_info[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"WARNING: No gradient for parameter </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradient_info</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># core/training.py - Training step coordination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates the complete training step sequence.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">, optimizer: </span><span style=\"color:#9ECBFF\">'Optimizer'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 loss_function: </span><span style=\"color:#9ECBFF\">'Loss'</span><span style=\"color:#E1E4E8\">, monitor: Optional[TrainingMonitor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_function </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loss_function</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> monitor </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> TrainingMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> training_step</span><span style=\"color:#E1E4E8\">(self, inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, targets: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute complete training step with full coordination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary with loss value and training metrics</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify model is in training mode - call model.train()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Clear gradients from previous step - call optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute forward pass - predictions = model(inputs)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute loss - loss = loss_function(predictions, targets)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for numerical issues - verify loss is finite</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Execute backward pass - loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Check gradient flow - ensure all parameters have gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Apply parameter updates - optimizer.step()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Log training metrics - monitor.log_step()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Clean up computation graph - clear references</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use try-except blocks to catch and handle numerical errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check tensor.grad is not None for all parameters after backward()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Monitor gradient norms to detect vanishing/exploding gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validation_step</span><span style=\"color:#E1E4E8\">(self, inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, targets: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute validation step without gradient computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to evaluation mode - model.eval()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Disable gradient computation - use no_grad context if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute forward pass - predictions = model(inputs)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute loss - loss = loss_function(predictions, targets)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compute additional metrics - accuracy, etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return metrics dictionary</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># core/data_flow.py - Forward/backward pass coordination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> coordinate_forward_pass</span><span style=\"color:#E1E4E8\">(model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">, inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinate forward pass with graph construction tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify input tensor shapes match model expectations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Track all intermediate tensors created during forward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute model forward pass - outputs = model(inputs)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify output tensor shapes and numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return outputs and list of intermediate tensors for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> coordinate_backward_pass</span><span style=\"color:#E1E4E8\">(loss: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, parameters: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinate backward pass with gradient validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify loss is scalar tensor with requires_grad=True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute backward pass - loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect all computed gradients from parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate gradient shapes match parameter shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for gradient numerical issues (NaN, inf, very small/large)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return dictionary mapping parameter names to gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use parameter.grad to access computed gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Check gradient.shape == parameter.shape for each parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use np.isfinite() to check for NaN/inf in gradient values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_gradient_computation</span><span style=\"color:#E1E4E8\">(model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">, inputs: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                targets: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, loss_fn: </span><span style=\"color:#9ECBFF\">'Loss'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate gradient computation using numerical differentiation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute analytical gradients using backward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute numerical gradients using finite differences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare analytical vs numerical gradients for each parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return True if all gradients match within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use numerical_gradient() function from autograd utilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Compare gradients element-wise using np.allclose()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>Memory Management</strong>: Use <code>weakref.WeakSet()</code> to track tensors without preventing garbage collection</li>\n<li><strong>Numerical Stability</strong>: Use <code>np.isfinite()</code> to check for NaN/inf values in tensors and gradients  </li>\n<li><strong>Context Management</strong>: Implement <code>__enter__</code> and <code>__exit__</code> methods for automatic resource cleanup</li>\n<li><strong>Error Propagation</strong>: Use specific exception types (<code>ValueError</code>, <code>RuntimeError</code>) with descriptive messages</li>\n<li><strong>Debugging Output</strong>: Use <code>f-string</code> formatting for readable debug messages with tensor shapes and values</li>\n<li><strong>Type Hints</strong>: Include <code>Optional[Tensor]</code> for gradients that may be None during forward pass</li>\n<li><strong>List Comprehensions</strong>: Use generator expressions with <code>gc.get_objects()</code> for memory debugging</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the interactions and data flow coordination:</p>\n<ol>\n<li><strong>Run Training Step Test</strong>: Execute <code>python -m pytest tests/test_training_step.py -v</code></li>\n<li><strong>Expected Output</strong>: All tests pass showing forward pass, backward pass, and parameter updates work correctly</li>\n<li><strong>Manual Verification</strong>: Run the simple training loop example with <code>python examples/simple_training_loop.py</code></li>\n<li><strong>Expected Behavior</strong>: Should see decreasing loss values over iterations without NaN/inf values</li>\n<li><strong>Debug Check</strong>: Monitor memory usage - it should remain stable across training iterations</li>\n<li><strong>Gradient Validation</strong>: Run gradient checking test to verify analytical vs numerical gradients match</li>\n</ol>\n<p><strong>Signs of Correct Implementation:</strong></p>\n<ul>\n<li>Loss decreases smoothly over training iterations  </li>\n<li>All parameters receive gradients after backward pass</li>\n<li>Memory usage remains stable across training steps</li>\n<li>No NaN or infinite values in loss, gradients, or parameters</li>\n<li>Gradient magnitudes are reasonable (not too small/large)</li>\n</ul>\n<p><strong>Signs of Problems:</strong></p>\n<ul>\n<li>Loss becomes NaN or infinite after a few steps → Check learning rate, gradient clipping</li>\n<li>Memory usage increases over time → Check computation graph cleanup</li>\n<li>Some parameters have no gradients → Check requires_grad flags and graph connectivity</li>\n<li>Loss doesn&#39;t decrease → Check optimizer implementation and learning rate</li>\n</ul>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - error handling is critical throughout tensor operations, automatic differentiation, neural modules, and training loops</p>\n</blockquote>\n<p>Building a neural network framework is like constructing a precision manufacturing pipeline where small errors can cascade into catastrophic failures. Just as a factory needs robust quality control systems to detect defective parts, validate assembly processes, and prevent equipment damage, our framework must anticipate and gracefully handle the numerous failure modes that arise when working with numerical computation, dynamic memory management, and complex interdependent systems.</p>\n<p>The challenge of error handling in neural network frameworks extends far beyond simple input validation. We must detect subtle numerical instabilities that can corrupt gradients, manage memory usage in dynamically-constructed computation graphs, validate tensor shape compatibility across broadcasting operations, and provide meaningful diagnostics when training diverges. The interconnected nature of our four-layer architecture means that failures can propagate across boundaries - a shape mismatch in tensor operations can manifest as gradient computation errors, while memory leaks in computation graphs can cause training crashes hours into a long experiment.</p>\n<h3 id=\"the-quality-control-analogy\">The Quality Control Analogy</h3>\n<p>Think of error handling in our neural framework like a multi-stage quality control system in an automotive assembly plant. At each station, inspectors check for specific types of defects: dimensional tolerances at machining stations, electrical continuity at wiring stations, and structural integrity at assembly stations. When defects are detected, the system must decide whether to repair the part, reject it entirely, or shut down the line to prevent further damage.</p>\n<p>In our framework, we implement similar quality control checkpoints. The tensor operations layer validates shape compatibility and numerical stability. The automatic differentiation engine checks gradient flow integrity and computation graph structure. The neural modules layer verifies parameter initialization and forward pass consistency. The training loop monitors convergence metrics and resource utilization. Each layer has specific failure modes, detection mechanisms, and recovery strategies tailored to its responsibilities.</p>\n<p>The key insight is that different types of errors require different handling strategies. Some errors, like shape mismatches, should fail fast with clear diagnostic messages. Others, like numerical instabilities, may require graceful degradation with warnings and automatic remediation. Resource exhaustion errors need cleanup and recovery mechanisms. Understanding these categories helps us design appropriate error handling for each component.</p>\n<h3 id=\"shape-and-broadcasting-errors\">Shape and Broadcasting Errors</h3>\n<p>Shape and broadcasting errors represent the most common category of failures in tensor-based neural networks. These errors occur when operations attempt to combine tensors with incompatible dimensions, when broadcasting rules cannot be applied to expand shapes, or when the resulting tensor shapes violate assumptions made by subsequent operations.</p>\n<p>The mental model for shape errors is dimensional analysis in physics - just as you cannot add meters to seconds without a conversion factor, you cannot add a <code>(32, 128)</code> tensor to a <code>(64, 256)</code> tensor without broadcasting rules that make the operation mathematically meaningful. Our framework must detect these incompatibilities early, provide clear diagnostic information about what went wrong, and suggest potential fixes.</p>\n<h4 id=\"shape-compatibility-validation\">Shape Compatibility Validation</h4>\n<p>The foundation of shape error handling lies in comprehensive validation during tensor operations. Every operation that combines multiple tensors must verify shape compatibility before proceeding with computation. This validation occurs at multiple levels: basic dimensional compatibility, broadcasting rule compliance, and operation-specific constraints.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Type</th>\n<th>Check Performed</th>\n<th>Error Condition</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dimension Count</td>\n<td>Verify tensor ranks</td>\n<td>Incompatible ranks for non-broadcastable ops</td>\n<td>Suggest reshape operations</td>\n</tr>\n<tr>\n<td>Broadcasting Rules</td>\n<td>Apply NumPy broadcasting algorithm</td>\n<td>Shapes cannot be broadcast together</td>\n<td>Show broadcasting expansion steps</td>\n</tr>\n<tr>\n<td>Matrix Operations</td>\n<td>Verify inner dimensions match</td>\n<td>MatMul dimension mismatch</td>\n<td>Display expected vs actual shapes</td>\n</tr>\n<tr>\n<td>Reduction Operations</td>\n<td>Validate axis parameters</td>\n<td>Axis out of range for tensor</td>\n<td>List valid axis options</td>\n</tr>\n<tr>\n<td>Index Operations</td>\n<td>Check bounds and slice validity</td>\n<td>Index exceeds tensor dimensions</td>\n<td>Show valid index ranges</td>\n</tr>\n</tbody></table>\n<p>The validation process follows a systematic approach. First, we extract the shapes of all input tensors and normalize them for comparison. Second, we apply operation-specific compatibility rules, such as matrix multiplication&#39;s requirement that the last dimension of the first tensor matches the second-to-last dimension of the second tensor. Third, we attempt to compute the output shape using broadcasting rules. If any step fails, we construct detailed error messages that include the problematic shapes, the operation being attempted, and suggestions for resolution.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Shape validation must occur before any computation begins, not during or after. Once numerical computation starts with incompatible shapes, the resulting errors can be cryptic and difficult to trace back to the root cause. Early validation provides clean error messages at the point of the mistake.</p>\n</blockquote>\n<h4 id=\"broadcasting-error-diagnostics\">Broadcasting Error Diagnostics</h4>\n<p>Broadcasting errors require special attention because they involve complex multi-dimensional shape transformations that can be difficult to visualize and debug. When broadcasting fails, programmers need to understand not just that the shapes are incompatible, but exactly how the broadcasting algorithm attempted to align the dimensions and where it failed.</p>\n<p>Our diagnostic system provides step-by-step visualization of the broadcasting process. When shapes <code>(3, 1, 4)</code> and <code>(2, 7)</code> cannot be broadcast together, the error message shows the alignment attempt:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Broadcasting Error: Cannot broadcast shapes (3, 1, 4) and (2, 7)\n\nAlignment attempt:\n  Shape 1: (3, 1, 4)  ← original\n  Shape 2:    (2, 7)  ← right-aligned\n  \nStep-by-step analysis:\n  Dimension 0: 3 vs missing → OK (extend with 1)\n  Dimension 1: 1 vs 2 → OK (broadcast 1 to 2)  \n  Dimension 2: 4 vs 7 → FAIL (neither is 1, cannot broadcast)\n\nSuggestion: Reshape tensors or use explicit broadcasting operations</code></pre></div>\n\n<p>This detailed breakdown helps programmers understand exactly where the broadcasting algorithm failed and how to fix the problem. The error message includes the original shapes, the right-aligned comparison that broadcasting uses, and specific analysis of each dimension pair.</p>\n<blockquote>\n<p><strong>Decision: Detailed Broadcasting Diagnostics</strong></p>\n<ul>\n<li><strong>Context</strong>: Broadcasting errors are common but the failure reasons are often opaque to users</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Simple &quot;incompatible shapes&quot; message</li>\n<li>Show aligned shapes only</li>\n<li>Step-by-step dimension analysis with suggestions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement comprehensive step-by-step analysis</li>\n<li><strong>Rationale</strong>: Educational framework should help users learn broadcasting rules, not just report failures</li>\n<li><strong>Consequences</strong>: More complex error handling code, but dramatically improved debugging experience for learners</li>\n</ul>\n</blockquote>\n<h4 id=\"gradient-shape-validation\">Gradient Shape Validation</h4>\n<p>A particularly subtle category of shape errors occurs during the backward pass when gradients must be accumulated and propagated through the computation graph. These errors arise because gradient tensors must exactly match the shapes of the tensors they correspond to, but broadcasting during the forward pass can create gradients with expanded shapes that need to be reduced back to the original parameter shapes.</p>\n<p>The gradient shape validation system tracks original tensor shapes throughout the computation graph and validates that gradients conform to these shapes during backpropagation. When shape mismatches occur, the system provides diagnostics that connect the gradient shape error back to the forward pass operation that caused it.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Error Type</th>\n<th>Cause</th>\n<th>Detection Method</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Expanded Gradient</td>\n<td>Broadcasting expanded tensor in forward pass</td>\n<td>Compare gradient shape to original tensor</td>\n<td>Apply sum reduction along broadcasted dims</td>\n</tr>\n<tr>\n<td>Missing Gradient Dimensions</td>\n<td>Reduction operation in forward pass</td>\n<td>Gradient has fewer dims than original</td>\n<td>Insert dimensions with size 1</td>\n</tr>\n<tr>\n<td>Accumulated Shape Mismatch</td>\n<td>Multiple gradient contributions with different shapes</td>\n<td>Shape check during gradient accumulation</td>\n<td>Validate all gradients before summing</td>\n</tr>\n<tr>\n<td>Parameter Update Shape Error</td>\n<td>Optimizer receives wrong gradient shape</td>\n<td>Shape validation in optimizer step</td>\n<td>Trace back through computation graph</td>\n</tr>\n</tbody></table>\n<h4 id=\"common-shape-error-pitfalls\">Common Shape Error Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: In-Place Operations Breaking Shape Tracking</strong>\nWhen tensors are modified in-place, their shape metadata may become inconsistent with their actual data, leading to cascading errors in subsequent operations. The framework must either prohibit in-place operations on tensors with <code>requires_grad=True</code>, or carefully update all metadata when in-place modifications occur.</p>\n<p>⚠️ <strong>Pitfall: Broadcasting Gradient Accumulation</strong>\nDuring backpropagation, gradients that were broadcasted during the forward pass must be &quot;un-broadcasted&quot; by summing along the expanded dimensions. Forgetting this step causes gradient tensors to have the wrong shape for parameter updates. The solution is to track which dimensions were broadcasted and automatically reduce them during gradient computation.</p>\n<p>⚠️ <strong>Pitfall: Shape Assumptions in Custom Operations</strong>\nCustom operations often make implicit assumptions about input tensor shapes without validating them. These assumptions fail when the operations are composed in unexpected ways. Every operation must explicitly validate its shape requirements and provide clear error messages when they are violated.</p>\n<h3 id=\"gradient-related-problems\">Gradient-Related Problems</h3>\n<p>Gradient-related problems represent some of the most challenging debugging scenarios in neural network frameworks. Unlike shape errors, which typically cause immediate failures with clear symptoms, gradient problems often manifest as subtle training instabilities, convergence failures, or numerical anomalies that develop over many training steps.</p>\n<p>The mental model for gradient problems is a mountain climbing expedition where each parameter update represents a step toward the summit (optimal loss). Gradient explosions are like taking enormous leaps that overshoot the target and land in dangerous territory. Vanishing gradients are like taking steps so small that progress becomes imperceptible. NaN gradients are like losing the compass entirely - the expedition can no longer navigate toward the goal.</p>\n<h4 id=\"gradient-explosion-detection-and-mitigation\">Gradient Explosion Detection and Mitigation</h4>\n<p>Gradient explosion occurs when gradient values become extremely large, causing parameter updates that destabilize the training process. This typically happens in deep networks where gradients are multiplied through many layers, causing exponential growth in gradient magnitude. Detection requires monitoring gradient norms and parameter update magnitudes throughout training.</p>\n<p>The gradient explosion detection system tracks multiple metrics to identify problematic gradient behavior before it corrupts the model. These metrics include individual parameter gradient norms, global gradient norm across all parameters, and the ratio of parameter update magnitude to current parameter values.</p>\n<table>\n<thead>\n<tr>\n<th>Detection Metric</th>\n<th>Threshold</th>\n<th>Interpretation</th>\n<th>Response Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global Gradient Norm</td>\n<td>&gt; 10.0</td>\n<td>Potential explosion beginning</td>\n<td>Enable gradient clipping</td>\n</tr>\n<tr>\n<td>Parameter Gradient Max</td>\n<td>&gt; 100.0</td>\n<td>Individual parameter exploding</td>\n<td>Investigate specific layer</td>\n</tr>\n<tr>\n<td>Update-to-Parameter Ratio</td>\n<td>&gt; 0.1</td>\n<td>Updates too large relative to values</td>\n<td>Reduce learning rate</td>\n</tr>\n<tr>\n<td>Gradient Norm Growth Rate</td>\n<td>&gt; 2x per step</td>\n<td>Exponential growth detected</td>\n<td>Emergency gradient clipping</td>\n</tr>\n<tr>\n<td>NaN/Inf Detection</td>\n<td>Any NaN/Inf</td>\n<td>Numerical overflow occurred</td>\n<td>Reset to last valid state</td>\n</tr>\n</tbody></table>\n<p>The gradient clipping implementation provides both global norm clipping and per-parameter clipping strategies. Global norm clipping scales all gradients by the same factor to maintain relative magnitudes while constraining the total gradient norm. Per-parameter clipping applies individual constraints to each parameter&#39;s gradients, which can be more aggressive but may distort the gradient direction.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Gradient Clipping Algorithm:\n1. Compute global gradient norm across all parameters\n2. If global norm exceeds threshold:\n   a. Compute scaling factor = threshold / global_norm\n   b. Scale all parameter gradients by scaling factor\n   c. Log clipping event and scaling factor\n3. For each parameter individually:\n   a. If parameter gradient norm exceeds per-param threshold\n   b. Clip parameter gradient to threshold magnitude\n   c. Preserve gradient direction\n4. Update gradient statistics for monitoring</code></pre></div>\n\n<h4 id=\"vanishing-gradient-detection\">Vanishing Gradient Detection</h4>\n<p>Vanishing gradients occur when gradient magnitudes become extremely small, effectively stopping parameter updates and preventing learning in affected layers. This problem is particularly common in deep networks and recurrent architectures where gradients are repeatedly multiplied by small values during backpropagation.</p>\n<p>Detection of vanishing gradients requires monitoring gradient magnitudes over time and identifying parameters whose gradients consistently fall below meaningful thresholds. Unlike gradient explosion, which causes immediate obvious problems, vanishing gradients create subtle learning stagnation that may only become apparent after many training steps.</p>\n<table>\n<thead>\n<tr>\n<th>Vanishing Gradient Indicator</th>\n<th>Threshold</th>\n<th>Detection Window</th>\n<th>Remediation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter Gradient Norm</td>\n<td>&lt; 1e-7</td>\n<td>50 consecutive steps</td>\n<td>Flag parameter as inactive</td>\n</tr>\n<tr>\n<td>Layer-wise Gradient Ratio</td>\n<td>&lt; 0.01 vs input layer</td>\n<td>Current step</td>\n<td>Investigate layer depth</td>\n</tr>\n<tr>\n<td>Parameter Update Magnitude</td>\n<td>&lt; 1e-10</td>\n<td>100 consecutive steps</td>\n<td>Consider parameter frozen</td>\n</tr>\n<tr>\n<td>Gradient Variance</td>\n<td>&lt; 1e-12</td>\n<td>20-step rolling window</td>\n<td>Check initialization scheme</td>\n</tr>\n</tbody></table>\n<p>The vanishing gradient diagnostic system provides layer-wise gradient analysis to identify where in the network gradients begin to diminish. This analysis computes gradient norms at each layer and compares them to the input layer gradients, helping identify problematic depth ranges or specific layer types that contribute to gradient decay.</p>\n<h4 id=\"nan-and-infinity-handling\">NaN and Infinity Handling</h4>\n<p>Numerical instabilities that produce NaN (Not a Number) or infinity values represent critical failures that can permanently corrupt training if not handled immediately. These instabilities arise from division by zero, logarithms of negative numbers, exponential overflow, or accumulated floating-point errors that exceed representable ranges.</p>\n<p>The NaN/infinity detection system performs comprehensive numerical validation at multiple checkpoints throughout the training process. Detection occurs during forward pass computation, gradient computation, parameter updates, and loss evaluation. When instabilities are detected, the system must decide whether to attempt recovery or abort training with detailed diagnostics.</p>\n<table>\n<thead>\n<tr>\n<th>Instability Source</th>\n<th>Common Causes</th>\n<th>Detection Point</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Division by Zero</td>\n<td>Normalizing by zero variance</td>\n<td>After normalization ops</td>\n<td>Add epsilon constant</td>\n</tr>\n<tr>\n<td>Exponential Overflow</td>\n<td>Large logits in softmax</td>\n<td>After activation functions</td>\n<td>Apply gradient clipping</td>\n</tr>\n<tr>\n<td>Logarithm of Non-positive</td>\n<td>Negative probabilities</td>\n<td>After probability computations</td>\n<td>Clamp to minimum value</td>\n</tr>\n<tr>\n<td>Accumulated Floating Point Error</td>\n<td>Long computation chains</td>\n<td>After gradient accumulation</td>\n<td>Reset computation graph</td>\n</tr>\n<tr>\n<td>Optimizer State Corruption</td>\n<td>NaN gradients affect momentum</td>\n<td>During optimizer step</td>\n<td>Reset optimizer state</td>\n</tr>\n</tbody></table>\n<p>The recovery system implements a hierarchical approach to handling numerical instabilities. Minor instabilities trigger automatic remediation such as adding epsilon values or clamping to valid ranges. Moderate instabilities trigger warnings and temporary gradient clipping or learning rate reduction. Severe instabilities that cannot be automatically remediated trigger training termination with comprehensive diagnostic information.</p>\n<blockquote>\n<p><strong>Decision: Immediate NaN Detection vs Batch Validation</strong></p>\n<ul>\n<li><strong>Context</strong>: NaN values can propagate through computation graphs and corrupt multiple tensors before detection</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Check every tensor operation result immediately</li>\n<li>Validate tensors at end of forward/backward passes</li>\n<li>Periodic validation every N steps</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Immediate detection during training, periodic validation during inference</li>\n<li><strong>Rationale</strong>: Training failures are expensive and hard to recover from, while inference can tolerate some performance overhead</li>\n<li><strong>Consequences</strong>: Higher computational cost during training, but much faster debugging and recovery from numerical issues</li>\n</ul>\n</blockquote>\n<h4 id=\"gradient-validation-and-testing\">Gradient Validation and Testing</h4>\n<p>Robust gradient computation requires systematic validation against known correct results. The gradient validation system compares automatic differentiation results with numerical differentiation to detect implementation bugs, subtle numerical errors, and edge cases where gradient computation fails.</p>\n<p>Numerical differentiation provides ground truth by computing gradients using the mathematical definition: the limit of the difference quotient as the step size approaches zero. While computationally expensive, numerical gradients serve as a reference for validating automatic differentiation implementations.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Test</th>\n<th>Method</th>\n<th>Tolerance</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single Operation Gradients</td>\n<td>Compare autodiff vs numerical</td>\n<td>1e-5</td>\n<td>Validate operation implementations</td>\n</tr>\n<tr>\n<td>Composition Gradients</td>\n<td>Chain multiple operations</td>\n<td>1e-4</td>\n<td>Test chain rule application</td>\n</tr>\n<tr>\n<td>Broadcasting Gradients</td>\n<td>Operations with broadcasting</td>\n<td>1e-5</td>\n<td>Validate shape handling</td>\n</tr>\n<tr>\n<td>Accumulation Gradients</td>\n<td>Multiple paths to same tensor</td>\n<td>1e-6</td>\n<td>Test gradient accumulation</td>\n</tr>\n<tr>\n<td>Second-Order Gradients</td>\n<td>Gradients of gradients</td>\n<td>1e-3</td>\n<td>Validate higher-order derivatives</td>\n</tr>\n</tbody></table>\n<p>The gradient testing infrastructure provides utilities for systematic validation across different tensor shapes, operation combinations, and edge cases. These tests run during development to catch regressions and can be enabled during training to validate custom operations or suspected gradient computation issues.</p>\n<h4 id=\"common-gradient-implementation-pitfalls\">Common Gradient Implementation Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Gradient Not Accumulated Across Multiple Uses</strong>\nWhen a tensor participates in multiple operations within a computation graph, its gradients must be accumulated (summed) from all paths. Forgetting to accumulate gradients causes incorrect gradient values and poor training performance. The solution is to always add incoming gradients to existing gradient values rather than overwriting them.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Topological Ordering in Backward Pass</strong>\nThe backward pass must visit computation graph nodes in reverse topological order to ensure gradients are computed after all dependent gradients are available. Incorrect ordering can cause gradients to be computed with incomplete information. The solution is to implement proper topological sorting based on the computation graph structure.</p>\n<p>⚠️ <strong>Pitfall: Broadcasting Gradients Not Reduced</strong>\nWhen tensors are broadcasted during forward pass operations, their gradients must be reduced (summed) along the broadcasted dimensions during the backward pass. This ensures gradient tensors have the same shape as their corresponding parameter tensors. The solution is to track broadcasting operations and automatically reduce gradients appropriately.</p>\n<h3 id=\"memory-and-resource-management\">Memory and Resource Management</h3>\n<p>Memory and resource management in neural network frameworks presents unique challenges due to the dynamic nature of computation graphs, the large size of tensors, and the complex interdependencies between training components. Unlike traditional applications where memory allocation follows predictable patterns, neural frameworks must manage memory for dynamically-constructed graphs that can grow arbitrarily large and contain circular references that prevent automatic garbage collection.</p>\n<p>Think of memory management in our framework like managing a large construction project with multiple work sites, shared equipment, and temporary structures. Each computation graph is like a construction site that accumulates scaffolding (intermediate tensors), equipment (gradient functions), and materials (cached computations). Without proper cleanup protocols, these resources accumulate across sites, eventually exhausting available capacity and bringing all work to a halt.</p>\n<h4 id=\"computation-graph-memory-leaks\">Computation Graph Memory Leaks</h4>\n<p>Computation graphs create complex webs of references between tensors and operations that can prevent proper memory reclamation. Each tensor holds references to the operations that created it, and each operation holds references to its input tensors. This creates cycles that Python&#39;s garbage collector cannot automatically resolve, leading to memory leaks that accumulate over training iterations.</p>\n<p>The fundamental challenge is that computation graphs must remain intact during the forward pass and through gradient computation, but they must be cleanly destroyed afterward to free memory for subsequent iterations. This requires careful lifecycle management that tracks graph components and ensures timely cleanup without interfering with ongoing computations.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Leak Source</th>\n<th>Reference Pattern</th>\n<th>Detection Method</th>\n<th>Cleanup Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tensor-Operation Cycles</td>\n<td>Tensor.grad_fn → Operation → inputs → Tensor</td>\n<td>Weak reference analysis</td>\n<td>Explicit graph destruction</td>\n</tr>\n<tr>\n<td>Accumulated Gradients</td>\n<td>Multiple gradients referencing same tensors</td>\n<td>Memory profiling over time</td>\n<td>Periodic gradient clearing</td>\n</tr>\n<tr>\n<td>Cached Intermediate Values</td>\n<td>Operations storing forward pass results</td>\n<td>Object count tracking</td>\n<td>LRU cache with size limits</td>\n</tr>\n<tr>\n<td>Optimizer State</td>\n<td>Growing state dictionaries per parameter</td>\n<td>Memory usage monitoring</td>\n<td>State compression and pruning</td>\n</tr>\n<tr>\n<td>Training History</td>\n<td>Loss values, metrics, gradient norms</td>\n<td>Linear growth detection</td>\n<td>Rolling window storage</td>\n</tr>\n</tbody></table>\n<p>The graph cleanup system implements a multi-phase approach to memory reclamation. After each training step, it identifies computation graphs that are no longer needed, breaks reference cycles by clearing operation inputs, and explicitly destroys tensor gradients and cached values. This cleanup must be coordinated carefully to avoid interfering with ongoing gradient computation or optimizer state updates.</p>\n<h4 id=\"memory-efficient-graph-construction\">Memory-Efficient Graph Construction</h4>\n<p>Large neural networks and long training sequences can create computation graphs that exceed available memory even with proper cleanup. Memory-efficient graph construction techniques reduce peak memory usage by strategically releasing intermediate values, using gradient checkpointing, and implementing streaming computation patterns.</p>\n<p>Gradient checkpointing represents a key trade-off between memory usage and computation time. Instead of storing all intermediate activations throughout the forward pass, gradient checkpointing saves only selected checkpoints and recomputes intermediate values during the backward pass. This reduces memory usage at the cost of additional computation.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Optimization</th>\n<th>Memory Reduction</th>\n<th>Computation Overhead</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradient Checkpointing</td>\n<td>50-80%</td>\n<td>30-50% increase</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>In-Place Operations</td>\n<td>20-40%</td>\n<td>Minimal</td>\n<td>High (gradient safety)</td>\n</tr>\n<tr>\n<td>Streaming Computation</td>\n<td>60-90%</td>\n<td>Variable</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Dynamic Graph Pruning</td>\n<td>30-60%</td>\n<td>10-20% increase</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Compressed Gradients</td>\n<td>40-70%</td>\n<td>15-25% increase</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>The memory management system provides automatic memory monitoring and adaptive strategies based on available resources. When memory usage approaches system limits, the framework automatically enables more aggressive optimization techniques such as gradient checkpointing or compressed gradient storage.</p>\n<blockquote>\n<p><strong>Decision: Automatic vs Manual Memory Management</strong></p>\n<ul>\n<li><strong>Context</strong>: Learners should focus on core concepts rather than memory management details, but memory leaks can make training impossible</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Fully manual memory management requiring explicit cleanup calls</li>\n<li>Automatic cleanup with performance overhead</li>\n<li>Hybrid approach with automatic cleanup and manual override options</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Automatic cleanup with monitoring and manual override capabilities</li>\n<li><strong>Rationale</strong>: Educational framework should not burden learners with memory details, but advanced users need control for large experiments</li>\n<li><strong>Consequences</strong>: More complex implementation, but better learning experience and scalability for serious projects</li>\n</ul>\n</blockquote>\n<h4 id=\"large-tensor-allocation-strategies\">Large Tensor Allocation Strategies</h4>\n<p>Neural networks often require tensors that approach or exceed available system memory, particularly during batch processing or when working with high-resolution data. Large tensor allocation requires strategies for detecting memory constraints, implementing out-of-core computation, and gracefully degrading when resources are insufficient.</p>\n<p>The large tensor management system monitors available memory and automatically adjusts allocation strategies based on resource constraints. When standard in-memory allocation would exceed available resources, the system can switch to memory-mapped files, reduce batch sizes, or implement streaming computation patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Allocation Strategy</th>\n<th>Memory Requirement</th>\n<th>Performance Impact</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Standard In-Memory</td>\n<td>Full tensor size</td>\n<td>Optimal</td>\n<td>Small to medium tensors</td>\n</tr>\n<tr>\n<td>Memory-Mapped Files</td>\n<td>Minimal RAM usage</td>\n<td>I/O bound operations</td>\n<td>Very large tensors</td>\n</tr>\n<tr>\n<td>Chunked Processing</td>\n<td>Chunk size only</td>\n<td>CPU overhead</td>\n<td>Batch dimension too large</td>\n</tr>\n<tr>\n<td>Compressed Storage</td>\n<td>30-70% reduction</td>\n<td>Compression/decompression cost</td>\n<td>Storage-bound scenarios</td>\n</tr>\n<tr>\n<td>Distributed Tensors</td>\n<td>Split across devices</td>\n<td>Communication overhead</td>\n<td>Multi-GPU scenarios</td>\n</tr>\n</tbody></table>\n<p>The allocation system provides automatic fallback strategies when primary allocation methods fail. If in-memory allocation fails due to insufficient memory, the system automatically attempts memory-mapped allocation. If batch sizes are too large, it automatically reduces batch sizes and adjusts learning rate accordingly.</p>\n<h4 id=\"resource-monitoring-and-alerts\">Resource Monitoring and Alerts</h4>\n<p>Effective resource management requires continuous monitoring of memory usage, computation time, and system resources throughout training. The monitoring system tracks resource consumption patterns, identifies potential problems before they cause failures, and provides alerts when resource usage approaches dangerous levels.</p>\n<p>The monitoring infrastructure collects metrics at multiple levels: per-tensor memory usage, per-operation computation time, per-layer gradient statistics, and system-wide resource utilization. These metrics are analyzed to identify trends, detect anomalies, and predict potential resource exhaustion.</p>\n<table>\n<thead>\n<tr>\n<th>Monitoring Metric</th>\n<th>Alert Threshold</th>\n<th>Trend Analysis</th>\n<th>Remediation Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Usage</td>\n<td>80% of available</td>\n<td>Growth rate &gt; 5% per epoch</td>\n<td>Enable aggressive cleanup</td>\n</tr>\n<tr>\n<td>Gradient Norms</td>\n<td>3x standard deviation</td>\n<td>Increasing variance</td>\n<td>Enable gradient monitoring</td>\n</tr>\n<tr>\n<td>Computation Time</td>\n<td>2x expected duration</td>\n<td>Sudden increases</td>\n<td>Profile operation performance</td>\n</tr>\n<tr>\n<td>Graph Size</td>\n<td>10000+ operations</td>\n<td>Linear growth</td>\n<td>Enable graph pruning</td>\n</tr>\n<tr>\n<td>Parameter Count</td>\n<td>RAM capacity limit</td>\n<td>Approaching limits</td>\n<td>Suggest model reduction</td>\n</tr>\n</tbody></table>\n<p>The alert system provides graduated responses based on severity levels. Warning alerts log information for later analysis without interrupting training. Critical alerts pause training and provide diagnostic information to help identify the cause of resource problems. Emergency alerts terminate training to prevent system instability.</p>\n<h4 id=\"common-memory-management-pitfalls\">Common Memory Management Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Retaining References to Computation Graphs</strong>\nStoring references to tensors or intermediate results from previous training iterations prevents garbage collection of entire computation graphs. The solution is to extract scalar values (like loss) rather than retaining tensor objects, and to explicitly clear any collections that might hold tensor references.</p>\n<p>⚠️ <strong>Pitfall: Accumulating Optimizer State Without Bounds</strong>\nSome optimizers accumulate state (like momentum buffers) that grows over time, especially when new parameters are added dynamically. The solution is to implement state cleanup when parameters are removed and to monitor optimizer memory usage as part of overall resource tracking.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in Custom Operations</strong>\nCustom operations that allocate temporary memory or create intermediate tensors without proper cleanup can cause subtle memory leaks. The solution is to implement explicit cleanup methods for custom operations and to use memory profiling during testing to detect leaks.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides the infrastructure and starter code needed to implement robust error handling throughout your neural network framework. The error handling system must integrate with all four layers of the architecture while remaining simple enough for educational purposes.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape Validation</td>\n<td>Custom validation functions with NumPy</td>\n<td>Formal shape algebra with symbolic shapes</td>\n</tr>\n<tr>\n<td>Error Messages</td>\n<td>String formatting with templates</td>\n<td>Rich error objects with structured data</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>Python memory_profiler</td>\n<td>Custom C extension with detailed tracking</td>\n</tr>\n<tr>\n<td>Gradient Validation</td>\n<td>Numerical differentiation with finite differences</td>\n<td>Symbolic differentiation with computer algebra</td>\n</tr>\n<tr>\n<td>Resource Alerts</td>\n<td>Simple logging with thresholds</td>\n<td>Prometheus metrics with alerting rules</td>\n</tr>\n<tr>\n<td>Graph Cleanup</td>\n<td>Manual reference breaking</td>\n<td>Weak references with automatic cleanup</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n  core/\n    errors.py              ← Exception classes and error handling utilities  \n    validation.py          ← Shape and gradient validation functions\n    diagnostics.py         ← Error diagnosis and debugging tools\n  tensor/\n    tensor.py             ← Tensor class with error checking\n    operations.py         ← Operations with validation\n    broadcasting.py       ← Broadcasting utilities and error handling\n  autodiff/\n    engine.py            ← Autodiff with gradient validation\n    graph.py             ← Graph management and cleanup\n  training/\n    monitor.py           ← Training monitoring and alerts\n    coordinator.py       ← Training coordination with error handling\n  utils/\n    memory.py            ← Memory management utilities\n    profiling.py         ← Performance and memory profiling</code></pre></div>\n\n<h4 id=\"core-error-handling-infrastructure\">Core Error Handling Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># core/errors.py - Complete error handling system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, List, Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NeuralFrameworkError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all neural framework errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.details </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> details </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stack_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> traceback.format_stack()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ShapeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when tensor shapes are incompatible for operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, shapes: List[Tuple], operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shapes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shapes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shape_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \", \"</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"shape_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> i, shape </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.shapes))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.args[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> in </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape_info</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BroadcastingError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ShapeError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when broadcasting rules cannot align tensor shapes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, shape1: Tuple, shape2: Tuple, step_analysis: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shape1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shape1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shape2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shape2</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_analysis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> step_analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        analysis_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> step </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> step_analysis)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Cannot broadcast shapes </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> and </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape2</span><span style=\"color:#79B8FF\">}\\n{</span><span style=\"color:#E1E4E8\">analysis_text</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, [shape1, shape2], </span><span style=\"color:#9ECBFF\">\"broadcasting\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when gradient computation fails or produces invalid results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NumericalInstabilityError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when NaN or infinity values are detected.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tensor_info: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tensor_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor_info</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_tensor_finite</span><span style=\"color:#E1E4E8\">(tensor: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that tensor contains only finite values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(tensor.data).all():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nan_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.isnan(tensor.data).sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inf_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.isinf(tensor.data).sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tensor_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'shape'</span><span style=\"color:#E1E4E8\">: tensor.data.shape,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'dtype'</span><span style=\"color:#E1E4E8\">: tensor.data.dtype,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'nan_count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(nan_count),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'inf_count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(inf_count),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'operation'</span><span style=\"color:#E1E4E8\">: operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#E1E4E8\"> NumericalInstabilityError(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Tensor contains </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">nan_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> NaN and </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">inf_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> infinite values after </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tensor_info</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_shape_error_message</span><span style=\"color:#E1E4E8\">(shapes: List[Tuple], operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             expected_pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create detailed error message for shape mismatches.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shape_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Input </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> i, shape </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(shapes)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shape_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(shape_list)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Shape mismatch in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">\\n{</span><span style=\"color:#E1E4E8\">shape_text</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> expected_pattern:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Expected pattern: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_pattern</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> message</span></span></code></pre></div>\n\n<h4 id=\"broadcasting-validation-and-diagnostics\">Broadcasting Validation and Diagnostics</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tensor/broadcasting.py - Broadcasting utilities with comprehensive error handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BroadcastingError, ShapeError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> broadcast_shapes</span><span style=\"color:#E1E4E8\">(shape1: Tuple, shape2: Tuple) -> Tuple:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute the broadcasted shape for two input shapes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises BroadcastingError with detailed analysis if broadcasting fails.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Right-align shapes by padding with 1s on the left</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare dimensions from right to left</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each dimension pair, apply broadcasting rules:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #         - If dimensions are equal, use that dimension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #         - If one dimension is 1, use the other dimension  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #         - Otherwise, shapes are incompatible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Collect step-by-step analysis for error reporting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return resulting shape or raise BroadcastingError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> analyze_broadcasting_failure</span><span style=\"color:#E1E4E8\">(shape1: Tuple, shape2: Tuple) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Analyze why two shapes cannot be broadcasted together.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns step-by-step analysis for error messages.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Right-align shapes and show alignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare each dimension pair and classify compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify the first incompatible dimension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Suggest potential fixes (reshape operations, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> unbroadcast_gradient</span><span style=\"color:#E1E4E8\">(grad: np.ndarray, original_shape: Tuple) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Reduce gradient tensor to match original parameter shape.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This reverses the broadcasting that occurred during forward pass.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Identify dimensions that were broadcasted (size 1 in original)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Sum along broadcasted dimensions to reduce gradient</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Squeeze dimensions that were added during broadcasting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate final gradient shape matches original shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle edge case where grad is scalar but original has shape</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_broadcasting_safety</span><span style=\"color:#E1E4E8\">(tensors: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">], operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validate that a list of tensors can be safely broadcasted together.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns the final broadcasted shape or raises detailed error.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> tensors:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Cannot broadcast empty tensor list\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tensors) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tensors[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].data.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensors[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].data.shape</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(tensors[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:], </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> broadcast_shapes(result_shape, tensor.data.shape)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result_shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#E1E4E8\"> BroadcastingError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Enhance error message with operation context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        enhanced_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e.args[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#E1E4E8\"> BroadcastingError(e.shape1, e.shape2, e.step_analysis) </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> e</span></span></code></pre></div>\n\n<h4 id=\"gradient-validation-system\">Gradient Validation System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># core/validation.py - Gradient validation and numerical checking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Dict, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> GradientError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> numerical_gradient</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">], h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> List[np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute numerical gradients using central difference method.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Used as ground truth for validating automatic differentiation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: For each input tensor, compute partial derivatives</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use central difference: (f(x+h) - f(x-h)) / (2*h)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle each element of each input tensor individually</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return list of gradient arrays matching input shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Use appropriate step size for tensor dtype precision</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">], tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compare automatic differentiation gradients with numerical gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns True if gradients match within tolerance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute gradients using automatic differentiation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute gradients using numerical differentiation  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare gradients element-wise within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Report detailed analysis of any mismatches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return validation result and diagnostic information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_gradient_flow</span><span style=\"color:#E1E4E8\">(model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">, loss: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Check that gradients flow properly through all model parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns dictionary with gradient statistics for each parameter.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check that each parameter has non-None gradient</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute gradient norm, mean, std for each parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify parameters with zero or very small gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Flag potential vanishing gradient problems</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return comprehensive gradient flow analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradient_stats</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitors gradient statistics during training to detect problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, check_frequency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_frequency</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(self, model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Perform gradient health checks and return diagnostic information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute gradient norms for all parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for NaN or infinite gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect gradient explosion (norms too large)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect gradient vanishing (norms too small)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compare current gradients to historical values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return comprehensive diagnostic report</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"memory-management-infrastructure\">Memory Management Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># utils/memory.py - Memory management and resource monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> gc</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> weakref</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set, List, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> NeuralFrameworkError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComputationGraphTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks active computation graphs to prevent memory leaks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._active_graphs: Set[weakref.ref] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cleanup_callbacks: List[Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_tensor</span><span style=\"color:#E1E4E8\">(self, tensor: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a tensor as part of active computation graph.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create weak reference to tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add cleanup callback for when tensor is garbage collected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Track tensor in active graphs set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_graphs</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Force cleanup of unreachable computation graphs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Identify graphs with no strong references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Break circular references between tensors and operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Clear gradient functions and cached values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Force garbage collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return number of cleaned up graphs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitors memory usage and provides alerts for resource problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, warning_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#E1E4E8\">, critical_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.9</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.warning_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> warning_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.critical_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> critical_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process().memory_info().rss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_memory_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check current memory usage and return status information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        process </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> process.memory_info()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> memory_info.rss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        available_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().available </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        usage_fraction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> memory_info.rss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().total</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'current_mb'</span><span style=\"color:#E1E4E8\">: current_mb,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'available_mb'</span><span style=\"color:#E1E4E8\">: available_mb, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'usage_fraction'</span><span style=\"color:#E1E4E8\">: usage_fraction,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'alert_level'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'normal'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compare usage against thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set appropriate alert level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Include memory growth rate if tracking over time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add recommendations for memory reduction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> status</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cleanup_training_step</span><span style=\"color:#E1E4E8\">(model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">, optimizer: </span><span style=\"color:#9ECBFF\">'Optimizer'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Perform cleanup after training step to prevent memory leaks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Clear gradients from model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Clean up computation graphs from this step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Force garbage collection if memory usage is high</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clear any cached values in optimizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"training-monitoring-and-alerts\">Training Monitoring and Alerts</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># training/monitor.py - Training monitoring with error detection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> NumericalInstabilityError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitors training progress and detects problems automatically.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, check_frequency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_frequency</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_norms: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_step</span><span style=\"color:#E1E4E8\">(self, loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, gradients: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log training step and check for problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history.append(loss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute global gradient norm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for NaN or infinite loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for gradient explosion or vanishing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect loss divergence or stagnation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Log warnings or alerts as appropriate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update gradient statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._perform_health_checks()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _perform_health_checks</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Perform comprehensive training health checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Analyze loss trend over recent steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check gradient norm stability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate memory usage is stable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for numerical instabilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate alerts or recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_diagnostics</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return comprehensive training diagnostics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute loss statistics (mean, std, trend)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute gradient statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify potential problems</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Provide recommendations for fixes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_training_step</span><span style=\"color:#E1E4E8\">(model: </span><span style=\"color:#9ECBFF\">'Module'</span><span style=\"color:#E1E4E8\">, loss: </span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         optimizer: </span><span style=\"color:#9ECBFF\">'Optimizer'</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validate that training step completed successfully.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns list of warnings or empty list if everything is OK.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warnings </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that loss is finite and reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify all parameters have gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check gradient magnitudes are reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate optimizer state is consistent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for memory leaks or unusual memory growth</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of any warnings found</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> warnings</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing error handling infrastructure, verify your system with these checkpoints:</p>\n<p><strong>Checkpoint 1: Shape Error Detection</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test that shape errors are caught and reported clearly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tensor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># This should raise a clear BroadcastingError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.random.randn(</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.random.randn(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> b  </span><span style=\"color:#6A737D\"># Should fail with detailed error message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#E1E4E8\"> BroadcastingError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Broadcasting error detected correctly\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Error message: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Checkpoint 2: Gradient Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test gradient correctness using numerical differentiation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> simple_function</span><span style=\"color:#E1E4E8\">(x):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> (x </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">).sum()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.array([</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">]), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> simple_function(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result.backward()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should validate that gradients match numerical approximation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">is_correct </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_gradients(simple_function, [x], </span><span style=\"color:#FFAB70\">tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> is_correct, </span><span style=\"color:#9ECBFF\">\"Gradients don't match numerical approximation\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Gradient validation working correctly\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Checkpoint 3: Memory Leak Detection</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test that computation graphs are cleaned up properly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> gc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ComputationGraphTracker()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">initial_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(gc.get_objects())</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Create and destroy many computation graphs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.random.randn(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.matmul(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y.sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Should automatically clean up graph</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">final_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(gc.get_objects())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">growth </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> final_count </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> initial_count</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> growth </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Memory leak detected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">growth</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> objects accumulated\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Memory management working correctly\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - testing is critical throughout tensor operations, automatic differentiation, neural modules, and training loops to ensure correctness and catch bugs early</p>\n</blockquote>\n<p>Building a neural network framework requires rigorous testing because the mathematical complexity creates many opportunities for subtle bugs. Think of testing as conducting scientific experiments to verify our mathematical claims. Just as physicists verify theoretical predictions with experimental measurements, we must verify our automatic differentiation implementation against known mathematical ground truth.</p>\n<p>The testing strategy for a neural network framework differs fundamentally from typical software testing because we&#39;re implementing mathematical algorithms with precise, verifiable correct answers. Unlike testing a web API where we might mock external services, neural network testing relies heavily on mathematical verification - comparing our computed gradients against analytically or numerically derived ground truth.</p>\n<p>Testing becomes even more critical because bugs in neural network frameworks often manifest as poor training performance rather than obvious crashes. A subtle gradient computation error might allow a model to train but converge slowly or to suboptimal solutions. Our testing strategy must catch these mathematical errors before they become mysterious training issues.</p>\n<h3 id=\"gradient-correctness-testing\">Gradient Correctness Testing</h3>\n<p>The cornerstone of neural network framework testing is <strong>gradient correctness testing</strong> - verifying that our automatic differentiation implementation produces mathematically correct gradients. Think of this as calibrating a scientific instrument: before trusting our automatic differentiation &quot;instrument&quot; to compute gradients for complex models, we must verify it produces correct results on simple, analytically tractable problems.</p>\n<p>Gradient correctness testing relies on comparing our automatic differentiation results against <strong>numerical differentiation</strong> - a brute-force but mathematically sound technique that approximates derivatives using the definition of a derivative as a limit. While numerical differentiation is too slow for training neural networks, it provides reliable ground truth for testing.</p>\n<p>The mathematical foundation is the finite difference approximation of derivatives. For a scalar function f(x), the derivative is:</p>\n<p>f&#39;(x) ≈ [f(x + h) - f(x - h)] / (2h)</p>\n<p>For multivariable functions, we compute partial derivatives by perturbing each input dimension independently while holding others constant. This <strong>numerical gradient</strong> computation is slow (requires 2n function evaluations for n parameters) but mathematically reliable.</p>\n<p>Our gradient testing framework implements several key verification procedures:</p>\n<table>\n<thead>\n<tr>\n<th>Testing Method</th>\n<th>Purpose</th>\n<th>When to Use</th>\n<th>Limitations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Analytical Verification</strong></td>\n<td>Compare against hand-computed derivatives</td>\n<td>Simple functions like x², sin(x), matrix multiply</td>\n<td>Only feasible for simple operations</td>\n</tr>\n<tr>\n<td><strong>Numerical Differentiation</strong></td>\n<td>Compare against finite difference approximation</td>\n<td>All operations and composed functions</td>\n<td>Slow, numerical precision issues</td>\n</tr>\n<tr>\n<td><strong>Gradient Checking</strong></td>\n<td>Automated numerical vs autodiff comparison</td>\n<td>During development and debugging</td>\n<td>Requires careful tolerance selection</td>\n</tr>\n<tr>\n<td><strong>Property Testing</strong></td>\n<td>Verify mathematical properties like linearity</td>\n<td>Testing operation implementations</td>\n<td>Doesn&#39;t catch all error types</td>\n</tr>\n</tbody></table>\n<p><strong>Numerical differentiation implementation</strong> requires careful attention to the step size h. Too large, and we get poor approximations due to higher-order terms in the Taylor expansion. Too small, and floating-point precision errors dominate. The typical choice is h = 1e-5, which balances approximation accuracy with numerical stability.</p>\n<p>The gradient checking procedure follows these steps:</p>\n<ol>\n<li><strong>Function Wrapping</strong>: Create a scalar-valued function from our neural network computation by taking a single output element and fixing all other inputs</li>\n<li><strong>Parameter Perturbation</strong>: For each parameter, compute f(θ + h⋅eᵢ) and f(θ - h⋅eᵢ) where eᵢ is the i-th unit vector</li>\n<li><strong>Numerical Gradient</strong>: Compute numerical gradient as [f(θ + h⋅eᵢ) - f(θ - h⋅eᵢ)] / (2h)</li>\n<li><strong>Autodiff Gradient</strong>: Run our automatic differentiation to compute the same gradient</li>\n<li><strong>Comparison</strong>: Check that |numerical_grad - autodiff_grad| / max(|numerical_grad|, |autodiff_grad|, 1e-8) &lt; tolerance</li>\n</ol>\n<p>The relative error formula handles cases where gradients are near zero and provides scale-invariant comparison. A typical tolerance is 1e-5, though some operations may require looser tolerances due to numerical precision limitations.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Gradient checking is your mathematical safety net. Every operation you implement should pass gradient checking on multiple random inputs before you trust it in a real neural network. A single gradient error can make training fail mysteriously.</p>\n</blockquote>\n<p><strong>Property-based testing</strong> verifies mathematical properties that should hold regardless of specific input values:</p>\n<table>\n<thead>\n<tr>\n<th>Property</th>\n<th>Mathematical Rule</th>\n<th>How to Test</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Linearity of Gradients</strong></td>\n<td>∇(af + bg) = a∇f + b∇g</td>\n<td>Compute gradients of linear combinations vs combinations of gradients</td>\n</tr>\n<tr>\n<td><strong>Chain Rule</strong></td>\n<td>∇(f∘g) = (∇f)(g) ⊙ ∇g</td>\n<td>Test composed operations vs manual composition</td>\n</tr>\n<tr>\n<td><strong>Product Rule</strong></td>\n<td>∇(fg) = f∇g + g∇f</td>\n<td>Test element-wise multiplication gradients</td>\n</tr>\n<tr>\n<td><strong>Zero Gradient</strong></td>\n<td>∇c = 0 for constants</td>\n<td>Verify constant tensors have zero gradients</td>\n</tr>\n<tr>\n<td><strong>Broadcasting Consistency</strong></td>\n<td>Gradients unbroadcast correctly</td>\n<td>Check gradient shapes match original parameter shapes</td>\n</tr>\n</tbody></table>\n<p><strong>Comprehensive test coverage</strong> for gradient correctness includes:</p>\n<ul>\n<li><strong>Elementary Operations</strong>: Addition, multiplication, division, power functions with various input shapes and broadcasting patterns</li>\n<li><strong>Transcendental Functions</strong>: exp, log, sin, cos, tanh and their compositions</li>\n<li><strong>Matrix Operations</strong>: Matrix multiplication, transpose, reshape with different dimensions</li>\n<li><strong>Reduction Operations</strong>: Sum, mean, max with different axis specifications</li>\n<li><strong>Composed Functions</strong>: Complex expressions combining multiple operations to test chain rule implementation</li>\n<li><strong>Edge Cases</strong>: Zero gradients, very large/small values, boundary conditions</li>\n</ul>\n<blockquote>\n<p><strong>Decision: Gradient Checking Integration Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance thorough gradient verification with reasonable test execution times</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Manual gradient checking for each operation</li>\n<li>Automated gradient checking in unit tests</li>\n<li>Optional gradient checking with environment flag</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Automated gradient checking in unit tests with configurable tolerance</li>\n<li><strong>Rationale</strong>: Catches gradient bugs immediately during development while allowing fast iteration</li>\n<li><strong>Consequences</strong>: Longer test execution time but much higher confidence in mathematical correctness</li>\n</ul>\n</blockquote>\n<h3 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h3>\n<p>Each milestone represents a major capability that must be thoroughly verified before proceeding to the next stage. Think of these checkpoints as <strong>integration tests</strong> that verify not just individual components but their interaction as a coherent system.</p>\n<p>Milestone verification goes beyond unit testing individual methods - it tests the <strong>emergent behavior</strong> that arises when components work together. For example, Milestone 2 tests whether the entire automatic differentiation pipeline (tensor operations + computation graph + backpropagation) produces correct gradients for realistic neural network computations.</p>\n<h4 id=\"milestone-1-tensor-amp-operations-verification\">Milestone 1: Tensor &amp; Operations Verification</h4>\n<p>The tensor operations milestone verification focuses on ensuring our tensor abstraction behaves identically to NumPy for forward computations while correctly tracking gradient metadata.</p>\n<p><strong>Core Tensor Functionality Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Specific Tests</th>\n<th>Expected Behavior</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tensor Creation</strong></td>\n<td>Various dtypes, shapes, requires_grad combinations</td>\n<td>Tensors store data correctly, metadata accessible</td>\n<td>Shape mismatches, dtype conversion errors</td>\n</tr>\n<tr>\n<td><strong>Arithmetic Operations</strong></td>\n<td>+, -, *, / with scalars and tensors</td>\n<td>Results match NumPy, grad_fn set correctly</td>\n<td>Wrong results, missing gradient tracking</td>\n</tr>\n<tr>\n<td><strong>Broadcasting</strong></td>\n<td>Mismatched shapes following NumPy rules</td>\n<td>Automatic shape expansion, result shapes correct</td>\n<td>Broadcasting failures, wrong output shapes</td>\n</tr>\n<tr>\n<td><strong>Matrix Operations</strong></td>\n<td>2D and batched matrix multiplication</td>\n<td>Correct matrix multiply results, batch handling</td>\n<td>Dimension errors, wrong batch semantics</td>\n</tr>\n<tr>\n<td><strong>Shape Operations</strong></td>\n<td>Reshape, transpose, indexing</td>\n<td>Shape transformations work, data preserved</td>\n<td>Data corruption, shape inconsistencies</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Procedure for Milestone 1:</strong></p>\n<ol>\n<li><strong>NumPy Equivalence Testing</strong>: For every tensor operation, create equivalent NumPy arrays and verify our results match NumPy exactly (within floating-point precision)</li>\n<li><strong>Gradient Tracking Verification</strong>: Ensure operations on <code>requires_grad=True</code> tensors create appropriate <code>grad_fn</code> references</li>\n<li><strong>Broadcasting Edge Cases</strong>: Test all NumPy broadcasting patterns including scalar expansion, dimension addition, and size-1 expansion</li>\n<li><strong>Memory Layout Testing</strong>: Verify tensors maintain consistent memory layout and shape metadata after operations</li>\n<li><strong>Error Handling</strong>: Confirm operations fail gracefully with informative error messages for incompatible shapes</li>\n</ol>\n<p><strong>Milestone 1 Checkpoint Criteria:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># After implementing Milestone 1, these operations should work perfectly:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([[</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4.0</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">10.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">20.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> b  </span><span style=\"color:#6A737D\"># Broadcasting: (2,2) + (2,) -> (2,2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">d </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a.matmul(a.T)  </span><span style=\"color:#6A737D\"># Matrix multiply with transpose</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">e </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> c </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> d  </span><span style=\"color:#6A737D\"># Element-wise multiply</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># All results should match NumPy exactly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># All tensors should have appropriate grad_fn references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># No memory leaks or shape inconsistencies</span></span></code></pre></div>\n\n<h4 id=\"milestone-2-automatic-differentiation-verification\">Milestone 2: Automatic Differentiation Verification</h4>\n<p>Automatic differentiation verification ensures our gradient computation infrastructure produces mathematically correct results for increasingly complex computation graphs.</p>\n<p><strong>Gradient Computation Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Specific Tests</th>\n<th>Expected Behavior</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Simple Operations</strong></td>\n<td>Gradients of x², x+y, x*y</td>\n<td>Match analytical derivatives exactly</td>\n<td>Wrong gradient values</td>\n</tr>\n<tr>\n<td><strong>Chain Rule</strong></td>\n<td>Gradients of f(g(x)) compositions</td>\n<td>Proper chain rule application</td>\n<td>Missing gradient contributions</td>\n</tr>\n<tr>\n<td><strong>Multiple Paths</strong></td>\n<td>Same tensor used multiple times</td>\n<td>Gradient accumulation works correctly</td>\n<td>Missing accumulated gradients</td>\n</tr>\n<tr>\n<td><strong>Complex Graphs</strong></td>\n<td>Deep computation trees</td>\n<td>Correct topological ordering</td>\n<td>Wrong gradient ordering</td>\n</tr>\n<tr>\n<td><strong>Zero Gradients</strong></td>\n<td>Constants and non-differentiable paths</td>\n<td>Zero gradients where expected</td>\n<td>Non-zero gradients for constants</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Procedure for Milestone 2:</strong></p>\n<ol>\n<li><strong>Gradient Checking</strong>: Every operation passes numerical differentiation comparison with tolerance 1e-5</li>\n<li><strong>Computation Graph Inspection</strong>: Verify graph structure matches expected topology for complex expressions</li>\n<li><strong>Gradient Accumulation</strong>: Test expressions like y = x + x to ensure gradient accumulation sums correctly</li>\n<li><strong>Topological Ordering</strong>: Manually verify backward pass processes nodes in correct dependency order</li>\n<li><strong>Memory Management</strong>: Ensure computation graphs can be garbage collected after backward pass</li>\n</ol>\n<p><strong>Progressive Complexity Testing</strong> builds confidence by starting with simple cases and adding complexity:</p>\n<table>\n<thead>\n<tr>\n<th>Complexity Level</th>\n<th>Example Expression</th>\n<th>Key Test Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Linear</strong></td>\n<td>y = 3*x + 2</td>\n<td>Basic gradient computation and constants</td>\n</tr>\n<tr>\n<td><strong>Quadratic</strong></td>\n<td>y = x² + 2*x + 1</td>\n<td>Power rule and addition</td>\n</tr>\n<tr>\n<td><strong>Composition</strong></td>\n<td>y = sin(x²)</td>\n<td>Chain rule through transcendental functions</td>\n</tr>\n<tr>\n<td><strong>Multi-variable</strong></td>\n<td>z = x*y + x²</td>\n<td>Multiple inputs and gradient accumulation</td>\n</tr>\n<tr>\n<td><strong>Deep Trees</strong></td>\n<td>((x<em>2)+3)</em>((x+1)*4)</td>\n<td>Complex computation graphs</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 2 Checkpoint Criteria:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># After implementing Milestone 2, gradient computation should work:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">z </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">x</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> y</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#6A737D\">  # Complex expression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">z.backward()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># x.grad and y.grad should match analytical derivatives</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Numerical gradient checking should pass with tolerance 1e-5</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Computation graph should be properly formed and traversed</span></span></code></pre></div>\n\n<h4 id=\"milestone-3-neural-network-modules-verification\">Milestone 3: Neural Network Modules Verification</h4>\n<p>Module system verification ensures our building blocks compose correctly and manage parameters properly for realistic neural networks.</p>\n<p><strong>Module System Tests:</strong></p>\n<p>| Test Category | Specific Tests | Expected Behavior | Failure Indicators |\n|---|---|---|\n| <strong>Parameter Registration</strong> | parameters() collects all trainable tensors | All parameters found recursively | Missing parameters |\n| <strong>Module Composition</strong> | Sequential chains modules correctly | Data flows through pipeline | Wrong data flow |\n| <strong>Forward Pass</strong> | Linear layers and activations work | Correct output shapes and values | Shape errors, wrong computations |\n| <strong>Training/Eval Mode</strong> | Behavior changes appropriately | Mode affects relevant modules | Mode switching ignored |\n| <strong>Parameter Initialization</strong> | Weights initialized reasonably | No NaN/inf values, appropriate scales | Poor initialization |</p>\n<p><strong>Verification Procedure for Milestone 3:</strong></p>\n<ol>\n<li><strong>Parameter Collection</strong>: Build nested module hierarchies and verify <code>parameters()</code> finds all trainable tensors</li>\n<li><strong>Forward Pass Testing</strong>: Create known input/output pairs for Linear layers and verify exact computation</li>\n<li><strong>Activation Function Verification</strong>: Test activation functions against analytical derivatives</li>\n<li><strong>Module Composition</strong>: Build <code>Sequential</code> containers and verify data flows correctly</li>\n<li><strong>Gradient Flow</strong>: Ensure gradients flow backward through entire module hierarchies</li>\n</ol>\n<p><strong>Neural Network Integration Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Network Architecture</th>\n<th>Purpose</th>\n<th>Key Verification Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Single Linear Layer</strong></td>\n<td>Basic parameter handling</td>\n<td>Weight shapes, bias handling, parameter collection</td>\n</tr>\n<tr>\n<td><strong>Multi-Layer Perceptron</strong></td>\n<td>Sequential composition</td>\n<td>Data flow, gradient flow, parameter counting</td>\n</tr>\n<tr>\n<td><strong>Branched Network</strong></td>\n<td>Complex module graphs</td>\n<td>Multiple paths, gradient accumulation</td>\n</tr>\n<tr>\n<td><strong>Nested Modules</strong></td>\n<td>Hierarchical organization</td>\n<td>Recursive parameter collection, naming</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 3 Checkpoint Criteria:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># After implementing Milestone 3, module system should work:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Sequential([</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Linear(</span><span style=\"color:#79B8FF\">784</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReLU(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Linear(</span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor(np.random.randn(</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">784</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model(x)  </span><span style=\"color:#6A737D\"># Forward pass works</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> y.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Correct output shape</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">(model.parameters())) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">  # Two weight + bias pairs</span></span></code></pre></div>\n\n<h4 id=\"milestone-4-optimizers-amp-training-verification\">Milestone 4: Optimizers &amp; Training Verification</h4>\n<p>Training system verification ensures the complete training loop produces learning behavior on simple, well-understood problems.</p>\n<p><strong>Optimizer Verification Tests:</strong></p>\n<p>| Test Category | Specific Tests | Expected Behavior | Failure Indicators |\n|---|---|---|\n| <strong>Parameter Updates</strong> | SGD and Adam update rules | Parameters change in gradient direction | No updates, wrong directions |\n| <strong>Learning Rate Effects</strong> | Different learning rates | Higher rates = bigger updates | Learning rate ignored |\n| <strong>Momentum Behavior</strong> | SGD with momentum | Velocity accumulation works | No momentum effect |\n| <strong>Adam Convergence</strong> | Adaptive learning rates | Faster convergence than SGD | No adaptive behavior |\n| <strong>Zero Grad</strong> | Gradient clearing | Previous gradients don&#39;t accumulate | Gradient accumulation errors |</p>\n<p><strong>Training Loop Integration Tests:</strong></p>\n<p>| Test Scenario | Purpose | Success Criteria | Failure Indicators |\n|---|---|---|\n| <strong>Overfitting Small Dataset</strong> | Basic training functionality | Loss decreases monotonically | Loss doesn&#39;t decrease |\n| <strong>Linear Regression</strong> | Known optimal solution | Converges to analytical solution | Wrong convergence |\n| <strong>XOR Problem</strong> | Nonlinear learning | Solves XOR with small MLP | Can&#39;t learn XOR |\n| <strong>Classification</strong> | Multi-class learning | Achieves reasonable accuracy | Random performance |</p>\n<p><strong>Milestone 4 Checkpoint Criteria:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># After implementing Milestone 4, training should work:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Sequential([Linear(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">), ReLU(), Linear(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SGD(model.parameters(), </span><span style=\"color:#FFAB70\">lr</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">loss_fn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MSELoss()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Training loop should reduce loss on simple regression problem</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> epoch </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    predictions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model(x_train)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loss_fn(predictions, y_train)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    optimizer.step()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Loss should generally decrease over time</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>Decision: Milestone Verification Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to ensure each milestone is solid before building on it</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Unit tests only for each component</li>\n<li>Integration tests verifying milestone completion</li>\n<li>Manual testing with example problems</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Combination of automated integration tests plus manual verification examples</li>\n<li><strong>Rationale</strong>: Automated tests catch regressions, manual examples build intuition and catch emergent issues</li>\n<li><strong>Consequences</strong>: More thorough verification but requires maintaining both test suites and examples</li>\n</ul>\n</blockquote>\n<h3 id=\"end-to-end-training-tests\">End-to-End Training Tests</h3>\n<p>End-to-end training tests verify that our entire framework integrates correctly by training complete neural networks on well-understood problems with known expected behaviors. Think of these tests as <strong>clinical trials</strong> for our neural network framework - controlled experiments that demonstrate the framework works for its intended purpose.</p>\n<p>Unlike unit tests that verify individual components or milestone tests that verify major subsystems, end-to-end tests verify <strong>emergent behavior</strong> that arises from the complete integration. These tests catch integration bugs that might not appear in isolated component testing but manifest when all systems work together during real training scenarios.</p>\n<p>The key principle is testing on <strong>toy problems with known solutions</strong>. We avoid complex datasets like ImageNet and instead use carefully constructed problems where we can predict expected behavior, convergence rates, and final accuracy. This allows us to distinguish between framework bugs and normal machine learning challenges.</p>\n<h4 id=\"toy-dataset-design-principles\">Toy Dataset Design Principles</h4>\n<p>Effective end-to-end testing requires carefully designed toy problems that exercise different aspects of the neural network training process:</p>\n<table>\n<thead>\n<tr>\n<th>Problem Type</th>\n<th>Mathematical Structure</th>\n<th>What It Tests</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Linear Regression</strong></td>\n<td>y = Wx + b with Gaussian noise</td>\n<td>Basic gradient flow, convex optimization</td>\n<td>Converges to analytical least-squares solution</td>\n</tr>\n<tr>\n<td><strong>Polynomial Fitting</strong></td>\n<td>y = x³ + 2x² - x + noise</td>\n<td>Nonlinear fitting, overfitting detection</td>\n<td>Can fit training data, generalizes reasonably</td>\n</tr>\n<tr>\n<td><strong>XOR Problem</strong></td>\n<td>Classic non-linearly separable classification</td>\n<td>Nonlinear learning, hidden layer necessity</td>\n<td>100% accuracy with small MLP</td>\n</tr>\n<tr>\n<td><strong>Spiral Dataset</strong></td>\n<td>Two-class spiral pattern</td>\n<td>Complex decision boundaries</td>\n<td>Smooth decision boundaries, good accuracy</td>\n</tr>\n<tr>\n<td><strong>MNIST Subset</strong></td>\n<td>10 examples per digit, 100 total</td>\n<td>Real data, multiclass classification</td>\n<td>High accuracy on small dataset</td>\n</tr>\n</tbody></table>\n<p><strong>Synthetic dataset generation</strong> ensures reproducible, controllable test conditions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Linear regression: y = 3*x + 2 + noise</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should converge to weights ≈ [3] and bias ≈ 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_linear </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y_linear </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> X_linear </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> np.random.randn(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># XOR problem: classic test of nonlinear learning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Perfect solution exists with small MLP</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_xor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y_xor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]])  </span><span style=\"color:#6A737D\"># XOR truth table</span></span></code></pre></div>\n\n<h4 id=\"training-convergence-verification\">Training Convergence Verification</h4>\n<p>End-to-end tests must verify that training exhibits expected convergence behavior rather than just checking final accuracy. This catches subtle bugs in optimization or gradient computation that might allow eventual convergence but with poor efficiency.</p>\n<p><strong>Convergence Pattern Analysis:</strong></p>\n<p>| Metric | Expected Pattern | Measurement Method | Failure Indicators |\n|---|---|---|\n| <strong>Loss Trajectory</strong> | Monotonic decrease (with noise) | Track loss every epoch | Loss increases, plateaus early |\n| <strong>Gradient Norms</strong> | Start large, decrease over time | L2 norm of parameter gradients | Constant gradients, exploding norms |\n| <strong>Parameter Changes</strong> | Large initially, smaller as converged | L2 norm of parameter updates | No parameter movement |\n| <strong>Learning Rate Sensitivity</strong> | Higher LR = faster initial progress | Compare different learning rates | No LR effect |\n| <strong>Convergence Speed</strong> | Reach target accuracy in expected epochs | Epochs to achieve threshold accuracy | Too slow or too fast convergence |</p>\n<p><strong>Statistical Validation</strong> ensures training behavior is consistent across multiple runs:</p>\n<ul>\n<li><strong>Multiple Random Seeds</strong>: Run same experiment with 5+ different random seeds to verify consistent convergence</li>\n<li><strong>Convergence Statistics</strong>: Measure mean and variance of final loss/accuracy across runs</li>\n<li><strong>Outlier Detection</strong>: Flag runs that converge unusually slowly or to poor solutions</li>\n<li><strong>Learning Curve Analysis</strong>: Verify learning curves have expected shape (rapid initial improvement, then saturation)</li>\n</ul>\n<h4 id=\"framework-integration-testing\">Framework Integration Testing</h4>\n<p>End-to-end tests specifically target integration points where different framework components must coordinate correctly:</p>\n<p><strong>Gradient Flow Integration:</strong></p>\n<ul>\n<li>Verify gradients flow correctly from loss function through entire network back to input layers</li>\n<li>Test gradient accumulation when same parameter appears in multiple computational paths</li>\n<li>Confirm gradient magnitudes are reasonable (not vanishing or exploding) throughout network</li>\n</ul>\n<p><strong>Memory Management Integration:</strong></p>\n<ul>\n<li>Ensure computation graphs are properly cleaned up after backward pass</li>\n<li>Verify no memory leaks during extended training loops</li>\n<li>Test behavior when training very deep networks that create large computation graphs</li>\n</ul>\n<p><strong>Optimizer Integration:</strong></p>\n<ul>\n<li>Confirm optimizers correctly access and update parameters from complex module hierarchies</li>\n<li>Verify learning rate scheduling affects actual parameter updates</li>\n<li>Test momentum and adaptive learning rate features work correctly during real training</li>\n</ul>\n<p><strong>Training Loop Coordination:</strong></p>\n<ul>\n<li>Verify forward pass, loss computation, backward pass, and parameter update sequence works correctly</li>\n<li>Test batch processing with different batch sizes</li>\n<li>Confirm training/evaluation mode switching affects network behavior appropriately</li>\n</ul>\n<h4 id=\"specific-end-to-end-test-cases\">Specific End-to-End Test Cases</h4>\n<p><strong>Linear Regression Convergence Test:</strong></p>\n<p>This test verifies basic gradient flow and convex optimization:</p>\n<ol>\n<li><strong>Problem Setup</strong>: Generate y = 3x + 2 + noise with known ground truth</li>\n<li><strong>Model</strong>: Single Linear layer (input=1, output=1)</li>\n<li><strong>Expected Behavior</strong>: Should converge to weight ≈ 3, bias ≈ 2 within 100 epochs</li>\n<li><strong>Verification</strong>: Final parameters within 10% of ground truth, loss &lt; 0.01</li>\n<li><strong>Failure Detection</strong>: Parameters don&#39;t converge, loss plateaus too high, training diverges</li>\n</ol>\n<p><strong>XOR Classification Test:</strong></p>\n<p>This test verifies nonlinear learning capability:</p>\n<ol>\n<li><strong>Problem Setup</strong>: 4-point XOR dataset with perfect separability</li>\n<li><strong>Model</strong>: MLP with hidden layer (2→4→1 with ReLU)</li>\n<li><strong>Expected Behavior</strong>: 100% training accuracy within 1000 epochs</li>\n<li><strong>Verification</strong>: All 4 XOR points classified correctly</li>\n<li><strong>Failure Detection</strong>: Can&#39;t achieve perfect accuracy, training stalls</li>\n</ol>\n<p><strong>Overfitting Detection Test:</strong></p>\n<p>This test verifies the framework can overfit (confirming it can learn):</p>\n<ol>\n<li><strong>Problem Setup</strong>: Complex function with small training set (10 points)</li>\n<li><strong>Model</strong>: Overparameterized MLP (much larger than needed)</li>\n<li><strong>Expected Behavior</strong>: Perfect training accuracy, poor validation accuracy</li>\n<li><strong>Verification</strong>: Training loss → 0, validation loss increases</li>\n<li><strong>Failure Detection</strong>: Can&#39;t fit training data, no overfitting observed</li>\n</ol>\n<p><strong>Multi-Epoch Training Stability:</strong></p>\n<p>This test verifies long-term training stability:</p>\n<ol>\n<li><strong>Problem Setup</strong>: Medium-sized dataset requiring extended training</li>\n<li><strong>Model</strong>: Standard MLP architecture</li>\n<li><strong>Expected Behavior</strong>: Stable training for 1000+ epochs without crashes</li>\n<li><strong>Verification</strong>: No memory leaks, consistent convergence, no NaN values</li>\n<li><strong>Failure Detection</strong>: Memory usage grows, training becomes unstable, numerical issues</li>\n</ol>\n<h4 id=\"automated-test-infrastructure\">Automated Test Infrastructure</h4>\n<p>End-to-end tests require infrastructure to run automatically and report meaningful failures:</p>\n<p><strong>Test Execution Framework:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Dataset Generation</strong></td>\n<td>Create reproducible toy datasets</td>\n<td>Fixed random seeds, parameterized problem generators</td>\n</tr>\n<tr>\n<td><strong>Training Orchestration</strong></td>\n<td>Run complete training loops</td>\n<td>Configurable epochs, learning rates, architectures</td>\n</tr>\n<tr>\n<td><strong>Metrics Collection</strong></td>\n<td>Track convergence behavior</td>\n<td>Loss history, gradient norms, parameter evolution</td>\n</tr>\n<tr>\n<td><strong>Statistical Analysis</strong></td>\n<td>Verify expected behavior patterns</td>\n<td>Hypothesis tests for convergence, outlier detection</td>\n</tr>\n<tr>\n<td><strong>Failure Diagnosis</strong></td>\n<td>Identify root cause of failures</td>\n<td>Detailed logging, intermediate state inspection</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Benchmarking:</strong></p>\n<p>While not focused on production performance, end-to-end tests should verify reasonable performance characteristics:</p>\n<ul>\n<li><strong>Training Speed</strong>: Verify training completes in reasonable time (not 100x slower than PyTorch)</li>\n<li><strong>Memory Usage</strong>: Ensure memory usage scales appropriately with model size</li>\n<li><strong>Convergence Efficiency</strong>: Compare epochs-to-convergence with reference implementations</li>\n</ul>\n<blockquote>\n<p><strong>Decision: End-to-End Test Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Need comprehensive integration testing without excessive test suite complexity</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Single complex dataset (like MNIST)</li>\n<li>Multiple toy problems testing different aspects</li>\n<li>Random problem generation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Curated set of toy problems with known expected behaviors</li>\n<li><strong>Rationale</strong>: Toy problems allow precise verification of expected behavior while testing different framework aspects</li>\n<li><strong>Consequences</strong>: More test maintenance but much clearer failure diagnosis and root cause identification</li>\n</ul>\n</blockquote>\n<p><strong>Common End-to-End Testing Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Using Complex Datasets Too Early</strong>\nMany learners jump to MNIST or CIFAR-10 for end-to-end testing. This makes it impossible to distinguish between framework bugs and normal machine learning challenges. A framework bug that causes 10% accuracy loss might be masked by dataset difficulty, making debugging extremely difficult.</p>\n<p>⚠️ <strong>Pitfall: Not Testing Convergence Patterns</strong>\nTesting only final accuracy misses important bugs in optimization or gradient flow. A bug that makes training 10x slower but eventually converges might go unnoticed, leading to poor user experience.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Statistical Variation</strong>\nNeural network training has inherent randomness. Testing with a single random seed can miss intermittent failures or give false confidence. Always test with multiple seeds and analyze statistical patterns.</p>\n<p>⚠️ <strong>Pitfall: No Baseline Comparison</strong>\nWithout comparing against known-good implementations (like PyTorch), it&#39;s hard to know if poor performance indicates bugs or is simply expected. Implement the same problem in PyTorch as a reference.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical infrastructure for implementing comprehensive testing of your neural network framework. The testing code is as important as the framework code itself - mathematical correctness is non-negotiable.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Testing Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Test Framework</strong></td>\n<td>Python unittest (built-in)</td>\n<td>pytest with fixtures and parameterization</td>\n</tr>\n<tr>\n<td><strong>Numerical Computing</strong></td>\n<td>NumPy for reference implementations</td>\n<td>SciPy for advanced numerical methods</td>\n</tr>\n<tr>\n<td><strong>Gradient Checking</strong></td>\n<td>Custom finite difference implementation</td>\n<td>AutoGrad for reference gradients</td>\n</tr>\n<tr>\n<td><strong>Statistical Testing</strong></td>\n<td>Manual mean/std analysis</td>\n<td>scipy.stats for hypothesis testing</td>\n</tr>\n<tr>\n<td><strong>Visualization</strong></td>\n<td>matplotlib for loss curves</td>\n<td>tensorboard-style logging</td>\n</tr>\n<tr>\n<td><strong>Performance Testing</strong></td>\n<td>time.time() for basic timing</td>\n<td>cProfile for detailed performance analysis</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-testing-structure\">Recommended Testing Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n  tests/\n    test_gradients/\n      test_gradient_checking.py     ← numerical differentiation vs autodiff\n      test_operation_gradients.py   ← individual operation gradient tests\n      test_complex_expressions.py   ← composed function gradient tests\n    test_milestones/\n      test_milestone1_tensors.py    ← tensor operations verification\n      test_milestone2_autodiff.py   ← automatic differentiation verification\n      test_milestone3_modules.py    ← neural network modules verification\n      test_milestone4_training.py   ← optimizers and training verification\n    test_integration/\n      test_end_to_end.py           ← complete training on toy problems\n      toy_datasets.py              ← synthetic dataset generation\n      reference_solutions.py       ← known correct solutions for comparison\n    conftest.py                    ← pytest fixtures and test utilities\n    test_utils.py                  ← testing helper functions</code></pre></div>\n\n<h4 id=\"core-testing-infrastructure-complete-implementation\">Core Testing Infrastructure (Complete Implementation)</h4>\n<p><strong>Gradient Checking Utilities:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> numerical_gradient</span><span style=\"color:#E1E4E8\">(f: Callable, inputs: List[np.ndarray], h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> List[np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute numerical gradients using central difference approximation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        f: Function that takes list of arrays, returns scalar</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        inputs: List of input arrays to differentiate w.r.t</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        h: Step size for finite differences</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of gradient arrays, same shapes as inputs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradients </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, x </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(inputs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros_like(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Flatten for easier iteration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        x_flat </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_flat </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> j </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(x_flat)):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Create perturbed versions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            x_plus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x_flat.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            x_minus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x_flat.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            x_plus[j] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            x_minus[j] </span><span style=\"color:#F97583\">-=</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Reconstruct input lists with perturbations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            inputs_plus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inputs.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            inputs_minus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inputs.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            inputs_plus[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x_plus.reshape(x.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            inputs_minus[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x_minus.reshape(x.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Central difference approximation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_plus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(inputs_plus)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f_minus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(inputs_minus)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad_flat[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (f_plus </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> f_minus) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradients.append(grad.reshape(x.shape))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradients</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> relative_error</span><span style=\"color:#E1E4E8\">(a: np.ndarray, b: np.ndarray, eps: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-8</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compute relative error between two arrays.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numerator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.abs(a </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> b)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    denominator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.maximum(np.abs(a), np.abs(b), eps)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> np.max(numerator </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> denominator)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    f: Callable, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inputs: List[np.ndarray], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    autodiff_grads: List[np.ndarray],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compare autodiff gradients against numerical gradients.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        (passed, error_message): Test result and diagnostic info</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        numerical_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> numerical_gradient(f, inputs, h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(numerical_grads) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(autodiff_grads):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient count mismatch: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(numerical_grads)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(autodiff_grads)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, (num_grad, auto_grad) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(numerical_grads, autodiff_grads)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> num_grad.shape </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> auto_grad.shape:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> shape mismatch: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">num_grad.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">auto_grad.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            rel_err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> relative_error(num_grad, auto_grad)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> rel_err </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> tolerance:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"Gradient </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> relative error </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">rel_err</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> > tolerance </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tolerance</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"Numerical: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">num_grad.flatten()[:</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"Autodiff:  </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">auto_grad.flatten()[:</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Gradient check passed\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient checking failed with exception: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientTester</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper class for systematic gradient testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.test_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, test_function: Callable) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test gradients for a specific operation with multiple random inputs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        passed_tests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_tests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">  # Test with 5 random inputs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> test_idx </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(total_tests):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Generate random test inputs (implementation specific)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                inputs, expected_output, autodiff_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> test_function()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Define scalar function for gradient checking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                def</span><span style=\"color:#B392F0\"> scalar_func</span><span style=\"color:#E1E4E8\">(input_list):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # This would call your framework's forward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # and return a scalar (e.g., sum of output)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> your_framework_forward(input_list)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> np.sum(result)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                passed, message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_gradients(scalar_func, inputs, autodiff_grads, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tolerance)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> passed:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    passed_tests </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"FAIL </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> test </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test_idx</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"ERROR </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> test </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test_idx</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> passed_tests </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> total_tests</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.test_results.append((operation_name, success_rate))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> success_rate </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#6A737D\">  # Require 80% success rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> summary_report</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate summary of all gradient tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        report </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Gradient Test Summary:</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#F97583\"> +</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> op_name, success_rate </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.test_results:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            status </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"PASS\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> success_rate </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#F97583\"> else</span><span style=\"color:#9ECBFF\"> \"FAIL\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            report </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">status</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">op_name</span><span style=\"color:#F97583\">:20</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">success_rate</span><span style=\"color:#F97583\">:.1%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> success rate)</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        overall_success </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(rate </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _, rate </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.test_results) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.test_results)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        report </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Overall Success Rate: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">overall_success</span><span style=\"color:#F97583\">:.1%</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> report</span></span></code></pre></div>\n\n<p><strong>Toy Dataset Generation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ToyDatasets</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate synthetic datasets for end-to-end testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> linear_regression</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">, noise: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">, seed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 42</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate linear regression problem: y = 3*x + 2 + noise.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(seed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(n_samples, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3.0</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> X </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 2.0</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> noise </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.random.randn(n_samples, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> X, y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> polynomial_regression</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">, noise: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">, seed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 42</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate polynomial regression: y = x^3 + 2*x^2 - x + noise.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(seed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.uniform(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, (n_samples, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> X</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">X</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> X </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.random.randn(n_samples, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> X, y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> xor_problem</span><span style=\"color:#E1E4E8\">() -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate XOR dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">np.float32)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]], </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">np.float32)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> X, y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> spiral_dataset</span><span style=\"color:#E1E4E8\">(n_points: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">, noise: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">, seed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 42</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate two-class spiral dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(seed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        N </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> n_points </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Generate spiral arms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        theta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linspace(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">np.pi, N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linspace(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Class 0 spiral</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        x0 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.cos(theta) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.random.randn(N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y0 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.sin(theta) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.random.randn(N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Class 1 spiral (rotated)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        x1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.cos(theta </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> np.pi) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.random.randn(N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.sin(theta </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> np.pi) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> np.random.randn(N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Combine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.column_stack([</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            np.concatenate([x0, x1]),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            np.concatenate([y0, y1])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.concatenate([np.zeros(N), np.ones(N)]).reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> X, y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> classification_grid</span><span style=\"color:#E1E4E8\">(n_per_class: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#E1E4E8\">, n_classes: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">, seed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 42</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate multi-class classification with Gaussian clusters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(seed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Place class centers in a grid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        centers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [(i, j) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(n_classes) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> j </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(n_classes)][:n_classes]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> class_idx, (cx, cy) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(centers):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Generate points around center</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            X_class </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(n_per_class, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> np.array([cx, cy])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            y_class </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.full((n_per_class,), class_idx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            X_list.append(X_class)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            y_list.append(y_class)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.vstack(X_list)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.concatenate(y_list).reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> X, y</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> expected_linear_regression_solution</span><span style=\"color:#E1E4E8\">(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compute analytical least squares solution for linear regression.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Add bias column to X</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    X_with_bias </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.column_stack([X, np.ones(X.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Analytical solution: θ = (X^T X)^-1 X^T y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    theta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linalg.solve(X_with_bias.T </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> X_with_bias, X_with_bias.T </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> y)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    weight </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> theta[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bias </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> theta[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> weight, bias</span></span></code></pre></div>\n\n<p><strong>End-to-End Test Framework:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Results from training run for analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    final_loss: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    final_accuracy: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss_history: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    convergence_epoch: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Epoch when target accuracy reached</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    training_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EndToEndTester</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Framework for end-to-end integration testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tolerance: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'loss'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'accuracy'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'convergence_epochs'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_linear_regression_convergence</span><span style=\"color:#E1E4E8\">(self, framework_classes) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test linear regression convergence to analytical solution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate linear regression dataset using ToyDatasets.linear_regression()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create single Linear layer model using your framework</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create MSE loss function and SGD optimizer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Train for 500 epochs, collecting loss history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compare final parameters to analytical solution within 10% tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Verify loss decreased monotonically (allowing some noise)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return True if all checks pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_xor_classification</span><span style=\"color:#E1E4E8\">(self, framework_classes) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test XOR problem solving with small MLP.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get XOR dataset using ToyDatasets.xor_problem()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create MLP model: Linear(2, 4) -> ReLU -> Linear(4, 1) -> Sigmoid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create binary cross-entropy loss and Adam optimizer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Train until perfect accuracy or 2000 epochs max</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify all 4 XOR points classified correctly (>0.5 threshold)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return True if perfect classification achieved</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_overfitting_capability</span><span style=\"color:#E1E4E8\">(self, framework_classes) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test framework can overfit small dataset (confirms learning works).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate small polynomial dataset (10 points) with complex function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create overparameterized model (much larger than needed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Train until training loss approaches zero</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify training accuracy reaches 100% while validation accuracy is poor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: This confirms the framework can learn (overfitting is expected)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_training_stability</span><span style=\"color:#E1E4E8\">(self, framework_classes) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test extended training without crashes or instabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create medium-size dataset and model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Train for 2000+ epochs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Monitor for NaN/inf values in loss or parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check memory usage doesn't grow unboundedly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify training remains stable throughout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_statistical_validation</span><span style=\"color:#E1E4E8\">(self, test_function: Callable, n_runs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run test multiple times with different seeds for statistical validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> seed </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(n_runs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            np.random.seed(seed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Set your framework's random seed here too</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> test_function()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            results.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'passed'</span><span style=\"color:#E1E4E8\">: result,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'time'</span><span style=\"color:#E1E4E8\">: end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'seed'</span><span style=\"color:#E1E4E8\">: seed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Analyze results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> r[</span><span style=\"color:#9ECBFF\">'passed'</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(results)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.mean([r[</span><span style=\"color:#9ECBFF\">'time'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'success_rate'</span><span style=\"color:#E1E4E8\">: success_rate,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'average_time'</span><span style=\"color:#E1E4E8\">: avg_time,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'individual_results'</span><span style=\"color:#E1E4E8\">: results,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'passed_overall'</span><span style=\"color:#E1E4E8\">: success_rate </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#6A737D\">  # Require 80% success</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint Templates:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MilestoneCheckpoints</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Automated verification for each milestone completion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_milestone_1</span><span style=\"color:#E1E4E8\">(tensor_class, operation_classes):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify tensor operations work correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Verifying Milestone 1: Tensor &#x26; Operations\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test tensor creation with various shapes and dtypes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify arithmetic operations match NumPy exactly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test broadcasting follows NumPy rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify matrix multiplication works for 2D and batched</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check requires_grad and grad_fn attributes set correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Test error handling for incompatible operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: All tensor operations work identically to NumPy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Gradient tracking metadata is properly maintained</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Broadcasting and shape errors provide clear messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_milestone_2</span><span style=\"color:#E1E4E8\">(tensor_class, operation_classes):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify automatic differentiation produces correct gradients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Verifying Milestone 2: Automatic Differentiation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test gradients of simple operations (x^2, x+y, x*y)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify chain rule works for composed functions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test gradient accumulation for expressions like x+x</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check topological sort produces correct ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Run comprehensive gradient checking on all operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Test backward pass on complex computation graphs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: All gradients pass numerical differentiation comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Complex expressions compute gradients correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Gradient accumulation works for shared tensors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_milestone_3</span><span style=\"color:#E1E4E8\">(module_classes):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify neural network module system works.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Verifying Milestone 3: Neural Network Modules\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test parameter registration and collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify Linear layer forward pass and gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test activation functions and their derivatives</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check Sequential module chains operations correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify training/eval mode switching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Test nested module parameter collection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: All modules compose correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Parameters are tracked and accessible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Forward and backward passes work through module hierarchies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_milestone_4</span><span style=\"color:#E1E4E8\">(optimizer_classes, loss_classes):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify optimizers and training loop work.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Verifying Milestone 4: Optimizers &#x26; Training\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test SGD parameter updates follow gradient descent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify Adam optimizer convergence behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test learning rate effects on convergence speed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check gradient clearing between training steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify loss functions compute correct gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Test complete training loop integration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Optimizers update parameters correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Training loop reduces loss on simple problems</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Expected: Different optimizers show expected convergence patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage example for learners:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # After implementing each milestone, run verification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    from</span><span style=\"color:#E1E4E8\"> your_framework </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tensor, Linear, Sequential, ReLU, </span><span style=\"color:#79B8FF\">SGD</span><span style=\"color:#E1E4E8\">, Adam</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify milestone completion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MilestoneCheckpoints.verify_milestone_1(Tensor, [Add, Multiply, MatMul])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MilestoneCheckpoints.verify_milestone_2(Tensor, [Add, Multiply, MatMul])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MilestoneCheckpoints.verify_milestone_3([Linear, ReLU, Sequential])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MilestoneCheckpoints.verify_milestone_4([</span><span style=\"color:#79B8FF\">SGD</span><span style=\"color:#E1E4E8\">, Adam], [MSELoss])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Run end-to-end tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tester </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> EndToEndTester()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    framework_classes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'Tensor'</span><span style=\"color:#E1E4E8\">: Tensor, </span><span style=\"color:#9ECBFF\">'Linear'</span><span style=\"color:#E1E4E8\">: Linear, </span><span style=\"color:#9ECBFF\">'SGD'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">SGD</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Running end-to-end integration tests...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    linear_test </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tester.test_linear_regression_convergence(framework_classes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    xor_test </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tester.test_xor_classification(framework_classes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Linear regression test: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">'PASS'</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> linear_test </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'FAIL'</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"XOR classification test: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">'PASS'</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> xor_test </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'FAIL'</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p>This testing infrastructure provides comprehensive verification of your neural network framework from individual operations through complete training scenarios. The gradient checking utilities are particularly crucial - they provide mathematical ground truth that&#39;s impossible to achieve through manual inspection.</p>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - debugging is essential throughout tensor operations, automatic differentiation, neural modules, and training loops to ensure correctness and identify implementation problems</p>\n</blockquote>\n<p>Building a neural network framework from scratch introduces numerous opportunities for subtle bugs that can be challenging to diagnose. Unlike high-level application bugs that often have obvious symptoms, framework-level bugs frequently manifest as mysterious numerical behavior, silent gradient computation failures, or training that simply doesn&#39;t converge as expected. This section provides systematic approaches to identify, diagnose, and fix the most common categories of bugs that learners encounter when implementing their own automatic differentiation system.</p>\n<p>Think of debugging a neural network framework like being a detective investigating a crime scene where the evidence is often numerical rather than visual. The &quot;crime&quot; might be gradients that vanish mysteriously, shapes that don&#39;t align properly, or training loops that appear to run successfully but never actually learn. Unlike debugging a web application where you can inspect HTTP requests and database queries, framework debugging requires understanding the mathematical relationships between tensors, the structure of computation graphs, and the flow of gradients through complex networks.</p>\n<p>The debugging process for neural network frameworks follows a layered approach that mirrors the four-layer architecture described earlier. Tensor operation bugs typically manifest as shape mismatches or incorrect numerical results that can be caught with targeted unit tests. Automatic differentiation bugs are more insidious - they often allow forward passes to succeed while silently corrupting gradient computation, leading to training that appears to run but doesn&#39;t converge. Neural module bugs frequently involve parameter registration failures or incorrect weight initialization that prevents effective learning. Training loop bugs encompass optimizer configuration issues, learning rate problems, and loss function mistakes that can cause oscillating behavior or premature convergence to poor solutions.</p>\n<h3 id=\"gradient-computation-issues\">Gradient Computation Issues</h3>\n<p>Gradient computation problems represent the most challenging category of bugs in neural network frameworks because they often produce no immediate error messages while completely undermining the learning process. These bugs can be particularly frustrating because the forward pass typically works perfectly, producing reasonable predictions, while the backward pass silently computes incorrect gradients that prevent the network from improving during training.</p>\n<p>The most fundamental tool for diagnosing gradient computation issues is <strong>gradient validation</strong> using numerical differentiation. This technique compares the gradients computed by your automatic differentiation implementation against gradients calculated using the mathematical definition of derivatives with finite differences. The central difference method provides a reliable reference implementation that can expose even subtle gradient computation errors.</p>\n<p><strong>Gradient Validation Implementation:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Validation Component</th>\n<th>Purpose</th>\n<th>Key Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>numerical_gradient(f, inputs, h)</code></td>\n<td>Compute reference gradients using finite differences</td>\n<td>Step size <code>h</code> must balance numerical precision vs truncation error</td>\n</tr>\n<tr>\n<td><code>check_gradients(f, inputs, tolerance)</code></td>\n<td>Compare autodiff vs numerical gradients</td>\n<td>Relative error accounts for gradient magnitude differences</td>\n</tr>\n<tr>\n<td><code>relative_error(a, b, eps)</code></td>\n<td>Scale-invariant error metric</td>\n<td>Handles both small and large gradient magnitudes appropriately</td>\n</tr>\n</tbody></table>\n<p>The gradient validation process involves creating a scalar function from your computation that maps input tensors to a single output value, then comparing your framework&#39;s computed gradients against numerical approximations. For a function f(x), the central difference approximation f&#39;(x) ≈ [f(x+h) - f(x-h)] / (2h) provides higher accuracy than forward differences and helps identify gradient computation errors with high reliability.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Gradient validation should be performed on isolated operations first, then progressively on more complex computation graphs. A bug in basic tensor addition will corrupt gradients in every neural network layer, so establish correctness at the foundation before building higher-level components.</p>\n</blockquote>\n<p><strong>Common Gradient Flow Diagnostics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Diagnostic Technique</th>\n<th>What It Reveals</th>\n<th>When To Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradient magnitude inspection</td>\n<td>Vanishing or exploding gradient problems</td>\n<td>After backward pass, before optimizer step</td>\n</tr>\n<tr>\n<td>Computation graph visualization</td>\n<td>Missing connections or incorrect topology</td>\n<td>When gradients are None or unexpectedly zero</td>\n</tr>\n<tr>\n<td>Parameter-specific gradient checking</td>\n<td>Which layers are receiving gradients correctly</td>\n<td>During training loop debugging</td>\n</tr>\n<tr>\n<td>Gradient accumulation verification</td>\n<td>Whether multiple uses of tensors sum gradients</td>\n<td>For networks with skip connections or parameter sharing</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Gradient Not Accumulated</strong></p>\n<p>One of the most common automatic differentiation bugs occurs when a tensor is used multiple times in a computation graph, but gradients from different paths are not properly accumulated. For example, if parameter W is used in both the main computation path and a regularization term, the gradient with respect to W should be the sum of gradients from both uses. Failing to accumulate these gradients results in the optimizer only seeing partial gradient information, leading to incorrect parameter updates.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code># Example scenario where this occurs:\nmain_output = model.linear(x)  # Uses W\nl2_penalty = torch.sum(model.linear.weight ** 2)  # Uses W again\ntotal_loss = mse_loss(main_output, target) + 0.01 * l2_penalty\n# W.grad should contain gradients from both paths</code></pre></div>\n\n<p>The bug typically manifests as training that converges more slowly than expected or reaches suboptimal solutions. To diagnose this issue, check whether tensors used multiple times in your computation have gradient magnitudes that seem too small compared to numerical differentiation results.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Topological Ordering</strong></p>\n<p>The backward pass must traverse the computation graph in reverse topological order to ensure that when a node computes its input gradients, all of its output gradients have already been computed and accumulated. Incorrect ordering leads to some gradients being computed before their dependencies are ready, resulting in partial or completely incorrect gradient information.</p>\n<p>This bug often occurs when implementing the <code>topological_sort</code> function for backward pass traversal. The symptoms include gradients that are sometimes correct and sometimes wrong (depending on the randomness in graph traversal), or gradients that are systematically too small because only some paths through the computation graph are correctly handled.</p>\n<p><strong>Gradient Computation Verification Steps:</strong></p>\n<ol>\n<li><p><strong>Isolate the Operation</strong>: Test gradient computation for individual operations (addition, multiplication, matrix multiplication) in isolation before combining them into complex networks.</p>\n</li>\n<li><p><strong>Verify Graph Construction</strong>: Ensure that every operation correctly sets the <code>grad_fn</code> attribute on its output tensor and maintains references to its input tensors.</p>\n</li>\n<li><p><strong>Check Topological Sort</strong>: Manually trace through the backward pass order for a simple computation graph to ensure dependencies are respected.</p>\n</li>\n<li><p><strong>Validate Gradient Shapes</strong>: Confirm that computed gradients have the same shape as their corresponding tensors and that broadcasting operations properly reduce gradients back to parameter shapes.</p>\n</li>\n<li><p><strong>Test Gradient Accumulation</strong>: Create test cases where the same parameter is used multiple times and verify that gradients are summed correctly.</p>\n</li>\n<li><p><strong>Monitor for Numerical Issues</strong>: Watch for NaN or infinity values in gradients, which often indicate numerical instability in gradient computation rather than algorithmic errors.</p>\n</li>\n</ol>\n<p><strong>Gradient Debugging Workflow:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Action</th>\n<th>Expected Outcome</th>\n<th>If Different</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>Run <code>numerical_gradient</code> on simple operation</td>\n<td>Reference gradients computed successfully</td>\n<td>Check numerical differentiation implementation</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Run automatic differentiation on same operation</td>\n<td>Gradients match reference within tolerance</td>\n<td>Debug specific operation&#39;s backward method</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Test operation in isolation with various input shapes</td>\n<td>All shape combinations work correctly</td>\n<td>Debug broadcasting and shape handling</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Combine operations into small computation graph</td>\n<td>Gradients still match numerical reference</td>\n<td>Debug graph construction and traversal</td>\n</tr>\n<tr>\n<td>5</td>\n<td>Scale up to full neural network</td>\n<td>Training converges as expected</td>\n<td>Debug parameter registration and optimizer integration</td>\n</tr>\n</tbody></table>\n<h3 id=\"shape-and-broadcasting-bugs\">Shape and Broadcasting Bugs</h3>\n<p>Shape-related bugs in tensor operations represent some of the most immediately visible failures in neural network frameworks, but they can also be among the most confusing to debug when broadcasting rules are involved. Unlike gradient computation bugs that fail silently, shape bugs typically produce immediate exceptions, but the error messages often don&#39;t clearly explain why the shapes are incompatible or how to fix them.</p>\n<p>Broadcasting is particularly problematic because it involves implicit shape transformations that can mask underlying architecture problems. A shape mismatch that would clearly indicate a design error without broadcasting might instead trigger a broadcasting operation that produces a result with unexpected dimensions, leading to subtle bugs later in the computation pipeline.</p>\n<p><strong>Shape Error Diagnostic Framework:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Typical Symptoms</th>\n<th>Root Cause Categories</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ShapeError</code></td>\n<td>Immediate failure during operation</td>\n<td>Fundamental incompatibility between tensor dimensions</td>\n</tr>\n<tr>\n<td><code>BroadcastingError</code></td>\n<td>Unexpected output shapes from operations</td>\n<td>Broadcasting rules applied when explicit reshaping was needed</td>\n</tr>\n<tr>\n<td>Gradient shape mismatch</td>\n<td>Backward pass fails with shape errors</td>\n<td>Parameter gradients don&#39;t match parameter shapes after broadcasting</td>\n</tr>\n<tr>\n<td>Silent broadcasting</td>\n<td>Operations succeed but produce wrong results</td>\n<td>Broadcasting masks architecture errors</td>\n</tr>\n</tbody></table>\n<p>The most effective approach to debugging shape issues involves creating a systematic tracing mechanism that logs tensor shapes at every operation, combined with broadcasting simulation that shows exactly how shapes would be transformed. This visibility is crucial because shape transformations in neural networks often involve multiple sequential operations where an error in early stages compounds into confusing failures later.</p>\n<p><strong>Broadcasting Debugging Strategy:</strong></p>\n<p>Broadcasting follows specific rules that can be simulated step-by-step to understand why operations fail or produce unexpected results. The NumPy broadcasting rules that most frameworks follow involve aligning shapes from the rightmost dimension and expanding dimensions of size 1 or adding new dimensions as needed.</p>\n<table>\n<thead>\n<tr>\n<th>Broadcasting Step</th>\n<th>Rule</th>\n<th>Example</th>\n<th>Potential Issues</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Right-align shapes</td>\n<td>Compare dimensions from right to left</td>\n<td>(3, 1, 4) vs (2, 4)</td>\n<td>Different number of dimensions</td>\n</tr>\n<tr>\n<td>Check compatibility</td>\n<td>Each dimension must be equal or one must be 1</td>\n<td>4 == 4 ✓, 3 vs 2 ✗</td>\n<td>Incompatible dimension sizes</td>\n</tr>\n<tr>\n<td>Expand dimensions</td>\n<td>Dimensions of 1 broadcast to match larger dimension</td>\n<td>(3, 1, 4) → (3, 2, 4)</td>\n<td>Memory implications for large expansions</td>\n</tr>\n<tr>\n<td>Add leading dimensions</td>\n<td>Shorter shape gets leading 1s</td>\n<td>(2, 4) → (1, 2, 4)</td>\n<td>Changes tensor interpretation</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Broadcasting Gradient Shapes</strong></p>\n<p>A particularly subtle category of shape bugs occurs during the backward pass when gradients computed for broadcasted operations need to be reduced back to the original parameter shapes. If a parameter with shape (10,) is broadcasted to (5, 10) during forward computation, the gradient flowing back will have shape (5, 10) but must be summed along the first dimension to produce a gradient with shape (10,) that matches the parameter.</p>\n<p>This bug often manifests as shape mismatches during optimizer updates, where the optimizer expects parameter gradients to have the same shape as the parameters themselves. The error message typically indicates that gradients have too many dimensions or are larger than expected.</p>\n<p><strong>Shape Tracing Implementation:</strong></p>\n<p>Creating a comprehensive shape debugging system requires instrumentation at multiple levels of the tensor operation stack. The most effective approach combines automatic shape logging with manual inspection utilities that can trace shape transformations through complex computation graphs.</p>\n<table>\n<thead>\n<tr>\n<th>Debugging Tool</th>\n<th>Purpose</th>\n<th>Implementation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>validate_broadcasting_safety</code></td>\n<td>Check if tensors can be broadcasted before attempting operation</td>\n<td>Pre-flight validation prevents confusing error messages</td>\n</tr>\n<tr>\n<td><code>analyze_broadcasting_failure</code></td>\n<td>Diagnose why broadcasting failed with detailed step analysis</td>\n<td>Post-failure analysis for debugging</td>\n</tr>\n<tr>\n<td><code>create_shape_error_message</code></td>\n<td>Generate helpful error messages with expected vs actual shapes</td>\n<td>User experience improvement</td>\n</tr>\n<tr>\n<td><code>trace_shape_transformations</code></td>\n<td>Log shape changes through computation graph</td>\n<td>Debugging tool for complex networks</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: In-Place Operations Breaking Broadcasting</strong></p>\n<p>In-place tensor operations can create particularly confusing shape-related bugs because they modify tensor shapes during computation in ways that break subsequent operations. If an in-place operation changes a tensor&#39;s shape, other tensors that reference it may suddenly become incompatible with operations that previously worked correctly.</p>\n<p>This issue is especially problematic in neural network frameworks because parameters are often reused across multiple forward passes, and in-place modifications during one forward pass can break subsequent passes. The symptoms typically include operations that work correctly the first time but fail on subsequent iterations with shape mismatch errors.</p>\n<p><strong>Shape Debugging Workflow:</strong></p>\n<ol>\n<li><p><strong>Enable Shape Logging</strong>: Instrument all tensor operations to log input and output shapes, making it possible to trace exactly where shape mismatches occur.</p>\n</li>\n<li><p><strong>Simulate Broadcasting</strong>: Before performing operations, simulate the broadcasting rules to predict output shapes and identify potential issues.</p>\n</li>\n<li><p><strong>Validate Gradient Shapes</strong>: After backward passes, verify that all parameter gradients have shapes matching their corresponding parameters.</p>\n</li>\n<li><p><strong>Test Edge Cases</strong>: Create unit tests for broadcasting with tensors of very different shapes, including scalar broadcasting and high-dimensional tensor combinations.</p>\n</li>\n<li><p><strong>Check Parameter Registration</strong>: Ensure that neural network modules correctly register parameters with their intended shapes and that these shapes remain consistent throughout training.</p>\n</li>\n</ol>\n<h3 id=\"training-loop-problems\">Training Loop Problems</h3>\n<p>Training loop debugging represents the most complex category of neural network framework bugs because it involves the interaction of multiple components - tensors, automatic differentiation, neural modules, and optimizers - all coordinating to produce emergent learning behavior. Training loop bugs are particularly challenging because they often don&#39;t produce error messages, instead manifesting as training that runs to completion but fails to learn effectively.</p>\n<p>The symptoms of training loop problems can be subtle and misleading. Training might appear to run smoothly with loss values that change over time, but the changes might be in the wrong direction, too slow, or might plateau prematurely at poor solutions. These behaviors can result from optimizer configuration errors, learning rate problems, loss function bugs, or subtle issues in the coordination between forward passes, gradient computation, and parameter updates.</p>\n<p><strong>Training Loop Failure Modes:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Symptoms</th>\n<th>Common Causes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No learning</td>\n<td>Loss remains constant across epochs</td>\n<td>Gradients not flowing to parameters, learning rate too small, optimizer not updating</td>\n</tr>\n<tr>\n<td>Unstable training</td>\n<td>Loss oscillates wildly or increases</td>\n<td>Learning rate too large, gradient explosion, numerical instability</td>\n</tr>\n<tr>\n<td>Slow convergence</td>\n<td>Learning much slower than expected</td>\n<td>Learning rate too small, poor initialization, gradient vanishing</td>\n</tr>\n<tr>\n<td>Premature plateauing</td>\n<td>Training stops improving early</td>\n<td>Local minimum, learning rate decay too aggressive</td>\n</tr>\n<tr>\n<td>Perfect training loss, poor validation</td>\n<td>Overfitting without generalization</td>\n<td>No regularization, model too complex for dataset</td>\n</tr>\n</tbody></table>\n<p>The most effective approach to debugging training problems involves creating systematic monitoring and validation systems that track not just loss values but also gradient magnitudes, parameter update sizes, learning rates, and other training dynamics that provide insight into what the optimization process is actually doing.</p>\n<p><strong>Training Loop Monitoring Framework:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Monitoring Component</th>\n<th>Purpose</th>\n<th>Key Metrics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TrainingMonitor</code></td>\n<td>Track training dynamics and detect problems</td>\n<td>Loss history, gradient norms, parameter update magnitudes</td>\n</tr>\n<tr>\n<td><code>validate_training_step</code></td>\n<td>Verify each training step completed correctly</td>\n<td>Forward pass output shapes, gradient computation success, parameter updates applied</td>\n</tr>\n<tr>\n<td><code>check_gradient_flow</code></td>\n<td>Ensure gradients flow through all parameters</td>\n<td>Parameter-wise gradient magnitudes, layers with zero gradients</td>\n</tr>\n<tr>\n<td><code>validate_gradient_computation</code></td>\n<td>Compare gradients against numerical differentiation</td>\n<td>Gradient correctness on training batches</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Optimizer State Not Reset</strong></p>\n<p>A common but subtle training loop bug occurs when optimizer state (such as momentum buffers in SGD or moment estimates in Adam) is not properly reset between different training runs or when loading checkpoints. This can cause training to behave unpredictably, especially with optimizers like Adam that maintain running averages of gradients and squared gradients.</p>\n<p>The symptoms often include training that works correctly when started fresh but behaves differently when resumed from checkpoints, or training runs that are not reproducible even with the same random seeds. The optimizer may be applying updates based on stale momentum or moment estimates from previous training sessions.</p>\n<p><strong>Loss Function Debugging:</strong></p>\n<p>Loss function bugs can be particularly insidious because they often allow training to proceed but guide the optimization process toward incorrect solutions. The most common issues include incorrect gradient computation in custom loss functions, numerical instability in standard loss functions, and mismatched expectations between loss function outputs and optimizer inputs.</p>\n<table>\n<thead>\n<tr>\n<th>Loss Function Issue</th>\n<th>Detection Method</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Incorrect gradients</td>\n<td>Compare with numerical differentiation</td>\n<td>Debug loss function backward pass</td>\n</tr>\n<tr>\n<td>Numerical instability</td>\n<td>Monitor for NaN/inf in loss values</td>\n<td>Add numerical stability constants</td>\n</tr>\n<tr>\n<td>Wrong reduction</td>\n<td>Loss magnitude much larger/smaller than expected</td>\n<td>Check reduction parameter (mean vs sum)</td>\n</tr>\n<tr>\n<td>Target format mismatch</td>\n<td>Training doesn&#39;t improve despite correct gradients</td>\n<td>Verify target tensor shape and dtype</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Learning Rate Scheduling Configuration</strong></p>\n<p>Learning rate scheduling bugs often manifest as training that starts well but then suddenly stops learning or begins to diverge. These issues typically occur when learning rate schedulers are configured incorrectly, applied at the wrong frequency, or use inappropriate decay schedules for the specific optimization problem.</p>\n<p>Common scheduling mistakes include applying exponential decay too aggressively (causing learning to stop prematurely), using step-based schedules with inappropriate step sizes, or forgetting to call the scheduler&#39;s step method at the correct frequency during training.</p>\n<p><strong>Training Loop Debugging Workflow:</strong></p>\n<ol>\n<li><p><strong>Verify Individual Components</strong>: Test tensors, gradients, and optimizers in isolation before integrating them into the full training loop.</p>\n</li>\n<li><p><strong>Monitor Training Dynamics</strong>: Track not just loss values but also gradient magnitudes, parameter norms, and update sizes to understand what the optimizer is actually doing.</p>\n</li>\n<li><p><strong>Validate on Toy Problems</strong>: Test the training loop on simple problems with known solutions (linear regression, XOR) to establish that the basic mechanism works.</p>\n</li>\n<li><p><strong>Check Learning Rate Sensitivity</strong>: Test multiple learning rates across several orders of magnitude to ensure the optimizer can learn with appropriate configuration.</p>\n</li>\n<li><p><strong>Verify Gradient Flow</strong>: Ensure that gradients computed during training match gradients computed by numerical differentiation on the same inputs.</p>\n</li>\n<li><p><strong>Test Reproducibility</strong>: Verify that training runs with the same random seed produce identical results, indicating that the training loop is deterministic and properly implemented.</p>\n</li>\n</ol>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The debugging infrastructure for a neural network framework requires both systematic monitoring tools and manual debugging utilities that help diagnose problems when they occur. The implementation approach should prioritize clear error messages and detailed diagnostic information over performance, since debugging tools are typically used during development rather than production.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Handling</td>\n<td>Python exceptions with detailed messages</td>\n<td>Custom exception hierarchy with error codes</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python logging module with structured output</td>\n<td>Structured logging with JSON output and filtering</td>\n</tr>\n<tr>\n<td>Numerical Validation</td>\n<td>NumPy-based numerical differentiation</td>\n<td>Autograd or JAX for reference implementations</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>Python tracemalloc for memory usage</td>\n<td>psutil for system-wide resource monitoring</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Matplotlib for gradient/loss plotting</td>\n<td>TensorBoard-style web interface</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n  core/\n    tensor.py                 ← Core tensor implementation\n    operations.py             ← Tensor operations with gradient computation\n    autodiff.py              ← Automatic differentiation engine\n  nn/\n    modules.py               ← Neural network modules\n    optimizers.py            ← Optimization algorithms\n    losses.py                ← Loss functions\n  debug/\n    __init__.py\n    gradient_checker.py      ← Gradient validation utilities\n    shape_tracer.py          ← Shape debugging tools\n    training_monitor.py      ← Training loop monitoring\n    error_handlers.py        ← Custom exceptions and error messages\n  tests/\n    test_gradients.py        ← Gradient checking tests\n    test_shapes.py           ← Broadcasting and shape tests\n    test_training.py         ← End-to-end training tests</code></pre></div>\n\n<p><strong>Core Debugging Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debug/error_handlers.py - Complete exception hierarchy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Tuple, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NeuralFrameworkError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for neural framework errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.details </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> details </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stack_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> traceback.format_stack()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">__class__</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.details:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Details: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.details</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ShapeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error for tensor shape mismatches.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, shapes: List[Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]], operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 expected_pattern: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shapes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shapes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_pattern</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.create_shape_error_message(shapes, operation, expected_pattern)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, {</span><span style=\"color:#9ECBFF\">\"shapes\"</span><span style=\"color:#E1E4E8\">: shapes, </span><span style=\"color:#9ECBFF\">\"operation\"</span><span style=\"color:#E1E4E8\">: operation})</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_shape_error_message</span><span style=\"color:#E1E4E8\">(self, shapes: List[Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_pattern: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create detailed shape mismatch error message.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shape_strs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(shape) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> shape </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> shapes]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Shape mismatch in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: got shapes </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">', '</span><span style=\"color:#E1E4E8\">.join(shape_strs)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> expected_pattern:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            message </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Expected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_pattern</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add specific guidance for common operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> operation </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"matmul\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(shapes) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            message </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">For matrix multiplication: (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shapes[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) @ (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shapes[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            message </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Inner dimensions must match: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shapes[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shapes[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BroadcastingError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ShapeError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error for broadcasting failures with detailed analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, shape1: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], shape2: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shape1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shape1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shape2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shape2</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_analysis </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.analyze_broadcasting_failure(shape1, shape2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Broadcasting failed between </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> and </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Step-by-step analysis:</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.step_analysis)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">([shape1, shape2], </span><span style=\"color:#9ECBFF\">\"broadcasting\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_broadcasting_failure</span><span style=\"color:#E1E4E8\">(self, shape1: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   shape2: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze why broadcasting failed with detailed steps.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        steps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Right-align shapes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_dims </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(shape1), </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(shape2))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        padded1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (max_dims </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(shape1)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> shape1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        padded2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (max_dims </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(shape2)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> shape2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        steps.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Right-aligned shapes: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">padded1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">padded2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check each dimension</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, (d1, d2) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(padded1, padded2)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> d1 </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> d2:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                steps.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Dimension </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> == </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ✓\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> d1 </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                steps.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Dimension </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> broadcasts to </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ✓\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> d2 </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                steps.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Dimension </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> broadcasts to </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ✓\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                steps.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Dimension </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d1</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">d2</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ✗ (INCOMPATIBLE)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> steps</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for gradient computation errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NumericalInstabilityError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error for NaN/inf values in computations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tensor_info: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tensor_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor_info</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Numerical instability detected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tensor_info</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, tensor_info)</span></span></code></pre></div>\n\n<p><strong>Gradient Validation Utilities (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debug/gradient_checker.py - Complete gradient validation system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Dict, Any, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientTester</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete gradient validation system comparing autodiff vs numerical.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.test_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> numerical_gradient</span><span style=\"color:#E1E4E8\">(self, f: Callable, inputs: List, h: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> List[np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute numerical gradients using central differences.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradients </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, input_tensor </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(inputs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros_like(input_tensor.data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> input_tensor.data.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            flat_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad.flatten()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> j </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(flat_input)):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Save original value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                original </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input[j]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Compute f(x + h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                flat_input[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> original </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.data.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                f_plus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs).item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Compute f(x - h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                flat_input[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> original </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> h</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.data.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                f_minus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs).item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Central difference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                flat_grad[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (f_plus </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> f_minus) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> h)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Restore original value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                flat_input[j] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> original</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            input_tensor.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flat_input.reshape(input_tensor.data.shape)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gradients.append(flat_grad.reshape(input_tensor.data.shape))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> relative_error</span><span style=\"color:#E1E4E8\">(self, a: np.ndarray, b: np.ndarray, eps: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-8</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute relative error between arrays.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> np.max(np.abs(a </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> b) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (np.abs(a) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> np.abs(b) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> eps))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_gradients</span><span style=\"color:#E1E4E8\">(self, f: Callable, inputs: List, tolerance: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare autodiff gradients against numerical gradients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tolerance </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute autodiff gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            inp.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Clear previous gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        autodiff_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [inp.grad.data </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> inp.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> np.zeros_like(inp.data) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                         for</span><span style=\"color:#E1E4E8\"> inp </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> inputs]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute numerical gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        numerical_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.numerical_gradient(f, inputs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compare each gradient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        all_passed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, (auto_grad, num_grad) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(autodiff_grads, numerical_grads)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            error </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.relative_error(auto_grad, num_grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            passed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> error </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_passed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> all_passed </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> passed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.test_results.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'input_index'</span><span style=\"color:#E1E4E8\">: i,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'relative_error'</span><span style=\"color:#E1E4E8\">: error,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'tolerance'</span><span style=\"color:#E1E4E8\">: tolerance,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'passed'</span><span style=\"color:#E1E4E8\">: passed,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'autodiff_grad'</span><span style=\"color:#E1E4E8\">: auto_grad,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'numerical_grad'</span><span style=\"color:#E1E4E8\">: num_grad</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> passed:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                warnings.warn(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient check failed for input </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                            f</span><span style=\"color:#9ECBFF\">\"relative error </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> > tolerance </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tolerance</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> all_passed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, test_function: Callable) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test gradients for a specific operation with multiple random inputs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Testing gradients for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        passed_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_tests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">  # Test with 5 random inputs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> test_idx </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(total_tests):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> test_function()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> result:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    passed_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Test </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test_idx </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\">: FAILED\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Test </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test_idx </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\">: ERROR - </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> passed_count </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> total_tests</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Success rate: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">passed_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">total_tests</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">success_rate</span><span style=\"color:#F97583\">:.1%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> success_rate </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#6A737D\">  # Require 80% success rate</span></span></code></pre></div>\n\n<p><strong>Training Loop Monitoring (Core Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debug/training_monitor.py - Training dynamics monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitor training dynamics and detect problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, check_frequency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_frequency</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_norms </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_step</span><span style=\"color:#E1E4E8\">(self, loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, gradients: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log training step and check for problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history.append(loss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute gradient norms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param_name, grad </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> gradients.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                grad_norm </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> np.sum(grad.data </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sqrt(grad_norm)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_norms.append(grad_norm)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for problems periodically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._check_training_health()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _check_training_health</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check for common training problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for gradient explosion (gradient norm > 10.0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for gradient vanishing (gradient norm &#x3C; 1e-7)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for loss not decreasing over recent steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for numerical instability (NaN/inf in loss)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Emit warnings for detected problems</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_training_step</span><span style=\"color:#E1E4E8\">(self, model, loss, optimizer) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that training step completed successfully.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that loss is finite</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check that all parameters have finite gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check that optimizer state is consistent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return True if everything looks correct</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitor gradient flow through neural network parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, check_frequency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_frequency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_frequency</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_gradient_flow</span><span style=\"color:#E1E4E8\">(self, model, loss) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check gradient magnitudes for all model parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute gradient magnitude for each parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify parameters with zero or very small gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return dictionary mapping parameter names to gradient magnitudes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Warn about parameters that may not be learning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints:</strong></p>\n<p>After implementing the debugging infrastructure, verify it works with these tests:</p>\n<p><strong>Test 1: Gradient Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_gradients.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected: All gradient tests pass with relative errors &lt; 1e-5</p>\n<p><strong>Test 2: Shape Error Detection</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Should raise clear BroadcastingError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">]])  </span><span style=\"color:#6A737D\"># Shape (2, 2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tensor2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tensor([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])         </span><span style=\"color:#6A737D\"># Shape (3,)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor1 </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> tensor2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#E1E4E8\"> BroadcastingError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(e)  </span><span style=\"color:#6A737D\"># Should show step-by-step broadcasting analysis</span></span></code></pre></div>\n\n<p><strong>Test 3: Training Loop Monitoring</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Should detect gradient explosion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TrainingMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> step </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Simulate training step with exploding gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fake_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> step </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fake_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"param1\"</span><span style=\"color:#E1E4E8\">: np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, step </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    monitor.log_step(fake_loss, fake_grads)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should emit warnings about gradient explosion</span></span></code></pre></div>\n\n<p>The debugging infrastructure should catch problems early and provide clear guidance on how to fix them. Focus on making error messages educational rather than just reporting failures.</p>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section builds upon the complete framework to explore advanced features, performance optimizations, and ecosystem integrations that extend beyond the core learning objectives</p>\n</blockquote>\n<p>Building a neural network framework from scratch provides an excellent foundation for understanding deep learning systems at a fundamental level. However, our educational framework represents just the beginning of what production-grade neural network systems require. Think of our current implementation as a well-crafted bicycle - it teaches you balance, steering, and propulsion in their purest forms. The extensions in this section are like upgrading to motorcycles, cars, and eventually spacecraft - each step builds upon the fundamental principles while adding layers of sophistication, performance, and capability.</p>\n<p>The extensions we&#39;ll explore fall into three major categories that mirror the evolution of real-world deep learning frameworks. First, performance optimizations transform our educational code into something that can handle real-world workloads with acceptable speed and memory efficiency. Second, advanced neural network features expand the types of problems our framework can solve beyond basic feedforward networks. Third, ecosystem and tooling additions make our framework practical for real research and development work by integrating with existing machine learning pipelines and providing essential developer tools.</p>\n<p>Understanding these extensions serves multiple purposes in your learning journey. Most immediately, it helps you recognize the gap between educational implementations and production systems, giving you realistic expectations about what building real frameworks entails. More importantly, it provides a roadmap for your continued learning - each extension represents a deep technical domain where you can dive deeper based on your interests and career goals. Finally, implementing some of these extensions makes for excellent portfolio projects that demonstrate your understanding of both theoretical concepts and practical engineering challenges.</p>\n<p>The mental model for approaching these extensions is that of a master craftsperson expanding their workshop. You&#39;ve learned the fundamental skills of working with the essential tools. Now you&#39;re ready to add specialized equipment, learn advanced techniques, and integrate your craft with broader industrial processes. Some extensions are natural next steps that improve what you&#39;re already doing. Others open entirely new domains of possibility. The key is understanding which extensions serve your learning goals and which represent scope creep that could distract from mastering the fundamentals.</p>\n<h3 id=\"performance-optimizations\">Performance Optimizations</h3>\n<p>Performance optimization in neural network frameworks is like transforming a carefully crafted prototype into a high-performance production vehicle. Our current educational implementation prioritizes clarity and understanding over speed - every operation is explicit, every computation graph node is individually allocated, and every gradient calculation follows the textbook algorithm step by step. While this approach excellently serves learning objectives, real-world neural networks require orders of magnitude better performance to train on meaningful datasets within reasonable timeframes.</p>\n<p>The performance optimization landscape spans multiple dimensions, each with distinct technical challenges and trade-offs. <strong>GPU acceleration</strong> moves computation from sequential CPU operations to massively parallel GPU kernels, but requires understanding GPU programming models, memory hierarchies, and kernel optimization techniques. <strong>Operation fusion</strong> combines multiple simple operations into optimized compound operations, reducing memory bandwidth and improving cache efficiency, but increases implementation complexity and requires sophisticated compiler techniques. <strong>Memory optimization</strong> techniques like gradient checkpointing and activation recomputation trade compute for memory, enabling training of much larger models, but require careful analysis of the memory-computation trade-off space.</p>\n<h4 id=\"gpu-acceleration-foundation\">GPU Acceleration Foundation</h4>\n<p>GPU acceleration transforms our CPU-bound tensor operations into massively parallel computations that can achieve 10-100x speedups on suitable workloads. The fundamental insight is that neural network operations like matrix multiplication, element-wise arithmetic, and convolutions exhibit data parallelism - the same operation can be applied to thousands of data elements simultaneously.</p>\n<p>The <strong>GPU programming model</strong> requires restructuring our tensor operations to work with GPU memory hierarchies and parallel execution units. Instead of our current approach where <code>matmul</code> calls NumPy&#39;s sequential CPU implementation, GPU acceleration involves writing or calling optimized CUDA kernels that distribute computation across thousands of GPU threads. Each thread processes a small portion of the output tensor, with careful coordination to ensure memory access patterns maximize GPU memory bandwidth utilization.</p>\n<p><strong>Memory management</strong> becomes significantly more complex with GPU acceleration. Our current <code>Tensor</code> class stores data in CPU memory and performs all operations in-place or with immediate allocation. GPU tensors require managing data movement between CPU and GPU memory spaces, with explicit control over when data transfers occur. The key insight is that GPU memory bandwidth to CPU memory is limited, so minimizing host-device transfers is crucial for performance.</p>\n<table>\n<thead>\n<tr>\n<th>GPU Memory Management Component</th>\n<th>CPU Implementation</th>\n<th>GPU Implementation</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tensor Storage</td>\n<td><code>data: np.ndarray</code> in CPU RAM</td>\n<td><code>data: GPUArray</code> in GPU memory</td>\n<td>10-100x faster compute, but transfer overhead</td>\n</tr>\n<tr>\n<td>Operation Execution</td>\n<td>Sequential NumPy calls</td>\n<td>Parallel GPU kernels</td>\n<td>Massive parallelism for suitable operations</td>\n</tr>\n<tr>\n<td>Memory Allocation</td>\n<td>Python garbage collector</td>\n<td>Explicit GPU memory pools</td>\n<td>Faster allocation, manual lifecycle management</td>\n</tr>\n<tr>\n<td>Data Movement</td>\n<td>Implicit copies</td>\n<td>Explicit CPU↔GPU transfers</td>\n<td>Must minimize transfers for performance</td>\n</tr>\n</tbody></table>\n<p>The <strong>CUDA integration strategy</strong> involves several architectural decisions that significantly impact both performance and implementation complexity. The simplest approach wraps existing CUDA libraries like cuBLAS for matrix operations and cuDNN for neural network primitives, providing good performance with minimal implementation effort. More advanced approaches involve writing custom CUDA kernels for fused operations, providing maximum performance at the cost of significant GPU programming expertise requirements.</p>\n<blockquote>\n<p><strong>Decision: GPU Backend Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to accelerate tensor operations while maintaining our clean tensor API</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Wrapper around CuPy/PyTorch CUDA tensors</li>\n<li>Custom CUDA kernel implementation</li>\n<li>OpenCL for cross-platform GPU support</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Start with CuPy wrapper, provide hooks for custom kernels</li>\n<li><strong>Rationale</strong>: CuPy provides immediate GPU acceleration with minimal implementation complexity, while custom kernel hooks allow optimization of bottleneck operations later</li>\n<li><strong>Consequences</strong>: Enables GPU acceleration quickly but creates dependency on CuPy ecosystem</li>\n</ul>\n</blockquote>\n<p>The <strong>kernel optimization</strong> process for custom GPU operations follows a systematic approach to maximize GPU utilization. First, identify bottleneck operations through profiling - typically matrix multiplication, large element-wise operations, and reduction operations like sum or max. Second, design memory access patterns that maximize coalesced memory accesses and minimize bank conflicts. Third, optimize thread block configurations and shared memory usage to maximize occupancy. Finally, implement multiple kernel variants optimized for different input sizes and shapes.</p>\n<h4 id=\"operation-fusion-techniques\">Operation Fusion Techniques</h4>\n<p>Operation fusion combines multiple simple operations into single optimized kernels, dramatically reducing memory bandwidth requirements and improving cache efficiency. Consider a common neural network operation like <code>relu(matmul(x, weight) + bias)</code> - our current implementation performs three separate operations with three separate memory passes. Fusion combines these into a single GPU kernel that computes the matrix multiplication, adds bias, and applies ReLU activation while the data is still in GPU registers and shared memory.</p>\n<p>The <strong>fusion identification</strong> process systematically analyzes computation graphs to identify fusion opportunities. Element-wise operations like addition, multiplication, and activation functions are excellent fusion candidates because they have the same memory access patterns and can share loop structures. Reduction operations like sum, mean, and softmax can be fused with preceding element-wise operations. More complex fusions involve combining matrix operations with element-wise operations, like fusing bias addition into matrix multiplication kernels.</p>\n<p><strong>Automatic fusion</strong> requires building a graph optimization pass that identifies fusible operation sequences and generates optimized implementations. The fusion algorithm walks the computation graph looking for patterns like consecutive element-wise operations or matrix operations followed by element-wise operations. For each identified pattern, it replaces the original operation sequence with a single fused operation that implements the same mathematical computation but with optimized memory access patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Fusion Pattern</th>\n<th>Operations</th>\n<th>Memory Passes (Unfused)</th>\n<th>Memory Passes (Fused)</th>\n<th>Typical Speedup</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Element-wise Chain</td>\n<td><code>relu(tanh(x + y))</code></td>\n<td>3 passes</td>\n<td>1 pass</td>\n<td>2-3x</td>\n</tr>\n<tr>\n<td>MatMul + Bias</td>\n<td><code>matmul(x, w) + b</code></td>\n<td>2 passes</td>\n<td>1 pass</td>\n<td>1.5-2x</td>\n</tr>\n<tr>\n<td>Broadcast Fusion</td>\n<td><code>x * y + z</code> with broadcasting</td>\n<td>2 passes</td>\n<td>1 pass</td>\n<td>1.5-2.5x</td>\n</tr>\n<tr>\n<td>Reduction Fusion</td>\n<td><code>sum(relu(x * y))</code></td>\n<td>2 passes</td>\n<td>1 pass</td>\n<td>2-4x</td>\n</tr>\n</tbody></table>\n<p>The <strong>code generation</strong> approach for fused operations can follow several strategies depending on implementation complexity preferences. Template-based generation uses parameterized kernel templates that get specialized for specific operation combinations. Just-in-time compilation generates and compiles CUDA kernels at runtime based on the specific operation sequence and tensor shapes. More sophisticated approaches use domain-specific languages like Halide or TVM to express the computation and automatically generate optimized implementations.</p>\n<h4 id=\"memory-optimization-strategies\">Memory Optimization Strategies</h4>\n<p>Memory optimization techniques enable training neural networks that are much larger than available GPU memory through careful trade-offs between computation and memory usage. The fundamental insight is that many intermediate activations can be recomputed during the backward pass rather than stored, trading additional forward pass computation for reduced memory footprint.</p>\n<p><strong>Gradient checkpointing</strong> selectively saves activation values at certain layers while discarding activations at intermediate layers. During the backward pass, discarded activations are recomputed by re-running the forward pass from the most recent checkpoint. This technique can reduce memory usage by 50-80% at the cost of approximately 30-50% additional computation time. The key design challenge is selecting optimal checkpoint locations that minimize recomputation while providing sufficient memory savings.</p>\n<p>The <strong>checkpointing strategy</strong> involves analyzing the computation graph to identify optimal checkpoint placement. Memory-intensive layers like large matrix multiplications and convolutions are prime candidates for activation recomputation. The algorithm calculates the memory footprint and recomputation cost for different checkpointing configurations, selecting the configuration that minimizes total training time while staying within memory constraints.</p>\n<p><strong>Activation recomputation</strong> requires modifying our backward pass algorithm to support dynamic forward pass re-execution. When the backward pass reaches a layer whose activations were discarded, it triggers a mini forward pass from the most recent checkpoint to the current layer. This requires careful coordination between the forward pass execution engine and the backward pass gradient computation to ensure consistent execution and proper gradient flow.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Optimization Technique</th>\n<th>Memory Reduction</th>\n<th>Computation Overhead</th>\n<th>Implementation Complexity</th>\n<th>Best Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradient Checkpointing</td>\n<td>50-80%</td>\n<td>30-50% slower</td>\n<td>Moderate</td>\n<td>Large models, memory-constrained training</td>\n</tr>\n<tr>\n<td>Mixed Precision Training</td>\n<td>40-50%</td>\n<td>10-20% faster</td>\n<td>Low</td>\n<td>Modern GPUs with Tensor Cores</td>\n</tr>\n<tr>\n<td>Dynamic Loss Scaling</td>\n<td>Variable</td>\n<td>Minimal</td>\n<td>Moderate</td>\n<td>Preventing underflow in mixed precision</td>\n</tr>\n<tr>\n<td>Offload to CPU Memory</td>\n<td>80-90%</td>\n<td>2-10x slower</td>\n<td>High</td>\n<td>Very large models exceeding GPU memory</td>\n</tr>\n</tbody></table>\n<p><strong>Mixed precision training</strong> uses 16-bit floating point for most operations while keeping 32-bit precision for operations that require higher numerical precision. This technique reduces memory usage by approximately 50% while often improving training speed on modern GPUs with specialized Tensor Core units. However, it requires careful handling of gradient scaling to prevent underflow and selective precision upgrading for numerically sensitive operations.</p>\n<p>The <strong>implementation architecture</strong> for memory optimization requires extending our tensor and operation classes to support checkpointing metadata and recomputation triggers. The <code>Tensor</code> class needs additional fields to track whether activations are stored or discarded, and checkpoint identifiers for recomputation. The backward pass algorithm requires modification to detect missing activations and trigger appropriate recomputation sequences.</p>\n<h3 id=\"advanced-neural-network-features\">Advanced Neural Network Features</h3>\n<p>Advanced neural network features expand our framework&#39;s capability beyond basic feedforward networks to support the full spectrum of modern deep learning architectures. Think of our current framework as a foundation that understands the language of tensors, gradients, and optimization. The advanced features are like learning specialized dialects that enable communication in specific domains - computer vision through convolutional operations, sequential data through recurrent architectures, and stable training through normalization techniques.</p>\n<p>The design philosophy for adding advanced features should maintain the educational clarity and compositional nature of our existing module system while introducing the mathematical sophistication required for modern architectures. Each new feature should feel like a natural extension of our existing tensor and module abstractions rather than a bolt-on addition that breaks the conceptual model.</p>\n<h4 id=\"convolutional-neural-network-support\">Convolutional Neural Network Support</h4>\n<p>Convolutional operations represent a fundamental shift from our current dense linear algebra operations to spatially-aware computations that respect the local structure of image and signal data. The core insight is that convolution operations can be expressed as specialized matrix multiplications, but efficient implementations require understanding of im2col transformations, memory layout optimizations, and multi-dimensional indexing patterns.</p>\n<p>The <strong>convolution operation</strong> implementation requires extending our tensor operations to handle multi-dimensional convolution with proper gradient computation. Unlike our current <code>MatMul</code> operation that works with 2D matrices, convolution operates on 4D tensors with dimensions <code>(batch_size, channels, height, width)</code> for 2D convolutions. The forward pass involves sliding a kernel across input spatial dimensions, computing dot products at each position. The backward pass requires computing gradients with respect to both input data and convolution weights through transposed convolution operations.</p>\n<p><strong>Im2col transformation</strong> converts the convolution operation into matrix multiplication by reshaping input patches into columns of a matrix. This transformation enables leveraging our existing optimized matrix multiplication implementations for convolution computation. However, it requires careful memory management since the im2col transformation can significantly increase memory usage for large kernels or small strides.</p>\n<table>\n<thead>\n<tr>\n<th>Convolution Component</th>\n<th>Mathematical Operation</th>\n<th>Implementation Strategy</th>\n<th>Memory Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2D Convolution</td>\n<td><code>out[b,c,h,w] = sum(input[b,:,h:h+kh,w:w+kw] * kernel[c,:,:,:])</code></td>\n<td>Im2col + MatMul</td>\n<td>O(batch * channels * height * width * kernel_size²)</td>\n</tr>\n<tr>\n<td>Gradient wrt Input</td>\n<td>Transposed convolution</td>\n<td>Gradient im2col</td>\n<td>Same as forward pass</td>\n</tr>\n<tr>\n<td>Gradient wrt Kernel</td>\n<td>Input-output correlation</td>\n<td>Correlation via convolution</td>\n<td>O(output_channels * input_channels * kernel_size²)</td>\n</tr>\n<tr>\n<td>Padding Handling</td>\n<td>Zero-pad or reflect input</td>\n<td>Explicit padding in im2col</td>\n<td>Additional memory for padded tensors</td>\n</tr>\n</tbody></table>\n<p>The <strong>pooling operations</strong> provide spatial downsampling and local aggregation functionality essential for CNN architectures. Max pooling and average pooling operations reduce spatial dimensions while preserving channel depth. The implementation challenge involves efficient indexing to identify maximum values for max pooling gradient computation and proper handling of overlapping pooling windows.</p>\n<p><strong>Batch normalization</strong> normalizes layer inputs to have zero mean and unit variance across the batch dimension, significantly improving training stability and convergence speed. The implementation requires computing running statistics for inference mode and proper gradient computation through the normalization transformation. The key challenge is handling the different behaviors in training mode (using batch statistics) versus inference mode (using running statistics).</p>\n<blockquote>\n<p><strong>Decision: Convolution Implementation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need efficient convolution operations while maintaining gradient computation compatibility</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Direct convolution with nested loops</li>\n<li>Im2col transformation + existing matrix multiplication</li>\n<li>FFT-based convolution for large kernels</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement im2col + matrix multiplication with hooks for FFT optimization</li>\n<li><strong>Rationale</strong>: Im2col leverages existing optimized matrix operations while providing reasonable performance; FFT hooks enable future optimization for large kernels</li>\n<li><strong>Consequences</strong>: Good performance with implementation complexity manageable for educational purposes</li>\n</ul>\n</blockquote>\n<h4 id=\"recurrent-neural-network-architectures\">Recurrent Neural Network Architectures</h4>\n<p>Recurrent neural networks process sequential data by maintaining hidden state across time steps, requiring our framework to handle variable-length sequences and temporal gradient flow. The fundamental challenge is extending our static computation graph approach to handle dynamic unrolling of recurrent operations across sequence lengths while maintaining efficient memory usage and gradient computation.</p>\n<p>The <strong>RNN cell abstraction</strong> provides the building block for all recurrent architectures. An RNN cell takes current input and previous hidden state as inputs and produces new hidden state and optional output. The cell can be applied repeatedly across sequence time steps, with hidden state flowing from one time step to the next. This abstraction naturally extends our module system since RNN cells are modules with specific input/output contracts.</p>\n<p><strong>LSTM and GRU implementations</strong> require multiple internal gates with sigmoid and tanh activations, element-wise multiplications, and careful state management. The LSTM cell maintains both hidden state and cell state, with forget, input, and output gates controlling information flow. The implementation challenge involves efficient computation of multiple gate operations and proper gradient flow through the complex gate interactions.</p>\n<table>\n<thead>\n<tr>\n<th>RNN Architecture</th>\n<th>State Components</th>\n<th>Gate Operations</th>\n<th>Gradient Flow Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Vanilla RNN</td>\n<td>Hidden state only</td>\n<td>Single tanh activation</td>\n<td>Linear gradient path, prone to vanishing gradients</td>\n</tr>\n<tr>\n<td>LSTM</td>\n<td>Hidden + cell state</td>\n<td>Forget, input, output gates</td>\n<td>Multiple parallel paths, more stable gradients</td>\n</tr>\n<tr>\n<td>GRU</td>\n<td>Hidden state only</td>\n<td>Reset, update gates</td>\n<td>Simplified gating, good gradient properties</td>\n</tr>\n<tr>\n<td>Bidirectional</td>\n<td>Forward + backward hidden</td>\n<td>Duplicate computations</td>\n<td>Independent forward/backward gradient paths</td>\n</tr>\n</tbody></table>\n<p>The <strong>sequence processing</strong> strategy must handle variable-length sequences efficiently while maintaining batch processing capabilities. Padding sequences to uniform length enables batch processing but requires masking to ignore padded positions in loss computation. Packed sequences avoid padding overhead but require more complex indexing and sorting operations.</p>\n<p><strong>Attention mechanisms</strong> enable models to focus on specific parts of input sequences, dramatically improving performance on long sequences. The basic attention operation computes weighted combinations of sequence elements based on learned attention weights. Self-attention (used in Transformers) computes attention weights between all pairs of sequence positions, enabling parallel processing that overcomes RNN sequential dependencies.</p>\n<h4 id=\"normalization-and-regularization-techniques\">Normalization and Regularization Techniques</h4>\n<p>Normalization and regularization techniques stabilize training and improve generalization by controlling activation distributions and preventing overfitting. These techniques require understanding statistical computation, gradient flow through normalization operations, and careful mode switching between training and inference behaviors.</p>\n<p><strong>Layer normalization</strong> normalizes activations across the feature dimension rather than the batch dimension like batch normalization. This approach works better for recurrent networks and eliminates dependence on batch size. The implementation computes mean and variance across the last dimension of input tensors, then applies the standard normalization transformation with learnable scale and shift parameters.</p>\n<p><strong>Dropout regularization</strong> randomly sets activation values to zero during training to prevent overfitting. The implementation requires random mask generation, proper scaling of remaining activations, and mode-dependent behavior (dropout during training, identity during inference). The gradient computation must account for the masking pattern used in the forward pass.</p>\n<table>\n<thead>\n<tr>\n<th>Normalization Technique</th>\n<th>Normalization Axes</th>\n<th>Learnable Parameters</th>\n<th>Mode Dependency</th>\n<th>Primary Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Batch Normalization</td>\n<td>Across batch dimension</td>\n<td>Scale (γ) and shift (β)</td>\n<td>Training vs inference</td>\n<td>CNNs, feedforward networks</td>\n</tr>\n<tr>\n<td>Layer Normalization</td>\n<td>Across feature dimension</td>\n<td>Scale (γ) and shift (β)</td>\n<td>None</td>\n<td>RNNs, Transformers</td>\n</tr>\n<tr>\n<td>Instance Normalization</td>\n<td>Across spatial dimensions</td>\n<td>Scale (γ) and shift (β)</td>\n<td>None</td>\n<td>Style transfer, GANs</td>\n</tr>\n<tr>\n<td>Group Normalization</td>\n<td>Across channel groups</td>\n<td>Scale (γ) and shift (β)</td>\n<td>None</td>\n<td>Small batch training</td>\n</tr>\n</tbody></table>\n<p>The <strong>implementation strategy</strong> for normalization layers requires extending our module system to support running statistics tracking, mode-dependent computation, and proper parameter initialization. The key architectural challenge is coordinating between training and inference modes while ensuring gradient computation remains correct in both modes.</p>\n<h3 id=\"ecosystem-and-tooling\">Ecosystem and Tooling</h3>\n<p>Ecosystem and tooling transforms our educational framework into a practical development environment that integrates with existing machine learning workflows and provides essential developer productivity tools. Think of this as building the workshop around your crafted tools - adding workbenches, storage systems, measurement devices, and connections to the broader industrial ecosystem that make your framework useful for real work rather than just learning exercises.</p>\n<p>The ecosystem development should follow the principle of progressive disclosure - start with essential tooling that immediate enables productive work, then gradually add sophisticated features that support advanced use cases. Each tool should integrate naturally with our existing architecture while providing clear value for common development workflows.</p>\n<h4 id=\"model-serialization-and-persistence\">Model Serialization and Persistence</h4>\n<p>Model serialization enables saving trained models and loading them for inference or continued training, a fundamental requirement for any practical machine learning framework. Our current framework exists only in memory during program execution - once training completes, all learned parameters disappear unless explicitly extracted. Production workflows require robust serialization that preserves model architecture, trained parameters, optimizer state, and training metadata.</p>\n<p>The <strong>serialization architecture</strong> must handle the hierarchical nature of our module system while supporting backward compatibility and cross-platform portability. The key insight is that serialization involves two distinct components: architecture serialization (how modules are connected) and parameter serialization (learned weights and optimizer state). Architecture serialization can use JSON or protocol buffers to describe the module hierarchy, while parameter serialization requires efficient binary formats for large tensor arrays.</p>\n<p><strong>State dictionary abstraction</strong> provides a clean interface for extracting and loading model state. The <code>state_dict()</code> method recursively traverses the module hierarchy to collect all parameters with hierarchical names like <code>&quot;linear1.weight&quot;</code> and <code>&quot;linear1.bias&quot;</code>. The corresponding <code>load_state_dict()</code> method accepts a state dictionary and assigns parameters back to the appropriate modules. This abstraction separates model architecture from parameter values, enabling techniques like transfer learning and parameter sharing.</p>\n<table>\n<thead>\n<tr>\n<th>Serialization Component</th>\n<th>Format Choice</th>\n<th>Rationale</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Architecture</td>\n<td>JSON or YAML</td>\n<td>Human readable, language agnostic</td>\n<td>Larger file size, requires parsing</td>\n</tr>\n<tr>\n<td>Parameter Tensors</td>\n<td>NumPy .npz or HDF5</td>\n<td>Efficient binary, good compression</td>\n<td>Platform-specific endianness issues</td>\n</tr>\n<tr>\n<td>Optimizer State</td>\n<td>Pickle or MessagePack</td>\n<td>Handles arbitrary Python objects</td>\n<td>Python-specific, security concerns</td>\n</tr>\n<tr>\n<td>Training Metadata</td>\n<td>JSON embedded</td>\n<td>Easy to inspect and modify</td>\n<td>Limited to simple data types</td>\n</tr>\n</tbody></table>\n<p>The <strong>checkpoint system</strong> coordinates serialization during training to enable resumption after interruption and model evaluation at different training epochs. Checkpoints should include model state, optimizer state, random number generator state, and training metadata like current epoch and loss history. The implementation requires careful coordination with the training loop to ensure checkpoints represent consistent training state.</p>\n<p><strong>Version compatibility</strong> becomes crucial as models are shared between different versions of the framework. The serialization format should include version metadata that enables detecting compatibility issues and providing appropriate error messages. More sophisticated approaches can implement automatic migration between format versions or graceful degradation when non-critical features are unavailable.</p>\n<blockquote>\n<p><strong>Decision: Serialization Format Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need robust model persistence with good performance and cross-platform compatibility</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Pure JSON with base64-encoded tensors</li>\n<li>JSON architecture + NumPy binary tensors</li>\n<li>Protocol Buffers with custom tensor encoding</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: JSON architecture + NumPy .npz for tensors</li>\n<li><strong>Rationale</strong>: JSON provides human-readable architecture description while NumPy .npz offers efficient tensor serialization with good Python ecosystem integration</li>\n<li><strong>Consequences</strong>: Easy inspection and debugging of model files, but requires careful coordination between JSON and binary components</li>\n</ul>\n</blockquote>\n<h4 id=\"visualization-and-debugging-tools\">Visualization and Debugging Tools</h4>\n<p>Visualization and debugging tools provide insight into model behavior, training dynamics, and computational performance that are essential for productive deep learning development. Our current framework provides no visibility into internal behavior beyond final loss values - developers need tools to understand gradient flow, activation distributions, computational bottlenecks, and training convergence patterns.</p>\n<p>The <strong>computation graph visualization</strong> enables developers to inspect the structure of computation graphs generated during forward passes. This tool should render the directed acyclic graph showing tensor operations, tensor shapes, and gradient flow paths. Interactive features like zooming, filtering, and operation inspection help developers understand complex model architectures and debug gradient flow issues.</p>\n<p><strong>Training metrics dashboard</strong> provides real-time monitoring of training progress with plots of loss curves, gradient magnitudes, parameter distributions, and custom metrics. The dashboard should support multiple training runs for comparison, automatic detection of training anomalies like gradient explosion, and export of plots for presentations and papers. Integration with existing tools like TensorBoard or Weights &amp; Biases provides immediate access to sophisticated visualization capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Visualization Tool</th>\n<th>Information Provided</th>\n<th>Implementation Strategy</th>\n<th>Primary Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Computation Graph</td>\n<td>Operation structure, tensor shapes, memory usage</td>\n<td>Graphviz or D3.js rendering</td>\n<td>Architecture debugging, optimization</td>\n</tr>\n<tr>\n<td>Loss Curves</td>\n<td>Training/validation loss over time</td>\n<td>Matplotlib or web dashboard</td>\n<td>Training progress monitoring</td>\n</tr>\n<tr>\n<td>Gradient Flow</td>\n<td>Gradient magnitudes by layer</td>\n<td>Histogram plots over training steps</td>\n<td>Diagnosing vanishing/exploding gradients</td>\n</tr>\n<tr>\n<td>Parameter Distributions</td>\n<td>Weight/bias histograms by layer</td>\n<td>Statistical plots with updates</td>\n<td>Understanding parameter evolution</td>\n</tr>\n<tr>\n<td>Activation Distributions</td>\n<td>Layer output statistics</td>\n<td>Real-time histograms during training</td>\n<td>Detecting saturation or dead neurons</td>\n</tr>\n</tbody></table>\n<p>The <strong>profiling integration</strong> helps developers identify computational bottlenecks and optimize performance. The profiler should track execution time for individual operations, memory allocation patterns, GPU utilization, and data loading overhead. Integration with system profilers like <code>cProfile</code> or GPU profilers like <code>nvprof</code> provides detailed performance insights that guide optimization efforts.</p>\n<p><strong>Interactive debugging</strong> capabilities enable developers to inspect tensor values, gradient computations, and module state during training. Features like setting breakpoints on specific operations, examining intermediate activations, and manually stepping through gradient computation help diagnose subtle bugs that are difficult to catch with traditional debugging approaches.</p>\n<h4 id=\"integration-with-existing-ecosystem\">Integration with Existing Ecosystem</h4>\n<p>Integration with the existing machine learning ecosystem enables leveraging mature tools for data loading, preprocessing, evaluation, and deployment while using our framework for the core model definition and training logic. The key insight is that deep learning workflows involve much more than just neural network implementation - data pipelines, evaluation metrics, hyperparameter optimization, and deployment infrastructure are equally important for practical applications.</p>\n<p>The <strong>data loading integration</strong> should support common dataset formats and preprocessing pipelines from libraries like <code>torchvision</code>, <code>tensorflow-datasets</code>, or custom data loaders. Our framework&#39;s <code>DataLoader</code> class should provide adapters that can consume data from these sources while maintaining compatibility with our tensor format and training loop structure. Efficient data loading with prefetching, parallel processing, and GPU transfer coordination significantly impacts training performance.</p>\n<p><strong>Interoperability with NumPy and SciPy</strong> enables leveraging the rich ecosystem of scientific computing tools for data preprocessing, evaluation metrics, and post-training analysis. Our <code>Tensor</code> class should provide seamless conversion to and from NumPy arrays, enabling integration with existing codebases and scientific libraries. Zero-copy conversion when possible minimizes overhead for interoperability.</p>\n<table>\n<thead>\n<tr>\n<th>Ecosystem Integration</th>\n<th>Interface Strategy</th>\n<th>Benefits</th>\n<th>Implementation Challenges</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading</td>\n<td>Adapter pattern for common loaders</td>\n<td>Leverage existing datasets and preprocessing</td>\n<td>Coordinate different tensor formats and batch layouts</td>\n</tr>\n<tr>\n<td>Evaluation Metrics</td>\n<td>NumPy array conversion</td>\n<td>Use mature metric implementations</td>\n<td>Ensure consistent tensor→NumPy conversion</td>\n</tr>\n<tr>\n<td>Hyperparameter Optimization</td>\n<td>Export training function interface</td>\n<td>Leverage tools like Optuna or Ray Tune</td>\n<td>Design clean interfaces for external control</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td>ONNX export or REST API wrapper</td>\n<td>Enable production deployment</td>\n<td>Handle serialization and inference optimization</td>\n</tr>\n<tr>\n<td>Distributed Training</td>\n<td>Multi-process coordination</td>\n<td>Scale training to multiple GPUs/nodes</td>\n<td>Complex synchronization and communication</td>\n</tr>\n</tbody></table>\n<p>The <strong>ONNX export capability</strong> enables deploying models trained with our framework in production environments that use different inference engines. ONNX (Open Neural Network Exchange) provides a standardized format for representing neural network models that can be executed by various runtime engines. Implementing ONNX export requires mapping our module hierarchy and operations to equivalent ONNX operations while preserving mathematical behavior.</p>\n<p><strong>Hyperparameter optimization integration</strong> enables systematic exploration of model architectures and training configurations using tools like Optuna, Ray Tune, or Weights &amp; Biases Sweeps. Our training loop should expose clean interfaces for external hyperparameter optimization tools to control model configuration, training parameters, and evaluation metrics. This integration dramatically improves the efficiency of model development workflows.</p>\n<p>The <strong>deployment infrastructure</strong> should provide utilities for packaging trained models for production inference, including REST API wrappers, batch inference utilities, and integration with serving frameworks like TorchServe or TensorFlow Serving. While full deployment infrastructure is beyond our scope, providing basic utilities and clear interfaces enables users to integrate our framework into production workflows.</p>\n<h3 id=\"common-extension-implementation-pitfalls\">Common Extension Implementation Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Premature Optimization</strong>\nMany developers jump directly to advanced performance optimizations before fully understanding the baseline implementation and its bottlenecks. This approach often leads to complex code that provides minimal performance benefits while sacrificing educational clarity. Always profile to identify actual bottlenecks before optimizing, and maintain simple reference implementations alongside optimized versions.</p>\n<p>⚠️ <strong>Pitfall: Breaking Existing APIs</strong>\nAdding advanced features by modifying core interfaces like <code>Tensor</code> or <code>Module</code> can break existing code and violate the principle of backward compatibility. Design extensions as additive features that enhance existing functionality without changing established interfaces. Use composition and inheritance patterns that extend behavior while maintaining compatibility.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Gradient Computation</strong>\nAdvanced operations like convolution, normalization, and attention mechanisms involve complex gradient computations that must remain consistent with the automatic differentiation system. Implementing custom backward passes that don&#39;t properly integrate with the computation graph or accumulate gradients incorrectly leads to subtle training bugs that are extremely difficult to debug.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in GPU Code</strong>\nGPU memory management requires explicit allocation and deallocation, unlike CPU memory that relies on garbage collection. Failing to properly release GPU memory in extension code leads to memory leaks that eventually crash training jobs. Always pair GPU memory allocations with appropriate deallocations, and consider using memory pool patterns for frequently allocated objects.</p>\n<p>⚠️ <strong>Pitfall: Platform-Specific Dependencies</strong>\nAdding advanced features that depend on platform-specific libraries (like CUDA-only GPU acceleration) without providing fallback implementations breaks portability. Design extension architectures that gracefully degrade on platforms where advanced features are unavailable, and provide clear error messages when required dependencies are missing.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation of framework extensions requires careful planning to maintain the educational clarity and extensible architecture established in the core framework while adding the sophistication necessary for real-world applications.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Production Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>GPU Acceleration</td>\n<td>CuPy wrapper for basic operations</td>\n<td>Custom CUDA kernels for fusion</td>\n<td>TensorRT integration for inference</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Matplotlib + NetworkX for graphs</td>\n<td>Web dashboard with D3.js</td>\n<td>TensorBoard or Weights &amp; Biases</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON + NumPy .npz files</td>\n<td>Protocol Buffers + custom format</td>\n<td>ONNX with optimization passes</td>\n</tr>\n<tr>\n<td>Data Loading</td>\n<td>Manual NumPy array handling</td>\n<td>Custom DataLoader with prefetching</td>\n<td>Integration with Hugging Face datasets</td>\n</tr>\n<tr>\n<td>Profiling</td>\n<td>Python cProfile integration</td>\n<td>NVIDIA Nsight for GPU profiling</td>\n<td>Custom performance monitoring</td>\n</tr>\n</tbody></table>\n<h4 id=\"project-structure-for-extensions\">Project Structure for Extensions</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>neural_framework/\n  core/                    ← existing core framework\n    tensor.py\n    modules.py\n    optimizers.py\n  extensions/\n    performance/\n      gpu_backend.py       ← GPU acceleration\n      fusion_passes.py     ← operation fusion\n      memory_optimizer.py  ← memory optimization\n    networks/\n      convolution.py       ← CNN layers\n      recurrent.py         ← RNN/LSTM layers  \n      attention.py         ← attention mechanisms\n      normalization.py     ← batch norm, layer norm\n    ecosystem/\n      serialization.py     ← model save/load\n      visualization.py     ← plotting and graphing\n      data_integration.py  ← external data loaders\n      deployment.py        ← model serving utilities\n  examples/\n    cnn_image_classification.py\n    rnn_language_model.py\n    transformer_attention.py</code></pre></div>\n\n<h4 id=\"gpu-acceleration-starter-code\">GPU Acceleration Starter Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># gpu_backend.py - Complete GPU acceleration infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    import</span><span style=\"color:#E1E4E8\"> cupy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cp</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GPU_AVAILABLE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#79B8FF\"> ImportError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GPU_AVAILABLE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Fallback: create dummy cupy module that raises informative errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    class</span><span style=\"color:#B392F0\"> _CupyMock</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        def</span><span style=\"color:#79B8FF\"> __getattr__</span><span style=\"color:#E1E4E8\">(self, name):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"GPU operations require CuPy installation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> _CupyMock()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GPUTensor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"GPU-accelerated tensor wrapper that maintains API compatibility.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data, requires_grad</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, grad_fn</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, device</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'cpu'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> device </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'cuda'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> GPU_AVAILABLE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CUDA requested but CuPy not available\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> device</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.requires_grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requires_grad</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.grad_fn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad_fn</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.grad </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store data on appropriate device</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> device </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'cuda'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> GPU_AVAILABLE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cp.asarray(data) </span><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data, cp.ndarray) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.asarray(data) </span><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data, np.ndarray) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to</span><span style=\"color:#E1E4E8\">(self, device):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Move tensor to specified device.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if tensor already on target device, return self if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For cuda→cpu: convert cupy array to numpy array  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For cpu→cuda: convert numpy array to cupy array</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create new GPUTensor with converted data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle case where target device not available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __add__</span><span style=\"color:#E1E4E8\">(self, other):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"GPU-accelerated addition with automatic device coordination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Ensure both tensors on same device, move if necessary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use cupy operations if on GPU, numpy if on CPU</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create appropriate computation graph node for autodiff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return new GPUTensor with result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Complete memory pool for efficient GPU allocation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GPUMemoryPool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> GPU_AVAILABLE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cp.get_default_memory_pool()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.pinned_pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cp.get_default_pinned_memory_pool()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> allocate</span><span style=\"color:#E1E4E8\">(self, shape, dtype):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Allocate GPU memory from pool.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> cp.zeros(shape, </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">dtype)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> free_unused_blocks</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Release unused memory blocks back to system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> GPU_AVAILABLE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.pool.free_all_blocks()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.pinned_pool.free_all_blocks()</span></span></code></pre></div>\n\n<h4 id=\"operation-fusion-skeleton\">Operation Fusion Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># fusion_passes.py - Computation graph optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FusionOptimizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Identifies and fuses compatible operations in computation graphs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fusion_patterns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ElementwiseFusionPattern(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MatMulBiasFusionPattern(), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ActivationFusionPattern()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> optimize_graph</span><span style=\"color:#E1E4E8\">(self, root_tensor):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply fusion optimizations to computation graph.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Traverse computation graph to build operation list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each fusion pattern, identify matching operation sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Replace fusible operation chains with single fused operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update tensor grad_fn references to point to fused operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return optimized graph with reduced operation count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ElementwiseFusionPattern</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Fuses chains of element-wise operations like add→multiply→relu.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> matches</span><span style=\"color:#E1E4E8\">(self, operations):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if operation sequence matches this fusion pattern.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify all operations are element-wise (same output shape as input)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check that operations can be chained (output of one feeds input of next)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure no intermediate results needed elsewhere in graph</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return True if fusible, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_fused_operation</span><span style=\"color:#E1E4E8\">(self, operations):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create single operation that replaces the operation chain.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Analyze operation chain to determine combined mathematical function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate optimized kernel code (or use predefined templates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create new Operation subclass with fused forward/backward methods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return fused operation that produces identical mathematical results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"cnn-module-implementation\">CNN Module Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># convolution.py - Convolutional neural network layers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Conv2d</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"2D convolution layer with proper gradient computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, in_channels, out_channels, kernel_size, stride</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, padding</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.in_channels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> in_channels</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.out_channels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> out_channels</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.kernel_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> kernel_size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stride </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stride</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.padding </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> padding</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize convolution weights using He initialization</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.weight </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Parameter(torch.randn(out_channels, in_channels, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                          kernel_size, kernel_size) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               np.sqrt(</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> (in_channels </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> kernel_size </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> kernel_size)))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.bias </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Parameter(torch.zeros(out_channels))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute 2D convolution using im2col transformation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply padding to input tensor if padding > 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use im2col to reshape input patches into matrix columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Reshape convolution weights into matrix for multiplication  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Perform matrix multiplication: output = weight_matrix @ im2col_input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add bias and reshape output to proper 4D convolution shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create Conv2dOp node for gradient computation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> im2col</span><span style=\"color:#E1E4E8\">(input_tensor, kernel_size, stride, padding):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Transform convolution input into matrix multiplication format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate output spatial dimensions after convolution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create output matrix with shape (kernel_h*kernel_w*channels, output_h*output_w*batch)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each output position, extract corresponding input patch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Flatten input patch and store as column in output matrix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle edge cases with padding and stride correctly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MaxPool2d</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"2D max pooling with gradient tracking for non-differentiable max operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply max pooling and store argmax indices for gradient computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Divide input into pooling windows based on kernel_size and stride</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find maximum value in each pooling window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store indices of maximum values for backward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return pooled output with reduced spatial dimensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"serialization-infrastructure\">Serialization Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># serialization.py - Model persistence and loading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelSerializer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles saving and loading complete model state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_model</span><span style=\"color:#E1E4E8\">(model, optimizer, path, metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save complete training state to disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create save directory if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Save model architecture description as JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Save model parameters using numpy .npz format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Save optimizer state (including momentum buffers)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Save training metadata (epoch, loss history, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create manifest file listing all saved components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_model</span><span style=\"color:#E1E4E8\">(path, model_class</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load complete training state from disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Read manifest file to understand save format version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load model architecture and instantiate model object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load parameter tensors and assign to model modules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load optimizer state and create optimizer object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Load training metadata for training resumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Verify loaded state consistency and return components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> model_to_onnx</span><span style=\"color:#E1E4E8\">(model, example_input, output_path):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Export trained model to ONNX format for deployment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Trace model execution with example input to build computation graph</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Map framework operations to equivalent ONNX operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Convert parameter tensors to ONNX tensor format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create ONNX model proto with graph structure and parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate ONNX model produces same outputs as original</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Save ONNX model to specified output path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"performance-monitoring-infrastructure\">Performance Monitoring Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># performance_monitor.py - Complete performance analysis toolkit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FrameworkProfiler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive profiling for neural network framework operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, enable_gpu_profiling</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gpu_utilization </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enable_gpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> enable_gpu_profiling </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> GPU_AVAILABLE</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.profiling_active </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_profiling</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Begin collecting performance metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Reset all metric collections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start background thread for memory/GPU monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Install operation hooks for timing measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set profiling_active flag to enable metric collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> profile_operation</span><span style=\"color:#E1E4E8\">(self, operation_name, func, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Time execution of specific operation and collect metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Record start time and memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute function with provided arguments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Record end time and final memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store timing and memory delta in operation_times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return function result unchanged</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_report</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create comprehensive performance report.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate statistics (mean, std, min, max) for each operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify performance bottlenecks and memory leaks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate plots for timing trends and memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create summary table with optimization recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return formatted report with actionable insights</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"extension-integration-checkpoints\">Extension Integration Checkpoints</h4>\n<p>After implementing performance optimizations:</p>\n<ul>\n<li>Verify GPU operations produce identical results to CPU versions using <code>torch.allclose(cpu_result, gpu_result.cpu(), atol=1e-6)</code></li>\n<li>Measure speedup on matrix multiplication: should see 10-50x improvement for large matrices (1000x1000 or larger)</li>\n<li>Check memory usage with <code>nvidia-smi</code> - GPU memory should increase during computation, decrease after cleanup</li>\n<li>Test operation fusion by comparing execution time of fused vs unfused operation chains</li>\n</ul>\n<p>After implementing advanced neural network features:</p>\n<ul>\n<li>Train CNN on MNIST dataset - should achieve &gt;95% accuracy within 5 epochs</li>\n<li>Train LSTM on simple sequence prediction task - should learn basic patterns within reasonable time  </li>\n<li>Verify batch normalization stabilizes training by comparing loss curves with and without normalization</li>\n<li>Test gradient flow through deep networks (10+ layers) - gradients should not vanish or explode</li>\n</ul>\n<p>After implementing ecosystem integration:</p>\n<ul>\n<li>Save and load trained model, verify identical predictions on test inputs</li>\n<li>Export model to ONNX format, verify ONNX runtime produces same outputs</li>\n<li>Integrate with external data loader, verify proper batch handling and preprocessing</li>\n<li>Use hyperparameter optimization tool to find optimal learning rate automatically</li>\n</ul>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - provides definitions for fundamental concepts used throughout tensor operations, automatic differentiation, neural modules, and training loops</p>\n</blockquote>\n<p>Building a neural network framework introduces a rich vocabulary of interconnected technical concepts. This glossary serves as your reference dictionary, providing precise definitions for every term used throughout the system design. Think of it as the foundation layer beneath all other components - without shared understanding of these concepts, the architectural discussions become ambiguous and implementation becomes inconsistent.</p>\n<p>Each definition here connects to specific implementation elements in our framework. These aren&#39;t abstract academic definitions, but practical explanations of how concepts manifest in working code. The terms are organized thematically to show relationships and build conceptual understanding progressively.</p>\n<h3 id=\"core-tensor-and-automatic-differentiation-concepts\">Core Tensor and Automatic Differentiation Concepts</h3>\n<p>The mathematical and algorithmic foundations that enable neural networks to learn through gradient-based optimization form the conceptual bedrock of our framework.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tensor</strong></td>\n<td>N-dimensional array data structure that stores numerical values with shape metadata and optional gradient tracking. Acts as the fundamental data unit flowing through neural networks.</td>\n<td>Implemented as <code>Tensor</code> class with <code>data: np.ndarray</code>, <code>requires_grad: bool</code>, <code>grad: Optional[Tensor]</code>, and <code>grad_fn: Optional[Operation]</code> fields</td>\n</tr>\n<tr>\n<td><strong>Automatic Differentiation (Autodiff)</strong></td>\n<td>Mechanical computation of exact derivatives by applying the chain rule to elementary operations. Eliminates manual gradient derivation and reduces human error in complex neural networks.</td>\n<td>Core capability provided by <code>Operation</code> classes implementing <code>forward()</code> and <code>backward()</code> methods with computation graph tracking</td>\n</tr>\n<tr>\n<td><strong>Reverse-Mode Autodiff</strong></td>\n<td>Specific autodiff strategy that computes gradients by traversing the computation graph backwards from outputs to inputs. More efficient than forward-mode for neural networks with many parameters and few outputs.</td>\n<td>Implemented through <code>backward()</code> methods that propagate gradients from loss tensor back through <code>grad_fn</code> operation chain</td>\n</tr>\n<tr>\n<td><strong>Computation Graph</strong></td>\n<td>Directed acyclic graph where nodes represent operations and edges represent tensor data flow. Records the history of computations during forward pass to enable gradient computation during backward pass.</td>\n<td>Built dynamically during forward pass as operations create new tensors with <code>grad_fn</code> references to parent operations</td>\n</tr>\n<tr>\n<td><strong>Chain Rule</strong></td>\n<td>Fundamental calculus principle for differentiating composed functions: d/dx[f(g(x))] = f&#39;(g(x)) × g&#39;(x). Enables automatic differentiation by breaking complex expressions into elementary operations.</td>\n<td>Applied in each <code>Operation.backward()</code> method which multiplies incoming gradients by local gradients before propagating to inputs</td>\n</tr>\n<tr>\n<td><strong>Broadcasting</strong></td>\n<td>Automatic shape expansion mechanism that allows operations between tensors of different but compatible dimensions. Follows NumPy rules to avoid explicit tensor reshaping in user code.</td>\n<td>Implemented in tensor arithmetic operations with corresponding gradient unbroadcasting in backward pass to maintain correct parameter shapes</td>\n</tr>\n<tr>\n<td><strong>Gradient Accumulation</strong></td>\n<td>Process of summing gradients when a tensor participates in multiple operations within the same computation graph. Prevents gradient loss in networks with parameter sharing or complex connectivity.</td>\n<td>Handled automatically by <code>Tensor.backward()</code> which accumulates gradients: <code>self.grad = gradient if self.grad is None else self.grad + gradient</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"neural-network-architecture-concepts\">Neural Network Architecture Concepts</h3>\n<p>The building blocks and organizational patterns that structure neural networks into learnable, composable systems.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Module System</strong></td>\n<td>Hierarchical organization pattern where neural network components are composable building blocks containing parameters and submodules. Enables complex architectures through simple composition.</td>\n<td>Implemented through <code>Module</code> base class with <code>_parameters: Dict[str, Tensor]</code> and <code>_modules: Dict[str, Module]</code> collections</td>\n</tr>\n<tr>\n<td><strong>Parameter Registration</strong></td>\n<td>Automatic tracking mechanism that identifies and collects trainable tensors within modules. Ensures optimizers can find all parameters requiring gradient updates.</td>\n<td>Managed by <code>register_parameter(name, param)</code> method which validates tensor properties and adds to <code>_parameters</code> dictionary</td>\n</tr>\n<tr>\n<td><strong>Recursive Parameter Collection</strong></td>\n<td>Depth-first traversal algorithm that gathers parameters from nested module hierarchies. Enables complex architectures while maintaining simple optimizer interfaces.</td>\n<td>Implemented in <code>parameters()</code> method which yields own parameters then recursively calls <code>parameters()</code> on all submodules</td>\n</tr>\n<tr>\n<td><strong>Weight Initialization</strong></td>\n<td>Strategy for setting initial parameter values that promote stable training dynamics. Critical for avoiding vanishing/exploding gradients in deep networks.</td>\n<td>Provided through initialization functions like <code>xavier_uniform_(tensor, gain)</code> and <code>kaiming_uniform_(tensor, a, mode)</code></td>\n</tr>\n<tr>\n<td><strong>Xavier Initialization</strong></td>\n<td>Weight initialization scheme that maintains activation variance across layers by scaling initial weights based on fan-in and fan-out dimensions. Particularly effective for sigmoid/tanh activations.</td>\n<td>Draws weights from uniform distribution with bounds ±√(6/(fan_in + fan_out)) scaled by activation gain</td>\n</tr>\n<tr>\n<td><strong>He Initialization</strong></td>\n<td>Weight initialization optimized for ReLU activations that scales weights based on fan-in dimension only. Compensates for ReLU&#39;s activation pattern to maintain signal variance.</td>\n<td>Draws weights from uniform distribution with bounds ±√(6/fan_in) to account for ReLU&#39;s zero-clipping behavior</td>\n</tr>\n<tr>\n<td><strong>Sequential Container</strong></td>\n<td>Module that chains other modules in linear order, passing output of each module as input to the next. Simplifies construction of feedforward architectures.</td>\n<td>Implemented as <code>Sequential</code> class storing ordered list of modules and forwarding data through each sequentially</td>\n</tr>\n<tr>\n<td><strong>Mode Switching</strong></td>\n<td>Mechanism for changing neural network behavior between training and evaluation phases. Affects layers like dropout and batch normalization that behave differently during inference.</td>\n<td>Controlled by <code>training: bool</code> flag with <code>train(mode=True)</code> and <code>eval()</code> methods propagating state to all submodules</td>\n</tr>\n<tr>\n<td><strong>Hierarchical Naming</strong></td>\n<td>Dot notation system for identifying parameters and submodules within complex architectures. Enables targeted parameter access and debugging.</td>\n<td>Generated by <code>named_parameters(prefix)</code> which concatenates module names with dots to create unique parameter paths</td>\n</tr>\n</tbody></table>\n<h3 id=\"optimization-and-training-concepts\">Optimization and Training Concepts</h3>\n<p>The algorithms and processes that enable neural networks to learn from data through iterative parameter adjustment.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Optimizer</strong></td>\n<td>Algorithm that updates model parameters using gradients to minimize loss function. Encapsulates update rules, learning rates, and momentum strategies.</td>\n<td>Implemented as <code>Optimizer</code> base class with <code>param_groups: List[Dict]</code> and <code>state: Dict[int, Dict]</code> for parameter and algorithm state management</td>\n</tr>\n<tr>\n<td><strong>Stochastic Gradient Descent (SGD)</strong></td>\n<td>Fundamental optimization algorithm that updates parameters in the negative gradient direction scaled by learning rate. Foundation for more sophisticated optimizers.</td>\n<td>Implemented as <code>SGD</code> class with momentum support: parameter update follows <code>param -= lr * (momentum * velocity + gradient)</code></td>\n</tr>\n<tr>\n<td><strong>Momentum</strong></td>\n<td>Velocity accumulation technique that dampens oscillations and accelerates convergence by incorporating previous gradient directions. Helps optimization escape local minima.</td>\n<td>Stored in optimizer state as velocity buffers: <code>velocity = momentum * velocity + gradient</code> before applying parameter update</td>\n</tr>\n<tr>\n<td><strong>Adam</strong></td>\n<td>Adaptive moment estimation optimizer that maintains per-parameter learning rates based on first and second moment estimates of gradients. Often converges faster than SGD.</td>\n<td>Combines momentum-like first moments and RMSprop-like second moments with bias correction: <code>param -= lr * m_hat / (√v_hat + ε)</code></td>\n</tr>\n<tr>\n<td><strong>Bias Correction</strong></td>\n<td>Adjustment technique in Adam optimizer that compensates for initialization bias in moment estimates during early training iterations. Critical for proper convergence behavior.</td>\n<td>Applied as <code>m_hat = m / (1 - beta1^t)</code> and <code>v_hat = v / (1 - beta2^t)</code> where t is iteration count</td>\n</tr>\n<tr>\n<td><strong>Learning Rate Scheduling</strong></td>\n<td>Dynamic adjustment of learning rate during training based on metrics or predefined schedules. Balances exploration and convergence as training progresses.</td>\n<td>Implemented through <code>LRScheduler</code> classes that modify optimizer learning rates based on training progress</td>\n</tr>\n<tr>\n<td><strong>Mini-Batch Training</strong></td>\n<td>Processing fixed-size subsets of training data rather than individual samples or entire datasets. Balances computational efficiency with gradient estimate quality.</td>\n<td>Managed by <code>DataLoader</code> with <code>batch_size: int</code> and <code>shuffle: bool</code> parameters controlling data batching and randomization</td>\n</tr>\n<tr>\n<td><strong>Parameter Updates</strong></td>\n<td>Modification of model weights using optimizer-computed updates based on accumulated gradients. The core mechanism through which neural networks learn.</td>\n<td>Coordinated by <code>step()</code> method which applies computed updates to all parameters in optimizer&#39;s parameter groups</td>\n</tr>\n</tbody></table>\n<h3 id=\"training-loop-and-coordination-concepts\">Training Loop and Coordination Concepts</h3>\n<p>The orchestration and management systems that coordinate the interaction between all framework components during the learning process.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Training Loop</strong></td>\n<td>Orchestrated sequence of forward pass, loss computation, backward pass, and parameter updates repeated over multiple epochs. The central process driving neural network learning.</td>\n<td>Implemented in <code>train_epoch()</code> function coordinating model, data loader, optimizer, and loss function interactions</td>\n</tr>\n<tr>\n<td><strong>Forward Pass Data Flow</strong></td>\n<td>Process where input data flows through network modules while building computation graph for gradient tracking. Produces predictions and prepares for gradient computation.</td>\n<td>Managed by <code>coordinate_forward_pass()</code> which calls <code>model(inputs)</code> while tracking tensor operations and building computation graph</td>\n</tr>\n<tr>\n<td><strong>Backward Pass Coordination</strong></td>\n<td>Gradient computation flow from loss back through network to compute parameter gradients. Follows computation graph in reverse topological order.</td>\n<td>Initiated by <code>loss.backward()</code> which triggers <code>coordinate_backward_pass()</code> managing gradient computation and validation</td>\n</tr>\n<tr>\n<td><strong>Complete Training Step Sequence</strong></td>\n<td>End-to-end training process: forward pass → loss computation → backward pass → parameter updates. Represents one iteration of the learning algorithm.</td>\n<td>Implemented in <code>training_step()</code> method returning metrics and coordinating all training components</td>\n</tr>\n<tr>\n<td><strong>Loss Function</strong></td>\n<td>Differentiable measure of prediction quality that quantifies difference between model outputs and target values. Provides gradient signal for optimization.</td>\n<td>Implemented through <code>Loss</code> base class with subclasses like cross-entropy and mean squared error supporting gradient computation</td>\n</tr>\n<tr>\n<td><strong>Cross-Entropy Loss</strong></td>\n<td>Standard loss function for classification tasks that measures divergence between predicted probability distributions and target labels. Well-suited for softmax outputs.</td>\n<td>Computes <code>-∑ target_i * log(prediction_i)</code> with numerical stability handling and proper gradient computation</td>\n</tr>\n<tr>\n<td><strong>Mean Squared Error</strong></td>\n<td>Standard loss function for regression tasks that penalizes squared differences between predictions and targets. Provides smooth gradients for continuous optimization.</td>\n<td>Computes <code>mean((predictions - targets)²)</code> with straightforward gradient: <code>2 * (predictions - targets) / batch_size</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"error-handling-and-debugging-concepts\">Error Handling and Debugging Concepts</h3>\n<p>The systems and strategies for detecting, diagnosing, and recovering from failures during neural network development and training.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Shape Mismatch</strong></td>\n<td>Error condition where tensor dimensions are incompatible for requested operations. Common source of failures in neural network implementations.</td>\n<td>Detected by operations before computation with descriptive <code>ShapeError</code> containing <code>shapes: List[Tuple]</code> and <code>operation: str</code></td>\n</tr>\n<tr>\n<td><strong>Broadcasting Failure</strong></td>\n<td>Specific type of shape mismatch where tensors cannot be automatically expanded according to broadcasting rules. Indicates incompatible tensor dimensions.</td>\n<td>Handled by <code>BroadcastingError</code> with <code>shape1: Tuple</code>, <code>shape2: Tuple</code>, and <code>step_analysis: List[str]</code> for diagnostic information</td>\n</tr>\n<tr>\n<td><strong>Gradient Explosion</strong></td>\n<td>Pathological training condition where gradient magnitudes become extremely large, leading to unstable parameter updates and training failure.</td>\n<td>Detected by <code>GradientMonitor</code> tracking gradient norms and triggering alerts when values exceed reasonable thresholds</td>\n</tr>\n<tr>\n<td><strong>Gradient Vanishing</strong></td>\n<td>Training problem where gradient magnitudes become extremely small, preventing effective learning in deep networks. Often caused by activation functions or weight initialization.</td>\n<td>Identified through gradient magnitude analysis in <code>check_gradient_flow()</code> which compares gradients across network layers</td>\n</tr>\n<tr>\n<td><strong>Numerical Instability</strong></td>\n<td>Computational condition where operations produce NaN (Not a Number) or infinity values, breaking subsequent computations. Requires careful numerical handling.</td>\n<td>Caught by <code>validate_tensor_finite()</code> checks and reported through <code>NumericalInstabilityError</code> with tensor statistics</td>\n</tr>\n<tr>\n<td><strong>Computation Graph Memory Leak</strong></td>\n<td>Memory management issue where circular references in computation graphs prevent garbage collection. Can exhaust memory during long training runs.</td>\n<td>Prevented by <code>ComputationGraphTracker</code> managing graph lifecycle and <code>cleanup_graphs()</code> breaking circular references</td>\n</tr>\n<tr>\n<td><strong>Memory Management</strong></td>\n<td>Systematic approach to controlling memory usage during training, including computation graph cleanup and tensor lifecycle management. Critical for training stability.</td>\n<td>Implemented through <code>MemoryMonitor</code> tracking usage patterns and automatic cleanup systems preventing memory exhaustion</td>\n</tr>\n</tbody></table>\n<h3 id=\"testing-and-validation-concepts\">Testing and Validation Concepts</h3>\n<p>The methodologies and tools for verifying correctness of neural network framework implementations.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Gradient Correctness Testing</strong></td>\n<td>Validation technique comparing automatic differentiation results with numerical differentiation to verify implementation accuracy. Essential for debugging autodiff systems.</td>\n<td>Implemented in <code>GradientTester</code> class using finite differences to validate <code>Operation.backward()</code> implementations</td>\n</tr>\n<tr>\n<td><strong>Numerical Differentiation</strong></td>\n<td>Finite difference approximation of derivatives using small perturbations: f&#39;(x) ≈ [f(x+h) - f(x-h)]/(2h). Provides ground truth for gradient validation.</td>\n<td>Used in <code>numerical_gradient()</code> function with configurable step size <code>h=1e-5</code> for comparing against autodiff results</td>\n</tr>\n<tr>\n<td><strong>Central Difference</strong></td>\n<td>Specific numerical differentiation method using symmetric perturbations around evaluation point. More accurate than forward/backward differences for gradient checking.</td>\n<td>Applied as <code>[f(x+h) - f(x-h)] / (2h)</code> in gradient validation providing second-order accuracy</td>\n</tr>\n<tr>\n<td><strong>Relative Error</strong></td>\n<td>Scale-invariant comparison metric for gradient accuracy: `</td>\n<td>computed - expected</td>\n</tr>\n<tr>\n<td><strong>Milestone Verification</strong></td>\n<td>Integration testing after each major development stage to confirm framework components work correctly together. Prevents compound errors in complex systems.</td>\n<td>Implemented as <code>MilestoneCheckpoints</code> class with automated verification for each project milestone</td>\n</tr>\n<tr>\n<td><strong>End-to-End Training Tests</strong></td>\n<td>Complete neural network training on toy problems with known expected behaviors. Validates entire framework integration from data loading to convergence.</td>\n<td>Executed by <code>EndToEndTester</code> using synthetic datasets like XOR problem and linear regression with convergence validation</td>\n</tr>\n<tr>\n<td><strong>Toy Datasets</strong></td>\n<td>Synthetic problems with known solutions designed for testing framework correctness. Include linear regression, XOR classification, and overfitting tests.</td>\n<td>Generated by <code>ToyDatasets</code> methods creating predictable data patterns for systematic framework validation</td>\n</tr>\n<tr>\n<td><strong>Statistical Validation</strong></td>\n<td>Testing approach using multiple random seeds and statistical analysis to ensure consistent framework behavior across different initialization conditions.</td>\n<td>Implemented in <code>run_statistical_validation()</code> executing tests multiple times and analyzing result distributions</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-and-extension-concepts\">Performance and Extension Concepts</h3>\n<p>Advanced features and optimizations that extend the basic framework capabilities for production use and specialized applications.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>GPU Acceleration</strong></td>\n<td>Massively parallel computation using graphics processors to accelerate tensor operations. Essential for training large neural networks efficiently.</td>\n<td>Supported through <code>GPUTensor</code> class with <code>device: str</code> attribute and <code>to(device)</code> method for CPU/GPU transfers</td>\n</tr>\n<tr>\n<td><strong>Operation Fusion</strong></td>\n<td>Optimization technique combining multiple elementary operations into single optimized kernels. Reduces memory bandwidth and improves computational efficiency.</td>\n<td>Implemented by <code>FusionOptimizer</code> analyzing computation graphs for <code>fusion_patterns: List</code> and applying transformations</td>\n</tr>\n<tr>\n<td><strong>Mixed Precision Training</strong></td>\n<td>Memory efficiency technique using 16-bit floating point for most operations while maintaining 32-bit precision for gradient computation. Accelerates training on modern hardware.</td>\n<td>Requires specialized tensor types and automatic loss scaling to prevent gradient underflow</td>\n</tr>\n<tr>\n<td><strong>Model Serialization</strong></td>\n<td>Saving and loading trained model state including parameters, optimizer state, and training metadata. Essential for model deployment and checkpoint recovery.</td>\n<td>Provided by <code>ModelSerializer</code> with <code>save_model()</code> and <code>load_model()</code> methods handling complete training state</td>\n</tr>\n<tr>\n<td><strong>ONNX Export</strong></td>\n<td>Converting models to Open Neural Network Exchange format for interoperability with other frameworks and deployment platforms. Enables production integration.</td>\n<td>Implemented through <code>model_to_onnx()</code> function translating framework operations to standardized representation</td>\n</tr>\n<tr>\n<td><strong>Ecosystem Integration</strong></td>\n<td>Connecting framework with existing machine learning tools, data pipelines, and deployment infrastructure. Bridges educational implementation with practical applications.</td>\n<td>Includes compatibility layers, data format converters, and API adapters for common ML workflows</td>\n</tr>\n</tbody></table>\n<h3 id=\"framework-specific-implementation-terms\">Framework-Specific Implementation Terms</h3>\n<p>Terminology specific to our educational neural network framework design and implementation choices.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Framework Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Educational Clarity</strong></td>\n<td>Design principle prioritizing code readability and conceptual understanding over maximum performance optimization. Guides architectural decisions throughout framework.</td>\n<td>Influences API design, error messages, internal documentation, and implementation complexity trade-offs</td>\n</tr>\n<tr>\n<td><strong>Four-Layer Architecture</strong></td>\n<td>Framework organization with distinct layers: tensor operations, autodiff engine, neural modules, and optimizers. Provides clear separation of concerns and learning progression.</td>\n<td>Reflected in project structure with separate modules for each layer and well-defined interfaces between them</td>\n</tr>\n<tr>\n<td><strong>Define-by-Run</strong></td>\n<td>Dynamic computation graph construction during forward pass execution. Enables flexible control flow and debugging compared to static graph approaches.</td>\n<td>Implemented through immediate operation execution with automatic graph building via <code>grad_fn</code> references</td>\n</tr>\n<tr>\n<td><strong>Eager Execution</strong></td>\n<td>Immediate evaluation of operations as they are called rather than building static computation graphs. Simplifies debugging and enables Python control flow.</td>\n<td>Default behavior where tensor operations execute immediately while maintaining gradient tracking capabilities</td>\n</tr>\n<tr>\n<td><strong>Scope Creep</strong></td>\n<td>Uncontrolled addition of features beyond core learning objectives that can overwhelm learners and complicate implementation. Must be actively managed in educational projects.</td>\n<td>Prevented through explicit non-goals and milestone-focused development restricting feature additions</td>\n</tr>\n<tr>\n<td><strong>Topological Sort</strong></td>\n<td>Graph algorithm ensuring correct traversal order during backward pass gradient computation. Critical for proper gradient flow in complex computation graphs.</td>\n<td>Implemented in <code>topological_sort()</code> function providing dependency-ordered sequence of operations for backward pass</td>\n</tr>\n</tbody></table>\n<h3 id=\"constants-and-configuration-values\">Constants and Configuration Values</h3>\n<p>Standard values and thresholds used throughout the framework for numerical stability, testing, and monitoring.</p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Value</th>\n<th>Purpose</th>\n<th>Usage Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>tolerance</strong></td>\n<td><code>1e-5</code></td>\n<td>Default tolerance for gradient checking and numerical comparisons</td>\n<td>Used in <code>check_gradients()</code> and relative error computations</td>\n</tr>\n<tr>\n<td><strong>h</strong></td>\n<td><code>1e-5</code></td>\n<td>Step size for numerical differentiation in gradient validation</td>\n<td>Applied in <code>numerical_gradient()</code> finite difference computations</td>\n</tr>\n<tr>\n<td><strong>epsilon</strong></td>\n<td><code>1e-8</code></td>\n<td>Small constant for numerical stability in optimizers and computations</td>\n<td>Used in Adam optimizer denominator and division-by-zero prevention</td>\n</tr>\n<tr>\n<td><strong>beta1</strong></td>\n<td><code>0.9</code></td>\n<td>Exponential decay rate for first moment estimates in Adam optimizer</td>\n<td>Default value for momentum-like term in adaptive optimization</td>\n</tr>\n<tr>\n<td><strong>beta2</strong></td>\n<td><code>0.999</code></td>\n<td>Exponential decay rate for second moment estimates in Adam optimizer</td>\n<td>Default value for variance-like term in adaptive optimization</td>\n</tr>\n<tr>\n<td><strong>learning_rate</strong></td>\n<td><code>0.01</code></td>\n<td>Default step size multiplier for parameter updates</td>\n<td>Base value for SGD and initial rate for other optimizers</td>\n</tr>\n<tr>\n<td><strong>momentum</strong></td>\n<td><code>0.9</code></td>\n<td>Default coefficient for velocity accumulation in SGD with momentum</td>\n<td>Balances current gradient with historical direction</td>\n</tr>\n<tr>\n<td><strong>check_frequency</strong></td>\n<td><code>10</code></td>\n<td>Sampling rate for gradient monitoring and validation during training</td>\n<td>Determines how often gradient health checks are performed</td>\n</tr>\n<tr>\n<td><strong>warning_threshold</strong></td>\n<td><code>0.8</code></td>\n<td>Memory usage fraction triggering warnings in memory monitoring</td>\n<td>Used by <code>MemoryMonitor</code> to alert before critical memory pressure</td>\n</tr>\n<tr>\n<td><strong>critical_threshold</strong></td>\n<td><code>0.9</code></td>\n<td>Memory usage fraction triggering critical alerts and cleanup</td>\n<td>Threshold for forced garbage collection and graph cleanup</td>\n</tr>\n<tr>\n<td><strong>success_rate_threshold</strong></td>\n<td><code>0.8</code></td>\n<td>Minimum success rate for statistical validation test passage</td>\n<td>Used in multi-seed testing to determine overall test success</td>\n</tr>\n</tbody></table>\n<h3 id=\"error-types-and-exception-hierarchy\">Error Types and Exception Hierarchy</h3>\n<p>Structured error handling system providing detailed diagnostic information for common failure modes in neural network frameworks.</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Hierarchy</th>\n<th>Key Fields</th>\n<th>Usage Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>NeuralFrameworkError</strong></td>\n<td>Base class</td>\n<td><code>message: str</code>, <code>details: Dict</code>, <code>stack_info: List</code></td>\n<td>Root exception type for all framework-specific errors</td>\n</tr>\n<tr>\n<td><strong>ShapeError</strong></td>\n<td>Extends NeuralFrameworkError</td>\n<td><code>shapes: List[Tuple]</code>, <code>operation: str</code></td>\n<td>Tensor dimension incompatibility in operations</td>\n</tr>\n<tr>\n<td><strong>BroadcastingError</strong></td>\n<td>Extends ShapeError</td>\n<td><code>shape1: Tuple</code>, <code>shape2: Tuple</code>, <code>step_analysis: List[str]</code></td>\n<td>Broadcasting rule violations with detailed analysis</td>\n</tr>\n<tr>\n<td><strong>GradientError</strong></td>\n<td>Extends NeuralFrameworkError</td>\n<td>Base fields plus gradient-specific context</td>\n<td>Base class for all gradient computation failures</td>\n</tr>\n<tr>\n<td><strong>NumericalInstabilityError</strong></td>\n<td>Extends GradientError</td>\n<td><code>tensor_info: Dict</code> with <code>nan_count</code>, <code>inf_count</code></td>\n<td>NaN/infinity detection in computations</td>\n</tr>\n</tbody></table>\n<p>This comprehensive glossary serves as both reference documentation and conceptual foundation. Each term connects directly to implementation elements, making it a practical guide for both understanding and implementing the neural network framework. The definitions progress from fundamental mathematical concepts through architectural patterns to practical implementation details, mirroring the learning journey from theory to working code.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The glossary itself doesn&#39;t require implementation code, but maintaining consistent terminology throughout your framework is critical for code clarity and debugging effectiveness. Here&#39;s how to integrate these concepts into your implementation:</p>\n<p><strong>A. Technology Recommendations for Documentation:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Code Documentation</td>\n<td>Docstrings with type hints</td>\n<td>Sphinx with automatic API generation</td>\n</tr>\n<tr>\n<td>Error Messages</td>\n<td>String formatting with context</td>\n<td>Structured error objects with detailed diagnostics</td>\n</tr>\n<tr>\n<td>Debugging Output</td>\n<td>Print statements with prefixes</td>\n<td>Logging framework with levels and formatting</td>\n</tr>\n<tr>\n<td>Type Checking</td>\n<td>Manual validation in methods</td>\n<td>Static type checker like mypy</td>\n</tr>\n</tbody></table>\n<p><strong>B. Terminology Consistency Enforcement:</strong></p>\n<p>Create a constants file that defines all standard terminology to prevent naming inconsistencies:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># framework/constants.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Central definitions for all framework terminology and constants.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Ensures consistent naming across modules.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Numerical constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_TOLERANCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">NUMERICAL_DIFF_STEP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EPSILON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-8</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Optimizer defaults</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_LEARNING_RATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_MOMENTUM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.9</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">ADAM_BETA1</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.9</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">ADAM_BETA2</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.999</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Monitoring thresholds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">GRADIENT_CHECK_FREQUENCY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_WARNING_THRESHOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.8</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_CRITICAL_THRESHOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.9</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Error message templates</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SHAPE_MISMATCH_MSG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"Shape mismatch in </span><span style=\"color:#79B8FF\">{operation}</span><span style=\"color:#9ECBFF\">: expected </span><span style=\"color:#79B8FF\">{expected}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{actual}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">BROADCASTING_FAILED_MSG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"Cannot broadcast shapes </span><span style=\"color:#79B8FF\">{shape1}</span><span style=\"color:#9ECBFF\"> and </span><span style=\"color:#79B8FF\">{shape2}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{reason}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">GRADIENT_EXPLOSION_MSG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"Gradient explosion detected: max gradient norm </span><span style=\"color:#79B8FF\">{max_norm</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Field names (prevent typos in attribute access)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">REQUIRES_GRAD_FIELD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"requires_grad\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">GRAD_FN_FIELD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"grad_fn\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DATA_FIELD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"data\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">PARAMETERS_FIELD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"_parameters\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MODULES_FIELD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"_modules\"</span></span></code></pre></div>\n\n<p><strong>C. Error Message Infrastructure:</strong></p>\n<p>Build a diagnostic system that uses consistent terminology in error reporting:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># framework/errors.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Structured error handling with consistent terminology.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NeuralFrameworkError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all framework-specific errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, details: Dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, operation_context: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store message, details dict, and operation context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Capture stack trace information for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format error message with consistent terminology</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._format_message())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_message</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Combine message, details, and context into readable format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include relevant tensor shapes, operation names, parameter values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Suggest potential fixes based on error type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ShapeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tensor shape incompatibility errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, shapes: List[Tuple], operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store shape information and operation context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate detailed shape analysis showing incompatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Suggest shape corrections (transpose, reshape, broadcast)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NeuralFrameworkError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for gradient computation failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tensor_info: Dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, gradient_context: Dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Capture gradient-specific diagnostic information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include gradient magnitudes, tensor shapes, operation chain</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Provide debugging suggestions for gradient flow issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>D. Debugging Integration:</strong></p>\n<p>Create debugging utilities that use consistent terminology for problem diagnosis:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># framework/debugging.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Debugging utilities with consistent terminology and detailed diagnostics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_gradient_flow</span><span style=\"color:#E1E4E8\">(model: Module, loss: Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Check gradient flow through all model parameters using standard terminology.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns diagnostic information with consistent field names.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Iterate through model.named_parameters() </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check each parameter's .grad field for None, zeros, or extreme values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Report gradient norms using consistent magnitude classifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify layers with gradient flow problems using standard descriptions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> analyze_tensor_shapes</span><span style=\"color:#E1E4E8\">(tensors: List[Tensor], operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Analyze tensor shape compatibility using framework terminology.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check shapes against broadcasting rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate step-by-step broadcasting analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Suggest fixes using standard reshape/transpose terminology</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return structured analysis with consistent field names</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_numerical_stability</span><span style=\"color:#E1E4E8\">(tensor: Tensor, operation_context: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validate tensor contains finite values using standard stability checks.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Count NaN, infinity, and extreme values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check gradient magnitudes against standard thresholds  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate stability report with consistent terminology</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Suggest numerical fixes (gradient clipping, learning rate adjustment)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>E. Milestone Checkpoints with Terminology Validation:</strong></p>\n<p>Ensure your implementation uses correct terminology by building verification into milestone tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/test_terminology.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Verify consistent terminology usage across framework implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_tensor_field_names</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify Tensor class uses exact field names from glossary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create Tensor instance and verify it has 'data', 'requires_grad', 'grad', 'grad_fn' fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check field types match specifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify no extra or missing fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_module_interface_consistency</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify Module classes use standard method names and signatures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check Module has 'parameters()', 'named_parameters()', 'train()', 'eval()' methods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify method signatures match glossary specifications  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test parameter registration uses '_parameters' and '_modules' fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_message_terminology</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify error messages use consistent terminology from glossary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Trigger shape mismatch and check error message uses standard terms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test broadcasting failure uses correct shape analysis terminology</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify gradient errors use standard gradient flow descriptions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>F. Documentation Generation:</strong></p>\n<p>Use the glossary terms to generate consistent API documentation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># docs/generate_glossary.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Generate framework documentation using glossary terminology.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_api_docs</span><span style=\"color:#E1E4E8\">(module_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generate API documentation with glossary term definitions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract class and method definitions from module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Cross-reference with glossary definitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate documentation with consistent terminology</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include links between related concepts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_docstring_terminology</span><span style=\"color:#E1E4E8\">(module_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Check docstrings use terminology consistent with glossary.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse all docstrings in module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check against approved terminology list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Report inconsistencies with suggested corrections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate terminology compliance report</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p>This implementation guidance ensures your neural network framework maintains terminological consistency that matches the comprehensive definitions in this glossary. Consistent terminology is crucial for debugging, collaboration, and educational clarity.</p>\n","toc":[{"level":1,"text":"Neural Network Framework: Design Document","id":"neural-network-framework-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"The Recipe Book Analogy","id":"the-recipe-book-analogy"},{"level":3,"text":"The Gradient Computation Challenge","id":"the-gradient-computation-challenge"},{"level":3,"text":"Existing Framework Comparison","id":"existing-framework-comparison"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Functional Requirements","id":"functional-requirements"},{"level":3,"text":"Performance and Quality Goals","id":"performance-and-quality-goals"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Four-Layer Architecture","id":"four-layer-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":4,"text":"Tensor Operations Layer Responsibilities","id":"tensor-operations-layer-responsibilities"},{"level":4,"text":"Automatic Differentiation Engine Responsibilities","id":"automatic-differentiation-engine-responsibilities"},{"level":4,"text":"Neural Network Modules Layer Responsibilities","id":"neural-network-modules-layer-responsibilities"},{"level":4,"text":"Optimizers and Training Layer Responsibilities","id":"optimizers-and-training-layer-responsibilities"},{"level":4,"text":"Inter-Layer Communication Patterns","id":"inter-layer-communication-patterns"},{"level":3,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Directory Layout","id":"directory-layout"},{"level":4,"text":"Package Organization Rationale","id":"package-organization-rationale"},{"level":4,"text":"Import Structure and API Design","id":"import-structure-and-api-design"},{"level":4,"text":"Development Workflow Organization","id":"development-workflow-organization"},{"level":4,"text":"Testing Strategy Integration","id":"testing-strategy-integration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure Setup","id":"recommended-project-structure-setup"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Tensor Data Structure","id":"tensor-data-structure"},{"level":3,"text":"Computation Graph Representation","id":"computation-graph-representation"},{"level":3,"text":"Parameter and Module Hierarchy","id":"parameter-and-module-hierarchy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Tensor Operations Layer (Milestone 1)","id":"tensor-operations-layer-milestone-1"},{"level":3,"text":"Tensor as Smart Arrays","id":"tensor-as-smart-arrays"},{"level":3,"text":"Tensor API Design","id":"tensor-api-design"},{"level":3,"text":"Broadcasting Implementation","id":"broadcasting-implementation"},{"level":3,"text":"Tensor Design Decisions","id":"tensor-design-decisions"},{"level":3,"text":"Common Tensor Implementation Pitfalls","id":"common-tensor-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Automatic Differentiation Engine (Milestone 2)","id":"automatic-differentiation-engine-milestone-2"},{"level":3,"text":"The Assembly Line Metaphor","id":"the-assembly-line-metaphor"},{"level":3,"text":"Forward Pass Graph Building","id":"forward-pass-graph-building"},{"level":3,"text":"Reverse-Mode Differentiation Algorithm","id":"reverse-mode-differentiation-algorithm"},{"level":3,"text":"Gradient Accumulation Strategy","id":"gradient-accumulation-strategy"},{"level":3,"text":"Autodiff Architecture Decisions","id":"autodiff-architecture-decisions"},{"level":3,"text":"Common Autodiff Pitfalls","id":"common-autodiff-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Neural Network Modules (Milestone 3)","id":"neural-network-modules-milestone-3"},{"level":3,"text":"Modules as LEGO Blocks","id":"modules-as-lego-blocks"},{"level":3,"text":"Module Base Class Design","id":"module-base-class-design"},{"level":3,"text":"Linear Layer Implementation","id":"linear-layer-implementation"},{"level":3,"text":"Activation Function Modules","id":"activation-function-modules"},{"level":3,"text":"Parameter Registration System","id":"parameter-registration-system"},{"level":3,"text":"Module System Architecture Decisions","id":"module-system-architecture-decisions"},{"level":3,"text":"Common Module Implementation Pitfalls","id":"common-module-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Optimizers and Training Loop (Milestone 4)","id":"optimizers-and-training-loop-milestone-4"},{"level":3,"text":"Optimizers as GPS Navigation","id":"optimizers-as-gps-navigation"},{"level":3,"text":"Stochastic Gradient Descent","id":"stochastic-gradient-descent"},{"level":3,"text":"Adam Optimizer","id":"adam-optimizer"},{"level":3,"text":"Training Loop Architecture","id":"training-loop-architecture"},{"level":3,"text":"Loss Function Implementation","id":"loss-function-implementation"},{"level":3,"text":"Training Architecture Decisions","id":"training-architecture-decisions"},{"level":3,"text":"Common Training Pitfalls","id":"common-training-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Forward Pass Data Flow","id":"forward-pass-data-flow"},{"level":3,"text":"Backward Pass Coordination","id":"backward-pass-coordination"},{"level":3,"text":"Complete Training Step Sequence","id":"complete-training-step-sequence"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"The Quality Control Analogy","id":"the-quality-control-analogy"},{"level":3,"text":"Shape and Broadcasting Errors","id":"shape-and-broadcasting-errors"},{"level":4,"text":"Shape Compatibility Validation","id":"shape-compatibility-validation"},{"level":4,"text":"Broadcasting Error Diagnostics","id":"broadcasting-error-diagnostics"},{"level":4,"text":"Gradient Shape Validation","id":"gradient-shape-validation"},{"level":4,"text":"Common Shape Error Pitfalls","id":"common-shape-error-pitfalls"},{"level":3,"text":"Gradient-Related Problems","id":"gradient-related-problems"},{"level":4,"text":"Gradient Explosion Detection and Mitigation","id":"gradient-explosion-detection-and-mitigation"},{"level":4,"text":"Vanishing Gradient Detection","id":"vanishing-gradient-detection"},{"level":4,"text":"NaN and Infinity Handling","id":"nan-and-infinity-handling"},{"level":4,"text":"Gradient Validation and Testing","id":"gradient-validation-and-testing"},{"level":4,"text":"Common Gradient Implementation Pitfalls","id":"common-gradient-implementation-pitfalls"},{"level":3,"text":"Memory and Resource Management","id":"memory-and-resource-management"},{"level":4,"text":"Computation Graph Memory Leaks","id":"computation-graph-memory-leaks"},{"level":4,"text":"Memory-Efficient Graph Construction","id":"memory-efficient-graph-construction"},{"level":4,"text":"Large Tensor Allocation Strategies","id":"large-tensor-allocation-strategies"},{"level":4,"text":"Resource Monitoring and Alerts","id":"resource-monitoring-and-alerts"},{"level":4,"text":"Common Memory Management Pitfalls","id":"common-memory-management-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Error Handling Infrastructure","id":"core-error-handling-infrastructure"},{"level":4,"text":"Broadcasting Validation and Diagnostics","id":"broadcasting-validation-and-diagnostics"},{"level":4,"text":"Gradient Validation System","id":"gradient-validation-system"},{"level":4,"text":"Memory Management Infrastructure","id":"memory-management-infrastructure"},{"level":4,"text":"Training Monitoring and Alerts","id":"training-monitoring-and-alerts"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Gradient Correctness Testing","id":"gradient-correctness-testing"},{"level":3,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":4,"text":"Milestone 1: Tensor &amp; Operations Verification","id":"milestone-1-tensor-amp-operations-verification"},{"level":4,"text":"Milestone 2: Automatic Differentiation Verification","id":"milestone-2-automatic-differentiation-verification"},{"level":4,"text":"Milestone 3: Neural Network Modules Verification","id":"milestone-3-neural-network-modules-verification"},{"level":4,"text":"Milestone 4: Optimizers &amp; Training Verification","id":"milestone-4-optimizers-amp-training-verification"},{"level":3,"text":"End-to-End Training Tests","id":"end-to-end-training-tests"},{"level":4,"text":"Toy Dataset Design Principles","id":"toy-dataset-design-principles"},{"level":4,"text":"Training Convergence Verification","id":"training-convergence-verification"},{"level":4,"text":"Framework Integration Testing","id":"framework-integration-testing"},{"level":4,"text":"Specific End-to-End Test Cases","id":"specific-end-to-end-test-cases"},{"level":4,"text":"Automated Test Infrastructure","id":"automated-test-infrastructure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Testing Structure","id":"recommended-testing-structure"},{"level":4,"text":"Core Testing Infrastructure (Complete Implementation)","id":"core-testing-infrastructure-complete-implementation"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Gradient Computation Issues","id":"gradient-computation-issues"},{"level":3,"text":"Shape and Broadcasting Bugs","id":"shape-and-broadcasting-bugs"},{"level":3,"text":"Training Loop Problems","id":"training-loop-problems"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Performance Optimizations","id":"performance-optimizations"},{"level":4,"text":"GPU Acceleration Foundation","id":"gpu-acceleration-foundation"},{"level":4,"text":"Operation Fusion Techniques","id":"operation-fusion-techniques"},{"level":4,"text":"Memory Optimization Strategies","id":"memory-optimization-strategies"},{"level":3,"text":"Advanced Neural Network Features","id":"advanced-neural-network-features"},{"level":4,"text":"Convolutional Neural Network Support","id":"convolutional-neural-network-support"},{"level":4,"text":"Recurrent Neural Network Architectures","id":"recurrent-neural-network-architectures"},{"level":4,"text":"Normalization and Regularization Techniques","id":"normalization-and-regularization-techniques"},{"level":3,"text":"Ecosystem and Tooling","id":"ecosystem-and-tooling"},{"level":4,"text":"Model Serialization and Persistence","id":"model-serialization-and-persistence"},{"level":4,"text":"Visualization and Debugging Tools","id":"visualization-and-debugging-tools"},{"level":4,"text":"Integration with Existing Ecosystem","id":"integration-with-existing-ecosystem"},{"level":3,"text":"Common Extension Implementation Pitfalls","id":"common-extension-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Project Structure for Extensions","id":"project-structure-for-extensions"},{"level":4,"text":"GPU Acceleration Starter Code","id":"gpu-acceleration-starter-code"},{"level":4,"text":"Operation Fusion Skeleton","id":"operation-fusion-skeleton"},{"level":4,"text":"CNN Module Implementation","id":"cnn-module-implementation"},{"level":4,"text":"Serialization Infrastructure","id":"serialization-infrastructure"},{"level":4,"text":"Performance Monitoring Infrastructure","id":"performance-monitoring-infrastructure"},{"level":4,"text":"Extension Integration Checkpoints","id":"extension-integration-checkpoints"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Core Tensor and Automatic Differentiation Concepts","id":"core-tensor-and-automatic-differentiation-concepts"},{"level":3,"text":"Neural Network Architecture Concepts","id":"neural-network-architecture-concepts"},{"level":3,"text":"Optimization and Training Concepts","id":"optimization-and-training-concepts"},{"level":3,"text":"Training Loop and Coordination Concepts","id":"training-loop-and-coordination-concepts"},{"level":3,"text":"Error Handling and Debugging Concepts","id":"error-handling-and-debugging-concepts"},{"level":3,"text":"Testing and Validation Concepts","id":"testing-and-validation-concepts"},{"level":3,"text":"Performance and Extension Concepts","id":"performance-and-extension-concepts"},{"level":3,"text":"Framework-Specific Implementation Terms","id":"framework-specific-implementation-terms"},{"level":3,"text":"Constants and Configuration Values","id":"constants-and-configuration-values"},{"level":3,"text":"Error Types and Exception Hierarchy","id":"error-types-and-exception-hierarchy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"}],"title":"Neural Network Framework: Design Document","markdown":"# Neural Network Framework: Design Document\n\n\n## Overview\n\nThis system implements a PyTorch-like deep learning framework with automatic differentiation, enabling users to build and train neural networks. The key architectural challenge is efficiently computing gradients through dynamic computation graphs using reverse-mode automatic differentiation while maintaining a clean, extensible API.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** Foundation for all milestones - establishes the core problem that automatic differentiation solves\n\nBuilding neural networks from scratch presents a fundamental challenge that every deep learning practitioner must understand: **gradient computation**. While forward propagation through a neural network is conceptually straightforward—data flows through layers, each applying mathematical transformations—the backward pass that enables learning requires computing gradients of complex, nested functions with respect to hundreds or thousands of parameters. This process, known as **backpropagation**, becomes intractable to implement manually as networks grow in complexity.\n\nModern deep learning frameworks like PyTorch, TensorFlow, and JAX solve this challenge through **automatic differentiation** (autodiff), a technique that mechanically computes exact derivatives of computer programs. Rather than requiring developers to manually derive and implement gradient formulas for every possible combination of operations, autodiff systems automatically track computations during the forward pass and then systematically apply the chain rule during the backward pass to compute all necessary gradients.\n\nThe framework we will build implements **reverse-mode automatic differentiation**, the same approach used by PyTorch and TensorFlow's eager mode. This technique constructs a computational graph during forward execution, then traverses this graph in reverse topological order to compute gradients efficiently. Understanding how to build such a system from the ground up provides deep insights into how modern neural network training actually works under the hood.\n\n### The Recipe Book Analogy\n\nTo understand why automatic differentiation is necessary and how it works, consider neural network training as following a complex recipe book where we need to track how changing any ingredient affects the final dish's quality.\n\nImagine you're a chef developing a new recipe by combining multiple sub-recipes. Your main dish requires making a sauce (which itself requires combining spices, oils, and acids), preparing vegetables (involving chopping, seasoning, and cooking), and combining everything with a protein. Each sub-recipe transforms its ingredients through specific operations—heating, mixing, seasoning—and passes the result to the next step.\n\nIn this analogy, **tensors are ingredients**, **operations are cooking techniques**, and the **computation graph is your recipe dependency chart** showing which ingredients flow into which preparation steps. During cooking (forward pass), you execute each step and note exactly which ingredients went into which operations and in what order. This creates a detailed record of your cooking process—the computational graph.\n\nNow suppose your final dish tastes slightly too salty, and you want to know exactly how much less salt to use in each sub-recipe to achieve the perfect flavor balance. This is analogous to having a loss function that measures how far your neural network's predictions are from the target, and wanting to know how to adjust each parameter (weight and bias) to reduce that loss.\n\nTo solve this, you need to trace backwards through your recipe: the saltiness came from the final combination step, which got contributions from the sauce, the vegetables, and the protein seasoning. The sauce's saltiness came from the spice blend and the acid reduction. Each step in reverse tells you how much changing that ingredient would affect the final taste, and you can use the chain rule to combine these effects: if reducing the sauce salt by X affects the final dish by Y, and the sauce salt comes from the spice blend with ratio Z, then reducing the spice blend affects the final dish by Y×Z.\n\nThis backwards tracing through the recipe dependency chart, applying the chain rule at each step, is exactly how **reverse-mode automatic differentiation** works. The computation graph records the \"recipe\" of your neural network's computation, and backpropagation systematically traces backwards through this graph, computing how much each parameter (ingredient) contributes to the final loss (dish quality).\n\nThe key insight is that you don't need to manually figure out all possible ways ingredients could affect the final dish—you just need to record what actually happened during this particular cooking session (forward pass), then systematically trace backwards through those recorded steps. The automatic differentiation system handles this bookkeeping automatically, ensuring that no matter how complex your \"recipe\" (neural network architecture) becomes, gradients are computed correctly and efficiently.\n\n![Computation Graph Structure](./diagrams/computation-graph.svg)\n\n### The Gradient Computation Challenge\n\nManual gradient computation becomes intractable for neural networks due to the **exponential explosion of partial derivatives** required as network complexity grows. To understand why automatic differentiation is essential, consider the mathematical challenges faced when implementing backpropagation manually.\n\nFor a simple two-layer neural network with input `x`, hidden layer weights `W1`, hidden layer biases `b1`, activation function `σ`, output weights `W2`, output biases `b2`, and loss function `L`, the forward pass computes:\n\n```\nh = σ(W1 @ x + b1)\ny = W2 @ h + b2  \nloss = L(y, target)\n```\n\nComputing gradients manually requires applying the chain rule to find how `loss` changes with respect to each parameter. For `W2`, this involves ∂L/∂W2 = ∂L/∂y × ∂y/∂W2. For `W1`, the chain becomes longer: ∂L/∂W1 = ∂L/∂y × ∂y/∂h × ∂h/∂W1, requiring intermediate gradient computations and careful tracking of tensor shapes for matrix derivatives.\n\nAs networks grow deeper and more complex, several challenges compound this difficulty:\n\n**Computational Complexity Explosion**: A network with L layers requires computing L sets of gradients, each depending on gradients from subsequent layers. For ResNet-style skip connections, gradients flow through multiple paths, requiring careful accumulation. Modern architectures like Transformers with attention mechanisms involve complex tensor operations (scaled dot-product attention, layer normalization) where manual derivative computation for each operation becomes extremely error-prone.\n\n**Shape Management Complexity**: Tensor operations involve broadcasting, reshaping, and dimension manipulation that affect gradient computation. Computing gradients for broadcasted operations requires \"unbroadcasting\" gradients back to original shapes. Matrix operations require transposition and careful axis management. Batch dimensions add another layer of complexity where gradients must be summed across batch elements.\n\n**Dynamic Computation Graphs**: Modern neural networks often involve conditional execution, loops, and dynamic shapes that change based on input data. Recurrent networks process variable-length sequences, requiring gradient computation through dynamic unrolling. Attention mechanisms involve dynamic masking and variable sequence lengths. Manual gradient computation cannot handle these dynamic patterns systematically.\n\n**Numerical Stability Issues**: Manual implementations often suffer from numerical precision problems. Gradient clipping, proper initialization, and handling of edge cases (like log(0) in cross-entropy loss) require careful implementation. Automatic differentiation systems can implement these safeguards systematically across all operations.\n\n**Error-Prone Implementation**: Manual gradient computation requires implementing derivatives for every operation, ensuring consistency between forward and backward passes, and maintaining this consistency as code evolves. A single error in any gradient formula can cause subtle training failures that are difficult to debug.\n\n**Composition Complexity**: Neural networks compose operations in arbitrary ways—convolutions followed by batch normalization, then activation functions, then attention mechanisms. Manual implementation requires deriving gradient formulas for every possible composition, an exponentially growing challenge.\n\nConsider the gradient computation for a single attention head in a Transformer:\n\n```\nQ, K, V = input @ WQ, input @ WK, input @ WV\nscores = Q @ K.T / sqrt(d_k)  \nattn_weights = softmax(scores, mask=mask)\noutput = attn_weights @ V\n```\n\nComputing ∂loss/∂WQ manually requires tracing through matrix multiplications, softmax with masking, another matrix multiplication, and handling variable sequence lengths—a derivation that spans multiple pages of mathematical notation and is extremely error-prone to implement.\n\n**The Manual Approach Breakdown**: To illustrate why manual approaches fail, consider implementing a simple three-layer network manually. You must:\n\n1. Derive gradient formulas for each layer's weights and biases\n2. Implement forward pass storing all intermediate activations  \n3. Implement backward pass applying chain rule in reverse order\n4. Handle broadcasting and shape changes correctly at each step\n5. Ensure numerical stability and proper gradient accumulation\n6. Debug gradient computation by comparing with numerical differentiation\n7. Maintain consistency as you modify the network architecture\n\nThis process must be repeated for every new layer type, activation function, or architectural change. The implementation becomes a maintenance nightmare that prevents experimentation and innovation.\n\n**Automatic Differentiation as Solution**: Automatic differentiation transforms this intractable manual process into a systematic, mechanical procedure. Instead of deriving gradients by hand, you implement each operation's forward pass and its corresponding gradient computation rule once. The autodiff system automatically composes these rules using the chain rule, handles shape management, and ensures correctness regardless of how operations are combined.\n\nThis transformation enables the rapid experimentation and complex architectures that drive modern deep learning research. Without automatic differentiation, the field would be limited to simple, manually tractable network architectures, preventing the development of transformative models like ResNets, Transformers, and modern generative models.\n\n### Existing Framework Comparison\n\nUnderstanding how established frameworks approach automatic differentiation reveals different design philosophies and trade-offs that inform our implementation choices. Each major framework—PyTorch, TensorFlow, and JAX—represents a distinct approach to solving the gradient computation challenge.\n\n**PyTorch: Dynamic Computation Graphs with Eager Execution**\n\nPyTorch pioneered the **define-by-run** paradigm where computation graphs are built dynamically during forward execution. Each tensor operation creates nodes in the computation graph immediately, enabling flexible control flow and debugging.\n\n| Aspect | Implementation | Benefits | Trade-offs |\n|--------|---------------|----------|------------|\n| Graph Construction | Built during forward pass execution | Natural Python control flow, easy debugging | Runtime overhead, memory growth |\n| Execution Model | Eager evaluation of operations | Immediate results, interactive development | Cannot optimize across operations |\n| Gradient Computation | Reverse-mode autodiff with `autograd` | Efficient for ML workloads, handles dynamic shapes | Requires keeping computation history |\n| Memory Management | Reference counting with cycle detection | Automatic cleanup, predictable behavior | Memory peaks during backward pass |\n| API Design | Object-oriented with operator overloading | Intuitive syntax, familiar to NumPy users | Harder to extend with new operations |\n\nPyTorch's design prioritizes **developer experience and flexibility**. Researchers can implement complex, dynamic architectures with conditional execution and variable-length sequences naturally. The immediate feedback from eager execution makes debugging straightforward—you can inspect tensor values at any point during computation.\n\nHowever, this flexibility comes with performance costs. Each operation incurs Python overhead, and the dynamic graph prevents compile-time optimizations. Memory usage can be high since the entire computation history must be retained for gradient computation.\n\n**TensorFlow: Static Graphs with Compilation (TF 1.x) + Eager Mode (TF 2.x)**\n\nTensorFlow originally used **define-then-run** static computation graphs, later adding eager execution to compete with PyTorch's usability.\n\n| Aspect | Static Graphs (TF 1.x) | Eager Mode (TF 2.x) | Design Impact |\n|--------|------------------------|---------------------|---------------|\n| Graph Construction | Explicit graph definition phase | Dynamic like PyTorch | Hybrid approach complexity |\n| Execution Model | Compiled graph execution | Eager evaluation option | Performance vs usability trade-off |\n| Optimization | Aggressive graph-level optimizations | Limited optimization scope | Best performance requires graph mode |\n| Debugging | Difficult, requires sessions | Natural Python debugging | Two different mental models |\n| Control Flow | Special control flow operations | Native Python control flow | API inconsistency between modes |\n\nTensorFlow's evolution reflects the tension between **performance and usability**. Static graphs enable powerful optimizations—operation fusion, memory layout optimization, and cross-device scheduling—but at the cost of programming complexity. TensorFlow 2.x attempts to provide both through `@tf.function` decorators that trace eager code into static graphs, but this hybrid approach introduces subtle complexities around when tracing occurs and how Python semantics translate to graph operations.\n\nThe XLA (Accelerated Linear Algebra) compiler in TensorFlow demonstrates static graphs' optimization potential, achieving significant speedups through operation fusion and memory optimization that dynamic approaches cannot match.\n\n**JAX: Functional Programming with Composable Transformations**\n\nJAX takes a radically different approach, treating automatic differentiation as one of several **composable program transformations** applied to pure functions.\n\n| Aspect | JAX Approach | Unique Benefits | Limitations |\n|--------|--------------|-----------------|-------------|\n| Programming Model | Pure functional, no mutable state | Mathematical clarity, composability | Requires functional thinking |\n| AD Implementation | Multiple transformation modes | Forward-mode, reverse-mode, mixed | More complex implementation |\n| Transformations | Composable: `grad`, `jit`, `vmap`, `pmap` | Powerful abstractions, research flexibility | Steep learning curve |\n| Graph Representation | Traced JaxPR intermediate representation | Clean separation of concerns | Less transparent than PyTorch |\n| Performance | XLA compilation for all operations | Consistent high performance | Compilation overhead |\n\nJAX's `grad` transformation converts a function `f: a -> b` into its gradient function `grad(f): a -> a`, which can be composed with other transformations like `jit` (compilation), `vmap` (vectorization), and `pmap` (parallelization). This composability enables expressing complex training patterns concisely:\n\n```python\n# JAX: compose transformations\nbatched_grad = vmap(grad(loss_fn), in_axes=(None, 0, 0))\ncompiled_update = jit(lambda params, batch: sgd_update(params, batched_grad(params, batch.x, batch.y)))\n```\n\nThe functional approach eliminates many sources of bugs common in stateful frameworks—no accidental mutation, no hidden state, clear data dependencies. However, it requires rethinking neural network implementation patterns and has a steeper learning curve for developers accustomed to object-oriented frameworks.\n\n**Framework Comparison Analysis**\n\n> **Decision: Framework Design Philosophy for Our Implementation**\n> - **Context**: We need to choose which framework's approach best serves the educational goals of understanding automatic differentiation fundamentals\n> - **Options Considered**: \n>   1. PyTorch-style dynamic graphs with eager execution\n>   2. TensorFlow-style static graph compilation  \n>   3. JAX-style functional transformations\n> - **Decision**: PyTorch-style dynamic computation graphs\n> - **Rationale**: Dynamic graphs make the computation graph construction and traversal explicit and observable, which is essential for learning how autodiff works. Students can inspect the graph structure, understand the backward pass step-by-step, and debug gradient computation interactively. The immediate feedback from eager execution helps build intuition.\n> - **Consequences**: We sacrifice some performance optimizations possible with static graphs, but gain clarity and debuggability that serves the educational mission. The implementation will be more straightforward and the resulting framework more approachable for experimentation.\n\n| Design Dimension | PyTorch | TensorFlow | JAX | Our Choice |\n|------------------|---------|------------|-----|------------|\n| Learning Curve | Gentle, familiar to NumPy users | Moderate, two modes to learn | Steep, requires functional thinking | Gentle ✓ |\n| Graph Transparency | High, easy to inspect | Low in static mode, medium in eager | Low, abstract transformations | High ✓ |\n| Implementation Complexity | Medium, object-oriented | High, hybrid system | High, transformation system | Medium ✓ |\n| Debugging Experience | Excellent, natural Python | Poor in static, good in eager | Good but requires functional debugging | Excellent ✓ |\n| Educational Value | High, concepts map directly | Medium, abstractions hide details | High but advanced | High ✓ |\n\n**Key Insights from Framework Analysis**\n\nThe framework comparison reveals several critical insights that will guide our implementation:\n\n**Eager Execution Aids Learning**: PyTorch's success in research and education demonstrates that immediate feedback and transparent execution help developers understand what's happening. Our framework should prioritize clarity over optimization.\n\n**Graph Construction Strategy Matters**: How and when the computation graph is built fundamentally affects the programming model. Dynamic construction aligns with natural Python control flow and makes the autodiff process observable.\n\n**API Design Shapes User Experience**: PyTorch's operator overloading creates an intuitive interface where `a + b` automatically creates computation graph nodes. This removes boilerplate and lets users focus on model logic rather than framework mechanics.\n\n**Memory Management Complexity**: All frameworks struggle with memory management during gradient computation. Our implementation must carefully consider when to release intermediate values and how to handle gradient accumulation.\n\n**The Abstraction Level Trade-off**: More abstraction (like JAX transformations) can enable powerful patterns but may hide the underlying mechanics we want students to understand. Our framework should expose the computation graph and backward pass explicitly.\n\nThese insights inform our design principles: prioritize transparency and learnability, use dynamic computation graphs with eager execution, provide an intuitive tensor API with operator overloading, and make the autodiff process observable and debuggable. While this may sacrifice some performance optimizations, it serves the educational goal of deeply understanding how automatic differentiation enables modern neural network training.\n\n### Implementation Guidance\n\nUnderstanding how existing frameworks solve the gradient computation problem provides the foundation for implementing our own neural network framework. This section bridges the conceptual understanding developed above with practical implementation choices and starter code.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Tensor Storage | Pure NumPy arrays with Python lists for metadata | NumPy arrays with custom memory pools |\n| Graph Representation | Python objects with references | Optimized node structures with arena allocation |\n| Operation Implementation | Individual Python classes per operation | Single dispatch or visitor pattern |\n| Gradient Computation | Recursive traversal with Python call stack | Iterative traversal with explicit stack |\n| Broadcasting | NumPy's broadcasting with wrapper functions | Custom broadcasting implementation |\n| Testing Framework | Built-in unittest with numerical gradient checking | pytest with property-based testing |\n\nFor learning purposes, we recommend starting with the simple options to understand the core concepts, then optionally exploring advanced optimizations once the basic system works.\n\n**B. Recommended Project Structure**\n\nOrganize your neural network framework with clear separation of concerns that mirrors the conceptual architecture:\n\n```\nneural_framework/\n├── core/\n│   ├── __init__.py\n│   ├── tensor.py              ← Tensor class with operations\n│   ├── autograd.py            ← Automatic differentiation engine  \n│   └── operations.py          ← Operation classes (Add, Multiply, etc.)\n├── nn/\n│   ├── __init__.py\n│   ├── module.py              ← Module base class and parameter management\n│   ├── layers.py              ← Linear, activation layers\n│   └── loss.py                ← Loss functions\n├── optim/\n│   ├── __init__.py\n│   ├── optimizer.py           ← Optimizer base class\n│   └── sgd.py, adam.py        ← Specific optimizers\n├── utils/\n│   ├── __init__.py\n│   └── testing.py             ← Gradient checking utilities\n├── examples/\n│   ├── simple_regression.py   ← End-to-end training example\n│   └── mnist_classifier.py    ← More complex example\n└── tests/\n    ├── test_tensor.py\n    ├── test_autograd.py\n    └── test_training.py\n```\n\nThis structure separates the core autodiff functionality from the higher-level neural network abstractions, making it easier to understand and test each component independently.\n\n**C. Infrastructure Starter Code**\n\nHere's complete, working infrastructure code that handles the non-core-learning components, allowing you to focus on the automatic differentiation logic:\n\n**Gradient Checking Utility (`utils/testing.py`)**:\n```python\nimport numpy as np\nfrom typing import Callable, List\nfrom core.tensor import Tensor\n\ndef numerical_gradient(f: Callable[[Tensor], Tensor], \n                      inputs: List[Tensor], \n                      h: float = 1e-5) -> List[np.ndarray]:\n    \"\"\"\n    Compute numerical gradients using finite differences.\n    Used to verify automatic differentiation correctness.\n    \"\"\"\n    numerical_grads = []\n    \n    for input_tensor in inputs:\n        grad = np.zeros_like(input_tensor.data)\n        \n        # Flatten for easier iteration\n        flat_input = input_tensor.data.flatten()\n        flat_grad = grad.flatten()\n        \n        for i in range(len(flat_input)):\n            # Compute f(x + h)\n            flat_input[i] += h\n            input_tensor.data = flat_input.reshape(input_tensor.shape)\n            f_plus = f(input_tensor).data\n            \n            # Compute f(x - h)  \n            flat_input[i] -= 2 * h\n            input_tensor.data = flat_input.reshape(input_tensor.shape)\n            f_minus = f(input_tensor).data\n            \n            # Restore original value\n            flat_input[i] += h\n            input_tensor.data = flat_input.reshape(input_tensor.shape)\n            \n            # Compute numerical gradient\n            flat_grad[i] = np.sum((f_plus - f_minus) / (2 * h))\n        \n        numerical_grads.append(flat_grad.reshape(input_tensor.shape))\n    \n    return numerical_grads\n\ndef check_gradients(f: Callable[[Tensor], Tensor],\n                   inputs: List[Tensor],\n                   tolerance: float = 1e-6) -> bool:\n    \"\"\"\n    Compare automatic gradients with numerical gradients.\n    Returns True if they match within tolerance.\n    \"\"\"\n    # Compute automatic gradients\n    for inp in inputs:\n        inp.requires_grad = True\n        inp.grad = None\n    \n    output = f(*inputs)\n    output.backward()\n    \n    auto_grads = [inp.grad.data for inp in inputs]\n    \n    # Compute numerical gradients\n    numerical_grads = numerical_gradient(lambda *args: f(*args), inputs)\n    \n    # Compare\n    for auto_grad, num_grad in zip(auto_grads, numerical_grads):\n        if not np.allclose(auto_grad, num_grad, atol=tolerance):\n            print(f\"Gradient mismatch!\")\n            print(f\"Automatic: {auto_grad}\")\n            print(f\"Numerical: {num_grad}\")\n            print(f\"Difference: {np.abs(auto_grad - num_grad)}\")\n            return False\n    \n    return True\n```\n\n**Broadcasting Utilities (`core/broadcasting.py`)**:\n```python\nimport numpy as np\nfrom typing import Tuple\n\ndef broadcast_shapes(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -> Tuple[int, ...]:\n    \"\"\"\n    Compute the broadcasted shape following NumPy broadcasting rules.\n    \"\"\"\n    return np.broadcast_shapes(shape1, shape2)\n\ndef unbroadcast_gradient(grad: np.ndarray, original_shape: Tuple[int, ...]) -> np.ndarray:\n    \"\"\"\n    Reduce gradient from broadcasted shape back to original tensor shape.\n    This is crucial for gradient computation in broadcasted operations.\n    \"\"\"\n    # Sum out added dimensions (from left)\n    ndims_added = len(grad.shape) - len(original_shape)\n    for _ in range(ndims_added):\n        grad = grad.sum(axis=0)\n    \n    # Sum over broadcasted dimensions  \n    for i, (grad_dim, orig_dim) in enumerate(zip(grad.shape, original_shape)):\n        if orig_dim == 1 and grad_dim > 1:\n            grad = grad.sum(axis=i, keepdims=True)\n    \n    return grad.reshape(original_shape)\n\ndef can_broadcast(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -> bool:\n    \"\"\"\n    Check if two shapes can be broadcasted together.\n    \"\"\"\n    try:\n        broadcast_shapes(shape1, shape2)\n        return True\n    except ValueError:\n        return False\n```\n\n**D. Core Logic Skeleton Code**\n\nHere are the essential class signatures and method skeletons that you need to implement, with detailed TODOs mapping to the concepts discussed above:\n\n**Tensor Class Skeleton (`core/tensor.py`)**:\n```python\nimport numpy as np\nfrom typing import Optional, Tuple, Union, List\n\nclass Tensor:\n    \"\"\"\n    N-dimensional array with automatic differentiation support.\n    Similar to PyTorch tensors but simplified for learning.\n    \"\"\"\n    \n    def __init__(self, \n                 data: Union[np.ndarray, List, float, int],\n                 requires_grad: bool = False,\n                 grad_fn: Optional['Operation'] = None):\n        \"\"\"\n        Initialize tensor with data and gradient tracking.\n        \n        Args:\n            data: The actual numerical data (converted to numpy array)\n            requires_grad: Whether to compute gradients for this tensor\n            grad_fn: The operation that created this tensor (for autodiff)\n        \"\"\"\n        # TODO 1: Convert data to numpy array and store in self.data\n        # TODO 2: Store shape, dtype from the numpy array  \n        # TODO 3: Set requires_grad flag and initialize grad to None\n        # TODO 4: Store grad_fn for backward pass linkage\n        # Hint: Use np.asarray() to handle different input types\n    \n    def backward(self, gradient: Optional['Tensor'] = None) -> None:\n        \"\"\"\n        Initiate backpropagation from this tensor.\n        This is the entry point that triggers gradient computation.\n        \"\"\"\n        # TODO 1: If gradient not provided, create tensor of ones with same shape\n        # TODO 2: Call _backward() to start recursive gradient computation  \n        # TODO 3: Handle the case where this tensor doesn't require gradients\n        # Hint: gradient=None means this is the loss tensor (scalar)\n    \n    def _backward(self, gradient: 'Tensor') -> None:\n        \"\"\"\n        Internal method for recursive gradient computation.\n        This implements the core autodiff logic.\n        \"\"\"\n        # TODO 1: If this tensor requires gradients, accumulate gradient\n        # TODO 2: If this tensor has grad_fn, call its backward method\n        # TODO 3: Handle gradient accumulation (+=, not =) for shared tensors\n        # Hint: Check if self.grad is None before accumulating\n    \n    def __add__(self, other: Union['Tensor', float, int]) -> 'Tensor':\n        \"\"\"Tensor addition with gradient tracking.\"\"\"\n        # TODO 1: Convert other to Tensor if it's a scalar\n        # TODO 2: Create Add operation and apply it\n        # TODO 3: Return result tensor with proper grad_fn\n        # Hint: Import Add operation from operations.py\n    \n    def __mul__(self, other: Union['Tensor', float, int]) -> 'Tensor':\n        \"\"\"Element-wise multiplication with gradient tracking.\"\"\"\n        # TODO: Similar to __add__ but use Multiply operation\n    \n    def matmul(self, other: 'Tensor') -> 'Tensor':\n        \"\"\"Matrix multiplication with gradient tracking.\"\"\"\n        # TODO: Create MatMul operation and apply it\n        # Hint: This will be more complex due to shape handling\n```\n\n**Operation Base Class (`core/operations.py`)**:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple\nfrom core.tensor import Tensor\n\nclass Operation(ABC):\n    \"\"\"\n    Base class for all operations that can compute gradients.\n    Each operation knows how to compute its forward pass and backward pass.\n    \"\"\"\n    \n    def __init__(self):\n        self.inputs: Tuple[Tensor, ...] = ()\n    \n    def __call__(self, *inputs: Tensor) -> Tensor:\n        \"\"\"\n        Apply this operation to input tensors.\n        \"\"\"\n        # TODO 1: Store inputs for backward pass\n        # TODO 2: Call forward() to compute result\n        # TODO 3: Create result tensor with this operation as grad_fn\n        # TODO 4: Set requires_grad=True if any input requires gradients\n    \n    @abstractmethod\n    def forward(self, *inputs: Tensor) -> np.ndarray:\n        \"\"\"Compute the forward pass. Return raw numpy array.\"\"\"\n        pass\n    \n    @abstractmethod  \n    def backward(self, grad_output: Tensor) -> Tuple[Tensor, ...]:\n        \"\"\"\n        Compute gradients with respect to inputs.\n        \n        Args:\n            grad_output: Gradient flowing back from subsequent operations\n            \n        Returns:\n            Tuple of gradients for each input tensor\n        \"\"\"\n        pass\n\nclass Add(Operation):\n    \"\"\"Element-wise addition operation.\"\"\"\n    \n    def forward(self, a: Tensor, b: Tensor) -> np.ndarray:\n        # TODO 1: Add the data arrays using numpy\n        # TODO 2: Handle broadcasting automatically via numpy\n        # Hint: Just return a.data + b.data, numpy handles broadcasting\n    \n    def backward(self, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Gradient of addition: ∂(a+b)/∂a = 1, ∂(a+b)/∂b = 1\n        But we must handle broadcasting by unbroadcasting gradients.\n        \"\"\"\n        # TODO 1: Gradient w.r.t. first input is grad_output  \n        # TODO 2: Gradient w.r.t. second input is also grad_output\n        # TODO 3: Unbroadcast gradients to match original input shapes\n        # TODO 4: Return tuple of gradient tensors\n        # Hint: Use unbroadcast_gradient() from broadcasting.py\n```\n\n**E. Language-Specific Hints**\n\n**NumPy Broadcasting**: Use `np.broadcast_arrays(a, b)` to see how arrays will be broadcasted, but let operations happen naturally - NumPy handles broadcasting automatically in arithmetic operations.\n\n**Memory Management**: Python's garbage collector handles most cleanup, but be aware that computation graphs can create reference cycles. Consider implementing `zero_grad()` methods that explicitly clear gradients to free memory.\n\n**Shape Debugging**: Use `tensor.shape` frequently and add shape assertions. Many bugs come from shape mismatches that numpy silently broadcasts in unexpected ways.\n\n**Gradient Accumulation**: Always use `+=` when accumulating gradients, never `=`. Shared tensors (used multiple times) need gradients summed from all uses.\n\n**Testing Strategy**: Start with simple operations (addition, multiplication) and verify gradients using numerical differentiation before moving to complex operations like matrix multiplication.\n\n**F. Milestone Checkpoint**\n\nAfter implementing the basic tensor and autodiff system:\n\n**Test Command**: `python -c \"from utils.testing import check_gradients; from core.tensor import Tensor; import numpy as np; a = Tensor([1.0, 2.0], requires_grad=True); b = Tensor([3.0, 4.0], requires_grad=True); print(check_gradients(lambda x, y: x + y, [a, b]))\"`\n\n**Expected Output**: `True` (indicating gradients match numerical computation)\n\n**Manual Verification**:\n1. Create two tensors with `requires_grad=True`\n2. Perform addition: `c = a + b`  \n3. Call `c.backward()`\n4. Check that `a.grad` and `b.grad` both contain arrays of ones\n5. Verify the computation graph exists: `c.grad_fn` should be an `Add` operation\n\n**Signs of Problems**:\n- `AttributeError` on `backward()`: Tensor class not properly initialized\n- Gradient is `None`: Either `requires_grad=False` or backward pass not implemented\n- Shape mismatch errors: Broadcasting not handled correctly in gradient computation\n- Gradients don't match numerical: Error in backward pass implementation\n\nThis checkpoint ensures your foundational tensor and autodiff system works before building neural network layers on top.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** Foundation for all milestones - establishes clear scope and learning objectives for the entire project\n\nBuilding a neural network framework from scratch is an ambitious undertaking that could easily spiral into a years-long project if we attempt to match the full feature set of production frameworks like PyTorch or TensorFlow. To ensure this remains an effective learning experience, we must carefully define what we will and won't implement. Think of this like planning a cross-country road trip - without clear destinations and route boundaries, you'll find yourself lost in fascinating detours that prevent you from reaching your core learning objectives.\n\nThis section establishes our framework's scope by defining three critical boundaries: the functional requirements that form our core learning goals, the performance and quality standards that prioritize educational value over production optimization, and the explicit non-goals that we'll consciously exclude to maintain focus. These boundaries aren't limitations - they're strategic choices that ensure we build deep understanding of automatic differentiation, tensor operations, and neural network fundamentals rather than getting lost in the peripheral complexity of production systems.\n\n### Functional Requirements\n\nOur neural network framework must implement a carefully curated set of core features that demonstrate the fundamental principles of automatic differentiation and neural network training. These requirements represent the minimum viable functionality needed to train real neural networks while keeping the implementation scope manageable for a learning project.\n\nThe functional requirements are organized around four key capabilities that mirror the milestone structure: tensor operations that provide the computational foundation, automatic differentiation that enables gradient-based learning, neural network modules that offer composable building blocks, and optimization algorithms that drive the training process.\n\n| Requirement Category | Must Support | Success Criteria |\n|---------------------|-------------|------------------|\n| **Tensor Operations** | N-dimensional arrays with shape tracking | Create tensors with arbitrary dimensions, track shape and dtype correctly |\n| | Element-wise arithmetic | Add, subtract, multiply, divide tensors with broadcasting |\n| | Matrix multiplication | 2D matrix multiplication and batched operations |\n| | Broadcasting | NumPy-compatible shape expansion for mismatched tensors |\n| | Gradient tracking | `requires_grad` flag enables/disables gradient computation |\n| **Automatic Differentiation** | Computation graph construction | Operations automatically build DAG during forward pass |\n| | Reverse-mode backpropagation | Chain rule application through topological sort |\n| | Gradient accumulation | Multiple uses of same tensor accumulate gradients correctly |\n| | Gradient computation | Matches numerical differentiation within tolerance |\n| **Neural Network Modules** | Linear layers | Fully connected layers with weight and bias parameters |\n| | Activation functions | ReLU, sigmoid, tanh with correct gradients |\n| | Parameter management | Automatic registration and collection of trainable parameters |\n| | Module composition | Sequential and nested module support |\n| **Optimization** | SGD optimizer | Parameter updates with learning rate and momentum |\n| | Adam optimizer | Adaptive learning rates with bias correction |\n| | Loss functions | Cross-entropy and mean squared error |\n| | Training loop | Mini-batch processing with forward/backward/update cycle |\n\n> **Decision: Functional Scope**\n> - **Context**: Neural networks require dozens of operations and layers for production use, but implementing all would obscure core learning objectives\n> - **Options Considered**: \n>   1. Full PyTorch API compatibility (200+ operations)\n>   2. Minimal subset for basic networks (10-15 operations)\n>   3. Core operations plus extensible architecture (20-30 operations)\n> - **Decision**: Core operations plus extensible architecture\n> - **Rationale**: Provides sufficient functionality to train meaningful models while demonstrating all key automatic differentiation principles. Extensible design allows learners to add operations after mastering fundamentals\n> - **Consequences**: Can train multi-layer perceptrons and simple convolutional networks, but lacks advanced operations like attention or specialized layers\n\nThe tensor operations form the computational foundation of our framework. Every tensor must track its shape, data type, and whether it requires gradient computation. Element-wise operations like addition and multiplication must support NumPy-compatible broadcasting, automatically expanding tensor dimensions to enable operations between tensors of different shapes. Matrix multiplication must handle both 2D matrices and batched operations where multiple matrix multiplications are performed in parallel across batch dimensions.\n\nThe automatic differentiation engine represents the core intellectual challenge of the project. During the forward pass, each operation must construct nodes in a computation graph that records the relationships between input and output tensors. The backward pass must traverse this graph in reverse topological order, applying the chain rule to compute gradients. When tensors are used multiple times in a computation, their gradients must be accumulated rather than overwritten.\n\nNeural network modules provide the building blocks for constructing complex models. The `Module` base class must automatically track parameters in nested hierarchies, allowing optimizers to find and update all trainable weights. Linear layers implement the fundamental `y = Wx + b` transformation with proper weight initialization. Activation functions apply element-wise nonlinearities while preserving gradient flow through the network.\n\nThe optimization system coordinates the training process by implementing gradient-based parameter updates. SGD must support momentum for accelerated convergence, while Adam provides adaptive learning rates with bias correction for first and second moment estimates. Loss functions must compute both the loss value and provide gradients that initiate the backward pass.\n\n### Performance and Quality Goals\n\nOur framework prioritizes educational clarity and correctness over production-level performance optimizations. This represents a fundamental trade-off that shapes every design decision - we choose implementations that are easy to understand, debug, and extend rather than those that maximize computational efficiency.\n\nThink of this like the difference between a driving instructor's car and a Formula 1 race car. The instructor's car has clear visibility, simple controls, and forgiving handling characteristics that help students learn fundamental driving skills. The race car optimizes for maximum speed with complex controls that would overwhelm a learning driver. Our framework is the instructor's car - designed to teach automatic differentiation principles clearly rather than achieve maximum throughput.\n\n| Quality Dimension | Goal | Rationale | Measurement |\n|------------------|------|-----------|-------------|\n| **Educational Clarity** | Code readability over performance | Learning objective is understanding autodiff | Can explain algorithm by reading code |\n| **Correctness** | Gradients match numerical differentiation | Foundation for all neural network learning | Gradient checker passes with 1e-6 tolerance |\n| **Debuggability** | Clear error messages and inspection tools | Students need to diagnose their mistakes | Shape mismatches include tensor dimensions |\n| **Extensibility** | Easy to add new operations | Encourages experimentation beyond core features | New operation requires <50 lines of code |\n| **Simplicity** | Minimize dependencies and abstraction layers | Reduces cognitive load during learning | Core autodiff engine fits in single file |\n\n> **Decision: Performance vs. Clarity Trade-off**\n> - **Context**: Production frameworks use complex optimizations (operation fusion, memory pooling, CUDA kernels) that obscure core algorithms\n> - **Options Considered**: \n>   1. Performance-first with extensive optimizations\n>   2. Clarity-first with readable but slower implementations  \n>   3. Hybrid approach with optional optimization layers\n> - **Decision**: Clarity-first with readable implementations\n> - **Rationale**: Learning automatic differentiation requires understanding the mathematical operations and graph traversal algorithms. Performance optimizations create abstraction layers that hide these fundamentals\n> - **Consequences**: Framework will be 10-100x slower than PyTorch but students can trace every operation step-by-step\n\nEducational clarity means that someone reading the code should be able to understand the mathematical operations being performed without deciphering complex optimization layers. When implementing matrix multiplication, we use clear NumPy operations rather than optimized BLAS routines. When traversing the computation graph, we use explicit loops and condition checks rather than vectorized graph algorithms.\n\nCorrectness takes priority over speed in all situations. Every operation must produce mathematically correct results, and gradients must match numerical differentiation within reasonable tolerance. We implement comprehensive gradient checking utilities that compare automatic differentiation results against finite difference approximations. When there's a choice between a fast algorithm with edge cases and a slower algorithm that handles all cases correctly, we choose correctness.\n\nDebuggability means providing rich error messages and inspection tools that help learners diagnose problems. When tensor shapes don't match for an operation, the error message should include the actual shapes and the operation being attempted. The computation graph should be inspectable so students can visualize the operations being performed and verify that gradients flow correctly.\n\nExtensibility ensures that students can experiment with new operations and layer types after mastering the core framework. Adding a new operation should require implementing only the forward pass computation and gradient calculation, with the framework handling graph construction and backpropagation automatically. The module system should make it trivial to compose new layer types from existing building blocks.\n\n### Explicit Non-Goals\n\nTo maintain focus on core learning objectives, we explicitly exclude several categories of functionality that would be essential for production use but would distract from understanding automatic differentiation principles. These non-goals aren't shortcomings - they're strategic exclusions that keep the project scope manageable and the learning objectives clear.\n\nThink of these non-goals as the advanced driving techniques we skip in basic driving instruction. Students learning to drive don't start with parallel parking in tight spaces, highway merging in heavy traffic, or performance driving techniques. They master basic vehicle control, traffic rules, and safety principles first. Similarly, our framework focuses on automatic differentiation fundamentals rather than production deployment challenges.\n\n| Non-Goal Category | Excluded Features | Rationale for Exclusion |\n|-------------------|------------------|------------------------|\n| **Production Performance** | CUDA kernels, operation fusion, memory pooling | Implementation complexity would obscure core algorithms |\n| **Advanced Operations** | Convolutions, attention, batch normalization | Requires understanding of specialized mathematical algorithms |\n| **Distributed Training** | Multi-GPU, parameter servers, gradient synchronization | Networking and coordination complexity unrelated to autodiff |\n| **Deployment Optimization** | Quantization, pruning, ONNX export | Model optimization techniques beyond basic training |\n| **Advanced Optimizers** | AdaGrad, RMSprop, learning rate scheduling | Additional complexity beyond demonstrating optimization principles |\n| **Data Pipeline** | Data loaders, augmentation, distributed datasets | Infrastructure concerns separate from neural network computation |\n| **Model Architecture** | Pre-trained models, standard architectures (ResNet, Transformer) | Focus is on building blocks, not complete model zoo |\n| **Debugging Tools** | Profilers, visualization, TensorBoard integration | Tooling complexity beyond educational scope |\n\n> **Decision: Advanced Operations Exclusion**\n> - **Context**: Modern deep learning relies heavily on convolutions, attention mechanisms, and normalization layers\n> - **Options Considered**: \n>   1. Include convolutions as core operation\n>   2. Focus on dense layers and element-wise operations only\n>   3. Provide convolution as extension example\n> - **Decision**: Focus on dense layers, provide extension pathway\n> - **Rationale**: Convolutions require understanding im2col transformations, padding modes, and stride calculations that are orthogonal to automatic differentiation principles. Students can add convolutions after mastering basic tensor operations and gradient computation\n> - **Consequences**: Framework can't train modern computer vision models out-of-box, but students understand autodiff principles that apply to any operation\n\nProduction performance optimizations like GPU acceleration and operation fusion would require implementing CUDA kernels or complex CPU vectorization that obscures the underlying mathematical operations. While these optimizations are critical for practical deep learning, they add layers of systems complexity that distract from understanding how gradients propagate through computation graphs.\n\nAdvanced neural network operations like convolutions and attention mechanisms involve sophisticated mathematical algorithms that are interesting topics in their own right. However, implementing these operations requires understanding specialized techniques (like im2col transformations for convolutions or scaled dot-product attention) that are orthogonal to automatic differentiation principles. Students can add these operations after mastering the core framework.\n\nDistributed training introduces networking protocols, fault tolerance, and gradient synchronization challenges that belong more in a distributed systems course than an automatic differentiation tutorial. While distributed training is essential for large-scale machine learning, the coordination complexity would overwhelm the core learning objectives around computation graphs and gradient computation.\n\nDeployment optimizations like quantization and model pruning represent post-training techniques for reducing model size and inference cost. These optimizations are valuable for production systems but don't contribute to understanding how neural networks learn through gradient-based optimization.\n\nThe data pipeline infrastructure required for production machine learning - data loaders, augmentation pipelines, distributed datasets - represents a substantial engineering effort that's largely separate from neural network computation. Students can use simple NumPy arrays or basic data loading utilities while focusing on the automatic differentiation algorithms.\n\n> ⚠️ **Pitfall: Scope Creep During Implementation**\n> Students often encounter interesting optimization opportunities or missing features during implementation and want to add them immediately. This leads to projects that become too complex to complete or understand. Resist the temptation to add \"just one more feature\" - focus on the core learning objectives first. Advanced features can always be added after mastering the fundamentals, but attempting everything at once typically results in completing nothing well.\n\nThese explicit non-goals don't represent permanent limitations - they're training wheels that help students focus on core concepts. After mastering automatic differentiation with our simplified framework, students will have the foundational knowledge needed to understand how production frameworks implement these advanced features. The goal is deep understanding of fundamental principles rather than broad coverage of peripheral features.\n\n### Implementation Guidance\n\nThe goals and non-goals defined above translate into specific technical choices and architectural constraints that guide the implementation process. This guidance helps maintain focus on educational objectives while providing concrete direction for technical decisions.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option | Recommended for Learning |\n|-----------|---------------|-----------------|-------------------------|\n| **Numerical Computing** | Pure Python + NumPy | NumPy + SciPy + BLAS optimization | Pure Python + NumPy |\n| **Testing Framework** | Built-in unittest | pytest with fixtures | pytest (better error messages) |\n| **Gradient Checking** | Simple finite differences | scipy.optimize.approx_fprime | Custom implementation |\n| **Graph Visualization** | Print statements + manual inspection | graphviz + matplotlib | Manual inspection first |\n| **Documentation** | Docstrings + comments | Sphinx + readthedocs | Rich docstrings |\n\n**B. Recommended Project Structure:**\n\nThe project organization should reflect the four-layer architecture while keeping related functionality grouped together:\n\n```\nneural-framework/\n├── neuralnet/\n│   ├── __init__.py              ← Public API exports\n│   ├── tensor.py                ← Tensor class and basic operations\n│   ├── operations.py            ← Operation subclasses (Add, Multiply, MatMul)\n│   ├── autodiff.py              ← Backward pass and gradient computation\n│   ├── module.py                ← Module base class and parameter management\n│   ├── layers.py                ← Linear layer and activation functions\n│   ├── optimizers.py            ← SGD and Adam implementations\n│   ├── losses.py                ← Loss functions\n│   └── utils.py                 ← Gradient checking and debugging utilities\n├── tests/\n│   ├── test_tensor.py           ← Tensor operation tests\n│   ├── test_autodiff.py         ← Gradient computation tests  \n│   ├── test_modules.py          ← Layer and module tests\n│   ├── test_optimizers.py       ← Optimizer tests\n│   └── test_integration.py      ← End-to-end training tests\n├── examples/\n│   ├── linear_regression.py     ← Simple regression example\n│   ├── mnist_mlp.py            ← Multi-layer perceptron\n│   └── gradient_checking.py     ← Debugging utilities demo\n└── README.md                    ← Project overview and usage\n```\n\n**C. Core Design Constraints:**\n\nBased on our goals and non-goals, implement with these constraints:\n\n| Constraint Category | Specific Requirements |\n|---------------------|---------------------|\n| **Dependencies** | NumPy only for numerical computation (no torch, tensorflow, jax) |\n| **Performance** | Prefer readable code over optimization (no Cython, no custom C extensions) |\n| **API Design** | Mirror PyTorch conventions where possible for familiar interface |\n| **Error Handling** | Detailed error messages with tensor shapes and operation context |\n| **Testing** | Every operation must pass gradient check with tolerance=1e-6 |\n| **Documentation** | Every public method documented with examples and mathematical notation |\n\n**D. Architecture Validation Checklist:**\n\nUse this checklist to ensure implementations stay aligned with educational goals:\n\n| Design Decision | Educational Priority Questions |\n|-----------------|-------------------------------|\n| **Adding new operation** | Can student trace through forward and backward pass by hand? |\n| **Choosing algorithm** | Is the mathematical relationship clear from the code? |\n| **Error handling** | Do error messages help student understand what went wrong? |\n| **API design** | Can student predict behavior without reading documentation? |\n| **Performance optimization** | Does optimization obscure the underlying algorithm? |\n\n**E. Milestone Validation Strategy:**\n\nAfter completing each milestone, validate that goals are met:\n\n| Milestone | Validation Approach | Success Criteria |\n|-----------|-------------------|------------------|\n| **Tensor Operations** | Manual calculation verification | Hand-computed results match tensor operations |\n| **Automatic Differentiation** | Numerical gradient comparison | gradient_check passes for all operations |\n| **Neural Modules** | Parameter counting and initialization | Module.parameters() returns expected tensors |\n| **Optimizers** | Simple function minimization | SGD and Adam converge on quadratic function |\n\n**F. Scope Management Guidelines:**\n\nTo prevent scope creep during implementation:\n\n| Tempting Addition | Decision Framework | Recommended Action |\n|-------------------|-------------------|-------------------|\n| **\"Just one more operation\"** | Does it demonstrate new autodiff principle? | Usually defer to post-completion extension |\n| **\"Better error messages\"** | Does it help debug common student mistakes? | Generally worth the investment |\n| **\"Performance improvement\"** | Does it maintain algorithm clarity? | Usually violates educational goals |\n| **\"Production feature\"** | Is it needed for basic neural network training? | Add to explicit non-goals list |\n\n> The key insight for scope management is that every feature addition should be justified by its contribution to understanding automatic differentiation principles, not by its practical utility for production machine learning. This framework succeeds when students understand how gradients flow through computation graphs, not when it achieves competitive benchmark performance.\n\nThis implementation guidance provides concrete guardrails for maintaining focus on educational objectives while building a framework that demonstrates all core automatic differentiation concepts. The structure and constraints ensure that complexity stays manageable while covering sufficient functionality to train real neural networks on meaningful problems.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** Foundation for all milestones - establishes the overall system design and component relationships\n\nBuilding a neural network framework is like constructing a sophisticated manufacturing plant where raw materials (data) flow through an assembly line of operations, with each station (layer) performing specific transformations while keeping detailed records of every step for quality control (gradient computation). Just as a modern factory has distinct departments—receiving (tensor operations), production line (computation graph), quality assurance (modules), and management (optimizers)—our framework is organized into four distinct architectural layers that work together seamlessly.\n\nThe genius of this layered architecture lies in its separation of concerns: lower layers handle the mechanical details of computation while higher layers focus on the intelligent orchestration of learning. Each layer builds upon the foundation provided by the layer below, creating a clean abstraction hierarchy that makes the framework both powerful and maintainable.\n\n![Neural Framework System Architecture](./diagrams/system-architecture.svg)\n\n### Four-Layer Architecture\n\nOur neural network framework follows a **four-layer architectural pattern** that mirrors the natural hierarchy of concepts in deep learning. Think of it as a pyramid where each level provides services to the level above while depending only on services from levels below. This creates clear boundaries and enables each layer to evolve independently without breaking the entire system.\n\nThe **bottom layer** handles **tensor operations**—the fundamental mathematical building blocks. Like the foundation of a building, this layer must be rock-solid because everything else depends on it. Tensors at this level are \"smart arrays\" that know their shape, track whether they need gradients, and can perform basic arithmetic operations with broadcasting support.\n\nThe **second layer** implements the **automatic differentiation engine**—the computation graph that records operations and computes gradients. This is the nervous system of our framework, automatically tracking every computation during the forward pass and enabling efficient gradient computation during the backward pass. The key insight is that this layer transforms the imperative code users write into a functional computation graph suitable for differentiation.\n\nThe **third layer** provides the **neural network module system**—composable building blocks like linear layers and activation functions. This layer is where the framework becomes user-friendly, providing the LEGO-like components that users combine to build complex architectures. Modules automatically register their parameters and handle the forward pass, abstracting away the low-level tensor operations.\n\nThe **top layer** contains **optimizers and training infrastructure**—the algorithms that actually learn by updating parameters based on computed gradients. This layer orchestrates the entire training process, coordinating forward passes, loss computation, backward passes, and parameter updates in the correct sequence.\n\n| Layer | Primary Responsibility | Key Components | Dependencies |\n|-------|----------------------|----------------|--------------|\n| Tensor Operations | Mathematical primitives and shape management | `Tensor`, arithmetic operators, broadcasting | NumPy arrays, memory management |\n| Autodiff Engine | Computation graph construction and gradient computation | `Operation` nodes, topological sort, chain rule | Tensor layer |\n| Neural Modules | Composable network components and parameter management | `Module`, `Linear`, activation functions | Tensor and Autodiff layers |\n| Optimizers | Parameter updates and training coordination | `SGD`, `Adam`, loss functions, training loops | All lower layers |\n\nThe beauty of this architecture is its **define-by-run** nature: users write imperative Python code using tensors and modules, but behind the scenes, the autodiff engine automatically constructs a computation graph that enables efficient gradient computation. This combines the ease of imperative programming with the mathematical rigor required for automatic differentiation.\n\n> **Key Architectural Insight:** Each layer provides a different level of abstraction—from mechanical tensor operations to high-level training orchestration—but they all work together transparently. Users interact primarily with the top two layers while the bottom two layers handle the complex mathematics automatically.\n\nThe layers communicate through **well-defined interfaces** that maintain the abstraction boundaries. Modules create and manipulate tensors, tensors automatically build the computation graph during operations, the autodiff engine computes gradients when requested, and optimizers use those gradients to update parameters. This creates a clean data flow that's easy to understand and debug.\n\n### Component Responsibilities\n\nUnderstanding what each architectural layer owns and how they depend on each other is crucial for implementing a maintainable framework. Think of each layer as a specialized department in our neural network factory, with clear job descriptions and communication protocols.\n\n#### Tensor Operations Layer Responsibilities\n\nThe **Tensor Operations Layer** serves as the mathematical foundation, responsible for all numerical computation and shape management. Like the engine room of a ship, this layer handles the mechanical work that powers everything else.\n\n| Responsibility Area | Specific Duties | Key Interfaces |\n|-------------------|----------------|---------------|\n| Data Storage | N-dimensional array storage, dtype tracking, shape validation | `data`, `shape`, `dtype` attributes |\n| Arithmetic Operations | Element-wise operations, matrix multiplication, broadcasting | `__add__`, `__mul__`, `matmul` methods |\n| Shape Management | Broadcasting rule implementation, shape compatibility checking | `broadcast_shapes`, dimension expansion |\n| Gradient Metadata | Tracking gradient requirements, storing gradient values | `requires_grad`, `grad`, `grad_fn` attributes |\n| Memory Management | Efficient array allocation, in-place operation prevention | Memory layout optimization, copy semantics |\n\nThis layer **depends only on NumPy** for the actual numerical computations, making it the foundation that all other layers build upon. It must handle edge cases like shape mismatches and provide clear error messages when operations are incompatible.\n\nThe tensor layer **exposes its capabilities** through operator overloading, making tensor arithmetic feel natural to Python users while automatically tracking the operations needed for gradient computation. Every operation returns a new tensor (avoiding in-place modifications that break gradients) with proper gradient metadata attached.\n\n#### Automatic Differentiation Engine Responsibilities  \n\nThe **Autodiff Engine** acts as the framework's memory system, automatically recording every operation and enabling gradient computation through the chain rule. Think of it as a detailed accountant who tracks every mathematical operation so we can later compute how changes propagate backward through the computation.\n\n| Responsibility Area | Specific Duties | Key Interfaces |\n|-------------------|----------------|---------------|\n| Graph Construction | Recording operations as nodes, linking tensors as edges | `Operation` base class, graph node creation |\n| Topological Sorting | Ordering nodes for correct backward pass traversal | Dependency resolution, cycle detection |\n| Chain Rule Application | Computing gradients using calculus chain rule | `backward()` method, gradient propagation |\n| Gradient Accumulation | Summing gradients when tensors used multiple times | Gradient aggregation, memory management |\n| Graph Memory Management | Preventing circular references, cleaning up graphs | Reference counting, graph lifecycle |\n\nThe autodiff engine **depends on the tensor layer** for the actual tensor operations but adds the crucial capability of reversibility. It transforms the forward computation into a data structure that can be traversed backward to compute gradients efficiently.\n\nThis layer **provides gradient computation services** to the upper layers through the `backward()` method, which triggers the reverse-mode differentiation algorithm. The key insight is that this layer makes gradient computation completely automatic—users never need to manually compute derivatives.\n\n> **Critical Design Decision:** The autodiff engine uses **reverse-mode differentiation** (backpropagation) rather than forward-mode because reverse-mode is more efficient for the typical case where we have many parameters but few outputs (like a scalar loss function).\n\n#### Neural Network Modules Layer Responsibilities\n\nThe **Modules Layer** provides the user-facing building blocks for constructing neural networks. Like a hardware store that sells pre-made components, this layer offers tested, composable pieces that users can combine to build complex architectures without worrying about the underlying mathematical details.\n\n| Responsibility Area | Specific Duties | Key Interfaces |\n|-------------------|----------------|---------------|\n| Parameter Management | Registering, initializing, and tracking trainable parameters | `parameters()` method, parameter registration |\n| Forward Pass Logic | Implementing layer-specific computations | `forward()` method, modular composition |\n| Initialization Strategies | Setting initial parameter values for stable training | Weight initialization schemes, bias handling |\n| Composability | Enabling layers to be combined and nested | `Module` base class, container patterns |\n| State Management | Handling training vs. inference modes, layer state | Mode switching, stateful layer support |\n\nThe modules layer **depends on both tensor and autodiff layers** because modules create tensors (which automatically build computation graphs) and rely on gradient computation for training. However, modules abstract away these details, providing a clean interface for network construction.\n\nThis layer **serves the optimizer layer** by exposing all trainable parameters through the `parameters()` method, enabling optimizers to update all network weights without needing to understand the network structure. The recursive parameter collection is a key design feature that makes nested modules work seamlessly.\n\n#### Optimizers and Training Layer Responsibilities\n\nThe **Optimizers Layer** coordinates the entire learning process, orchestrating forward passes, loss computation, gradient computation, and parameter updates. Think of optimizers as conductors who coordinate all the musicians (modules) in the orchestra to create a harmonious performance (successful training).\n\n| Responsibility Area | Specific Duties | Key Interfaces |\n|-------------------|----------------|---------------|\n| Parameter Updates | Applying gradient-based updates to model parameters | `step()` method, parameter modification |\n| Optimizer State | Maintaining momentum, adaptive learning rates, etc. | State dictionaries, optimizer-specific variables |\n| Training Coordination | Managing forward/backward/update sequence | Training loop orchestration, batch processing |\n| Loss Computation | Computing scalar loss values for gradient computation | Loss function implementations, reduction strategies |\n| Learning Rate Management | Adjusting learning rates according to schedules | Learning rate scheduling, adaptive algorithms |\n\nThe optimizers layer **depends on all lower layers**: it uses modules to get parameters, relies on autodiff for gradients, and manipulates tensors directly during parameter updates. This layer sits at the top of the dependency hierarchy because it orchestrates all the other components.\n\nThis layer **provides the primary user interface** for training neural networks, exposing simple methods like `optimizer.step()` that hide the complexity of gradient computation and parameter updates. The training loop logic coordinates all the lower layers to implement the complete learning algorithm.\n\n#### Inter-Layer Communication Patterns\n\nThe layers communicate through **well-defined data flows** that maintain clean abstractions while enabling the complex coordination required for neural network training.\n\n| Communication Path | Data Flow | Interface | Purpose |\n|-------------------|-----------|-----------|---------|\n| Modules → Tensors | Parameter creation, forward computation | Tensor constructors, arithmetic operators | Enable neural computation |\n| Tensors → Autodiff | Operation recording, graph construction | `Operation` registration, gradient functions | Enable automatic differentiation |\n| Autodiff → Tensors | Gradient computation, backward propagation | `grad` attribute population | Provide computed gradients |\n| Optimizers → Modules | Parameter collection, state inspection | `parameters()` method | Access trainable parameters |\n| Optimizers → Autodiff | Gradient computation triggering | `backward()` method calls | Compute parameter gradients |\n| Optimizers → Tensors | Parameter updates, gradient clearing | Direct tensor manipulation | Update model weights |\n\nThe key insight is that **data flows both up and down** the architectural stack: forward computation flows upward (tensors → autodiff → modules → optimizers) while gradient computation flows downward (optimizers trigger backward pass → autodiff computes gradients → tensors store results → optimizers apply updates).\n\n### Recommended Project Structure\n\nA well-organized project structure is like a well-designed library—everything has its place, related components are grouped together, and users can quickly find what they need. Our framework's four-layer architecture naturally suggests a directory structure that mirrors the conceptual organization.\n\nThe project structure should **separate core framework code from user examples**, **group related functionality together**, and **make the dependency hierarchy obvious** from the directory layout. This helps both implementers understand where to put new code and users understand how the framework is organized.\n\n#### Directory Layout\n\n```\nneural-framework/\n├── neural_framework/              # Core framework package\n│   ├── __init__.py               # Public API exports\n│   ├── tensor/                   # Tensor Operations Layer\n│   │   ├── __init__.py          # Tensor class exports\n│   │   ├── tensor.py            # Core Tensor implementation\n│   │   ├── operations.py        # Arithmetic operations (+, *, etc.)\n│   │   └── broadcasting.py      # Broadcasting utilities\n│   ├── autodiff/                # Automatic Differentiation Engine\n│   │   ├── __init__.py         # Autodiff exports\n│   │   ├── operation.py        # Operation base class\n│   │   ├── graph.py            # Computation graph management\n│   │   └── backward.py         # Backward pass algorithms\n│   ├── modules/                # Neural Network Modules Layer\n│   │   ├── __init__.py        # Module class exports\n│   │   ├── module.py          # Base Module class\n│   │   ├── linear.py          # Linear/Dense layers\n│   │   ├── activation.py      # Activation functions\n│   │   └── container.py       # Sequential, ModuleList, etc.\n│   ├── optim/                 # Optimizers and Training Layer\n│   │   ├── __init__.py       # Optimizer exports\n│   │   ├── optimizer.py      # Base Optimizer class\n│   │   ├── sgd.py           # SGD implementation\n│   │   ├── adam.py          # Adam implementation\n│   │   └── loss.py          # Loss functions\n│   └── utils/               # Cross-cutting utilities\n│       ├── __init__.py     # Utility exports\n│       ├── gradient_check.py # Numerical gradient checking\n│       └── testing.py      # Testing utilities\n├── examples/               # Example usage and tutorials\n│   ├── basic_tensor.py    # Tensor operation examples\n│   ├── simple_network.py  # Basic neural network training\n│   ├── mnist_classifier.py # Complete classification example\n│   └── gradient_check_demo.py # Gradient checking examples\n├── tests/                 # Test suite\n│   ├── test_tensor.py    # Tensor operation tests\n│   ├── test_autodiff.py  # Gradient computation tests\n│   ├── test_modules.py   # Neural module tests\n│   ├── test_optimizers.py # Optimizer tests\n│   └── test_integration.py # End-to-end tests\n├── docs/                 # Documentation\n│   ├── tutorial.md      # Getting started guide\n│   ├── api_reference.md # Complete API documentation\n│   └── design_notes.md  # Architecture explanations\n├── setup.py             # Package installation\n├── requirements.txt     # Dependencies\n└── README.md           # Project overview\n```\n\n#### Package Organization Rationale\n\nThe directory structure reflects several key design principles that make the framework easy to understand and maintain.\n\n> **Decision: Layer-Based Package Organization**\n> - **Context**: Need to organize code in a way that reflects the architectural layers and makes dependencies clear\n> - **Options Considered**: \n>   1. Flat structure with all classes in one package\n>   2. Feature-based packages (training, inference, etc.)\n>   3. Layer-based packages matching architectural design\n> - **Decision**: Use layer-based packages (`tensor/`, `autodiff/`, `modules/`, `optim/`)\n> - **Rationale**: Makes the dependency hierarchy obvious, prevents circular imports, and helps developers understand where to implement new features\n> - **Consequences**: Clear separation of concerns but requires understanding the layer architecture to navigate the codebase\n\nEach package corresponds to one architectural layer, making the **dependency flow obvious**: `tensor/` has no internal dependencies, `autodiff/` depends only on `tensor/`, `modules/` depends on `tensor/` and `autodiff/`, and `optim/` depends on all lower layers.\n\n| Package | Layer | Key Files | Dependencies | Exported Classes |\n|---------|-------|-----------|--------------|------------------|\n| `tensor/` | Tensor Operations | `tensor.py`, `operations.py`, `broadcasting.py` | NumPy only | `Tensor`, arithmetic functions |\n| `autodiff/` | Autodiff Engine | `operation.py`, `graph.py`, `backward.py` | `tensor/` | `Operation`, graph utilities |\n| `modules/` | Neural Modules | `module.py`, `linear.py`, `activation.py` | `tensor/`, `autodiff/` | `Module`, `Linear`, `ReLU`, etc. |\n| `optim/` | Optimizers | `optimizer.py`, `sgd.py`, `adam.py`, `loss.py` | All layers | `SGD`, `Adam`, loss functions |\n\nThe **`__init__.py` files** in each package serve as **API gateways**, exposing only the classes and functions that users of that layer should access. This creates clean public interfaces while hiding implementation details.\n\n#### Import Structure and API Design\n\nThe framework should provide a **PyTorch-like import experience** where users can access everything they need through intuitive import paths. The main `neural_framework/__init__.py` file serves as the primary API entry point.\n\n```python\n# neural_framework/__init__.py - Main API exports\nfrom .tensor import Tensor\nfrom .modules import Module, Linear, ReLU, Sequential  \nfrom .optim import SGD, Adam\nfrom .optim.loss import CrossEntropyLoss, MSELoss\n\n# Version and metadata\n__version__ = \"0.1.0\"\n__all__ = [\"Tensor\", \"Module\", \"Linear\", \"ReLU\", \"Sequential\", \"SGD\", \"Adam\", \"CrossEntropyLoss\", \"MSELoss\"]\n```\n\nThis enables **clean user imports** like `from neural_framework import Tensor, Linear, SGD`, mirroring the experience that PyTorch users expect. Advanced users who need lower-level access can import directly from subpackages like `from neural_framework.autodiff import Operation`.\n\n#### Development Workflow Organization\n\nThe project structure supports a **milestone-driven development approach** where implementers can build and test each layer independently before integrating with higher layers.\n\n| Milestone | Primary Packages | Test Files | Example Files |\n|-----------|-----------------|------------|---------------|\n| Milestone 1: Tensors | `tensor/` | `test_tensor.py` | `basic_tensor.py` |\n| Milestone 2: Autodiff | `autodiff/` | `test_autodiff.py` | `gradient_check_demo.py` |\n| Milestone 3: Modules | `modules/` | `test_modules.py` | `simple_network.py` |\n| Milestone 4: Training | `optim/` | `test_optimizers.py`, `test_integration.py` | `mnist_classifier.py` |\n\nEach milestone can be **developed and tested independently** because the package structure enforces the dependency hierarchy. Developers can implement the tensor layer, verify it works with tests and examples, then move to the autodiff layer knowing they have a solid foundation.\n\nThe **`examples/` directory** provides **learning checkpoints** where users can verify their implementation works correctly after each milestone. These examples start simple (`basic_tensor.py`) and gradually build complexity (`mnist_classifier.py`), providing a clear learning progression.\n\n> **Educational Architecture Insight:** The directory structure itself teaches the framework architecture—by organizing code, developers internalize the layer dependencies and understand where different types of functionality belong.\n\n#### Testing Strategy Integration\n\nThe test structure supports both **unit testing** (individual layer testing) and **integration testing** (cross-layer functionality). Each package has corresponding test files that can be run independently or as part of a complete test suite.\n\n| Test Category | Files | Purpose | Dependencies |\n|---------------|-------|---------|--------------|\n| Unit Tests | `test_tensor.py`, `test_autodiff.py`, etc. | Test individual layer functionality | Only the layer being tested |\n| Integration Tests | `test_integration.py` | Test complete training workflows | All layers |\n| Gradient Tests | `gradient_check_demo.py` | Verify autodiff correctness | `tensor/`, `autodiff/`, `utils/` |\n| Performance Tests | `test_performance.py` | Benchmark critical operations | All layers |\n\nThe **`utils/` package** provides testing infrastructure like `gradient_check.py` that compares automatic differentiation results with numerical differentiation, helping verify that the autodiff implementation is mathematically correct.\n\n### Implementation Guidance\n\nBuilding a neural network framework requires careful technology choices and well-structured starter code that provides necessary infrastructure without solving the core learning challenges. The implementation should prioritize **educational clarity** over performance optimization while still demonstrating professional software development practices.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Recommended for Learning |\n|-----------|--------------|-----------------|-------------------------|\n| Tensor Storage | Pure NumPy arrays | Custom memory management with stride handling | Pure NumPy arrays |\n| Gradient Computation | Python lists for graph nodes | C++ extensions for graph traversal | Python lists for graph nodes |\n| Broadcasting | Manual shape expansion | Vectorized NumPy broadcasting | Manual implementation then NumPy |\n| Parameter Initialization | Simple random normal | Xavier/He initialization schemes | Start simple, add advanced schemes |\n| Testing Framework | Built-in `assert` statements | `pytest` with fixtures | `pytest` with numerical gradient checking |\n| GPU Support | CPU-only with NumPy | CUDA integration with CuPy | CPU-only (optional CuPy extension) |\n\nThe **Simple Option** choices prioritize understanding the underlying algorithms over performance. Students should implement core concepts manually first, then optionally optimize with advanced techniques after mastering the fundamentals.\n\n#### Recommended Project Structure Setup\n\nStart with this directory structure and gradually populate it as you complete each milestone:\n\n```\nneural-framework/\n├── neural_framework/\n│   ├── __init__.py              # Start empty, add exports as you build\n│   ├── tensor/\n│   │   ├── __init__.py         # from .tensor import Tensor\n│   │   ├── tensor.py           # Core Tensor class (YOUR IMPLEMENTATION)\n│   │   ├── operations.py       # Arithmetic operations (YOUR IMPLEMENTATION) \n│   │   └── broadcasting.py     # Broadcasting utilities (STARTER CODE)\n│   ├── autodiff/\n│   │   ├── __init__.py        # from .operation import Operation\n│   │   ├── operation.py       # Operation base class (YOUR IMPLEMENTATION)\n│   │   ├── graph.py          # Graph utilities (STARTER CODE)\n│   │   └── backward.py       # Backward pass (YOUR IMPLEMENTATION)\n│   ├── modules/\n│   │   ├── __init__.py       # Module exports (add as you implement)\n│   │   ├── module.py         # Base Module class (YOUR IMPLEMENTATION)\n│   │   ├── linear.py         # Linear layer (YOUR IMPLEMENTATION)\n│   │   └── activation.py     # Activation functions (YOUR IMPLEMENTATION)\n│   ├── optim/\n│   │   ├── __init__.py      # Optimizer exports\n│   │   ├── optimizer.py     # Base Optimizer (STARTER CODE)\n│   │   ├── sgd.py          # SGD implementation (YOUR IMPLEMENTATION)\n│   │   ├── adam.py         # Adam implementation (YOUR IMPLEMENTATION)  \n│   │   └── loss.py         # Loss functions (YOUR IMPLEMENTATION)\n│   └── utils/\n│       ├── __init__.py     # Utility exports\n│       ├── gradient_check.py  # Numerical gradient checking (STARTER CODE)\n│       └── testing.py         # Test utilities (STARTER CODE)\n├── examples/                   # Add examples after each milestone\n├── tests/                     # Add tests as you implement\n└── setup.py                   # Basic package setup (STARTER CODE)\n```\n\nCreate this structure first, then implement files marked \"YOUR IMPLEMENTATION\" during the corresponding milestones. Files marked \"STARTER CODE\" are provided below as complete, working implementations.\n\n#### Infrastructure Starter Code\n\n**`neural_framework/utils/gradient_check.py`** - Complete numerical gradient checking utilities:\n\n```python\n\"\"\"Numerical gradient checking utilities for verifying autodiff correctness.\"\"\"\nimport numpy as np\nfrom typing import Callable, List, Optional\nfrom ..tensor import Tensor\n\ndef numerical_gradient(f: Callable, inputs: List[Tensor], h: float = 1e-5) -> List[np.ndarray]:\n    \"\"\"\n    Compute numerical gradients using finite differences.\n    This is the reference implementation for checking autodiff correctness.\n    \"\"\"\n    gradients = []\n    \n    for input_tensor in inputs:\n        grad = np.zeros_like(input_tensor.data)\n        it = np.nditer(input_tensor.data, flags=['multi_index'])\n        \n        while not it.finished:\n            idx = it.multi_index\n            \n            # Compute f(x + h)\n            old_value = input_tensor.data[idx]\n            input_tensor.data[idx] = old_value + h\n            fxh_pos = f()\n            \n            # Compute f(x - h)  \n            input_tensor.data[idx] = old_value - h\n            fxh_neg = f()\n            \n            # Restore original value\n            input_tensor.data[idx] = old_value\n            \n            # Finite difference approximation\n            grad[idx] = (fxh_pos - fxh_neg) / (2 * h)\n            \n            it.iternext()\n            \n        gradients.append(grad)\n    \n    return gradients\n\ndef check_gradients(f: Callable, inputs: List[Tensor], tolerance: float = 1e-6) -> bool:\n    \"\"\"\n    Compare automatic differentiation gradients with numerical gradients.\n    Returns True if gradients match within tolerance.\n    \"\"\"\n    # Compute numerical gradients\n    numerical_grads = numerical_gradient(f, inputs)\n    \n    # Compute automatic gradients\n    output = f()\n    output.backward()\n    auto_grads = [inp.grad.data for inp in inputs]\n    \n    # Compare gradients\n    for i, (num_grad, auto_grad) in enumerate(zip(numerical_grads, auto_grads)):\n        diff = np.abs(num_grad - auto_grad)\n        if np.max(diff) > tolerance:\n            print(f\"Gradient check failed for input {i}\")\n            print(f\"Max difference: {np.max(diff)}\")\n            print(f\"Numerical gradient: {num_grad}\")\n            print(f\"Automatic gradient: {auto_grad}\")\n            return False\n    \n    print(\"Gradient check passed!\")\n    return True\n```\n\n**`neural_framework/utils/testing.py`** - Testing utilities for framework validation:\n\n```python\n\"\"\"Testing utilities for neural framework validation.\"\"\"\nimport numpy as np\nfrom typing import List\nfrom ..tensor import Tensor\n\ndef create_test_tensors(shapes: List[tuple], requires_grad: bool = True) -> List[Tensor]:\n    \"\"\"Create test tensors with random data for testing.\"\"\"\n    tensors = []\n    for shape in shapes:\n        data = np.random.randn(*shape).astype(np.float32) \n        tensor = Tensor(data, requires_grad=requires_grad)\n        tensors.append(tensor)\n    return tensors\n\ndef assert_tensor_equal(a: Tensor, b: Tensor, tolerance: float = 1e-6):\n    \"\"\"Assert two tensors have equal data within tolerance.\"\"\"\n    assert a.shape == b.shape, f\"Shapes don't match: {a.shape} vs {b.shape}\"\n    diff = np.abs(a.data - b.data)\n    assert np.max(diff) < tolerance, f\"Tensors differ by {np.max(diff)}\"\n\ndef assert_gradient_exists(tensor: Tensor):\n    \"\"\"Assert that tensor has computed gradients.\"\"\"\n    assert tensor.grad is not None, \"Gradient not computed\"\n    assert not np.isnan(tensor.grad.data).any(), \"Gradient contains NaN\"\n    assert np.isfinite(tensor.grad.data).all(), \"Gradient contains infinite values\"\n```\n\n**`neural_framework/tensor/broadcasting.py`** - Broadcasting utilities:\n\n```python\n\"\"\"Broadcasting utilities for tensor operations.\"\"\"\nimport numpy as np\nfrom typing import Tuple\n\ndef broadcast_shapes(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -> Tuple[int, ...]:\n    \"\"\"\n    Compute the broadcasted shape for two input shapes.\n    Follows NumPy broadcasting rules.\n    \"\"\"\n    # Pad with 1s to make shapes same length\n    max_dims = max(len(shape1), len(shape2))\n    shape1 = (1,) * (max_dims - len(shape1)) + shape1\n    shape2 = (1,) * (max_dims - len(shape2)) + shape2\n    \n    # Compute broadcasted shape\n    result_shape = []\n    for dim1, dim2 in zip(shape1, shape2):\n        if dim1 == 1:\n            result_shape.append(dim2)\n        elif dim2 == 1:\n            result_shape.append(dim1)\n        elif dim1 == dim2:\n            result_shape.append(dim1)\n        else:\n            raise ValueError(f\"Cannot broadcast shapes {shape1} and {shape2}\")\n    \n    return tuple(result_shape)\n\ndef unbroadcast_gradient(grad: np.ndarray, original_shape: Tuple[int, ...]) -> np.ndarray:\n    \"\"\"\n    Reduce gradient from broadcasted shape back to original shape.\n    This is crucial for correct gradient computation.\n    \"\"\"\n    # Handle scalar case\n    if original_shape == ():\n        return np.sum(grad)\n    \n    # Sum out added dimensions\n    ndims_added = grad.ndim - len(original_shape)\n    for _ in range(ndims_added):\n        grad = grad.sum(axis=0)\n    \n    # Sum over broadcasted dimensions  \n    for i, (grad_dim, orig_dim) in enumerate(zip(grad.shape, original_shape)):\n        if orig_dim == 1 and grad_dim > 1:\n            grad = grad.sum(axis=i, keepdims=True)\n    \n    return grad\n```\n\n#### Core Logic Skeleton Code\n\n**`neural_framework/tensor/tensor.py`** - Tensor class skeleton for your implementation:\n\n```python\n\"\"\"Core Tensor class implementation.\"\"\"\nimport numpy as np\nfrom typing import Optional, Tuple, Union, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from ..autodiff.operation import Operation\n\nclass Tensor:\n    \"\"\"\n    N-dimensional array with automatic differentiation support.\n    This is the foundational class that everything else builds upon.\n    \"\"\"\n    \n    def __init__(self, \n                 data: Union[np.ndarray, list, float], \n                 requires_grad: bool = False,\n                 grad_fn: Optional['Operation'] = None):\n        \"\"\"Initialize tensor with data and gradient tracking.\"\"\"\n        # TODO 1: Convert data to numpy array if needed, ensure float32 dtype\n        # TODO 2: Store data, requires_grad, and grad_fn attributes  \n        # TODO 3: Initialize grad to None, shape and dtype from data\n        # Hint: Use np.asarray() and .astype(np.float32) for data conversion\n        pass\n    \n    def backward(self, gradient: Optional['Tensor'] = None) -> None:\n        \"\"\"Initiate backpropagation from this tensor.\"\"\"\n        # TODO 1: If gradient is None, create gradient of ones with same shape\n        # TODO 2: Call _backward() to start recursive gradient computation\n        # TODO 3: Handle case where tensor doesn't require gradients\n        # Hint: This is the public interface that users call\n        pass\n    \n    def _backward(self, gradient: 'Tensor') -> None:\n        \"\"\"Internal recursive gradient computation.\"\"\"\n        # TODO 1: Accumulate gradient into self.grad (handle None case)\n        # TODO 2: If grad_fn exists, call its backward method\n        # TODO 3: Ensure gradients are properly accumulated for shared tensors\n        # Hint: This implements the recursive chain rule application\n        pass\n    \n    def __add__(self, other: Union['Tensor', float]) -> 'Tensor':\n        \"\"\"Tensor addition with automatic differentiation.\"\"\"\n        # TODO 1: Convert other to Tensor if it's a scalar\n        # TODO 2: Create Add operation and call its forward method\n        # TODO 3: Return new Tensor with proper grad_fn for autodiff\n        # Hint: Import Add operation from ..autodiff.operations\n        pass\n    \n    def __mul__(self, other: Union['Tensor', float]) -> 'Tensor':\n        \"\"\"Element-wise multiplication with automatic differentiation.\"\"\"\n        # TODO 1: Convert other to Tensor if it's a scalar\n        # TODO 2: Create Multiply operation and call its forward method\n        # TODO 3: Return new Tensor with proper grad_fn for autodiff\n        pass\n    \n    def matmul(self, other: 'Tensor') -> 'Tensor':\n        \"\"\"Matrix multiplication with automatic differentiation.\"\"\"\n        # TODO 1: Validate shapes are compatible for matrix multiplication\n        # TODO 2: Create MatMul operation and call its forward method\n        # TODO 3: Handle batched matrix multiplication (3+ dimensions)\n        # TODO 4: Return new Tensor with proper grad_fn for autodiff\n        pass\n    \n    @property\n    def shape(self) -> Tuple[int, ...]:\n        \"\"\"Return shape of tensor data.\"\"\"\n        # TODO: Return self.data.shape\n        pass\n    \n    @property\n    def dtype(self) -> np.dtype:\n        \"\"\"Return data type of tensor.\"\"\"\n        # TODO: Return self.data.dtype\n        pass\n```\n\n**`neural_framework/autodiff/operation.py`** - Operation base class skeleton:\n\n```python\n\"\"\"Base class for automatic differentiation operations.\"\"\"\nfrom abc import ABC, abstractmethod\nimport numpy as np\nfrom typing import Tuple, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from ..tensor import Tensor\n\nclass Operation(ABC):\n    \"\"\"\n    Base class for differentiable operations.\n    Each operation knows how to compute forward pass and backward gradients.\n    \"\"\"\n    \n    def __init__(self, inputs: Tuple['Tensor', ...]):\n        \"\"\"Store input tensors for gradient computation.\"\"\"\n        # TODO 1: Store inputs tuple for backward pass\n        # TODO 2: Determine if this operation requires gradients\n        # Hint: requires_grad if any input requires gradients\n        self.inputs = inputs\n        self.requires_grad = any(inp.requires_grad for inp in inputs)\n    \n    @abstractmethod\n    def forward(self) -> np.ndarray:\n        \"\"\"Compute forward pass, return numpy array.\"\"\"\n        # TODO: Implement in subclasses (Add, Multiply, etc.)\n        pass\n    \n    @abstractmethod  \n    def backward(self, grad_output: 'Tensor') -> None:\n        \"\"\"Compute gradients w.r.t inputs and propagate backwards.\"\"\"\n        # TODO: Implement in subclasses using chain rule\n        pass\n```\n\n#### Milestone Checkpoints\n\nAfter implementing each milestone, verify your implementation with these checkpoints:\n\n**Milestone 1 Checkpoint - Tensor Operations:**\n```python\n# Create this as examples/test_milestone1.py\nfrom neural_framework import Tensor\nimport numpy as np\n\n# Test basic tensor creation\na = Tensor([1.0, 2.0, 3.0], requires_grad=True)\nb = Tensor([4.0, 5.0, 6.0], requires_grad=True) \n\n# Test arithmetic operations\nc = a + b  # Should give [5.0, 7.0, 9.0]\nd = a * b  # Should give [4.0, 10.0, 18.0]\n\n# Test matrix multiplication\nx = Tensor([[1, 2], [3, 4]], requires_grad=True)\ny = Tensor([[5, 6], [7, 8]], requires_grad=True)\nz = x.matmul(y)  # Should give [[19, 22], [43, 50]]\n\nprint(\"Milestone 1 checkpoint passed!\")\n```\n\n**Expected Output:** All operations complete without errors, tensors have correct values and shapes.\n\n**Milestone 2 Checkpoint - Automatic Differentiation:**\n```python\n# Test gradient computation\nfrom neural_framework.utils import check_gradients\n\ndef test_function():\n    x = Tensor([[2.0, 3.0]], requires_grad=True)\n    y = x * x + x  # y = x² + x\n    return y.sum()\n\n# This should pass if autodiff is implemented correctly\ncheck_gradients(test_function, [x])\n```\n\n**Expected Output:** \"Gradient check passed!\" confirming autodiff matches numerical gradients.\n\n#### Language-Specific Implementation Hints\n\n**NumPy Integration:**\n- Use `np.asarray()` to convert inputs to arrays\n- Use `.astype(np.float32)` for consistent float precision\n- Use `np.broadcast_arrays()` for automatic shape expansion\n- Use `np.sum(axis=...)` with `keepdims=True` for gradient unbroadcasting\n\n**Memory Management:**\n- Always create new tensors instead of in-place operations\n- Use weak references or manual cleanup for computation graphs\n- Consider implementing `__del__` methods for large tensor cleanup\n\n**Error Handling:**\n- Provide clear error messages for shape mismatches\n- Check for NaN/infinity in gradients during debugging\n- Validate tensor shapes before operations\n\n**Testing Integration:**\n- Use `pytest` for organized test discovery: `pytest tests/`\n- Implement property-based tests with random tensor shapes\n- Use `np.allclose()` for floating-point comparisons with tolerance\n\n\n## Data Model\n\n> **Milestone(s):** Milestone 1 (Tensor & Operations), Milestone 2 (Automatic Differentiation), Milestone 3 (Layers & Modules) - establishes core data structures that support tensor operations, gradient computation, and neural network organization\n\nThe data model forms the foundation of our neural network framework, defining the core data structures that enable tensor computations, automatic differentiation, and neural network construction. Think of the data model as the **blueprint for a smart calculator** - not just any calculator, but one that remembers every calculation it performs and can work backwards to figure out how each input contributed to the final result. Just as a calculator needs number storage, operation buttons, and a display, our framework needs tensors to store data, operations to transform it, and a way to organize everything into trainable models.\n\nThis smart calculator analogy reveals why our data model is more complex than simple arrays. When you press \"+\" on a regular calculator, the numbers get added and that's it - the calculation history is lost. But our neural network framework must remember that \"tensor A was added to tensor B to produce tensor C\" because later, when we discover tensor C was slightly wrong, we need to trace back and figure out how to adjust tensors A and B to fix the error. This backward tracing is the essence of automatic differentiation and gradient-based learning.\n\n![Neural Framework System Architecture](./diagrams/system-architecture.svg)\n\n![Tensor and Operation Type Hierarchy](./diagrams/tensor-relationships.svg)\n\n![Module System Organization](./diagrams/module-hierarchy.svg)\n\nThe data model consists of three interconnected hierarchies that work together to enable neural network computation. The **tensor hierarchy** provides the fundamental data containers with gradient tracking capabilities. The **operation hierarchy** defines the computational nodes that transform tensors while building the computation graph. The **module hierarchy** organizes trainable parameters and neural network layers into composable building blocks. These three hierarchies interact during every forward and backward pass, with tensors flowing through operations that are organized within modules.\n\nUnderstanding the relationships between these data structures is crucial for implementing the framework correctly. A tensor knows which operation created it (through `grad_fn`), an operation knows which tensors it operates on (through `inputs`), and a module knows which parameters it owns (through parameter registration). This forms a web of references that enables automatic differentiation to trace computation backwards and modules to expose their trainable parameters to optimizers.\n\n### Tensor Data Structure\n\nThe `Tensor` class serves as the fundamental building block of our framework, much like how **LEGO blocks** are the basic unit of construction in LEGO sets. But these aren't ordinary LEGO blocks - they're smart blocks that remember how they were assembled and can tell you exactly which other blocks contributed to building any structure. Each tensor carries both the actual numerical data (like the plastic of the LEGO block) and the metadata needed for automatic differentiation (like a memory chip inside each block that tracks its construction history).\n\nThe intelligence of our tensor lies in its ability to participate in **define-by-run** computation graphs. Unlike traditional arrays that simply hold numbers, our tensors are active participants in computation. When you add two tensors together, the result isn't just a new array with the sum - it's a new tensor that remembers it was created by adding two specific parent tensors. This memory enables the automatic differentiation engine to later traverse backwards through the computation, applying the chain rule to compute gradients.\n\nThe tensor's dual nature as both data container and computation node creates design challenges that don't exist with regular arrays. Every tensor operation must simultaneously produce correct numerical results and maintain the computation graph structure needed for gradient computation. This means tensors must carefully track their creation history, handle broadcasting for mismatched shapes, and manage memory efficiently while supporting both eager execution and gradient computation.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `data` | `np.ndarray` | The actual numerical data stored as a NumPy array, supporting N-dimensional arrays with efficient memory layout and vectorized operations |\n| `requires_grad` | `bool` | Flag indicating whether this tensor should track gradients during automatic differentiation - leaf tensors with this flag become optimization targets |\n| `grad` | `Optional[Tensor]` | Accumulated gradient tensor with same shape as data, initially None until backward pass computes and stores gradients |\n| `grad_fn` | `Optional[Operation]` | Reference to the operation that created this tensor, forming parent-child links in the computation graph for backpropagation |\n| `shape` | `Tuple[int, ...]` | Tuple describing tensor dimensions, cached from underlying NumPy array for quick access during shape validation and broadcasting |\n| `dtype` | `np.dtype` | Data type of tensor elements (float32, float64, int32, etc.), inherited from NumPy array for memory efficiency and type safety |\n\nThe `data` field contains the actual numerical values as a NumPy array, leveraging NumPy's mature implementation of N-dimensional arrays with efficient memory layout and vectorized operations. By building on NumPy rather than implementing our own array library, we inherit decades of optimization work and maintain compatibility with the broader scientific Python ecosystem. The tensor wraps this NumPy array with additional metadata needed for automatic differentiation.\n\nThe `requires_grad` flag determines whether this tensor participates in gradient computation. Leaf tensors (those not created by operations) with `requires_grad=True` become the variables that optimizers will update during training. Intermediate tensors inherit their gradient requirement from their inputs - if any input requires gradients, the output will also require gradients. This automatic propagation ensures the computation graph includes all tensors needed for backpropagation.\n\nThe `grad` field accumulates gradients during the backward pass, starting as None and being populated when `backward()` is called on some downstream tensor. Gradient accumulation is crucial because tensors can be used multiple times in a computation (like when a variable appears multiple times in an equation), and the total gradient is the sum of all partial contributions. The gradient tensor always has the same shape as the original tensor's data.\n\nThe `grad_fn` field creates the parent-child links that form the computation graph. When an operation produces a new tensor, it sets the tensor's `grad_fn` to point back to itself. This creates a directed acyclic graph where each operation node knows its input tensors, and each tensor knows the operation that created it. During backpropagation, this graph structure enables traversal from output tensors back to input tensors.\n\n> **Decision: NumPy Array Backend**\n> - **Context**: Need efficient N-dimensional array operations with broadcasting and vectorization\n> - **Options Considered**: \n>   1. Custom array implementation with full control over memory layout\n>   2. NumPy arrays with wrapper for gradient tracking\n>   3. Pure Python lists with manual broadcasting\n> - **Decision**: NumPy arrays with gradient tracking wrapper\n> - **Rationale**: NumPy provides battle-tested implementations of broadcasting, vectorized operations, and memory management. Building custom arrays would require months of optimization work with marginal educational benefit.\n> - **Consequences**: Inherits NumPy's performance characteristics and broadcasting semantics. Limits us to CPU computation without additional GPU backends, but enables focus on automatic differentiation concepts rather than array implementation details.\n\nThe tensor creation process involves careful initialization of all fields to ensure proper gradient tracking. When creating a leaf tensor (from raw data), only `data`, `requires_grad`, and `dtype` are specified, with `grad` starting as None and `grad_fn` remaining None to indicate this tensor wasn't created by an operation. When operations create new tensors, they set the `grad_fn` field appropriately and inherit `requires_grad` from their inputs.\n\nMemory management becomes critical when tensors form large computation graphs. The `grad_fn` references create a chain of objects that must be carefully managed to prevent memory leaks. In production frameworks, computation graphs are often released after each backward pass, but for educational purposes we maintain them to enable inspection and debugging. The tensor's lifecycle involves creation during forward pass, gradient accumulation during backward pass, and eventual cleanup when the computation graph is released.\n\n### Computation Graph Representation\n\nThe computation graph transforms our neural network framework from a simple calculator into a **time machine for mathematics**. Just as a time machine must record every moment to enable traveling backwards, our computation graph records every operation to enable gradient computation backwards through the network. The graph isn't built ahead of time like a blueprint - instead, it emerges dynamically during the forward pass as operations execute, creating a perfect record of the computational path taken.\n\nThis **define-by-run** approach means the computation graph reflects exactly what happened during the forward pass, capturing conditional logic, loops, and dynamic tensor shapes. Unlike static graph frameworks that require pre-declaring the computation structure, our dynamic graph adapts to the actual execution path. This flexibility comes at the cost of some optimization opportunities, but provides the intuitive programming model that has made PyTorch popular for research and education.\n\nThe graph structure follows a specific pattern: **operations are nodes, tensors are edges**. Each operation node knows its input tensors and can compute gradients with respect to those inputs. Each tensor edge knows which operation created it and whether it requires gradient computation. This dual representation enables efficient forward computation (following tensor edges) and backward gradient computation (following operation nodes in reverse).\n\n![Computation Graph Structure](./diagrams/computation-graph.svg)\n\nThe `Operation` base class defines the interface that all computational nodes must implement. Operations serve as the **factories** in our computation assembly line, taking input tensors, producing output tensors, and recording their transformation in the computation graph. Each operation must implement both forward computation (producing numerical results) and backward computation (producing gradients with respect to inputs).\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `forward` | `*inputs: Tensor` | `np.ndarray` | Computes forward pass result as NumPy array, implementing the mathematical operation on input data |\n| `backward` | `grad_output: np.ndarray` | `Tuple[np.ndarray, ...]` | Computes gradients with respect to each input using chain rule, returning tuple matching input count |\n| `__init__` | `*inputs: Tensor` | `None` | Stores input tensors and validates shapes/types, setting up operation node in computation graph |\n\nThe `inputs` field in every operation stores references to the input tensors, creating the parent-child relationships needed for backpropagation. When an operation executes, it stores these references permanently, ensuring the computation graph remains intact until explicitly released. This creates a memory chain where leaf tensors are kept alive by intermediate operations, which are kept alive by their output tensors.\n\nOperations must handle **broadcasting** carefully during both forward and backward passes. When tensors of different shapes are combined, NumPy's broadcasting rules automatically expand dimensions to make the operation valid. However, during backpropagation, gradients must be reduced back to the original tensor shapes to ensure dimensional consistency. This requires operations to remember the original shapes of their inputs and apply appropriate reduction operations during backward pass.\n\n> **Decision: Operation Node Design**\n> - **Context**: Need to represent mathematical operations in computation graph for automatic differentiation\n> - **Options Considered**:\n>   1. Function-based operations that capture closures\n>   2. Class-based operations with inheritance hierarchy\n>   3. Single operation class with type discriminator\n> - **Decision**: Class-based operations with inheritance hierarchy\n> - **Rationale**: Classes provide clear structure for forward/backward method pairs and enable specialized gradient computation for each operation type. Inheritance allows sharing common functionality while specializing gradient computation logic.\n> - **Consequences**: Requires creating operation subclasses for each mathematical operation but provides type safety and clear separation of concerns. Makes gradient computation debugging easier by isolating each operation's backward logic.\n\nThe operation hierarchy includes concrete implementations for common mathematical operations:\n\n| Operation | Forward Computation | Backward Gradient | Broadcasting Behavior |\n|-----------|-------------------|-------------------|---------------------|\n| `Add` | Element-wise sum `a + b` | Gradient passed unchanged to both inputs | Supports full NumPy broadcasting rules |\n| `Multiply` | Element-wise product `a * b` | Gradient multiplied by other input `grad * b`, `grad * a` | Supports full NumPy broadcasting rules |\n| `MatMul` | Matrix multiplication `a @ b` | `grad @ b.T`, `a.T @ grad` for 2D case | No broadcasting, requires compatible dimensions |\n| `ReLU` | `max(0, x)` element-wise | Gradient where input positive, zero elsewhere | No broadcasting, operates element-wise |\n\nEach operation implementation must carefully handle the mathematical correctness of both forward and backward computations. The forward pass simply implements the mathematical operation using NumPy functions. The backward pass requires applying the chain rule by computing the partial derivative of the operation with respect to each input and multiplying by the incoming gradient.\n\nThe **topological ordering** of operations becomes crucial during backpropagation. Gradients must flow backwards through the graph in an order that ensures all downstream gradients are computed before upstream gradients. This requires maintaining the computation graph structure during forward pass and performing a topological sort during backward pass. The sort ensures that when an operation's `backward` method is called, gradients from all downstream operations have already been computed and accumulated.\n\nGradient accumulation handling is essential for correctness when tensors are used multiple times in a computation. Consider a simple case like `y = x + x` where the same tensor `x` appears twice. The gradient of `y` with respect to `x` should be 2, not 1, because `x` contributes to `y` through two different paths. The computation graph must track all these paths and sum the gradients appropriately.\n\n| Graph Property | Implementation | Purpose |\n|----------------|---------------|---------|\n| Acyclic Structure | Operations only reference earlier tensors | Prevents circular dependencies in gradient computation |\n| Dynamic Construction | Graph built during forward pass execution | Supports conditional logic and dynamic shapes |\n| Reference Integrity | Operations maintain strong references to inputs | Keeps computation graph alive for gradient computation |\n| Topological Ordering | Depth-first traversal from output to inputs | Ensures correct gradient flow direction during backpropagation |\n\n### Parameter and Module Hierarchy\n\nThe module system transforms our collection of tensors and operations into a **LEGO construction system** for neural networks. Just as LEGO sets provide specialized pieces (wheels, bricks, windows) that snap together to build complex structures, our module system provides specialized neural network components (linear layers, activations, normalization) that compose to build sophisticated models. Each module knows what pieces it contains and can recursively report all its trainable parts to parent modules.\n\nThe module hierarchy solves the **parameter organization problem** that emerges when building complex neural networks. A large model might contain millions of parameters spread across hundreds of layers, each with different initialization requirements, learning rates, and regularization settings. Without systematic organization, these parameters become impossible to manage. The module system provides a structured way to group related parameters, nest modules within modules, and recursively collect all trainable tensors for optimization.\n\nThe `Module` base class serves as the **universal connector** that enables arbitrary composition of neural network components. Every neural network layer, activation function, and complete model inherits from this base class, ensuring consistent interfaces for parameter management, forward propagation, and recursive traversal. This design enables building networks by simply declaring modules and connecting them, without manually tracking parameters or implementing forward pass logic.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `forward` | `*inputs: Tensor` | `Tensor` | Abstract method that subclasses implement to define layer's computation, automatically called by `__call__` |\n| `parameters` | `recursive: bool = True` | `List[Tensor]` | Recursively collects all trainable tensors from this module and submodules, used by optimizers for updates |\n| `register_parameter` | `name: str, param: Tensor` | `None` | Registers a tensor as a trainable parameter, adding it to the module's parameter dictionary for collection |\n| `register_module` | `name: str, module: Module` | `None` | Registers a submodule, enabling recursive parameter collection and nested module organization |\n| `__call__` | `*inputs: Tensor` | `Tensor` | Invokes forward method with additional hooks and validation, providing consistent interface for module execution |\n\nThe parameter registration system enables modules to declare their trainable tensors in a structured way. When a module creates parameters (like weight matrices), it calls `register_parameter` to add them to an internal dictionary. This dictionary enables the `parameters()` method to collect all trainable tensors recursively. Parameter registration happens during module initialization, ensuring parameters are available for optimization before any forward pass.\n\nSubmodule registration follows the same pattern but for child modules rather than individual tensors. When a module contains other modules (like a Sequential containing Linear layers), it registers them using `register_module`. This creates a tree structure where each module knows its children, enabling recursive operations like parameter collection, device movement, and serialization.\n\n> **Decision: Recursive Parameter Collection**\n> - **Context**: Need to gather all trainable parameters from complex nested module hierarchies for optimizer updates\n> - **Options Considered**:\n>   1. Manual parameter list maintenance by users\n>   2. Global parameter registry with automatic discovery\n>   3. Recursive traversal of module hierarchy\n> - **Decision**: Recursive traversal of module hierarchy\n> - **Rationale**: Recursive traversal provides automatic parameter discovery without global state or manual bookkeeping. Each module only needs to track its direct parameters and submodules, with recursion handling arbitrary nesting depth.\n> - **Consequences**: Enables compositional module design where complex models are built by nesting simple modules. Requires careful implementation to avoid infinite recursion or duplicate parameter collection.\n\nThe module hierarchy includes several essential concrete implementations:\n\n| Module Type | Parameters | Computation | Typical Usage |\n|-------------|------------|-------------|---------------|\n| `Linear` | `weight: Tensor`, `bias: Optional[Tensor]` | `y = x @ weight.T + bias` | Fully connected layers, output projections |\n| `Sequential` | `modules: List[Module]` | Chains forward passes through contained modules | Building feedforward networks, feature extractors |\n| `ReLU` | None (no parameters) | `max(0, x)` element-wise | Nonlinear activations between linear layers |\n| `Sigmoid` | None (no parameters) | `1 / (1 + exp(-x))` element-wise | Output activations for binary classification |\n\nThe `Linear` module demonstrates parameter management principles. During initialization, it creates weight and bias tensors with appropriate shapes and initialization values, then registers them as parameters. The forward method implements matrix multiplication followed by bias addition, handling batch dimensions correctly. The gradient computation happens automatically through the underlying tensor operations.\n\nThe `Sequential` module demonstrates composition patterns. It accepts a list of modules during initialization and registers each one as a submodule. Its forward method simply chains the modules together, passing the output of each module as input to the next. This enables building complex networks with a simple declarative syntax.\n\nParameter initialization presents important design considerations for training stability. Different layer types require different initialization strategies to prevent vanishing or exploding gradients. The module system must provide sensible defaults while allowing customization for advanced users.\n\n| Initialization Strategy | Distribution | Typical Usage | Mathematical Justification |\n|------------------------|--------------|---------------|---------------------------|\n| Xavier Uniform | `Uniform(-sqrt(6/(fan_in+fan_out)), sqrt(6/(fan_in+fan_out)))` | Tanh/Sigmoid activations | Maintains activation variance across layers |\n| He Normal | `Normal(0, sqrt(2/fan_in))` | ReLU activations | Accounts for ReLU's variance reduction |\n| Zero Initialization | `Constant(0)` | Bias terms | Prevents systematic shifts in initial activations |\n| Identity | `Eye()` with appropriate scaling | Residual connections | Enables identity mapping for skip connections |\n\nThe module lifecycle involves initialization, repeated forward passes, and parameter updates. During initialization, modules create and register their parameters with appropriate initialization. During training, the optimizer calls `parameters()` to collect all trainable tensors, then updates them based on computed gradients. The module system must ensure parameters remain correctly registered throughout this lifecycle.\n\nModule composition enables powerful design patterns for building complex architectures. Modules can be nested arbitrarily deeply, with each level adding its own parameters and functionality. This compositional approach scales from simple feedforward networks to complex architectures like transformers or residual networks, all using the same underlying module interface.\n\n⚠️ **Pitfall: Parameter Not Registered**\n\nA common mistake is creating parameter tensors without registering them with the module system. For example:\n\n```python\nclass BrokenLinear(Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # WRONG: Creates parameter but doesn't register it\n        self.weight = Tensor(np.random.randn(out_features, in_features), requires_grad=True)\n```\n\nThis breaks parameter collection because `parameters()` only returns registered parameters. The optimizer won't see these parameters and won't update them during training. The fix is always calling `register_parameter`:\n\n```python\nclass CorrectLinear(Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        weight = Tensor(np.random.randn(out_features, in_features), requires_grad=True)\n        self.register_parameter('weight', weight)  # CORRECT: Registers parameter\n```\n\n⚠️ **Pitfall: Circular Module References**\n\nAnother common mistake is creating circular references between modules, which can cause infinite recursion during parameter collection. This typically happens when implementing attention mechanisms or recurrent networks incorrectly:\n\n```python\n# WRONG: Creates circular reference\nmodule_a.child = module_b\nmodule_b.parent = module_a  # Circular reference\n```\n\nThe recursive parameter collection will infinitely traverse this cycle. The fix is carefully designing module hierarchies to be truly hierarchical (tree-like) rather than containing cycles.\n\n⚠️ **Pitfall: In-Place Parameter Modification**\n\nModifying parameters in-place during forward pass breaks gradient computation:\n\n```python\ndef forward(self, x):\n    self.weight += 0.01  # WRONG: In-place modification breaks gradients\n    return x @ self.weight.T\n```\n\nIn-place modifications destroy the computation graph needed for backpropagation. Parameter updates should only happen through optimizers during the update step, never during forward pass.\n\n### Implementation Guidance\n\nBuilding the data model requires careful attention to object relationships and memory management. The tensor, operation, and module hierarchies must work together seamlessly while maintaining clear separation of responsibilities.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Array Backend | NumPy arrays with manual gradient tracking | Custom array class with integrated autodiff |\n| Graph Storage | Python lists and dictionaries | Specialized graph data structures |\n| Memory Management | Python garbage collection with manual graph cleanup | Weak references and custom memory pools |\n| Parameter Storage | Python dictionaries with string keys | Custom parameter containers with type safety |\n\n**Recommended File Structure:**\n```\nframework/\n  tensor.py              ← Core Tensor class with gradient tracking\n  operations.py          ← Operation base class and implementations (Add, Multiply, MatMul, etc.)\n  module.py             ← Module base class and parameter management\n  layers.py             ← Concrete layer implementations (Linear, Sequential, etc.)\n  functional.py         ← Functional versions of operations for advanced users\n  __init__.py           ← Public API exports\ntests/\n  test_tensor.py        ← Tensor creation, arithmetic, and gradient tests\n  test_operations.py    ← Forward/backward correctness for all operations\n  test_modules.py       ← Parameter collection and module composition tests\n```\n\n**Core Tensor Implementation (starter template):**\n\n```python\nimport numpy as np\nfrom typing import Optional, Tuple, List, Union\n\nclass Tensor:\n    \"\"\"N-dimensional array with automatic differentiation support.\"\"\"\n    \n    def __init__(self, \n                 data: Union[np.ndarray, list, float, int], \n                 requires_grad: bool = False,\n                 grad_fn: Optional['Operation'] = None):\n        \"\"\"Initialize tensor with data and gradient tracking.\n        \n        Args:\n            data: Numerical data as NumPy array or convertible type\n            requires_grad: Whether to track gradients for this tensor\n            grad_fn: Operation that created this tensor (None for leaf tensors)\n        \"\"\"\n        # TODO 1: Convert data to NumPy array if needed\n        # TODO 2: Set requires_grad flag\n        # TODO 3: Initialize grad to None\n        # TODO 4: Store grad_fn reference\n        # TODO 5: Cache shape and dtype from NumPy array\n        # Hint: Use np.asarray() for robust data conversion\n        pass\n    \n    def backward(self, gradient: Optional['Tensor'] = None) -> None:\n        \"\"\"Compute gradients by backpropagating through computation graph.\n        \n        Args:\n            gradient: Incoming gradient tensor (defaults to ones for scalar output)\n        \"\"\"\n        # TODO 1: Handle gradient=None case by creating ones tensor with same shape\n        # TODO 2: Accumulate gradient in self.grad (handle None case)\n        # TODO 3: If grad_fn exists, call its backward method\n        # TODO 4: Propagate gradients to input tensors\n        # Hint: Use topological sort to ensure correct ordering\n        pass\n    \n    def __add__(self, other: 'Tensor') -> 'Tensor':\n        \"\"\"Element-wise addition with gradient tracking.\"\"\"\n        # TODO 1: Import Add operation class\n        # TODO 2: Create Add operation with self and other as inputs\n        # TODO 3: Compute forward result using operation\n        # TODO 4: Create result tensor with appropriate grad_fn\n        # TODO 5: Set requires_grad if either input requires gradients\n        pass\n    \n    # TODO: Implement __mul__, __sub__, __truediv__, matmul methods\n    # Follow same pattern as __add__ with appropriate operation classes\n```\n\n**Operation Base Class (complete implementation):**\n\n```python\nfrom abc import ABC, abstractmethod\nimport numpy as np\nfrom typing import Tuple\n\nclass Operation(ABC):\n    \"\"\"Base class for all operations in computation graph.\"\"\"\n    \n    def __init__(self, *inputs: 'Tensor'):\n        \"\"\"Store input tensors for gradient computation.\"\"\"\n        self.inputs = inputs\n    \n    @abstractmethod\n    def forward(self, *input_arrays: np.ndarray) -> np.ndarray:\n        \"\"\"Compute forward pass result.\n        \n        Args:\n            input_arrays: NumPy arrays from input tensors\n            \n        Returns:\n            Result as NumPy array\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def backward(self, grad_output: np.ndarray) -> Tuple[np.ndarray, ...]:\n        \"\"\"Compute gradients with respect to inputs.\n        \n        Args:\n            grad_output: Gradient of loss with respect to operation output\n            \n        Returns:\n            Tuple of gradients with respect to each input\n        \"\"\"\n        pass\n\n# Utility functions for gradient handling\ndef unbroadcast_gradient(grad: np.ndarray, target_shape: Tuple[int, ...]) -> np.ndarray:\n    \"\"\"Reduce broadcasted gradient back to target shape.\"\"\"\n    # Sum over dimensions that were broadcasted\n    # This is provided as complete utility since broadcasting is complex\n    # but not the main learning objective\n    \n    # Handle scalar case\n    if target_shape == ():\n        return np.sum(grad)\n    \n    # Sum over extra dimensions\n    ndim_extra = grad.ndim - len(target_shape)\n    for _ in range(ndim_extra):\n        grad = np.sum(grad, axis=0)\n    \n    # Sum over broadcasted dimensions\n    for i, (grad_dim, target_dim) in enumerate(zip(grad.shape, target_shape)):\n        if target_dim == 1 and grad_dim > 1:\n            grad = np.sum(grad, axis=i, keepdims=True)\n    \n    return grad\n```\n\n**Module Base Class (skeleton for implementation):**\n\n```python\nclass Module:\n    \"\"\"Base class for all neural network modules.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize module with empty parameter and submodule dictionaries.\"\"\"\n        # TODO 1: Create empty dictionary for parameters\n        # TODO 2: Create empty dictionary for submodules\n        # Hint: Use descriptive names like _parameters and _modules\n        pass\n    \n    def register_parameter(self, name: str, param: 'Tensor') -> None:\n        \"\"\"Register a tensor as a trainable parameter.\n        \n        Args:\n            name: Parameter name for identification\n            param: Tensor with requires_grad=True\n        \"\"\"\n        # TODO 1: Validate that param is a Tensor\n        # TODO 2: Validate that param.requires_grad is True\n        # TODO 3: Store in parameters dictionary\n        # TODO 4: Set as attribute on self for easy access\n        pass\n    \n    def register_module(self, name: str, module: 'Module') -> None:\n        \"\"\"Register a submodule for recursive operations.\"\"\"\n        # TODO 1: Validate that module is a Module instance\n        # TODO 2: Store in modules dictionary  \n        # TODO 3: Set as attribute on self\n        pass\n    \n    def parameters(self, recursive: bool = True) -> List['Tensor']:\n        \"\"\"Collect all trainable parameters.\n        \n        Args:\n            recursive: Whether to include submodule parameters\n            \n        Returns:\n            List of all parameter tensors\n        \"\"\"\n        # TODO 1: Start with parameters from this module\n        # TODO 2: If recursive, iterate through submodules\n        # TODO 3: Recursively call parameters() on each submodule\n        # TODO 4: Combine all parameters into single list\n        # Hint: Use list comprehension or extend() for efficiency\n        pass\n    \n    @abstractmethod\n    def forward(self, *inputs: 'Tensor') -> 'Tensor':\n        \"\"\"Define module computation - must be implemented by subclasses.\"\"\"\n        pass\n    \n    def __call__(self, *inputs: 'Tensor') -> 'Tensor':\n        \"\"\"Execute forward pass with validation.\"\"\"\n        # This is provided complete since it's infrastructure, not core learning\n        return self.forward(*inputs)\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the data model, verify correct behavior with these tests:\n\n```python\n# Test 1: Tensor creation and basic properties\nx = Tensor([[1, 2], [3, 4]], requires_grad=True)\nassert x.shape == (2, 2)\nassert x.requires_grad == True\nassert x.grad is None\n\n# Test 2: Operation creates computation graph\ny = x + x\nassert y.grad_fn is not None\nassert isinstance(y.grad_fn, Add)\nassert y.grad_fn.inputs == (x, x)\n\n# Test 3: Module parameter collection\nlinear = Linear(2, 3)\nparams = linear.parameters()\nassert len(params) == 2  # weight and bias\nassert all(p.requires_grad for p in params)\n\n# Test 4: Nested module hierarchy\nmodel = Sequential([Linear(2, 4), ReLU(), Linear(4, 1)])\nparams = model.parameters()\nassert len(params) == 4  # 2 linear layers × 2 parameters each\n```\n\nExpected output should show tensors with correct shapes, operations properly linked in computation graph, and parameters correctly collected from module hierarchy. Any assertion failures indicate issues with data structure implementation that must be fixed before proceeding to automatic differentiation.\n\n**Common Implementation Issues:**\n\n| Symptom | Likely Cause | How to Debug | Fix |\n|---------|--------------|--------------|-----|\n| `grad_fn` is None after operations | Operation not setting grad_fn on result tensor | Print grad_fn after each operation | Set `result.grad_fn = operation` in tensor arithmetic methods |\n| Parameters not found by optimizer | Parameters not registered with module | Print `module._parameters.keys()` | Call `self.register_parameter()` in module `__init__` |\n| Shape errors during backward pass | Broadcasting not handled in gradient computation | Print tensor shapes before/after operations | Implement `unbroadcast_gradient()` for all operations |\n| Circular import errors | Tensor and Operation importing each other | Check import structure | Use string type hints and import at bottom of files |\n\nThe data model serves as the foundation for all subsequent functionality. Solid implementation of these core data structures is essential for correct automatic differentiation and neural network training. Take time to thoroughly test and debug the tensor, operation, and module classes before proceeding to implement the automatic differentiation engine.\n\n\n## Tensor Operations Layer (Milestone 1)\n\n> **Milestone(s):** Milestone 1 (Tensor & Operations) - implements the foundation tensor class with N-dimensional arrays, broadcasting, and basic arithmetic operations\n\nThe tensor operations layer forms the bedrock of our neural network framework, providing the fundamental data structure and computational primitives that enable machine learning. This layer implements tensors as intelligent multi-dimensional arrays that not only store numerical data but also track their computational lineage for automatic differentiation. The challenge lies in creating tensors that seamlessly integrate NumPy-style operations with gradient tracking while maintaining clean, intuitive APIs that feel familiar to users of existing frameworks.\n\n### Tensor as Smart Arrays\n\nThink of tensors as **smart notebooks** that not only contain your mathematical calculations but also remember exactly how each number was computed. Imagine you're working through a complex physics problem in a notebook - a regular notebook just shows your final answers, but a \"smart notebook\" would remember that \"this velocity came from dividing distance by time\" and \"this acceleration came from the derivative of velocity.\" When you later need to understand how changing the initial distance affects the final result, the smart notebook can trace backwards through all the computational steps automatically.\n\nThis analogy captures the essence of what makes tensors \"smart\" compared to plain NumPy arrays. A `Tensor` wraps numerical data with additional metadata that enables automatic differentiation. The tensor remembers not just its current values, but also whether those values need gradients computed (`requires_grad`), what operation created it (`grad_fn`), and where to store the computed gradients (`grad`). This metadata transforms simple array operations into building blocks for neural network training.\n\nThe mental model for tensor lifecycle follows three phases: **creation, computation, and gradient flow**. During creation, we initialize tensors with data and specify whether they participate in gradient computation. During the computation phase, operations between tensors automatically construct a computation graph by linking output tensors back to their input tensors through operation nodes. Finally, during gradient flow (backpropagation), this graph enables automatic computation of how changes to any tensor would affect the final loss.\n\nConsider a simple example to illustrate this smart behavior: when we multiply two tensors `a * b`, a regular NumPy operation just produces numerical results. However, a smart tensor operation produces results that \"remember\" they came from multiplication of `a` and `b`. Later, when gradients flow backwards, the multiplication operation can automatically compute that the gradient with respect to `a` should be multiplied by the values in `b`, and vice versa. This automatic bookkeeping eliminates the error-prone manual gradient calculations that plague traditional numerical optimization.\n\nThe \"smartness\" extends to shape and broadcasting intelligence as well. Smart tensors automatically handle dimension mismatches through NumPy-compatible broadcasting while ensuring that gradients flow back to their original shapes correctly. This means users can write intuitive mathematical expressions like `tensor_2x3 + scalar` without worrying about the underlying shape manipulations, while the framework handles both forward computation and backward gradient unbroadcasting automatically.\n\n### Tensor API Design\n\nThe `Tensor` class provides a comprehensive interface that balances mathematical expressiveness with gradient tracking capabilities. The API design follows the principle of **least surprise** - operations that work with NumPy arrays should work identically with tensors, while gradient-specific functionality remains opt-in through explicit flags and methods.\n\nThe core tensor creation interface provides multiple pathways for instantiating tensors from different data sources:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `Tensor(data, requires_grad=False)` | `data: array_like, requires_grad: bool` | `Tensor` | Primary constructor accepting NumPy arrays, lists, or scalars |\n| `zeros(shape, requires_grad=False)` | `shape: Tuple[int, ...], requires_grad: bool` | `Tensor` | Create tensor filled with zeros in specified shape |\n| `ones(shape, requires_grad=False)` | `shape: Tuple[int, ...], requires_grad: bool` | `Tensor` | Create tensor filled with ones in specified shape |\n| `randn(shape, requires_grad=False)` | `shape: Tuple[int, ...], requires_grad: bool` | `Tensor` | Create tensor with random normal distribution values |\n| `from_numpy(array, requires_grad=False)` | `array: np.ndarray, requires_grad: bool` | `Tensor` | Wrap existing NumPy array as tensor |\n\nThe arithmetic operations interface leverages Python's operator overloading to provide natural mathematical syntax while maintaining gradient tracking:\n\n| Operation | Method | Parameters | Returns | Description |\n|-----------|---------|------------|---------|-------------|\n| Addition | `__add__(other)` | `other: Union[Tensor, Number]` | `Tensor` | Element-wise addition with broadcasting |\n| Subtraction | `__sub__(other)` | `other: Union[Tensor, Number]` | `Tensor` | Element-wise subtraction with broadcasting |\n| Multiplication | `__mul__(other)` | `other: Union[Tensor, Number]` | `Tensor` | Element-wise multiplication with broadcasting |\n| Division | `__truediv__(other)` | `other: Union[Tensor, Number]` | `Tensor` | Element-wise division with broadcasting |\n| Matrix Multiply | `matmul(other)` | `other: Tensor` | `Tensor` | Matrix multiplication following Einstein notation |\n| Power | `__pow__(other)` | `other: Union[Tensor, Number]` | `Tensor` | Element-wise exponentiation |\n\nThe shape manipulation and inspection interface provides essential tensor metadata access:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `shape` | Property | `Tuple[int, ...]` | Current tensor dimensions |\n| `dtype` | Property | `np.dtype` | Data type of tensor elements |\n| `ndim` | Property | `int` | Number of tensor dimensions |\n| `size` | Property | `int` | Total number of elements |\n| `reshape(shape)` | `shape: Tuple[int, ...]` | `Tensor` | Return tensor with new shape, same data |\n| `transpose()` | None | `Tensor` | Return tensor with reversed dimension order |\n| `squeeze(dim=None)` | `dim: Optional[int]` | `Tensor` | Remove dimensions of size 1 |\n| `unsqueeze(dim)` | `dim: int` | `Tensor` | Add dimension of size 1 at specified position |\n\nThe gradient computation interface enables automatic differentiation integration:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `backward(gradient=None)` | `gradient: Optional[Tensor]` | `None` | Initiate backpropagation from this tensor |\n| `detach()` | None | `Tensor` | Create new tensor sharing data but no gradient tracking |\n| `requires_grad_(requires_grad)` | `requires_grad: bool` | `Tensor` | In-place modification of gradient tracking flag |\n| `zero_grad()` | None | `None` | Reset accumulated gradients to zero |\n\nThe tensor data access interface provides NumPy compatibility for inspection and conversion:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `numpy()` | None | `np.ndarray` | Return underlying NumPy array (detached from gradients) |\n| `item()` | None | `Number` | Return single element as Python scalar (for 0-d tensors) |\n| `tolist()` | None | `List` | Convert tensor to nested Python lists |\n\n> **Design Insight: Operator Overloading Strategy**\n>\n> The decision to overload Python's arithmetic operators (`+`, `-`, `*`, `/`) rather than requiring explicit method calls (`add()`, `sub()`, etc.) prioritizes mathematical readability. Neural network code involves extensive mathematical expressions, and operator overloading makes these expressions more natural to read and write. The trade-off is slightly more complex implementation (handling both tensor-tensor and tensor-scalar operations), but the usability benefit justifies this complexity.\n\n### Broadcasting Implementation\n\nBroadcasting represents one of the most sophisticated aspects of tensor operations, automatically handling shape mismatches in arithmetic operations while preserving mathematical semantics and gradient flow. The implementation follows NumPy's broadcasting rules exactly, ensuring compatibility with existing numerical code while extending the mechanism to support gradient backpropagation.\n\nThe **broadcasting algorithm** operates through a systematic shape alignment and expansion process:\n\n1. **Shape Alignment Phase**: Align tensor shapes by padding shorter shapes with dimensions of size 1 on the left. For example, shapes `(3, 4)` and `(4,)` become `(3, 4)` and `(1, 4)` respectively.\n\n2. **Compatibility Checking Phase**: Verify that aligned dimensions are either equal or one of them is size 1. Dimensions `(3, 4)` and `(1, 4)` are compatible because the first dimension has sizes 3 and 1 (one is size 1), and the second dimension has matching sizes 4 and 4.\n\n3. **Output Shape Determination Phase**: The broadcasted shape takes the maximum size along each dimension. From `(3, 4)` and `(1, 4)`, the output shape becomes `(3, 4)` (max of 3 and 1, max of 4 and 4).\n\n4. **Data Expansion Phase**: Conceptually expand tensors to the broadcasted shape by repeating elements along dimensions of size 1. The tensor with shape `(1, 4)` gets repeated 3 times along the first dimension.\n\n5. **Element-wise Operation Phase**: Perform the requested operation (addition, multiplication, etc.) on the expanded tensors element by element.\n\nThe broadcasting shape computation can be implemented through this systematic comparison:\n\n| Input Shape A | Input Shape B | Aligned A | Aligned B | Compatible? | Output Shape |\n|---------------|---------------|-----------|-----------|-------------|--------------|\n| `(3, 4)` | `(4,)` | `(3, 4)` | `(1, 4)` | ✓ | `(3, 4)` |\n| `(2, 1, 3)` | `(1, 5, 1)` | `(2, 1, 3)` | `(1, 5, 1)` | ✓ | `(2, 5, 3)` |\n| `(3, 4)` | `(2, 4)` | `(3, 4)` | `(2, 4)` | ✗ | Error |\n| `(5,)` | `(3, 1)` | `(1, 5)` | `(3, 1)` | ✓ | `(3, 5)` |\n\nThe gradient unbroadcasting process reverses this expansion to ensure gradients flow back to their original tensor shapes. This involves identifying which dimensions were expanded during broadcasting and summing the gradients along those dimensions:\n\n1. **Expansion Detection Phase**: Compare the original tensor shape with the broadcasted result shape to identify expanded dimensions.\n\n2. **Summation Phase**: Sum gradients along dimensions that were expanded from size 1 to larger sizes.\n\n3. **Reshape Phase**: Remove dimensions that were added during left-padding by reshaping to the original number of dimensions.\n\nConsider gradient flow through a broadcast operation `a + b` where `a` has shape `(3, 4)` and `b` has shape `(4,)`. The forward pass broadcasts `b` to shape `(3, 4)` and performs element-wise addition. During backpropagation, the gradient with shape `(3, 4)` must be unbroadcast to match the original shapes: the gradient for `a` keeps shape `(3, 4)`, while the gradient for `b` gets summed along the first dimension to produce shape `(4,)`.\n\n> **Critical Implementation Detail: Memory Efficiency**\n>\n> Actual broadcasting implementations avoid physically expanding arrays in memory. Instead, they use NumPy's stride manipulation to create views that behave as if the data were expanded. This saves enormous amounts of memory when broadcasting small tensors to large shapes. However, the conceptual model of expansion remains useful for understanding gradient flow.\n\n### Tensor Design Decisions\n\nThe tensor implementation requires careful architectural decisions that balance performance, usability, and educational clarity. Each decision involves trade-offs between competing priorities, and understanding these trade-offs illuminates the complexity underlying seemingly simple tensor operations.\n\n> **Decision: NumPy Backend for Data Storage**\n> - **Context**: Need underlying numerical array implementation for tensor data storage and computation\n> - **Options Considered**: Pure Python lists, custom C extension, NumPy arrays, PyTorch tensors as backend\n> - **Decision**: Use NumPy arrays as the underlying data storage mechanism\n> - **Rationale**: NumPy provides mature, optimized implementations of broadcasting, mathematical operations, and memory management. Building equivalent functionality from scratch would require substantial C/C++ development and optimization work. NumPy's broadcasting rules are the de facto standard that users expect. The educational goal focuses on automatic differentiation concepts rather than low-level array implementation.\n> - **Consequences**: Inherits NumPy's performance characteristics and memory layout. Simplifies implementation by delegating numerical computation to NumPy. Limits control over memory allocation patterns and GPU integration options. Creates dependency on NumPy version compatibility.\n\n| Backend Option | Performance | Implementation Complexity | Educational Value | GPU Support |\n|----------------|-------------|---------------------------|-------------------|-------------|\n| NumPy arrays | High | Low | High (focus on autodiff) | External only |\n| Pure Python | Very Low | High | Medium | None |\n| Custom C extension | Very High | Very High | Low (distracted by C) | Custom |\n| PyTorch backend | Very High | Medium | Low (defeats purpose) | Native |\n\n> **Decision: Eager Execution Model**\n> - **Context**: Must choose between eager execution (operations execute immediately) vs lazy evaluation (operations build symbolic graphs for later execution)\n> - **Options Considered**: Eager execution like PyTorch, lazy evaluation like TensorFlow 1.x, hybrid approach\n> - **Decision**: Implement eager execution where operations execute immediately and build computation graphs dynamically\n> - **Rationale**: Eager execution provides intuitive debugging experience since tensors contain actual values that can be inspected immediately. The define-by-run approach matches how users naturally think about mathematical operations. Educational clarity benefits from seeing immediate results rather than symbolic placeholders. Implementation complexity is lower without needing separate compilation and execution phases.\n> - **Consequences**: Enables immediate value inspection and debugging. Requires computation graph construction during forward pass. May have slightly higher memory usage due to storing intermediate results. Limits some optimization opportunities available in static graph systems.\n\n> **Decision: Reference-Based Gradient Storage**\n> - **Context**: Need to decide how to store and manage gradient information for each tensor\n> - **Options Considered**: Store gradients directly in tensors, separate gradient dictionary, gradient tensors as separate objects\n> - **Decision**: Store gradients as optional `Tensor` references within each tensor object\n> - **Rationale**: Direct storage provides intuitive access pattern (`tensor.grad`) matching PyTorch conventions. Simplifies gradient accumulation since each tensor owns its gradient storage. Enables automatic gradient initialization and management. Memory overhead only affects tensors that actually require gradients.\n> - **Consequences**: Each tensor carries gradient storage overhead even when not needed. Simplifies API design with direct `.grad` attribute access. Creates potential for circular references if not managed carefully. Enables straightforward gradient accumulation semantics.\n\n> **Decision: Immutable Tensor Operations**\n> - **Context**: Must decide whether arithmetic operations modify existing tensors (in-place) or create new tensors\n> - **Options Considered**: All operations in-place for memory efficiency, all operations create new tensors, mixed approach with explicit in-place variants\n> - **Decision**: Implement all basic arithmetic operations as immutable (creating new tensors) with optional in-place variants\n> - **Rationale**: Immutable operations prevent accidental modification of tensors that might be referenced elsewhere in the computation graph. Simplifies reasoning about gradient flow since intermediate results remain stable. Matches functional programming principles and mathematical intuition where `a + b` doesn't modify `a` or `b`. In-place operations can break gradient computation if not handled carefully.\n> - **Consequences**: Higher memory usage due to creating intermediate tensors. Clearer semantics for gradient computation. May require explicit memory management in memory-constrained scenarios. Provides predictable behavior that matches mathematical expectations.\n\nThe data type handling strategy follows NumPy's conventions while adding gradient-specific considerations:\n\n| Data Type | Use Case | Gradient Support | Memory per Element |\n|-----------|----------|------------------|-------------------|\n| `float32` | Default for neural networks | Yes | 4 bytes |\n| `float64` | High-precision computations | Yes | 8 bytes |\n| `int32` | Indices and discrete values | No | 4 bytes |\n| `int64` | Large integer indices | No | 8 bytes |\n| `bool` | Masks and conditions | No | 1 byte |\n| `complex64` | Complex number computations | Limited | 8 bytes |\n\n### Common Tensor Implementation Pitfalls\n\nImplementing tensors correctly requires navigating several subtle pitfalls that frequently trap developers building neural network frameworks. These pitfalls often manifest as hard-to-debug issues during training, making awareness and prevention crucial for successful implementation.\n\n⚠️ **Pitfall: In-Place Operations Breaking Gradient Flow**\n\nThe most dangerous pitfall involves in-place modifications of tensors that participate in gradient computation. When a tensor's values change after being used in the computation graph, the stored references become invalid, leading to incorrect gradient calculations.\n\nConsider this problematic sequence: create tensor `a`, compute `b = a * 2`, then modify `a` in-place with `a += 1`. When gradients flow back through the multiplication operation, the gradient computation tries to access the original values of `a`, but finds the modified values instead. This produces incorrect gradients that can cause training divergence or mysterious convergence failures.\n\nThe fix involves strict separation between tensors used in gradient computation and those being modified. Implement safeguards that detect when in-place operations would affect tensors with `requires_grad=True` or tensors referenced in active computation graphs. Provide clear error messages that identify the problematic operation and suggest alternatives like `a = a + 1` instead of `a += 1`.\n\n⚠️ **Pitfall: Broadcasting Gradient Shape Mismatches**\n\nBroadcasting operations create subtle gradient flow issues when the backward pass attempts to return gradients to their original shapes. The forward pass successfully broadcasts tensors to compatible shapes, but the backward pass must carefully unbroadcast gradients to match the input tensor shapes.\n\nThis manifests when broadcasting a scalar to a matrix shape during forward pass, then trying to assign a matrix-shaped gradient back to the scalar tensor during backward pass. The shapes are fundamentally incompatible, causing either runtime errors or silent gradient corruption.\n\nThe solution requires implementing robust `unbroadcast_gradient` functionality that identifies which dimensions were expanded during broadcasting and appropriately sums gradients along those dimensions. Every arithmetic operation must store sufficient metadata about the original input shapes to enable correct gradient unbroadcasting.\n\n⚠️ **Pitfall: Memory Aliasing in Tensor Construction**\n\nMemory aliasing occurs when multiple tensors share the same underlying NumPy array storage without proper isolation. Modifications to one tensor unexpectedly affect other tensors, creating confusing bugs where seemingly independent operations interfere with each other.\n\nThis typically happens when tensors are created by slicing existing tensors or when NumPy arrays are reused across multiple tensor objects. The issue becomes particularly problematic during gradient computation, where gradient updates to one tensor may corrupt gradients for other tensors sharing the same memory.\n\nPrevention requires careful copying of data during tensor construction and explicit checks for memory sharing. Implement tensor creation methods that default to copying data unless sharing is explicitly requested. Provide debugging utilities that can detect memory aliasing and warn users about potentially problematic sharing patterns.\n\n⚠️ **Pitfall: Gradient Accumulation Race Conditions**\n\nIn complex computation graphs where the same tensor is used multiple times, gradients must be accumulated (summed) rather than overwritten. Failing to implement proper gradient accumulation leads to lost gradient information and incorrect parameter updates.\n\nThis occurs when a tensor appears in multiple branches of a computation graph, such as `c = a + a` where tensor `a` is used twice. During backpropagation, both uses of `a` contribute gradients, and these contributions must be summed together. If the second gradient overwrites the first instead of adding to it, half the gradient information disappears.\n\nThe solution involves implementing gradient accumulation logic that detects when gradients already exist and adds new contributions rather than replacing them. Initialize gradients to zero rather than leaving them uninitialized, and ensure all gradient assignment operations use addition semantics.\n\n⚠️ **Pitfall: Dtype Promotion Confusion**\n\nArithmetic operations between tensors with different data types require careful type promotion to maintain numerical precision and prevent unexpected behavior. NumPy's automatic type promotion rules can be complex and counterintuitive, leading to precision loss or memory usage increases.\n\nFor example, operations between `float32` and `float64` tensors typically promote results to `float64`, doubling memory usage unexpectedly. Operations between integer and floating-point tensors may produce different results than expected due to precision differences in the promotion process.\n\nAddress this by implementing explicit type promotion policies that match user expectations and provide clear documentation about type conversion behavior. Consider defaulting to `float32` for neural network applications where double precision is rarely needed, but provide mechanisms for users to control type promotion explicitly when required.\n\n### Implementation Guidance\n\nBuilding robust tensors requires careful technology choices and systematic implementation of the core data structures and operations. This guidance provides complete starter infrastructure and skeleton code for implementing the tensor layer while maintaining focus on the educational objectives.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Array Backend | NumPy arrays (recommended) | Custom C extension with Python bindings |\n| Data Types | NumPy dtypes (float32, int32, bool) | Extended precision types and custom numeric types |\n| Memory Management | Python garbage collection | Manual memory pools and allocation tracking |\n| Broadcasting | NumPy broadcast_arrays function | Custom broadcasting with optimized stride patterns |\n| Error Handling | Python exceptions with descriptive messages | Structured error codes with recovery suggestions |\n| Testing | NumPy testing utilities (assert_allclose) | Property-based testing with Hypothesis |\n\n#### Recommended File Structure\n\nThe tensor operations layer should be organized to separate core tensor functionality from operation implementations and testing utilities:\n\n```\nneural_framework/\n├── tensor/\n│   ├── __init__.py              ← Public tensor API exports\n│   ├── tensor.py                ← Core Tensor class implementation\n│   ├── operations.py            ← Operation base class and arithmetic ops\n│   ├── creation.py              ← Tensor creation utilities (zeros, ones, randn)\n│   ├── broadcasting.py          ← Broadcasting utilities and shape manipulation\n│   └── testing_utils.py         ← Gradient checking and numerical testing\n├── tests/\n│   ├── test_tensor.py           ← Core tensor functionality tests\n│   ├── test_operations.py       ← Arithmetic operation tests\n│   ├── test_broadcasting.py     ← Broadcasting behavior tests\n│   └── test_gradients.py        ← Gradient correctness tests\n└── examples/\n    ├── tensor_basics.py         ← Simple tensor usage examples\n    └── gradient_flow.py         ← Autodiff demonstration\n```\n\n#### Infrastructure Starter Code\n\n**File: tensor/broadcasting.py** (Complete implementation for learner use)\n\n```python\n\"\"\"Broadcasting utilities for tensor operations.\n\nThis module provides complete broadcasting functionality that learners can use\nwithout implementation. Focus remains on tensor and autodiff concepts.\n\"\"\"\nimport numpy as np\nfrom typing import Tuple, Union\n\n\ndef broadcast_shapes(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -> Tuple[int, ...]:\n    \"\"\"Compute the broadcasted shape for two input shapes.\n    \n    Args:\n        shape1: Shape of first tensor\n        shape2: Shape of second tensor\n        \n    Returns:\n        Broadcasted output shape\n        \n    Raises:\n        ValueError: If shapes are not broadcastable\n    \"\"\"\n    # Align shapes by prepending 1s to shorter shape\n    max_len = max(len(shape1), len(shape2))\n    aligned_shape1 = (1,) * (max_len - len(shape1)) + shape1\n    aligned_shape2 = (1,) * (max_len - len(shape2)) + shape2\n    \n    # Check compatibility and compute output shape\n    output_shape = []\n    for dim1, dim2 in zip(aligned_shape1, aligned_shape2):\n        if dim1 == 1:\n            output_shape.append(dim2)\n        elif dim2 == 1:\n            output_shape.append(dim1)\n        elif dim1 == dim2:\n            output_shape.append(dim1)\n        else:\n            raise ValueError(f\"Cannot broadcast shapes {shape1} and {shape2}\")\n    \n    return tuple(output_shape)\n\n\ndef unbroadcast_gradient(grad: np.ndarray, original_shape: Tuple[int, ...]) -> np.ndarray:\n    \"\"\"Reduce gradient to original tensor shape by summing over broadcasted dimensions.\n    \n    Args:\n        grad: Gradient array with broadcasted shape\n        original_shape: Target shape to reduce gradient to\n        \n    Returns:\n        Gradient array reduced to original shape\n    \"\"\"\n    # Handle scalar case\n    if len(original_shape) == 0:\n        return np.sum(grad)\n    \n    # Determine how many dimensions were prepended\n    ndim_added = grad.ndim - len(original_shape)\n    \n    # Sum over prepended dimensions\n    for _ in range(ndim_added):\n        grad = grad.sum(axis=0)\n    \n    # Sum over dimensions that were size 1 in original\n    for i, (grad_dim, orig_dim) in enumerate(zip(grad.shape, original_shape)):\n        if orig_dim == 1 and grad_dim > 1:\n            grad = grad.sum(axis=i, keepdims=True)\n    \n    return grad.reshape(original_shape)\n\n\ndef check_broadcastable(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -> bool:\n    \"\"\"Check if two shapes are broadcastable without computing result.\n    \n    Args:\n        shape1: Shape of first tensor\n        shape2: Shape of second tensor\n        \n    Returns:\n        True if shapes are broadcastable, False otherwise\n    \"\"\"\n    try:\n        broadcast_shapes(shape1, shape2)\n        return True\n    except ValueError:\n        return False\n```\n\n**File: tensor/testing_utils.py** (Complete implementation for learner use)\n\n```python\n\"\"\"Testing utilities for gradient checking and numerical verification.\"\"\"\nimport numpy as np\nfrom typing import Callable, List, Union\n\n\ndef numerical_gradient(f: Callable, inputs: List['Tensor'], h: float = 1e-5) -> List[np.ndarray]:\n    \"\"\"Compute numerical gradients using finite differences.\n    \n    Args:\n        f: Function that takes tensors and returns scalar tensor\n        inputs: List of input tensors\n        h: Step size for finite differences\n        \n    Returns:\n        List of numerical gradient arrays, one per input tensor\n    \"\"\"\n    gradients = []\n    \n    for i, tensor in enumerate(inputs):\n        grad = np.zeros_like(tensor.data)\n        flat_data = tensor.data.flatten()\n        flat_grad = grad.flatten()\n        \n        for j in range(len(flat_data)):\n            # Forward step\n            old_val = flat_data[j]\n            flat_data[j] = old_val + h\n            f_plus = f(*inputs).data.item()\n            \n            # Backward step\n            flat_data[j] = old_val - h\n            f_minus = f(*inputs).data.item()\n            \n            # Compute gradient\n            flat_grad[j] = (f_plus - f_minus) / (2 * h)\n            \n            # Restore original value\n            flat_data[j] = old_val\n            \n        gradients.append(grad)\n    \n    return gradients\n\n\ndef check_gradients(f: Callable, inputs: List['Tensor'], tolerance: float = 1e-6) -> bool:\n    \"\"\"Compare automatic differentiation gradients with numerical gradients.\n    \n    Args:\n        f: Function that takes tensors and returns scalar tensor  \n        inputs: List of input tensors (must have requires_grad=True)\n        tolerance: Maximum allowed difference between gradients\n        \n    Returns:\n        True if gradients match within tolerance, False otherwise\n    \"\"\"\n    # Compute automatic gradients\n    for inp in inputs:\n        if inp.grad is not None:\n            inp.grad.data.fill(0.0)  # Zero existing gradients\n    \n    output = f(*inputs)\n    output.backward()\n    \n    auto_grads = [inp.grad.data for inp in inputs]\n    \n    # Compute numerical gradients  \n    numerical_grads = numerical_gradient(f, inputs)\n    \n    # Compare gradients\n    for auto_grad, num_grad in zip(auto_grads, numerical_grads):\n        if not np.allclose(auto_grad, num_grad, atol=tolerance):\n            max_diff = np.max(np.abs(auto_grad - num_grad))\n            print(f\"Gradient check failed! Max difference: {max_diff}\")\n            print(f\"Automatic gradient:\\n{auto_grad}\")\n            print(f\"Numerical gradient:\\n{num_grad}\")\n            return False\n    \n    return True\n```\n\n#### Core Logic Skeleton Code\n\n**File: tensor/tensor.py** (Skeleton for learner implementation)\n\n```python\n\"\"\"Core Tensor implementation with gradient tracking.\"\"\"\nimport numpy as np\nfrom typing import Optional, Tuple, Union, Any\nfrom .broadcasting import broadcast_shapes, unbroadcast_gradient\n\n\nclass Tensor:\n    \"\"\"N-dimensional array with automatic differentiation support.\"\"\"\n    \n    def __init__(self, data: Union[np.ndarray, list, float], \n                 requires_grad: bool = False,\n                 grad_fn: Optional['Operation'] = None):\n        \"\"\"Initialize tensor with data and gradient tracking.\n        \n        Args:\n            data: Numerical data (array, list, or scalar)\n            requires_grad: Whether to track gradients for this tensor\n            grad_fn: Operation that created this tensor (for autodiff)\n        \"\"\"\n        # TODO 1: Convert data to NumPy array if not already\n        # TODO 2: Store data, requires_grad flag, and grad_fn\n        # TODO 3: Initialize grad to None (will be created when needed)\n        # TODO 4: Store shape and dtype properties from data\n        # Hint: Use np.asarray() to convert various inputs to arrays\n        pass\n    \n    @property\n    def shape(self) -> Tuple[int, ...]:\n        \"\"\"Return shape of tensor data.\"\"\"\n        # TODO: Return shape tuple from self.data\n        pass\n    \n    @property \n    def dtype(self) -> np.dtype:\n        \"\"\"Return data type of tensor elements.\"\"\"\n        # TODO: Return dtype from self.data\n        pass\n    \n    @property\n    def ndim(self) -> int:\n        \"\"\"Return number of dimensions.\"\"\"\n        # TODO: Return ndim from self.data\n        pass\n    \n    @property\n    def size(self) -> int:\n        \"\"\"Return total number of elements.\"\"\"\n        # TODO: Return size from self.data\n        pass\n    \n    def __add__(self, other: Union['Tensor', float]) -> 'Tensor':\n        \"\"\"Element-wise addition with broadcasting and gradient tracking.\"\"\"\n        # TODO 1: Convert other to Tensor if it's a scalar\n        # TODO 2: Use broadcast_shapes to compute output shape\n        # TODO 3: Perform NumPy addition: result_data = self.data + other.data\n        # TODO 4: Determine if result requires gradients (either input requires grad)\n        # TODO 5: Create Add operation if gradients needed\n        # TODO 6: Return new Tensor with result data and gradient tracking\n        # Hint: from .operations import Add\n        pass\n    \n    def __mul__(self, other: Union['Tensor', float]) -> 'Tensor':\n        \"\"\"Element-wise multiplication with broadcasting and gradient tracking.\"\"\"\n        # TODO 1: Convert other to Tensor if it's a scalar\n        # TODO 2: Use broadcast_shapes to compute output shape\n        # TODO 3: Perform NumPy multiplication: result_data = self.data * other.data\n        # TODO 4: Determine if result requires gradients\n        # TODO 5: Create Multiply operation if gradients needed\n        # TODO 6: Return new Tensor with result and gradient tracking\n        pass\n    \n    def matmul(self, other: 'Tensor') -> 'Tensor':\n        \"\"\"Matrix multiplication with gradient tracking.\"\"\"\n        # TODO 1: Check that shapes are compatible for matrix multiplication\n        # TODO 2: Perform NumPy matmul: result_data = np.matmul(self.data, other.data)\n        # TODO 3: Determine if result requires gradients\n        # TODO 4: Create MatMul operation if gradients needed\n        # TODO 5: Return new Tensor with result and gradient tracking\n        # Hint: Use np.matmul for the actual computation\n        pass\n    \n    def backward(self, gradient: Optional['Tensor'] = None) -> None:\n        \"\"\"Initiate backpropagation from this tensor.\n        \n        Args:\n            gradient: Gradient to backpropagate (defaults to ones_like)\n        \"\"\"\n        # TODO 1: If gradient not provided, create tensor of ones with same shape\n        # TODO 2: Call _backward method to perform recursive backpropagation\n        # TODO 3: Handle case where tensor doesn't require gradients\n        # Hint: This is the public entry point that sets up the initial gradient\n        pass\n    \n    def _backward(self, gradient: 'Tensor') -> None:\n        \"\"\"Internal recursive gradient computation.\n        \n        Args:\n            gradient: Gradient flowing back to this tensor\n        \"\"\"\n        # TODO 1: Accumulate gradient into self.grad (create if doesn't exist)\n        # TODO 2: If this tensor has grad_fn, call its backward method\n        # TODO 3: Handle gradient accumulation (sum if grad already exists)\n        # Hint: Use += for gradient accumulation, create new Tensor if grad is None\n        pass\n    \n    def detach(self) -> 'Tensor':\n        \"\"\"Create new tensor sharing data but not requiring gradients.\"\"\"\n        # TODO 1: Create new Tensor with same data\n        # TODO 2: Set requires_grad=False and grad_fn=None\n        # TODO 3: Return detached tensor\n        # Hint: This breaks gradient tracking while sharing memory\n        pass\n    \n    def numpy(self) -> np.ndarray:\n        \"\"\"Return data as NumPy array (detached from gradients).\"\"\"\n        # TODO: Return self.data (already a NumPy array)\n        pass\n    \n    def __repr__(self) -> str:\n        \"\"\"String representation for debugging.\"\"\"\n        grad_str = f\", requires_grad={self.requires_grad}\" if self.requires_grad else \"\"\n        return f\"Tensor({self.data}{grad_str})\"\n\n\n# Tensor creation utilities\ndef zeros(shape: Tuple[int, ...], requires_grad: bool = False) -> Tensor:\n    \"\"\"Create tensor filled with zeros.\"\"\"\n    # TODO: Use np.zeros to create data, return Tensor\n    pass\n\ndef ones(shape: Tuple[int, ...], requires_grad: bool = False) -> Tensor:\n    \"\"\"Create tensor filled with ones.\"\"\"\n    # TODO: Use np.ones to create data, return Tensor  \n    pass\n\ndef randn(shape: Tuple[int, ...], requires_grad: bool = False) -> Tensor:\n    \"\"\"Create tensor with random normal distribution.\"\"\"\n    # TODO: Use np.random.randn to create data, return Tensor\n    pass\n```\n\n**File: tensor/operations.py** (Skeleton for learner implementation)\n\n```python\n\"\"\"Operation base class and arithmetic operations for automatic differentiation.\"\"\"\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple\nfrom .broadcasting import unbroadcast_gradient\n\n\nclass Operation(ABC):\n    \"\"\"Base class for differentiable operations.\"\"\"\n    \n    def __init__(self, inputs: Tuple['Tensor', ...]):\n        \"\"\"Store input tensors for gradient computation.\n        \n        Args:\n            inputs: Tuple of input tensors that created this operation\n        \"\"\"\n        # TODO: Store inputs tuple for backward pass\n        pass\n    \n    @abstractmethod\n    def forward(self, *inputs: np.ndarray) -> np.ndarray:\n        \"\"\"Compute forward pass result.\n        \n        Args:\n            *inputs: Input arrays\n            \n        Returns:\n            Output array\n        \"\"\"\n        pass\n    \n    @abstractmethod  \n    def backward(self, grad_output: 'Tensor') -> None:\n        \"\"\"Compute gradients and propagate to input tensors.\n        \n        Args:\n            grad_output: Gradient flowing back from output\n        \"\"\"\n        pass\n\n\nclass Add(Operation):\n    \"\"\"Element-wise addition operation.\"\"\"\n    \n    def forward(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"Compute a + b with broadcasting.\"\"\"\n        # TODO: Return a + b (NumPy handles broadcasting automatically)\n        pass\n    \n    def backward(self, grad_output: 'Tensor') -> None:\n        \"\"\"Compute gradients for addition inputs.\"\"\"\n        # TODO 1: Get original shapes of both input tensors\n        # TODO 2: Gradient w.r.t. first input is grad_output (identity function)\n        # TODO 3: Gradient w.r.t. second input is also grad_output  \n        # TODO 4: Use unbroadcast_gradient to reduce gradients to original shapes\n        # TODO 5: Call _backward on both input tensors with their gradients\n        # Hint: Addition gradient is just pass-through, but must handle broadcasting\n        pass\n\n\nclass Multiply(Operation):\n    \"\"\"Element-wise multiplication operation.\"\"\"\n    \n    def forward(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"Compute a * b with broadcasting.\"\"\"\n        # TODO: Return a * b\n        pass\n    \n    def backward(self, grad_output: 'Tensor') -> None:\n        \"\"\"Compute gradients for multiplication inputs.\"\"\" \n        # TODO 1: Get data from both input tensors\n        # TODO 2: Gradient w.r.t. first input is grad_output * second_input_data\n        # TODO 3: Gradient w.r.t. second input is grad_output * first_input_data\n        # TODO 4: Use unbroadcast_gradient to handle broadcasting\n        # TODO 5: Call _backward on both input tensors\n        # Hint: Multiplication rule: d(a*b)/da = b, d(a*b)/db = a\n        pass\n\n\nclass MatMul(Operation):\n    \"\"\"Matrix multiplication operation.\"\"\"\n    \n    def forward(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"Compute matrix multiplication a @ b.\"\"\"\n        # TODO: Return np.matmul(a, b)\n        pass\n    \n    def backward(self, grad_output: 'Tensor') -> None:\n        \"\"\"Compute gradients for matrix multiplication.\"\"\"\n        # TODO 1: Get data from both input tensors  \n        # TODO 2: Gradient w.r.t. first input: grad_output @ second_input.T\n        # TODO 3: Gradient w.r.t. second input: first_input.T @ grad_output\n        # TODO 4: Handle transpose for different tensor dimensions\n        # TODO 5: Call _backward on both input tensors\n        # Hint: Matrix multiplication chain rule involves transposes\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the tensor operations layer, verify correct behavior with these checkpoints:\n\n**Basic Tensor Operations Test:**\n```python\n# Run this test to verify tensor creation and arithmetic\nimport numpy as np\nfrom tensor import Tensor, zeros, ones\n\n# Test tensor creation\na = Tensor([1.0, 2.0, 3.0], requires_grad=True)\nb = Tensor([4.0, 5.0, 6.0], requires_grad=True) \nc = zeros((2, 3), requires_grad=True)\n\n# Test arithmetic operations\nresult = a + b  # Should be [5.0, 7.0, 9.0]\nproduct = a * b  # Should be [4.0, 10.0, 18.0]\n\nprint(f\"Addition result: {result}\")\nprint(f\"Multiplication result: {product}\")\nprint(f\"Shapes preserved: {result.shape == a.shape}\")\n```\n\n**Broadcasting Test:**\n```python\n# Test broadcasting behavior\nscalar = Tensor(2.0, requires_grad=True)\nvector = Tensor([1.0, 2.0, 3.0], requires_grad=True)\nmatrix = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n\n# These should work with broadcasting\nscalar_vector = scalar + vector  # Shape: (3,)\nscalar_matrix = scalar * matrix  # Shape: (2, 2)\n\nprint(f\"Scalar + vector: {scalar_vector}\")  \nprint(f\"Scalar * matrix: {scalar_matrix}\")\n```\n\n**Expected behavior indicators:**\n- Tensor creation produces objects with correct shape and dtype attributes\n- Arithmetic operations return new Tensor objects (not NumPy arrays)  \n- Broadcasting operations produce correctly shaped results\n- `requires_grad` flag propagates correctly to operation results\n- Operations between tensors and scalars work without explicit conversion\n\n**Signs something is wrong:**\n- Operations return NumPy arrays instead of Tensor objects\n- Shape mismatches cause crashes instead of broadcasting\n- Gradient tracking flags are not preserved through operations\n- Scalar operations require manual tensor conversion\n\n\n## Automatic Differentiation Engine (Milestone 2)\n\n> **Milestone(s):** Milestone 2 (Automatic Differentiation) - implements reverse-mode automatic differentiation with computation graph construction and gradient backpropagation\n\nThe automatic differentiation engine is the beating heart of any modern neural network framework. While tensors provide the data structures and operations provide the computations, the autodiff engine transforms these simple building blocks into a powerful gradient computation system that can automatically compute derivatives of arbitrarily complex functions. This capability is what makes training deep neural networks practical—without it, we would need to manually derive and implement gradients for every possible combination of operations, which becomes intractable for networks with millions of parameters.\n\nThe autodiff engine implements **reverse-mode automatic differentiation**, also known as backpropagation, which efficiently computes gradients by traversing the computation graph in reverse topological order. Unlike numerical differentiation (which approximates derivatives using finite differences) or symbolic differentiation (which manipulates algebraic expressions), automatic differentiation computes exact derivatives to machine precision by mechanically applying the chain rule during program execution.\n\n### The Assembly Line Metaphor\n\nThink of the automatic differentiation engine as a sophisticated factory assembly line that can run in reverse. During the **forward pass**, raw materials (input tensors) move through a series of workstations (operations like addition, multiplication, matrix multiplication), with each station transforming the materials and passing them to the next station. The final product emerges at the end of the line (the output tensor).\n\nWhat makes this factory special is that it meticulously records every transformation that happens at each workstation. It tracks which raw materials entered each station, what operations were performed, and where the processed materials went next. This creates a detailed **computation graph**—essentially a blueprint of the entire assembly process.\n\nDuring the **backward pass**, the factory runs in reverse. Starting from the final product, we trace back through each workstation to determine how much each raw material contributed to the final result. If we want to know how changing the input by a small amount would affect the output (the gradient), we can propagate this information backwards through the assembly line, accumulating the contributions from each workstation using the chain rule.\n\nThe key insight is that each workstation (operation) only needs to know how to compute its own local gradients—how its inputs affect its outputs. The autodiff engine handles the global coordination, ensuring that gradients flow backwards through the entire network in the correct order and that contributions from multiple paths are properly accumulated.\n\nThis metaphor captures several critical aspects of automatic differentiation: the forward pass builds the computation graph (records the assembly process), the backward pass traverses it in reverse (traces contributions backwards), and each operation contributes its local gradient information (each workstation knows its own transformation rules).\n\n### Forward Pass Graph Building\n\nThe forward pass is where the magic of dynamic computation graph construction happens. Unlike static graph frameworks that require you to define the entire network structure upfront, our framework uses a **define-by-run** approach where the computation graph is built dynamically as operations execute. This provides tremendous flexibility—the graph can change based on control flow, input data, or runtime conditions.\n\nWhen a tensor operation executes (like `c = a + b`), three things happen simultaneously: the numerical computation occurs, a new tensor is created to hold the result, and an edge is added to the computation graph. The graph construction is entirely automatic and transparent to the user—they simply write natural-looking mathematical expressions, and the framework handles all the bookkeeping required for later gradient computation.\n\nThe computation graph is a **directed acyclic graph (DAG)** where nodes represent either tensors (data) or operations (computations). Tensor nodes store the actual numerical data and metadata like shape and gradient requirements. Operation nodes store references to their input tensors, the function used to compute the output, and most importantly, the gradient function needed for backpropagation.\n\nEvery tensor that participates in gradient computation carries a `grad_fn` field that points to the operation that created it. This creates a chain of references that allows the autodiff engine to traverse backwards from any tensor to discover all the operations that contributed to computing it. Leaf tensors (like parameters and inputs) have `grad_fn = None` since they weren't created by any operation—they're the starting points of the computation.\n\nThe graph building process is eager and immediate. As soon as you execute `c = a.matmul(b)`, the matrix multiplication happens, the result tensor `c` is created with the computed values, and `c.grad_fn` is set to point to a `MatMul` operation node that remembers `a` and `b` as its inputs. This means the computation graph always reflects the exact sequence of operations that were actually executed.\n\nHere's how the core tensor operations participate in graph construction:\n\n| Operation | Graph Node Created | Inputs Stored | Gradient Function |\n|-----------|-------------------|---------------|-------------------|\n| `a + b` | `Add` operation | References to tensors `a` and `b` | Computes gradients w.r.t both inputs |\n| `a * b` | `Multiply` operation | References to tensors `a` and `b` | Implements product rule derivatives |\n| `a.matmul(b)` | `MatMul` operation | References to tensors `a` and `b` | Handles matrix multiplication gradients |\n| `a.sum()` | `Sum` operation | Reference to tensor `a` and reduction axes | Broadcasts gradient back to original shape |\n\nThe graph construction must handle several important details to support correct gradient computation. First, it preserves the exact tensor shapes at each operation, since gradients must flow back through the same shape transformations in reverse. Second, it maintains references to the original input tensors (not copies), ensuring that gradients accumulate in the correct locations. Third, it tracks whether each tensor requires gradients, allowing optimization by skipping gradient computation for tensors that don't need it.\n\n> **Critical Design Insight**: The forward pass serves dual purposes—it computes the actual function values needed by the application AND builds the data structure needed for gradient computation. This dual purpose is what makes automatic differentiation so powerful—gradient computation comes \"for free\" with any forward computation.\n\n### Reverse-Mode Differentiation Algorithm\n\nThe reverse-mode differentiation algorithm is the core of the autodiff engine, implementing the systematic application of the chain rule to compute gradients efficiently. The algorithm operates in two phases: topological ordering of the computation graph and gradient propagation in reverse topological order.\n\n**Phase 1: Topological Sort**\n\nBefore gradients can be computed, the autodiff engine must determine the correct order for processing nodes during the backward pass. This requires a topological sort of the computation graph, which orders nodes such that every node appears before any nodes that depend on it. In the context of gradient computation, this means processing nodes in reverse order of their creation during the forward pass.\n\nThe topological sort is essential because gradients must be computed in dependency order. If tensor `c` depends on tensor `b`, and tensor `b` depends on tensor `a`, then we must compute gradients with respect to `c` before we can compute gradients with respect to `b`, and gradients with respect to `b` before gradients with respect to `a`. Processing nodes in the wrong order would mean trying to use gradient information that hasn't been computed yet.\n\nThe algorithm for topological sort in our context works as follows:\n\n1. Start from the output tensor (where `backward()` was called) and perform a depth-first traversal of the computation graph\n2. Visit each operation node by following `grad_fn` pointers from tensors to operations and from operations to their input tensors\n3. Mark nodes as visited to handle cases where the same tensor is used multiple times in the computation\n4. Build a list of operation nodes ordered by their \"finish time\" in the depth-first traversal\n5. Reverse this list to get the correct processing order for the backward pass\n\nThis produces a topologically sorted list where operations are ordered from the output backwards toward the inputs, ensuring that when we process each operation, all operations that depend on its outputs have already been processed.\n\n**Phase 2: Gradient Propagation**\n\nOnce the topological order is established, gradient propagation proceeds by visiting each operation node in order and computing the gradients of its inputs based on the gradient of its output. This systematically applies the chain rule: if we know how the final loss depends on an operation's output, we can compute how it depends on the operation's inputs using the operation's local gradient function.\n\nThe gradient propagation algorithm follows these steps:\n\n1. Initialize the gradient of the starting tensor (usually the loss) to a tensor of ones with the same shape\n2. For each operation in topologically sorted order:\n   - Retrieve the accumulated gradient with respect to the operation's output\n   - Call the operation's `backward()` method to compute gradients with respect to its inputs\n   - For each input tensor that requires gradients, accumulate the computed gradient\n3. Continue until all operations have been processed\n\nEach operation's `backward()` method implements the local gradient computation for that specific operation type. For example:\n\n| Operation | Local Gradient Computation |\n|-----------|----------------------------|\n| `Add` | Gradient w.r.t both inputs is the same as output gradient (derivative of `a + b` is 1 w.r.t both `a` and `b`) |\n| `Multiply` | Gradient w.r.t first input is output gradient times second input; gradient w.r.t second input is output gradient times first input |\n| `MatMul` | Gradient w.r.t first input is output gradient matrix-multiplied by transpose of second input |\n| `Sum` | Gradient is broadcasted from reduced shape back to original input shape |\n\nThe beauty of this approach is that each operation only needs to know its own local derivatives. The autodiff engine handles the global coordination, ensuring that gradients flow backwards through the entire network correctly.\n\n> **Chain Rule in Action**: The reverse-mode algorithm is essentially a mechanical application of the multivariate chain rule. If we have a composition of functions `f(g(h(x)))`, the derivative is `f'(g(h(x))) * g'(h(x)) * h'(x)`. The backward pass computes these derivatives from right to left, accumulating the product as it goes.\n\n### Gradient Accumulation Strategy\n\nOne of the most subtle aspects of automatic differentiation is handling cases where the same tensor appears multiple times in a computation. When a tensor contributes to the output through multiple paths in the computation graph, its total gradient is the sum of contributions from all paths. This is a direct consequence of the multivariate chain rule—if `y = f(x) + g(x)`, then `dy/dx = df/dx + dg/dx`.\n\nConsider a simple example where we compute `y = x * x + x * 2`. The tensor `x` appears three times in this expression: twice in the first term and once in the second term. During the forward pass, this creates a computation graph where `x` has multiple outgoing edges. During the backward pass, gradient contributions will flow back through each of these edges, and they must be summed to get the total gradient with respect to `x`.\n\nThe gradient accumulation strategy must handle several scenarios:\n\n**Scenario 1: Direct Multiple Usage**\nWhen a tensor is used directly in multiple operations, like `y = a + a`, the gradient with respect to `a` is the sum of contributions from both addition operands. The `Add` operation computes gradients with respect to both its inputs, and both gradients (which happen to be identical in this case) must be accumulated into `a.grad`.\n\n**Scenario 2: Indirect Multiple Usage**\nMore complex cases arise when a tensor influences the output through multiple intermediate computations. For example, in `b = a * 2; c = a + 1; y = b + c`, the tensor `a` influences `y` through both the `b` path and the `c` path. The gradients must be accumulated from both paths.\n\n**Scenario 3: Loop and Control Flow Usage**\nIn dynamic graphs, the same tensor might be used in loops or conditional branches, creating multiple gradient contributions that must be accumulated. This is particularly important for recurrent neural networks where the same parameters are used at multiple time steps.\n\nThe gradient accumulation implementation follows these principles:\n\n| Accumulation Rule | Implementation Strategy |\n|------------------|------------------------|\n| First gradient contribution | Initialize `tensor.grad = computed_gradient` |\n| Subsequent contributions | Update `tensor.grad += computed_gradient` |\n| Shape compatibility | Ensure accumulated gradients maintain correct tensor shapes |\n| Memory management | Avoid creating unnecessary intermediate gradient tensors |\n\nThe accumulation happens automatically during the backward pass. When an operation computes gradients with respect to its inputs, it checks whether each input tensor already has accumulated gradients. If the `grad` field is `None`, the computed gradient is stored directly. If gradients already exist, the new gradient is added to the existing accumulated gradient.\n\nThis requires careful attention to tensor shapes and broadcasting. When gradients are accumulated, they must be compatible for element-wise addition. If broadcasting was used during the forward pass, the gradients must be \"unbroadcast\" back to the original tensor shapes before accumulation.\n\n> **Memory Efficiency Insight**: Gradient accumulation reuses the same `grad` tensors throughout the backward pass, avoiding the creation of many temporary gradient tensors. This is crucial for memory efficiency in large neural networks where creating copies of all gradients would quickly exhaust available memory.\n\n### Autodiff Architecture Decisions\n\nThe design of the automatic differentiation engine involves several critical architecture decisions that affect performance, memory usage, and ease of implementation. Each decision represents a trade-off between different priorities, and understanding these trade-offs is essential for building a robust autodiff system.\n\n> **Decision: Define-by-Run vs. Static Graph Construction**\n> - **Context**: We need to choose between building the computation graph dynamically during forward pass execution (define-by-run) or requiring users to define the graph structure upfront before execution (static graphs)\n> - **Options Considered**: PyTorch-style dynamic graphs, TensorFlow 1.x-style static graphs, hybrid approaches with tracing\n> - **Decision**: Implement define-by-run dynamic graph construction\n> - **Rationale**: Dynamic graphs provide superior debugging experience, support natural control flow like loops and conditionals, and offer more intuitive APIs for educational purposes. The slight performance overhead is acceptable for our learning-focused framework\n> - **Consequences**: Enables flexible model architectures but requires careful memory management to avoid retaining computation graphs longer than necessary\n\n| Graph Construction Approach | Pros | Cons | Chosen? |\n|----------------------------|------|------|---------|\n| Define-by-run (PyTorch) | Natural control flow, easy debugging, flexible architectures | Slight runtime overhead, harder to optimize | ✅ Yes |\n| Static graphs (TF 1.x) | Better optimization opportunities, clear separation of definition/execution | Complex control flow, difficult debugging | ❌ No |\n| Hybrid tracing (JAX) | Best of both worlds | Added complexity, requires sophisticated tracing | ❌ Too complex |\n\n> **Decision: Operation Storage and Memory Management**\n> - **Context**: We need to decide how long to retain operation nodes and intermediate tensors in the computation graph, balancing memory usage against gradient computation requirements\n> - **Options Considered**: Retain entire graph until manual release, automatic graph cleanup after backward pass, reference counting with weak references\n> - **Decision**: Retain graph until backward pass completes, then automatic cleanup\n> - **Rationale**: Provides predictable memory behavior while ensuring all gradient information remains available during backpropagation. Automatic cleanup prevents common memory leak bugs that beginners encounter\n> - **Consequences**: Higher peak memory usage during training but prevents accidental memory leaks and provides clear memory lifecycle\n\n> **Decision: Gradient Storage Location**\n> - **Context**: We need to choose where to store computed gradients—in the operation nodes, in the tensors themselves, or in a separate gradient tape structure\n> - **Options Considered**: Store gradients in tensors (`.grad` attribute), store in operation nodes, maintain separate gradient dictionary\n> - **Decision**: Store gradients directly in tensor objects as `.grad` attribute\n> - **Rationale**: Matches PyTorch's intuitive API, makes gradients easily accessible for debugging and optimization, and simplifies the parameter update process for optimizers\n> - **Consequences**: Tensors become slightly heavier objects but provide much more convenient API for users\n\n| Gradient Storage Approach | Pros | Cons | Chosen? |\n|--------------------------|------|------|---------|\n| Tensor `.grad` attribute | Intuitive API, easy optimizer access, simple debugging | Heavier tensor objects | ✅ Yes |\n| Operation node storage | Lighter tensors, clear separation | Complex gradient retrieval, poor debugging | ❌ No |\n| Separate gradient dictionary | Memory efficient, flexible | Complex API, hard to debug | ❌ No |\n\n> **Decision: Eager vs. Lazy Gradient Computation**\n> - **Context**: We can either compute gradients immediately during the backward pass (eager) or defer computation until gradients are actually needed (lazy)\n> - **Options Considered**: Eager computation during backward pass, lazy computation on gradient access, hybrid with caching\n> - **Decision**: Implement eager gradient computation during backward pass\n> - **Rationale**: Simpler implementation, predictable performance characteristics, and easier debugging since gradients are available immediately after `backward()` call\n> - **Consequences**: May compute gradients that are never used, but provides more predictable behavior for educational purposes\n\nThe autodiff engine also must handle several implementation details that affect correctness and performance:\n\n**Graph Node Lifecycle Management**: Operation nodes must remain alive until the backward pass completes, but should be cleaned up afterwards to prevent memory leaks. This requires careful reference management between tensors and operations.\n\n**Thread Safety Considerations**: While our educational framework doesn't target multi-threaded execution, the autodiff engine should avoid obvious thread safety issues that could cause confusion if users accidentally use it in multi-threaded contexts.\n\n**Gradient Dtype Consistency**: Gradients must maintain the same data type as the tensors they correspond to, requiring careful dtype handling during gradient computation and accumulation.\n\n**Shape Preservation**: The autodiff engine must carefully preserve tensor shapes throughout the backward pass, ensuring that gradients have exactly the same shape as the tensors they correspond to.\n\n### Common Autodiff Pitfalls\n\nBuilding an automatic differentiation system presents numerous opportunities for subtle bugs that can be difficult to debug. Understanding these common pitfalls and how to avoid them is crucial for successful implementation.\n\n⚠️ **Pitfall: Gradient Not Accumulated Across Multiple Uses**\n\nOne of the most common mistakes is failing to properly accumulate gradients when a tensor is used multiple times in a computation. Beginners often implement gradient assignment (`tensor.grad = new_gradient`) instead of gradient accumulation (`tensor.grad += new_gradient`), causing later gradient contributions to overwrite earlier ones.\n\nThis manifests as mysteriously incorrect gradients that seem to have the right magnitude but wrong values. For example, if computing `y = x + x`, the gradient with respect to `x` should be 2, but without proper accumulation it would be 1 (only the contribution from one of the addition operands).\n\nThe fix is to always check if a tensor already has accumulated gradients before storing new ones:\n\n```python\n# Wrong: overwrites existing gradients\ntensor.grad = computed_gradient\n\n# Correct: accumulates gradients\nif tensor.grad is None:\n    tensor.grad = computed_gradient\nelse:\n    tensor.grad += computed_gradient\n```\n\n⚠️ **Pitfall: Incorrect Topological Ordering**\n\nThe backward pass must process operations in reverse topological order, but implementing topological sort correctly is tricky. Common mistakes include not handling cycles properly (though computation graphs should be acyclic), processing nodes in creation order instead of dependency order, or not handling the case where the same operation is reachable through multiple paths.\n\nIncorrect ordering leads to gradients being computed before their dependencies are ready, resulting in missing or incorrect gradient values. The symptoms include gradients that are zero when they should be non-zero, or exceptions about undefined gradient values.\n\nThe fix requires implementing a proper depth-first search with post-order traversal:\n1. Start from the output tensor and follow `grad_fn` pointers\n2. Mark nodes as visited to avoid infinite loops\n3. Only add a node to the topological order after visiting all its dependencies\n4. Reverse the final order to get correct backward pass sequence\n\n⚠️ **Pitfall: Memory Leaks from Circular References**\n\nThe computation graph creates a web of references between tensors and operations that can lead to circular references and memory leaks. Tensors hold references to operations through `grad_fn`, and operations hold references to input tensors, creating cycles that Python's garbage collector may not handle efficiently.\n\nThis manifests as steadily increasing memory usage during training, even when the computation graphs should be freed after each backward pass. The memory usage grows until the system runs out of memory, even for small models.\n\nThe fix involves carefully breaking reference cycles after gradient computation completes:\n1. Clear `grad_fn` references from tensors after backward pass\n2. Clear input tensor references from operation nodes\n3. Consider using weak references for some connections to break cycles automatically\n\n⚠️ **Pitfall: Broadcasting Gradient Shape Mismatches**\n\nWhen operations involve broadcasting, the backward pass must \"unbroadcast\" gradients back to their original shapes. Failing to handle this correctly leads to gradient tensors with the wrong shape, causing shape mismatch errors during gradient accumulation or parameter updates.\n\nFor example, if computing `c = a + b` where `a` has shape `(3, 1)` and `b` has shape `(3, 4)`, broadcasting expands `a` to `(3, 4)` during the forward pass. During the backward pass, the gradient with respect to `a` starts with shape `(3, 4)` but must be reduced back to `(3, 1)` by summing along the appropriate axes.\n\nThe fix requires implementing proper gradient unbroadcasting:\n1. Track the original shapes of all input tensors during the forward pass\n2. During backward pass, reduce gradient tensors back to their original shapes\n3. Use sum operations to collapse broadcasted dimensions\n4. Ensure the final gradient shape exactly matches the original tensor shape\n\n⚠️ **Pitfall: In-Place Operations Breaking Gradient Flow**\n\nIn-place operations that modify tensors directly can break gradient computation by changing values that are needed for computing gradients of earlier operations. This is particularly problematic when the same tensor is used in multiple operations and then modified in-place.\n\nFor example, if computing `b = a * 2` followed by `a += 1`, the in-place addition changes the value of `a` that is needed to compute gradients for the multiplication operation. This leads to incorrect gradients and unpredictable behavior.\n\nThe fix is to either prohibit in-place operations on tensors that require gradients, or implement sophisticated version tracking to detect when gradients might be affected by in-place modifications.\n\n⚠️ **Pitfall: Forgetting to Set requires_grad**\n\nThe `requires_grad` flag controls whether a tensor participates in gradient computation. Forgetting to set this flag on parameters or input tensors means no gradients will be computed for them, leading to parameters that don't update during training.\n\nThis manifests as models that don't learn—loss doesn't decrease, parameters remain at their initial values, and gradient inspection shows all gradients are None or zero.\n\nThe fix is to ensure that all tensors that should have gradients computed are properly marked:\n- Set `requires_grad=True` on all model parameters during initialization\n- Set `requires_grad=True` on input tensors if computing gradients with respect to inputs\n- Verify that intermediate tensors inherit gradient requirements from their inputs\n\n| Common Pitfall | Symptom | Root Cause | Fix |\n|----------------|---------|------------|-----|\n| Gradient not accumulated | Incorrect gradient magnitudes | Assignment instead of accumulation | Use `+=` for gradient updates |\n| Wrong topological order | Missing or zero gradients | Incorrect dependency ordering | Implement proper DFS post-order traversal |\n| Memory leaks | Ever-increasing memory usage | Circular tensor-operation references | Break cycles after backward pass |\n| Broadcasting shape mismatch | Shape errors during accumulation | Missing gradient unbroadcasting | Reduce gradients to original shapes |\n| In-place operations | Incorrect gradients | Modified values needed for gradients | Prohibit in-place ops with gradients |\n| Missing requires_grad | No gradients computed | Flag not set on parameters | Mark all learnable tensors |\n\n### Implementation Guidance\n\nThe automatic differentiation engine bridges the gap between the mathematical theory of differentiation and the practical implementation of gradient computation in neural networks. This section provides concrete guidance for implementing the core components while avoiding common pitfalls.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Graph Storage | Python lists and dictionaries | Custom graph data structures with optimized traversal |\n| Topological Sort | Recursive DFS with visited set | Iterative DFS with explicit stack to avoid recursion limits |\n| Gradient Storage | Direct tensor attributes | Separate gradient tape with lazy computation |\n| Memory Management | Manual reference clearing | Weak references and context managers |\n| Numerical Verification | Simple finite differences | Sophisticated gradient checking with multiple step sizes |\n\n**Recommended File Structure**\n\nThe autodiff engine should be organized to separate concerns clearly while maintaining easy integration with the tensor operations:\n\n```\nneural_framework/\n  autodiff/\n    __init__.py              ← public API exports\n    engine.py                ← core backward pass algorithm\n    operations.py            ← operation base class and implementations\n    graph.py                 ← computation graph utilities\n    gradient_check.py        ← numerical verification utilities\n  tensor/\n    tensor.py                ← tensor class with grad_fn integration\n  test/\n    test_autodiff.py         ← comprehensive gradient tests\n    test_gradient_check.py   ← numerical verification tests\n```\n\n**Core Infrastructure (Complete Implementation)**\n\nThe following infrastructure components are essential but not the primary learning focus. These can be used as-is to support the core autodiff implementation:\n\n**Gradient Checking Utilities** - Complete implementation for verifying gradient correctness:\n\n```python\n\"\"\"\nGradient checking utilities for verifying automatic differentiation correctness.\nThese utilities compare automatic gradients against numerical approximations.\n\"\"\"\nimport numpy as np\nfrom typing import Callable, List, Tuple, Optional\n\ndef numerical_gradient(f: Callable, inputs: List['Tensor'], h: float = 1e-5) -> List[np.ndarray]:\n    \"\"\"\n    Compute numerical gradients using finite differences.\n    \n    Args:\n        f: Function that takes list of tensors and returns single tensor\n        inputs: List of input tensors to compute gradients for\n        h: Step size for finite differences\n    \n    Returns:\n        List of numerical gradient arrays, one per input tensor\n    \"\"\"\n    gradients = []\n    \n    for i, input_tensor in enumerate(inputs):\n        grad = np.zeros_like(input_tensor.data)\n        flat_input = input_tensor.data.flatten()\n        flat_grad = grad.flatten()\n        \n        for j in range(len(flat_input)):\n            # Compute f(x + h)\n            original_value = flat_input[j]\n            flat_input[j] = original_value + h\n            input_tensor.data = flat_input.reshape(input_tensor.shape)\n            f_plus = f(inputs).data.item() if f(inputs).data.size == 1 else f(inputs).data.sum()\n            \n            # Compute f(x - h)\n            flat_input[j] = original_value - h\n            input_tensor.data = flat_input.reshape(input_tensor.shape)\n            f_minus = f(inputs).data.item() if f(inputs).data.size == 1 else f(inputs).data.sum()\n            \n            # Numerical gradient\n            flat_grad[j] = (f_plus - f_minus) / (2 * h)\n            \n            # Restore original value\n            flat_input[j] = original_value\n            input_tensor.data = flat_input.reshape(input_tensor.shape)\n        \n        gradients.append(flat_grad.reshape(input_tensor.shape))\n    \n    return gradients\n\ndef check_gradients(f: Callable, inputs: List['Tensor'], \n                   tolerance: float = 1e-6) -> bool:\n    \"\"\"\n    Compare automatic gradients against numerical gradients.\n    \n    Args:\n        f: Function to test (should return scalar tensor)\n        inputs: Input tensors with requires_grad=True\n        tolerance: Maximum allowed difference between gradients\n    \n    Returns:\n        True if gradients match within tolerance\n    \"\"\"\n    # Compute automatic gradients\n    for inp in inputs:\n        inp.grad = None  # Clear any existing gradients\n    \n    output = f(inputs)\n    output.backward()\n    \n    auto_grads = [inp.grad.data if inp.grad is not None else np.zeros_like(inp.data) \n                  for inp in inputs]\n    \n    # Compute numerical gradients\n    numerical_grads = numerical_gradient(f, inputs)\n    \n    # Compare gradients\n    all_match = True\n    for i, (auto_grad, num_grad) in enumerate(zip(auto_grads, numerical_grads)):\n        diff = np.abs(auto_grad - num_grad)\n        max_diff = np.max(diff)\n        \n        if max_diff > tolerance:\n            print(f\"Gradient mismatch for input {i}: max difference = {max_diff}\")\n            print(f\"Automatic: {auto_grad.flatten()[:5]}...\")\n            print(f\"Numerical: {num_grad.flatten()[:5]}...\")\n            all_match = False\n    \n    return all_match\n```\n\n**Graph Traversal Utilities** - Complete implementation for topological sorting:\n\n```python\n\"\"\"\nComputation graph traversal utilities.\nHandles topological sorting and graph cleanup for automatic differentiation.\n\"\"\"\nfrom typing import List, Set, Optional\nfrom collections import deque\n\ndef topological_sort(start_tensor: 'Tensor') -> List['Operation']:\n    \"\"\"\n    Perform topological sort starting from output tensor.\n    Returns operations in reverse topological order (correct for backward pass).\n    \n    Args:\n        start_tensor: Output tensor to start traversal from\n    \n    Returns:\n        List of operations in backward pass order\n    \"\"\"\n    visited = set()\n    topo_order = []\n    \n    def dfs_visit(tensor: 'Tensor'):\n        if id(tensor) in visited:\n            return\n        \n        visited.add(id(tensor))\n        \n        if tensor.grad_fn is not None:\n            operation = tensor.grad_fn\n            \n            # Visit all input tensors first\n            for input_tensor in operation.inputs:\n                dfs_visit(input_tensor)\n            \n            # Add operation to topological order\n            if operation not in topo_order:\n                topo_order.append(operation)\n    \n    dfs_visit(start_tensor)\n    return list(reversed(topo_order))\n\ndef clear_computation_graph(tensor: 'Tensor'):\n    \"\"\"\n    Clear computation graph to prevent memory leaks.\n    Breaks reference cycles between tensors and operations.\n    \n    Args:\n        tensor: Root tensor to start cleanup from\n    \"\"\"\n    visited = set()\n    \n    def clear_recursive(t: 'Tensor'):\n        if id(t) in visited:\n            return\n        \n        visited.add(id(t))\n        \n        if t.grad_fn is not None:\n            operation = t.grad_fn\n            \n            # Recursively clear inputs\n            for input_tensor in operation.inputs:\n                clear_recursive(input_tensor)\n            \n            # Break references\n            operation.inputs = ()\n            t.grad_fn = None\n    \n    clear_recursive(tensor)\n```\n\n**Core Autodiff Implementation (Skeleton with TODOs)**\n\nThe following skeleton provides the structure for the core automatic differentiation components. Each TODO corresponds to a specific algorithm step from the design discussion above:\n\n```python\n\"\"\"\nCore automatic differentiation engine implementation.\nStudents should implement the TODOs to complete the backward pass algorithm.\n\"\"\"\nfrom typing import Optional, Tuple, List\nimport numpy as np\n\nclass Operation:\n    \"\"\"\n    Base class for all operations that participate in automatic differentiation.\n    Each operation must implement forward and backward methods.\n    \"\"\"\n    \n    def __init__(self, *inputs: 'Tensor'):\n        # TODO 1: Store references to input tensors\n        # Hint: self.inputs = inputs\n        pass\n    \n    def forward(self, *input_arrays: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute forward pass operation on numpy arrays.\n        Should be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement forward\")\n    \n    def backward(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], ...]:\n        \"\"\"\n        Compute gradients with respect to inputs given gradient of output.\n        Should be implemented by subclasses.\n        \n        Args:\n            grad_output: Gradient with respect to operation output\n        \n        Returns:\n            Tuple of gradients with respect to each input (None if input doesn't need gradients)\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement backward\")\n\nclass Add(Operation):\n    \"\"\"Addition operation: output = input1 + input2\"\"\"\n    \n    def forward(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        # TODO 2: Implement forward pass for addition\n        # Hint: return a + b (numpy handles broadcasting)\n        pass\n    \n    def backward(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Gradient of addition: d(a+b)/da = 1, d(a+b)/db = 1\n        Must handle broadcasting by unbroadcasting gradients to input shapes.\n        \"\"\"\n        # TODO 3: Compute gradients for both inputs\n        # TODO 4: Handle broadcasting - use unbroadcast_gradient helper\n        # TODO 5: Return None for inputs that don't require gradients\n        # Hints:\n        # - Both gradients are initially the same as grad_output\n        # - Use self.inputs[0].shape and self.inputs[1].shape for target shapes\n        # - Check self.inputs[i].requires_grad before computing gradients\n        pass\n\nclass Multiply(Operation):\n    \"\"\"Element-wise multiplication: output = input1 * input2\"\"\"\n    \n    def forward(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        # TODO 6: Implement forward pass for multiplication\n        pass\n    \n    def backward(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Gradient of multiplication: d(a*b)/da = b, d(a*b)/db = a\n        \"\"\"\n        # TODO 7: Compute gradient w.r.t first input (grad_output * second_input)\n        # TODO 8: Compute gradient w.r.t second input (grad_output * first_input)\n        # TODO 9: Handle broadcasting and requires_grad checks\n        pass\n\nclass MatMul(Operation):\n    \"\"\"Matrix multiplication: output = input1 @ input2\"\"\"\n    \n    def forward(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        # TODO 10: Implement matrix multiplication\n        # Hint: Use np.matmul or @ operator\n        pass\n    \n    def backward(self, grad_output: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Gradient of matrix multiplication:\n        d(A@B)/dA = grad_output @ B^T\n        d(A@B)/dB = A^T @ grad_output\n        \"\"\"\n        # TODO 11: Compute gradient w.r.t first input (grad_output @ B.T)\n        # TODO 12: Compute gradient w.r.t second input (A.T @ grad_output)\n        # TODO 13: Handle batch dimensions and requires_grad checks\n        # Hints:\n        # - Use np.swapaxes(-1, -2) for transpose of last two dimensions\n        # - Access input data via self.inputs[0].data and self.inputs[1].data\n        pass\n\ndef unbroadcast_gradient(grad: np.ndarray, target_shape: Tuple[int, ...]) -> np.ndarray:\n    \"\"\"\n    Reduce gradient tensor to target shape by summing over broadcasted dimensions.\n    This reverses the effect of broadcasting during forward pass.\n    \"\"\"\n    # TODO 14: Handle case where gradient needs to be summed over extra dimensions\n    # TODO 15: Handle case where gradient needs to be summed and squeezed\n    # Algorithm:\n    # 1. Sum over dimensions that were added by broadcasting (leading dimensions)\n    # 2. Sum over dimensions that were expanded from size 1 (keepdims=True)\n    # 3. Ensure final shape exactly matches target_shape\n    pass\n\n# Integration with Tensor class\ndef backward_pass(tensor: 'Tensor', gradient: Optional['Tensor'] = None):\n    \"\"\"\n    Execute backward pass starting from the given tensor.\n    This is called by tensor.backward() method.\n    \"\"\"\n    # TODO 16: Initialize gradient if not provided (ones with same shape as tensor)\n    # TODO 17: Get topologically sorted operations using imported utility\n    # TODO 18: For each operation in sorted order:\n    #   - Get accumulated gradient for operation output\n    #   - Call operation.backward() to compute input gradients\n    #   - Accumulate gradients in input tensors\n    # TODO 19: Clear computation graph to prevent memory leaks\n    \n    # Algorithm outline:\n    # 1. if gradient is None: gradient = ones_like(tensor)\n    # 2. operations = topological_sort(tensor)\n    # 3. for op in operations:\n    #      output_grad = get_tensor_gradient(op.output)\n    #      input_grads = op.backward(output_grad)\n    #      for input_tensor, grad in zip(op.inputs, input_grads):\n    #          accumulate_gradient(input_tensor, grad)\n    # 4. clear_computation_graph(tensor)\n    pass\n```\n\n**Milestone Checkpoint**\n\nAfter implementing the automatic differentiation engine, verify correctness with these tests:\n\n**Basic Gradient Computation Test**:\n```python\n# Create test tensors\na = Tensor([2.0], requires_grad=True)\nb = Tensor([3.0], requires_grad=True)\n\n# Compute function: f = a * b + a\nc = a * b\nd = c + a\nf = d  # f = a*b + a = a*(b+1)\n\n# Expected gradients: df/da = b+1 = 4, df/db = a = 2\nf.backward()\n\nassert np.allclose(a.grad.data, [4.0]), f\"Expected a.grad=[4.0], got {a.grad.data}\"\nassert np.allclose(b.grad.data, [2.0]), f\"Expected b.grad=[2.0], got {b.grad.data}\"\nprint(\"✓ Basic gradient computation test passed\")\n```\n\n**Gradient Accumulation Test**:\n```python\n# Test multiple usage of same tensor\nx = Tensor([2.0], requires_grad=True)\ny = x + x + x  # y = 3*x, dy/dx should be 3\n\ny.backward()\nassert np.allclose(x.grad.data, [3.0]), f\"Expected x.grad=[3.0], got {x.grad.data}\"\nprint(\"✓ Gradient accumulation test passed\")\n```\n\n**Numerical Gradient Verification**:\n```python\n# Verify against numerical gradients\ndef test_function(inputs):\n    a, b = inputs\n    return a * a + b * a\n\na = Tensor([2.0], requires_grad=True)\nb = Tensor([3.0], requires_grad=True)\n\nis_correct = check_gradients(test_function, [a, b], tolerance=1e-5)\nassert is_correct, \"Gradients don't match numerical approximation\"\nprint(\"✓ Numerical gradient verification passed\")\n```\n\n**Debugging Tips**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| All gradients are None | `requires_grad` not set | Check `tensor.requires_grad` | Set `requires_grad=True` on inputs |\n| Gradients have wrong shape | Broadcasting not handled | Print shapes during backward pass | Implement `unbroadcast_gradient` |\n| Gradients are half expected value | Missing accumulation | Check if tensor used multiple times | Use `+=` not `=` for gradient updates |\n| Memory keeps growing | Computation graph not cleared | Monitor reference counts | Call `clear_computation_graph` |\n| Random gradient errors | Wrong topological order | Print operation execution order | Fix topological sort implementation |\n\nThe automatic differentiation engine is complete when all gradient tests pass, numerical verification succeeds, and memory usage remains stable across multiple forward/backward passes.\n\n![Computation Graph Structure](./diagrams/computation-graph.svg)\n\n![Gradient Backpropagation Flow](./diagrams/gradient-flow.svg)\n\n\n## Neural Network Modules (Milestone 3)\n\n> **Milestone(s):** Milestone 3 (Layers & Modules) - implements the module system with layers like Linear and activations, plus parameter management and initialization.\n\nNeural network modules are the building blocks that transform our powerful tensor operations and automatic differentiation engine into practical tools for constructing machine learning models. While the previous milestones gave us the mathematical foundation—tensors that can store multidimensional data and automatically compute gradients—this milestone focuses on the abstraction layer that makes neural network construction intuitive and composable. Think of modules as intelligent LEGO blocks that not only snap together in meaningful ways but also automatically handle the complex bookkeeping required for gradient-based learning.\n\n![Module System Organization](./diagrams/module-hierarchy.svg)\n\nThe module system addresses several critical challenges that emerge when building neural networks. First, it provides a clean abstraction for common neural network operations like linear transformations and activation functions, encapsulating both the forward computation and the gradient computation logic. Second, it implements a parameter management system that automatically tracks all trainable parameters across potentially complex nested network architectures. Third, it establishes conventions for weight initialization that can significantly impact training success. Finally, it creates a composable architecture where complex networks can be built by combining simpler, well-tested components.\n\n### Modules as LEGO Blocks\n\n> The power of the module system lies in its composability—simple components can be combined to create arbitrarily complex architectures while maintaining clean interfaces and automatic parameter tracking.\n\nUnderstanding neural network modules requires thinking beyond individual mathematical operations to consider how we can create reusable, composable components. Imagine building with LEGO blocks where each block not only has physical connection points but also carries information about how to process data and how to learn from mistakes. A linear layer is like a specialized LEGO block that knows how to perform matrix multiplication and adjust its internal weights based on feedback. An activation function is like another specialized block that applies a nonlinear transformation. A sequential container is like a baseplate that connects multiple blocks in a specific order, allowing data to flow through them systematically.\n\nThe key insight is that each module encapsulates both **state** (the parameters that define its behavior) and **behavior** (the forward computation and gradient handling). This encapsulation allows us to reason about network components at a higher level of abstraction. When we compose modules together, we don't need to manually track which tensors belong to which layer or manually chain gradient computations—the module system handles this automatically.\n\nConsider how this composability manifests in practice. A simple neural network might consist of a linear layer, followed by a ReLU activation, followed by another linear layer. Each component has a well-defined responsibility: the linear layers perform learnable transformations, and the ReLU introduces nonlinearity. When we chain them together in a sequential module, the result is a complete neural network that automatically handles forward computation, parameter tracking, and gradient flow.\n\nThis modular approach provides several crucial advantages. **Reusability** means we can define a linear layer once and use it throughout our network without duplication. **Testability** allows us to verify each component in isolation before composing them into larger systems. **Maintainability** lets us modify or replace individual components without affecting the entire network. **Extensibility** enables us to add new layer types by following established patterns and interfaces.\n\nThe module abstraction also handles complex scenarios that would be error-prone if managed manually. When modules are nested—such as a sequential module containing other sequential modules—the parameter tracking system recursively discovers and exposes all trainable parameters. When the same module instance is used multiple times in a network (weight sharing), the gradient accumulation system ensures that gradients from all usage sites are properly combined.\n\n### Module Base Class Design\n\nThe `Module` base class serves as the foundation for all neural network components in our framework, establishing the essential interface and common functionality that every layer and network component must provide. The design centers around three core responsibilities: managing trainable parameters, defining the forward computation contract, and enabling recursive operations across potentially complex nested module hierarchies.\n\nThe fundamental challenge in designing the module base class is balancing flexibility with structure. We need an interface that's general enough to accommodate diverse layer types—from simple element-wise activations to complex attention mechanisms—while providing enough structure to enable automatic parameter management and gradient flow. The solution is to define a minimal but powerful interface that handles the common concerns while allowing subclasses to implement their specific computational logic.\n\n**Module Interface Definition:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `forward` | `*inputs: Tensor` | `Tensor` | Abstract method defining forward computation logic |\n| `parameters` | None | `List[Tensor]` | Recursively collect all trainable parameters |\n| `named_parameters` | `prefix: str = \"\"` | `Dict[str, Tensor]` | Collect parameters with full hierarchical names |\n| `register_parameter` | `name: str, param: Tensor` | `None` | Register a tensor as trainable parameter |\n| `register_module` | `name: str, module: Module` | `None` | Register submodule for recursive operations |\n| `train` | `mode: bool = True` | `Module` | Set training/evaluation mode recursively |\n| `eval` | None | `Module` | Set to evaluation mode (calls train(False)) |\n| `zero_grad` | None | `None` | Reset gradients for all parameters to None |\n| `__call__` | `*inputs: Tensor` | `Tensor` | Make module callable, delegates to forward |\n\n**Module State Management:**\n\n| Attribute | Type | Description |\n|-----------|------|-------------|\n| `_parameters` | `Dict[str, Tensor]` | Dictionary storing registered trainable parameters |\n| `_modules` | `Dict[str, Module]` | Dictionary storing registered submodules |\n| `training` | `bool` | Flag indicating training vs evaluation mode |\n| `_name` | `Optional[str]` | Optional name for debugging and visualization |\n\nThe `forward` method represents the core abstraction of the module system. Every module must implement this method to define how it transforms input tensors into output tensors. By making this an abstract method, we ensure that all modules provide their computational logic while leaving the specific implementation details to each module type. The method signature accepts variable arguments to accommodate modules that take multiple inputs, such as attention layers that might receive query, key, and value tensors.\n\nThe parameter management system centers around the `register_parameter` and `register_module` methods. When a module registers a parameter, it's added to the internal `_parameters` dictionary with the given name. The parameter must be a `Tensor` with `requires_grad=True` to be considered trainable. When a module registers a submodule, the submodule is added to the `_modules` dictionary, enabling recursive operations like parameter collection and mode switching.\n\nThe recursive nature of parameter collection is crucial for handling nested module hierarchies. The `parameters` method implements a depth-first traversal that visits all submodules and collects their parameters. This means that a complex network built from nested sequential modules and individual layers will automatically expose all its trainable parameters through a single method call. The `named_parameters` method extends this by providing fully qualified names that reflect the module hierarchy, which is essential for debugging and parameter analysis.\n\n> **Decision: Abstract Base Class vs Interface**\n> - **Context**: Need to define common module functionality while allowing diverse implementations\n> - **Options Considered**: Pure interface with no shared code, abstract base class with common implementations, concrete base class with overridable methods\n> - **Decision**: Abstract base class with implemented parameter management and abstract forward method\n> - **Rationale**: Provides essential shared functionality (parameter tracking, recursive operations) while enforcing interface contract. Reduces boilerplate in subclasses while maintaining type safety.\n> - **Consequences**: Slightly more complex inheritance hierarchy, but significantly reduces implementation burden for new module types\n\n**Architecture Decision Comparison:**\n\n| Approach | Pros | Cons | Chosen? |\n|----------|------|------|---------|\n| Pure Interface | Maximum flexibility, no coupling | Duplicated parameter management code | No |\n| Abstract Base Class | Shared functionality, enforced interface | Some implementation constraints | **Yes** |\n| Concrete Base Class | Default implementations, very easy to extend | Weak interface contract, harder to verify | No |\n\nThe mode management system handles the distinction between training and evaluation phases. Some modules behave differently during training (dropout, batch normalization), so the base class provides infrastructure for propagating mode changes throughout the entire module hierarchy. The `train` and `eval` methods recursively update all submodules, ensuring consistent behavior across the entire network.\n\nThe `__call__` method enables modules to be used as callable objects, providing a clean interface where `output = module(input)` automatically invokes the forward method. This design pattern, borrowed from PyTorch, makes module usage intuitive while maintaining the separation between the public interface (`__call__`) and the implementation detail (`forward`).\n\nError handling in the module base class focuses on parameter registration validation and recursive operation safety. When registering parameters, the base class verifies that the provided object is actually a tensor and optionally checks that it has gradient tracking enabled. When performing recursive operations, the base class guards against infinite recursion by maintaining visited module sets during traversal.\n\n### Linear Layer Implementation\n\nThe linear layer represents the fundamental building block of feedforward neural networks, implementing the mathematical operation `y = Wx + b` where `W` is a learnable weight matrix, `b` is an optional learnable bias vector, and the operation represents matrix multiplication followed by bias addition. This seemingly simple transformation is remarkably powerful and serves as the foundation for fully connected networks, the final classification layers of convolutional networks, and components within more complex architectures like transformers.\n\nUnderstanding the linear layer requires appreciating both its mathematical simplicity and its implementation complexity. Mathematically, we're performing a linear transformation that maps input vectors from one dimension space to another. If our input has dimension `in_features` and we want output dimension `out_features`, we need a weight matrix of shape `(out_features, in_features)` and a bias vector of shape `(out_features,)`. However, the implementation must handle batched inputs, proper weight initialization, gradient flow, and broadcasting edge cases.\n\nThe design challenges for the linear layer center around shape management and initialization strategy. Neural networks typically process batches of inputs simultaneously, so our linear layer must handle input tensors of shape `(batch_size, in_features)` and produce outputs of shape `(batch_size, out_features)`. The weight matrix multiplication must be configured correctly to work with batched inputs, and the bias addition must broadcast properly across the batch dimension.\n\n**Linear Layer Interface:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `__init__` | `in_features: int, out_features: int, bias: bool = True` | None | Initialize layer with specified dimensions |\n| `forward` | `input: Tensor` | `Tensor` | Compute y = xW^T + b for input batch |\n| `reset_parameters` | None | None | Reinitialize weights and bias using default strategy |\n\n**Linear Layer Parameters:**\n\n| Parameter | Shape | Description | Initialization |\n|-----------|-------|-------------|----------------|\n| `weight` | `(out_features, in_features)` | Learnable transformation matrix | Xavier uniform |\n| `bias` | `(out_features,)` | Optional learnable bias vector | Zeros or small uniform |\n\nThe weight initialization strategy significantly impacts training dynamics and convergence behavior. Poor initialization can lead to vanishing or exploding gradients, particularly in deeper networks. The linear layer implements Xavier (also called Glorot) initialization by default, which sets initial weights by sampling from a uniform distribution with bounds calculated as `±sqrt(6 / (in_features + out_features))`. This initialization scheme aims to maintain similar variance in activations and gradients across layers, promoting stable training.\n\nThe forward computation involves matrix multiplication between the batched input and the transposed weight matrix, followed by bias addition if bias is enabled. For input tensor `x` with shape `(batch_size, in_features)` and weight tensor `W` with shape `(out_features, in_features)`, we compute `x @ W.T` to get shape `(batch_size, out_features)`. The bias tensor broadcasts automatically across the batch dimension during addition.\n\n**Forward Pass Algorithm:**\n\n1. **Input validation**: Verify input tensor has exactly 2 dimensions (batch_size, in_features)\n2. **Shape compatibility check**: Confirm input's second dimension matches layer's in_features\n3. **Weight matrix multiplication**: Compute `input @ self.weight.T` using tensor matmul operation\n4. **Bias addition**: If bias enabled, add bias tensor (broadcasts automatically across batch dimension)\n5. **Return result**: Output tensor has shape (batch_size, out_features)\n\nThe gradient flow through linear layers demonstrates the elegance of automatic differentiation. During backpropagation, gradients flow from the output back to both the input and the parameters. The gradient with respect to the input is computed as `grad_output @ weight`, enabling gradient flow to previous layers. The gradient with respect to the weight is computed as `grad_output.T @ input`, accumulating the contribution from all batch samples. The bias gradient is simply the sum of grad_output across the batch dimension.\n\n> **Decision: Weight Matrix Orientation**\n> - **Context**: Matrix multiplication can be organized as input @ weight.T or input @ weight depending on weight shape\n> - **Options Considered**: Weight shape (in_features, out_features) with input @ weight, weight shape (out_features, in_features) with input @ weight.T, weight shape matches PyTorch convention\n> - **Decision**: Weight shape (out_features, in_features) with input @ weight.T computation\n> - **Rationale**: Matches PyTorch convention for compatibility and intuitive parameter counting. Weight[i] represents the incoming connections to output neuron i.\n> - **Consequences**: Requires transpose during forward pass but enables intuitive parameter interpretation and framework compatibility\n\nBias handling requires careful consideration of when bias should be included versus omitted. Many modern architectures omit bias in certain contexts, particularly when batch normalization follows immediately after the linear transformation. The linear layer constructor accepts a `bias` parameter that controls whether bias is created and registered as a parameter. When bias is disabled, the bias parameter is set to None and no bias addition occurs during forward computation.\n\nThe parameter registration system ensures that both weight and bias tensors are properly tracked by the module system. During initialization, the layer calls `self.register_parameter('weight', self.weight)` and conditionally `self.register_parameter('bias', self.bias)` to make these tensors discoverable by optimizers and parameter collection methods.\n\n### Activation Function Modules\n\nActivation functions introduce nonlinearity into neural networks, enabling them to learn complex patterns and approximate arbitrary functions. Without activation functions, neural networks would be limited to linear transformations, regardless of depth. The mathematical property that makes activation functions powerful is their element-wise application and nonlinear response, which allows networks to model curved decision boundaries and complex feature interactions.\n\nThe implementation of activation function modules differs fundamentally from linear layers because they typically have no learnable parameters and operate element-wise on their inputs. However, they still benefit from the module abstraction for consistency, composability, and potential future extensions (like learnable activation functions). Each activation function must implement both the forward transformation and ensure proper gradient computation during backpropagation.\n\n**Common Activation Functions:**\n\n| Function | Formula | Derivative | Characteristics |\n|----------|---------|------------|-----------------|\n| ReLU | `max(0, x)` | `1 if x > 0 else 0` | Simple, addresses vanishing gradients, sparse |\n| Sigmoid | `1 / (1 + exp(-x))` | `sigmoid(x) * (1 - sigmoid(x))` | Smooth, bounded output [0,1], can saturate |\n| Tanh | `(exp(x) - exp(-x)) / (exp(x) + exp(-x))` | `1 - tanh²(x)` | Smooth, bounded output [-1,1], zero-centered |\n| Leaky ReLU | `max(0.01*x, x)` | `1 if x > 0 else 0.01` | Addresses dying ReLU problem |\n\n**ReLU Implementation Details:**\n\nThe Rectified Linear Unit (ReLU) has become the default activation function in many neural network architectures due to its computational simplicity and beneficial training properties. ReLU addresses the vanishing gradient problem that plagued earlier activation functions like sigmoid and tanh, particularly in deep networks. The function's derivative is either 0 or 1, which means gradients can flow through without attenuation when the unit is active.\n\nThe forward pass for ReLU is mathematically simple: `output = maximum(0, input)`. However, the implementation must handle the gradient computation correctly. During backpropagation, gradients only flow through units where the input was positive. This creates a sparse gradient pattern that can be computationally efficient but also leads to the \"dying ReLU\" problem where neurons can become permanently inactive.\n\n**Sigmoid and Tanh Considerations:**\n\nSigmoid and tanh activation functions were historically important but have largely been superseded by ReLU and its variants in hidden layers. However, they remain useful in specific contexts: sigmoid for binary classification output layers and tanh when zero-centered activations are beneficial. Both functions suffer from saturation issues where large positive or negative inputs produce gradients very close to zero, leading to vanishing gradients in deep networks.\n\nThe implementation of sigmoid requires careful numerical stability considerations. The naive implementation `1 / (1 + exp(-x))` can overflow when x is large and negative. A more stable implementation uses the identity `sigmoid(x) = exp(x) / (1 + exp(x))` for positive x and the original formula for negative x, avoiding overflow in both directions.\n\n**Activation Module Interface:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `__init__` | `inplace: bool = False` | None | Initialize activation with optional in-place operation |\n| `forward` | `input: Tensor` | `Tensor` | Apply activation function element-wise |\n\n**Gradient Flow Architecture:**\n\nActivation functions participate in the computation graph like any other operation, creating nodes that link input tensors to output tensors with appropriate backward functions. The automatic differentiation system handles the derivative computation automatically, but the activation function implementations must ensure they create the correct computational graph structure.\n\nFor ReLU, the backward function receives the gradient with respect to the output and must compute the gradient with respect to the input. This involves element-wise multiplication of the output gradient with a mask indicating where the original input was positive. The mask can be computed from the original input (stored during forward pass) or derived from the output (since output > 0 if and only if input > 0 for ReLU).\n\n> **Decision: In-place vs Out-of-place Operations**\n> - **Context**: Activation functions can potentially modify input tensors directly or create new output tensors\n> - **Options Considered**: Always out-of-place for safety, optional in-place for memory efficiency, always in-place for performance\n> - **Decision**: Optional in-place with default out-of-place behavior\n> - **Rationale**: Out-of-place is safer for gradient computation and debugging, but in-place can be crucial for memory efficiency in large networks. Optional flag provides flexibility.\n> - **Consequences**: Requires careful gradient handling for in-place operations, but enables memory optimization when needed\n\nThe numerical stability of activation function implementations becomes critical when dealing with extreme input values. For exponential-based functions like sigmoid and tanh, input values with large magnitude can cause numerical overflow or underflow. Modern implementations use mathematically equivalent formulations that are more numerically stable, such as using the log-sum-exp trick or choosing different computational paths based on input sign.\n\n**Parameter-free Module Benefits:**\n\nAlthough activation functions typically have no learnable parameters, implementing them as modules provides several advantages. First, it maintains consistency in the module hierarchy, allowing activations to be treated uniformly with other layer types in sequential containers. Second, it provides a foundation for parameterized activation functions like Leaky ReLU or learnable activations. Third, it enables proper participation in module operations like mode switching and recursive traversal.\n\nThe absence of parameters means activation modules have trivial parameter registration—they simply register no parameters with the base module class. This allows them to participate in parameter collection operations without contributing any trainable parameters, which is the expected behavior for standard activation functions.\n\n### Parameter Registration System\n\nThe parameter registration system forms the backbone of automatic parameter management in neural networks, enabling optimizers to discover and update all trainable parameters without manual bookkeeping. This system must solve several challenging problems: discovering parameters in arbitrarily nested module hierarchies, providing unique names for debugging and analysis, handling parameter sharing scenarios, and maintaining efficient access patterns for training loops.\n\nThe fundamental challenge is that neural networks can have complex, nested structures where modules contain other modules, which in turn contain their own parameters and submodules. A transformer model, for example, might have multiple encoder layers, each containing attention modules and feedforward modules, each with their own linear layers and parameters. The parameter registration system must traverse this hierarchy automatically and expose all trainable parameters through a clean interface.\n\n**Parameter Registration Workflow:**\n\n1. **Module initialization**: During module creation, parameters are created as `Tensor` objects with `requires_grad=True`\n2. **Registration call**: Module calls `self.register_parameter(name, tensor)` for each trainable parameter\n3. **Storage and validation**: Base class stores parameter in `_parameters` dict after validation\n4. **Submodule registration**: Complex modules register submodules using `self.register_module(name, module)`\n5. **Recursive collection**: Parameter collection methods traverse module tree to gather all parameters\n6. **Name generation**: Hierarchical names are constructed by concatenating module and parameter names\n\nThe registration validation process ensures that only appropriate objects are registered as parameters. The base module class checks that registered parameters are `Tensor` instances and optionally verifies that they have gradient tracking enabled. This validation catches common errors like accidentally registering regular numpy arrays or tensors with gradient tracking disabled.\n\n**Parameter Collection Interface:**\n\n| Method Name | Returns | Description |\n|-------------|---------|-------------|\n| `parameters()` | `Iterator[Tensor]` | Yield all parameters in depth-first order |\n| `named_parameters(prefix=\"\")` | `Iterator[Tuple[str, Tensor]]` | Yield (name, parameter) pairs with hierarchical names |\n| `parameter_count()` | `int` | Count total trainable parameters including all submodules |\n| `parameter_summary()` | `Dict[str, Any]` | Detailed parameter statistics for analysis |\n\nThe recursive parameter collection algorithm implements depth-first traversal to ensure consistent parameter ordering across multiple collection calls. For each module, the algorithm first yields the module's own parameters, then recursively traverses each submodule. This ordering is important for reproducible initialization and consistent optimizer state management.\n\n**Recursive Collection Algorithm:**\n\n1. **Initialize empty result list**: Create container for collected parameters\n2. **Add local parameters**: Iterate through `_parameters` dict and add each tensor\n3. **Traverse submodules**: For each module in `_modules` dict, recursively call parameter collection\n4. **Generate hierarchical names**: Construct names by joining module path with parameter name using dot notation\n5. **Handle duplicates**: Check for parameter sharing and avoid duplicate collection\n6. **Return results**: Yield parameters in consistent depth-first order\n\nThe hierarchical naming system provides crucial debugging capabilities by showing exactly where each parameter originates in the module hierarchy. For example, a parameter might have the name `\"encoder.layer_2.attention.query.weight\"`, immediately indicating its location and purpose. This naming convention follows the same patterns as PyTorch and other major frameworks, ensuring familiar debugging experiences.\n\n**Parameter Sharing Scenarios:**\n\nParameter sharing occurs when the same tensor object is used in multiple places within a network, either through module instance reuse or explicit tensor sharing. The parameter registration system must handle these scenarios correctly to avoid duplicate parameter collection and ensure proper gradient accumulation.\n\nWhen the same module instance appears multiple times in a network (such as using the same embedding layer for input and output), the parameter collection system recognizes that the parameter tensors are identical objects and collects them only once. The automatic differentiation system handles gradient accumulation automatically when the same parameter contributes to loss multiple times.\n\n> **Decision: Parameter Collection Caching**\n> - **Context**: Parameter collection involves recursive traversal which could be expensive for large networks\n> - **Options Considered**: No caching for simplicity, cache with invalidation on parameter changes, cache with manual refresh\n> - **Decision**: No caching with efficient iterator implementation\n> - **Rationale**: Parameter collection typically happens once per training step, so caching complexity isn't justified. Iterator approach avoids creating large intermediate lists.\n> - **Consequences**: Slightly more expensive repeated calls, but simpler implementation and no cache invalidation logic\n\n**Memory Management Considerations:**\n\nThe parameter registration system must carefully manage references to avoid creating circular dependencies that prevent garbage collection. The base module class stores direct references to parameter tensors and submodules, creating a tree structure rooted at the top-level module. When modules are no longer referenced externally, the entire subtree should be eligible for garbage collection.\n\nHowever, the computation graph created during forward passes can create temporary circular references between tensors and operations. The parameter registration system doesn't interfere with this automatic differentiation mechanism but ensures that the module hierarchy itself doesn't prevent proper cleanup.\n\n**Integration with Optimizers:**\n\nThe parameter registration system is designed to integrate seamlessly with optimizer implementations. Optimizers typically call `model.parameters()` to discover all trainable parameters, then maintain internal state (like momentum buffers) associated with each parameter. The consistent ordering provided by the registration system ensures that optimizer state remains properly aligned with parameters across training steps.\n\nFor advanced scenarios like fine-tuning where some parameters should be frozen, the parameter registration system supports parameter filtering. Methods like `named_parameters()` can be combined with filtering logic to expose only specific subsets of parameters to optimizers, enabling selective training scenarios.\n\n### Module System Architecture Decisions\n\nThe design of the module system involves numerous architectural decisions that impact usability, performance, extensibility, and maintainability. These decisions must balance competing concerns: the system should be simple enough for educational purposes while remaining powerful enough to support realistic neural network architectures. Each design choice creates trade-offs that ripple through the entire framework.\n\n> **Decision: Eager vs Lazy Parameter Initialization**\n> - **Context**: Parameters can be initialized during module creation or delayed until first forward pass\n> - **Options Considered**: Eager initialization during `__init__`, lazy initialization on first forward pass, hybrid approach with shape inference\n> - **Decision**: Eager initialization with explicit shape requirements\n> - **Rationale**: Simpler implementation, immediate error detection, predictable memory usage. Educational framework benefits from explicit parameter management.\n> - **Consequences**: Requires explicit input/output shapes during layer creation, but provides clearer error messages and deterministic initialization\n\n**Architecture Decision Comparison:**\n\n| Approach | Pros | Cons | Chosen? |\n|----------|------|------|---------|\n| Eager Init | Simple, predictable, immediate errors | Requires shape specification upfront | **Yes** |\n| Lazy Init | Shape inference, more flexible API | Complex state management, delayed errors | No |\n| Hybrid | Best of both worlds | Increased complexity, harder to debug | No |\n\nThe module composition strategy determines how complex networks are constructed from simpler components. The framework supports both explicit composition (manually connecting layers) and container-based composition (using Sequential and other container modules). This dual approach provides flexibility while maintaining simplicity for common cases.\n\n**Sequential Container Design:**\n\nSequential containers represent the most common neural network architecture pattern: a linear chain of modules where the output of one module becomes the input to the next. The implementation is remarkably simple yet powerful, automatically handling forward pass propagation and parameter collection across all contained modules.\n\nThe Sequential module stores an ordered list of submodules and implements forward pass by iterating through them in sequence, passing the output of each module as input to the next. Parameter collection works through the standard recursive mechanism, automatically discovering all parameters in all contained modules.\n\n**Sequential Implementation Characteristics:**\n\n| Aspect | Behavior | Benefit |\n|--------|----------|---------|\n| Forward Pass | Sequential application of submodules | Automatic data flow chaining |\n| Parameter Collection | Recursive traversal of submodules | Automatic parameter discovery |\n| Gradient Flow | Automatic through computation graph | No manual gradient routing |\n| Error Propagation | Shape mismatches caught at runtime | Clear error location identification |\n\n> **Decision: Module Mutability After Creation**\n> - **Context**: Should modules allow parameter modification, layer addition/removal after creation?\n> - **Options Considered**: Immutable modules for safety, mutable with careful state management, copy-on-write semantics\n> - **Decision**: Mutable modules with explicit state management\n> - **Rationale**: Educational framework benefits from experimentation and modification. Real frameworks are mutable. Complexity is manageable with clear documentation.\n> - **Consequences**: Potential for state inconsistencies, but enables important use cases like transfer learning and architecture search\n\nThe initialization strategy architecture decision impacts training success significantly. Different layer types benefit from different initialization schemes, and the framework must provide both sensible defaults and customization capabilities. The approach balances automatic initialization with explicit control when needed.\n\n**Initialization Strategy Options:**\n\n| Strategy | Use Case | Mathematical Basis | Implementation |\n|----------|----------|-------------------|----------------|\n| Xavier/Glorot | General feedforward networks | Maintains activation variance | Uniform(-bound, bound) where bound = sqrt(6/(fan_in + fan_out)) |\n| He/Kaiming | ReLU networks | Accounts for ReLU's zero region | Normal(0, sqrt(2/fan_in)) |\n| Zero | Bias initialization | No initial bias preference | All zeros |\n| Identity | Skip connections | Preserve input initially | Identity matrix where applicable |\n\n**State Management Architecture:**\n\nModule state management encompasses both trainable parameters and non-trainable state like running statistics in batch normalization or cached values for efficiency. The framework distinguishes between these state types while providing consistent interfaces for state access and modification.\n\nThe training/evaluation mode system represents a critical state management concern. Some modules behave differently during training versus inference, and the framework must propagate mode changes throughout the entire module hierarchy automatically. This is implemented through recursive mode setting that traverses all submodules.\n\n**Error Handling Strategy:**\n\nThe module system implements defensive error handling that catches common mistakes early with informative error messages. Shape mismatches, parameter registration errors, and invalid module compositions are detected and reported with context about the specific module and operation involved.\n\n**Common Error Detection:**\n\n| Error Type | Detection Point | Error Message Strategy |\n|------------|----------------|----------------------|\n| Shape Mismatch | Forward pass entry | Include expected vs actual shapes, layer name |\n| Parameter Registration | Registration call | Validate tensor type, gradient requirements |\n| Module Composition | Container construction | Check compatibility of adjacent layers |\n| Initialization | Parameter creation | Validate initialization parameters, warn about common issues |\n\nThe extensibility architecture ensures that new module types can be added easily while following established patterns. The base class provides the essential infrastructure, and new modules need only implement the forward computation logic while following parameter registration conventions.\n\n**Extension Points:**\n\n1. **Custom Layer Types**: Implement Module subclass with forward method and parameter registration\n2. **Custom Initialization**: Override reset_parameters method or provide custom initialization functions\n3. **Custom Containers**: Implement Module subclass with custom composition logic and parameter forwarding\n4. **Custom State Management**: Extend base class with additional state tracking for specialized requirements\n\n### Common Module Implementation Pitfalls\n\nBuilding neural network modules involves numerous subtle details that can lead to hard-to-debug issues when implemented incorrectly. These pitfalls often manifest as training instabilities, incorrect gradients, or runtime errors that occur only in specific scenarios. Understanding and avoiding these common mistakes is crucial for successful framework implementation.\n\n⚠️ **Pitfall: Parameters Not Properly Registered**\n\nOne of the most common and frustrating errors occurs when module parameters are created but not registered with the parameter management system. This happens when developers create parameter tensors during module initialization but forget to call `self.register_parameter()`, or when they modify parameter references after registration.\n\nThe symptom is that optimizers cannot find the parameters, leading to no learning during training. The model forward pass works correctly, but parameters never update despite calling optimizer.step(). This issue is particularly insidious because the forward pass computation is correct, making the problem seem like an optimizer or gradient computation bug.\n\n**Why it's wrong**: Unregistered parameters are invisible to the parameter collection system, so optimizers never see them. Even if gradients are computed correctly, the optimizer has no reference to the parameter tensors for updates.\n\n**How to fix**: Always call `self.register_parameter(name, tensor)` immediately after creating parameter tensors. Use descriptive names and verify registration by calling `list(module.parameters())` after module creation. Implement parameter registration checks in module unit tests.\n\n⚠️ **Pitfall: Incorrect Weight Initialization Destroying Training**\n\nWeight initialization has a dramatic impact on training dynamics, and poor initialization choices can make networks untrainable regardless of architecture or optimization algorithm quality. Common mistakes include using initialization schemes inappropriate for the activation function, failing to account for layer fan-in/fan-out, or using the same initialization for all layer types.\n\nThe symptoms include vanishing gradients (loss doesn't decrease, gradients become very small), exploding gradients (loss becomes NaN, gradients grow exponentially), or extremely slow convergence (training makes progress but very slowly). These issues often manifest differently depending on network depth and architecture.\n\n**Why it's wrong**: Inappropriate initialization breaks the careful balance of activation and gradient magnitudes that enable effective gradient-based learning. Xavier initialization assumes linear/sigmoid activations, while He initialization is designed for ReLU networks. Using the wrong scheme can cause activation saturation or gradient attenuation.\n\n**How to fix**: Use Xavier/Glorot initialization for sigmoid/tanh networks and He/Kaiming initialization for ReLU networks. Initialize biases to zero unless specific architectural reasons suggest otherwise. Implement multiple initialization schemes and provide clear documentation about when to use each. Test initialization effects on toy problems to verify behavior.\n\n⚠️ **Pitfall: In-Place Operations Breaking Gradient Computation**\n\nIn-place operations modify tensor data directly rather than creating new tensors, which can break the computation graph required for backpropagation. This occurs when modules modify input tensors directly or when activation functions use in-place operations without proper gradient handling.\n\nThe symptom is incorrect gradients during backpropagation, often manifesting as NaN gradients or gradients that don't match numerical differentiation. The error might not appear immediately but surface during gradient checking or when gradients are examined manually.\n\n**Why it's wrong**: In-place operations can overwrite data needed for gradient computation or break the computation graph links between tensors and operations. The automatic differentiation system relies on preserving the computation history, which in-place operations can corrupt.\n\n**How to fix**: Default to out-of-place operations for safety. When implementing in-place operations for memory efficiency, ensure they properly handle gradient computation by preserving necessary information for backpropagation. Implement gradient checking tests that compare automatic differentiation results with numerical differentiation.\n\n⚠️ **Pitfall: Shape Broadcasting Edge Cases**\n\nBroadcasting rules can create subtle bugs when tensor shapes interact in unexpected ways, particularly when batch dimensions, singleton dimensions, or empty tensors are involved. Common issues include bias vectors that don't broadcast correctly, weight matrices with transposed dimensions, or operations that broadcast when they shouldn't.\n\nThe symptoms include shape errors during forward pass, incorrect output shapes that cause downstream failures, or silent correctness bugs where operations produce wrong results with correct shapes. These issues often appear only with specific input shapes or batch sizes.\n\n**Why it's wrong**: Incorrect shape handling leads to mathematical operations that don't correspond to the intended neural network computation. Even when operations succeed due to broadcasting, the result may not represent the desired linear transformation or element-wise operation.\n\n**How to fix**: Implement explicit shape checking in forward methods, particularly for input validation and output verification. Test modules with various input shapes including edge cases like batch size 1, different input dimensions, and boundary conditions. Use descriptive variable names that indicate tensor shapes and include shape comments in complex operations.\n\n⚠️ **Pitfall: Module State Inconsistencies During Mode Switching**\n\nNeural network modules can have different behavior during training versus evaluation (like dropout or batch normalization), and failing to handle mode switching correctly leads to inconsistent behavior across training and inference phases. This often occurs when custom modules don't properly implement mode switching or when mode changes aren't propagated to submodules.\n\nThe symptoms include different outputs for the same input during training vs evaluation when no difference is expected, or identical outputs when differences are expected (like dropout being active in both modes). Performance discrepancies between training and inference can also indicate mode switching issues.\n\n**Why it's wrong**: Inconsistent mode handling means the model behavior during training doesn't match inference behavior, leading to poor generalization or incorrect evaluation metrics. Some modules fundamentally require different behavior between modes to function correctly.\n\n**How to fix**: Implement proper mode switching in the base Module class and ensure it propagates recursively to all submodules. Test modules in both training and evaluation modes to verify expected behavior differences. For stateless modules like most activations, ensure mode switching is properly ignored without causing errors.\n\n⚠️ **Pitfall: Parameter Sharing Reference Management**\n\nWhen modules share parameters (like using the same embedding layer for input and output), improper reference management can lead to gradient accumulation issues, memory leaks, or incorrect parameter updates. This is particularly common in architectures with weight tying or when manually sharing parameters across modules.\n\nThe symptoms include gradients that are too large (doubled when two modules share parameters), memory usage that grows over time, or parameters that update incorrectly during optimization. These issues can be subtle and may only appear in specific architectural configurations.\n\n**Why it's wrong**: Parameter sharing requires careful coordination between the modules that share parameters, the automatic differentiation system that accumulates gradients, and the optimizer that applies updates. Incorrect reference management can break any of these interactions.\n\n**How to fix**: When implementing parameter sharing, ensure that shared parameters are registered properly in all modules that use them. Test gradient accumulation behavior with shared parameters using numerical differentiation. Implement memory management tests to detect reference cycle issues that prevent garbage collection.\n\n### Implementation Guidance\n\nThe module system represents the highest-level abstraction in our neural network framework, building directly on the tensor operations and automatic differentiation components developed in previous milestones. This implementation guidance provides concrete code structure and implementation patterns for creating the module hierarchy, parameter management system, and common layer implementations.\n\n**A. Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Base Module Class | Plain Python class with dict storage | Abstract base class with metaclass registration |\n| Parameter Storage | Simple dict with string keys | OrderedDict for consistent parameter ordering |\n| Initialization | Basic random initialization | Multiple initialization schemes with automatic selection |\n| Container Modules | List-based sequential container | Generic container with arbitrary connection patterns |\n| Type Checking | Runtime assertions | Full type hints with mypy validation |\n\n**B. Recommended File Structure:**\n```\nframework/\n  modules/\n    __init__.py              ← Module exports and public API\n    module.py                ← Base Module class and core functionality\n    linear.py                ← Linear layer implementation\n    activation.py            ← Activation function modules (ReLU, Sigmoid, Tanh)\n    container.py             ← Sequential and other container modules\n    parameter.py             ← Parameter registration and collection utilities\n    init.py                  ← Weight initialization functions\n  tests/\n    test_modules.py          ← Module system integration tests\n    test_linear.py           ← Linear layer specific tests\n    test_activations.py      ← Activation function tests\n    test_parameter_mgmt.py   ← Parameter management system tests\n```\n\n**C. Infrastructure Starter Code:**\n\n**Parameter Registration Utilities (framework/modules/parameter.py):**\n```python\n\"\"\"Parameter management utilities for module system.\"\"\"\nfrom typing import Iterator, Tuple, Dict, Any, Optional\nfrom ..tensor import Tensor\n\ndef validate_parameter(param: Any) -> Tensor:\n    \"\"\"Validate that an object is a proper parameter tensor.\n    \n    Args:\n        param: Object to validate as parameter\n        \n    Returns:\n        Validated tensor parameter\n        \n    Raises:\n        TypeError: If param is not a Tensor\n        ValueError: If param doesn't have requires_grad=True\n    \"\"\"\n    if not isinstance(param, Tensor):\n        raise TypeError(f\"Parameters must be Tensor instances, got {type(param)}\")\n    \n    if not param.requires_grad:\n        raise ValueError(\"Parameter tensors must have requires_grad=True for training\")\n    \n    return param\n\ndef count_parameters(parameters: Iterator[Tensor]) -> int:\n    \"\"\"Count total number of trainable parameters.\n    \n    Args:\n        parameters: Iterator over parameter tensors\n        \n    Returns:\n        Total number of scalar parameters\n    \"\"\"\n    total = 0\n    for param in parameters:\n        total += param.data.size\n    return total\n\ndef parameter_summary(named_params: Iterator[Tuple[str, Tensor]]) -> Dict[str, Any]:\n    \"\"\"Generate detailed parameter statistics.\n    \n    Args:\n        named_params: Iterator over (name, parameter) pairs\n        \n    Returns:\n        Dictionary with parameter statistics and analysis\n    \"\"\"\n    summary = {\n        'total_params': 0,\n        'trainable_params': 0,\n        'layers': {},\n        'param_sizes': []\n    }\n    \n    for name, param in named_params:\n        param_count = param.data.size\n        summary['total_params'] += param_count\n        if param.requires_grad:\n            summary['trainable_params'] += param_count\n        \n        layer_name = name.split('.')[0] if '.' in name else 'root'\n        if layer_name not in summary['layers']:\n            summary['layers'][layer_name] = 0\n        summary['layers'][layer_name] += param_count\n        \n        summary['param_sizes'].append((name, param.shape, param_count))\n    \n    return summary\n```\n\n**Weight Initialization Functions (framework/modules/init.py):**\n```python\n\"\"\"Weight initialization schemes for neural network parameters.\"\"\"\nimport numpy as np\nfrom ..tensor import Tensor\nimport math\n\ndef xavier_uniform_(tensor: Tensor, gain: float = 1.0) -> Tensor:\n    \"\"\"Initialize tensor with Xavier/Glorot uniform distribution.\n    \n    Args:\n        tensor: Parameter tensor to initialize\n        gain: Scaling factor for initialization range\n        \n    Returns:\n        Initialized tensor (modified in-place)\n    \"\"\"\n    fan_in = tensor.shape[-1] if len(tensor.shape) >= 2 else tensor.shape[0]\n    fan_out = tensor.shape[0] if len(tensor.shape) >= 2 else tensor.shape[0]\n    \n    std = gain * math.sqrt(6.0 / (fan_in + fan_out))\n    bound = std  # For uniform distribution, bound = std\n    \n    tensor.data = np.random.uniform(-bound, bound, tensor.shape)\n    return tensor\n\ndef kaiming_uniform_(tensor: Tensor, a: float = 0, mode: str = 'fan_in') -> Tensor:\n    \"\"\"Initialize tensor with Kaiming/He uniform distribution.\n    \n    Args:\n        tensor: Parameter tensor to initialize\n        a: Negative slope for LeakyReLU (0 for ReLU)\n        mode: Either 'fan_in' or 'fan_out'\n        \n    Returns:\n        Initialized tensor (modified in-place)\n    \"\"\"\n    num_input = tensor.shape[-1] if len(tensor.shape) >= 2 else tensor.shape[0]\n    num_output = tensor.shape[0] if len(tensor.shape) >= 2 else tensor.shape[0]\n    \n    fan = num_input if mode == 'fan_in' else num_output\n    gain = math.sqrt(2.0 / (1 + a ** 2))\n    std = gain / math.sqrt(fan)\n    bound = math.sqrt(3.0) * std\n    \n    tensor.data = np.random.uniform(-bound, bound, tensor.shape)\n    return tensor\n\ndef zeros_(tensor: Tensor) -> Tensor:\n    \"\"\"Initialize tensor with zeros.\n    \n    Args:\n        tensor: Parameter tensor to initialize\n        \n    Returns:\n        Initialized tensor (modified in-place)\n    \"\"\"\n    tensor.data = np.zeros(tensor.shape)\n    return tensor\n\ndef ones_(tensor: Tensor) -> Tensor:\n    \"\"\"Initialize tensor with ones.\n    \n    Args:\n        tensor: Parameter tensor to initialize\n        \n    Returns:\n        Initialized tensor (modified in-place)\n    \"\"\"\n    tensor.data = np.ones(tensor.shape)\n    return tensor\n```\n\n**D. Core Logic Skeleton Code:**\n\n**Base Module Class (framework/modules/module.py):**\n```python\n\"\"\"Base Module class providing core neural network module functionality.\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Iterator, Tuple, Optional, Any\nfrom ..tensor import Tensor\nfrom .parameter import validate_parameter, count_parameters, parameter_summary\n\nclass Module(ABC):\n    \"\"\"Base class for all neural network modules.\n    \n    Provides parameter registration, recursive operations, and training/eval modes.\n    All custom layers should inherit from this class and implement forward().\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize module with empty parameter and submodule storage.\"\"\"\n        self._parameters: Dict[str, Tensor] = {}\n        self._modules: Dict[str, 'Module'] = {}\n        self.training: bool = True\n        self._name: Optional[str] = None\n    \n    @abstractmethod\n    def forward(self, *inputs: Tensor) -> Tensor:\n        \"\"\"Define forward computation logic.\n        \n        Args:\n            *inputs: Input tensors for computation\n            \n        Returns:\n            Output tensor from forward computation\n            \n        Note:\n            Subclasses must implement this method with their specific logic.\n        \"\"\"\n        # TODO 1: Implement forward pass computation specific to your module type\n        # TODO 2: Ensure all operations create proper computation graph for gradients\n        # TODO 3: Handle both single tensor and multi-tensor inputs as appropriate\n        # TODO 4: Return tensor with correct shape and gradient tracking\n        raise NotImplementedError(\"Subclasses must implement forward() method\")\n    \n    def __call__(self, *inputs: Tensor) -> Tensor:\n        \"\"\"Make module callable, delegating to forward method.\n        \n        Args:\n            *inputs: Input tensors\n            \n        Returns:\n            Output from forward pass\n        \"\"\"\n        # TODO 1: Add any pre-forward hooks or validation here\n        # TODO 2: Call self.forward() with inputs\n        # TODO 3: Add any post-forward hooks or processing here\n        # TODO 4: Return the forward pass result\n        pass\n    \n    def register_parameter(self, name: str, param: Optional[Tensor]) -> None:\n        \"\"\"Register a parameter tensor for training.\n        \n        Args:\n            name: Parameter name for identification\n            param: Parameter tensor (None to remove parameter)\n        \"\"\"\n        # TODO 1: Validate parameter name is not empty and is string\n        # TODO 2: If param is None, remove from _parameters dict if present\n        # TODO 3: If param is not None, validate it using validate_parameter()\n        # TODO 4: Store validated parameter in _parameters dict with given name\n        # TODO 5: Set the tensor's parameter name attribute for debugging\n        pass\n    \n    def register_module(self, name: str, module: Optional['Module']) -> None:\n        \"\"\"Register a submodule for recursive operations.\n        \n        Args:\n            name: Submodule name for identification\n            module: Submodule instance (None to remove submodule)\n        \"\"\"\n        # TODO 1: Validate module name is not empty and is string\n        # TODO 2: If module is None, remove from _modules dict if present\n        # TODO 3: If module is not None, validate it's a Module instance\n        # TODO 4: Store module in _modules dict with given name\n        # TODO 5: Set the submodule's _name attribute for debugging\n        pass\n    \n    def parameters(self, recurse: bool = True) -> Iterator[Tensor]:\n        \"\"\"Return iterator over module parameters.\n        \n        Args:\n            recurse: If True, include parameters from submodules\n            \n        Yields:\n            Parameter tensors in depth-first order\n        \"\"\"\n        # TODO 1: Iterate through self._parameters.values() and yield each parameter\n        # TODO 2: If recurse is True, iterate through self._modules.values()\n        # TODO 3: For each submodule, recursively call submodule.parameters(recurse=True)\n        # TODO 4: Yield all parameters from submodules in depth-first order\n        # TODO 5: Ensure consistent ordering across multiple calls\n        pass\n    \n    def named_parameters(self, prefix: str = \"\", recurse: bool = True) -> Iterator[Tuple[str, Tensor]]:\n        \"\"\"Return iterator over module parameters with names.\n        \n        Args:\n            prefix: Prefix to prepend to parameter names\n            recurse: If True, include parameters from submodules\n            \n        Yields:\n            (name, parameter) pairs with hierarchical names\n        \"\"\"\n        # TODO 1: Iterate through self._parameters.items() \n        # TODO 2: For each (name, param), construct full_name using prefix\n        # TODO 3: Yield (full_name, param) tuple\n        # TODO 4: If recurse is True, iterate through self._modules.items()\n        # TODO 5: For each submodule, construct submodule_prefix and recurse\n        # TODO 6: Ensure proper dot notation for hierarchical names\n        pass\n    \n    def train(self, mode: bool = True) -> 'Module':\n        \"\"\"Set training mode recursively.\n        \n        Args:\n            mode: Training mode (True) or evaluation mode (False)\n            \n        Returns:\n            Self for method chaining\n        \"\"\"\n        # TODO 1: Set self.training = mode\n        # TODO 2: Iterate through all submodules in self._modules.values()\n        # TODO 3: Call submodule.train(mode) for each submodule\n        # TODO 4: Return self to enable method chaining\n        pass\n    \n    def eval(self) -> 'Module':\n        \"\"\"Set evaluation mode recursively.\n        \n        Returns:\n            Self for method chaining\n        \"\"\"\n        # TODO 1: Call self.train(False) to set evaluation mode\n        # TODO 2: Return the result for method chaining\n        pass\n    \n    def zero_grad(self) -> None:\n        \"\"\"Reset gradients for all parameters to None.\"\"\"\n        # TODO 1: Iterate through all parameters using self.parameters()\n        # TODO 2: For each parameter, set param.grad = None\n        # TODO 3: This clears accumulated gradients before backward pass\n        pass\n```\n\n**Linear Layer Implementation (framework/modules/linear.py):**\n```python\n\"\"\"Linear (fully connected) layer implementation.\"\"\"\nimport math\nfrom typing import Optional\nfrom .module import Module\nfrom .init import xavier_uniform_, zeros_\nfrom ..tensor import Tensor\n\nclass Linear(Module):\n    \"\"\"Linear transformation layer: y = xW^T + b\n    \n    Applies linear transformation to incoming data with learnable weights and bias.\n    Supports batched inputs and automatic gradient computation.\n    \"\"\"\n    \n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        \"\"\"Initialize linear layer.\n        \n        Args:\n            in_features: Size of input features\n            out_features: Size of output features  \n            bias: If True, add learnable bias vector\n        \"\"\"\n        super().__init__()\n        \n        self.in_features = in_features\n        self.out_features = out_features\n        \n        # TODO 1: Create weight tensor with shape (out_features, in_features)\n        # TODO 2: Set requires_grad=True for weight tensor\n        # TODO 3: Register weight as parameter using self.register_parameter()\n        # TODO 4: If bias=True, create bias tensor with shape (out_features,)\n        # TODO 5: Set requires_grad=True for bias and register as parameter\n        # TODO 6: If bias=False, set self.bias = None\n        # TODO 7: Call self.reset_parameters() to initialize weights\n        \n    def reset_parameters(self) -> None:\n        \"\"\"Initialize parameters using appropriate initialization scheme.\"\"\"\n        # TODO 1: Initialize self.weight using xavier_uniform_()\n        # TODO 2: If self.bias is not None, initialize bias using zeros_()\n        # Hint: Xavier initialization is good default for general networks\n        pass\n    \n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"Forward pass: compute y = xW^T + b\n        \n        Args:\n            input: Input tensor of shape (batch_size, in_features)\n            \n        Returns:\n            Output tensor of shape (batch_size, out_features)\n        \"\"\"\n        # TODO 1: Validate input tensor has correct number of dimensions (should be 2D)\n        # TODO 2: Validate input.shape[-1] == self.in_features\n        # TODO 3: Compute matrix multiplication: input @ self.weight.T\n        # TODO 4: If self.bias is not None, add bias to result\n        # TODO 5: Return result tensor with proper gradient tracking\n        # Hint: Use tensor's matmul() method for matrix multiplication\n        # Hint: Bias addition will broadcast automatically across batch dimension\n        pass\n\n    def extra_repr(self) -> str:\n        \"\"\"Return extra information for module representation.\"\"\"\n        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n```\n\n**ReLU Activation Implementation (framework/modules/activation.py):**\n```python\n\"\"\"Activation function modules.\"\"\"\nfrom .module import Module\nfrom ..tensor import Tensor\nimport numpy as np\n\nclass ReLU(Module):\n    \"\"\"Rectified Linear Unit activation: f(x) = max(0, x)\n    \n    Applies element-wise ReLU function with optional in-place operation.\n    \"\"\"\n    \n    def __init__(self, inplace: bool = False):\n        \"\"\"Initialize ReLU activation.\n        \n        Args:\n            inplace: If True, modify input tensor directly (saves memory)\n        \"\"\"\n        super().__init__()\n        self.inplace = inplace\n    \n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"Apply ReLU activation element-wise.\n        \n        Args:\n            input: Input tensor of any shape\n            \n        Returns:\n            Output tensor with ReLU applied element-wise\n        \"\"\"\n        # TODO 1: Apply max(0, x) operation element-wise to input\n        # TODO 2: If self.inplace is True, modify input tensor directly\n        # TODO 3: If self.inplace is False, create new tensor for output\n        # TODO 4: Ensure proper gradient tracking in computation graph\n        # TODO 5: Return result tensor\n        # Hint: You may need to implement a ReLU operation in your tensor class\n        # Hint: Can use tensor comparison and multiplication operations if no direct ReLU\n        pass\n\nclass Sigmoid(Module):\n    \"\"\"Sigmoid activation: f(x) = 1 / (1 + exp(-x))\n    \n    Applies element-wise sigmoid function with numerical stability.\n    \"\"\"\n    \n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"Apply sigmoid activation element-wise.\n        \n        Args:\n            input: Input tensor of any shape\n            \n        Returns:\n            Output tensor with sigmoid applied element-wise\n        \"\"\"\n        # TODO 1: Implement numerically stable sigmoid computation\n        # TODO 2: For positive x, use: exp(x) / (1 + exp(x))  \n        # TODO 3: For negative x, use: 1 / (1 + exp(-x))\n        # TODO 4: This avoids overflow for large positive/negative values\n        # TODO 5: Return result with proper gradient tracking\n        # Hint: May need to implement conditional operations or use numpy functions\n        pass\n\nclass Tanh(Module):\n    \"\"\"Hyperbolic tangent activation: f(x) = tanh(x)\n    \n    Applies element-wise tanh function.\n    \"\"\"\n    \n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"Apply tanh activation element-wise.\n        \n        Args:\n            input: Input tensor of any shape\n            \n        Returns:\n            Output tensor with tanh applied element-wise  \n        \"\"\"\n        # TODO 1: Apply tanh function element-wise to input\n        # TODO 2: Can use numpy.tanh or implement using exponentials\n        # TODO 3: Ensure proper gradient computation in automatic differentiation\n        # TODO 4: Return result tensor with gradient tracking\n        pass\n```\n\n**Sequential Container Implementation (framework/modules/container.py):**\n```python\n\"\"\"Container modules for composing neural networks.\"\"\"\nfrom typing import List, Union\nfrom .module import Module\nfrom ..tensor import Tensor\n\nclass Sequential(Module):\n    \"\"\"Sequential container that chains modules in order.\n    \n    Passes input through each module sequentially, where output of module i\n    becomes input to module i+1.\n    \"\"\"\n    \n    def __init__(self, *modules: Module):\n        \"\"\"Initialize sequential container.\n        \n        Args:\n            *modules: Variable number of modules to chain sequentially\n        \"\"\"\n        super().__init__()\n        \n        # TODO 1: Store modules in an ordered list or dict\n        # TODO 2: Register each module using self.register_module() with numeric names\n        # TODO 3: Validate that all provided objects are Module instances\n        # TODO 4: Handle empty module list case appropriately\n        # Hint: Use enumerate() to generate numeric names like \"0\", \"1\", \"2\"\n        \n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"Forward pass through all modules sequentially.\n        \n        Args:\n            input: Input tensor\n            \n        Returns:\n            Output tensor after passing through all modules\n        \"\"\"\n        # TODO 1: Start with input tensor as current value\n        # TODO 2: Iterate through all registered modules in order\n        # TODO 3: For each module, call module(current_value) to get next value\n        # TODO 4: Update current_value with the result\n        # TODO 5: Return final current_value as output\n        # TODO 6: Handle case where no modules are registered\n        pass\n    \n    def append(self, module: Module) -> 'Sequential':\n        \"\"\"Add a module to the end of the sequence.\n        \n        Args:\n            module: Module to append\n            \n        Returns:\n            Self for method chaining\n        \"\"\"\n        # TODO 1: Generate next numeric name for the module\n        # TODO 2: Register module using self.register_module()\n        # TODO 3: Return self for method chaining\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- Use `super().__init__()` in all module constructors to initialize base class properly\n- Store module parameters in `_parameters` dict and submodules in `_modules` dict with descriptive names\n- Use `isinstance(obj, Tensor)` to validate parameter types during registration\n- Implement `__repr__()` methods in modules for better debugging and visualization\n- Use `typing` module for clear type hints, especially `Optional[Tensor]` for optional parameters\n- Consider using `@property` decorators for computed attributes like parameter counts\n- Use descriptive variable names that indicate tensor shapes: `input_batch`, `weight_matrix`, `bias_vector`\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the module system, verify functionality with these tests:\n\n```python\n# Test linear layer creation and forward pass\nlinear = Linear(in_features=784, out_features=128)\ninput_batch = Tensor(np.random.randn(32, 784), requires_grad=True)\noutput = linear(input_batch)\nassert output.shape == (32, 128)\n\n# Test parameter collection\nparams = list(linear.parameters())\nassert len(params) == 2  # weight and bias\nassert params[0].shape == (128, 784)  # weight\nassert params[1].shape == (128,)      # bias\n\n# Test sequential container\nmodel = Sequential(\n    Linear(784, 256),\n    ReLU(),\n    Linear(256, 128),\n    ReLU(), \n    Linear(128, 10)\n)\n\n# Test end-to-end forward pass\noutput = model(input_batch)\nassert output.shape == (32, 10)\n\n# Test parameter counting\ntotal_params = sum(p.data.size for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")\n\n# Test gradient flow\nloss = output.sum()\nloss.backward()\nfor param in model.parameters():\n    assert param.grad is not None\n    assert param.grad.shape == param.shape\n```\n\nExpected behaviors:\n- Linear layer should initialize weights with reasonable values (not all zeros or very large)\n- ReLU should zero out negative values while preserving positive values\n- Sequential container should chain operations correctly with proper shape propagation\n- Parameter collection should find all parameters recursively through the hierarchy\n- Gradients should flow back through all parameters after loss.backward()\n\nSigns something is wrong:\n- Parameters are all zeros after initialization → Check initialization implementation\n- Shapes don't match expectations → Review broadcasting and matrix multiplication logic\n- Parameters not found by optimizer → Check parameter registration calls\n- Gradients are None after backward pass → Verify computation graph construction in forward pass\n\n\n## Optimizers and Training Loop (Milestone 4)\n\n> **Milestone(s):** Milestone 4 (Optimizers & Training) - implements optimization algorithms like SGD and Adam, plus training loop infrastructure with loss functions and learning rate scheduling.\n\nNow that we have built tensors with automatic differentiation and composable neural network modules, we face the final challenge: how do we actually train these networks to learn from data? The training process requires coordinating multiple components—computing predictions, measuring errors, calculating gradients, and updating parameters—in a carefully orchestrated dance that transforms random weights into a functioning neural network.\n\nThe optimization layer sits at the top of our four-layer architecture, orchestrating the interaction between all lower layers. It takes the gradients computed by our automatic differentiation engine and uses them to improve the parameters stored in our neural network modules. This process of iterative improvement, guided by mathematical optimization principles, is what enables neural networks to learn complex patterns from data.\n\n### Optimizers as GPS Navigation\n\nThink of training a neural network like navigating through an unfamiliar mountainous landscape in dense fog, where your goal is to reach the lowest valley (the minimum loss). You can't see the entire terrain, but you have a special GPS device that tells you the slope of the ground right where you're standing. This slope information is your **gradient**—it points in the direction of steepest uphill climb.\n\nAn **optimizer** is like your navigation strategy for using this gradient information to reach the valley. Just as different GPS routing algorithms make different trade-offs between speed and accuracy, different optimizers use gradient information in different ways:\n\n- **Stochastic Gradient Descent (SGD)** is like always walking directly downhill from your current position. It's simple and reliable, but might get stuck in small dips or take inefficient zigzag paths down narrow valleys.\n\n- **SGD with Momentum** is like rolling a heavy ball downhill instead of walking. The ball builds up speed in consistent directions and pushes through small obstacles, but takes longer to change direction when the terrain shifts.\n\n- **Adam** is like having an advanced GPS that remembers the terrain you've already explored and adapts its recommendations based on both recent slopes and long-term patterns. It automatically adjusts step sizes and has built-in shock absorbers for noisy terrain.\n\nThe key insight is that optimizers don't just use the current gradient—they maintain **state** about the optimization process. This state might include momentum from previous steps, running averages of gradient magnitudes, or adaptive learning rates. Just as your GPS routing improves by remembering traffic patterns and road conditions, optimizers improve parameter updates by remembering the history of gradients.\n\n> **Design Insight**: The optimizer is the only component that modifies parameter values. Modules compute gradients, but they never update their own parameters. This separation of concerns ensures that optimization strategies can be changed independently of network architecture.\n\n### Stochastic Gradient Descent\n\nStochastic Gradient Descent represents the foundation of neural network optimization. Despite its simplicity, SGD with proper tuning can train state-of-the-art models and provides the conceptual baseline for understanding more sophisticated optimizers.\n\nThe core SGD algorithm updates each parameter by subtracting a small fraction of its gradient. If we think of the loss function as a hilly landscape, the gradient at any point indicates the direction of steepest ascent. By moving in the opposite direction (negative gradient), we move toward lower loss values. The **learning rate** controls how large steps we take—too small and training progresses slowly, too large and we might overshoot the minimum.\n\n**Basic SGD Update Rule:**\n1. Compute gradients for all parameters using backpropagation\n2. For each parameter, subtract learning rate times its gradient\n3. Mathematically: `parameter = parameter - learning_rate * gradient`\n4. Clear gradients to prepare for the next training step\n\nHowever, basic SGD suffers from several limitations. In narrow valleys, it oscillates back and forth across the valley walls instead of making steady progress along the valley floor. When the loss landscape has different curvatures in different directions, SGD struggles to find efficient paths to the minimum.\n\n**Momentum Enhancement** addresses these issues by accumulating a velocity vector that represents the exponentially weighted moving average of gradients. This creates inertia that smooths out oscillations and accelerates movement in consistent directions.\n\n**SGD with Momentum Algorithm:**\n1. Compute current gradients using backpropagation\n2. Update velocity: `velocity = momentum_coefficient * previous_velocity + gradient`\n3. Update parameters: `parameter = parameter - learning_rate * velocity`\n4. Store velocity for the next iteration\n5. Clear gradients\n\nThe momentum coefficient (typically 0.9) controls how much previous gradients influence the current update. Higher values create more inertia, which helps with optimization stability but can make the optimizer slower to respond to changes in gradient direction.\n\n**SGD Data Structures:**\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| `parameters` | `List[Tensor]` | References to all trainable tensors in the model |\n| `learning_rate` | `float` | Step size multiplier for parameter updates |\n| `momentum` | `float` | Coefficient for velocity accumulation (0.0 to 1.0) |\n| `velocity_buffers` | `Dict[Tensor, Tensor]` | Stores momentum velocity for each parameter |\n| `weight_decay` | `float` | L2 regularization coefficient for parameter shrinkage |\n\n**SGD Methods:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `__init__` | `parameters, lr, momentum, weight_decay` | `None` | Initialize optimizer with parameter references |\n| `step` | `None` | `None` | Apply one optimization step to all parameters |\n| `zero_grad` | `None` | `None` | Reset all parameter gradients to None |\n| `state_dict` | `None` | `Dict` | Export optimizer state for checkpointing |\n| `load_state_dict` | `state_dict` | `None` | Restore optimizer state from checkpoint |\n\n> **Decision: Separate Gradient Clearing from Parameter Updates**\n> - **Context**: We need to decide whether `optimizer.step()` should automatically clear gradients or require explicit `zero_grad()` calls\n> - **Options Considered**: Auto-clear after updates vs. manual clearing vs. clear before updates\n> - **Decision**: Manual clearing with separate `zero_grad()` method\n> - **Rationale**: Explicit gradient clearing provides more control for advanced techniques like gradient accumulation across multiple batches, and matches PyTorch conventions for familiarity\n> - **Consequences**: Requires developers to remember to call `zero_grad()`, but enables gradient accumulation and debugging workflows where gradients need inspection\n\n### Adam Optimizer\n\nThe Adam optimizer (Adaptive Moment Estimation) represents one of the most significant advances in neural network optimization. Adam combines the benefits of momentum with adaptive learning rates that automatically adjust based on the historical behavior of each parameter's gradients. This makes Adam particularly effective for training deep networks and handling sparse gradients.\n\nAdam maintains two types of moving averages for each parameter: **first moments** (exponentially weighted average of gradients) and **second moments** (exponentially weighted average of squared gradients). The first moment provides momentum-like behavior, while the second moment enables adaptive learning rates that are larger for parameters with consistently small gradients and smaller for parameters with large or variable gradients.\n\n**Adam's Key Innovations:**\n1. **Per-parameter adaptive learning rates** based on gradient magnitude history\n2. **Bias correction** that accounts for initialization bias in the moving averages\n3. **Numerical stability** through epsilon term preventing division by zero\n4. **Default hyperparameters** that work well across a wide range of problems\n\n**Adam Algorithm Steps:**\n1. Compute current gradients using backpropagation\n2. Update first moment estimate: `m = beta1 * m + (1 - beta1) * gradient`\n3. Update second moment estimate: `v = beta2 * v + (1 - beta2) * gradient²`\n4. Apply bias correction: `m_corrected = m / (1 - beta1^t)` and `v_corrected = v / (1 - beta2^t)`\n5. Update parameters: `parameter = parameter - learning_rate * m_corrected / (sqrt(v_corrected) + epsilon)`\n6. Increment time step counter `t`\n7. Clear gradients\n\nThe **bias correction** step is crucial and frequently misunderstood. Because the first and second moment estimates are initialized to zero, they are biased toward zero during the early steps of training. The correction terms `(1 - beta1^t)` and `(1 - beta2^t)` compensate for this bias, where `t` is the current time step. As training progresses and `t` increases, these correction terms approach 1.0 and have minimal effect.\n\n**Adam Data Structures:**\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| `parameters` | `List[Tensor]` | References to all trainable tensors in the model |\n| `learning_rate` | `float` | Base learning rate before adaptive scaling |\n| `beta1` | `float` | Exponential decay rate for first moment estimates (default: 0.9) |\n| `beta2` | `float` | Exponential decay rate for second moment estimates (default: 0.999) |\n| `epsilon` | `float` | Small constant for numerical stability (default: 1e-8) |\n| `weight_decay` | `float` | L2 regularization coefficient |\n| `first_moments` | `Dict[Tensor, Tensor]` | First moment estimates for each parameter |\n| `second_moments` | `Dict[Tensor, Tensor]` | Second moment estimates for each parameter |\n| `step_count` | `int` | Current optimization step for bias correction |\n\n**Adam Methods:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `__init__` | `parameters, lr, betas, eps, weight_decay` | `None` | Initialize Adam with hyperparameters |\n| `step` | `None` | `None` | Apply one Adam optimization step with bias correction |\n| `zero_grad` | `None` | `None` | Reset all parameter gradients to None |\n| `state_dict` | `None` | `Dict` | Export optimizer state including step count and moments |\n| `load_state_dict` | `state_dict` | `None` | Restore optimizer state from checkpoint |\n\n> **Decision: Store Per-Parameter State vs. Global State**\n> - **Context**: Adam needs to track first and second moments for each parameter separately\n> - **Options Considered**: Dictionary mapping tensors to state vs. global state arrays vs. parameter-attached state\n> - **Decision**: Dictionary mapping parameter tensors to their moment estimates\n> - **Rationale**: Provides natural association between parameters and their optimization state, handles variable parameter counts gracefully, and matches established optimizer patterns\n> - **Consequences**: Requires careful handling of tensor identity for dictionary keys, but provides clear state organization and supports dynamic parameter addition/removal\n\n### Training Loop Architecture\n\nThe training loop represents the high-level orchestration that coordinates all components of our neural network framework. It manages the flow of data through the network, coordinates forward and backward passes, applies optimization steps, and tracks training progress. A well-designed training loop abstracts away the mechanical details while providing flexibility for different training strategies.\n\nThink of the training loop as the conductor of an orchestra, ensuring that each section (data loading, forward pass, loss computation, backpropagation, optimization) plays their part at exactly the right time. The conductor doesn't play any instruments directly, but coordinates the entire performance to create a coherent result.\n\n**Core Training Loop Components:**\n1. **Data Loading**: Batching samples from the dataset and shuffling for each epoch\n2. **Forward Pass**: Computing model predictions for the current batch\n3. **Loss Computation**: Measuring prediction quality against ground truth labels\n4. **Backward Pass**: Computing gradients via automatic differentiation\n5. **Parameter Update**: Applying optimizer step to improve parameters\n6. **Progress Tracking**: Logging metrics and monitoring convergence\n\n**Training Loop Data Flow:**\n1. Load a mini-batch of samples and labels from the training dataset\n2. Forward propagate samples through the network to generate predictions\n3. Compute loss by comparing predictions against ground truth labels\n4. Clear previous gradients from all parameters using `optimizer.zero_grad()`\n5. Backward propagate loss through the network to compute parameter gradients\n6. Apply optimizer step to update parameters based on computed gradients\n7. Record training metrics (loss, accuracy) for monitoring progress\n8. Repeat for next batch until epoch complete, then shuffle data for next epoch\n\n**Training State Management:**\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| `model` | `Module` | Neural network with trainable parameters |\n| `optimizer` | `Optimizer` | Parameter update strategy (SGD, Adam, etc.) |\n| `loss_function` | `Loss` | Differentiable loss computation |\n| `data_loader` | `DataLoader` | Batched, shuffled dataset iteration |\n| `current_epoch` | `int` | Current training epoch number |\n| `global_step` | `int` | Total number of optimization steps taken |\n| `best_loss` | `float` | Best validation loss seen during training |\n| `scheduler` | `LRScheduler` | Learning rate decay strategy |\n\n**Training Methods:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `train_epoch` | `model, data_loader, optimizer, loss_fn` | `float` | Train for one epoch, return average loss |\n| `validate` | `model, data_loader, loss_fn` | `float` | Evaluate model on validation set |\n| `training_step` | `batch, labels` | `Tensor` | Single forward/backward/update cycle |\n| `save_checkpoint` | `path, epoch, model, optimizer` | `None` | Save training state to disk |\n| `load_checkpoint` | `path` | `Dict` | Restore training state from disk |\n\nThe training loop also handles **mode switching** between training and evaluation. During training, modules like dropout and batch normalization behave differently than during evaluation. The `model.train()` and `model.eval()` methods recursively set the mode for all modules in the network.\n\n**Batch Processing Strategy:**\nMini-batch training provides a crucial balance between gradient accuracy and computational efficiency. Processing individual samples would provide the most frequent parameter updates but would be computationally inefficient and provide noisy gradient estimates. Processing the entire dataset would provide the most accurate gradients but would be computationally prohibitive and could lead to poor generalization.\n\nMini-batches typically contain 32-256 samples, chosen based on available memory and convergence characteristics. The batch size affects both training speed and final model quality—larger batches provide more stable gradients but may converge to worse local minima.\n\n> **Decision: Separate Training Step from Training Loop**\n> - **Context**: We need to decide how to structure the relationship between individual training steps and the overall training loop\n> - **Options Considered**: Monolithic training loop vs. separate training step function vs. trainer class with methods\n> - **Decision**: Separate `training_step` function called by higher-level training loop\n> - **Rationale**: Enables testing individual training steps in isolation, supports different training strategies (standard training, adversarial training, etc.), and provides clear separation between batch processing and epoch management\n> - **Consequences**: Requires passing state between functions, but improves modularity and testing capabilities\n\n### Loss Function Implementation\n\nLoss functions serve as the bridge between model predictions and the optimization process by providing a differentiable measure of prediction quality. They must compute both the scalar loss value used for monitoring progress and the gradients needed for backpropagation. The design of loss functions significantly impacts training dynamics, convergence speed, and final model performance.\n\nA loss function transforms the model's predictions and ground truth labels into a scalar value that quantifies how \"wrong\" the predictions are. This scalar serves as the starting point for backpropagation—we compute gradients of this loss with respect to all model parameters, then use those gradients to improve the parameters via optimization.\n\n**Loss Function Requirements:**\n1. **Differentiability**: Must provide gradients for all inputs to enable backpropagation\n2. **Numerical Stability**: Should avoid operations that cause overflow, underflow, or NaN values\n3. **Appropriate Scale**: Loss magnitude should be reasonable for the optimizer's learning rate\n4. **Task Alignment**: Loss should accurately reflect the quality metric we care about\n5. **Batch Processing**: Must handle mini-batches efficiently with proper reduction\n\n**Mean Squared Error (MSE) Loss:**\nMSE loss is primarily used for regression tasks where we predict continuous values. It computes the average squared difference between predictions and targets, which penalizes large errors more heavily than small errors due to the squaring operation.\n\n**MSE Computation Steps:**\n1. Compute elementwise squared differences: `squared_diff = (predictions - targets)²`\n2. Sum across all dimensions to get per-sample losses\n3. Average across the batch dimension: `loss = mean(sum(squared_diff, dims=-1))`\n4. For backpropagation, gradient w.r.t. predictions: `2 * (predictions - targets) / batch_size`\n\n**Cross-Entropy Loss:**\nCross-entropy loss is the standard choice for classification tasks. It measures the difference between the predicted probability distribution and the true distribution (typically one-hot encoded labels). Cross-entropy loss encourages the model to assign high probability to the correct class while keeping probabilities for incorrect classes low.\n\n**Cross-Entropy Computation Steps:**\n1. Apply softmax to convert logits to probabilities: `probabilities = softmax(logits)`\n2. Compute negative log-likelihood: `loss = -sum(targets * log(probabilities + epsilon))`\n3. Average across batch dimension\n4. For backpropagation, gradient combines softmax and cross-entropy derivatives\n\nThe epsilon term in cross-entropy prevents taking the logarithm of zero, which would result in infinite loss. However, a more numerically stable approach combines softmax and cross-entropy computation to avoid intermediate probability values that might underflow.\n\n**Loss Function Data Structures:**\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| `reduction` | `str` | How to reduce batch dimension: 'mean', 'sum', or 'none' |\n| `ignore_index` | `int` | Label value to ignore in loss computation |\n| `label_smoothing` | `float` | Smoothing factor to prevent overconfident predictions |\n| `class_weights` | `Tensor` | Per-class weights for handling imbalanced datasets |\n\n**Loss Function Methods:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `forward` | `predictions, targets` | `Tensor` | Compute loss value with gradient tracking |\n| `__call__` | `predictions, targets` | `Tensor` | Convenience method that calls forward |\n| `backward` | `grad_output` | `Tuple[Tensor, ...]` | Compute gradients w.r.t. inputs |\n\n**Numerical Stability Considerations:**\nLoss functions must handle edge cases that commonly occur during training. For cross-entropy loss, predicted probabilities near zero cause log(0) which results in infinite loss. For MSE loss, very large prediction errors can cause gradient explosion. Proper implementation includes clipping, epsilon terms, and numerically stable formulations.\n\n**Loss Reduction Strategies:**\nLoss functions typically compute per-sample losses, then reduce across the batch dimension. The reduction strategy affects gradient magnitudes and training dynamics:\n- **Mean reduction**: Gradients are independent of batch size, providing consistent training dynamics\n- **Sum reduction**: Gradients scale with batch size, requiring learning rate adjustment\n- **No reduction**: Returns per-sample losses, useful for custom weighting or analysis\n\n> **Decision: Numerically Stable Cross-Entropy Implementation**\n> - **Context**: Standard cross-entropy computation can suffer from numerical instability when probabilities approach zero\n> - **Options Considered**: Separate softmax + cross-entropy vs. fused log-softmax + NLL vs. epsilon clipping\n> - **Decision**: Fused log-softmax computation that combines operations for numerical stability\n> - **Rationale**: Avoids intermediate probability computation that can underflow, provides more accurate gradients, and matches behavior of production frameworks\n> - **Consequences**: Requires more complex implementation but provides better numerical stability and gradient accuracy\n\n### Training Architecture Decisions\n\nThe design of the training system involves several critical architectural decisions that affect flexibility, performance, and maintainability. These decisions establish patterns that will influence how users interact with the framework and how easily it can be extended with new features.\n\n> **Decision: Optimizer Parameter Registration Strategy**\n> - **Context**: Optimizers need references to all trainable parameters, but modules can be nested arbitrarily deep and parameters can be added dynamically\n> - **Options Considered**: Pass parameters at optimizer creation vs. automatic parameter discovery vs. manual registration per parameter\n> - **Decision**: Pass parameter iterator from `model.parameters()` at optimizer creation\n> - **Rationale**: Leverages the module system's recursive parameter collection, works with any module hierarchy, and follows established patterns from PyTorch\n> - **Consequences**: Requires rebuilding optimizer if model structure changes, but provides clean separation between model definition and optimization strategy\n\n> **Decision: Learning Rate Scheduling Architecture**\n> - **Context**: Learning rates typically need to decay during training based on epochs, steps, or validation metrics\n> - **Options Considered**: Built into optimizers vs. separate scheduler objects vs. callback system\n> - **Decision**: Separate scheduler objects that modify optimizer learning rates\n> - **Rationale**: Allows combining any scheduler with any optimizer, enables complex scheduling strategies, and separates concerns cleanly\n> - **Consequences**: Requires coordinating scheduler.step() calls with training loop, but provides maximum flexibility\n\n> **Decision: Training Loop State Management**\n> - **Context**: Training requires coordinating state across multiple components (model, optimizer, scheduler, metrics)\n> - **Options Considered**: Monolithic trainer class vs. functional training loop vs. stateful training manager\n> - **Decision**: Functional training utilities with explicit state passing\n> - **Rationale**: Provides maximum flexibility for custom training loops, easier to test individual components, and avoids complex state management\n> - **Consequences**: Requires more manual coordination but enables advanced training strategies and easier debugging\n\n**Optimizer State Persistence:**\nOptimizers maintain internal state (momentum buffers, Adam moments) that should be saved and restored during checkpointing. This state is often larger than the model parameters themselves, especially for optimizers like Adam that maintain multiple state tensors per parameter.\n\n| State Component | Description | Persistence Strategy |\n|-----------------|-------------|---------------------|\n| Parameter References | Pointers to trainable tensors | Not saved (reconstructed from model) |\n| Momentum Buffers | Velocity accumulation for SGD | Saved as tensor dictionaries |\n| First/Second Moments | Adam's gradient statistics | Saved with parameter tensor keys |\n| Step Counters | For bias correction and scheduling | Saved as scalar values |\n| Hyperparameters | Learning rate, beta values | Saved for reconstruction |\n\n**Learning Rate Scheduling Strategies:**\n\n| Schedule Type | Description | Use Cases |\n|---------------|-------------|-----------|\n| Step Decay | Multiply by factor every N epochs | Simple baseline, well-understood |\n| Exponential Decay | Continuous exponential reduction | Smooth decay, theoretical justification |\n| Cosine Annealing | Cosine curve with restarts | Modern deep learning, cyclical training |\n| Reduce on Plateau | Decay when validation loss stagnates | Adaptive to training progress |\n| Warmup + Decay | Linear increase then exponential decay | Large batch training, transformer models |\n\n### Common Training Pitfalls\n\nTraining neural networks involves numerous subtle implementation details that can silently break the optimization process. These pitfalls often manifest as slow convergence, poor final performance, or training instability rather than obvious errors. Understanding and avoiding these common mistakes is crucial for successful framework implementation.\n\n⚠️ **Pitfall: Incorrect Adam Bias Correction**\nMany implementations incorrectly apply bias correction by using the current step count for all parameters, rather than tracking separate step counts per parameter. This becomes critical when parameters are added or removed during training, or when using different optimizers for different parameter groups.\n\n**Why it's wrong**: Adam's bias correction terms `(1 - beta1^t)` and `(1 - beta2^t)` compensate for the initialization bias in moment estimates. Using a global step counter means parameters added later in training receive incorrect bias correction, leading to poor optimization behavior.\n\n**How to fix**: Track step counts per parameter or per parameter group, not globally. Initialize step count to 1 when parameters are first added to the optimizer.\n\n⚠️ **Pitfall: Learning Rate Scale Mismatch**\nDifferent optimizers require different learning rate scales due to their internal mechanics. A learning rate that works well for SGD will typically be too large for Adam, and vice versa. Additionally, batch size changes require learning rate adjustments for consistent training dynamics.\n\n**Why it's wrong**: Adam's adaptive scaling effectively increases the effective learning rate compared to SGD, especially early in training. Using SGD learning rates with Adam can cause unstable training or poor convergence.\n\n**How to fix**: Use established learning rate ranges for each optimizer (SGD: 0.1-0.01, Adam: 0.001-0.0001). When changing batch sizes, scale learning rate proportionally for SGD but consider keeping it constant for Adam.\n\n⚠️ **Pitfall: Gradient Accumulation Errors**\nWhen implementing gradient accumulation across multiple batches, developers often forget to scale gradients by the accumulation factor, leading to effectively larger learning rates than intended.\n\n**Why it's wrong**: Accumulating gradients from multiple batches without scaling creates gradient magnitudes equivalent to using a much larger batch size, which changes the effective learning rate and can destabilize training.\n\n**How to fix**: Scale accumulated gradients by the number of accumulation steps before applying the optimizer step, or scale the learning rate by the same factor.\n\n⚠️ **Pitfall: Data Shuffling Inconsistencies**\nForgetting to shuffle training data between epochs, or shuffling validation data, can lead to misleading training dynamics and evaluation metrics.\n\n**Why it's wrong**: Without shuffling, the model sees samples in the same order each epoch, which can lead to overfitting to the sample sequence rather than learning generalizable patterns. Shuffling validation data makes it harder to track consistent progress.\n\n**How to fix**: Always shuffle training data at the start of each epoch, but never shuffle validation or test data. Use fixed random seeds for validation splits to ensure reproducible evaluation.\n\n⚠️ **Pitfall: Loss Scaling Issues**\nLoss functions that aren't properly scaled for the batch size or output dimensions can lead to gradient magnitudes that are too large or too small for the optimizer's learning rate.\n\n**Why it's wrong**: Very large losses (e.g., summing instead of averaging across batch dimensions) create large gradients that require smaller learning rates. Very small losses create gradients that vanish and prevent learning.\n\n**How to fix**: Use mean reduction for loss functions to maintain consistent gradient scales across different batch sizes. Monitor gradient norms during training to ensure they're in reasonable ranges (typically 1e-3 to 1e1).\n\n⚠️ **Pitfall: Optimizer State Corruption**\nModifying parameters outside of the optimizer step (e.g., manual weight clipping or re-initialization) can corrupt optimizer state like momentum buffers or Adam moments, leading to unexpected optimization behavior.\n\n**Why it's wrong**: Optimizers assume they have exclusive control over parameter updates. External modifications break assumptions about gradient history and can cause momentum or adaptive learning rates to become misaligned with actual parameter values.\n\n**How to fix**: Perform parameter modifications through the optimizer when possible, or explicitly reset optimizer state after manual parameter changes using `optimizer.state_dict()` manipulation.\n\n### Implementation Guidance\n\nThis implementation guidance provides the concrete code infrastructure needed to build optimization components. The optimizer implementations focus on educational clarity while maintaining the essential features needed for training neural networks effectively.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Optimizer Base | Single class with virtual methods | Abstract base with plugin architecture |\n| State Storage | Python dictionaries with tensor keys | Memory-mapped state for large models |\n| Learning Rate Scheduling | Function-based schedules | Class-based schedulers with state |\n| Loss Functions | Separate forward/backward methods | Autograd-integrated implementations |\n| Training Loop | Simple function with explicit steps | Iterator-based training framework |\n\n**Recommended File Structure:**\n```\nneural_framework/\n  optimizers/\n    __init__.py              ← optimizer exports\n    base.py                  ← abstract optimizer base class\n    sgd.py                   ← SGD with momentum implementation\n    adam.py                  ← Adam optimizer implementation\n    lr_scheduler.py          ← learning rate scheduling utilities\n  losses/\n    __init__.py              ← loss function exports\n    mse.py                   ← mean squared error loss\n    cross_entropy.py         ← cross-entropy loss with softmax\n  training/\n    __init__.py              ← training utilities exports\n    trainer.py               ← training loop implementation\n    metrics.py               ← training metric computation\n    checkpoints.py           ← model and optimizer state saving\n```\n\n**Infrastructure Starter Code - Base Optimizer:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Iterator\nimport numpy as np\nfrom neural_framework.tensor import Tensor\n\nclass Optimizer(ABC):\n    \"\"\"Base class for all optimizers providing common functionality.\"\"\"\n    \n    def __init__(self, parameters: Iterator[Tensor]):\n        \"\"\"Initialize optimizer with parameters to optimize.\n        \n        Args:\n            parameters: Iterator of tensors with requires_grad=True\n        \"\"\"\n        self.param_groups = []\n        # Convert parameters to list and validate\n        param_list = list(parameters)\n        if len(param_list) == 0:\n            raise ValueError(\"Optimizer got empty parameter list\")\n        \n        # Validate all parameters require gradients\n        for param in param_list:\n            if not param.requires_grad:\n                raise ValueError(\"All parameters must have requires_grad=True\")\n        \n        # Store as single parameter group (can extend later for multiple groups)\n        self.param_groups.append({\n            'params': param_list,\n            'lr': 0.01,  # default learning rate\n        })\n        \n        # State dict for optimizer-specific state\n        self.state = {}\n    \n    @abstractmethod\n    def step(self) -> None:\n        \"\"\"Perform single optimization step.\"\"\"\n        pass\n    \n    def zero_grad(self) -> None:\n        \"\"\"Clear gradients for all parameters.\"\"\"\n        for group in self.param_groups:\n            for param in group['params']:\n                if param.grad is not None:\n                    param.grad = None\n    \n    def state_dict(self) -> Dict[str, Any]:\n        \"\"\"Return optimizer state for checkpointing.\"\"\"\n        return {\n            'state': self.state,\n            'param_groups': self.param_groups\n        }\n    \n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        \"\"\"Load optimizer state from checkpoint.\"\"\"\n        self.state = state_dict['state']\n        self.param_groups = state_dict['param_groups']\n\nclass LRScheduler(ABC):\n    \"\"\"Base class for learning rate schedulers.\"\"\"\n    \n    def __init__(self, optimizer: Optimizer):\n        self.optimizer = optimizer\n        self.last_epoch = 0\n    \n    @abstractmethod\n    def get_lr(self) -> List[float]:\n        \"\"\"Compute learning rate for current epoch.\"\"\"\n        pass\n    \n    def step(self, epoch: Optional[int] = None) -> None:\n        \"\"\"Update learning rates for all parameter groups.\"\"\"\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        \n        self.last_epoch = epoch\n        learning_rates = self.get_lr()\n        \n        for param_group, lr in zip(self.optimizer.param_groups, learning_rates):\n            param_group['lr'] = lr\n```\n\n**Infrastructure Starter Code - Training Utilities:**\n\n```python\nimport numpy as np\nfrom typing import Tuple, Iterator, Dict, Any\nfrom neural_framework.tensor import Tensor\nfrom neural_framework.module import Module\nfrom neural_framework.losses import Loss\nfrom neural_framework.optimizers import Optimizer\n\nclass DataLoader:\n    \"\"\"Simple data loader with batching and shuffling.\"\"\"\n    \n    def __init__(self, dataset: Tuple[np.ndarray, np.ndarray], \n                 batch_size: int, shuffle: bool = True):\n        \"\"\"Initialize data loader.\n        \n        Args:\n            dataset: Tuple of (samples, labels) as numpy arrays\n            batch_size: Number of samples per batch\n            shuffle: Whether to shuffle data each epoch\n        \"\"\"\n        self.samples, self.labels = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        if len(self.samples) != len(self.labels):\n            raise ValueError(\"Samples and labels must have same length\")\n    \n    def __iter__(self) -> Iterator[Tuple[Tensor, Tensor]]:\n        \"\"\"Iterate over batches for one epoch.\"\"\"\n        n_samples = len(self.samples)\n        indices = np.arange(n_samples)\n        \n        if self.shuffle:\n            np.random.shuffle(indices)\n        \n        for start_idx in range(0, n_samples, self.batch_size):\n            end_idx = min(start_idx + self.batch_size, n_samples)\n            batch_indices = indices[start_idx:end_idx]\n            \n            batch_samples = Tensor(self.samples[batch_indices], requires_grad=False)\n            batch_labels = Tensor(self.labels[batch_indices], requires_grad=False)\n            \n            yield batch_samples, batch_labels\n\ndef save_checkpoint(path: str, epoch: int, model: Module, \n                   optimizer: Optimizer, loss: float) -> None:\n    \"\"\"Save training checkpoint to disk.\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }\n    # In real implementation, use pickle or torch.save\n    # For now, just print what would be saved\n    print(f\"Would save checkpoint to {path} with keys: {checkpoint.keys()}\")\n\ndef load_checkpoint(path: str) -> Dict[str, Any]:\n    \"\"\"Load training checkpoint from disk.\"\"\"\n    # In real implementation, use pickle or torch.load\n    # For now, return empty dict\n    print(f\"Would load checkpoint from {path}\")\n    return {}\n```\n\n**Core Logic Skeleton - SGD Optimizer:**\n\n```python\nfrom neural_framework.optimizers.base import Optimizer\nfrom neural_framework.tensor import Tensor\nfrom typing import Iterator\n\nclass SGD(Optimizer):\n    \"\"\"Stochastic Gradient Descent optimizer with momentum support.\"\"\"\n    \n    def __init__(self, parameters: Iterator[Tensor], lr: float = 0.01,\n                 momentum: float = 0.0, weight_decay: float = 0.0):\n        \"\"\"Initialize SGD optimizer.\n        \n        Args:\n            parameters: Model parameters to optimize\n            lr: Learning rate for parameter updates\n            momentum: Momentum coefficient for velocity accumulation\n            weight_decay: L2 regularization coefficient\n        \"\"\"\n        super().__init__(parameters)\n        \n        # TODO 1: Store hyperparameters in param_groups\n        # Update self.param_groups[0] with lr, momentum, weight_decay\n        \n        # TODO 2: Initialize momentum buffers if momentum > 0\n        # For each parameter, create velocity buffer in self.state\n        # Use parameter id as key: self.state[id(param)] = {'momentum_buffer': zeros_like_param}\n    \n    def step(self) -> None:\n        \"\"\"Apply SGD update step to all parameters.\"\"\"\n        with_grad_parameters = []\n        \n        # TODO 3: Collect parameters that have gradients\n        # Iterate through param_groups and collect params where grad is not None\n        # Skip parameters without gradients (they don't need updates)\n        \n        if len(with_grad_parameters) == 0:\n            return  # No gradients to process\n        \n        for param in with_grad_parameters:\n            # TODO 4: Extract hyperparameters for this parameter\n            # Get lr, momentum, weight_decay from param_groups\n            \n            # TODO 5: Apply weight decay if specified\n            # Add weight_decay * param.data to gradient\n            # This implements L2 regularization: grad = grad + weight_decay * param\n            \n            # TODO 6: Apply momentum update if momentum > 0\n            # If momentum == 0: param.data = param.data - lr * grad\n            # If momentum > 0: \n            #   velocity = momentum * old_velocity + grad\n            #   param.data = param.data - lr * velocity\n            #   Store updated velocity in self.state[id(param)]['momentum_buffer']\n            \n            # TODO 7: Apply parameter update\n            # Subtract lr * effective_grad from param.data\n            # effective_grad is either raw gradient or momentum-modified gradient\n```\n\n**Core Logic Skeleton - Adam Optimizer:**\n\n```python\nfrom neural_framework.optimizers.base import Optimizer\nfrom neural_framework.tensor import Tensor\nimport numpy as np\nfrom typing import Iterator, Tuple\n\nclass Adam(Optimizer):\n    \"\"\"Adam optimizer with adaptive learning rates and bias correction.\"\"\"\n    \n    def __init__(self, parameters: Iterator[Tensor], lr: float = 0.001,\n                 betas: Tuple[float, float] = (0.9, 0.999), \n                 eps: float = 1e-8, weight_decay: float = 0.0):\n        \"\"\"Initialize Adam optimizer.\n        \n        Args:\n            parameters: Model parameters to optimize\n            lr: Learning rate\n            betas: Coefficients for moment estimates (beta1, beta2)\n            eps: Small constant for numerical stability\n            weight_decay: L2 regularization coefficient\n        \"\"\"\n        super().__init__(parameters)\n        \n        # TODO 1: Store hyperparameters in param_groups\n        # Add lr, betas, eps, weight_decay to param_groups[0]\n        \n        # TODO 2: Initialize state for each parameter\n        # For each param, create state dict with:\n        # - 'step': 0 (for bias correction)\n        # - 'exp_avg': zeros like param (first moment)\n        # - 'exp_avg_sq': zeros like param (second moment)\n    \n    def step(self) -> None:\n        \"\"\"Apply Adam optimization step with bias correction.\"\"\"\n        \n        for group in self.param_groups:\n            # TODO 3: Extract hyperparameters\n            # Get lr, beta1, beta2, eps, weight_decay from group\n            \n            for param in group['params']:\n                if param.grad is None:\n                    continue\n                \n                # TODO 4: Get parameter state and increment step count\n                # Get state dict for this parameter\n                # Increment step count: state['step'] += 1\n                \n                # TODO 5: Apply weight decay to gradient if specified\n                # grad = grad + weight_decay * param.data\n                \n                # TODO 6: Update first moment estimate (exponential moving average of gradients)\n                # exp_avg = beta1 * exp_avg + (1 - beta1) * grad\n                # Store updated exp_avg back to state\n                \n                # TODO 7: Update second moment estimate (exponential moving average of squared gradients)\n                # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * grad²\n                # Store updated exp_avg_sq back to state\n                \n                # TODO 8: Apply bias correction\n                # bias_correction1 = 1 - beta1 ** step\n                # bias_correction2 = 1 - beta2 ** step\n                # corrected_exp_avg = exp_avg / bias_correction1\n                # corrected_exp_avg_sq = exp_avg_sq / bias_correction2\n                \n                # TODO 9: Compute parameter update\n                # denominator = sqrt(corrected_exp_avg_sq) + eps\n                # step_size = lr / denominator\n                # param.data = param.data - step_size * corrected_exp_avg\n```\n\n**Core Logic Skeleton - Training Loop:**\n\n```python\nfrom neural_framework.module import Module\nfrom neural_framework.optimizers import Optimizer\nfrom neural_framework.losses import Loss\nfrom typing import Tuple\n\ndef train_epoch(model: Module, data_loader, optimizer: Optimizer, \n                loss_fn: Loss) -> float:\n    \"\"\"Train model for one epoch.\n    \n    Returns:\n        Average loss for the epoch\n    \"\"\"\n    # TODO 1: Set model to training mode\n    # Call model.train() to enable training-specific behavior\n    \n    total_loss = 0.0\n    num_batches = 0\n    \n    for batch_samples, batch_labels in data_loader:\n        # TODO 2: Clear gradients from previous step\n        # Call optimizer.zero_grad()\n        \n        # TODO 3: Forward pass - compute predictions\n        # predictions = model.forward(batch_samples)\n        # Ensure predictions is a Tensor with requires_grad=True\n        \n        # TODO 4: Compute loss\n        # loss = loss_fn(predictions, batch_labels)\n        # Accumulate loss value: total_loss += loss.data\n        \n        # TODO 5: Backward pass - compute gradients\n        # Call loss.backward() to compute gradients for all parameters\n        \n        # TODO 6: Update parameters\n        # Call optimizer.step() to apply parameter updates\n        \n        num_batches += 1\n    \n    # TODO 7: Return average loss for epoch\n    # return total_loss / num_batches if num_batches > 0 else 0.0\n\ndef validate(model: Module, data_loader, loss_fn: Loss) -> float:\n    \"\"\"Evaluate model on validation set.\"\"\"\n    # TODO 8: Set model to evaluation mode\n    # Call model.eval() to disable training-specific behavior (dropout, etc.)\n    \n    total_loss = 0.0\n    num_batches = 0\n    \n    # TODO 9: Disable gradient computation for efficiency\n    # In real implementation, use context manager to disable gradients\n    \n    for batch_samples, batch_labels in data_loader:\n        # TODO 10: Forward pass only (no backward pass needed)\n        # predictions = model.forward(batch_samples)\n        # loss = loss_fn(predictions, batch_labels)\n        # total_loss += loss.data\n        \n        num_batches += 1\n    \n    # TODO 11: Return average validation loss\n    # return total_loss / num_batches if num_batches > 0 else 0.0\n```\n\n**Milestone Checkpoint:**\nAfter implementing the optimizer and training components, verify the complete framework integration:\n\n1. **Basic Training Test**: Create a simple linear model, generate synthetic data, and train for a few epochs. Loss should decrease consistently.\n\n2. **Optimizer State Test**: Save and load optimizer state, verify that momentum/Adam buffers are preserved correctly.\n\n3. **Learning Rate Scheduling**: Implement step decay scheduler, verify learning rate changes at specified epochs.\n\n4. **Gradient Checking**: Compare optimizer updates with numerical gradients on a small model to verify correctness.\n\n5. **End-to-End Training**: Train a multi-layer network on a toy dataset (e.g., XOR problem), achieve convergence to demonstrate complete framework functionality.\n\nExpected behavior: Training loss should decrease smoothly, validation loss should track reasonably with training loss, and the trained model should make sensible predictions on test data. Monitor gradient norms to ensure they remain in reasonable ranges (1e-4 to 1e0 typically).\n\n![Training Sequence Diagram](./diagrams/training-sequence.svg)\n\n![Optimizer State Transitions](./diagrams/optimizer-states.svg)\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** All milestones - describes how components communicate during training, from tensor operations through gradient computation to parameter updates\n\nUnderstanding how data flows through a neural network framework during training is like watching a well-choreographed dance where multiple performers coordinate their movements in perfect harmony. Each component—tensors, modules, optimizers, and the automatic differentiation engine—has its role to play, and the beauty of the system emerges from their seamless coordination. This section dissects the intricate choreography that transforms input data into learned parameters through the interplay of forward passes, gradient computation, and optimization steps.\n\nThe training process orchestrates three distinct but interconnected data flows. During the **forward pass**, input data cascades through the neural network modules while the automatic differentiation engine silently constructs a computation graph that records every operation. The **backward pass** reverses this flow, sending gradients from the loss function back through the network using the recorded computation graph to compute exact derivatives. Finally, the **optimization step** uses these gradients to update model parameters, completing one iteration of the learning cycle.\n\n### Forward Pass Data Flow\n\nThink of the forward pass as a factory assembly line where raw materials (input data) move through a series of specialized stations (neural network modules), with each station transforming the materials and passing them to the next station. However, unlike a typical assembly line, this one has a quality inspector (the automatic differentiation engine) who meticulously records every transformation in a logbook (computation graph) so that if quality issues are discovered later, the inspector can trace back through the entire process to identify exactly which stations need adjustment.\n\nThe forward pass begins when input data enters the neural network as a `Tensor` object. This tensor carries not only the numerical data in its `data` field but also metadata that governs how it participates in gradient computation. The `requires_grad` flag indicates whether this tensor needs gradient tracking, while the `grad_fn` field will eventually point to the operation that created this tensor during computation.\n\n| Tensor Attribute | Role in Forward Pass | Example Value |\n|---|---|---|\n| `data` | Contains the actual numerical values flowing through the network | `np.array([[1.0, 2.0], [3.0, 4.0]])` |\n| `requires_grad` | Determines if this tensor participates in gradient computation | `True` for parameters, `False` for constants |\n| `grad_fn` | Points to the operation that created this tensor | `AddOperation` instance after addition |\n| `shape` | Defines tensor dimensions for broadcasting and validation | `(2, 2)` for 2x2 matrix |\n| `grad` | Accumulates gradients during backward pass (initially None) | `None` during forward pass |\n\nAs the input tensor flows through each module in the network, a sophisticated graph construction process occurs behind the scenes. Every operation that the tensor undergoes—whether element-wise addition in an activation function or matrix multiplication in a linear layer—creates an `Operation` node that records its inputs and maintains a reference to the function needed to compute gradients during backpropagation.\n\nThe module system orchestrates this data flow through a consistent interface. When a tensor enters a module's `forward` method, the module applies its transformation and returns a new tensor. Crucially, this new tensor has its `grad_fn` set to point to the operation that created it, establishing a link in the computation graph that the automatic differentiation engine will later traverse.\n\nConsider the data flow through a simple two-layer network with ReLU activation. The input tensor first enters the first `Linear` module, which performs matrix multiplication `y = Wx + b`. This operation creates a `MatMul` node in the computation graph with the input tensor and weight tensor as inputs. The resulting tensor then flows to the `ReLU` module, which applies the activation function element-wise, creating another operation node. This pattern continues through each module, building an increasingly complex computation graph.\n\n| Forward Pass Stage | Input | Operation Created | Output | Graph Node Connection |\n|---|---|---|---|---|\n| First Linear Layer | Input tensor `x` | `MatMul(x, weight1) + bias1` | Hidden tensor `h1` | `h1.grad_fn` → `AddOperation` |\n| ReLU Activation | Hidden tensor `h1` | `ReLU(h1)` | Activated tensor `a1` | `a1.grad_fn` → `ReLUOperation` |\n| Second Linear Layer | Activated tensor `a1` | `MatMul(a1, weight2) + bias2` | Output tensor `y` | `y.grad_fn` → `AddOperation` |\n| Loss Computation | Output `y`, Target `t` | `CrossEntropy(y, t)` | Loss scalar `L` | `L.grad_fn` → `CrossEntropyOperation` |\n\nThe computation graph construction follows a define-by-run approach where the graph structure emerges dynamically as operations execute. Each operation node maintains references to its input tensors, creating a directed acyclic graph that mirrors the forward computation. This graph serves as a blueprint for the backward pass, encoding not only what operations were performed but also the order in which gradients must be computed.\n\n> **Design Insight**: The forward pass serves double duty—it computes the network's predictions while simultaneously building the roadmap for gradient computation. This define-by-run approach provides flexibility for dynamic network architectures but requires careful memory management to prevent the computation graph from accumulating indefinitely.\n\n**Decision: Eager Graph Construction vs. Lazy Graph Construction**\n- **Context**: During forward pass, we need to decide when to build computation graph nodes\n- **Options Considered**: Build nodes immediately during operations (eager) vs. defer until backward pass needed (lazy) vs. static graph definition before execution\n- **Decision**: Eager graph construction during forward pass operations\n- **Rationale**: Eager construction provides immediate feedback for debugging, allows dynamic control flow, and aligns with define-by-run semantics. The overhead is manageable for educational frameworks.\n- **Consequences**: Enables dynamic networks and easy debugging but requires more memory and careful graph cleanup to prevent leaks.\n\n| Graph Construction Approach | Memory Usage | Debugging Ease | Dynamic Networks | Implementation Complexity |\n|---|---|---|---|---|\n| Eager (chosen) | Higher during forward pass | Excellent - immediate graph inspection | Full support | Moderate |\n| Lazy | Lower until backward needed | Difficult - no graph until backprop | Limited support | High |\n| Static | Lowest - fixed graph | Good - graph known upfront | No support | Low |\n\nThe forward pass also handles broadcasting automatically when tensors of different shapes interact. The `broadcast_shapes` function computes the result shape according to NumPy broadcasting rules, and the operation nodes record both the original input shapes and the broadcasted shapes. This information becomes crucial during the backward pass when gradients must be reduced back to their original parameter shapes.\n\nParameter tensors flow through the network differently than input data. When a `Linear` module's weight tensor participates in matrix multiplication, it becomes part of the computation graph just like any other tensor. However, parameters have `requires_grad=True` by default, ensuring they receive gradients during backpropagation. The module system's parameter registration mechanism ensures that optimizers can discover and update these parameter tensors after gradient computation.\n\n**Common Pitfalls in Forward Pass Data Flow**\n\n⚠️ **Pitfall: In-place Operations Breaking Gradient Flow**\nMany learners accidentally use in-place operations (like `+=` or `*=`) on tensors that require gradients. In-place operations modify the tensor's data directly without creating new operation nodes, which breaks the computation graph and prevents proper gradient computation. For example, writing `hidden_state += bias` instead of `hidden_state = hidden_state + bias` destroys the gradient flow because no `Add` operation node is created to record the computation.\n\n⚠️ **Pitfall: Forgetting to Set requires_grad on Parameters**\nParameter tensors must have `requires_grad=True` to participate in gradient computation. If a learner manually creates parameter tensors and forgets this flag, the parameters will be treated as constants during forward pass, no gradients will be computed for them, and the optimizer will have no gradients to use for updates. Always verify that `param.requires_grad` is True for all trainable parameters.\n\n⚠️ **Pitfall: Graph Memory Leaks from Retained References**\nThe computation graph holds references to all tensors involved in operations. If learners store references to intermediate tensors or operation nodes beyond their needed lifetime, the entire computation graph remains in memory. This is particularly problematic in training loops where each iteration creates a new graph. Always clear computation graphs after each backward pass by calling `loss.backward()` followed by `optimizer.zero_grad()`.\n\n### Backward Pass Coordination\n\nImagine the backward pass as a detective investigating a crime scene in reverse chronological order. Starting from the crime (the loss function), the detective traces back through each piece of evidence (tensor operations) to determine exactly how each suspect (parameter) contributed to the final outcome. The detective follows a strict protocol: always examine evidence in reverse order of occurrence, and for each piece of evidence, calculate precisely how much each suspect influenced the situation using forensic analysis (chain rule).\n\nThe backward pass transforms the computation graph built during forward pass into a gradient computation engine. This transformation requires careful coordination between multiple components: the topological sorting algorithm that determines the order of gradient computation, the chain rule application that computes local gradients at each operation node, and the gradient accumulation mechanism that handles cases where tensors are used multiple times in the computation.\n\nBackward pass initiation occurs when `loss.backward()` is called on the final tensor in the computation graph, typically the loss value. This call triggers a cascading process that computes gradients for every tensor in the graph that has `requires_grad=True`. The process begins by setting the loss tensor's gradient to a tensor of ones (representing the derivative of the loss with respect to itself), then traverses the computation graph in reverse topological order.\n\n| Backward Pass Component | Responsibility | Key Algorithm | Error Handling |\n|---|---|---|---|\n| Topological Sort | Determine gradient computation order | Depth-first traversal from loss tensor | Detect cycles, handle disconnected nodes |\n| Chain Rule Application | Compute local gradients at each node | Apply operation-specific backward functions | Handle numerical instability, zero gradients |\n| Gradient Accumulation | Sum gradients for multiply-used tensors | Add incoming gradients to existing values | Initialize gradients, handle None values |\n| Memory Management | Clean up computation graph after use | Clear references between nodes | Prevent memory leaks, handle circular refs |\n\nThe topological sort algorithm ensures that gradients are computed in the correct dependency order. Starting from the loss tensor, the algorithm performs a depth-first traversal of the computation graph, visiting each operation node and its input tensors. The sort produces an ordering where each tensor's gradient is computed only after all tensors that depend on it have had their gradients computed.\n\n**Decision: Recursive vs. Iterative Topological Sort**\n- **Context**: Need to traverse computation graph in correct order for gradient computation\n- **Options Considered**: Recursive depth-first search vs. iterative with explicit stack vs. breadth-first queue-based approach\n- **Decision**: Iterative depth-first search with explicit stack\n- **Rationale**: Recursive approaches can hit Python's recursion limit on deep networks. Iterative provides the same ordering guarantees while handling arbitrarily deep graphs. Stack-based maintains DFS semantics needed for proper dependency resolution.\n- **Consequences**: More complex implementation but handles deep networks reliably and provides better debugging capabilities through stack inspection.\n\nThe chain rule application at each operation node follows a precise protocol. When an operation node receives a gradient from its output (representing how the loss changes with respect to the node's output), it must compute how the loss changes with respect to each of its inputs. This computation uses the operation's backward function, which implements the mathematical derivative of the forward operation.\n\nConsider gradient flow through a matrix multiplication operation `C = matmul(A, B)`. When this operation receives gradient `grad_C` from downstream computations, it must compute gradients for both input matrices A and B. The backward function applies the chain rule: `grad_A = matmul(grad_C, B.transpose())` and `grad_B = matmul(A.transpose(), grad_C)`. These computed gradients then propagate to the operations that produced tensors A and B.\n\n| Operation Type | Forward Computation | Gradient Computation (Chain Rule) | Special Considerations |\n|---|---|---|---|\n| `Add(a, b)` | `result = a + b` | `grad_a = grad_result`, `grad_b = grad_result` | Handle broadcasting shapes |\n| `Multiply(a, b)` | `result = a * b` | `grad_a = grad_result * b`, `grad_b = grad_result * a` | Element-wise multiplication |\n| `MatMul(a, b)` | `result = a @ b` | `grad_a = grad_result @ b.T`, `grad_b = a.T @ grad_result` | Matrix dimension compatibility |\n| `ReLU(x)` | `result = max(0, x)` | `grad_x = grad_result * (x > 0)` | Zero gradient for negative inputs |\n\nGradient accumulation becomes critical when the same tensor appears multiple times in the computation graph. This situation arises commonly in neural networks—for example, when the same weight tensor is used in multiple operations or when control flow causes a tensor to be processed along multiple paths. The backward pass must sum all gradient contributions for such tensors rather than overwriting previous gradients.\n\nThe accumulation process follows a careful protocol. When a tensor receives its first gradient during backward pass, the framework initializes the tensor's `grad` field with this gradient. For subsequent gradients from other operations, the framework adds the new gradient to the existing accumulated value. This summation implements the mathematical requirement that the total derivative equals the sum of partial derivatives from all usage paths.\n\n> **Critical Insight**: Gradient accumulation is not just an optimization detail—it's a mathematical requirement. When a tensor contributes to the loss through multiple computational paths, the chain rule demands that we sum the gradients from all paths to get the correct total derivative.\n\nThe backward pass also handles broadcasting-related gradient adjustments. When forward pass operations used broadcasting to make tensors compatible for element-wise operations, the backward pass must \"unbroadcast\" the gradients to match the original tensor shapes. This process uses the `unbroadcast_gradient` function to sum gradients across the dimensions that were expanded during broadcasting.\n\n| Broadcasting Scenario | Forward Shape Change | Backward Gradient Adjustment | Implementation |\n|---|---|---|---|\n| Scalar + Vector | `(1,) + (5,)` → `(5,)` | Sum gradient across expanded dimension | `grad.sum(axis=0, keepdims=True)` |\n| Vector + Matrix | `(3,) + (4,3)` → `(4,3)` | Sum gradient across batch dimension | `grad.sum(axis=0)` |\n| Bias Addition | `(1,) + (64,10)` → `(64,10)` | Sum across both batch and feature dims | `grad.sum(axis=(0,1))` |\n\nMemory management during backward pass requires careful attention to prevent leaks and circular references. As gradients flow through the computation graph, operation nodes can be deallocated once their gradients have been computed and propagated. The framework implements this cleanup by clearing references between nodes as the backward pass progresses, allowing Python's garbage collector to reclaim memory.\n\n**Common Pitfalls in Backward Pass Coordination**\n\n⚠️ **Pitfall: Incorrect Topological Sort Leading to Wrong Gradient Order**\nThe most critical error in backward pass implementation is computing gradients in the wrong order. This happens when the topological sort algorithm has bugs or when the computation graph contains cycles (which should never happen in valid neural networks). Wrong ordering leads to gradients being computed using stale or incorrect values from downstream operations. To debug this, print the order of operations during backward pass and verify that each operation receives gradients only after all operations that depend on it have completed.\n\n⚠️ **Pitfall: Gradients Not Accumulated for Shared Tensors**\nWhen the same parameter tensor is used multiple times in a network (such as shared embeddings or recurrent weight sharing), learners often overwrite gradients instead of accumulating them. This leads to incorrect gradient values and poor training performance. The fix is to always check if `tensor.grad` is None before assignment: if None, assign the new gradient directly; if not None, add the new gradient to the existing value.\n\n⚠️ **Pitfall: Broadcasting Gradients Not Properly Unbroadcast**\nDuring forward pass, operations may broadcast tensors to compatible shapes, but during backward pass, gradients must be reduced back to the original tensor shapes. Learners often forget this step, leading to shape mismatches when trying to assign gradients to parameters. Always call `unbroadcast_gradient(grad, original_shape)` to ensure gradient shapes match parameter shapes before assignment.\n\n### Complete Training Step Sequence\n\nThe complete training step orchestrates the forward pass, backward pass, and optimization update into a coordinated sequence that transforms input data into learned parameters. Think of this sequence as a complete learning cycle, similar to how a human learns from a mistake: first, make a prediction (forward pass), then evaluate how wrong it was (loss computation), next understand exactly what led to the error (backward pass), and finally adjust behavior to avoid similar mistakes in the future (parameter update).\n\n![Complete Training Step Sequence](./diagrams/training-sequence.svg)\n\nThe training step sequence involves precise coordination between multiple components, each with specific responsibilities and timing requirements. The sequence must ensure data consistency, proper gradient computation, and correct parameter updates while managing memory efficiently and handling potential failures gracefully.\n\n| Training Step Phase | Primary Component | Key Operations | Success Criteria | Failure Handling |\n|---|---|---|---|---|\n| Data Loading | `DataLoader` | Load batch, apply transforms | Batch shape matches model input | Handle malformed data, missing files |\n| Forward Pass | `Module` hierarchy | Tensor operations, graph building | Output shape correct, no NaN values | Gradient clipping, numerical checks |\n| Loss Computation | Loss function | Compare predictions to targets | Single scalar loss value | Handle class imbalance, numerical stability |\n| Backward Pass | Autodiff engine | Gradient computation, accumulation | All parameters have gradients | Check gradient magnitudes, detect vanishing |\n| Parameter Update | `Optimizer` | Apply gradients to parameters | Parameters change by expected amount | Gradient clipping, learning rate adjustment |\n| Cleanup | Framework | Clear graphs, reset gradients | Memory usage returns to baseline | Force garbage collection if needed |\n\nThe sequence begins with data preparation where the `DataLoader` provides a mini-batch of training examples. The data loader handles shuffling, batching, and any necessary preprocessing to ensure the input data is in the correct format for the model. This phase also converts raw data into `Tensor` objects with appropriate device placement and dtype settings.\n\n**Step-by-step Training Sequence:**\n\n1. **Batch Preparation**: The `DataLoader` selects the next mini-batch from the shuffled training dataset, applies any configured data transformations, and converts the data into framework tensors with `requires_grad=False` (since input data doesn't need gradients).\n\n2. **Model Mode Setting**: The training loop calls `model.train()` to ensure all modules are in training mode, which affects the behavior of layers like dropout and batch normalization that behave differently during training versus inference.\n\n3. **Gradient Zeroing**: The optimizer's `zero_grad()` method clears any residual gradients from the previous training step by setting all parameter gradients to None, preventing accumulation across training steps.\n\n4. **Forward Pass Execution**: Input tensors flow through the model via `predictions = model(inputs)`, triggering the forward pass data flow described earlier while building the computation graph for automatic differentiation.\n\n5. **Loss Computation**: The loss function compares predictions to ground truth targets via `loss = loss_function(predictions, targets)`, producing a scalar tensor that represents the model's performance on this batch.\n\n6. **Backward Pass Initiation**: Calling `loss.backward()` initiates reverse-mode automatic differentiation, computing gradients for all parameters in the model by traversing the computation graph in reverse topological order.\n\n7. **Parameter Updates**: The optimizer's `step()` method applies the computed gradients to update model parameters according to the optimization algorithm (SGD, Adam, etc.), implementing one step of gradient-based learning.\n\n8. **Graph Cleanup**: The computation graph from this training step is deallocated, freeing memory for the next iteration. This happens automatically when the loss tensor goes out of scope, but can be forced by clearing tensor references.\n\nThe coordination between components during this sequence requires careful attention to state management and error propagation. Each phase depends on successful completion of previous phases, and failures must be handled gracefully to maintain training stability.\n\n**Decision: Synchronous vs. Asynchronous Training Step Execution**\n- **Context**: Training steps can potentially be pipelined or parallelized across multiple devices\n- **Options Considered**: Synchronous single-device execution vs. asynchronous multi-device vs. hybrid approach\n- **Decision**: Synchronous single-device execution for educational framework\n- **Rationale**: Synchronous execution is easier to debug, understand, and implement correctly. Educational frameworks prioritize clarity over performance optimization. Multi-device complexity would obscure the core learning objectives.\n- **Consequences**: Simpler implementation and debugging but limited scalability to large models or datasets. Future extensions could add asynchronous capabilities.\n\nMemory management throughout the training sequence follows a careful protocol to prevent memory leaks and ensure consistent memory usage across training steps. The computation graph built during forward pass must be completely deallocated after backward pass completion to prevent memory accumulation over many training iterations.\n\nError detection and handling occurs at multiple points in the training sequence. Numerical issues like NaN or infinite values can arise during forward pass, backward pass, or parameter updates. The framework implements checks at critical points to detect these issues early and provide meaningful error messages to help learners debug their implementations.\n\n| Error Type | Detection Point | Symptoms | Debugging Approach | Recovery Strategy |\n|---|---|---|---|---|\n| Shape Mismatch | Forward pass operations | Runtime exception during tensor operations | Print tensor shapes at each operation | Fix model architecture or input preprocessing |\n| Vanishing Gradients | After backward pass | All gradients near zero | Monitor gradient magnitudes | Adjust initialization, learning rate, or architecture |\n| Exploding Gradients | After backward pass | Very large gradient values | Check gradient norms | Implement gradient clipping |\n| NaN Values | Forward or backward pass | NaN in loss, gradients, or parameters | Print intermediate values | Check for division by zero, log of negative values |\n| Memory Leaks | Between training steps | Increasing memory usage | Monitor memory consumption over time | Ensure proper graph cleanup and reference clearing |\n\nThe training loop typically wraps the single training step in iteration logic that processes multiple batches per epoch and multiple epochs for complete training. This outer loop handles additional concerns like validation evaluation, checkpointing, and learning rate scheduling.\n\nLoss tracking and logging occur throughout the training sequence to provide visibility into training progress. The framework typically accumulates loss values across batches within an epoch and computes epoch-level statistics like average loss and accuracy metrics.\n\n**Common Pitfalls in Complete Training Step Sequence**\n\n⚠️ **Pitfall: Forgetting to Call optimizer.zero_grad()**\nOne of the most common mistakes is forgetting to clear gradients between training steps. PyTorch and similar frameworks accumulate gradients by default, so if `zero_grad()` is not called, gradients from previous steps will be added to gradients from the current step. This leads to incorrect gradient values that grow without bound and cause training instability. Always call `optimizer.zero_grad()` before the forward pass of each training step.\n\n⚠️ **Pitfall: Calling backward() Multiple Times on Same Loss**\nLearners sometimes accidentally call `loss.backward()` multiple times within the same training step, perhaps in different parts of the code or within conditional branches. This causes gradients to be computed and accumulated multiple times, leading to incorrect gradient magnitudes. Each loss tensor should have `backward()` called exactly once per training step.\n\n⚠️ **Pitfall: Not Setting Model to Training Mode**\nFailing to call `model.train()` before training and `model.eval()` before validation can cause subtle but serious issues. Layers like dropout and batch normalization behave differently in training versus evaluation mode. Without proper mode setting, the model may not learn effectively during training or may produce inconsistent results during evaluation.\n\n⚠️ **Pitfall: Gradient Explosion from Incorrect Learning Rates**\nUsing learning rates that are too large can cause gradients to explode, leading to NaN values in parameters after just a few training steps. This manifests as sudden spikes in loss values or NaN loss after a few iterations. Monitor gradient norms and implement gradient clipping as a safeguard, but also tune learning rates appropriately for the model and dataset size.\n\n![Gradient Backpropagation Flow](./diagrams/gradient-flow.svg)\n\nThe complete training step sequence represents the fundamental learning cycle in neural network training. Understanding this sequence and the coordination between its components is crucial for building reliable training loops and debugging issues that arise during model development. The sequence transforms raw data into learned knowledge through the precise orchestration of forward computation, gradient calculation, and parameter optimization.\n\n### Implementation Guidance\n\nThe coordination between components during training requires careful orchestration of state management, error handling, and resource cleanup. This implementation guidance provides concrete code structures and utilities to manage these complex interactions effectively.\n\n**A. Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Graph Traversal | Python list-based topological sort | NetworkX graph algorithms with cycle detection |\n| Memory Management | Manual reference clearing with weakref | Automatic graph cleanup with context managers |\n| Error Detection | Basic type checking and assertions | Comprehensive numerical stability monitoring |\n| Progress Tracking | Print statements with loss values | Structured logging with tensorboard integration |\n| Debugging Tools | Simple gradient printing utilities | Full computation graph visualization |\n\n**B. Recommended File Structure:**\n```\nneural_framework/\n  core/\n    __init__.py\n    training.py              ← Training coordination logic\n    data_flow.py            ← Forward/backward pass orchestration\n  autograd/\n    __init__.py\n    graph.py                ← Computation graph utilities\n    backward.py             ← Backward pass implementation\n  optimizers/\n    __init__.py\n    base.py                 ← Base optimizer with step coordination\n    sgd.py                  ← SGD implementation\n    adam.py                 ← Adam implementation\n  utils/\n    __init__.py\n    debugging.py            ← Training debugging utilities\n    memory.py               ← Memory management helpers\n  examples/\n    simple_training_loop.py ← Complete training example\n```\n\n**C. Infrastructure Starter Code (Complete):**\n\n```python\n# utils/debugging.py - Complete debugging utilities\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nimport weakref\nimport gc\n\nclass TrainingMonitor:\n    \"\"\"Complete monitoring system for training diagnostics.\"\"\"\n    \n    def __init__(self, check_frequency: int = 10):\n        self.check_frequency = check_frequency\n        self.step_count = 0\n        self.loss_history = []\n        self.gradient_norms = []\n        \n    def log_step(self, loss: float, gradients: Dict[str, 'Tensor']) -> None:\n        \"\"\"Log training step metrics with numerical stability checks.\"\"\"\n        self.step_count += 1\n        self.loss_history.append(loss)\n        \n        # Check for numerical issues\n        if np.isnan(loss) or np.isinf(loss):\n            raise ValueError(f\"Invalid loss value at step {self.step_count}: {loss}\")\n        \n        # Monitor gradient magnitudes\n        grad_norm = 0.0\n        for name, tensor in gradients.items():\n            if tensor.grad is not None:\n                norm = np.linalg.norm(tensor.grad.data)\n                grad_norm += norm * norm\n                \n        grad_norm = np.sqrt(grad_norm)\n        self.gradient_norms.append(grad_norm)\n        \n        if self.step_count % self.check_frequency == 0:\n            self._print_diagnostics()\n    \n    def _print_diagnostics(self) -> None:\n        \"\"\"Print comprehensive training diagnostics.\"\"\"\n        recent_loss = np.mean(self.loss_history[-self.check_frequency:])\n        recent_grad_norm = np.mean(self.gradient_norms[-self.check_frequency:])\n        \n        print(f\"Step {self.step_count}: Loss={recent_loss:.6f}, \"\n              f\"Grad Norm={recent_grad_norm:.6f}\")\n        \n        # Check for gradient issues\n        if recent_grad_norm < 1e-6:\n            print(\"WARNING: Very small gradients detected (vanishing gradients)\")\n        elif recent_grad_norm > 100:\n            print(\"WARNING: Very large gradients detected (exploding gradients)\")\n\nclass ComputationGraphTracker:\n    \"\"\"Track computation graph memory usage and provide cleanup utilities.\"\"\"\n    \n    def __init__(self):\n        self.active_graphs = weakref.WeakSet()\n        \n    def register_tensor(self, tensor: 'Tensor') -> None:\n        \"\"\"Register tensor for graph tracking.\"\"\"\n        if tensor.requires_grad:\n            self.active_graphs.add(tensor)\n    \n    def cleanup_graphs(self) -> int:\n        \"\"\"Force cleanup of computation graphs and return memory freed.\"\"\"\n        initial_objects = len(gc.get_objects())\n        \n        # Clear weak references\n        self.active_graphs.clear()\n        \n        # Force garbage collection\n        collected = gc.collect()\n        final_objects = len(gc.get_objects())\n        \n        objects_freed = initial_objects - final_objects\n        return objects_freed\n    \n    def get_memory_stats(self) -> Dict[str, int]:\n        \"\"\"Get current memory usage statistics.\"\"\"\n        return {\n            'active_graphs': len(self.active_graphs),\n            'total_objects': len(gc.get_objects()),\n            'tensor_objects': len([obj for obj in gc.get_objects() \n                                 if hasattr(obj, 'data') and hasattr(obj, 'grad')])\n        }\n\n# utils/memory.py - Memory management helpers\nclass GraphCleanupContext:\n    \"\"\"Context manager for automatic computation graph cleanup.\"\"\"\n    \n    def __init__(self, tensors: List['Tensor']):\n        self.tensors = tensors\n        \n    def __enter__(self):\n        return self\n        \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Clear computation graphs for all tracked tensors.\"\"\"\n        for tensor in self.tensors:\n            if hasattr(tensor, 'grad_fn'):\n                tensor.grad_fn = None\n            if hasattr(tensor, 'grad'):\n                tensor.grad = None\n        \n        # Force garbage collection\n        gc.collect()\n\ndef check_gradient_flow(model: 'Module', loss: 'Tensor') -> Dict[str, float]:\n    \"\"\"Check gradient flow through all model parameters.\"\"\"\n    gradient_info = {}\n    \n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = np.linalg.norm(param.grad.data)\n            gradient_info[name] = float(grad_norm)\n        else:\n            gradient_info[name] = 0.0\n            print(f\"WARNING: No gradient for parameter {name}\")\n    \n    return gradient_info\n```\n\n**D. Core Logic Skeleton Code:**\n\n```python\n# core/training.py - Training step coordination\nclass TrainingCoordinator:\n    \"\"\"Coordinates the complete training step sequence.\"\"\"\n    \n    def __init__(self, model: 'Module', optimizer: 'Optimizer', \n                 loss_function: 'Loss', monitor: Optional[TrainingMonitor] = None):\n        self.model = model\n        self.optimizer = optimizer\n        self.loss_function = loss_function\n        self.monitor = monitor or TrainingMonitor()\n    \n    def training_step(self, inputs: 'Tensor', targets: 'Tensor') -> Dict[str, float]:\n        \"\"\"Execute complete training step with full coordination.\n        \n        Returns:\n            Dictionary with loss value and training metrics\n        \"\"\"\n        # TODO 1: Verify model is in training mode - call model.train()\n        # TODO 2: Clear gradients from previous step - call optimizer.zero_grad()\n        # TODO 3: Execute forward pass - predictions = model(inputs)\n        # TODO 4: Compute loss - loss = loss_function(predictions, targets)\n        # TODO 5: Check for numerical issues - verify loss is finite\n        # TODO 6: Execute backward pass - loss.backward()\n        # TODO 7: Check gradient flow - ensure all parameters have gradients\n        # TODO 8: Apply parameter updates - optimizer.step()\n        # TODO 9: Log training metrics - monitor.log_step()\n        # TODO 10: Clean up computation graph - clear references\n        \n        # Hint: Use try-except blocks to catch and handle numerical errors\n        # Hint: Check tensor.grad is not None for all parameters after backward()\n        # Hint: Monitor gradient norms to detect vanishing/exploding gradients\n        \n        pass\n    \n    def validation_step(self, inputs: 'Tensor', targets: 'Tensor') -> Dict[str, float]:\n        \"\"\"Execute validation step without gradient computation.\"\"\"\n        # TODO 1: Set model to evaluation mode - model.eval()\n        # TODO 2: Disable gradient computation - use no_grad context if available\n        # TODO 3: Execute forward pass - predictions = model(inputs)\n        # TODO 4: Compute loss - loss = loss_function(predictions, targets)\n        # TODO 5: Compute additional metrics - accuracy, etc.\n        # TODO 6: Return metrics dictionary\n        \n        pass\n\n# core/data_flow.py - Forward/backward pass coordination\ndef coordinate_forward_pass(model: 'Module', inputs: 'Tensor') -> Tuple['Tensor', List['Tensor']]:\n    \"\"\"Coordinate forward pass with graph construction tracking.\"\"\"\n    # TODO 1: Verify input tensor shapes match model expectations\n    # TODO 2: Track all intermediate tensors created during forward pass\n    # TODO 3: Execute model forward pass - outputs = model(inputs)\n    # TODO 4: Verify output tensor shapes and numerical stability\n    # TODO 5: Return outputs and list of intermediate tensors for debugging\n    \n    pass\n\ndef coordinate_backward_pass(loss: 'Tensor', parameters: List['Tensor']) -> Dict[str, 'Tensor']:\n    \"\"\"Coordinate backward pass with gradient validation.\"\"\"\n    # TODO 1: Verify loss is scalar tensor with requires_grad=True\n    # TODO 2: Execute backward pass - loss.backward()\n    # TODO 3: Collect all computed gradients from parameters\n    # TODO 4: Validate gradient shapes match parameter shapes\n    # TODO 5: Check for gradient numerical issues (NaN, inf, very small/large)\n    # TODO 6: Return dictionary mapping parameter names to gradients\n    \n    # Hint: Use parameter.grad to access computed gradients\n    # Hint: Check gradient.shape == parameter.shape for each parameter\n    # Hint: Use np.isfinite() to check for NaN/inf in gradient values\n    \n    pass\n\ndef validate_gradient_computation(model: 'Module', inputs: 'Tensor', \n                                targets: 'Tensor', loss_fn: 'Loss', \n                                tolerance: float = 1e-5) -> bool:\n    \"\"\"Validate gradient computation using numerical differentiation.\"\"\"\n    # TODO 1: Compute analytical gradients using backward pass\n    # TODO 2: Compute numerical gradients using finite differences\n    # TODO 3: Compare analytical vs numerical gradients for each parameter\n    # TODO 4: Return True if all gradients match within tolerance\n    \n    # Hint: Use numerical_gradient() function from autograd utilities\n    # Hint: Compare gradients element-wise using np.allclose()\n    \n    pass\n```\n\n**E. Language-Specific Hints:**\n\n- **Memory Management**: Use `weakref.WeakSet()` to track tensors without preventing garbage collection\n- **Numerical Stability**: Use `np.isfinite()` to check for NaN/inf values in tensors and gradients  \n- **Context Management**: Implement `__enter__` and `__exit__` methods for automatic resource cleanup\n- **Error Propagation**: Use specific exception types (`ValueError`, `RuntimeError`) with descriptive messages\n- **Debugging Output**: Use `f-string` formatting for readable debug messages with tensor shapes and values\n- **Type Hints**: Include `Optional[Tensor]` for gradients that may be None during forward pass\n- **List Comprehensions**: Use generator expressions with `gc.get_objects()` for memory debugging\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the interactions and data flow coordination:\n\n1. **Run Training Step Test**: Execute `python -m pytest tests/test_training_step.py -v`\n2. **Expected Output**: All tests pass showing forward pass, backward pass, and parameter updates work correctly\n3. **Manual Verification**: Run the simple training loop example with `python examples/simple_training_loop.py`\n4. **Expected Behavior**: Should see decreasing loss values over iterations without NaN/inf values\n5. **Debug Check**: Monitor memory usage - it should remain stable across training iterations\n6. **Gradient Validation**: Run gradient checking test to verify analytical vs numerical gradients match\n\n**Signs of Correct Implementation:**\n- Loss decreases smoothly over training iterations  \n- All parameters receive gradients after backward pass\n- Memory usage remains stable across training steps\n- No NaN or infinite values in loss, gradients, or parameters\n- Gradient magnitudes are reasonable (not too small/large)\n\n**Signs of Problems:**\n- Loss becomes NaN or infinite after a few steps → Check learning rate, gradient clipping\n- Memory usage increases over time → Check computation graph cleanup\n- Some parameters have no gradients → Check requires_grad flags and graph connectivity\n- Loss doesn't decrease → Check optimizer implementation and learning rate\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones - error handling is critical throughout tensor operations, automatic differentiation, neural modules, and training loops\n\nBuilding a neural network framework is like constructing a precision manufacturing pipeline where small errors can cascade into catastrophic failures. Just as a factory needs robust quality control systems to detect defective parts, validate assembly processes, and prevent equipment damage, our framework must anticipate and gracefully handle the numerous failure modes that arise when working with numerical computation, dynamic memory management, and complex interdependent systems.\n\nThe challenge of error handling in neural network frameworks extends far beyond simple input validation. We must detect subtle numerical instabilities that can corrupt gradients, manage memory usage in dynamically-constructed computation graphs, validate tensor shape compatibility across broadcasting operations, and provide meaningful diagnostics when training diverges. The interconnected nature of our four-layer architecture means that failures can propagate across boundaries - a shape mismatch in tensor operations can manifest as gradient computation errors, while memory leaks in computation graphs can cause training crashes hours into a long experiment.\n\n### The Quality Control Analogy\n\nThink of error handling in our neural framework like a multi-stage quality control system in an automotive assembly plant. At each station, inspectors check for specific types of defects: dimensional tolerances at machining stations, electrical continuity at wiring stations, and structural integrity at assembly stations. When defects are detected, the system must decide whether to repair the part, reject it entirely, or shut down the line to prevent further damage.\n\nIn our framework, we implement similar quality control checkpoints. The tensor operations layer validates shape compatibility and numerical stability. The automatic differentiation engine checks gradient flow integrity and computation graph structure. The neural modules layer verifies parameter initialization and forward pass consistency. The training loop monitors convergence metrics and resource utilization. Each layer has specific failure modes, detection mechanisms, and recovery strategies tailored to its responsibilities.\n\nThe key insight is that different types of errors require different handling strategies. Some errors, like shape mismatches, should fail fast with clear diagnostic messages. Others, like numerical instabilities, may require graceful degradation with warnings and automatic remediation. Resource exhaustion errors need cleanup and recovery mechanisms. Understanding these categories helps us design appropriate error handling for each component.\n\n### Shape and Broadcasting Errors\n\nShape and broadcasting errors represent the most common category of failures in tensor-based neural networks. These errors occur when operations attempt to combine tensors with incompatible dimensions, when broadcasting rules cannot be applied to expand shapes, or when the resulting tensor shapes violate assumptions made by subsequent operations.\n\nThe mental model for shape errors is dimensional analysis in physics - just as you cannot add meters to seconds without a conversion factor, you cannot add a `(32, 128)` tensor to a `(64, 256)` tensor without broadcasting rules that make the operation mathematically meaningful. Our framework must detect these incompatibilities early, provide clear diagnostic information about what went wrong, and suggest potential fixes.\n\n#### Shape Compatibility Validation\n\nThe foundation of shape error handling lies in comprehensive validation during tensor operations. Every operation that combines multiple tensors must verify shape compatibility before proceeding with computation. This validation occurs at multiple levels: basic dimensional compatibility, broadcasting rule compliance, and operation-specific constraints.\n\n| Validation Type | Check Performed | Error Condition | Recovery Strategy |\n|-----------------|----------------|-----------------|-------------------|\n| Dimension Count | Verify tensor ranks | Incompatible ranks for non-broadcastable ops | Suggest reshape operations |\n| Broadcasting Rules | Apply NumPy broadcasting algorithm | Shapes cannot be broadcast together | Show broadcasting expansion steps |\n| Matrix Operations | Verify inner dimensions match | MatMul dimension mismatch | Display expected vs actual shapes |\n| Reduction Operations | Validate axis parameters | Axis out of range for tensor | List valid axis options |\n| Index Operations | Check bounds and slice validity | Index exceeds tensor dimensions | Show valid index ranges |\n\nThe validation process follows a systematic approach. First, we extract the shapes of all input tensors and normalize them for comparison. Second, we apply operation-specific compatibility rules, such as matrix multiplication's requirement that the last dimension of the first tensor matches the second-to-last dimension of the second tensor. Third, we attempt to compute the output shape using broadcasting rules. If any step fails, we construct detailed error messages that include the problematic shapes, the operation being attempted, and suggestions for resolution.\n\n> **Design Insight**: Shape validation must occur before any computation begins, not during or after. Once numerical computation starts with incompatible shapes, the resulting errors can be cryptic and difficult to trace back to the root cause. Early validation provides clean error messages at the point of the mistake.\n\n#### Broadcasting Error Diagnostics\n\nBroadcasting errors require special attention because they involve complex multi-dimensional shape transformations that can be difficult to visualize and debug. When broadcasting fails, programmers need to understand not just that the shapes are incompatible, but exactly how the broadcasting algorithm attempted to align the dimensions and where it failed.\n\nOur diagnostic system provides step-by-step visualization of the broadcasting process. When shapes `(3, 1, 4)` and `(2, 7)` cannot be broadcast together, the error message shows the alignment attempt:\n\n```\nBroadcasting Error: Cannot broadcast shapes (3, 1, 4) and (2, 7)\n\nAlignment attempt:\n  Shape 1: (3, 1, 4)  ← original\n  Shape 2:    (2, 7)  ← right-aligned\n  \nStep-by-step analysis:\n  Dimension 0: 3 vs missing → OK (extend with 1)\n  Dimension 1: 1 vs 2 → OK (broadcast 1 to 2)  \n  Dimension 2: 4 vs 7 → FAIL (neither is 1, cannot broadcast)\n\nSuggestion: Reshape tensors or use explicit broadcasting operations\n```\n\nThis detailed breakdown helps programmers understand exactly where the broadcasting algorithm failed and how to fix the problem. The error message includes the original shapes, the right-aligned comparison that broadcasting uses, and specific analysis of each dimension pair.\n\n> **Decision: Detailed Broadcasting Diagnostics**\n> - **Context**: Broadcasting errors are common but the failure reasons are often opaque to users\n> - **Options Considered**: \n>   1. Simple \"incompatible shapes\" message\n>   2. Show aligned shapes only\n>   3. Step-by-step dimension analysis with suggestions\n> - **Decision**: Implement comprehensive step-by-step analysis\n> - **Rationale**: Educational framework should help users learn broadcasting rules, not just report failures\n> - **Consequences**: More complex error handling code, but dramatically improved debugging experience for learners\n\n#### Gradient Shape Validation\n\nA particularly subtle category of shape errors occurs during the backward pass when gradients must be accumulated and propagated through the computation graph. These errors arise because gradient tensors must exactly match the shapes of the tensors they correspond to, but broadcasting during the forward pass can create gradients with expanded shapes that need to be reduced back to the original parameter shapes.\n\nThe gradient shape validation system tracks original tensor shapes throughout the computation graph and validates that gradients conform to these shapes during backpropagation. When shape mismatches occur, the system provides diagnostics that connect the gradient shape error back to the forward pass operation that caused it.\n\n| Gradient Error Type | Cause | Detection Method | Fix Strategy |\n|-------------------|--------|------------------|--------------|\n| Expanded Gradient | Broadcasting expanded tensor in forward pass | Compare gradient shape to original tensor | Apply sum reduction along broadcasted dims |\n| Missing Gradient Dimensions | Reduction operation in forward pass | Gradient has fewer dims than original | Insert dimensions with size 1 |\n| Accumulated Shape Mismatch | Multiple gradient contributions with different shapes | Shape check during gradient accumulation | Validate all gradients before summing |\n| Parameter Update Shape Error | Optimizer receives wrong gradient shape | Shape validation in optimizer step | Trace back through computation graph |\n\n#### Common Shape Error Pitfalls\n\n⚠️ **Pitfall: In-Place Operations Breaking Shape Tracking**\nWhen tensors are modified in-place, their shape metadata may become inconsistent with their actual data, leading to cascading errors in subsequent operations. The framework must either prohibit in-place operations on tensors with `requires_grad=True`, or carefully update all metadata when in-place modifications occur.\n\n⚠️ **Pitfall: Broadcasting Gradient Accumulation**\nDuring backpropagation, gradients that were broadcasted during the forward pass must be \"un-broadcasted\" by summing along the expanded dimensions. Forgetting this step causes gradient tensors to have the wrong shape for parameter updates. The solution is to track which dimensions were broadcasted and automatically reduce them during gradient computation.\n\n⚠️ **Pitfall: Shape Assumptions in Custom Operations**\nCustom operations often make implicit assumptions about input tensor shapes without validating them. These assumptions fail when the operations are composed in unexpected ways. Every operation must explicitly validate its shape requirements and provide clear error messages when they are violated.\n\n### Gradient-Related Problems\n\nGradient-related problems represent some of the most challenging debugging scenarios in neural network frameworks. Unlike shape errors, which typically cause immediate failures with clear symptoms, gradient problems often manifest as subtle training instabilities, convergence failures, or numerical anomalies that develop over many training steps.\n\nThe mental model for gradient problems is a mountain climbing expedition where each parameter update represents a step toward the summit (optimal loss). Gradient explosions are like taking enormous leaps that overshoot the target and land in dangerous territory. Vanishing gradients are like taking steps so small that progress becomes imperceptible. NaN gradients are like losing the compass entirely - the expedition can no longer navigate toward the goal.\n\n#### Gradient Explosion Detection and Mitigation\n\nGradient explosion occurs when gradient values become extremely large, causing parameter updates that destabilize the training process. This typically happens in deep networks where gradients are multiplied through many layers, causing exponential growth in gradient magnitude. Detection requires monitoring gradient norms and parameter update magnitudes throughout training.\n\nThe gradient explosion detection system tracks multiple metrics to identify problematic gradient behavior before it corrupts the model. These metrics include individual parameter gradient norms, global gradient norm across all parameters, and the ratio of parameter update magnitude to current parameter values.\n\n| Detection Metric | Threshold | Interpretation | Response Action |\n|------------------|-----------|----------------|-----------------|\n| Global Gradient Norm | > 10.0 | Potential explosion beginning | Enable gradient clipping |\n| Parameter Gradient Max | > 100.0 | Individual parameter exploding | Investigate specific layer |\n| Update-to-Parameter Ratio | > 0.1 | Updates too large relative to values | Reduce learning rate |\n| Gradient Norm Growth Rate | > 2x per step | Exponential growth detected | Emergency gradient clipping |\n| NaN/Inf Detection | Any NaN/Inf | Numerical overflow occurred | Reset to last valid state |\n\nThe gradient clipping implementation provides both global norm clipping and per-parameter clipping strategies. Global norm clipping scales all gradients by the same factor to maintain relative magnitudes while constraining the total gradient norm. Per-parameter clipping applies individual constraints to each parameter's gradients, which can be more aggressive but may distort the gradient direction.\n\n```\nGradient Clipping Algorithm:\n1. Compute global gradient norm across all parameters\n2. If global norm exceeds threshold:\n   a. Compute scaling factor = threshold / global_norm\n   b. Scale all parameter gradients by scaling factor\n   c. Log clipping event and scaling factor\n3. For each parameter individually:\n   a. If parameter gradient norm exceeds per-param threshold\n   b. Clip parameter gradient to threshold magnitude\n   c. Preserve gradient direction\n4. Update gradient statistics for monitoring\n```\n\n#### Vanishing Gradient Detection\n\nVanishing gradients occur when gradient magnitudes become extremely small, effectively stopping parameter updates and preventing learning in affected layers. This problem is particularly common in deep networks and recurrent architectures where gradients are repeatedly multiplied by small values during backpropagation.\n\nDetection of vanishing gradients requires monitoring gradient magnitudes over time and identifying parameters whose gradients consistently fall below meaningful thresholds. Unlike gradient explosion, which causes immediate obvious problems, vanishing gradients create subtle learning stagnation that may only become apparent after many training steps.\n\n| Vanishing Gradient Indicator | Threshold | Detection Window | Remediation |\n|-----------------------------|-----------|------------------|-------------|\n| Parameter Gradient Norm | < 1e-7 | 50 consecutive steps | Flag parameter as inactive |\n| Layer-wise Gradient Ratio | < 0.01 vs input layer | Current step | Investigate layer depth |\n| Parameter Update Magnitude | < 1e-10 | 100 consecutive steps | Consider parameter frozen |\n| Gradient Variance | < 1e-12 | 20-step rolling window | Check initialization scheme |\n\nThe vanishing gradient diagnostic system provides layer-wise gradient analysis to identify where in the network gradients begin to diminish. This analysis computes gradient norms at each layer and compares them to the input layer gradients, helping identify problematic depth ranges or specific layer types that contribute to gradient decay.\n\n#### NaN and Infinity Handling\n\nNumerical instabilities that produce NaN (Not a Number) or infinity values represent critical failures that can permanently corrupt training if not handled immediately. These instabilities arise from division by zero, logarithms of negative numbers, exponential overflow, or accumulated floating-point errors that exceed representable ranges.\n\nThe NaN/infinity detection system performs comprehensive numerical validation at multiple checkpoints throughout the training process. Detection occurs during forward pass computation, gradient computation, parameter updates, and loss evaluation. When instabilities are detected, the system must decide whether to attempt recovery or abort training with detailed diagnostics.\n\n| Instability Source | Common Causes | Detection Point | Recovery Strategy |\n|-------------------|---------------|-----------------|-------------------|\n| Division by Zero | Normalizing by zero variance | After normalization ops | Add epsilon constant |\n| Exponential Overflow | Large logits in softmax | After activation functions | Apply gradient clipping |\n| Logarithm of Non-positive | Negative probabilities | After probability computations | Clamp to minimum value |\n| Accumulated Floating Point Error | Long computation chains | After gradient accumulation | Reset computation graph |\n| Optimizer State Corruption | NaN gradients affect momentum | During optimizer step | Reset optimizer state |\n\nThe recovery system implements a hierarchical approach to handling numerical instabilities. Minor instabilities trigger automatic remediation such as adding epsilon values or clamping to valid ranges. Moderate instabilities trigger warnings and temporary gradient clipping or learning rate reduction. Severe instabilities that cannot be automatically remediated trigger training termination with comprehensive diagnostic information.\n\n> **Decision: Immediate NaN Detection vs Batch Validation**\n> - **Context**: NaN values can propagate through computation graphs and corrupt multiple tensors before detection\n> - **Options Considered**:\n>   1. Check every tensor operation result immediately\n>   2. Validate tensors at end of forward/backward passes\n>   3. Periodic validation every N steps\n> - **Decision**: Immediate detection during training, periodic validation during inference\n> - **Rationale**: Training failures are expensive and hard to recover from, while inference can tolerate some performance overhead\n> - **Consequences**: Higher computational cost during training, but much faster debugging and recovery from numerical issues\n\n#### Gradient Validation and Testing\n\nRobust gradient computation requires systematic validation against known correct results. The gradient validation system compares automatic differentiation results with numerical differentiation to detect implementation bugs, subtle numerical errors, and edge cases where gradient computation fails.\n\nNumerical differentiation provides ground truth by computing gradients using the mathematical definition: the limit of the difference quotient as the step size approaches zero. While computationally expensive, numerical gradients serve as a reference for validating automatic differentiation implementations.\n\n| Validation Test | Method | Tolerance | Purpose |\n|-----------------|--------|-----------|---------|\n| Single Operation Gradients | Compare autodiff vs numerical | 1e-5 | Validate operation implementations |\n| Composition Gradients | Chain multiple operations | 1e-4 | Test chain rule application |\n| Broadcasting Gradients | Operations with broadcasting | 1e-5 | Validate shape handling |\n| Accumulation Gradients | Multiple paths to same tensor | 1e-6 | Test gradient accumulation |\n| Second-Order Gradients | Gradients of gradients | 1e-3 | Validate higher-order derivatives |\n\nThe gradient testing infrastructure provides utilities for systematic validation across different tensor shapes, operation combinations, and edge cases. These tests run during development to catch regressions and can be enabled during training to validate custom operations or suspected gradient computation issues.\n\n#### Common Gradient Implementation Pitfalls\n\n⚠️ **Pitfall: Gradient Not Accumulated Across Multiple Uses**\nWhen a tensor participates in multiple operations within a computation graph, its gradients must be accumulated (summed) from all paths. Forgetting to accumulate gradients causes incorrect gradient values and poor training performance. The solution is to always add incoming gradients to existing gradient values rather than overwriting them.\n\n⚠️ **Pitfall: Incorrect Topological Ordering in Backward Pass**\nThe backward pass must visit computation graph nodes in reverse topological order to ensure gradients are computed after all dependent gradients are available. Incorrect ordering can cause gradients to be computed with incomplete information. The solution is to implement proper topological sorting based on the computation graph structure.\n\n⚠️ **Pitfall: Broadcasting Gradients Not Reduced**\nWhen tensors are broadcasted during forward pass operations, their gradients must be reduced (summed) along the broadcasted dimensions during the backward pass. This ensures gradient tensors have the same shape as their corresponding parameter tensors. The solution is to track broadcasting operations and automatically reduce gradients appropriately.\n\n### Memory and Resource Management\n\nMemory and resource management in neural network frameworks presents unique challenges due to the dynamic nature of computation graphs, the large size of tensors, and the complex interdependencies between training components. Unlike traditional applications where memory allocation follows predictable patterns, neural frameworks must manage memory for dynamically-constructed graphs that can grow arbitrarily large and contain circular references that prevent automatic garbage collection.\n\nThink of memory management in our framework like managing a large construction project with multiple work sites, shared equipment, and temporary structures. Each computation graph is like a construction site that accumulates scaffolding (intermediate tensors), equipment (gradient functions), and materials (cached computations). Without proper cleanup protocols, these resources accumulate across sites, eventually exhausting available capacity and bringing all work to a halt.\n\n#### Computation Graph Memory Leaks\n\nComputation graphs create complex webs of references between tensors and operations that can prevent proper memory reclamation. Each tensor holds references to the operations that created it, and each operation holds references to its input tensors. This creates cycles that Python's garbage collector cannot automatically resolve, leading to memory leaks that accumulate over training iterations.\n\nThe fundamental challenge is that computation graphs must remain intact during the forward pass and through gradient computation, but they must be cleanly destroyed afterward to free memory for subsequent iterations. This requires careful lifecycle management that tracks graph components and ensures timely cleanup without interfering with ongoing computations.\n\n| Memory Leak Source | Reference Pattern | Detection Method | Cleanup Strategy |\n|-------------------|------------------|------------------|------------------|\n| Tensor-Operation Cycles | Tensor.grad_fn → Operation → inputs → Tensor | Weak reference analysis | Explicit graph destruction |\n| Accumulated Gradients | Multiple gradients referencing same tensors | Memory profiling over time | Periodic gradient clearing |\n| Cached Intermediate Values | Operations storing forward pass results | Object count tracking | LRU cache with size limits |\n| Optimizer State | Growing state dictionaries per parameter | Memory usage monitoring | State compression and pruning |\n| Training History | Loss values, metrics, gradient norms | Linear growth detection | Rolling window storage |\n\nThe graph cleanup system implements a multi-phase approach to memory reclamation. After each training step, it identifies computation graphs that are no longer needed, breaks reference cycles by clearing operation inputs, and explicitly destroys tensor gradients and cached values. This cleanup must be coordinated carefully to avoid interfering with ongoing gradient computation or optimizer state updates.\n\n#### Memory-Efficient Graph Construction\n\nLarge neural networks and long training sequences can create computation graphs that exceed available memory even with proper cleanup. Memory-efficient graph construction techniques reduce peak memory usage by strategically releasing intermediate values, using gradient checkpointing, and implementing streaming computation patterns.\n\nGradient checkpointing represents a key trade-off between memory usage and computation time. Instead of storing all intermediate activations throughout the forward pass, gradient checkpointing saves only selected checkpoints and recomputes intermediate values during the backward pass. This reduces memory usage at the cost of additional computation.\n\n| Memory Optimization | Memory Reduction | Computation Overhead | Implementation Complexity |\n|-------------------|------------------|---------------------|--------------------------|\n| Gradient Checkpointing | 50-80% | 30-50% increase | Medium |\n| In-Place Operations | 20-40% | Minimal | High (gradient safety) |\n| Streaming Computation | 60-90% | Variable | High |\n| Dynamic Graph Pruning | 30-60% | 10-20% increase | Medium |\n| Compressed Gradients | 40-70% | 15-25% increase | Low |\n\nThe memory management system provides automatic memory monitoring and adaptive strategies based on available resources. When memory usage approaches system limits, the framework automatically enables more aggressive optimization techniques such as gradient checkpointing or compressed gradient storage.\n\n> **Decision: Automatic vs Manual Memory Management**\n> - **Context**: Learners should focus on core concepts rather than memory management details, but memory leaks can make training impossible\n> - **Options Considered**:\n>   1. Fully manual memory management requiring explicit cleanup calls\n>   2. Automatic cleanup with performance overhead\n>   3. Hybrid approach with automatic cleanup and manual override options\n> - **Decision**: Automatic cleanup with monitoring and manual override capabilities\n> - **Rationale**: Educational framework should not burden learners with memory details, but advanced users need control for large experiments\n> - **Consequences**: More complex implementation, but better learning experience and scalability for serious projects\n\n#### Large Tensor Allocation Strategies\n\nNeural networks often require tensors that approach or exceed available system memory, particularly during batch processing or when working with high-resolution data. Large tensor allocation requires strategies for detecting memory constraints, implementing out-of-core computation, and gracefully degrading when resources are insufficient.\n\nThe large tensor management system monitors available memory and automatically adjusts allocation strategies based on resource constraints. When standard in-memory allocation would exceed available resources, the system can switch to memory-mapped files, reduce batch sizes, or implement streaming computation patterns.\n\n| Allocation Strategy | Memory Requirement | Performance Impact | Use Case |\n|--------------------|-------------------|-------------------|----------|\n| Standard In-Memory | Full tensor size | Optimal | Small to medium tensors |\n| Memory-Mapped Files | Minimal RAM usage | I/O bound operations | Very large tensors |\n| Chunked Processing | Chunk size only | CPU overhead | Batch dimension too large |\n| Compressed Storage | 30-70% reduction | Compression/decompression cost | Storage-bound scenarios |\n| Distributed Tensors | Split across devices | Communication overhead | Multi-GPU scenarios |\n\nThe allocation system provides automatic fallback strategies when primary allocation methods fail. If in-memory allocation fails due to insufficient memory, the system automatically attempts memory-mapped allocation. If batch sizes are too large, it automatically reduces batch sizes and adjusts learning rate accordingly.\n\n#### Resource Monitoring and Alerts\n\nEffective resource management requires continuous monitoring of memory usage, computation time, and system resources throughout training. The monitoring system tracks resource consumption patterns, identifies potential problems before they cause failures, and provides alerts when resource usage approaches dangerous levels.\n\nThe monitoring infrastructure collects metrics at multiple levels: per-tensor memory usage, per-operation computation time, per-layer gradient statistics, and system-wide resource utilization. These metrics are analyzed to identify trends, detect anomalies, and predict potential resource exhaustion.\n\n| Monitoring Metric | Alert Threshold | Trend Analysis | Remediation Action |\n|-------------------|----------------|----------------|-------------------|\n| Memory Usage | 80% of available | Growth rate > 5% per epoch | Enable aggressive cleanup |\n| Gradient Norms | 3x standard deviation | Increasing variance | Enable gradient monitoring |\n| Computation Time | 2x expected duration | Sudden increases | Profile operation performance |\n| Graph Size | 10000+ operations | Linear growth | Enable graph pruning |\n| Parameter Count | RAM capacity limit | Approaching limits | Suggest model reduction |\n\nThe alert system provides graduated responses based on severity levels. Warning alerts log information for later analysis without interrupting training. Critical alerts pause training and provide diagnostic information to help identify the cause of resource problems. Emergency alerts terminate training to prevent system instability.\n\n#### Common Memory Management Pitfalls\n\n⚠️ **Pitfall: Retaining References to Computation Graphs**\nStoring references to tensors or intermediate results from previous training iterations prevents garbage collection of entire computation graphs. The solution is to extract scalar values (like loss) rather than retaining tensor objects, and to explicitly clear any collections that might hold tensor references.\n\n⚠️ **Pitfall: Accumulating Optimizer State Without Bounds**\nSome optimizers accumulate state (like momentum buffers) that grows over time, especially when new parameters are added dynamically. The solution is to implement state cleanup when parameters are removed and to monitor optimizer memory usage as part of overall resource tracking.\n\n⚠️ **Pitfall: Memory Leaks in Custom Operations**\nCustom operations that allocate temporary memory or create intermediate tensors without proper cleanup can cause subtle memory leaks. The solution is to implement explicit cleanup methods for custom operations and to use memory profiling during testing to detect leaks.\n\n### Implementation Guidance\n\nThis implementation guidance provides the infrastructure and starter code needed to implement robust error handling throughout your neural network framework. The error handling system must integrate with all four layers of the architecture while remaining simple enough for educational purposes.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Shape Validation | Custom validation functions with NumPy | Formal shape algebra with symbolic shapes |\n| Error Messages | String formatting with templates | Rich error objects with structured data |\n| Memory Monitoring | Python memory_profiler | Custom C extension with detailed tracking |\n| Gradient Validation | Numerical differentiation with finite differences | Symbolic differentiation with computer algebra |\n| Resource Alerts | Simple logging with thresholds | Prometheus metrics with alerting rules |\n| Graph Cleanup | Manual reference breaking | Weak references with automatic cleanup |\n\n#### Recommended File Structure\n\n```\nneural_framework/\n  core/\n    errors.py              ← Exception classes and error handling utilities  \n    validation.py          ← Shape and gradient validation functions\n    diagnostics.py         ← Error diagnosis and debugging tools\n  tensor/\n    tensor.py             ← Tensor class with error checking\n    operations.py         ← Operations with validation\n    broadcasting.py       ← Broadcasting utilities and error handling\n  autodiff/\n    engine.py            ← Autodiff with gradient validation\n    graph.py             ← Graph management and cleanup\n  training/\n    monitor.py           ← Training monitoring and alerts\n    coordinator.py       ← Training coordination with error handling\n  utils/\n    memory.py            ← Memory management utilities\n    profiling.py         ← Performance and memory profiling\n```\n\n#### Core Error Handling Infrastructure\n\n```python\n# core/errors.py - Complete error handling system\nimport numpy as np\nfrom typing import Tuple, List, Optional, Dict, Any\nimport traceback\nimport sys\n\nclass NeuralFrameworkError(Exception):\n    \"\"\"Base exception for all neural framework errors.\"\"\"\n    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n        super().__init__(message)\n        self.details = details or {}\n        self.stack_info = traceback.format_stack()\n\nclass ShapeError(NeuralFrameworkError):\n    \"\"\"Raised when tensor shapes are incompatible for operations.\"\"\"\n    def __init__(self, message: str, shapes: List[Tuple], operation: str):\n        super().__init__(message)\n        self.shapes = shapes\n        self.operation = operation\n        \n    def __str__(self):\n        shape_info = \", \".join(f\"shape_{i}: {shape}\" for i, shape in enumerate(self.shapes))\n        return f\"{self.args[0]} in {self.operation} ({shape_info})\"\n\nclass BroadcastingError(ShapeError):\n    \"\"\"Raised when broadcasting rules cannot align tensor shapes.\"\"\"\n    def __init__(self, shape1: Tuple, shape2: Tuple, step_analysis: List[str]):\n        self.shape1 = shape1\n        self.shape2 = shape2\n        self.step_analysis = step_analysis\n        \n        analysis_text = \"\\n\".join(f\"  {step}\" for step in step_analysis)\n        message = f\"Cannot broadcast shapes {shape1} and {shape2}\\n{analysis_text}\"\n        super().__init__(message, [shape1, shape2], \"broadcasting\")\n\nclass GradientError(NeuralFrameworkError):\n    \"\"\"Raised when gradient computation fails or produces invalid results.\"\"\"\n    pass\n\nclass NumericalInstabilityError(NeuralFrameworkError):\n    \"\"\"Raised when NaN or infinity values are detected.\"\"\"\n    def __init__(self, message: str, tensor_info: Dict[str, Any]):\n        super().__init__(message)\n        self.tensor_info = tensor_info\n\ndef validate_tensor_finite(tensor: 'Tensor', operation: str = \"unknown\") -> None:\n    \"\"\"Validate that tensor contains only finite values.\"\"\"\n    if not np.isfinite(tensor.data).all():\n        nan_count = np.isnan(tensor.data).sum()\n        inf_count = np.isinf(tensor.data).sum()\n        \n        tensor_info = {\n            'shape': tensor.data.shape,\n            'dtype': tensor.data.dtype,\n            'nan_count': int(nan_count),\n            'inf_count': int(inf_count),\n            'operation': operation\n        }\n        \n        raise NumericalInstabilityError(\n            f\"Tensor contains {nan_count} NaN and {inf_count} infinite values after {operation}\",\n            tensor_info\n        )\n\ndef create_shape_error_message(shapes: List[Tuple], operation: str, \n                             expected_pattern: str = None) -> str:\n    \"\"\"Create detailed error message for shape mismatches.\"\"\"\n    shape_list = [f\"  Input {i}: {shape}\" for i, shape in enumerate(shapes)]\n    shape_text = \"\\n\".join(shape_list)\n    \n    message = f\"Shape mismatch in {operation}:\\n{shape_text}\"\n    \n    if expected_pattern:\n        message += f\"\\nExpected pattern: {expected_pattern}\"\n        \n    return message\n```\n\n#### Broadcasting Validation and Diagnostics\n\n```python\n# tensor/broadcasting.py - Broadcasting utilities with comprehensive error handling\nimport numpy as np\nfrom typing import Tuple, List, Optional\nfrom core.errors import BroadcastingError, ShapeError\n\ndef broadcast_shapes(shape1: Tuple, shape2: Tuple) -> Tuple:\n    \"\"\"\n    Compute the broadcasted shape for two input shapes.\n    Raises BroadcastingError with detailed analysis if broadcasting fails.\n    \"\"\"\n    # TODO 1: Right-align shapes by padding with 1s on the left\n    # TODO 2: Compare dimensions from right to left\n    # TODO 3: For each dimension pair, apply broadcasting rules:\n    #         - If dimensions are equal, use that dimension\n    #         - If one dimension is 1, use the other dimension  \n    #         - Otherwise, shapes are incompatible\n    # TODO 4: Collect step-by-step analysis for error reporting\n    # TODO 5: Return resulting shape or raise BroadcastingError\n    pass\n\ndef analyze_broadcasting_failure(shape1: Tuple, shape2: Tuple) -> List[str]:\n    \"\"\"\n    Analyze why two shapes cannot be broadcasted together.\n    Returns step-by-step analysis for error messages.\n    \"\"\"\n    # TODO 1: Right-align shapes and show alignment\n    # TODO 2: Compare each dimension pair and classify compatibility\n    # TODO 3: Identify the first incompatible dimension\n    # TODO 4: Suggest potential fixes (reshape operations, etc.)\n    pass\n\ndef unbroadcast_gradient(grad: np.ndarray, original_shape: Tuple) -> np.ndarray:\n    \"\"\"\n    Reduce gradient tensor to match original parameter shape.\n    This reverses the broadcasting that occurred during forward pass.\n    \"\"\"\n    # TODO 1: Identify dimensions that were broadcasted (size 1 in original)\n    # TODO 2: Sum along broadcasted dimensions to reduce gradient\n    # TODO 3: Squeeze dimensions that were added during broadcasting\n    # TODO 4: Validate final gradient shape matches original shape\n    # TODO 5: Handle edge case where grad is scalar but original has shape\n    pass\n\ndef validate_broadcasting_safety(tensors: List['Tensor'], operation: str) -> Tuple:\n    \"\"\"\n    Validate that a list of tensors can be safely broadcasted together.\n    Returns the final broadcasted shape or raises detailed error.\n    \"\"\"\n    if not tensors:\n        raise ValueError(\"Cannot broadcast empty tensor list\")\n        \n    if len(tensors) == 1:\n        return tensors[0].data.shape\n        \n    try:\n        result_shape = tensors[0].data.shape\n        for i, tensor in enumerate(tensors[1:], 1):\n            result_shape = broadcast_shapes(result_shape, tensor.data.shape)\n        return result_shape\n        \n    except BroadcastingError as e:\n        # Enhance error message with operation context\n        enhanced_message = f\"{e.args[0]} in {operation}\"\n        raise BroadcastingError(e.shape1, e.shape2, e.step_analysis) from e\n```\n\n#### Gradient Validation System\n\n```python\n# core/validation.py - Gradient validation and numerical checking\nimport numpy as np\nfrom typing import Callable, List, Dict, Tuple\nfrom core.errors import GradientError\n\ndef numerical_gradient(f: Callable, inputs: List['Tensor'], h: float = 1e-5) -> List[np.ndarray]:\n    \"\"\"\n    Compute numerical gradients using central difference method.\n    Used as ground truth for validating automatic differentiation.\n    \"\"\"\n    # TODO 1: For each input tensor, compute partial derivatives\n    # TODO 2: Use central difference: (f(x+h) - f(x-h)) / (2*h)\n    # TODO 3: Handle each element of each input tensor individually\n    # TODO 4: Return list of gradient arrays matching input shapes\n    # TODO 5: Use appropriate step size for tensor dtype precision\n    pass\n\ndef check_gradients(f: Callable, inputs: List['Tensor'], tolerance: float = 1e-5) -> bool:\n    \"\"\"\n    Compare automatic differentiation gradients with numerical gradients.\n    Returns True if gradients match within tolerance.\n    \"\"\"\n    # TODO 1: Compute gradients using automatic differentiation\n    # TODO 2: Compute gradients using numerical differentiation  \n    # TODO 3: Compare gradients element-wise within tolerance\n    # TODO 4: Report detailed analysis of any mismatches\n    # TODO 5: Return validation result and diagnostic information\n    pass\n\ndef validate_gradient_flow(model: 'Module', loss: 'Tensor') -> Dict[str, float]:\n    \"\"\"\n    Check that gradients flow properly through all model parameters.\n    Returns dictionary with gradient statistics for each parameter.\n    \"\"\"\n    gradient_stats = {}\n    \n    # TODO 1: Iterate through all model parameters\n    # TODO 2: Check that each parameter has non-None gradient\n    # TODO 3: Compute gradient norm, mean, std for each parameter\n    # TODO 4: Identify parameters with zero or very small gradients\n    # TODO 5: Flag potential vanishing gradient problems\n    # TODO 6: Return comprehensive gradient flow analysis\n    \n    return gradient_stats\n\nclass GradientMonitor:\n    \"\"\"Monitors gradient statistics during training to detect problems.\"\"\"\n    \n    def __init__(self, check_frequency: int = 10):\n        self.check_frequency = check_frequency\n        self.step_count = 0\n        self.gradient_history = []\n        \n    def check_gradients(self, model: 'Module') -> Dict[str, Any]:\n        \"\"\"\n        Perform gradient health checks and return diagnostic information.\n        \"\"\"\n        self.step_count += 1\n        \n        if self.step_count % self.check_frequency != 0:\n            return {}\n            \n        # TODO 1: Compute gradient norms for all parameters\n        # TODO 2: Check for NaN or infinite gradients\n        # TODO 3: Detect gradient explosion (norms too large)\n        # TODO 4: Detect gradient vanishing (norms too small)\n        # TODO 5: Compare current gradients to historical values\n        # TODO 6: Return comprehensive diagnostic report\n        pass\n```\n\n#### Memory Management Infrastructure\n\n```python\n# utils/memory.py - Memory management and resource monitoring\nimport gc\nimport psutil\nimport weakref\nfrom typing import Set, List, Dict, Any, Optional\nfrom core.errors import NeuralFrameworkError\n\nclass ComputationGraphTracker:\n    \"\"\"Tracks active computation graphs to prevent memory leaks.\"\"\"\n    \n    def __init__(self):\n        self._active_graphs: Set[weakref.ref] = set()\n        self._cleanup_callbacks: List[Callable] = []\n        \n    def register_tensor(self, tensor: 'Tensor') -> None:\n        \"\"\"Register a tensor as part of active computation graph.\"\"\"\n        # TODO 1: Create weak reference to tensor\n        # TODO 2: Add cleanup callback for when tensor is garbage collected\n        # TODO 3: Track tensor in active graphs set\n        pass\n        \n    def cleanup_graphs(self) -> int:\n        \"\"\"Force cleanup of unreachable computation graphs.\"\"\"\n        # TODO 1: Identify graphs with no strong references\n        # TODO 2: Break circular references between tensors and operations\n        # TODO 3: Clear gradient functions and cached values\n        # TODO 4: Force garbage collection\n        # TODO 5: Return number of cleaned up graphs\n        pass\n\nclass MemoryMonitor:\n    \"\"\"Monitors memory usage and provides alerts for resource problems.\"\"\"\n    \n    def __init__(self, warning_threshold: float = 0.8, critical_threshold: float = 0.9):\n        self.warning_threshold = warning_threshold\n        self.critical_threshold = critical_threshold\n        self.baseline_memory = psutil.Process().memory_info().rss\n        \n    def check_memory_usage(self) -> Dict[str, Any]:\n        \"\"\"Check current memory usage and return status information.\"\"\"\n        process = psutil.Process()\n        memory_info = process.memory_info()\n        \n        current_mb = memory_info.rss / (1024 * 1024)\n        available_mb = psutil.virtual_memory().available / (1024 * 1024)\n        usage_fraction = memory_info.rss / psutil.virtual_memory().total\n        \n        status = {\n            'current_mb': current_mb,\n            'available_mb': available_mb, \n            'usage_fraction': usage_fraction,\n            'alert_level': 'normal'\n        }\n        \n        # TODO 1: Compare usage against thresholds\n        # TODO 2: Set appropriate alert level\n        # TODO 3: Include memory growth rate if tracking over time\n        # TODO 4: Add recommendations for memory reduction\n        \n        return status\n\ndef cleanup_training_step(model: 'Module', optimizer: 'Optimizer') -> None:\n    \"\"\"Perform cleanup after training step to prevent memory leaks.\"\"\"\n    # TODO 1: Clear gradients from model parameters\n    # TODO 2: Clean up computation graphs from this step\n    # TODO 3: Force garbage collection if memory usage is high\n    # TODO 4: Clear any cached values in optimizer\n    pass\n```\n\n#### Training Monitoring and Alerts\n\n```python\n# training/monitor.py - Training monitoring with error detection\nimport numpy as np\nfrom typing import Dict, List, Any, Optional\nimport logging\nfrom core.errors import NumericalInstabilityError\n\nclass TrainingMonitor:\n    \"\"\"Monitors training progress and detects problems automatically.\"\"\"\n    \n    def __init__(self, check_frequency: int = 10):\n        self.check_frequency = check_frequency\n        self.step_count = 0\n        self.loss_history: List[float] = []\n        self.gradient_norms: List[float] = []\n        \n    def log_step(self, loss: float, gradients: Dict[str, 'Tensor']) -> None:\n        \"\"\"Log training step and check for problems.\"\"\"\n        self.step_count += 1\n        self.loss_history.append(loss)\n        \n        # TODO 1: Compute global gradient norm\n        # TODO 2: Check for NaN or infinite loss\n        # TODO 3: Check for gradient explosion or vanishing\n        # TODO 4: Detect loss divergence or stagnation\n        # TODO 5: Log warnings or alerts as appropriate\n        # TODO 6: Update gradient statistics\n        \n        if self.step_count % self.check_frequency == 0:\n            self._perform_health_checks()\n            \n    def _perform_health_checks(self) -> None:\n        \"\"\"Perform comprehensive training health checks.\"\"\"\n        # TODO 1: Analyze loss trend over recent steps\n        # TODO 2: Check gradient norm stability\n        # TODO 3: Validate memory usage is stable\n        # TODO 4: Check for numerical instabilities\n        # TODO 5: Generate alerts or recommendations\n        pass\n        \n    def get_diagnostics(self) -> Dict[str, Any]:\n        \"\"\"Return comprehensive training diagnostics.\"\"\"\n        # TODO 1: Compute loss statistics (mean, std, trend)\n        # TODO 2: Compute gradient statistics\n        # TODO 3: Identify potential problems\n        # TODO 4: Provide recommendations for fixes\n        return {}\n\ndef validate_training_step(model: 'Module', loss: 'Tensor', \n                         optimizer: 'Optimizer') -> List[str]:\n    \"\"\"\n    Validate that training step completed successfully.\n    Returns list of warnings or empty list if everything is OK.\n    \"\"\"\n    warnings = []\n    \n    # TODO 1: Check that loss is finite and reasonable\n    # TODO 2: Verify all parameters have gradients\n    # TODO 3: Check gradient magnitudes are reasonable\n    # TODO 4: Validate optimizer state is consistent\n    # TODO 5: Check for memory leaks or unusual memory growth\n    # TODO 6: Return list of any warnings found\n    \n    return warnings\n```\n\n#### Milestone Checkpoints\n\nAfter implementing error handling infrastructure, verify your system with these checkpoints:\n\n**Checkpoint 1: Shape Error Detection**\n```python\n# Test that shape errors are caught and reported clearly\nimport numpy as np\nfrom tensor import Tensor\n\n# This should raise a clear BroadcastingError\ntry:\n    a = Tensor(np.random.randn(3, 4), requires_grad=True)\n    b = Tensor(np.random.randn(5, 2), requires_grad=True)\n    c = a + b  # Should fail with detailed error message\nexcept BroadcastingError as e:\n    print(\"✓ Broadcasting error detected correctly\")\n    print(f\"Error message: {e}\")\n```\n\n**Checkpoint 2: Gradient Validation**\n```python\n# Test gradient correctness using numerical differentiation\ndef simple_function(x):\n    return (x ** 2).sum()\n\nx = Tensor(np.array([1.0, 2.0, 3.0]), requires_grad=True)\nresult = simple_function(x)\nresult.backward()\n\n# Should validate that gradients match numerical approximation\nis_correct = check_gradients(simple_function, [x], tolerance=1e-5)\nassert is_correct, \"Gradients don't match numerical approximation\"\nprint(\"✓ Gradient validation working correctly\")\n```\n\n**Checkpoint 3: Memory Leak Detection**\n```python\n# Test that computation graphs are cleaned up properly\nimport gc\ntracker = ComputationGraphTracker()\n\ninitial_count = len(gc.get_objects())\n\n# Create and destroy many computation graphs\nfor i in range(100):\n    x = Tensor(np.random.randn(10, 10), requires_grad=True)\n    y = x.matmul(x)\n    loss = y.sum()\n    loss.backward()\n    # Should automatically clean up graph\n\nfinal_count = len(gc.get_objects())\ngrowth = final_count - initial_count\n\nassert growth < 100, f\"Memory leak detected: {growth} objects accumulated\"\nprint(\"✓ Memory management working correctly\")\n\n```\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones - testing is critical throughout tensor operations, automatic differentiation, neural modules, and training loops to ensure correctness and catch bugs early\n\nBuilding a neural network framework requires rigorous testing because the mathematical complexity creates many opportunities for subtle bugs. Think of testing as conducting scientific experiments to verify our mathematical claims. Just as physicists verify theoretical predictions with experimental measurements, we must verify our automatic differentiation implementation against known mathematical ground truth.\n\nThe testing strategy for a neural network framework differs fundamentally from typical software testing because we're implementing mathematical algorithms with precise, verifiable correct answers. Unlike testing a web API where we might mock external services, neural network testing relies heavily on mathematical verification - comparing our computed gradients against analytically or numerically derived ground truth.\n\nTesting becomes even more critical because bugs in neural network frameworks often manifest as poor training performance rather than obvious crashes. A subtle gradient computation error might allow a model to train but converge slowly or to suboptimal solutions. Our testing strategy must catch these mathematical errors before they become mysterious training issues.\n\n### Gradient Correctness Testing\n\nThe cornerstone of neural network framework testing is **gradient correctness testing** - verifying that our automatic differentiation implementation produces mathematically correct gradients. Think of this as calibrating a scientific instrument: before trusting our automatic differentiation \"instrument\" to compute gradients for complex models, we must verify it produces correct results on simple, analytically tractable problems.\n\nGradient correctness testing relies on comparing our automatic differentiation results against **numerical differentiation** - a brute-force but mathematically sound technique that approximates derivatives using the definition of a derivative as a limit. While numerical differentiation is too slow for training neural networks, it provides reliable ground truth for testing.\n\nThe mathematical foundation is the finite difference approximation of derivatives. For a scalar function f(x), the derivative is:\n\nf'(x) ≈ [f(x + h) - f(x - h)] / (2h)\n\nFor multivariable functions, we compute partial derivatives by perturbing each input dimension independently while holding others constant. This **numerical gradient** computation is slow (requires 2n function evaluations for n parameters) but mathematically reliable.\n\nOur gradient testing framework implements several key verification procedures:\n\n| Testing Method | Purpose | When to Use | Limitations |\n|---|---|---|---|\n| **Analytical Verification** | Compare against hand-computed derivatives | Simple functions like x², sin(x), matrix multiply | Only feasible for simple operations |\n| **Numerical Differentiation** | Compare against finite difference approximation | All operations and composed functions | Slow, numerical precision issues |\n| **Gradient Checking** | Automated numerical vs autodiff comparison | During development and debugging | Requires careful tolerance selection |\n| **Property Testing** | Verify mathematical properties like linearity | Testing operation implementations | Doesn't catch all error types |\n\n**Numerical differentiation implementation** requires careful attention to the step size h. Too large, and we get poor approximations due to higher-order terms in the Taylor expansion. Too small, and floating-point precision errors dominate. The typical choice is h = 1e-5, which balances approximation accuracy with numerical stability.\n\nThe gradient checking procedure follows these steps:\n\n1. **Function Wrapping**: Create a scalar-valued function from our neural network computation by taking a single output element and fixing all other inputs\n2. **Parameter Perturbation**: For each parameter, compute f(θ + h⋅eᵢ) and f(θ - h⋅eᵢ) where eᵢ is the i-th unit vector\n3. **Numerical Gradient**: Compute numerical gradient as [f(θ + h⋅eᵢ) - f(θ - h⋅eᵢ)] / (2h)\n4. **Autodiff Gradient**: Run our automatic differentiation to compute the same gradient\n5. **Comparison**: Check that |numerical_grad - autodiff_grad| / max(|numerical_grad|, |autodiff_grad|, 1e-8) < tolerance\n\nThe relative error formula handles cases where gradients are near zero and provides scale-invariant comparison. A typical tolerance is 1e-5, though some operations may require looser tolerances due to numerical precision limitations.\n\n> **Key Insight**: Gradient checking is your mathematical safety net. Every operation you implement should pass gradient checking on multiple random inputs before you trust it in a real neural network. A single gradient error can make training fail mysteriously.\n\n**Property-based testing** verifies mathematical properties that should hold regardless of specific input values:\n\n| Property | Mathematical Rule | How to Test |\n|---|---|---|\n| **Linearity of Gradients** | ∇(af + bg) = a∇f + b∇g | Compute gradients of linear combinations vs combinations of gradients |\n| **Chain Rule** | ∇(f∘g) = (∇f)(g) ⊙ ∇g | Test composed operations vs manual composition |\n| **Product Rule** | ∇(fg) = f∇g + g∇f | Test element-wise multiplication gradients |\n| **Zero Gradient** | ∇c = 0 for constants | Verify constant tensors have zero gradients |\n| **Broadcasting Consistency** | Gradients unbroadcast correctly | Check gradient shapes match original parameter shapes |\n\n**Comprehensive test coverage** for gradient correctness includes:\n\n- **Elementary Operations**: Addition, multiplication, division, power functions with various input shapes and broadcasting patterns\n- **Transcendental Functions**: exp, log, sin, cos, tanh and their compositions\n- **Matrix Operations**: Matrix multiplication, transpose, reshape with different dimensions\n- **Reduction Operations**: Sum, mean, max with different axis specifications\n- **Composed Functions**: Complex expressions combining multiple operations to test chain rule implementation\n- **Edge Cases**: Zero gradients, very large/small values, boundary conditions\n\n> **Decision: Gradient Checking Integration Strategy**\n> - **Context**: Need to balance thorough gradient verification with reasonable test execution times\n> - **Options Considered**: \n>   1. Manual gradient checking for each operation\n>   2. Automated gradient checking in unit tests\n>   3. Optional gradient checking with environment flag\n> - **Decision**: Automated gradient checking in unit tests with configurable tolerance\n> - **Rationale**: Catches gradient bugs immediately during development while allowing fast iteration\n> - **Consequences**: Longer test execution time but much higher confidence in mathematical correctness\n\n### Milestone Verification Checkpoints\n\nEach milestone represents a major capability that must be thoroughly verified before proceeding to the next stage. Think of these checkpoints as **integration tests** that verify not just individual components but their interaction as a coherent system.\n\nMilestone verification goes beyond unit testing individual methods - it tests the **emergent behavior** that arises when components work together. For example, Milestone 2 tests whether the entire automatic differentiation pipeline (tensor operations + computation graph + backpropagation) produces correct gradients for realistic neural network computations.\n\n#### Milestone 1: Tensor & Operations Verification\n\nThe tensor operations milestone verification focuses on ensuring our tensor abstraction behaves identically to NumPy for forward computations while correctly tracking gradient metadata.\n\n**Core Tensor Functionality Tests:**\n\n| Test Category | Specific Tests | Expected Behavior | Failure Indicators |\n|---|---|---|---|\n| **Tensor Creation** | Various dtypes, shapes, requires_grad combinations | Tensors store data correctly, metadata accessible | Shape mismatches, dtype conversion errors |\n| **Arithmetic Operations** | +, -, *, / with scalars and tensors | Results match NumPy, grad_fn set correctly | Wrong results, missing gradient tracking |\n| **Broadcasting** | Mismatched shapes following NumPy rules | Automatic shape expansion, result shapes correct | Broadcasting failures, wrong output shapes |\n| **Matrix Operations** | 2D and batched matrix multiplication | Correct matrix multiply results, batch handling | Dimension errors, wrong batch semantics |\n| **Shape Operations** | Reshape, transpose, indexing | Shape transformations work, data preserved | Data corruption, shape inconsistencies |\n\n**Verification Procedure for Milestone 1:**\n\n1. **NumPy Equivalence Testing**: For every tensor operation, create equivalent NumPy arrays and verify our results match NumPy exactly (within floating-point precision)\n2. **Gradient Tracking Verification**: Ensure operations on `requires_grad=True` tensors create appropriate `grad_fn` references\n3. **Broadcasting Edge Cases**: Test all NumPy broadcasting patterns including scalar expansion, dimension addition, and size-1 expansion\n4. **Memory Layout Testing**: Verify tensors maintain consistent memory layout and shape metadata after operations\n5. **Error Handling**: Confirm operations fail gracefully with informative error messages for incompatible shapes\n\n**Milestone 1 Checkpoint Criteria:**\n\n```python\n# After implementing Milestone 1, these operations should work perfectly:\na = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\nb = Tensor([10.0, 20.0], requires_grad=True)\nc = a + b  # Broadcasting: (2,2) + (2,) -> (2,2)\nd = a.matmul(a.T)  # Matrix multiply with transpose\ne = c * d  # Element-wise multiply\n\n# All results should match NumPy exactly\n# All tensors should have appropriate grad_fn references\n# No memory leaks or shape inconsistencies\n```\n\n#### Milestone 2: Automatic Differentiation Verification\n\nAutomatic differentiation verification ensures our gradient computation infrastructure produces mathematically correct results for increasingly complex computation graphs.\n\n**Gradient Computation Tests:**\n\n| Test Category | Specific Tests | Expected Behavior | Failure Indicators |\n|---|---|---|---|\n| **Simple Operations** | Gradients of x², x+y, x*y | Match analytical derivatives exactly | Wrong gradient values |\n| **Chain Rule** | Gradients of f(g(x)) compositions | Proper chain rule application | Missing gradient contributions |\n| **Multiple Paths** | Same tensor used multiple times | Gradient accumulation works correctly | Missing accumulated gradients |\n| **Complex Graphs** | Deep computation trees | Correct topological ordering | Wrong gradient ordering |\n| **Zero Gradients** | Constants and non-differentiable paths | Zero gradients where expected | Non-zero gradients for constants |\n\n**Verification Procedure for Milestone 2:**\n\n1. **Gradient Checking**: Every operation passes numerical differentiation comparison with tolerance 1e-5\n2. **Computation Graph Inspection**: Verify graph structure matches expected topology for complex expressions\n3. **Gradient Accumulation**: Test expressions like y = x + x to ensure gradient accumulation sums correctly\n4. **Topological Ordering**: Manually verify backward pass processes nodes in correct dependency order\n5. **Memory Management**: Ensure computation graphs can be garbage collected after backward pass\n\n**Progressive Complexity Testing** builds confidence by starting with simple cases and adding complexity:\n\n| Complexity Level | Example Expression | Key Test Focus |\n|---|---|---|\n| **Linear** | y = 3*x + 2 | Basic gradient computation and constants |\n| **Quadratic** | y = x² + 2*x + 1 | Power rule and addition |\n| **Composition** | y = sin(x²) | Chain rule through transcendental functions |\n| **Multi-variable** | z = x*y + x² | Multiple inputs and gradient accumulation |\n| **Deep Trees** | ((x*2)+3)*((x+1)*4) | Complex computation graphs |\n\n**Milestone 2 Checkpoint Criteria:**\n\n```python\n# After implementing Milestone 2, gradient computation should work:\nx = Tensor([2.0], requires_grad=True)\ny = Tensor([3.0], requires_grad=True)\nz = x**2 + 2*x*y + y**2  # Complex expression\nz.backward()\n\n# x.grad and y.grad should match analytical derivatives\n# Numerical gradient checking should pass with tolerance 1e-5\n# Computation graph should be properly formed and traversed\n```\n\n#### Milestone 3: Neural Network Modules Verification\n\nModule system verification ensures our building blocks compose correctly and manage parameters properly for realistic neural networks.\n\n**Module System Tests:**\n\n| Test Category | Specific Tests | Expected Behavior | Failure Indicators |\n|---|---|---|\n| **Parameter Registration** | parameters() collects all trainable tensors | All parameters found recursively | Missing parameters |\n| **Module Composition** | Sequential chains modules correctly | Data flows through pipeline | Wrong data flow |\n| **Forward Pass** | Linear layers and activations work | Correct output shapes and values | Shape errors, wrong computations |\n| **Training/Eval Mode** | Behavior changes appropriately | Mode affects relevant modules | Mode switching ignored |\n| **Parameter Initialization** | Weights initialized reasonably | No NaN/inf values, appropriate scales | Poor initialization |\n\n**Verification Procedure for Milestone 3:**\n\n1. **Parameter Collection**: Build nested module hierarchies and verify `parameters()` finds all trainable tensors\n2. **Forward Pass Testing**: Create known input/output pairs for Linear layers and verify exact computation\n3. **Activation Function Verification**: Test activation functions against analytical derivatives\n4. **Module Composition**: Build `Sequential` containers and verify data flows correctly\n5. **Gradient Flow**: Ensure gradients flow backward through entire module hierarchies\n\n**Neural Network Integration Tests:**\n\n| Network Architecture | Purpose | Key Verification Points |\n|---|---|---|\n| **Single Linear Layer** | Basic parameter handling | Weight shapes, bias handling, parameter collection |\n| **Multi-Layer Perceptron** | Sequential composition | Data flow, gradient flow, parameter counting |\n| **Branched Network** | Complex module graphs | Multiple paths, gradient accumulation |\n| **Nested Modules** | Hierarchical organization | Recursive parameter collection, naming |\n\n**Milestone 3 Checkpoint Criteria:**\n\n```python\n# After implementing Milestone 3, module system should work:\nmodel = Sequential([\n    Linear(784, 128),\n    ReLU(),\n    Linear(128, 10)\n])\n\nx = Tensor(np.random.randn(32, 784), requires_grad=True)\ny = model(x)  # Forward pass works\nassert y.shape == (32, 10)  # Correct output shape\nassert len(list(model.parameters())) == 4  # Two weight + bias pairs\n```\n\n#### Milestone 4: Optimizers & Training Verification\n\nTraining system verification ensures the complete training loop produces learning behavior on simple, well-understood problems.\n\n**Optimizer Verification Tests:**\n\n| Test Category | Specific Tests | Expected Behavior | Failure Indicators |\n|---|---|---|\n| **Parameter Updates** | SGD and Adam update rules | Parameters change in gradient direction | No updates, wrong directions |\n| **Learning Rate Effects** | Different learning rates | Higher rates = bigger updates | Learning rate ignored |\n| **Momentum Behavior** | SGD with momentum | Velocity accumulation works | No momentum effect |\n| **Adam Convergence** | Adaptive learning rates | Faster convergence than SGD | No adaptive behavior |\n| **Zero Grad** | Gradient clearing | Previous gradients don't accumulate | Gradient accumulation errors |\n\n**Training Loop Integration Tests:**\n\n| Test Scenario | Purpose | Success Criteria | Failure Indicators |\n|---|---|---|\n| **Overfitting Small Dataset** | Basic training functionality | Loss decreases monotonically | Loss doesn't decrease |\n| **Linear Regression** | Known optimal solution | Converges to analytical solution | Wrong convergence |\n| **XOR Problem** | Nonlinear learning | Solves XOR with small MLP | Can't learn XOR |\n| **Classification** | Multi-class learning | Achieves reasonable accuracy | Random performance |\n\n**Milestone 4 Checkpoint Criteria:**\n\n```python\n# After implementing Milestone 4, training should work:\nmodel = Sequential([Linear(2, 10), ReLU(), Linear(10, 1)])\noptimizer = SGD(model.parameters(), lr=0.01)\nloss_fn = MSELoss()\n\n# Training loop should reduce loss on simple regression problem\nfor epoch in range(100):\n    optimizer.zero_grad()\n    predictions = model(x_train)\n    loss = loss_fn(predictions, y_train)\n    loss.backward()\n    optimizer.step()\n    # Loss should generally decrease over time\n```\n\n> **Decision: Milestone Verification Strategy**\n> - **Context**: Need to ensure each milestone is solid before building on it\n> - **Options Considered**: \n>   1. Unit tests only for each component\n>   2. Integration tests verifying milestone completion\n>   3. Manual testing with example problems\n> - **Decision**: Combination of automated integration tests plus manual verification examples\n> - **Rationale**: Automated tests catch regressions, manual examples build intuition and catch emergent issues\n> - **Consequences**: More thorough verification but requires maintaining both test suites and examples\n\n### End-to-End Training Tests\n\nEnd-to-end training tests verify that our entire framework integrates correctly by training complete neural networks on well-understood problems with known expected behaviors. Think of these tests as **clinical trials** for our neural network framework - controlled experiments that demonstrate the framework works for its intended purpose.\n\nUnlike unit tests that verify individual components or milestone tests that verify major subsystems, end-to-end tests verify **emergent behavior** that arises from the complete integration. These tests catch integration bugs that might not appear in isolated component testing but manifest when all systems work together during real training scenarios.\n\nThe key principle is testing on **toy problems with known solutions**. We avoid complex datasets like ImageNet and instead use carefully constructed problems where we can predict expected behavior, convergence rates, and final accuracy. This allows us to distinguish between framework bugs and normal machine learning challenges.\n\n#### Toy Dataset Design Principles\n\nEffective end-to-end testing requires carefully designed toy problems that exercise different aspects of the neural network training process:\n\n| Problem Type | Mathematical Structure | What It Tests | Expected Behavior |\n|---|---|---|---|\n| **Linear Regression** | y = Wx + b with Gaussian noise | Basic gradient flow, convex optimization | Converges to analytical least-squares solution |\n| **Polynomial Fitting** | y = x³ + 2x² - x + noise | Nonlinear fitting, overfitting detection | Can fit training data, generalizes reasonably |\n| **XOR Problem** | Classic non-linearly separable classification | Nonlinear learning, hidden layer necessity | 100% accuracy with small MLP |\n| **Spiral Dataset** | Two-class spiral pattern | Complex decision boundaries | Smooth decision boundaries, good accuracy |\n| **MNIST Subset** | 10 examples per digit, 100 total | Real data, multiclass classification | High accuracy on small dataset |\n\n**Synthetic dataset generation** ensures reproducible, controllable test conditions:\n\n```python\n# Linear regression: y = 3*x + 2 + noise\n# Should converge to weights ≈ [3] and bias ≈ 2\nX_linear = np.random.randn(100, 1)\ny_linear = 3 * X_linear + 2 + 0.1 * np.random.randn(100, 1)\n\n# XOR problem: classic test of nonlinear learning\n# Perfect solution exists with small MLP\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_xor = np.array([[0], [1], [1], [0]])  # XOR truth table\n```\n\n#### Training Convergence Verification\n\nEnd-to-end tests must verify that training exhibits expected convergence behavior rather than just checking final accuracy. This catches subtle bugs in optimization or gradient computation that might allow eventual convergence but with poor efficiency.\n\n**Convergence Pattern Analysis:**\n\n| Metric | Expected Pattern | Measurement Method | Failure Indicators |\n|---|---|---|\n| **Loss Trajectory** | Monotonic decrease (with noise) | Track loss every epoch | Loss increases, plateaus early |\n| **Gradient Norms** | Start large, decrease over time | L2 norm of parameter gradients | Constant gradients, exploding norms |\n| **Parameter Changes** | Large initially, smaller as converged | L2 norm of parameter updates | No parameter movement |\n| **Learning Rate Sensitivity** | Higher LR = faster initial progress | Compare different learning rates | No LR effect |\n| **Convergence Speed** | Reach target accuracy in expected epochs | Epochs to achieve threshold accuracy | Too slow or too fast convergence |\n\n**Statistical Validation** ensures training behavior is consistent across multiple runs:\n\n- **Multiple Random Seeds**: Run same experiment with 5+ different random seeds to verify consistent convergence\n- **Convergence Statistics**: Measure mean and variance of final loss/accuracy across runs\n- **Outlier Detection**: Flag runs that converge unusually slowly or to poor solutions\n- **Learning Curve Analysis**: Verify learning curves have expected shape (rapid initial improvement, then saturation)\n\n#### Framework Integration Testing\n\nEnd-to-end tests specifically target integration points where different framework components must coordinate correctly:\n\n**Gradient Flow Integration:**\n- Verify gradients flow correctly from loss function through entire network back to input layers\n- Test gradient accumulation when same parameter appears in multiple computational paths\n- Confirm gradient magnitudes are reasonable (not vanishing or exploding) throughout network\n\n**Memory Management Integration:**\n- Ensure computation graphs are properly cleaned up after backward pass\n- Verify no memory leaks during extended training loops\n- Test behavior when training very deep networks that create large computation graphs\n\n**Optimizer Integration:**\n- Confirm optimizers correctly access and update parameters from complex module hierarchies\n- Verify learning rate scheduling affects actual parameter updates\n- Test momentum and adaptive learning rate features work correctly during real training\n\n**Training Loop Coordination:**\n- Verify forward pass, loss computation, backward pass, and parameter update sequence works correctly\n- Test batch processing with different batch sizes\n- Confirm training/evaluation mode switching affects network behavior appropriately\n\n#### Specific End-to-End Test Cases\n\n**Linear Regression Convergence Test:**\n\nThis test verifies basic gradient flow and convex optimization:\n\n1. **Problem Setup**: Generate y = 3x + 2 + noise with known ground truth\n2. **Model**: Single Linear layer (input=1, output=1)\n3. **Expected Behavior**: Should converge to weight ≈ 3, bias ≈ 2 within 100 epochs\n4. **Verification**: Final parameters within 10% of ground truth, loss < 0.01\n5. **Failure Detection**: Parameters don't converge, loss plateaus too high, training diverges\n\n**XOR Classification Test:**\n\nThis test verifies nonlinear learning capability:\n\n1. **Problem Setup**: 4-point XOR dataset with perfect separability\n2. **Model**: MLP with hidden layer (2→4→1 with ReLU)\n3. **Expected Behavior**: 100% training accuracy within 1000 epochs\n4. **Verification**: All 4 XOR points classified correctly\n5. **Failure Detection**: Can't achieve perfect accuracy, training stalls\n\n**Overfitting Detection Test:**\n\nThis test verifies the framework can overfit (confirming it can learn):\n\n1. **Problem Setup**: Complex function with small training set (10 points)\n2. **Model**: Overparameterized MLP (much larger than needed)\n3. **Expected Behavior**: Perfect training accuracy, poor validation accuracy\n4. **Verification**: Training loss → 0, validation loss increases\n5. **Failure Detection**: Can't fit training data, no overfitting observed\n\n**Multi-Epoch Training Stability:**\n\nThis test verifies long-term training stability:\n\n1. **Problem Setup**: Medium-sized dataset requiring extended training\n2. **Model**: Standard MLP architecture\n3. **Expected Behavior**: Stable training for 1000+ epochs without crashes\n4. **Verification**: No memory leaks, consistent convergence, no NaN values\n5. **Failure Detection**: Memory usage grows, training becomes unstable, numerical issues\n\n#### Automated Test Infrastructure\n\nEnd-to-end tests require infrastructure to run automatically and report meaningful failures:\n\n**Test Execution Framework:**\n\n| Component | Purpose | Implementation |\n|---|---|---|\n| **Dataset Generation** | Create reproducible toy datasets | Fixed random seeds, parameterized problem generators |\n| **Training Orchestration** | Run complete training loops | Configurable epochs, learning rates, architectures |\n| **Metrics Collection** | Track convergence behavior | Loss history, gradient norms, parameter evolution |\n| **Statistical Analysis** | Verify expected behavior patterns | Hypothesis tests for convergence, outlier detection |\n| **Failure Diagnosis** | Identify root cause of failures | Detailed logging, intermediate state inspection |\n\n**Performance Benchmarking:**\n\nWhile not focused on production performance, end-to-end tests should verify reasonable performance characteristics:\n\n- **Training Speed**: Verify training completes in reasonable time (not 100x slower than PyTorch)\n- **Memory Usage**: Ensure memory usage scales appropriately with model size\n- **Convergence Efficiency**: Compare epochs-to-convergence with reference implementations\n\n> **Decision: End-to-End Test Selection**\n> - **Context**: Need comprehensive integration testing without excessive test suite complexity\n> - **Options Considered**: \n>   1. Single complex dataset (like MNIST)\n>   2. Multiple toy problems testing different aspects\n>   3. Random problem generation\n> - **Decision**: Curated set of toy problems with known expected behaviors\n> - **Rationale**: Toy problems allow precise verification of expected behavior while testing different framework aspects\n> - **Consequences**: More test maintenance but much clearer failure diagnosis and root cause identification\n\n**Common End-to-End Testing Pitfalls:**\n\n⚠️ **Pitfall: Using Complex Datasets Too Early**\nMany learners jump to MNIST or CIFAR-10 for end-to-end testing. This makes it impossible to distinguish between framework bugs and normal machine learning challenges. A framework bug that causes 10% accuracy loss might be masked by dataset difficulty, making debugging extremely difficult.\n\n⚠️ **Pitfall: Not Testing Convergence Patterns**\nTesting only final accuracy misses important bugs in optimization or gradient flow. A bug that makes training 10x slower but eventually converges might go unnoticed, leading to poor user experience.\n\n⚠️ **Pitfall: Ignoring Statistical Variation**\nNeural network training has inherent randomness. Testing with a single random seed can miss intermittent failures or give false confidence. Always test with multiple seeds and analyze statistical patterns.\n\n⚠️ **Pitfall: No Baseline Comparison**\nWithout comparing against known-good implementations (like PyTorch), it's hard to know if poor performance indicates bugs or is simply expected. Implement the same problem in PyTorch as a reference.\n\n### Implementation Guidance\n\nThis section provides practical infrastructure for implementing comprehensive testing of your neural network framework. The testing code is as important as the framework code itself - mathematical correctness is non-negotiable.\n\n#### Technology Recommendations\n\n| Testing Component | Simple Option | Advanced Option |\n|---|---|---|\n| **Test Framework** | Python unittest (built-in) | pytest with fixtures and parameterization |\n| **Numerical Computing** | NumPy for reference implementations | SciPy for advanced numerical methods |\n| **Gradient Checking** | Custom finite difference implementation | AutoGrad for reference gradients |\n| **Statistical Testing** | Manual mean/std analysis | scipy.stats for hypothesis testing |\n| **Visualization** | matplotlib for loss curves | tensorboard-style logging |\n| **Performance Testing** | time.time() for basic timing | cProfile for detailed performance analysis |\n\n#### Recommended Testing Structure\n\n```\nneural_framework/\n  tests/\n    test_gradients/\n      test_gradient_checking.py     ← numerical differentiation vs autodiff\n      test_operation_gradients.py   ← individual operation gradient tests\n      test_complex_expressions.py   ← composed function gradient tests\n    test_milestones/\n      test_milestone1_tensors.py    ← tensor operations verification\n      test_milestone2_autodiff.py   ← automatic differentiation verification\n      test_milestone3_modules.py    ← neural network modules verification\n      test_milestone4_training.py   ← optimizers and training verification\n    test_integration/\n      test_end_to_end.py           ← complete training on toy problems\n      toy_datasets.py              ← synthetic dataset generation\n      reference_solutions.py       ← known correct solutions for comparison\n    conftest.py                    ← pytest fixtures and test utilities\n    test_utils.py                  ← testing helper functions\n```\n\n#### Core Testing Infrastructure (Complete Implementation)\n\n**Gradient Checking Utilities:**\n\n```python\nimport numpy as np\nfrom typing import Callable, List, Tuple, Optional\nimport warnings\n\ndef numerical_gradient(f: Callable, inputs: List[np.ndarray], h: float = 1e-5) -> List[np.ndarray]:\n    \"\"\"\n    Compute numerical gradients using central difference approximation.\n    \n    Args:\n        f: Function that takes list of arrays, returns scalar\n        inputs: List of input arrays to differentiate w.r.t\n        h: Step size for finite differences\n        \n    Returns:\n        List of gradient arrays, same shapes as inputs\n    \"\"\"\n    gradients = []\n    \n    for i, x in enumerate(inputs):\n        grad = np.zeros_like(x)\n        \n        # Flatten for easier iteration\n        x_flat = x.flatten()\n        grad_flat = grad.flatten()\n        \n        for j in range(len(x_flat)):\n            # Create perturbed versions\n            x_plus = x_flat.copy()\n            x_minus = x_flat.copy()\n            x_plus[j] += h\n            x_minus[j] -= h\n            \n            # Reconstruct input lists with perturbations\n            inputs_plus = inputs.copy()\n            inputs_minus = inputs.copy()\n            inputs_plus[i] = x_plus.reshape(x.shape)\n            inputs_minus[i] = x_minus.reshape(x.shape)\n            \n            # Central difference approximation\n            f_plus = f(inputs_plus)\n            f_minus = f(inputs_minus)\n            grad_flat[j] = (f_plus - f_minus) / (2 * h)\n        \n        gradients.append(grad.reshape(x.shape))\n    \n    return gradients\n\ndef relative_error(a: np.ndarray, b: np.ndarray, eps: float = 1e-8) -> float:\n    \"\"\"Compute relative error between two arrays.\"\"\"\n    numerator = np.abs(a - b)\n    denominator = np.maximum(np.abs(a), np.abs(b), eps)\n    return np.max(numerator / denominator)\n\ndef check_gradients(\n    f: Callable, \n    inputs: List[np.ndarray], \n    autodiff_grads: List[np.ndarray],\n    tolerance: float = 1e-5,\n    h: float = 1e-5\n) -> Tuple[bool, str]:\n    \"\"\"\n    Compare autodiff gradients against numerical gradients.\n    \n    Returns:\n        (passed, error_message): Test result and diagnostic info\n    \"\"\"\n    try:\n        numerical_grads = numerical_gradient(f, inputs, h)\n        \n        if len(numerical_grads) != len(autodiff_grads):\n            return False, f\"Gradient count mismatch: {len(numerical_grads)} vs {len(autodiff_grads)}\"\n        \n        for i, (num_grad, auto_grad) in enumerate(zip(numerical_grads, autodiff_grads)):\n            if num_grad.shape != auto_grad.shape:\n                return False, f\"Gradient {i} shape mismatch: {num_grad.shape} vs {auto_grad.shape}\"\n            \n            rel_err = relative_error(num_grad, auto_grad)\n            if rel_err > tolerance:\n                return False, (\n                    f\"Gradient {i} relative error {rel_err:.2e} > tolerance {tolerance:.2e}\\n\"\n                    f\"Numerical: {num_grad.flatten()[:5]}\\n\"\n                    f\"Autodiff:  {auto_grad.flatten()[:5]}\"\n                )\n        \n        return True, \"Gradient check passed\"\n        \n    except Exception as e:\n        return False, f\"Gradient checking failed with exception: {e}\"\n\nclass GradientTester:\n    \"\"\"Helper class for systematic gradient testing.\"\"\"\n    \n    def __init__(self, tolerance: float = 1e-5):\n        self.tolerance = tolerance\n        self.test_results = []\n    \n    def test_operation(self, operation_name: str, test_function: Callable) -> bool:\n        \"\"\"Test gradients for a specific operation with multiple random inputs.\"\"\"\n        passed_tests = 0\n        total_tests = 5  # Test with 5 random inputs\n        \n        for test_idx in range(total_tests):\n            try:\n                # Generate random test inputs (implementation specific)\n                inputs, expected_output, autodiff_grads = test_function()\n                \n                # Define scalar function for gradient checking\n                def scalar_func(input_list):\n                    # This would call your framework's forward pass\n                    # and return a scalar (e.g., sum of output)\n                    result = your_framework_forward(input_list)\n                    return np.sum(result)\n                \n                passed, message = check_gradients(scalar_func, inputs, autodiff_grads, self.tolerance)\n                \n                if passed:\n                    passed_tests += 1\n                else:\n                    print(f\"FAIL {operation_name} test {test_idx}: {message}\")\n                    \n            except Exception as e:\n                print(f\"ERROR {operation_name} test {test_idx}: {e}\")\n        \n        success_rate = passed_tests / total_tests\n        self.test_results.append((operation_name, success_rate))\n        \n        return success_rate >= 0.8  # Require 80% success rate\n    \n    def summary_report(self) -> str:\n        \"\"\"Generate summary of all gradient tests.\"\"\"\n        report = \"\\nGradient Test Summary:\\n\" + \"=\"*50 + \"\\n\"\n        \n        for op_name, success_rate in self.test_results:\n            status = \"PASS\" if success_rate >= 0.8 else \"FAIL\"\n            report += f\"{status}: {op_name:20} ({success_rate:.1%} success rate)\\n\"\n        \n        overall_success = sum(rate for _, rate in self.test_results) / len(self.test_results)\n        report += f\"\\nOverall Success Rate: {overall_success:.1%}\\n\"\n        \n        return report\n```\n\n**Toy Dataset Generation:**\n\n```python\nimport numpy as np\nfrom typing import Tuple\n\nclass ToyDatasets:\n    \"\"\"Generate synthetic datasets for end-to-end testing.\"\"\"\n    \n    @staticmethod\n    def linear_regression(n_samples: int = 100, noise: float = 0.1, seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate linear regression problem: y = 3*x + 2 + noise.\"\"\"\n        np.random.seed(seed)\n        X = np.random.randn(n_samples, 1)\n        y = 3.0 * X + 2.0 + noise * np.random.randn(n_samples, 1)\n        return X, y\n    \n    @staticmethod\n    def polynomial_regression(n_samples: int = 100, noise: float = 0.1, seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate polynomial regression: y = x^3 + 2*x^2 - x + noise.\"\"\"\n        np.random.seed(seed)\n        X = np.random.uniform(-2, 2, (n_samples, 1))\n        y = X**3 + 2*X**2 - X + noise * np.random.randn(n_samples, 1)\n        return X, y\n    \n    @staticmethod\n    def xor_problem() -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate XOR dataset.\"\"\"\n        X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n        y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n        return X, y\n    \n    @staticmethod\n    def spiral_dataset(n_points: int = 100, noise: float = 0.1, seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate two-class spiral dataset.\"\"\"\n        np.random.seed(seed)\n        N = n_points // 2\n        \n        # Generate spiral arms\n        theta = np.linspace(0, 4*np.pi, N)\n        r = np.linspace(0.1, 1, N)\n        \n        # Class 0 spiral\n        x0 = r * np.cos(theta) + noise * np.random.randn(N)\n        y0 = r * np.sin(theta) + noise * np.random.randn(N)\n        \n        # Class 1 spiral (rotated)\n        x1 = r * np.cos(theta + np.pi) + noise * np.random.randn(N)\n        y1 = r * np.sin(theta + np.pi) + noise * np.random.randn(N)\n        \n        # Combine\n        X = np.column_stack([\n            np.concatenate([x0, x1]),\n            np.concatenate([y0, y1])\n        ])\n        y = np.concatenate([np.zeros(N), np.ones(N)]).reshape(-1, 1)\n        \n        return X, y\n    \n    @staticmethod\n    def classification_grid(n_per_class: int = 50, n_classes: int = 3, seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate multi-class classification with Gaussian clusters.\"\"\"\n        np.random.seed(seed)\n        \n        X_list = []\n        y_list = []\n        \n        # Place class centers in a grid\n        centers = [(i, j) for i in range(n_classes) for j in range(n_classes)][:n_classes]\n        \n        for class_idx, (cx, cy) in enumerate(centers):\n            # Generate points around center\n            X_class = np.random.randn(n_per_class, 2) * 0.5 + np.array([cx, cy])\n            y_class = np.full((n_per_class,), class_idx)\n            \n            X_list.append(X_class)\n            y_list.append(y_class)\n        \n        X = np.vstack(X_list)\n        y = np.concatenate(y_list).reshape(-1, 1)\n        \n        return X, y\n\ndef expected_linear_regression_solution(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute analytical least squares solution for linear regression.\"\"\"\n    # Add bias column to X\n    X_with_bias = np.column_stack([X, np.ones(X.shape[0])])\n    \n    # Analytical solution: θ = (X^T X)^-1 X^T y\n    theta = np.linalg.solve(X_with_bias.T @ X_with_bias, X_with_bias.T @ y)\n    \n    weight = theta[:-1]\n    bias = theta[-1:]\n    \n    return weight, bias\n```\n\n**End-to-End Test Framework:**\n\n```python\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Callable\n\n@dataclass\nclass TrainingResult:\n    \"\"\"Results from training run for analysis.\"\"\"\n    final_loss: float\n    final_accuracy: float\n    loss_history: List[float]\n    convergence_epoch: Optional[int]  # Epoch when target accuracy reached\n    training_time: float\n    \nclass EndToEndTester:\n    \"\"\"Framework for end-to-end integration testing.\"\"\"\n    \n    def __init__(self, tolerance: Dict[str, float] = None):\n        self.tolerance = tolerance or {\n            'loss': 0.01,\n            'accuracy': 0.9,\n            'convergence_epochs': 1000\n        }\n        self.results = {}\n    \n    def test_linear_regression_convergence(self, framework_classes) -> bool:\n        \"\"\"Test linear regression convergence to analytical solution.\"\"\"\n        # TODO 1: Generate linear regression dataset using ToyDatasets.linear_regression()\n        # TODO 2: Create single Linear layer model using your framework\n        # TODO 3: Create MSE loss function and SGD optimizer\n        # TODO 4: Train for 500 epochs, collecting loss history\n        # TODO 5: Compare final parameters to analytical solution within 10% tolerance\n        # TODO 6: Verify loss decreased monotonically (allowing some noise)\n        # TODO 7: Return True if all checks pass\n        pass\n    \n    def test_xor_classification(self, framework_classes) -> bool:\n        \"\"\"Test XOR problem solving with small MLP.\"\"\"\n        # TODO 1: Get XOR dataset using ToyDatasets.xor_problem()\n        # TODO 2: Create MLP model: Linear(2, 4) -> ReLU -> Linear(4, 1) -> Sigmoid\n        # TODO 3: Create binary cross-entropy loss and Adam optimizer\n        # TODO 4: Train until perfect accuracy or 2000 epochs max\n        # TODO 5: Verify all 4 XOR points classified correctly (>0.5 threshold)\n        # TODO 6: Return True if perfect classification achieved\n        pass\n    \n    def test_overfitting_capability(self, framework_classes) -> bool:\n        \"\"\"Test framework can overfit small dataset (confirms learning works).\"\"\"\n        # TODO 1: Generate small polynomial dataset (10 points) with complex function\n        # TODO 2: Create overparameterized model (much larger than needed)\n        # TODO 3: Train until training loss approaches zero\n        # TODO 4: Verify training accuracy reaches 100% while validation accuracy is poor\n        # TODO 5: This confirms the framework can learn (overfitting is expected)\n        pass\n    \n    def test_training_stability(self, framework_classes) -> bool:\n        \"\"\"Test extended training without crashes or instabilities.\"\"\"\n        # TODO 1: Create medium-size dataset and model\n        # TODO 2: Train for 2000+ epochs\n        # TODO 3: Monitor for NaN/inf values in loss or parameters\n        # TODO 4: Check memory usage doesn't grow unboundedly\n        # TODO 5: Verify training remains stable throughout\n        pass\n    \n    def run_statistical_validation(self, test_function: Callable, n_runs: int = 5) -> Dict:\n        \"\"\"Run test multiple times with different seeds for statistical validation.\"\"\"\n        results = []\n        \n        for seed in range(n_runs):\n            np.random.seed(seed)\n            # Set your framework's random seed here too\n            \n            start_time = time.time()\n            result = test_function()\n            end_time = time.time()\n            \n            results.append({\n                'passed': result,\n                'time': end_time - start_time,\n                'seed': seed\n            })\n        \n        # Analyze results\n        success_rate = sum(1 for r in results if r['passed']) / len(results)\n        avg_time = np.mean([r['time'] for r in results])\n        \n        return {\n            'success_rate': success_rate,\n            'average_time': avg_time,\n            'individual_results': results,\n            'passed_overall': success_rate >= 0.8  # Require 80% success\n        }\n```\n\n**Milestone Checkpoint Templates:**\n\n```python\nclass MilestoneCheckpoints:\n    \"\"\"Automated verification for each milestone completion.\"\"\"\n    \n    @staticmethod\n    def verify_milestone_1(tensor_class, operation_classes):\n        \"\"\"Verify tensor operations work correctly.\"\"\"\n        print(\"Verifying Milestone 1: Tensor & Operations\")\n        \n        # TODO 1: Test tensor creation with various shapes and dtypes\n        # TODO 2: Verify arithmetic operations match NumPy exactly\n        # TODO 3: Test broadcasting follows NumPy rules\n        # TODO 4: Verify matrix multiplication works for 2D and batched\n        # TODO 5: Check requires_grad and grad_fn attributes set correctly\n        # TODO 6: Test error handling for incompatible operations\n        \n        # Expected: All tensor operations work identically to NumPy\n        # Expected: Gradient tracking metadata is properly maintained\n        # Expected: Broadcasting and shape errors provide clear messages\n        pass\n    \n    @staticmethod\n    def verify_milestone_2(tensor_class, operation_classes):\n        \"\"\"Verify automatic differentiation produces correct gradients.\"\"\"\n        print(\"Verifying Milestone 2: Automatic Differentiation\")\n        \n        # TODO 1: Test gradients of simple operations (x^2, x+y, x*y)\n        # TODO 2: Verify chain rule works for composed functions\n        # TODO 3: Test gradient accumulation for expressions like x+x\n        # TODO 4: Check topological sort produces correct ordering\n        # TODO 5: Run comprehensive gradient checking on all operations\n        # TODO 6: Test backward pass on complex computation graphs\n        \n        # Expected: All gradients pass numerical differentiation comparison\n        # Expected: Complex expressions compute gradients correctly\n        # Expected: Gradient accumulation works for shared tensors\n        pass\n    \n    @staticmethod\n    def verify_milestone_3(module_classes):\n        \"\"\"Verify neural network module system works.\"\"\"\n        print(\"Verifying Milestone 3: Neural Network Modules\")\n        \n        # TODO 1: Test parameter registration and collection\n        # TODO 2: Verify Linear layer forward pass and gradients\n        # TODO 3: Test activation functions and their derivatives\n        # TODO 4: Check Sequential module chains operations correctly\n        # TODO 5: Verify training/eval mode switching\n        # TODO 6: Test nested module parameter collection\n        \n        # Expected: All modules compose correctly\n        # Expected: Parameters are tracked and accessible\n        # Expected: Forward and backward passes work through module hierarchies\n        pass\n    \n    @staticmethod\n    def verify_milestone_4(optimizer_classes, loss_classes):\n        \"\"\"Verify optimizers and training loop work.\"\"\"\n        print(\"Verifying Milestone 4: Optimizers & Training\")\n        \n        # TODO 1: Test SGD parameter updates follow gradient descent\n        # TODO 2: Verify Adam optimizer convergence behavior\n        # TODO 3: Test learning rate effects on convergence speed\n        # TODO 4: Check gradient clearing between training steps\n        # TODO 5: Verify loss functions compute correct gradients\n        # TODO 6: Test complete training loop integration\n        \n        # Expected: Optimizers update parameters correctly\n        # Expected: Training loop reduces loss on simple problems\n        # Expected: Different optimizers show expected convergence patterns\n        pass\n\n# Usage example for learners:\nif __name__ == \"__main__\":\n    # After implementing each milestone, run verification\n    from your_framework import Tensor, Linear, Sequential, ReLU, SGD, Adam\n    \n    # Verify milestone completion\n    MilestoneCheckpoints.verify_milestone_1(Tensor, [Add, Multiply, MatMul])\n    MilestoneCheckpoints.verify_milestone_2(Tensor, [Add, Multiply, MatMul])\n    MilestoneCheckpoints.verify_milestone_3([Linear, ReLU, Sequential])\n    MilestoneCheckpoints.verify_milestone_4([SGD, Adam], [MSELoss])\n    \n    # Run end-to-end tests\n    tester = EndToEndTester()\n    framework_classes = {'Tensor': Tensor, 'Linear': Linear, 'SGD': SGD}\n    \n    print(\"Running end-to-end integration tests...\")\n    linear_test = tester.test_linear_regression_convergence(framework_classes)\n    xor_test = tester.test_xor_classification(framework_classes)\n    \n    print(f\"Linear regression test: {'PASS' if linear_test else 'FAIL'}\")\n    print(f\"XOR classification test: {'PASS' if xor_test else 'FAIL'}\")\n```\n\nThis testing infrastructure provides comprehensive verification of your neural network framework from individual operations through complete training scenarios. The gradient checking utilities are particularly crucial - they provide mathematical ground truth that's impossible to achieve through manual inspection.\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones - debugging is essential throughout tensor operations, automatic differentiation, neural modules, and training loops to ensure correctness and identify implementation problems\n\nBuilding a neural network framework from scratch introduces numerous opportunities for subtle bugs that can be challenging to diagnose. Unlike high-level application bugs that often have obvious symptoms, framework-level bugs frequently manifest as mysterious numerical behavior, silent gradient computation failures, or training that simply doesn't converge as expected. This section provides systematic approaches to identify, diagnose, and fix the most common categories of bugs that learners encounter when implementing their own automatic differentiation system.\n\nThink of debugging a neural network framework like being a detective investigating a crime scene where the evidence is often numerical rather than visual. The \"crime\" might be gradients that vanish mysteriously, shapes that don't align properly, or training loops that appear to run successfully but never actually learn. Unlike debugging a web application where you can inspect HTTP requests and database queries, framework debugging requires understanding the mathematical relationships between tensors, the structure of computation graphs, and the flow of gradients through complex networks.\n\nThe debugging process for neural network frameworks follows a layered approach that mirrors the four-layer architecture described earlier. Tensor operation bugs typically manifest as shape mismatches or incorrect numerical results that can be caught with targeted unit tests. Automatic differentiation bugs are more insidious - they often allow forward passes to succeed while silently corrupting gradient computation, leading to training that appears to run but doesn't converge. Neural module bugs frequently involve parameter registration failures or incorrect weight initialization that prevents effective learning. Training loop bugs encompass optimizer configuration issues, learning rate problems, and loss function mistakes that can cause oscillating behavior or premature convergence to poor solutions.\n\n### Gradient Computation Issues\n\nGradient computation problems represent the most challenging category of bugs in neural network frameworks because they often produce no immediate error messages while completely undermining the learning process. These bugs can be particularly frustrating because the forward pass typically works perfectly, producing reasonable predictions, while the backward pass silently computes incorrect gradients that prevent the network from improving during training.\n\nThe most fundamental tool for diagnosing gradient computation issues is **gradient validation** using numerical differentiation. This technique compares the gradients computed by your automatic differentiation implementation against gradients calculated using the mathematical definition of derivatives with finite differences. The central difference method provides a reliable reference implementation that can expose even subtle gradient computation errors.\n\n**Gradient Validation Implementation:**\n\n| Validation Component | Purpose | Key Considerations |\n|---------------------|---------|-------------------|\n| `numerical_gradient(f, inputs, h)` | Compute reference gradients using finite differences | Step size `h` must balance numerical precision vs truncation error |\n| `check_gradients(f, inputs, tolerance)` | Compare autodiff vs numerical gradients | Relative error accounts for gradient magnitude differences |\n| `relative_error(a, b, eps)` | Scale-invariant error metric | Handles both small and large gradient magnitudes appropriately |\n\nThe gradient validation process involves creating a scalar function from your computation that maps input tensors to a single output value, then comparing your framework's computed gradients against numerical approximations. For a function f(x), the central difference approximation f'(x) ≈ [f(x+h) - f(x-h)] / (2h) provides higher accuracy than forward differences and helps identify gradient computation errors with high reliability.\n\n> **Critical Insight**: Gradient validation should be performed on isolated operations first, then progressively on more complex computation graphs. A bug in basic tensor addition will corrupt gradients in every neural network layer, so establish correctness at the foundation before building higher-level components.\n\n**Common Gradient Flow Diagnostics:**\n\n| Diagnostic Technique | What It Reveals | When To Use |\n|---------------------|----------------|------------|\n| Gradient magnitude inspection | Vanishing or exploding gradient problems | After backward pass, before optimizer step |\n| Computation graph visualization | Missing connections or incorrect topology | When gradients are None or unexpectedly zero |\n| Parameter-specific gradient checking | Which layers are receiving gradients correctly | During training loop debugging |\n| Gradient accumulation verification | Whether multiple uses of tensors sum gradients | For networks with skip connections or parameter sharing |\n\n⚠️ **Pitfall: Gradient Not Accumulated**\n\nOne of the most common automatic differentiation bugs occurs when a tensor is used multiple times in a computation graph, but gradients from different paths are not properly accumulated. For example, if parameter W is used in both the main computation path and a regularization term, the gradient with respect to W should be the sum of gradients from both uses. Failing to accumulate these gradients results in the optimizer only seeing partial gradient information, leading to incorrect parameter updates.\n\n```\n# Example scenario where this occurs:\nmain_output = model.linear(x)  # Uses W\nl2_penalty = torch.sum(model.linear.weight ** 2)  # Uses W again\ntotal_loss = mse_loss(main_output, target) + 0.01 * l2_penalty\n# W.grad should contain gradients from both paths\n```\n\nThe bug typically manifests as training that converges more slowly than expected or reaches suboptimal solutions. To diagnose this issue, check whether tensors used multiple times in your computation have gradient magnitudes that seem too small compared to numerical differentiation results.\n\n⚠️ **Pitfall: Incorrect Topological Ordering**\n\nThe backward pass must traverse the computation graph in reverse topological order to ensure that when a node computes its input gradients, all of its output gradients have already been computed and accumulated. Incorrect ordering leads to some gradients being computed before their dependencies are ready, resulting in partial or completely incorrect gradient information.\n\nThis bug often occurs when implementing the `topological_sort` function for backward pass traversal. The symptoms include gradients that are sometimes correct and sometimes wrong (depending on the randomness in graph traversal), or gradients that are systematically too small because only some paths through the computation graph are correctly handled.\n\n**Gradient Computation Verification Steps:**\n\n1. **Isolate the Operation**: Test gradient computation for individual operations (addition, multiplication, matrix multiplication) in isolation before combining them into complex networks.\n\n2. **Verify Graph Construction**: Ensure that every operation correctly sets the `grad_fn` attribute on its output tensor and maintains references to its input tensors.\n\n3. **Check Topological Sort**: Manually trace through the backward pass order for a simple computation graph to ensure dependencies are respected.\n\n4. **Validate Gradient Shapes**: Confirm that computed gradients have the same shape as their corresponding tensors and that broadcasting operations properly reduce gradients back to parameter shapes.\n\n5. **Test Gradient Accumulation**: Create test cases where the same parameter is used multiple times and verify that gradients are summed correctly.\n\n6. **Monitor for Numerical Issues**: Watch for NaN or infinity values in gradients, which often indicate numerical instability in gradient computation rather than algorithmic errors.\n\n**Gradient Debugging Workflow:**\n\n| Step | Action | Expected Outcome | If Different |\n|------|--------|------------------|--------------|\n| 1 | Run `numerical_gradient` on simple operation | Reference gradients computed successfully | Check numerical differentiation implementation |\n| 2 | Run automatic differentiation on same operation | Gradients match reference within tolerance | Debug specific operation's backward method |\n| 3 | Test operation in isolation with various input shapes | All shape combinations work correctly | Debug broadcasting and shape handling |\n| 4 | Combine operations into small computation graph | Gradients still match numerical reference | Debug graph construction and traversal |\n| 5 | Scale up to full neural network | Training converges as expected | Debug parameter registration and optimizer integration |\n\n### Shape and Broadcasting Bugs\n\nShape-related bugs in tensor operations represent some of the most immediately visible failures in neural network frameworks, but they can also be among the most confusing to debug when broadcasting rules are involved. Unlike gradient computation bugs that fail silently, shape bugs typically produce immediate exceptions, but the error messages often don't clearly explain why the shapes are incompatible or how to fix them.\n\nBroadcasting is particularly problematic because it involves implicit shape transformations that can mask underlying architecture problems. A shape mismatch that would clearly indicate a design error without broadcasting might instead trigger a broadcasting operation that produces a result with unexpected dimensions, leading to subtle bugs later in the computation pipeline.\n\n**Shape Error Diagnostic Framework:**\n\n| Error Type | Typical Symptoms | Root Cause Categories |\n|------------|------------------|---------------------|\n| `ShapeError` | Immediate failure during operation | Fundamental incompatibility between tensor dimensions |\n| `BroadcastingError` | Unexpected output shapes from operations | Broadcasting rules applied when explicit reshaping was needed |\n| Gradient shape mismatch | Backward pass fails with shape errors | Parameter gradients don't match parameter shapes after broadcasting |\n| Silent broadcasting | Operations succeed but produce wrong results | Broadcasting masks architecture errors |\n\nThe most effective approach to debugging shape issues involves creating a systematic tracing mechanism that logs tensor shapes at every operation, combined with broadcasting simulation that shows exactly how shapes would be transformed. This visibility is crucial because shape transformations in neural networks often involve multiple sequential operations where an error in early stages compounds into confusing failures later.\n\n**Broadcasting Debugging Strategy:**\n\nBroadcasting follows specific rules that can be simulated step-by-step to understand why operations fail or produce unexpected results. The NumPy broadcasting rules that most frameworks follow involve aligning shapes from the rightmost dimension and expanding dimensions of size 1 or adding new dimensions as needed.\n\n| Broadcasting Step | Rule | Example | Potential Issues |\n|------------------|------|---------|------------------|\n| Right-align shapes | Compare dimensions from right to left | (3, 1, 4) vs (2, 4) | Different number of dimensions |\n| Check compatibility | Each dimension must be equal or one must be 1 | 4 == 4 ✓, 3 vs 2 ✗ | Incompatible dimension sizes |\n| Expand dimensions | Dimensions of 1 broadcast to match larger dimension | (3, 1, 4) → (3, 2, 4) | Memory implications for large expansions |\n| Add leading dimensions | Shorter shape gets leading 1s | (2, 4) → (1, 2, 4) | Changes tensor interpretation |\n\n⚠️ **Pitfall: Broadcasting Gradient Shapes**\n\nA particularly subtle category of shape bugs occurs during the backward pass when gradients computed for broadcasted operations need to be reduced back to the original parameter shapes. If a parameter with shape (10,) is broadcasted to (5, 10) during forward computation, the gradient flowing back will have shape (5, 10) but must be summed along the first dimension to produce a gradient with shape (10,) that matches the parameter.\n\nThis bug often manifests as shape mismatches during optimizer updates, where the optimizer expects parameter gradients to have the same shape as the parameters themselves. The error message typically indicates that gradients have too many dimensions or are larger than expected.\n\n**Shape Tracing Implementation:**\n\nCreating a comprehensive shape debugging system requires instrumentation at multiple levels of the tensor operation stack. The most effective approach combines automatic shape logging with manual inspection utilities that can trace shape transformations through complex computation graphs.\n\n| Debugging Tool | Purpose | Implementation Strategy |\n|----------------|---------|------------------------|\n| `validate_broadcasting_safety` | Check if tensors can be broadcasted before attempting operation | Pre-flight validation prevents confusing error messages |\n| `analyze_broadcasting_failure` | Diagnose why broadcasting failed with detailed step analysis | Post-failure analysis for debugging |\n| `create_shape_error_message` | Generate helpful error messages with expected vs actual shapes | User experience improvement |\n| `trace_shape_transformations` | Log shape changes through computation graph | Debugging tool for complex networks |\n\n⚠️ **Pitfall: In-Place Operations Breaking Broadcasting**\n\nIn-place tensor operations can create particularly confusing shape-related bugs because they modify tensor shapes during computation in ways that break subsequent operations. If an in-place operation changes a tensor's shape, other tensors that reference it may suddenly become incompatible with operations that previously worked correctly.\n\nThis issue is especially problematic in neural network frameworks because parameters are often reused across multiple forward passes, and in-place modifications during one forward pass can break subsequent passes. The symptoms typically include operations that work correctly the first time but fail on subsequent iterations with shape mismatch errors.\n\n**Shape Debugging Workflow:**\n\n1. **Enable Shape Logging**: Instrument all tensor operations to log input and output shapes, making it possible to trace exactly where shape mismatches occur.\n\n2. **Simulate Broadcasting**: Before performing operations, simulate the broadcasting rules to predict output shapes and identify potential issues.\n\n3. **Validate Gradient Shapes**: After backward passes, verify that all parameter gradients have shapes matching their corresponding parameters.\n\n4. **Test Edge Cases**: Create unit tests for broadcasting with tensors of very different shapes, including scalar broadcasting and high-dimensional tensor combinations.\n\n5. **Check Parameter Registration**: Ensure that neural network modules correctly register parameters with their intended shapes and that these shapes remain consistent throughout training.\n\n### Training Loop Problems\n\nTraining loop debugging represents the most complex category of neural network framework bugs because it involves the interaction of multiple components - tensors, automatic differentiation, neural modules, and optimizers - all coordinating to produce emergent learning behavior. Training loop bugs are particularly challenging because they often don't produce error messages, instead manifesting as training that runs to completion but fails to learn effectively.\n\nThe symptoms of training loop problems can be subtle and misleading. Training might appear to run smoothly with loss values that change over time, but the changes might be in the wrong direction, too slow, or might plateau prematurely at poor solutions. These behaviors can result from optimizer configuration errors, learning rate problems, loss function bugs, or subtle issues in the coordination between forward passes, gradient computation, and parameter updates.\n\n**Training Loop Failure Modes:**\n\n| Failure Mode | Symptoms | Common Causes |\n|-------------|----------|---------------|\n| No learning | Loss remains constant across epochs | Gradients not flowing to parameters, learning rate too small, optimizer not updating |\n| Unstable training | Loss oscillates wildly or increases | Learning rate too large, gradient explosion, numerical instability |\n| Slow convergence | Learning much slower than expected | Learning rate too small, poor initialization, gradient vanishing |\n| Premature plateauing | Training stops improving early | Local minimum, learning rate decay too aggressive |\n| Perfect training loss, poor validation | Overfitting without generalization | No regularization, model too complex for dataset |\n\nThe most effective approach to debugging training problems involves creating systematic monitoring and validation systems that track not just loss values but also gradient magnitudes, parameter update sizes, learning rates, and other training dynamics that provide insight into what the optimization process is actually doing.\n\n**Training Loop Monitoring Framework:**\n\n| Monitoring Component | Purpose | Key Metrics |\n|---------------------|---------|-------------|\n| `TrainingMonitor` | Track training dynamics and detect problems | Loss history, gradient norms, parameter update magnitudes |\n| `validate_training_step` | Verify each training step completed correctly | Forward pass output shapes, gradient computation success, parameter updates applied |\n| `check_gradient_flow` | Ensure gradients flow through all parameters | Parameter-wise gradient magnitudes, layers with zero gradients |\n| `validate_gradient_computation` | Compare gradients against numerical differentiation | Gradient correctness on training batches |\n\n⚠️ **Pitfall: Optimizer State Not Reset**\n\nA common but subtle training loop bug occurs when optimizer state (such as momentum buffers in SGD or moment estimates in Adam) is not properly reset between different training runs or when loading checkpoints. This can cause training to behave unpredictably, especially with optimizers like Adam that maintain running averages of gradients and squared gradients.\n\nThe symptoms often include training that works correctly when started fresh but behaves differently when resumed from checkpoints, or training runs that are not reproducible even with the same random seeds. The optimizer may be applying updates based on stale momentum or moment estimates from previous training sessions.\n\n**Loss Function Debugging:**\n\nLoss function bugs can be particularly insidious because they often allow training to proceed but guide the optimization process toward incorrect solutions. The most common issues include incorrect gradient computation in custom loss functions, numerical instability in standard loss functions, and mismatched expectations between loss function outputs and optimizer inputs.\n\n| Loss Function Issue | Detection Method | Fix Strategy |\n|--------------------|------------------|--------------|\n| Incorrect gradients | Compare with numerical differentiation | Debug loss function backward pass |\n| Numerical instability | Monitor for NaN/inf in loss values | Add numerical stability constants |\n| Wrong reduction | Loss magnitude much larger/smaller than expected | Check reduction parameter (mean vs sum) |\n| Target format mismatch | Training doesn't improve despite correct gradients | Verify target tensor shape and dtype |\n\n⚠️ **Pitfall: Learning Rate Scheduling Configuration**\n\nLearning rate scheduling bugs often manifest as training that starts well but then suddenly stops learning or begins to diverge. These issues typically occur when learning rate schedulers are configured incorrectly, applied at the wrong frequency, or use inappropriate decay schedules for the specific optimization problem.\n\nCommon scheduling mistakes include applying exponential decay too aggressively (causing learning to stop prematurely), using step-based schedules with inappropriate step sizes, or forgetting to call the scheduler's step method at the correct frequency during training.\n\n**Training Loop Debugging Workflow:**\n\n1. **Verify Individual Components**: Test tensors, gradients, and optimizers in isolation before integrating them into the full training loop.\n\n2. **Monitor Training Dynamics**: Track not just loss values but also gradient magnitudes, parameter norms, and update sizes to understand what the optimizer is actually doing.\n\n3. **Validate on Toy Problems**: Test the training loop on simple problems with known solutions (linear regression, XOR) to establish that the basic mechanism works.\n\n4. **Check Learning Rate Sensitivity**: Test multiple learning rates across several orders of magnitude to ensure the optimizer can learn with appropriate configuration.\n\n5. **Verify Gradient Flow**: Ensure that gradients computed during training match gradients computed by numerical differentiation on the same inputs.\n\n6. **Test Reproducibility**: Verify that training runs with the same random seed produce identical results, indicating that the training loop is deterministic and properly implemented.\n\n### Implementation Guidance\n\nThe debugging infrastructure for a neural network framework requires both systematic monitoring tools and manual debugging utilities that help diagnose problems when they occur. The implementation approach should prioritize clear error messages and detailed diagnostic information over performance, since debugging tools are typically used during development rather than production.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error Handling | Python exceptions with detailed messages | Custom exception hierarchy with error codes |\n| Logging | Python logging module with structured output | Structured logging with JSON output and filtering |\n| Numerical Validation | NumPy-based numerical differentiation | Autograd or JAX for reference implementations |\n| Memory Monitoring | Python tracemalloc for memory usage | psutil for system-wide resource monitoring |\n| Visualization | Matplotlib for gradient/loss plotting | TensorBoard-style web interface |\n\n**Recommended File Structure:**\n```\nneural_framework/\n  core/\n    tensor.py                 ← Core tensor implementation\n    operations.py             ← Tensor operations with gradient computation\n    autodiff.py              ← Automatic differentiation engine\n  nn/\n    modules.py               ← Neural network modules\n    optimizers.py            ← Optimization algorithms\n    losses.py                ← Loss functions\n  debug/\n    __init__.py\n    gradient_checker.py      ← Gradient validation utilities\n    shape_tracer.py          ← Shape debugging tools\n    training_monitor.py      ← Training loop monitoring\n    error_handlers.py        ← Custom exceptions and error messages\n  tests/\n    test_gradients.py        ← Gradient checking tests\n    test_shapes.py           ← Broadcasting and shape tests\n    test_training.py         ← End-to-end training tests\n```\n\n**Core Debugging Infrastructure (Complete Implementation):**\n\n```python\n# debug/error_handlers.py - Complete exception hierarchy\nimport traceback\nfrom typing import List, Dict, Any, Tuple, Optional\n\nclass NeuralFrameworkError(Exception):\n    \"\"\"Base exception for neural framework errors.\"\"\"\n    \n    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n        self.message = message\n        self.details = details or {}\n        self.stack_info = traceback.format_stack()\n        super().__init__(self.message)\n    \n    def __str__(self):\n        result = f\"{self.__class__.__name__}: {self.message}\"\n        if self.details:\n            result += f\"\\nDetails: {self.details}\"\n        return result\n\nclass ShapeError(NeuralFrameworkError):\n    \"\"\"Error for tensor shape mismatches.\"\"\"\n    \n    def __init__(self, shapes: List[Tuple[int, ...]], operation: str, \n                 expected_pattern: Optional[str] = None):\n        self.shapes = shapes\n        self.operation = operation\n        self.expected_pattern = expected_pattern\n        \n        message = self.create_shape_error_message(shapes, operation, expected_pattern)\n        super().__init__(message, {\"shapes\": shapes, \"operation\": operation})\n\n    def create_shape_error_message(self, shapes: List[Tuple[int, ...]], \n                                 operation: str, expected_pattern: Optional[str]) -> str:\n        \"\"\"Create detailed shape mismatch error message.\"\"\"\n        shape_strs = [str(shape) for shape in shapes]\n        message = f\"Shape mismatch in {operation}: got shapes {', '.join(shape_strs)}\"\n        \n        if expected_pattern:\n            message += f\"\\nExpected: {expected_pattern}\"\n        \n        # Add specific guidance for common operations\n        if operation == \"matmul\" and len(shapes) == 2:\n            message += f\"\\nFor matrix multiplication: ({shapes[0]}) @ ({shapes[1]})\"\n            message += f\"\\nInner dimensions must match: {shapes[0][-1]} != {shapes[1][-2]}\"\n        \n        return message\n\nclass BroadcastingError(ShapeError):\n    \"\"\"Error for broadcasting failures with detailed analysis.\"\"\"\n    \n    def __init__(self, shape1: Tuple[int, ...], shape2: Tuple[int, ...]):\n        self.shape1 = shape1\n        self.shape2 = shape2\n        self.step_analysis = self.analyze_broadcasting_failure(shape1, shape2)\n        \n        message = f\"Broadcasting failed between {shape1} and {shape2}\"\n        message += f\"\\nStep-by-step analysis:\\n\" + \"\\n\".join(self.step_analysis)\n        \n        super().__init__([shape1, shape2], \"broadcasting\")\n\n    def analyze_broadcasting_failure(self, shape1: Tuple[int, ...], \n                                   shape2: Tuple[int, ...]) -> List[str]:\n        \"\"\"Analyze why broadcasting failed with detailed steps.\"\"\"\n        steps = []\n        \n        # Right-align shapes\n        max_dims = max(len(shape1), len(shape2))\n        padded1 = (1,) * (max_dims - len(shape1)) + shape1\n        padded2 = (1,) * (max_dims - len(shape2)) + shape2\n        \n        steps.append(f\"Right-aligned shapes: {padded1} vs {padded2}\")\n        \n        # Check each dimension\n        for i, (d1, d2) in enumerate(zip(padded1, padded2)):\n            if d1 == d2:\n                steps.append(f\"Dimension {i}: {d1} == {d2} ✓\")\n            elif d1 == 1:\n                steps.append(f\"Dimension {i}: {d1} broadcasts to {d2} ✓\")\n            elif d2 == 1:\n                steps.append(f\"Dimension {i}: {d2} broadcasts to {d1} ✓\")\n            else:\n                steps.append(f\"Dimension {i}: {d1} != {d2} ✗ (INCOMPATIBLE)\")\n                break\n        \n        return steps\n\nclass GradientError(NeuralFrameworkError):\n    \"\"\"Base class for gradient computation errors.\"\"\"\n    pass\n\nclass NumericalInstabilityError(NeuralFrameworkError):\n    \"\"\"Error for NaN/inf values in computations.\"\"\"\n    \n    def __init__(self, tensor_info: Dict[str, Any]):\n        self.tensor_info = tensor_info\n        message = f\"Numerical instability detected: {tensor_info}\"\n        super().__init__(message, tensor_info)\n```\n\n**Gradient Validation Utilities (Complete Implementation):**\n\n```python\n# debug/gradient_checker.py - Complete gradient validation system\nimport numpy as np\nfrom typing import Callable, List, Dict, Any, Optional, Tuple\nimport warnings\n\nclass GradientTester:\n    \"\"\"Complete gradient validation system comparing autodiff vs numerical.\"\"\"\n    \n    def __init__(self, tolerance: float = 1e-5):\n        self.tolerance = tolerance\n        self.test_results = []\n    \n    def numerical_gradient(self, f: Callable, inputs: List, h: float = 1e-5) -> List[np.ndarray]:\n        \"\"\"Compute numerical gradients using central differences.\"\"\"\n        gradients = []\n        \n        for i, input_tensor in enumerate(inputs):\n            grad = np.zeros_like(input_tensor.data)\n            flat_input = input_tensor.data.flatten()\n            flat_grad = grad.flatten()\n            \n            for j in range(len(flat_input)):\n                # Save original value\n                original = flat_input[j]\n                \n                # Compute f(x + h)\n                flat_input[j] = original + h\n                input_tensor.data = flat_input.reshape(input_tensor.data.shape)\n                f_plus = f(*inputs).item()\n                \n                # Compute f(x - h)\n                flat_input[j] = original - h\n                input_tensor.data = flat_input.reshape(input_tensor.data.shape)\n                f_minus = f(*inputs).item()\n                \n                # Central difference\n                flat_grad[j] = (f_plus - f_minus) / (2 * h)\n                \n                # Restore original value\n                flat_input[j] = original\n            \n            input_tensor.data = flat_input.reshape(input_tensor.data.shape)\n            gradients.append(flat_grad.reshape(input_tensor.data.shape))\n        \n        return gradients\n    \n    def relative_error(self, a: np.ndarray, b: np.ndarray, eps: float = 1e-8) -> float:\n        \"\"\"Compute relative error between arrays.\"\"\"\n        return np.max(np.abs(a - b) / (np.abs(a) + np.abs(b) + eps))\n    \n    def check_gradients(self, f: Callable, inputs: List, tolerance: Optional[float] = None) -> bool:\n        \"\"\"Compare autodiff gradients against numerical gradients.\"\"\"\n        if tolerance is None:\n            tolerance = self.tolerance\n        \n        # Compute autodiff gradients\n        for inp in inputs:\n            inp.grad = None  # Clear previous gradients\n        \n        output = f(*inputs)\n        output.backward()\n        \n        autodiff_grads = [inp.grad.data if inp.grad is not None else np.zeros_like(inp.data) \n                         for inp in inputs]\n        \n        # Compute numerical gradients\n        numerical_grads = self.numerical_gradient(f, inputs)\n        \n        # Compare each gradient\n        all_passed = True\n        for i, (auto_grad, num_grad) in enumerate(zip(autodiff_grads, numerical_grads)):\n            error = self.relative_error(auto_grad, num_grad)\n            passed = error < tolerance\n            all_passed = all_passed and passed\n            \n            self.test_results.append({\n                'input_index': i,\n                'relative_error': error,\n                'tolerance': tolerance,\n                'passed': passed,\n                'autodiff_grad': auto_grad,\n                'numerical_grad': num_grad\n            })\n            \n            if not passed:\n                warnings.warn(f\"Gradient check failed for input {i}: \"\n                            f\"relative error {error:.2e} > tolerance {tolerance:.2e}\")\n        \n        return all_passed\n    \n    def test_operation(self, operation_name: str, test_function: Callable) -> bool:\n        \"\"\"Test gradients for a specific operation with multiple random inputs.\"\"\"\n        print(f\"Testing gradients for {operation_name}...\")\n        \n        passed_count = 0\n        total_tests = 5  # Test with 5 random inputs\n        \n        for test_idx in range(total_tests):\n            try:\n                result = test_function()\n                if result:\n                    passed_count += 1\n                else:\n                    print(f\"  Test {test_idx + 1}: FAILED\")\n            except Exception as e:\n                print(f\"  Test {test_idx + 1}: ERROR - {e}\")\n        \n        success_rate = passed_count / total_tests\n        print(f\"  Success rate: {passed_count}/{total_tests} ({success_rate:.1%})\")\n        \n        return success_rate >= 0.8  # Require 80% success rate\n```\n\n**Training Loop Monitoring (Core Skeleton):**\n\n```python\n# debug/training_monitor.py - Training dynamics monitoring\nfrom typing import Dict, List, Optional, Any\nimport numpy as np\n\nclass TrainingMonitor:\n    \"\"\"Monitor training dynamics and detect problems.\"\"\"\n    \n    def __init__(self, check_frequency: int = 10):\n        self.check_frequency = check_frequency\n        self.step_count = 0\n        self.loss_history = []\n        self.gradient_norms = []\n    \n    def log_step(self, loss: float, gradients: Dict[str, Any]) -> None:\n        \"\"\"Log training step and check for problems.\"\"\"\n        self.step_count += 1\n        self.loss_history.append(loss)\n        \n        # Compute gradient norms\n        grad_norm = 0.0\n        for param_name, grad in gradients.items():\n            if grad is not None:\n                grad_norm += np.sum(grad.data ** 2)\n        grad_norm = np.sqrt(grad_norm)\n        self.gradient_norms.append(grad_norm)\n        \n        # Check for problems periodically\n        if self.step_count % self.check_frequency == 0:\n            self._check_training_health()\n    \n    def _check_training_health(self) -> None:\n        \"\"\"Check for common training problems.\"\"\"\n        # TODO 1: Check for gradient explosion (gradient norm > 10.0)\n        # TODO 2: Check for gradient vanishing (gradient norm < 1e-7)\n        # TODO 3: Check for loss not decreasing over recent steps\n        # TODO 4: Check for numerical instability (NaN/inf in loss)\n        # TODO 5: Emit warnings for detected problems\n        pass\n    \n    def validate_training_step(self, model, loss, optimizer) -> bool:\n        \"\"\"Validate that training step completed successfully.\"\"\"\n        # TODO 1: Check that loss is finite\n        # TODO 2: Check that all parameters have finite gradients\n        # TODO 3: Check that optimizer state is consistent\n        # TODO 4: Return True if everything looks correct\n        pass\n\nclass GradientMonitor:\n    \"\"\"Monitor gradient flow through neural network parameters.\"\"\"\n    \n    def __init__(self, check_frequency: int = 10):\n        self.check_frequency = check_frequency\n        self.step_count = 0\n        self.gradient_history = []\n    \n    def check_gradient_flow(self, model, loss) -> Dict[str, float]:\n        \"\"\"Check gradient magnitudes for all model parameters.\"\"\"\n        # TODO 1: Iterate through all model parameters\n        # TODO 2: Compute gradient magnitude for each parameter\n        # TODO 3: Identify parameters with zero or very small gradients\n        # TODO 4: Return dictionary mapping parameter names to gradient magnitudes\n        # TODO 5: Warn about parameters that may not be learning\n        pass\n```\n\n**Milestone Checkpoints:**\n\nAfter implementing the debugging infrastructure, verify it works with these tests:\n\n**Test 1: Gradient Validation**\n```bash\npython -m pytest tests/test_gradients.py -v\n```\nExpected: All gradient tests pass with relative errors < 1e-5\n\n**Test 2: Shape Error Detection**\n```python\n# Should raise clear BroadcastingError\ntry:\n    tensor1 = Tensor([[1, 2], [3, 4]])  # Shape (2, 2)\n    tensor2 = Tensor([1, 2, 3])         # Shape (3,)\n    result = tensor1 + tensor2\nexcept BroadcastingError as e:\n    print(e)  # Should show step-by-step broadcasting analysis\n```\n\n**Test 3: Training Loop Monitoring**\n```python\n# Should detect gradient explosion\nmonitor = TrainingMonitor()\nfor step in range(100):\n    # Simulate training step with exploding gradients\n    fake_loss = 1.0 + step * 0.1\n    fake_grads = {\"param1\": np.random.normal(0, step * 10)}\n    monitor.log_step(fake_loss, fake_grads)\n# Should emit warnings about gradient explosion\n```\n\nThe debugging infrastructure should catch problems early and provide clear guidance on how to fix them. Focus on making error messages educational rather than just reporting failures.\n\n\n## Future Extensions\n\n> **Milestone(s):** All milestones - this section builds upon the complete framework to explore advanced features, performance optimizations, and ecosystem integrations that extend beyond the core learning objectives\n\nBuilding a neural network framework from scratch provides an excellent foundation for understanding deep learning systems at a fundamental level. However, our educational framework represents just the beginning of what production-grade neural network systems require. Think of our current implementation as a well-crafted bicycle - it teaches you balance, steering, and propulsion in their purest forms. The extensions in this section are like upgrading to motorcycles, cars, and eventually spacecraft - each step builds upon the fundamental principles while adding layers of sophistication, performance, and capability.\n\nThe extensions we'll explore fall into three major categories that mirror the evolution of real-world deep learning frameworks. First, performance optimizations transform our educational code into something that can handle real-world workloads with acceptable speed and memory efficiency. Second, advanced neural network features expand the types of problems our framework can solve beyond basic feedforward networks. Third, ecosystem and tooling additions make our framework practical for real research and development work by integrating with existing machine learning pipelines and providing essential developer tools.\n\nUnderstanding these extensions serves multiple purposes in your learning journey. Most immediately, it helps you recognize the gap between educational implementations and production systems, giving you realistic expectations about what building real frameworks entails. More importantly, it provides a roadmap for your continued learning - each extension represents a deep technical domain where you can dive deeper based on your interests and career goals. Finally, implementing some of these extensions makes for excellent portfolio projects that demonstrate your understanding of both theoretical concepts and practical engineering challenges.\n\nThe mental model for approaching these extensions is that of a master craftsperson expanding their workshop. You've learned the fundamental skills of working with the essential tools. Now you're ready to add specialized equipment, learn advanced techniques, and integrate your craft with broader industrial processes. Some extensions are natural next steps that improve what you're already doing. Others open entirely new domains of possibility. The key is understanding which extensions serve your learning goals and which represent scope creep that could distract from mastering the fundamentals.\n\n### Performance Optimizations\n\nPerformance optimization in neural network frameworks is like transforming a carefully crafted prototype into a high-performance production vehicle. Our current educational implementation prioritizes clarity and understanding over speed - every operation is explicit, every computation graph node is individually allocated, and every gradient calculation follows the textbook algorithm step by step. While this approach excellently serves learning objectives, real-world neural networks require orders of magnitude better performance to train on meaningful datasets within reasonable timeframes.\n\nThe performance optimization landscape spans multiple dimensions, each with distinct technical challenges and trade-offs. **GPU acceleration** moves computation from sequential CPU operations to massively parallel GPU kernels, but requires understanding GPU programming models, memory hierarchies, and kernel optimization techniques. **Operation fusion** combines multiple simple operations into optimized compound operations, reducing memory bandwidth and improving cache efficiency, but increases implementation complexity and requires sophisticated compiler techniques. **Memory optimization** techniques like gradient checkpointing and activation recomputation trade compute for memory, enabling training of much larger models, but require careful analysis of the memory-computation trade-off space.\n\n#### GPU Acceleration Foundation\n\nGPU acceleration transforms our CPU-bound tensor operations into massively parallel computations that can achieve 10-100x speedups on suitable workloads. The fundamental insight is that neural network operations like matrix multiplication, element-wise arithmetic, and convolutions exhibit data parallelism - the same operation can be applied to thousands of data elements simultaneously.\n\nThe **GPU programming model** requires restructuring our tensor operations to work with GPU memory hierarchies and parallel execution units. Instead of our current approach where `matmul` calls NumPy's sequential CPU implementation, GPU acceleration involves writing or calling optimized CUDA kernels that distribute computation across thousands of GPU threads. Each thread processes a small portion of the output tensor, with careful coordination to ensure memory access patterns maximize GPU memory bandwidth utilization.\n\n**Memory management** becomes significantly more complex with GPU acceleration. Our current `Tensor` class stores data in CPU memory and performs all operations in-place or with immediate allocation. GPU tensors require managing data movement between CPU and GPU memory spaces, with explicit control over when data transfers occur. The key insight is that GPU memory bandwidth to CPU memory is limited, so minimizing host-device transfers is crucial for performance.\n\n| GPU Memory Management Component | CPU Implementation | GPU Implementation | Performance Impact |\n|--------------------------------|-------------------|-------------------|-------------------|\n| Tensor Storage | `data: np.ndarray` in CPU RAM | `data: GPUArray` in GPU memory | 10-100x faster compute, but transfer overhead |\n| Operation Execution | Sequential NumPy calls | Parallel GPU kernels | Massive parallelism for suitable operations |\n| Memory Allocation | Python garbage collector | Explicit GPU memory pools | Faster allocation, manual lifecycle management |\n| Data Movement | Implicit copies | Explicit CPU↔GPU transfers | Must minimize transfers for performance |\n\nThe **CUDA integration strategy** involves several architectural decisions that significantly impact both performance and implementation complexity. The simplest approach wraps existing CUDA libraries like cuBLAS for matrix operations and cuDNN for neural network primitives, providing good performance with minimal implementation effort. More advanced approaches involve writing custom CUDA kernels for fused operations, providing maximum performance at the cost of significant GPU programming expertise requirements.\n\n> **Decision: GPU Backend Architecture**\n> - **Context**: Need to accelerate tensor operations while maintaining our clean tensor API\n> - **Options Considered**: \n>   1. Wrapper around CuPy/PyTorch CUDA tensors\n>   2. Custom CUDA kernel implementation\n>   3. OpenCL for cross-platform GPU support\n> - **Decision**: Start with CuPy wrapper, provide hooks for custom kernels\n> - **Rationale**: CuPy provides immediate GPU acceleration with minimal implementation complexity, while custom kernel hooks allow optimization of bottleneck operations later\n> - **Consequences**: Enables GPU acceleration quickly but creates dependency on CuPy ecosystem\n\nThe **kernel optimization** process for custom GPU operations follows a systematic approach to maximize GPU utilization. First, identify bottleneck operations through profiling - typically matrix multiplication, large element-wise operations, and reduction operations like sum or max. Second, design memory access patterns that maximize coalesced memory accesses and minimize bank conflicts. Third, optimize thread block configurations and shared memory usage to maximize occupancy. Finally, implement multiple kernel variants optimized for different input sizes and shapes.\n\n#### Operation Fusion Techniques\n\nOperation fusion combines multiple simple operations into single optimized kernels, dramatically reducing memory bandwidth requirements and improving cache efficiency. Consider a common neural network operation like `relu(matmul(x, weight) + bias)` - our current implementation performs three separate operations with three separate memory passes. Fusion combines these into a single GPU kernel that computes the matrix multiplication, adds bias, and applies ReLU activation while the data is still in GPU registers and shared memory.\n\nThe **fusion identification** process systematically analyzes computation graphs to identify fusion opportunities. Element-wise operations like addition, multiplication, and activation functions are excellent fusion candidates because they have the same memory access patterns and can share loop structures. Reduction operations like sum, mean, and softmax can be fused with preceding element-wise operations. More complex fusions involve combining matrix operations with element-wise operations, like fusing bias addition into matrix multiplication kernels.\n\n**Automatic fusion** requires building a graph optimization pass that identifies fusible operation sequences and generates optimized implementations. The fusion algorithm walks the computation graph looking for patterns like consecutive element-wise operations or matrix operations followed by element-wise operations. For each identified pattern, it replaces the original operation sequence with a single fused operation that implements the same mathematical computation but with optimized memory access patterns.\n\n| Fusion Pattern | Operations | Memory Passes (Unfused) | Memory Passes (Fused) | Typical Speedup |\n|----------------|-----------|------------------------|---------------------|-----------------|\n| Element-wise Chain | `relu(tanh(x + y))` | 3 passes | 1 pass | 2-3x |\n| MatMul + Bias | `matmul(x, w) + b` | 2 passes | 1 pass | 1.5-2x |\n| Broadcast Fusion | `x * y + z` with broadcasting | 2 passes | 1 pass | 1.5-2.5x |\n| Reduction Fusion | `sum(relu(x * y))` | 2 passes | 1 pass | 2-4x |\n\nThe **code generation** approach for fused operations can follow several strategies depending on implementation complexity preferences. Template-based generation uses parameterized kernel templates that get specialized for specific operation combinations. Just-in-time compilation generates and compiles CUDA kernels at runtime based on the specific operation sequence and tensor shapes. More sophisticated approaches use domain-specific languages like Halide or TVM to express the computation and automatically generate optimized implementations.\n\n#### Memory Optimization Strategies\n\nMemory optimization techniques enable training neural networks that are much larger than available GPU memory through careful trade-offs between computation and memory usage. The fundamental insight is that many intermediate activations can be recomputed during the backward pass rather than stored, trading additional forward pass computation for reduced memory footprint.\n\n**Gradient checkpointing** selectively saves activation values at certain layers while discarding activations at intermediate layers. During the backward pass, discarded activations are recomputed by re-running the forward pass from the most recent checkpoint. This technique can reduce memory usage by 50-80% at the cost of approximately 30-50% additional computation time. The key design challenge is selecting optimal checkpoint locations that minimize recomputation while providing sufficient memory savings.\n\nThe **checkpointing strategy** involves analyzing the computation graph to identify optimal checkpoint placement. Memory-intensive layers like large matrix multiplications and convolutions are prime candidates for activation recomputation. The algorithm calculates the memory footprint and recomputation cost for different checkpointing configurations, selecting the configuration that minimizes total training time while staying within memory constraints.\n\n**Activation recomputation** requires modifying our backward pass algorithm to support dynamic forward pass re-execution. When the backward pass reaches a layer whose activations were discarded, it triggers a mini forward pass from the most recent checkpoint to the current layer. This requires careful coordination between the forward pass execution engine and the backward pass gradient computation to ensure consistent execution and proper gradient flow.\n\n| Memory Optimization Technique | Memory Reduction | Computation Overhead | Implementation Complexity | Best Use Case |\n|------------------------------|------------------|---------------------|------------------------|---------------|\n| Gradient Checkpointing | 50-80% | 30-50% slower | Moderate | Large models, memory-constrained training |\n| Mixed Precision Training | 40-50% | 10-20% faster | Low | Modern GPUs with Tensor Cores |\n| Dynamic Loss Scaling | Variable | Minimal | Moderate | Preventing underflow in mixed precision |\n| Offload to CPU Memory | 80-90% | 2-10x slower | High | Very large models exceeding GPU memory |\n\n**Mixed precision training** uses 16-bit floating point for most operations while keeping 32-bit precision for operations that require higher numerical precision. This technique reduces memory usage by approximately 50% while often improving training speed on modern GPUs with specialized Tensor Core units. However, it requires careful handling of gradient scaling to prevent underflow and selective precision upgrading for numerically sensitive operations.\n\nThe **implementation architecture** for memory optimization requires extending our tensor and operation classes to support checkpointing metadata and recomputation triggers. The `Tensor` class needs additional fields to track whether activations are stored or discarded, and checkpoint identifiers for recomputation. The backward pass algorithm requires modification to detect missing activations and trigger appropriate recomputation sequences.\n\n### Advanced Neural Network Features\n\nAdvanced neural network features expand our framework's capability beyond basic feedforward networks to support the full spectrum of modern deep learning architectures. Think of our current framework as a foundation that understands the language of tensors, gradients, and optimization. The advanced features are like learning specialized dialects that enable communication in specific domains - computer vision through convolutional operations, sequential data through recurrent architectures, and stable training through normalization techniques.\n\nThe design philosophy for adding advanced features should maintain the educational clarity and compositional nature of our existing module system while introducing the mathematical sophistication required for modern architectures. Each new feature should feel like a natural extension of our existing tensor and module abstractions rather than a bolt-on addition that breaks the conceptual model.\n\n#### Convolutional Neural Network Support\n\nConvolutional operations represent a fundamental shift from our current dense linear algebra operations to spatially-aware computations that respect the local structure of image and signal data. The core insight is that convolution operations can be expressed as specialized matrix multiplications, but efficient implementations require understanding of im2col transformations, memory layout optimizations, and multi-dimensional indexing patterns.\n\nThe **convolution operation** implementation requires extending our tensor operations to handle multi-dimensional convolution with proper gradient computation. Unlike our current `MatMul` operation that works with 2D matrices, convolution operates on 4D tensors with dimensions `(batch_size, channels, height, width)` for 2D convolutions. The forward pass involves sliding a kernel across input spatial dimensions, computing dot products at each position. The backward pass requires computing gradients with respect to both input data and convolution weights through transposed convolution operations.\n\n**Im2col transformation** converts the convolution operation into matrix multiplication by reshaping input patches into columns of a matrix. This transformation enables leveraging our existing optimized matrix multiplication implementations for convolution computation. However, it requires careful memory management since the im2col transformation can significantly increase memory usage for large kernels or small strides.\n\n| Convolution Component | Mathematical Operation | Implementation Strategy | Memory Considerations |\n|----------------------|------------------------|------------------------|---------------------|\n| 2D Convolution | `out[b,c,h,w] = sum(input[b,:,h:h+kh,w:w+kw] * kernel[c,:,:,:])` | Im2col + MatMul | O(batch * channels * height * width * kernel_size²) |\n| Gradient wrt Input | Transposed convolution | Gradient im2col | Same as forward pass |\n| Gradient wrt Kernel | Input-output correlation | Correlation via convolution | O(output_channels * input_channels * kernel_size²) |\n| Padding Handling | Zero-pad or reflect input | Explicit padding in im2col | Additional memory for padded tensors |\n\nThe **pooling operations** provide spatial downsampling and local aggregation functionality essential for CNN architectures. Max pooling and average pooling operations reduce spatial dimensions while preserving channel depth. The implementation challenge involves efficient indexing to identify maximum values for max pooling gradient computation and proper handling of overlapping pooling windows.\n\n**Batch normalization** normalizes layer inputs to have zero mean and unit variance across the batch dimension, significantly improving training stability and convergence speed. The implementation requires computing running statistics for inference mode and proper gradient computation through the normalization transformation. The key challenge is handling the different behaviors in training mode (using batch statistics) versus inference mode (using running statistics).\n\n> **Decision: Convolution Implementation Strategy**\n> - **Context**: Need efficient convolution operations while maintaining gradient computation compatibility\n> - **Options Considered**: \n>   1. Direct convolution with nested loops\n>   2. Im2col transformation + existing matrix multiplication\n>   3. FFT-based convolution for large kernels\n> - **Decision**: Implement im2col + matrix multiplication with hooks for FFT optimization\n> - **Rationale**: Im2col leverages existing optimized matrix operations while providing reasonable performance; FFT hooks enable future optimization for large kernels\n> - **Consequences**: Good performance with implementation complexity manageable for educational purposes\n\n#### Recurrent Neural Network Architectures\n\nRecurrent neural networks process sequential data by maintaining hidden state across time steps, requiring our framework to handle variable-length sequences and temporal gradient flow. The fundamental challenge is extending our static computation graph approach to handle dynamic unrolling of recurrent operations across sequence lengths while maintaining efficient memory usage and gradient computation.\n\nThe **RNN cell abstraction** provides the building block for all recurrent architectures. An RNN cell takes current input and previous hidden state as inputs and produces new hidden state and optional output. The cell can be applied repeatedly across sequence time steps, with hidden state flowing from one time step to the next. This abstraction naturally extends our module system since RNN cells are modules with specific input/output contracts.\n\n**LSTM and GRU implementations** require multiple internal gates with sigmoid and tanh activations, element-wise multiplications, and careful state management. The LSTM cell maintains both hidden state and cell state, with forget, input, and output gates controlling information flow. The implementation challenge involves efficient computation of multiple gate operations and proper gradient flow through the complex gate interactions.\n\n| RNN Architecture | State Components | Gate Operations | Gradient Flow Complexity |\n|------------------|------------------|-----------------|------------------------|\n| Vanilla RNN | Hidden state only | Single tanh activation | Linear gradient path, prone to vanishing gradients |\n| LSTM | Hidden + cell state | Forget, input, output gates | Multiple parallel paths, more stable gradients |\n| GRU | Hidden state only | Reset, update gates | Simplified gating, good gradient properties |\n| Bidirectional | Forward + backward hidden | Duplicate computations | Independent forward/backward gradient paths |\n\nThe **sequence processing** strategy must handle variable-length sequences efficiently while maintaining batch processing capabilities. Padding sequences to uniform length enables batch processing but requires masking to ignore padded positions in loss computation. Packed sequences avoid padding overhead but require more complex indexing and sorting operations.\n\n**Attention mechanisms** enable models to focus on specific parts of input sequences, dramatically improving performance on long sequences. The basic attention operation computes weighted combinations of sequence elements based on learned attention weights. Self-attention (used in Transformers) computes attention weights between all pairs of sequence positions, enabling parallel processing that overcomes RNN sequential dependencies.\n\n#### Normalization and Regularization Techniques\n\nNormalization and regularization techniques stabilize training and improve generalization by controlling activation distributions and preventing overfitting. These techniques require understanding statistical computation, gradient flow through normalization operations, and careful mode switching between training and inference behaviors.\n\n**Layer normalization** normalizes activations across the feature dimension rather than the batch dimension like batch normalization. This approach works better for recurrent networks and eliminates dependence on batch size. The implementation computes mean and variance across the last dimension of input tensors, then applies the standard normalization transformation with learnable scale and shift parameters.\n\n**Dropout regularization** randomly sets activation values to zero during training to prevent overfitting. The implementation requires random mask generation, proper scaling of remaining activations, and mode-dependent behavior (dropout during training, identity during inference). The gradient computation must account for the masking pattern used in the forward pass.\n\n| Normalization Technique | Normalization Axes | Learnable Parameters | Mode Dependency | Primary Use Case |\n|------------------------|-------------------|---------------------|-----------------|------------------|\n| Batch Normalization | Across batch dimension | Scale (γ) and shift (β) | Training vs inference | CNNs, feedforward networks |\n| Layer Normalization | Across feature dimension | Scale (γ) and shift (β) | None | RNNs, Transformers |\n| Instance Normalization | Across spatial dimensions | Scale (γ) and shift (β) | None | Style transfer, GANs |\n| Group Normalization | Across channel groups | Scale (γ) and shift (β) | None | Small batch training |\n\nThe **implementation strategy** for normalization layers requires extending our module system to support running statistics tracking, mode-dependent computation, and proper parameter initialization. The key architectural challenge is coordinating between training and inference modes while ensuring gradient computation remains correct in both modes.\n\n### Ecosystem and Tooling\n\nEcosystem and tooling transforms our educational framework into a practical development environment that integrates with existing machine learning workflows and provides essential developer productivity tools. Think of this as building the workshop around your crafted tools - adding workbenches, storage systems, measurement devices, and connections to the broader industrial ecosystem that make your framework useful for real work rather than just learning exercises.\n\nThe ecosystem development should follow the principle of progressive disclosure - start with essential tooling that immediate enables productive work, then gradually add sophisticated features that support advanced use cases. Each tool should integrate naturally with our existing architecture while providing clear value for common development workflows.\n\n#### Model Serialization and Persistence\n\nModel serialization enables saving trained models and loading them for inference or continued training, a fundamental requirement for any practical machine learning framework. Our current framework exists only in memory during program execution - once training completes, all learned parameters disappear unless explicitly extracted. Production workflows require robust serialization that preserves model architecture, trained parameters, optimizer state, and training metadata.\n\nThe **serialization architecture** must handle the hierarchical nature of our module system while supporting backward compatibility and cross-platform portability. The key insight is that serialization involves two distinct components: architecture serialization (how modules are connected) and parameter serialization (learned weights and optimizer state). Architecture serialization can use JSON or protocol buffers to describe the module hierarchy, while parameter serialization requires efficient binary formats for large tensor arrays.\n\n**State dictionary abstraction** provides a clean interface for extracting and loading model state. The `state_dict()` method recursively traverses the module hierarchy to collect all parameters with hierarchical names like `\"linear1.weight\"` and `\"linear1.bias\"`. The corresponding `load_state_dict()` method accepts a state dictionary and assigns parameters back to the appropriate modules. This abstraction separates model architecture from parameter values, enabling techniques like transfer learning and parameter sharing.\n\n| Serialization Component | Format Choice | Rationale | Trade-offs |\n|------------------------|---------------|-----------|------------|\n| Model Architecture | JSON or YAML | Human readable, language agnostic | Larger file size, requires parsing |\n| Parameter Tensors | NumPy .npz or HDF5 | Efficient binary, good compression | Platform-specific endianness issues |\n| Optimizer State | Pickle or MessagePack | Handles arbitrary Python objects | Python-specific, security concerns |\n| Training Metadata | JSON embedded | Easy to inspect and modify | Limited to simple data types |\n\nThe **checkpoint system** coordinates serialization during training to enable resumption after interruption and model evaluation at different training epochs. Checkpoints should include model state, optimizer state, random number generator state, and training metadata like current epoch and loss history. The implementation requires careful coordination with the training loop to ensure checkpoints represent consistent training state.\n\n**Version compatibility** becomes crucial as models are shared between different versions of the framework. The serialization format should include version metadata that enables detecting compatibility issues and providing appropriate error messages. More sophisticated approaches can implement automatic migration between format versions or graceful degradation when non-critical features are unavailable.\n\n> **Decision: Serialization Format Strategy**\n> - **Context**: Need robust model persistence with good performance and cross-platform compatibility\n> - **Options Considered**: \n>   1. Pure JSON with base64-encoded tensors\n>   2. JSON architecture + NumPy binary tensors\n>   3. Protocol Buffers with custom tensor encoding\n> - **Decision**: JSON architecture + NumPy .npz for tensors\n> - **Rationale**: JSON provides human-readable architecture description while NumPy .npz offers efficient tensor serialization with good Python ecosystem integration\n> - **Consequences**: Easy inspection and debugging of model files, but requires careful coordination between JSON and binary components\n\n#### Visualization and Debugging Tools\n\nVisualization and debugging tools provide insight into model behavior, training dynamics, and computational performance that are essential for productive deep learning development. Our current framework provides no visibility into internal behavior beyond final loss values - developers need tools to understand gradient flow, activation distributions, computational bottlenecks, and training convergence patterns.\n\nThe **computation graph visualization** enables developers to inspect the structure of computation graphs generated during forward passes. This tool should render the directed acyclic graph showing tensor operations, tensor shapes, and gradient flow paths. Interactive features like zooming, filtering, and operation inspection help developers understand complex model architectures and debug gradient flow issues.\n\n**Training metrics dashboard** provides real-time monitoring of training progress with plots of loss curves, gradient magnitudes, parameter distributions, and custom metrics. The dashboard should support multiple training runs for comparison, automatic detection of training anomalies like gradient explosion, and export of plots for presentations and papers. Integration with existing tools like TensorBoard or Weights & Biases provides immediate access to sophisticated visualization capabilities.\n\n| Visualization Tool | Information Provided | Implementation Strategy | Primary Use Case |\n|-------------------|---------------------|------------------------|------------------|\n| Computation Graph | Operation structure, tensor shapes, memory usage | Graphviz or D3.js rendering | Architecture debugging, optimization |\n| Loss Curves | Training/validation loss over time | Matplotlib or web dashboard | Training progress monitoring |\n| Gradient Flow | Gradient magnitudes by layer | Histogram plots over training steps | Diagnosing vanishing/exploding gradients |\n| Parameter Distributions | Weight/bias histograms by layer | Statistical plots with updates | Understanding parameter evolution |\n| Activation Distributions | Layer output statistics | Real-time histograms during training | Detecting saturation or dead neurons |\n\nThe **profiling integration** helps developers identify computational bottlenecks and optimize performance. The profiler should track execution time for individual operations, memory allocation patterns, GPU utilization, and data loading overhead. Integration with system profilers like `cProfile` or GPU profilers like `nvprof` provides detailed performance insights that guide optimization efforts.\n\n**Interactive debugging** capabilities enable developers to inspect tensor values, gradient computations, and module state during training. Features like setting breakpoints on specific operations, examining intermediate activations, and manually stepping through gradient computation help diagnose subtle bugs that are difficult to catch with traditional debugging approaches.\n\n#### Integration with Existing Ecosystem\n\nIntegration with the existing machine learning ecosystem enables leveraging mature tools for data loading, preprocessing, evaluation, and deployment while using our framework for the core model definition and training logic. The key insight is that deep learning workflows involve much more than just neural network implementation - data pipelines, evaluation metrics, hyperparameter optimization, and deployment infrastructure are equally important for practical applications.\n\nThe **data loading integration** should support common dataset formats and preprocessing pipelines from libraries like `torchvision`, `tensorflow-datasets`, or custom data loaders. Our framework's `DataLoader` class should provide adapters that can consume data from these sources while maintaining compatibility with our tensor format and training loop structure. Efficient data loading with prefetching, parallel processing, and GPU transfer coordination significantly impacts training performance.\n\n**Interoperability with NumPy and SciPy** enables leveraging the rich ecosystem of scientific computing tools for data preprocessing, evaluation metrics, and post-training analysis. Our `Tensor` class should provide seamless conversion to and from NumPy arrays, enabling integration with existing codebases and scientific libraries. Zero-copy conversion when possible minimizes overhead for interoperability.\n\n| Ecosystem Integration | Interface Strategy | Benefits | Implementation Challenges |\n|----------------------|-------------------|----------|-------------------------|\n| Data Loading | Adapter pattern for common loaders | Leverage existing datasets and preprocessing | Coordinate different tensor formats and batch layouts |\n| Evaluation Metrics | NumPy array conversion | Use mature metric implementations | Ensure consistent tensor→NumPy conversion |\n| Hyperparameter Optimization | Export training function interface | Leverage tools like Optuna or Ray Tune | Design clean interfaces for external control |\n| Model Deployment | ONNX export or REST API wrapper | Enable production deployment | Handle serialization and inference optimization |\n| Distributed Training | Multi-process coordination | Scale training to multiple GPUs/nodes | Complex synchronization and communication |\n\nThe **ONNX export capability** enables deploying models trained with our framework in production environments that use different inference engines. ONNX (Open Neural Network Exchange) provides a standardized format for representing neural network models that can be executed by various runtime engines. Implementing ONNX export requires mapping our module hierarchy and operations to equivalent ONNX operations while preserving mathematical behavior.\n\n**Hyperparameter optimization integration** enables systematic exploration of model architectures and training configurations using tools like Optuna, Ray Tune, or Weights & Biases Sweeps. Our training loop should expose clean interfaces for external hyperparameter optimization tools to control model configuration, training parameters, and evaluation metrics. This integration dramatically improves the efficiency of model development workflows.\n\nThe **deployment infrastructure** should provide utilities for packaging trained models for production inference, including REST API wrappers, batch inference utilities, and integration with serving frameworks like TorchServe or TensorFlow Serving. While full deployment infrastructure is beyond our scope, providing basic utilities and clear interfaces enables users to integrate our framework into production workflows.\n\n### Common Extension Implementation Pitfalls\n\n⚠️ **Pitfall: Premature Optimization**\nMany developers jump directly to advanced performance optimizations before fully understanding the baseline implementation and its bottlenecks. This approach often leads to complex code that provides minimal performance benefits while sacrificing educational clarity. Always profile to identify actual bottlenecks before optimizing, and maintain simple reference implementations alongside optimized versions.\n\n⚠️ **Pitfall: Breaking Existing APIs**\nAdding advanced features by modifying core interfaces like `Tensor` or `Module` can break existing code and violate the principle of backward compatibility. Design extensions as additive features that enhance existing functionality without changing established interfaces. Use composition and inheritance patterns that extend behavior while maintaining compatibility.\n\n⚠️ **Pitfall: Inconsistent Gradient Computation**\nAdvanced operations like convolution, normalization, and attention mechanisms involve complex gradient computations that must remain consistent with the automatic differentiation system. Implementing custom backward passes that don't properly integrate with the computation graph or accumulate gradients incorrectly leads to subtle training bugs that are extremely difficult to debug.\n\n⚠️ **Pitfall: Memory Leaks in GPU Code**\nGPU memory management requires explicit allocation and deallocation, unlike CPU memory that relies on garbage collection. Failing to properly release GPU memory in extension code leads to memory leaks that eventually crash training jobs. Always pair GPU memory allocations with appropriate deallocations, and consider using memory pool patterns for frequently allocated objects.\n\n⚠️ **Pitfall: Platform-Specific Dependencies**\nAdding advanced features that depend on platform-specific libraries (like CUDA-only GPU acceleration) without providing fallback implementations breaks portability. Design extension architectures that gracefully degrade on platforms where advanced features are unavailable, and provide clear error messages when required dependencies are missing.\n\n### Implementation Guidance\n\nThe implementation of framework extensions requires careful planning to maintain the educational clarity and extensible architecture established in the core framework while adding the sophistication necessary for real-world applications.\n\n#### Technology Recommendations\n\n| Extension Category | Simple Option | Advanced Option | Production Option |\n|-------------------|---------------|-----------------|-------------------|\n| GPU Acceleration | CuPy wrapper for basic operations | Custom CUDA kernels for fusion | TensorRT integration for inference |\n| Visualization | Matplotlib + NetworkX for graphs | Web dashboard with D3.js | TensorBoard or Weights & Biases |\n| Serialization | JSON + NumPy .npz files | Protocol Buffers + custom format | ONNX with optimization passes |\n| Data Loading | Manual NumPy array handling | Custom DataLoader with prefetching | Integration with Hugging Face datasets |\n| Profiling | Python cProfile integration | NVIDIA Nsight for GPU profiling | Custom performance monitoring |\n\n#### Project Structure for Extensions\n\n```\nneural_framework/\n  core/                    ← existing core framework\n    tensor.py\n    modules.py\n    optimizers.py\n  extensions/\n    performance/\n      gpu_backend.py       ← GPU acceleration\n      fusion_passes.py     ← operation fusion\n      memory_optimizer.py  ← memory optimization\n    networks/\n      convolution.py       ← CNN layers\n      recurrent.py         ← RNN/LSTM layers  \n      attention.py         ← attention mechanisms\n      normalization.py     ← batch norm, layer norm\n    ecosystem/\n      serialization.py     ← model save/load\n      visualization.py     ← plotting and graphing\n      data_integration.py  ← external data loaders\n      deployment.py        ← model serving utilities\n  examples/\n    cnn_image_classification.py\n    rnn_language_model.py\n    transformer_attention.py\n```\n\n#### GPU Acceleration Starter Code\n\n```python\n# gpu_backend.py - Complete GPU acceleration infrastructure\nimport numpy as np\ntry:\n    import cupy as cp\n    GPU_AVAILABLE = True\nexcept ImportError:\n    GPU_AVAILABLE = False\n    # Fallback: create dummy cupy module that raises informative errors\n    class _CupyMock:\n        def __getattr__(self, name):\n            raise RuntimeError(\"GPU operations require CuPy installation\")\n    cp = _CupyMock()\n\nclass GPUTensor:\n    \"\"\"GPU-accelerated tensor wrapper that maintains API compatibility.\"\"\"\n    \n    def __init__(self, data, requires_grad=False, grad_fn=None, device='cpu'):\n        if device == 'cuda' and not GPU_AVAILABLE:\n            raise RuntimeError(\"CUDA requested but CuPy not available\")\n        \n        self.device = device\n        self.requires_grad = requires_grad\n        self.grad_fn = grad_fn\n        self.grad = None\n        \n        # Store data on appropriate device\n        if device == 'cuda' and GPU_AVAILABLE:\n            self.data = cp.asarray(data) if not isinstance(data, cp.ndarray) else data\n        else:\n            self.data = np.asarray(data) if not isinstance(data, np.ndarray) else data\n    \n    def to(self, device):\n        \"\"\"Move tensor to specified device.\"\"\"\n        # TODO 1: Check if tensor already on target device, return self if so\n        # TODO 2: For cuda→cpu: convert cupy array to numpy array  \n        # TODO 3: For cpu→cuda: convert numpy array to cupy array\n        # TODO 4: Create new GPUTensor with converted data\n        # TODO 5: Handle case where target device not available\n        pass\n    \n    def __add__(self, other):\n        \"\"\"GPU-accelerated addition with automatic device coordination.\"\"\"\n        # TODO 1: Ensure both tensors on same device, move if necessary\n        # TODO 2: Use cupy operations if on GPU, numpy if on CPU\n        # TODO 3: Create appropriate computation graph node for autodiff\n        # TODO 4: Return new GPUTensor with result\n        pass\n\n# Complete memory pool for efficient GPU allocation\nclass GPUMemoryPool:\n    def __init__(self):\n        if GPU_AVAILABLE:\n            self.pool = cp.get_default_memory_pool()\n            self.pinned_pool = cp.get_default_pinned_memory_pool()\n    \n    def allocate(self, shape, dtype):\n        \"\"\"Allocate GPU memory from pool.\"\"\"\n        return cp.zeros(shape, dtype=dtype)\n    \n    def free_unused_blocks(self):\n        \"\"\"Release unused memory blocks back to system.\"\"\"\n        if GPU_AVAILABLE:\n            self.pool.free_all_blocks()\n            self.pinned_pool.free_all_blocks()\n```\n\n#### Operation Fusion Skeleton\n\n```python\n# fusion_passes.py - Computation graph optimization\nclass FusionOptimizer:\n    \"\"\"Identifies and fuses compatible operations in computation graphs.\"\"\"\n    \n    def __init__(self):\n        self.fusion_patterns = [\n            ElementwiseFusionPattern(),\n            MatMulBiasFusionPattern(), \n            ActivationFusionPattern()\n        ]\n    \n    def optimize_graph(self, root_tensor):\n        \"\"\"Apply fusion optimizations to computation graph.\"\"\"\n        # TODO 1: Traverse computation graph to build operation list\n        # TODO 2: For each fusion pattern, identify matching operation sequences\n        # TODO 3: Replace fusible operation chains with single fused operations\n        # TODO 4: Update tensor grad_fn references to point to fused operations\n        # TODO 5: Return optimized graph with reduced operation count\n        pass\n\nclass ElementwiseFusionPattern:\n    \"\"\"Fuses chains of element-wise operations like add→multiply→relu.\"\"\"\n    \n    def matches(self, operations):\n        \"\"\"Check if operation sequence matches this fusion pattern.\"\"\"\n        # TODO 1: Verify all operations are element-wise (same output shape as input)\n        # TODO 2: Check that operations can be chained (output of one feeds input of next)\n        # TODO 3: Ensure no intermediate results needed elsewhere in graph\n        # TODO 4: Return True if fusible, False otherwise\n        pass\n    \n    def create_fused_operation(self, operations):\n        \"\"\"Create single operation that replaces the operation chain.\"\"\"\n        # TODO 1: Analyze operation chain to determine combined mathematical function\n        # TODO 2: Generate optimized kernel code (or use predefined templates)\n        # TODO 3: Create new Operation subclass with fused forward/backward methods\n        # TODO 4: Return fused operation that produces identical mathematical results\n        pass\n```\n\n#### CNN Module Implementation\n\n```python\n# convolution.py - Convolutional neural network layers\nclass Conv2d(Module):\n    \"\"\"2D convolution layer with proper gradient computation.\"\"\"\n    \n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        \n        # Initialize convolution weights using He initialization\n        self.weight = Parameter(torch.randn(out_channels, in_channels, \n                                          kernel_size, kernel_size) * \n                               np.sqrt(2.0 / (in_channels * kernel_size * kernel_size)))\n        self.bias = Parameter(torch.zeros(out_channels))\n    \n    def forward(self, x):\n        \"\"\"Compute 2D convolution using im2col transformation.\"\"\"\n        # TODO 1: Apply padding to input tensor if padding > 0\n        # TODO 2: Use im2col to reshape input patches into matrix columns\n        # TODO 3: Reshape convolution weights into matrix for multiplication  \n        # TODO 4: Perform matrix multiplication: output = weight_matrix @ im2col_input\n        # TODO 5: Add bias and reshape output to proper 4D convolution shape\n        # TODO 6: Create Conv2dOp node for gradient computation\n        pass\n\ndef im2col(input_tensor, kernel_size, stride, padding):\n    \"\"\"Transform convolution input into matrix multiplication format.\"\"\"\n    # TODO 1: Calculate output spatial dimensions after convolution\n    # TODO 2: Create output matrix with shape (kernel_h*kernel_w*channels, output_h*output_w*batch)\n    # TODO 3: For each output position, extract corresponding input patch\n    # TODO 4: Flatten input patch and store as column in output matrix\n    # TODO 5: Handle edge cases with padding and stride correctly\n    pass\n\nclass MaxPool2d(Module):\n    \"\"\"2D max pooling with gradient tracking for non-differentiable max operation.\"\"\"\n    \n    def forward(self, x):\n        \"\"\"Apply max pooling and store argmax indices for gradient computation.\"\"\"\n        # TODO 1: Divide input into pooling windows based on kernel_size and stride\n        # TODO 2: Find maximum value in each pooling window\n        # TODO 3: Store indices of maximum values for backward pass\n        # TODO 4: Return pooled output with reduced spatial dimensions\n        pass\n```\n\n#### Serialization Infrastructure\n\n```python\n# serialization.py - Model persistence and loading\nimport json\nimport numpy as np\nfrom pathlib import Path\n\nclass ModelSerializer:\n    \"\"\"Handles saving and loading complete model state.\"\"\"\n    \n    @staticmethod\n    def save_model(model, optimizer, path, metadata=None):\n        \"\"\"Save complete training state to disk.\"\"\"\n        # TODO 1: Create save directory if it doesn't exist\n        # TODO 2: Save model architecture description as JSON\n        # TODO 3: Save model parameters using numpy .npz format\n        # TODO 4: Save optimizer state (including momentum buffers)\n        # TODO 5: Save training metadata (epoch, loss history, etc.)\n        # TODO 6: Create manifest file listing all saved components\n        pass\n    \n    @staticmethod\n    def load_model(path, model_class=None):\n        \"\"\"Load complete training state from disk.\"\"\"\n        # TODO 1: Read manifest file to understand save format version\n        # TODO 2: Load model architecture and instantiate model object\n        # TODO 3: Load parameter tensors and assign to model modules\n        # TODO 4: Load optimizer state and create optimizer object\n        # TODO 5: Load training metadata for training resumption\n        # TODO 6: Verify loaded state consistency and return components\n        pass\n\ndef model_to_onnx(model, example_input, output_path):\n    \"\"\"Export trained model to ONNX format for deployment.\"\"\"\n    # TODO 1: Trace model execution with example input to build computation graph\n    # TODO 2: Map framework operations to equivalent ONNX operations\n    # TODO 3: Convert parameter tensors to ONNX tensor format\n    # TODO 4: Create ONNX model proto with graph structure and parameters\n    # TODO 5: Validate ONNX model produces same outputs as original\n    # TODO 6: Save ONNX model to specified output path\n    pass\n```\n\n#### Performance Monitoring Infrastructure\n\n```python\n# performance_monitor.py - Complete performance analysis toolkit\nimport time\nimport psutil\nimport threading\nfrom collections import defaultdict\n\nclass FrameworkProfiler:\n    \"\"\"Comprehensive profiling for neural network framework operations.\"\"\"\n    \n    def __init__(self, enable_gpu_profiling=True):\n        self.operation_times = defaultdict(list)\n        self.memory_usage = []\n        self.gpu_utilization = []\n        self.enable_gpu = enable_gpu_profiling and GPU_AVAILABLE\n        self.profiling_active = False\n    \n    def start_profiling(self):\n        \"\"\"Begin collecting performance metrics.\"\"\"\n        # TODO 1: Reset all metric collections\n        # TODO 2: Start background thread for memory/GPU monitoring\n        # TODO 3: Install operation hooks for timing measurement\n        # TODO 4: Set profiling_active flag to enable metric collection\n        pass\n    \n    def profile_operation(self, operation_name, func, *args, **kwargs):\n        \"\"\"Time execution of specific operation and collect metrics.\"\"\"\n        # TODO 1: Record start time and memory usage\n        # TODO 2: Execute function with provided arguments\n        # TODO 3: Record end time and final memory usage\n        # TODO 4: Store timing and memory delta in operation_times\n        # TODO 5: Return function result unchanged\n        pass\n    \n    def generate_report(self):\n        \"\"\"Create comprehensive performance report.\"\"\"\n        # TODO 1: Calculate statistics (mean, std, min, max) for each operation\n        # TODO 2: Identify performance bottlenecks and memory leaks\n        # TODO 3: Generate plots for timing trends and memory usage\n        # TODO 4: Create summary table with optimization recommendations\n        # TODO 5: Return formatted report with actionable insights\n        pass\n```\n\n#### Extension Integration Checkpoints\n\nAfter implementing performance optimizations:\n- Verify GPU operations produce identical results to CPU versions using `torch.allclose(cpu_result, gpu_result.cpu(), atol=1e-6)`\n- Measure speedup on matrix multiplication: should see 10-50x improvement for large matrices (1000x1000 or larger)\n- Check memory usage with `nvidia-smi` - GPU memory should increase during computation, decrease after cleanup\n- Test operation fusion by comparing execution time of fused vs unfused operation chains\n\nAfter implementing advanced neural network features:\n- Train CNN on MNIST dataset - should achieve >95% accuracy within 5 epochs\n- Train LSTM on simple sequence prediction task - should learn basic patterns within reasonable time  \n- Verify batch normalization stabilizes training by comparing loss curves with and without normalization\n- Test gradient flow through deep networks (10+ layers) - gradients should not vanish or explode\n\nAfter implementing ecosystem integration:\n- Save and load trained model, verify identical predictions on test inputs\n- Export model to ONNX format, verify ONNX runtime produces same outputs\n- Integrate with external data loader, verify proper batch handling and preprocessing\n- Use hyperparameter optimization tool to find optimal learning rate automatically\n\n\n## Glossary\n\n> **Milestone(s):** All milestones - provides definitions for fundamental concepts used throughout tensor operations, automatic differentiation, neural modules, and training loops\n\nBuilding a neural network framework introduces a rich vocabulary of interconnected technical concepts. This glossary serves as your reference dictionary, providing precise definitions for every term used throughout the system design. Think of it as the foundation layer beneath all other components - without shared understanding of these concepts, the architectural discussions become ambiguous and implementation becomes inconsistent.\n\nEach definition here connects to specific implementation elements in our framework. These aren't abstract academic definitions, but practical explanations of how concepts manifest in working code. The terms are organized thematically to show relationships and build conceptual understanding progressively.\n\n### Core Tensor and Automatic Differentiation Concepts\n\nThe mathematical and algorithmic foundations that enable neural networks to learn through gradient-based optimization form the conceptual bedrock of our framework.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **Tensor** | N-dimensional array data structure that stores numerical values with shape metadata and optional gradient tracking. Acts as the fundamental data unit flowing through neural networks. | Implemented as `Tensor` class with `data: np.ndarray`, `requires_grad: bool`, `grad: Optional[Tensor]`, and `grad_fn: Optional[Operation]` fields |\n| **Automatic Differentiation (Autodiff)** | Mechanical computation of exact derivatives by applying the chain rule to elementary operations. Eliminates manual gradient derivation and reduces human error in complex neural networks. | Core capability provided by `Operation` classes implementing `forward()` and `backward()` methods with computation graph tracking |\n| **Reverse-Mode Autodiff** | Specific autodiff strategy that computes gradients by traversing the computation graph backwards from outputs to inputs. More efficient than forward-mode for neural networks with many parameters and few outputs. | Implemented through `backward()` methods that propagate gradients from loss tensor back through `grad_fn` operation chain |\n| **Computation Graph** | Directed acyclic graph where nodes represent operations and edges represent tensor data flow. Records the history of computations during forward pass to enable gradient computation during backward pass. | Built dynamically during forward pass as operations create new tensors with `grad_fn` references to parent operations |\n| **Chain Rule** | Fundamental calculus principle for differentiating composed functions: d/dx[f(g(x))] = f'(g(x)) × g'(x). Enables automatic differentiation by breaking complex expressions into elementary operations. | Applied in each `Operation.backward()` method which multiplies incoming gradients by local gradients before propagating to inputs |\n| **Broadcasting** | Automatic shape expansion mechanism that allows operations between tensors of different but compatible dimensions. Follows NumPy rules to avoid explicit tensor reshaping in user code. | Implemented in tensor arithmetic operations with corresponding gradient unbroadcasting in backward pass to maintain correct parameter shapes |\n| **Gradient Accumulation** | Process of summing gradients when a tensor participates in multiple operations within the same computation graph. Prevents gradient loss in networks with parameter sharing or complex connectivity. | Handled automatically by `Tensor.backward()` which accumulates gradients: `self.grad = gradient if self.grad is None else self.grad + gradient` |\n\n### Neural Network Architecture Concepts\n\nThe building blocks and organizational patterns that structure neural networks into learnable, composable systems.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **Module System** | Hierarchical organization pattern where neural network components are composable building blocks containing parameters and submodules. Enables complex architectures through simple composition. | Implemented through `Module` base class with `_parameters: Dict[str, Tensor]` and `_modules: Dict[str, Module]` collections |\n| **Parameter Registration** | Automatic tracking mechanism that identifies and collects trainable tensors within modules. Ensures optimizers can find all parameters requiring gradient updates. | Managed by `register_parameter(name, param)` method which validates tensor properties and adds to `_parameters` dictionary |\n| **Recursive Parameter Collection** | Depth-first traversal algorithm that gathers parameters from nested module hierarchies. Enables complex architectures while maintaining simple optimizer interfaces. | Implemented in `parameters()` method which yields own parameters then recursively calls `parameters()` on all submodules |\n| **Weight Initialization** | Strategy for setting initial parameter values that promote stable training dynamics. Critical for avoiding vanishing/exploding gradients in deep networks. | Provided through initialization functions like `xavier_uniform_(tensor, gain)` and `kaiming_uniform_(tensor, a, mode)` |\n| **Xavier Initialization** | Weight initialization scheme that maintains activation variance across layers by scaling initial weights based on fan-in and fan-out dimensions. Particularly effective for sigmoid/tanh activations. | Draws weights from uniform distribution with bounds ±√(6/(fan_in + fan_out)) scaled by activation gain |\n| **He Initialization** | Weight initialization optimized for ReLU activations that scales weights based on fan-in dimension only. Compensates for ReLU's activation pattern to maintain signal variance. | Draws weights from uniform distribution with bounds ±√(6/fan_in) to account for ReLU's zero-clipping behavior |\n| **Sequential Container** | Module that chains other modules in linear order, passing output of each module as input to the next. Simplifies construction of feedforward architectures. | Implemented as `Sequential` class storing ordered list of modules and forwarding data through each sequentially |\n| **Mode Switching** | Mechanism for changing neural network behavior between training and evaluation phases. Affects layers like dropout and batch normalization that behave differently during inference. | Controlled by `training: bool` flag with `train(mode=True)` and `eval()` methods propagating state to all submodules |\n| **Hierarchical Naming** | Dot notation system for identifying parameters and submodules within complex architectures. Enables targeted parameter access and debugging. | Generated by `named_parameters(prefix)` which concatenates module names with dots to create unique parameter paths |\n\n### Optimization and Training Concepts\n\nThe algorithms and processes that enable neural networks to learn from data through iterative parameter adjustment.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **Optimizer** | Algorithm that updates model parameters using gradients to minimize loss function. Encapsulates update rules, learning rates, and momentum strategies. | Implemented as `Optimizer` base class with `param_groups: List[Dict]` and `state: Dict[int, Dict]` for parameter and algorithm state management |\n| **Stochastic Gradient Descent (SGD)** | Fundamental optimization algorithm that updates parameters in the negative gradient direction scaled by learning rate. Foundation for more sophisticated optimizers. | Implemented as `SGD` class with momentum support: parameter update follows `param -= lr * (momentum * velocity + gradient)` |\n| **Momentum** | Velocity accumulation technique that dampens oscillations and accelerates convergence by incorporating previous gradient directions. Helps optimization escape local minima. | Stored in optimizer state as velocity buffers: `velocity = momentum * velocity + gradient` before applying parameter update |\n| **Adam** | Adaptive moment estimation optimizer that maintains per-parameter learning rates based on first and second moment estimates of gradients. Often converges faster than SGD. | Combines momentum-like first moments and RMSprop-like second moments with bias correction: `param -= lr * m_hat / (√v_hat + ε)` |\n| **Bias Correction** | Adjustment technique in Adam optimizer that compensates for initialization bias in moment estimates during early training iterations. Critical for proper convergence behavior. | Applied as `m_hat = m / (1 - beta1^t)` and `v_hat = v / (1 - beta2^t)` where t is iteration count |\n| **Learning Rate Scheduling** | Dynamic adjustment of learning rate during training based on metrics or predefined schedules. Balances exploration and convergence as training progresses. | Implemented through `LRScheduler` classes that modify optimizer learning rates based on training progress |\n| **Mini-Batch Training** | Processing fixed-size subsets of training data rather than individual samples or entire datasets. Balances computational efficiency with gradient estimate quality. | Managed by `DataLoader` with `batch_size: int` and `shuffle: bool` parameters controlling data batching and randomization |\n| **Parameter Updates** | Modification of model weights using optimizer-computed updates based on accumulated gradients. The core mechanism through which neural networks learn. | Coordinated by `step()` method which applies computed updates to all parameters in optimizer's parameter groups |\n\n### Training Loop and Coordination Concepts\n\nThe orchestration and management systems that coordinate the interaction between all framework components during the learning process.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **Training Loop** | Orchestrated sequence of forward pass, loss computation, backward pass, and parameter updates repeated over multiple epochs. The central process driving neural network learning. | Implemented in `train_epoch()` function coordinating model, data loader, optimizer, and loss function interactions |\n| **Forward Pass Data Flow** | Process where input data flows through network modules while building computation graph for gradient tracking. Produces predictions and prepares for gradient computation. | Managed by `coordinate_forward_pass()` which calls `model(inputs)` while tracking tensor operations and building computation graph |\n| **Backward Pass Coordination** | Gradient computation flow from loss back through network to compute parameter gradients. Follows computation graph in reverse topological order. | Initiated by `loss.backward()` which triggers `coordinate_backward_pass()` managing gradient computation and validation |\n| **Complete Training Step Sequence** | End-to-end training process: forward pass → loss computation → backward pass → parameter updates. Represents one iteration of the learning algorithm. | Implemented in `training_step()` method returning metrics and coordinating all training components |\n| **Loss Function** | Differentiable measure of prediction quality that quantifies difference between model outputs and target values. Provides gradient signal for optimization. | Implemented through `Loss` base class with subclasses like cross-entropy and mean squared error supporting gradient computation |\n| **Cross-Entropy Loss** | Standard loss function for classification tasks that measures divergence between predicted probability distributions and target labels. Well-suited for softmax outputs. | Computes `-∑ target_i * log(prediction_i)` with numerical stability handling and proper gradient computation |\n| **Mean Squared Error** | Standard loss function for regression tasks that penalizes squared differences between predictions and targets. Provides smooth gradients for continuous optimization. | Computes `mean((predictions - targets)²)` with straightforward gradient: `2 * (predictions - targets) / batch_size` |\n\n### Error Handling and Debugging Concepts\n\nThe systems and strategies for detecting, diagnosing, and recovering from failures during neural network development and training.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **Shape Mismatch** | Error condition where tensor dimensions are incompatible for requested operations. Common source of failures in neural network implementations. | Detected by operations before computation with descriptive `ShapeError` containing `shapes: List[Tuple]` and `operation: str` |\n| **Broadcasting Failure** | Specific type of shape mismatch where tensors cannot be automatically expanded according to broadcasting rules. Indicates incompatible tensor dimensions. | Handled by `BroadcastingError` with `shape1: Tuple`, `shape2: Tuple`, and `step_analysis: List[str]` for diagnostic information |\n| **Gradient Explosion** | Pathological training condition where gradient magnitudes become extremely large, leading to unstable parameter updates and training failure. | Detected by `GradientMonitor` tracking gradient norms and triggering alerts when values exceed reasonable thresholds |\n| **Gradient Vanishing** | Training problem where gradient magnitudes become extremely small, preventing effective learning in deep networks. Often caused by activation functions or weight initialization. | Identified through gradient magnitude analysis in `check_gradient_flow()` which compares gradients across network layers |\n| **Numerical Instability** | Computational condition where operations produce NaN (Not a Number) or infinity values, breaking subsequent computations. Requires careful numerical handling. | Caught by `validate_tensor_finite()` checks and reported through `NumericalInstabilityError` with tensor statistics |\n| **Computation Graph Memory Leak** | Memory management issue where circular references in computation graphs prevent garbage collection. Can exhaust memory during long training runs. | Prevented by `ComputationGraphTracker` managing graph lifecycle and `cleanup_graphs()` breaking circular references |\n| **Memory Management** | Systematic approach to controlling memory usage during training, including computation graph cleanup and tensor lifecycle management. Critical for training stability. | Implemented through `MemoryMonitor` tracking usage patterns and automatic cleanup systems preventing memory exhaustion |\n\n### Testing and Validation Concepts\n\nThe methodologies and tools for verifying correctness of neural network framework implementations.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **Gradient Correctness Testing** | Validation technique comparing automatic differentiation results with numerical differentiation to verify implementation accuracy. Essential for debugging autodiff systems. | Implemented in `GradientTester` class using finite differences to validate `Operation.backward()` implementations |\n| **Numerical Differentiation** | Finite difference approximation of derivatives using small perturbations: f'(x) ≈ [f(x+h) - f(x-h)]/(2h). Provides ground truth for gradient validation. | Used in `numerical_gradient()` function with configurable step size `h=1e-5` for comparing against autodiff results |\n| **Central Difference** | Specific numerical differentiation method using symmetric perturbations around evaluation point. More accurate than forward/backward differences for gradient checking. | Applied as `[f(x+h) - f(x-h)] / (2h)` in gradient validation providing second-order accuracy |\n| **Relative Error** | Scale-invariant comparison metric for gradient accuracy: `|computed - expected| / max(|expected|, ε)`. Handles both large and small gradient magnitudes appropriately. | Computed by `relative_error()` function with tolerance `eps=1e-8` for robust gradient comparison |\n| **Milestone Verification** | Integration testing after each major development stage to confirm framework components work correctly together. Prevents compound errors in complex systems. | Implemented as `MilestoneCheckpoints` class with automated verification for each project milestone |\n| **End-to-End Training Tests** | Complete neural network training on toy problems with known expected behaviors. Validates entire framework integration from data loading to convergence. | Executed by `EndToEndTester` using synthetic datasets like XOR problem and linear regression with convergence validation |\n| **Toy Datasets** | Synthetic problems with known solutions designed for testing framework correctness. Include linear regression, XOR classification, and overfitting tests. | Generated by `ToyDatasets` methods creating predictable data patterns for systematic framework validation |\n| **Statistical Validation** | Testing approach using multiple random seeds and statistical analysis to ensure consistent framework behavior across different initialization conditions. | Implemented in `run_statistical_validation()` executing tests multiple times and analyzing result distributions |\n\n### Performance and Extension Concepts\n\nAdvanced features and optimizations that extend the basic framework capabilities for production use and specialized applications.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **GPU Acceleration** | Massively parallel computation using graphics processors to accelerate tensor operations. Essential for training large neural networks efficiently. | Supported through `GPUTensor` class with `device: str` attribute and `to(device)` method for CPU/GPU transfers |\n| **Operation Fusion** | Optimization technique combining multiple elementary operations into single optimized kernels. Reduces memory bandwidth and improves computational efficiency. | Implemented by `FusionOptimizer` analyzing computation graphs for `fusion_patterns: List` and applying transformations |\n| **Mixed Precision Training** | Memory efficiency technique using 16-bit floating point for most operations while maintaining 32-bit precision for gradient computation. Accelerates training on modern hardware. | Requires specialized tensor types and automatic loss scaling to prevent gradient underflow |\n| **Model Serialization** | Saving and loading trained model state including parameters, optimizer state, and training metadata. Essential for model deployment and checkpoint recovery. | Provided by `ModelSerializer` with `save_model()` and `load_model()` methods handling complete training state |\n| **ONNX Export** | Converting models to Open Neural Network Exchange format for interoperability with other frameworks and deployment platforms. Enables production integration. | Implemented through `model_to_onnx()` function translating framework operations to standardized representation |\n| **Ecosystem Integration** | Connecting framework with existing machine learning tools, data pipelines, and deployment infrastructure. Bridges educational implementation with practical applications. | Includes compatibility layers, data format converters, and API adapters for common ML workflows |\n\n### Framework-Specific Implementation Terms\n\nTerminology specific to our educational neural network framework design and implementation choices.\n\n| Term | Definition | Framework Connection |\n|------|------------|---------------------|\n| **Educational Clarity** | Design principle prioritizing code readability and conceptual understanding over maximum performance optimization. Guides architectural decisions throughout framework. | Influences API design, error messages, internal documentation, and implementation complexity trade-offs |\n| **Four-Layer Architecture** | Framework organization with distinct layers: tensor operations, autodiff engine, neural modules, and optimizers. Provides clear separation of concerns and learning progression. | Reflected in project structure with separate modules for each layer and well-defined interfaces between them |\n| **Define-by-Run** | Dynamic computation graph construction during forward pass execution. Enables flexible control flow and debugging compared to static graph approaches. | Implemented through immediate operation execution with automatic graph building via `grad_fn` references |\n| **Eager Execution** | Immediate evaluation of operations as they are called rather than building static computation graphs. Simplifies debugging and enables Python control flow. | Default behavior where tensor operations execute immediately while maintaining gradient tracking capabilities |\n| **Scope Creep** | Uncontrolled addition of features beyond core learning objectives that can overwhelm learners and complicate implementation. Must be actively managed in educational projects. | Prevented through explicit non-goals and milestone-focused development restricting feature additions |\n| **Topological Sort** | Graph algorithm ensuring correct traversal order during backward pass gradient computation. Critical for proper gradient flow in complex computation graphs. | Implemented in `topological_sort()` function providing dependency-ordered sequence of operations for backward pass |\n\n### Constants and Configuration Values\n\nStandard values and thresholds used throughout the framework for numerical stability, testing, and monitoring.\n\n| Constant | Value | Purpose | Usage Context |\n|----------|--------|---------|---------------|\n| **tolerance** | `1e-5` | Default tolerance for gradient checking and numerical comparisons | Used in `check_gradients()` and relative error computations |\n| **h** | `1e-5` | Step size for numerical differentiation in gradient validation | Applied in `numerical_gradient()` finite difference computations |\n| **epsilon** | `1e-8` | Small constant for numerical stability in optimizers and computations | Used in Adam optimizer denominator and division-by-zero prevention |\n| **beta1** | `0.9` | Exponential decay rate for first moment estimates in Adam optimizer | Default value for momentum-like term in adaptive optimization |\n| **beta2** | `0.999` | Exponential decay rate for second moment estimates in Adam optimizer | Default value for variance-like term in adaptive optimization |\n| **learning_rate** | `0.01` | Default step size multiplier for parameter updates | Base value for SGD and initial rate for other optimizers |\n| **momentum** | `0.9` | Default coefficient for velocity accumulation in SGD with momentum | Balances current gradient with historical direction |\n| **check_frequency** | `10` | Sampling rate for gradient monitoring and validation during training | Determines how often gradient health checks are performed |\n| **warning_threshold** | `0.8` | Memory usage fraction triggering warnings in memory monitoring | Used by `MemoryMonitor` to alert before critical memory pressure |\n| **critical_threshold** | `0.9` | Memory usage fraction triggering critical alerts and cleanup | Threshold for forced garbage collection and graph cleanup |\n| **success_rate_threshold** | `0.8` | Minimum success rate for statistical validation test passage | Used in multi-seed testing to determine overall test success |\n\n### Error Types and Exception Hierarchy\n\nStructured error handling system providing detailed diagnostic information for common failure modes in neural network frameworks.\n\n| Error Type | Hierarchy | Key Fields | Usage Context |\n|------------|-----------|------------|---------------|\n| **NeuralFrameworkError** | Base class | `message: str`, `details: Dict`, `stack_info: List` | Root exception type for all framework-specific errors |\n| **ShapeError** | Extends NeuralFrameworkError | `shapes: List[Tuple]`, `operation: str` | Tensor dimension incompatibility in operations |\n| **BroadcastingError** | Extends ShapeError | `shape1: Tuple`, `shape2: Tuple`, `step_analysis: List[str]` | Broadcasting rule violations with detailed analysis |\n| **GradientError** | Extends NeuralFrameworkError | Base fields plus gradient-specific context | Base class for all gradient computation failures |\n| **NumericalInstabilityError** | Extends GradientError | `tensor_info: Dict` with `nan_count`, `inf_count` | NaN/infinity detection in computations |\n\nThis comprehensive glossary serves as both reference documentation and conceptual foundation. Each term connects directly to implementation elements, making it a practical guide for both understanding and implementing the neural network framework. The definitions progress from fundamental mathematical concepts through architectural patterns to practical implementation details, mirroring the learning journey from theory to working code.\n\n### Implementation Guidance\n\nThe glossary itself doesn't require implementation code, but maintaining consistent terminology throughout your framework is critical for code clarity and debugging effectiveness. Here's how to integrate these concepts into your implementation:\n\n**A. Technology Recommendations for Documentation:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|------------------|\n| Code Documentation | Docstrings with type hints | Sphinx with automatic API generation |\n| Error Messages | String formatting with context | Structured error objects with detailed diagnostics |\n| Debugging Output | Print statements with prefixes | Logging framework with levels and formatting |\n| Type Checking | Manual validation in methods | Static type checker like mypy |\n\n**B. Terminology Consistency Enforcement:**\n\nCreate a constants file that defines all standard terminology to prevent naming inconsistencies:\n\n```python\n# framework/constants.py\n\"\"\"\nCentral definitions for all framework terminology and constants.\nEnsures consistent naming across modules.\n\"\"\"\n\n# Numerical constants\nDEFAULT_TOLERANCE = 1e-5\nNUMERICAL_DIFF_STEP = 1e-5\nEPSILON = 1e-8\n\n# Optimizer defaults\nDEFAULT_LEARNING_RATE = 0.01\nDEFAULT_MOMENTUM = 0.9\nADAM_BETA1 = 0.9\nADAM_BETA2 = 0.999\n\n# Monitoring thresholds\nGRADIENT_CHECK_FREQUENCY = 10\nMEMORY_WARNING_THRESHOLD = 0.8\nMEMORY_CRITICAL_THRESHOLD = 0.9\n\n# Error message templates\nSHAPE_MISMATCH_MSG = \"Shape mismatch in {operation}: expected {expected}, got {actual}\"\nBROADCASTING_FAILED_MSG = \"Cannot broadcast shapes {shape1} and {shape2}: {reason}\"\nGRADIENT_EXPLOSION_MSG = \"Gradient explosion detected: max gradient norm {max_norm:.2e}\"\n\n# Field names (prevent typos in attribute access)\nREQUIRES_GRAD_FIELD = \"requires_grad\"\nGRAD_FN_FIELD = \"grad_fn\" \nDATA_FIELD = \"data\"\nPARAMETERS_FIELD = \"_parameters\"\nMODULES_FIELD = \"_modules\"\n```\n\n**C. Error Message Infrastructure:**\n\nBuild a diagnostic system that uses consistent terminology in error reporting:\n\n```python\n# framework/errors.py\n\"\"\"\nStructured error handling with consistent terminology.\n\"\"\"\n\nclass NeuralFrameworkError(Exception):\n    \"\"\"Base class for all framework-specific errors.\"\"\"\n    \n    def __init__(self, message: str, details: Dict = None, operation_context: str = None):\n        # TODO: Store message, details dict, and operation context\n        # TODO: Capture stack trace information for debugging\n        # TODO: Format error message with consistent terminology\n        super().__init__(self._format_message())\n    \n    def _format_message(self) -> str:\n        # TODO: Combine message, details, and context into readable format\n        # TODO: Include relevant tensor shapes, operation names, parameter values\n        # TODO: Suggest potential fixes based on error type\n        pass\n\nclass ShapeError(NeuralFrameworkError):\n    \"\"\"Tensor shape incompatibility errors.\"\"\"\n    \n    def __init__(self, shapes: List[Tuple], operation: str, expected_pattern: str = None):\n        # TODO: Store shape information and operation context\n        # TODO: Generate detailed shape analysis showing incompatibility\n        # TODO: Suggest shape corrections (transpose, reshape, broadcast)\n        pass\n\nclass GradientError(NeuralFrameworkError):\n    \"\"\"Base class for gradient computation failures.\"\"\"\n    \n    def __init__(self, message: str, tensor_info: Dict = None, gradient_context: Dict = None):\n        # TODO: Capture gradient-specific diagnostic information\n        # TODO: Include gradient magnitudes, tensor shapes, operation chain\n        # TODO: Provide debugging suggestions for gradient flow issues\n        pass\n```\n\n**D. Debugging Integration:**\n\nCreate debugging utilities that use consistent terminology for problem diagnosis:\n\n```python\n# framework/debugging.py\n\"\"\"\nDebugging utilities with consistent terminology and detailed diagnostics.\n\"\"\"\n\ndef validate_gradient_flow(model: Module, loss: Tensor) -> Dict[str, Any]:\n    \"\"\"\n    Check gradient flow through all model parameters using standard terminology.\n    Returns diagnostic information with consistent field names.\n    \"\"\"\n    # TODO: Iterate through model.named_parameters() \n    # TODO: Check each parameter's .grad field for None, zeros, or extreme values\n    # TODO: Report gradient norms using consistent magnitude classifications\n    # TODO: Identify layers with gradient flow problems using standard descriptions\n    pass\n\ndef analyze_tensor_shapes(tensors: List[Tensor], operation: str) -> Dict[str, Any]:\n    \"\"\"\n    Analyze tensor shape compatibility using framework terminology.\n    \"\"\"\n    # TODO: Check shapes against broadcasting rules\n    # TODO: Generate step-by-step broadcasting analysis\n    # TODO: Suggest fixes using standard reshape/transpose terminology\n    # TODO: Return structured analysis with consistent field names\n    pass\n\ndef check_numerical_stability(tensor: Tensor, operation_context: str) -> Dict[str, Any]:\n    \"\"\"\n    Validate tensor contains finite values using standard stability checks.\n    \"\"\"\n    # TODO: Count NaN, infinity, and extreme values\n    # TODO: Check gradient magnitudes against standard thresholds  \n    # TODO: Generate stability report with consistent terminology\n    # TODO: Suggest numerical fixes (gradient clipping, learning rate adjustment)\n    pass\n```\n\n**E. Milestone Checkpoints with Terminology Validation:**\n\nEnsure your implementation uses correct terminology by building verification into milestone tests:\n\n```python\n# tests/test_terminology.py\n\"\"\"\nVerify consistent terminology usage across framework implementation.\n\"\"\"\n\ndef test_tensor_field_names():\n    \"\"\"Verify Tensor class uses exact field names from glossary.\"\"\"\n    # TODO: Create Tensor instance and verify it has 'data', 'requires_grad', 'grad', 'grad_fn' fields\n    # TODO: Check field types match specifications\n    # TODO: Verify no extra or missing fields\n    pass\n\ndef test_module_interface_consistency():\n    \"\"\"Verify Module classes use standard method names and signatures.\"\"\"\n    # TODO: Check Module has 'parameters()', 'named_parameters()', 'train()', 'eval()' methods\n    # TODO: Verify method signatures match glossary specifications  \n    # TODO: Test parameter registration uses '_parameters' and '_modules' fields\n    pass\n\ndef test_error_message_terminology():\n    \"\"\"Verify error messages use consistent terminology from glossary.\"\"\"\n    # TODO: Trigger shape mismatch and check error message uses standard terms\n    # TODO: Test broadcasting failure uses correct shape analysis terminology\n    # TODO: Verify gradient errors use standard gradient flow descriptions\n    pass\n```\n\n**F. Documentation Generation:**\n\nUse the glossary terms to generate consistent API documentation:\n\n```python\n# docs/generate_glossary.py\n\"\"\"\nGenerate framework documentation using glossary terminology.\n\"\"\"\n\ndef generate_api_docs(module_path: str) -> str:\n    \"\"\"\n    Generate API documentation with glossary term definitions.\n    \"\"\"\n    # TODO: Extract class and method definitions from module\n    # TODO: Cross-reference with glossary definitions\n    # TODO: Generate documentation with consistent terminology\n    # TODO: Include links between related concepts\n    pass\n\ndef validate_docstring_terminology(module_path: str) -> List[str]:\n    \"\"\"\n    Check docstrings use terminology consistent with glossary.\n    \"\"\"\n    # TODO: Parse all docstrings in module\n    # TODO: Check against approved terminology list\n    # TODO: Report inconsistencies with suggested corrections\n    # TODO: Generate terminology compliance report\n    pass\n```\n\nThis implementation guidance ensures your neural network framework maintains terminological consistency that matches the comprehensive definitions in this glossary. Consistent terminology is crucial for debugging, collaboration, and educational clarity.\n"}